

# =🚩 Python Tutorial - Getting Started
- The Zen of Python https://www.python.org/dev/peps/pep-0020/
- Python Tutorial https://docs.python.org/3/tutorial/index.html
- Python HOWTOs https://docs.python.org/3/howto/index.html
- Python Developer's Guide https://devguide.python.org
- PEP 0 -- Index of Python Enhancement Proposals (PEPs) https://www.python.org/dev/peps/#introduction

Python 手册内容介绍：

- 基础入门教程参考 The Python Tutorial；
- 进阶指导教程 Python HOWTOs；
- 各个 Python 版本新引入特性参数 What’s New in Python；
- 语言特性、语法词法规则参考 The Python Language Reference；
- 标准库及 API 使用参考 The Python Standard Library；
- 如果有计划扩展 CPython 现有功能，或在 C/C++ 项目中嵌入解析器运行时，参考：
    - Extending and Embedding the Python Interpreter
    - Python/C API Reference Manual；

除了以上官方手册提供的参数内容，终极的资料还是在 Python 源代码中，现在 Python 3.10.2 已经发布。

Python 是脚本语言，通过解析器执行脚本程序，打开解析器主程序就可以进行 Interactive Mode 解析用户输入的脚本。

最受欢迎的两个特性是“简单”和“可移植”，与可用的工具和库一起，Python 为建模、游戏脚本、仿真开发人员提供了一种很好的语言。最重要的是，它是免费的，为 Python 程序员编写的工具和库也是免费的。


The Python Tutorial

Python is an easy to learn, powerful programming language. It has efficient high-level data structures and a simple but effective approach to object-oriented programming. Python’s elegant syntax and dynamic typing, together with its interpreted nature, make it an ideal language for scripting and rapid application development in many areas on most platforms.

The Python interpreter and the extensive standard library are freely available in source or binary form for all major platforms from the Python Web site, https://www.python.org/, and may be freely distributed. The same site also contains distributions of and pointers to many free third party Python modules, programs and tools, and additional documentation.

The Python interpreter is easily extended with new functions and data types implemented in C or C++ (or other languages callable from C). Python is also suitable as an extension language for customizable applications.

This tutorial introduces the reader informally to the basic concepts and features of the Python language and system. It helps to have a Python interpreter handy for hands-on experience, but all examples are self-contained, so the tutorial can be read off-line as well.

For a description of standard objects and modules, see The Python Standard Library. The Python Language Reference gives a more formal definition of the language. To write extensions in C or C++, read Extending and Embedding the Python Interpreter and Python/C API Reference Manual. There are also several books covering Python in depth.

This tutorial does not attempt to be comprehensive and cover every single feature, or even every commonly used feature. Instead, it introduces many of Python’s most noteworthy features, and will give you a good idea of the language’s flavor and style. After reading it, you will be able to read and write Python modules and programs, and you will be ready to learn more about the various Python library modules described in The Python Standard Library.

The Glossary is also worth going through.

• 1. Whetting Your Appetite
• 2. Using the Python Interpreter
◦2.1. Invoking the Interpreter
◾2.1.1. Argument Passing
◾2.1.2. Interactive Mode
◦2.2. The Interpreter and Its Environment
◾2.2.1. Source Code Encoding

• 3. An Informal Introduction to Python
◦3.1. Using Python as a Calculator
◾3.1.1. Numbers
◾3.1.2. Strings
◾3.1.3. Lists

◦3.2. First Steps Towards Programming

• 4. More Control Flow Tools
◦4.1. if Statements
◦4.2. for Statements
◦4.3. The range() Function
◦4.4. break and continue Statements, and else Clauses on Loops
◦4.5. pass Statements
◦4.6. Defining Functions
◦4.7. More on Defining Functions
◾4.7.1. Default Argument Values
◾4.7.2. Keyword Arguments
◾4.7.3. Special parameters
◾4.7.3.1. Positional-or-Keyword Arguments
◾4.7.3.2. Positional-Only Parameters
◾4.7.3.3. Keyword-Only Arguments
◾4.7.3.4. Function Examples
◾4.7.3.5. Recap

◾4.7.4. Arbitrary Argument Lists
◾4.7.5. Unpacking Argument Lists
◾4.7.6. Lambda Expressions
◾4.7.7. Documentation Strings
◾4.7.8. Function Annotations

◦4.8. Intermezzo: Coding Style

• 5. Data Structures
◦5.1. More on Lists
◾5.1.1. Using Lists as Stacks
◾5.1.2. Using Lists as Queues
◾5.1.3. List Comprehensions
◾5.1.4. Nested List Comprehensions

◦5.2. The del statement
◦5.3. Tuples and Sequences
◦5.4. Sets
◦5.5. Dictionaries
◦5.6. Looping Techniques
◦5.7. More on Conditions
◦5.8. Comparing Sequences and Other Types

• 6. Modules
◦6.1. More on Modules
◾6.1.1. Executing modules as scripts
◾6.1.2. The Module Search Path
◾6.1.3. “Compiled” Python files

◦6.2. Standard Modules
◦6.3. The dir() Function
◦6.4. Packages
◾6.4.1. Importing * From a Package
◾6.4.2. Intra-package References
◾6.4.3. Packages in Multiple Directories


• 7. Input and Output
◦7.1. Fancier Output Formatting
◾7.1.1. Formatted String Literals
◾7.1.2. The String format() Method
◾7.1.3. Manual String Formatting
◾7.1.4. Old string formatting

◦7.2. Reading and Writing Files
◾7.2.1. Methods of File Objects
◾7.2.2. Saving structured data with json


• 8. Errors and Exceptions
◦8.1. Syntax Errors
◦8.2. Exceptions
◦8.3. Handling Exceptions
◦8.4. Raising Exceptions
◦8.5. Exception Chaining
◦8.6. User-defined Exceptions
◦8.7. Defining Clean-up Actions
◦8.8. Predefined Clean-up Actions

• 9. Classes
◦9.1. A Word About Names and Objects
◦9.2. Python Scopes and Namespaces
◾9.2.1. Scopes and Namespaces Example

◦9.3. A First Look at Classes
◾9.3.1. Class Definition Syntax
◾9.3.2. Class Objects
◾9.3.3. Instance Objects
◾9.3.4. Method Objects
◾9.3.5. Class and Instance Variables

◦9.4. Random Remarks
◦9.5. Inheritance
◾9.5.1. Multiple Inheritance

◦9.6. Private Variables
◦9.7. Odds and Ends
◦9.8. Iterators
◦9.9. Generators
◦9.10. Generator Expressions

• 10. Brief Tour of the Standard Library
◦10.1. Operating System Interface
◦10.2. File Wildcards
◦10.3. Command Line Arguments
◦10.4. Error Output Redirection and Program Termination
◦10.5. String Pattern Matching
◦10.6. Mathematics
◦10.7. Internet Access
◦10.8. Dates and Times
◦10.9. Data Compression
◦10.10. Performance Measurement
◦10.11. Quality Control
◦10.12. Batteries Included

• 11. Brief Tour of the Standard Library — Part II
◦11.1. Output Formatting
◦11.2. Templating
◦11.3. Working with Binary Data Record Layouts
◦11.4. Multi-threading
◦11.5. Logging
◦11.6. Weak References
◦11.7. Tools for Working with Lists
◦11.8. Decimal Floating Point Arithmetic

• 12. Virtual Environments and Packages
◦12.1. Introduction
◦12.2. Creating Virtual Environments
◦12.3. Managing Packages with pip

• 13. What Now?
• 14. Interactive Input Editing and History Substitution
◦14.1. Tab Completion and History Editing
◦14.2. Alternatives to the Interactive Interpreter

• 15. Floating Point Arithmetic: Issues and Limitations
◦15.1. Representation Error

• 16. Appendix
◦16.1. Interactive Mode
◾16.1.1. Error Handling
◾16.1.2. Executable Python Scripts
◾16.1.3. The Interactive Startup File
◾16.1.4. The Customization Modules

## ==⚡ 1. Whetting Your Appetite

Python vs C/C++，脚本语言的优势就是即时运行，不需要复杂的静态编译时间上的等候。这是 Python 的其中一大优势，另外，更重要的是 Python 拥有大量的基础模块，和良好的 C/C++ 扩展能力。

If you do much work on computers, eventually you find that there’s some task you’d like to automate. For example, you may wish to perform a search-and-replace over a large number of text files, or rename and rearrange a bunch of photo files in a complicated way. Perhaps you’d like to write a small custom database, or a specialized GUI application, or a simple game.

If you’re a professional software developer, you may have to work with several C/C++/Java libraries but find the usual write/compile/test/re-compile cycle is too slow. Perhaps you’re writing a test suite for such a library and find writing the testing code a tedious task. Or maybe you’ve written a program that could use an extension language, and you don’t want to design and implement a whole new language for your application.

Python is just the language for you.

You could write a Unix shell script or Windows batch files for some of these tasks, but shell scripts are best at moving around files and changing text data, not well-suited for GUI applications or games. You could write a C/C++/Java program, but it can take a lot of development time to get even a first-draft program. Python is simpler to use, available on Windows, Mac OS X, and Unix operating systems, and will help you get the job done more quickly.

Python is simple to use, but it is a real programming language, offering much more structure and support for large programs than shell scripts or batch files can offer. On the other hand, Python also offers much more error checking than C, and, being a very-high-level language, it has high-level data types built in, such as flexible arrays and dictionaries. Because of its more general data types Python is applicable to a much larger problem domain than Awk or even Perl, yet many things are at least as easy in Python as in those languages.

Python allows you to split your program into modules that can be reused in other Python programs. It comes with a large collection of standard modules that you can use as the basis of your programs — or as examples to start learning to program in Python. Some of these modules provide things like file I/O, system calls, sockets, and even interfaces to graphical user interface toolkits like Tk.

Python is an interpreted language, which can save you considerable time during program development because no compilation and linking is necessary. The interpreter can be used interactively, which makes it easy to experiment with features of the language, to write throw-away programs, or to test functions during bottom-up program development. It is also a handy desk calculator.

Python enables programs to be written compactly and readably. Programs written in Python are typically much shorter than equivalent C, C++, or Java programs, for several reasons:

• the high-level data types allow you to express complex operations in a single statement;
• statement grouping is done by indentation instead of beginning and ending brackets;
• no variable or argument declarations are necessary.

Python is extensible: if you know how to program in C it is easy to add a new built-in function or module to the interpreter, either to perform critical operations at maximum speed, or to link Python programs to libraries that may only be available in binary form (such as a vendor-specific graphics library). Once you are really hooked, you can link the Python interpreter into an application written in C and use it as an extension or command language for that application.

By the way, the language is named after the BBC show “Monty Python’s Flying Circus” and has nothing to do with reptiles. Making references to Monty Python skits in documentation is not only allowed, it is encouraged!

Now that you are all excited about Python, you’ll want to examine it in some more detail. Since the best way to learn a language is to use it, the tutorial invites you to play with the Python interpreter as you read.

In the next chapter, the mechanics of using the interpreter are explained. This is rather mundane information, but essential for trying out the examples shown later.

The rest of the tutorial introduces various features of the Python language and system through examples, beginning with simple expressions, statements and data types, through functions and modules, and finally touching upon advanced concepts like exceptions and user-defined classes.


## ==⚡ 2. Using the Python Interpreter

### ===🗝 2.1. Invoking the Interpreter

The Python interpreter is usually installed as /usr/local/bin/python3.9 on those machines where it is available; putting /usr/local/bin in your Unix shell’s search path makes it possible to start it by typing the command:


>python3.9


to the shell. [1] Since the choice of the directory where the interpreter lives is an installation option, other places are possible; check with your local Python guru or system administrator. (E.g., /usr/local/python is a popular alternative location.)

On Windows machines where you have installed Python from the Microsoft Store, the python3.9 command will be available. If you have the py.exe launcher installed, you can use the py command. See Excursus: Setting environment variables for other ways to launch Python.

Typing an end-of-file character (Control-D on Unix, Control-Z on Windows) at the primary prompt causes the interpreter to exit with a zero exit status. If that doesn’t work, you can exit the interpreter by typing the following command: quit().

The interpreter’s line-editing features include interactive editing, history substitution and code completion on systems that support the GNU Readline library. Perhaps the quickest check to see whether command line editing is supported is typing Control-P to the first Python prompt you get. If it beeps, you have command line editing; see Appendix Interactive Input Editing and History Substitution for an introduction to the keys. If nothing appears to happen, or if ^P is echoed, command line editing isn’t available; you’ll only be able to use backspace to remove characters from the current line.

The interpreter operates somewhat like the Unix shell: when called with standard input connected to a tty device, it reads and executes commands interactively; when called with a file name argument or with a file as standard input, it reads and executes a script from that file.

A second way of starting the interpreter is python -c command [arg] ..., which executes the statement(s) in command, analogous to the shell’s -c option. Since Python statements often contain spaces or other characters that are special to the shell, it is usually advised to quote command in its entirety with single quotes.

Some Python modules are also useful as scripts. These can be invoked using python -m module [arg] ..., which executes the source file for module as if you had spelled out its full name on the command line.

When a script file is used, it is sometimes useful to be able to run the script and enter interactive mode afterwards. This can be done by passing -i before the script.

All command line options are described in Command line and environment.


### ===🗝 2.1.1. Argument Passing

When known to the interpreter, the script name and additional arguments thereafter are turned into a list of strings and assigned to the argv variable in the sys module. You can access this list by executing import sys. The length of the list is at least one; when no script and no arguments are given, sys.argv[0] is an empty string. When the script name is given as '-' (meaning standard input), sys.argv[0] is set to '-'. When -c command is used, sys.argv[0] is set to '-c'. When -m module is used, sys.argv[0] is set to the full name of the located module. Options found after -c command or -m module are not consumed by the Python interpreter’s option processing but left in sys.argv for the command or module to handle.


### ===🗝 2.1.2. Interactive Mode

When commands are read from a tty, the interpreter is said to be in interactive mode. In this mode it prompts for the next command with the primary prompt, usually three greater-than signs (>>>); for continuation lines it prompts with the secondary prompt, by default three dots (...). The interpreter prints a welcome message stating its version number and a copyright notice before printing the first prompt:


>$ python3.9
Python 3.9 (default, June 4 2019, 09:25:04)
[GCC 4.8.2] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>>


Continuation lines are needed when entering a multi-line construct. As an example, take a look at this if statement:


>>> the_world_is_flat = True
>>> if the_world_is_flat:
...     print("Be careful not to fall off!")
...
Be careful not to fall off!


For more on interactive mode, see Interactive Mode.


### ===🗝 2.2. The Interpreter and Its Environment


2.2.1. Source Code Encoding

By default, Python source files are treated as encoded in UTF-8. In that encoding, characters of most languages in the world can be used simultaneously in string literals, identifiers and comments — although the standard library only uses ASCII characters for identifiers, a convention that any portable code should follow. To display all these characters properly, your editor must recognize that the file is UTF-8, and it must use a font that supports all the characters in the file.

To declare an encoding other than the default one, a special comment line should be added as the first line of the file. The syntax is as follows:


    # -*- coding: encoding -*-


where encoding is one of the valid codecs supported by Python.

For example, to declare that Windows-1252 encoding is to be used, the first line of your source code file should be:


    # -*- coding: cp1252 -*-


One exception to the first line rule is when the source code starts with a UNIX “shebang” line. In this case, the encoding declaration should be added as the second line of the file. For example:


    #!/usr/bin/env python3
    # -*- coding: cp1252 -*-


Footnotes

[1] On Unix, the Python 3.x interpreter is by default not installed with the executable named python, so that it does not conflict with a simultaneously installed Python 2.x executable. 

## ==⚡ 3. An Informal Introduction to Python


In the following examples, input and output are distinguished by the presence or absence of prompts (>>> and …): to repeat the example, you must type everything after the prompt, when the prompt appears; lines that do not begin with a prompt are output from the interpreter. Note that a secondary prompt on a line by itself in an example means you must type a blank line; this is used to end a multi-line command.

Many of the examples in this manual, even those entered at the interactive prompt, include comments. Comments in Python start with the hash character, #, and extend to the end of the physical line. A comment may appear at the start of a line or following whitespace or code, but not within a string literal. A hash character within a string literal is just a hash character. Since comments are to clarify code and are not interpreted by Python, they may be omitted when typing in examples.

Some examples:


>>># this is the first comment
spam = 1  # and this is the second comment
          # ... and now a third!
text = "# This is not a comment because it's inside quotes."


### ===🗝 3.1. Using Python as a Calculator

Let’s try some simple Python commands. Start the interpreter and wait for the primary prompt, >>>. (It shouldn’t take long.)


### ===🗝 3.1.1. Numbers

The interpreter acts as a simple calculator: you can type an expression at it and it will write the value. Expression syntax is straightforward: the operators +, -, * and / work just like in most other languages (for example, Pascal or C); parentheses (()) can be used for grouping. For example:


>>> 2 + 2
4
>>> 50 - 5*6
20
>>> (50 - 5*6) / 4
5.0
>>> 8 / 5  # division always returns a floating point number
1.6


The integer numbers (e.g. 2, 4, 20) have type int, the ones with a fractional part (e.g. 5.0, 1.6) have type float. We will see more about numeric types later in the tutorial.

Division (/) always returns a float. To do floor division and get an integer result (discarding any fractional result) you can use the // operator; to calculate the remainder you can use %:


>>> 17 / 3  # classic division returns a float
5.666666666666667
>>>
>>> 17 // 3  # floor division discards the fractional part
5
>>> 17 % 3  # the % operator returns the remainder of the division
2
>>> 5 * 3 + 2  # result * divisor + remainder
17


With Python, it is possible to use the ** operator to calculate powers [1]:


>>> 5 ** 2  # 5 squared
25
>>> 2 ** 7  # 2 to the power of 7
128


The equal sign (=) is used to assign a value to a variable. Afterwards, no result is displayed before the next interactive prompt:


>>> width = 20
>>> height = 5 * 9
>>> width * height
900


If a variable is not “defined” (assigned a value), trying to use it will give you an error:


>>> n  # try to access an undefined variable
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'n' is not defined


There is full support for floating point; operators with mixed type operands convert the integer operand to floating point:


>>> 4 * 3.75 - 1
14.0


In interactive mode, the last printed expression is assigned to the variable `_`. This means that when you are using Python as a desk calculator, it is somewhat easier to continue calculations, for example:


>>> tax = 12.5 / 100
>>> price = 100.50
>>> price * tax
12.5625
>>> price + _
113.0625
>>> round(_, 2)
113.06


This variable should be treated as read-only by the user. Don’t explicitly assign a value to it — you would create an independent local variable with the same name masking the built-in variable with its magic behavior.

In addition to int and float, Python supports other types of numbers, such as Decimal and Fraction. Python also has built-in support for complex numbers, and uses the j or J suffix to indicate the imaginary part (e.g. 3+5j).


### ===🗝 3.1.2. Strings

Besides numbers, Python can also manipulate strings, which can be expressed in several ways. They can be enclosed in single quotes ('...') or double quotes ("...") with the same result [2]. \ can be used to escape quotes:


>>> 'spam eggs'  # single quotes
'spam eggs'
>>> 'doesn\'t'  # use \' to escape the single quote...
"doesn't"
>>> "doesn't"  # ...or use double quotes instead
"doesn't"
>>> '"Yes," they said.'
'"Yes," they said.'
>>> "\"Yes,\" they said."
'"Yes," they said.'
>>> '"Isn\'t," they said.'
'"Isn\'t," they said.'


In the interactive interpreter, the output string is enclosed in quotes and special characters are escaped with backslashes. While this might sometimes look different from the input (the enclosing quotes could change), the two strings are equivalent. The string is enclosed in double quotes if the string contains a single quote and no double quotes, otherwise it is enclosed in single quotes. The print() function produces a more readable output, by omitting the enclosing quotes and by printing escaped and special characters:


>>> '"Isn\'t," they said.'
'"Isn\'t," they said.'
>>> print('"Isn\'t," they said.')
"Isn't," they said.
>>> s = 'First line.\nSecond line.'  # \n means newline
>>> s  # without print(), \n is included in the output
'First line.\nSecond line.'
>>> print(s)  # with print(), \n produces a new line
First line.
Second line.


If you don’t want characters prefaced by \ to be interpreted as special characters, you can use raw strings by adding an r before the first quote:


>>> print('C:\some\name')  # here \n means newline!
C:\some
ame
>>> print(r'C:\some\name')  # note the r before the quote
C:\some\name


String literals can span multiple lines. One way is using triple-quotes: """...""" or '''...'''. End of lines are automatically included in the string, but it’s possible to prevent this by adding a \ at the end of the line. The following example:


>>>print("""\
Usage: thingy [OPTIONS]
     -h                        Display this usage message
     -H hostname               Hostname to connect to
""")


produces the following output (note that the initial newline is not included):


>Usage: thingy [OPTIONS]
     -h                        Display this usage message
     -H hostname               Hostname to connect to


Strings can be concatenated (glued together) with the + operator, and repeated with * :


>>> #` 3 times 'un', followed by 'ium'
>>> 3 * 'un' + 'ium'
'unununium'


Two or more string literals (i.e. the ones enclosed between quotes) next to each other are automatically concatenated.


>>> 'Py' 'thon'
'Python'


This feature is particularly useful when you want to break long strings:


>>> text = ('Put several strings within parentheses '
...         'to have them joined together.')
>>> text
'Put several strings within parentheses to have them joined together.'


This only works with two literals though, not with variables or expressions:


>>> prefix = 'Py'
>>> prefix 'thon'  # can't concatenate a variable and a string literal
  File "<stdin>", line 1
    prefix 'thon'
                ^
SyntaxError: invalid syntax
>>> ('un' * 3) 'ium'
  File "<stdin>", line 1
    ('un' * 3) 'ium'
                   ^
SyntaxError: invalid syntax


If you want to concatenate variables or a variable and a literal, use +:


>>> prefix + 'thon'
'Python'


Strings can be indexed (subscripted), with the first character having index 0. There is no separate character type; a character is simply a string of size one:


>>> word = 'Python'
>>> word[0]  # character in position 0
'P'
>>> word[5]  # character in position 5
'n'


Indices may also be negative numbers, to start counting from the right:


>>> word[-1]  # last character
'n'
>>> word[-2]  # second-last character
'o'
>>> word[-6]
'P'


Note that since -0 is the same as 0, negative indices start from -1.

In addition to indexing, slicing is also supported. While indexing is used to obtain individual characters, slicing allows you to obtain substring:


>>> word[0:2]  # characters from position 0 (included) to 2 (excluded)
'Py'
>>> word[2:5]  # characters from position 2 (included) to 5 (excluded)
'tho'


Note how the start is always included, and the end always excluded. This makes sure that `s[:i] + s[i:]` is always equal to s:


>>> word[:2] + word[2:]
'Python'
>>> word[:4] + word[4:]
'Python'


Slice indices have useful defaults; an omitted first index defaults to zero, an omitted second index defaults to the size of the string being sliced.


>>> word[:2]   # character from the beginning to position 2 (excluded)
'Py'
>>> word[4:]   # characters from position 4 (included) to the end
'on'
>>> word[-2:]  # characters from the second-last (included) to the end
'on'


One way to remember how slices work is to think of the indices as pointing between characters, with the left edge of the first character numbered 0. Then the right edge of the last character of a string of n characters has index n, for example:


     +---+---+---+---+---+---+
     | P | y | t | h | o | n |
     +---+---+---+---+---+---+
     0   1   2   3   4   5   6
    -6  -5  -4  -3  -2  -1


The first row of numbers gives the position of the indices 0…6 in the string; the second row gives the corresponding negative indices. The slice from i to j consists of all characters between the edges labeled i and j, respectively.

For non-negative indices, the length of a slice is the difference of the indices, if both are within bounds. For example, the length of `word[1:3]` is 2.

Attempting to use an index that is too large will result in an error:


>>> word[42]  # the word only has 6 characters
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
IndexError: string index out of range


However, out of range slice indexes are handled gracefully when used for slicing:


>>> word[4:42]
'on'
>>> word[42:]
''


Python strings cannot be changed — they are `immutable`. Therefore, assigning to an indexed position in the string results in an error:


>>> word[0] = 'J'
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: 'str' object does not support item assignment
>>> word[2:] = 'py'
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: 'str' object does not support item assignment


If you need a different string, you should create a new one:


>>> 'J' + word[1:]
'Jython'
>>> word[:2] + 'py'
'Pypy'


The built-in function len() returns the length of a string:


>>> s = 'supercalifragilisticexpialidocious'
>>> len(s)
34


See also:

↪ Text Sequence Type — str
 Strings are examples of sequence types, and support the common operations supported by such types.
↪ String Methods
 Strings support a large number of methods for basic transformations and searching.
↪ Formatted string literals
 String literals that have embedded expressions.
↪ Format String Syntax
 Information about string formatting with str.format().
↪ printf-style String Formatting
 The old formatting operations invoked when strings are the left operand of the % operator are described in more detail here.

### ===🗝 3.1.3. Lists

Python knows a number of compound data types, used to group together other values. The most versatile is the list, which can be written as a list of comma-separated values (items) between square brackets. Lists might contain items of different types, but usually the items all have the same type.


>>> squares = [1, 4, 9, 16, 25]
>>> squares
[1, 4, 9, 16, 25]


Like strings (and all other built-in sequence types), lists can be indexed and sliced:


>>> squares[0]  # indexing returns the item
1
>>> squares[-1]
25
>>> squares[-3:]  # slicing returns a new list
[9, 16, 25]


All slice operations return a new list containing the requested elements. This means that the following slice returns a shallow copy of the list:


>>> squares[:]
[1, 4, 9, 16, 25]


Lists also support operations like concatenation:


>>> squares + [36, 49, 64, 81, 100]
[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]


Unlike strings, which are immutable, lists are a mutable type, i.e. it is possible to change their content:


>>> cubes = [1, 8, 27, 65, 125]  # something's wrong here
>>> 4 ** 3  # the cube of 4 is 64, not 65!
64
>>> cubes[3] = 64  # replace the wrong value
>>> cubes
[1, 8, 27, 64, 125]


You can also add new items at the end of the list, by using the append() method (we will see more about methods later):


>>> cubes.append(216)  # add the cube of 6
>>> cubes.append(7 ** 3)  # and the cube of 7
>>> cubes
[1, 8, 27, 64, 125, 216, 343]


Assignment to slices is also possible, and this can even change the size of the list or clear it entirely:


>>> letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g']
>>> letters
['a', 'b', 'c', 'd', 'e', 'f', 'g']
>>> #` replace some values
>>> letters[2:5] = ['C', 'D', 'E']
>>> letters
['a', 'b', 'C', 'D', 'E', 'f', 'g']
>>> #` now remove them
>>> letters[2:5] = []
>>> letters
['a', 'b', 'f', 'g']
>>> #` clear the list by replacing all the elements with an empty list
>>> letters[:] = []
>>> letters
[]


The built-in function len() also applies to lists:


>>> letters = ['a', 'b', 'c', 'd']
>>> len(letters)
4


It is possible to nest lists (create lists containing other lists), for example:


>>> a = ['a', 'b', 'c']
>>> n = [1, 2, 3]
>>> x = [a, n]
>>> x
[['a', 'b', 'c'], [1, 2, 3]]
>>> x[0]
['a', 'b', 'c']
>>> x[0][1]
'b'



### ===🗝 3.2. First Steps Towards Programming

Of course, we can use Python for more complicated tasks than adding two and two together. For instance, we can write an initial sub-sequence of the Fibonacci series as follows:


>>> # Fibonacci series:
... # the sum of two elements defines the next
... a, b = 0, 1
>>> while a < 10:
...     print(a)
...     a, b = b, a+b
...
0
1
1
2
3
5
8


This example introduces several new features.

• The first line contains a `multiple` assignment: the variables a and b simultaneously get the new values 0 and 1. On the last line this is used again, demonstrating that the expressions on the right-hand side are all evaluated first before any of the assignments take place. The right-hand side expressions are evaluated from the left to the right.


• The `while loop` executes as long as the condition (here: a < 10) remains true. In Python, like in C, any non-zero integer value is true; zero is false. The condition may also be a string or list value, in fact any sequence; anything with a non-zero length is true, empty sequences are false. The test used in the example is a simple comparison. The standard comparison operators are written the same as in C: 

↪ `<`  (less than), 
↪ `>`  (greater than), 
↪ `==` (equal to), 
↪ `<=` (less than or equal to), 
↪ `>=` (greater than or equal to) and 
↪ `!=` (not equal to).

• The body of the loop is indented: indentation is Python’s way of grouping statements. At the interactive prompt, you have to type a tab or space(s) for each indented line. In practice you will prepare more complicated input for Python with a text editor; all decent text editors have an auto-indent facility. When a compound statement is entered interactively, it must be followed by a blank line to indicate completion (since the parser cannot guess when you have typed the last line). Note that each line within a basic block must be indented by the same amount.


• The `print()` function writes the value of the argument(s) it is given. It differs from just writing the expression you want to write (as we did earlier in the calculator examples) in the way it handles multiple arguments, floating point quantities, and strings. Strings are printed without quotes, and a space is inserted between items, so you can format things nicely, like this:


>>> i = 256*256
>>> print('The value of i is', i)
The value of i is 65536


The keyword argument end can be used to avoid the newline after the output, or end the output with a different string:


>>> a, b = 0, 1
>>> while a < 1000:
...     print(a, end=',')
...     a, b = b, a+b
...
0,1,1,2,3,5,8,13,21,34,55,89,144,233,377,610,987,


Footnotes

[1] Since ** has higher precedence than -, -3 ** 2 will be interpreted as -(3 ** 2) and thus result in -9. To avoid this and get 9, you can use (-3) ** 2. 

[2] Unlike other languages, special characters such as \n have the same meaning with both single ('...') and double ("...") quotes. The only difference between the two is that within single quotes you don’t need to escape " (but you have to escape \') and vice versa. 



## ==⚡ 4. More Control Flow Tools


- Python 3.10.2 Documentation » The Python Tutorial » 4. More Control Flow Tools

Besides the while statement just introduced, Python uses the usual flow control statements known from other languages, with some twists.


### ===🗝 4.1. if Statements

Perhaps the most well-known statement type is the `if` statement. For example:


>>> x = int(input("Please enter an integer: "))
Please enter an integer: 42
>>> if x < 0:
...     x = 0
...     print('Negative changed to zero')
... elif x == 0:
...     print('Zero')
... elif x == 1:
...     print('Single')
... else:
...     print('More')
...
More


There can be zero or more `elif` parts, and the else part is optional. The keyword ‘`elif`’ is short for ‘`else if`’, and is useful to avoid excessive indentation. An `if … elif … elif …` sequence is a substitute for the switch or case statements found in other languages.

If you’re comparing the same value to several constants, or checking for specific types or attributes, you may also find the match statement useful. For more details see match Statements.


### ===🗝 4.2. for Statements

The `for` statement in Python differs a bit from what you may be used to in C or Pascal. Rather than always iterating over an arithmetic progression of numbers (like in Pascal), or giving the user the ability to define both the iteration step and halting condition (as C), Python’s `for` statement iterates over the items of any sequence (a list or a string), in the order that they appear in the sequence. For example (no pun intended):


>>> #` Measure some strings:
... words = ['cat', 'window', 'defenestrate']
>>> for w in words:
...     print(w, len(w))
...
cat 3
window 6
defenestrate 12


Code that modifies a collection while iterating over that same collection can be tricky to get right. Instead, it is usually more straight-forward to loop over a copy of the collection or to create a new collection:


```py
# Create a sample collection
users = {'Hans': 'active', 'Éléonore': 'inactive', '景太郎': 'active'}

# Strategy:  Iterate over a copy
for user, status in users.copy().items():
    if status == 'inactive':
        del users[user]

# Strategy:  Create a new collection
active_users = {}
for user, status in users.items():
    if status == 'active':
        active_users[user] = status
```



### ===🗝 4.3. The range() Function

If you do need to iterate over a sequence of numbers, the built-in function `range()` comes in handy. It generates arithmetic progressions:


>>> for i in range(5):
...     print(i)
...
0
1
2
3
4


The given end point is never part of the generated sequence; `range(10)` generates 10 values, the legal indices for items of a sequence of length 10. It is possible to let the range start at another number, or to specify a different increment (even negative; sometimes this is called the ‘`step`’):


>>> list(range(5, 10))
[5, 6, 7, 8, 9]

>>> list(range(0, 10, 3))
[0, 3, 6, 9]

>>> list(range(-10, -100, -30))
[-10, -40, -70]


To iterate over the indices of a sequence, you can combine `range()` and `len()` as follows:


>>> a = ['Mary', 'had', 'a', 'little', 'lamb']
>>> for i in range(len(a)):
...     print(i, a[i])
...
0 Mary
1 had
2 a
3 little
4 lamb


In most such cases, however, it is convenient to use the `enumerate()` function, see Looping Techniques.

A strange thing happens if you just print a range:


>>> range(10)
range(0, 10)


In many ways the object returned by `range()` behaves as if it is a list, but in fact it isn’t. It is an object which returns the successive items of the desired sequence when you iterate over it, but it doesn’t really make the list, thus saving space.

We say such an object is iterable, that is, suitable as a target for functions and constructs that expect something from which they can obtain successive items until the supply is exhausted. We have seen that the for statement is such a construct, while an example of a function that takes an iterable is `sum()`:


>>> sum(range(4))  # 0 + 1 + 2 + 3
6


Later we will see more functions that return iterables and take iterables as arguments. In chapter Data Structures, we will discuss in more detail about `list()`.


### ===🗝 4.4. break and continue Statements, and else Clauses on Loops

The `break` statement, like in C, breaks out of the innermost enclosing `for` or `while` loop.

Loop statements may have an else clause; it is executed when the loop terminates through exhaustion of the iterable (with `for`) or when the condition becomes false (with `while`), but not when the loop is terminated by a `break` statement. This is exemplified by the following loop, which searches for prime numbers:


>>> for n in range(2, 10):
...     for x in range(2, n):
...         if n % x == 0:
...             print(n, 'equals', x, '*', n//x)
...             break
...     else:
...         # loop fell through without finding a factor
...         print(n, 'is a prime number')
...
2 is a prime number
3 is a prime number
4 equals 2 * 2
5 is a prime number
6 equals 2 * 3
7 is a prime number
8 equals 2 * 4
9 equals 3 * 3


(Yes, this is the correct code. Look closely: the `else` clause belongs to the `for` loop, not the `if` statement.)

When used with a loop, the `else` clause has more in common with the `else` clause of a try statement than it does with that of if statements: a try statement’s `else` clause runs when no exception occurs, and a loop’s `else` clause runs when no break occurs. For more on the try statement and exceptions, see Handling Exceptions.

The continue statement, also borrowed from C, continues with the next iteration of the loop:


>>> for num in range(2, 10):
...     if num % 2 == 0:
...         print("Found an even number", num)
...         continue
...     print("Found an odd number", num)
...
Found an even number 2
Found an odd number 3
Found an even number 4
Found an odd number 5
Found an even number 6
Found an odd number 7
Found an even number 8
Found an odd number 9



### ===🗝 4.5. pass Statements

The `pass` statement does nothing. It can be used when a statement is required syntactically but the program requires no action. For example:


>>> while True:
...     pass  # Busy-wait for keyboard interrupt (Ctrl+C)
...


This is commonly used for creating minimal classes:


>>> class MyEmptyClass:
...     pass
...


Another place `pass` can be used is as a place-holder for a function or conditional body when you are working on new code, allowing you to keep thinking at a more abstract level. The `pass` is silently ignored:


>>> def initlog(*args):
...     pass   # Remember to implement this!
...



### ===🗝 4.6. match Statements

A `match` statement takes an expression and compares its value to successive patterns given as one or more case blocks. This is superficially similar to a switch statement in C, Java or JavaScript (and many other languages), but it can also extract components (sequence elements or object attributes) from the value into variables.

The simplest form compares a subject value against one or more literals:


```py
def http_error(status):
    match status:
        case 400:
            return "Bad request"
        case 404:
            return "Not found"
        case 418:
            return "I'm a teapot"
        case _:
            return "Something's wrong with the internet"
```


Note the last block: the “variable name” `_` acts as a wildcard and never fails to `match`. If no case matches, none of the branches is executed.

You can combine several literals in a single pattern using | (“or”):


```py
case 401 | 403 | 404:
    return "Not allowed"
```


Patterns can look like unpacking assignments, and can be used to bind variables:


```py
# point is an (x, y) tuple
match point:
    case (0, 0):
        print("Origin")
    case (0, y):
        print(f"Y={y}")
    case (x, 0):
        print(f"X={x}")
    case (x, y):
        print(f"X={x}, Y={y}")
    case _:
        raise ValueError("Not a point")
```


Study that one carefully! The first pattern has two literals, and can be thought of as an extension of the literal pattern shown above. But the next two patterns combine a literal and a variable, and the variable binds a value from the subject (point). The fourth pattern captures two values, which makes it conceptually similar to the unpacking assignment `(x, y) = point`.

If you are using classes to structure your data you can use the class name followed by an argument list resembling a constructor, but with the ability to capture attributes into variables:


```py
class Point:
    x: int
    y: int

def where_is(point):
    match point:
        case Point(x=0, y=0):
            print("Origin")
        case Point(x=0, y=y):
            print(f"Y={y}")
        case Point(x=x, y=0):
            print(f"X={x}")
        case Point():
            print("Somewhere else")
        case _:
            print("Not a point")
```


You can use positional parameters with some builtin classes that provide an ordering for their attributes (e.g. `dataclasses`). You can also define a specific position for attributes in patterns by setting the __match_args__ special attribute in your classes. If it’s set to (“x”, “y”), the following patterns are all equivalent (and all bind the y attribute to the var variable):


```py
Point(1, var)
Point(1, y=var)
Point(x=1, y=var)
Point(y=var, x=1)
```


A recommended way to read patterns is to look at them as an extended form of what you would put on the left of an assignment, to understand which variables would be set to what. Only the standalone names (like var above) are assigned to by a match statement. Dotted names (like foo.bar), attribute names (the x= and y= above) or class names (recognized by the “(…)” next to them like Point above) are never assigned to.

Patterns can be arbitrarily nested. For example, if we have a short list of points, we could match it like this:


```py
match points:
    case []:
        print("No points")
    case [Point(0, 0)]:
        print("The origin")
    case [Point(x, y)]:
        print(f"Single point {x}, {y}")
    case [Point(0, y1), Point(0, y2)]:
        print(f"Two on the Y axis at {y1}, {y2}")
    case _:
        print("Something else")
```


We can add an `if` clause to a pattern, known as a “guard”. If the guard is false, match goes on to try the next case block. Note that value capture happens before the guard is evaluated:


```py
match point:
    case Point(x, y) if x == y:
        print(f"Y=X at {x}")
    case Point(x, y):
        print(f"Not on the diagonal")
```


Several other key features of this statement:

↪• Like unpacking assignments, `tuple` and `list` patterns have exactly the same meaning and actually match arbitrary sequences. An important exception is that they don’t match iterators or strings.


↪• Sequence patterns support extended unpacking: `[x, y, *rest]` and `(x, y, *rest)` work similar to unpacking assignments. The name after `*` may also be `_`, so `(x, y, *_)` matches a sequence of at least two items without binding the remaining items.


↪• Mapping patterns: `{"bandwidth": b, "latency": l}` captures the "bandwidth" and "latency" values from a dictionary. Unlike sequence patterns, extra keys are ignored. An unpacking like `**rest` is also supported. (But `**_` would be redundant, so it is not allowed.)


↪• Subpatterns may be captured using the as keyword:


    case (Point(x1, y1), Point(x2, y2) as p2): ...


will capture the second element of the input as `p2` (as long as the input is a sequence of two points)


↪• Most literals are compared by equality, however the singletons `True`, `False` and `None` are compared by identity.


↪• Patterns may use named constants. These must be dotted names to prevent them from being interpreted as capture variable:


```py
from enum import Enum
class Color(Enum):
    RED = 'red'
    GREEN = 'green'
    BLUE = 'blue'

color = Color(input("Enter your choice of 'red', 'blue' or 'green': "))

match color:
    case Color.RED:
        print("I see red!")
    case Color.GREEN:
        print("Grass is green")
    case Color.BLUE:
        print("I'm feeling the blues :(")
```



For a more detailed explanation and additional examples, you can look into PEP 636 which is written in a tutorial format.

- PEP 636 – Structural Pattern Matching: Tutorial https://www.python.org/dev/peps/pep-0636


### ===🗝 4.7. Defining Functions

We can create a function that writes the Fibonacci series to an arbitrary boundary:


>>> def fib(n):    # write Fibonacci series up to n
...     """Print a Fibonacci series up to n."""
...     a, b = 0, 1
...     while a < n:
...         print(a, end=' ')
...         a, b = b, a+b
...     print()
...
>>> #` `Now call the function we just defined:
... fib(2000)
0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597


The keyword `def` introduces a function definition. It must be followed by the function name and the parenthesized list of formal parameters. The statements that form the body of the function start at the next line, and must be indented.

The first statement of the function body can optionally be a string literal; this string literal is the `function’s documentation string`, or `docstring`. (More about docstrings can be found in the section Documentation Strings.) There are tools which use docstrings to automatically produce online or printed documentation, or to let the user interactively browse through code; it’s good practice to include docstrings in code that you write, so make a habit of it.

The execution of a function introduces a new symbol table used for the `local variables` of the function. More precisely, all variable assignments in a function store the value in the `local symbol table`; whereas variable references first look in the local symbol table, then in the local symbol tables of enclosing functions, then in the `global symbol table`, and finally in the table of built-in names. Thus, `global variables` and variables of enclosing functions cannot be directly assigned a value within a function (unless, for global variables, named in a `global` statement, or, for variables of enclosing functions, named in a `nonlocal` statement), although they may be referenced.

The `actual parameters` (arguments) to a function call are introduced in the `local symbol table` of the called function when it is called; thus, arguments are passed using call by value (where the value is always an object reference, not the value of the object). [1] When a function calls another function, or calls itself recursively, a new local symbol table is created for that call.

A function definition associates the function name with the function object in the current symbol table. The interpreter recognizes the object pointed to by that name as a user-defined function. Other names can also point to that same function object and can also be used to access the function:


>>> fib
<function fib at 10042ed0>
>>> f = fib
>>> f(100)
0 1 1 2 3 5 8 13 21 34 55 89


Coming from other languages, you might object that `fib` is not a function but a procedure since it doesn’t return a value. In fact, even functions without a return statement do return a value, albeit a rather boring one. This value is called None (it’s a built-in name). Writing the value None is normally suppressed by the interpreter if it would be the only value written. You can see it if you really want to using `print()`:


>>> fib(0)
>>> print(fib(0))
None


It is simple to write a function that returns a list of the numbers of the Fibonacci series, instead of printing it:


>>> def fib2(n):  # return Fibonacci series up to n
...     """Return a list containing the Fibonacci series up to n."""
...     result = []
...     a, b = 0, 1
...     while a < n:
...         result.append(a)    # see below
...         a, b = b, a+b
...     return result
...
>>> f100 = fib2(100)    # call it
>>> f100                # write the result
[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89]


This example, as usual, demonstrates some new Python features:
↪• The `return` statement returns with a value from a function. `return` without an expression argument returns `None`. Falling off the end of a function also returns `None`.
↪• The statement `result.append(a)` calls a method of the `list` object `result`. A method is a function that ‘belongs’ to an object and is named `obj.methodname`, where obj is some object (this may be an expression), and methodname is the name of a method that is defined by the object’s type. Different types define different methods. Methods of different types may have the same name without causing ambiguity. (It is possible to define your own object types and methods, using classes, see Classes) The method `append()` shown in the example is defined for `list` objects; it adds a new element at the end of the `list`. In this example it is equivalent to `result = result + [a]`, but more efficient.


### ===🗝 4.8. More on Defining Functions

It is also possible to define functions with a variable number of arguments. There are three forms, which can be combined.


### ===🗝 4.8.1. Default Argument Values

The most useful form is to specify a `default value` for one or more arguments. This creates a function that can be called with fewer arguments than it is defined to allow. For example:


```py
def ask_ok(prompt, retries=4, reminder='Please try again!'):
    while True:
        ok = input(prompt)
        if ok in ('y', 'ye', 'yes'):
            return True
        if ok in ('n', 'no', 'nop', 'nope'):
            return False
        retries = retries - 1
        if retries < 0:
            raise ValueError('invalid user response')
        print(reminder)
```


This function can be called in several ways:
↪• giving only the mandatory argument: `ask_ok('Do you really want to quit?')`
↪• giving one of the optional arguments: `ask_ok('OK to overwrite the file?', 2)`
↪• or even giving all arguments: `ask_ok('OK to overwrite the file?', 2, 'Come on, only yes or no!')`

This example also introduces the `in` keyword. This tests whether or not a sequence contains a certain value.

The default values are evaluated at the point of function definition in the defining scope, so that


```py
i = 5

def f(arg=i):
    print(arg)

i = 6
f()
```


will print 5.

Important warning: **The default value is evaluated only once**. This makes a difference when the default is a mutable object such as a *list*, *dictionary*, or *instances* of most classes. For example, the following function accumulates the arguments passed to it on subsequent calls:


```py
def f(a, L=[]):
    L.append(a)
    return L

print(f(1))
print(f(2))
print(f(3))
```


This will print


    [1]
    [1, 2]
    [1, 2, 3]


If you don’t want the default to be shared between subsequent calls, you can write the function like this instead:


```py
def f(a, L=None):
    if L is None:
        L = []
    L.append(a)
    return L
```



### ===🗝 4.8.2. Keyword Arguments

Functions can also be called using keyword arguments of the form `kwarg=value`. For instance, the following function:


```py
def parrot(voltage, state='a stiff', action='voom', type='Norwegian Blue'):
    print("-- This parrot wouldn't", action, end=' ')
    print("if you put", voltage, "volts through it.")
    print("-- Lovely plumage, the", type)
    print("-- It's", state, "!")
```


accepts one required argument (voltage) and three optional arguments (state, action, and type). This function can be called in any of the following ways:


```py
parrot(1000)                                          # 1 positional argument
parrot(voltage=1000)                                  # 1 keyword argument
parrot(voltage=1000000, action='VOOOOOM')             # 2 keyword arguments
parrot(action='VOOOOOM', voltage=1000000)             # 2 keyword arguments
parrot('a million', 'bereft of life', 'jump')         # 3 positional arguments
parrot('a thousand', state='pushing up the daisies')  # 1 positional, 1 keyword
```


but all the following calls would be invalid:


```py
parrot()                     # required argument missing
parrot(voltage=5.0, 'dead')  # non-keyword argument after a keyword argument
parrot(110, voltage=220)     # duplicate value for the same argument
parrot(actor='John Cleese')  # unknown keyword argument
```


In a function call, **keyword arguments** must follow **positional arguments**. All the keyword arguments passed must match one of the arguments accepted by the function (e.g. `actor` is not a valid argument for the parrot function), and their order is not important. This also includes non-optional arguments (e.g. `parrot(voltage=1000)` is valid too). No argument may receive a value more than once. Here’s an example that fails due to this restriction:


>>> def function(a):
...     pass
...
>>> function(0, a=0)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: function() got multiple values for argument 'a'


When a final formal parameter of the form `**name` is present, it receives a **dictionary** (see Mapping Types — `dict`) containing all keyword arguments except for those corresponding to a formal parameter. This may be combined with a formal parameter of the form `*name` (described in the next subsection) which receives a `tuple` containing the positional arguments beyond the formal parameter list. (`*name` must occur before `**name`.) For example, if we define a function like this:


```py
def cheeseshop(kind, *arguments, **keywords):
    print("-- Do you have any", kind, "?")
    print("-- I'm sorry, we're all out of", kind)
    for arg in arguments:
        print(arg)
    print("-" * 40)
    for kw in keywords:
        print(kw, ":", keywords[kw])
```


It could be called like this:


```py
cheeseshop("Limburger", "It's very runny, sir.",
           "It's really very, VERY runny, sir.",
           shopkeeper="Michael Palin",
           client="John Cleese",
           sketch="Cheese Shop Sketch")
```


and of course it would print:

```sh
-- Do you have any Limburger ?
-- I'm sorry, we're all out of Limburger
It's very runny, sir.
It's really very, VERY runny, sir.
----------------------------------------
shopkeeper : Michael Palin
client : John Cleese
sketch : Cheese Shop Sketch
```


Note that the order in which the keyword arguments are printed is guaranteed to match the order in which they were provided in the function call.


### ===🗝 4.8.3. Special parameters

By default, arguments may be passed to a Python function either by position or explicitly by keyword. For readability and performance, it makes sense to restrict the way arguments can be passed so that a developer need only look at the function definition to determine if items are passed by position, by position or keyword, or by keyword.

A function definition may look like:

```sh
def f(pos1, pos2, /, pos_or_kwd, *, kwd1, kwd2):
      -----------    ----------     ----------
        |             |                  |
        |        Positional or keyword   |
        |                                - Keyword only
         -- Positional only
```


where / and * are optional. If used, these symbols indicate the kind of parameter by how the arguments may be passed to the function: positional-only, positional-or-keyword, and keyword-only. Keyword parameters are also referred to as named parameters.


### ===🗝 4.8.3.1. Positional-or-Keyword Arguments

If / and * are not present in the function definition, arguments may be passed to a function by position or by keyword.


### ===🗝 4.8.3.2. Positional-Only Parameters

Looking at this in a bit more detail, it is possible to mark certain parameters as *positional-only*. If *positional-only*, the parameters’ order matters, and the parameters cannot be passed by keyword. Positional-only parameters are placed before a / (forward-slash). The / is used to logically separate the positional-only parameters from the rest of the parameters. If there is no / in the function definition, there are no positional-only parameters.

Parameters following the / may be positional-or-keyword or keyword-only.


### ===🗝 4.8.3.3. Keyword-Only Arguments

To mark parameters as keyword-only, indicating the parameters must be passed by keyword argument, place an * in the arguments list just before the first keyword-only parameter.


### ===🗝 4.8.3.4. Function Examples

Consider the following example function definitions paying close attention to the markers / and * :


>>> def standard_arg(arg):
...     print(arg)
...
>>> def pos_only_arg(arg, /):
...     print(arg)
...
>>> def kwd_only_arg(*, arg):
...     print(arg)
...
>>> def combined_example(pos_only, /, standard, * , kwd_only):
...     print(pos_only, standard, kwd_only)


The first function definition, `standard_arg`, the most familiar form, places no restrictions on the calling convention and arguments may be passed by position or keyword:


>>> standard_arg(2)
2

>>> standard_arg(arg=2)
2


The second function `pos_only_arg` is restricted to only use positional parameters as there is a / in the function definition:


>>> pos_only_arg(1)
1

>>> pos_only_arg(arg=1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: pos_only_arg() got some positional-only arguments passed as keyword arguments: 'arg'


The third function `kwd_only_args` only allows keyword arguments as indicated by a * in the function definition:


>>> kwd_only_arg(3)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: kwd_only_arg() takes 0 positional arguments but 1 was given

>>> kwd_only_arg(arg=3)
3


And the last uses all three calling conventions in the same function definition:


>>> combined_example(1, 2, 3)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: combined_example() takes 2 positional arguments but 3 were given

>>> combined_example(1, 2, kwd_only=3)
1 2 3

>>> combined_example(1, standard=2, kwd_only=3)
1 2 3

>>> combined_example(pos_only=1, standard=2, kwd_only=3)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: combined_example() got some positional-only arguments passed as keyword arguments: 'pos_only'


Finally, consider this function definition which has a potential collision between the positional argument name and `**kwds` which has name as a key:


```py
def foo(name, **kwds):
    return 'name' in kwds
```


There is no possible call that will make it return True as the keyword 'name' will always bind to the first parameter. For example:


>>> foo(1, **{'name': 2})
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: foo() got multiple values for argument 'name'
>>>


But using / (positional only arguments), it is possible since it allows name as a positional argument and 'name' as a key in the keyword arguments:


```py
def foo(name, /, **kwds):
    return 'name' in kwds
>>> foo(1, **{'name': 2})
True
```


In other words, the names of positional-only parameters can be used in `**kwds` without ambiguity.


### ===🗝 4.8.3.5. Recap

The use case will determine which parameters to use in the function definition:


    def f(pos1, pos2, /, pos_or_kwd, *, kwd1, kwd2):


As guidance:
↪• Use `positional-only` if you want the name of the parameters to not be available to the user. This is useful when parameter names have no real meaning, if you want to enforce the order of the arguments when the function is called or if you need to take some positional parameters and arbitrary keywords.
↪• Use `keyword-only` when names have meaning and the function definition is more understandable by being explicit with names or you want to prevent users relying on the position of the argument being passed.
↪• For an API, use `positional-only` to prevent breaking API changes if the parameter’s name is modified in the future.


### ===🗝 4.8.4. Arbitrary Argument Lists

Finally, the least frequently used option is to specify that a function can be called with an arbitrary number of arguments. These arguments will be wrapped up in a `tuple` (see Tuples and Sequences). Before the variable number of arguments, zero or more normal arguments may occur.


```py
def write_multiple_items(file, separator, *args):
    file.write(separator.join(args))
```


Normally, these variadic arguments will be last in the list of formal parameters, because they scoop up all remaining input arguments that are passed to the function. Any formal parameters which occur after the `*args` parameter are ‘keyword-only’ arguments, meaning that they can only be used as keywords rather than positional arguments.


>>> def concat(*args, sep="/"):
...     return sep.join(args)
...
>>> concat("earth", "mars", "venus")
'earth/mars/venus'
>>> concat("earth", "mars", "venus", sep=".")
'earth.mars.venus'



### ===🗝 4.8.5. Unpacking Argument Lists

The reverse situation occurs when the arguments are already in a list or tuple but need to be unpacked for a function call requiring separate positional arguments. For instance, the built-in `range()` function expects separate start and stop arguments. If they are not available separately, write the function call with the `*-operator` to unpack the arguments out of a `list` or `tuple`:


>>> list(range(3, 6))            # normal call with separate arguments
[3, 4, 5]
>>> args = [3, 6]
>>> list(range(* args))            # call with arguments unpacked from a list
[3, 4, 5]


In the same fashion, dictionaries can deliver keyword arguments with the `**-operator`:


>>> def parrot(voltage, state='a stiff', action='voom'):
...     print("-- This parrot wouldn't", action, end=' ')
...     print("if you put", voltage, "volts through it.", end=' ')
...     print("E's", state, "!")
...
>>> d = {"voltage": "four million", "state": "bleedin' demised", "action": "VOOM"}
>>> parrot(** d)
-- This parrot wouldn't VOOM if you put four million volts through it. E's bleedin' demised !



### ===🗝 4.8.6. Lambda Expressions

Small anonymous functions can be created with the `lambda` keyword. This function returns the sum of its two arguments:` lambda a, b: a+b`. Lambda functions can be used wherever function objects are required. They are syntactically restricted to a single expression. Semantically, they are just syntactic sugar for a normal function definition. Like nested function definitions, `lambda` functions can reference variables from the containing scope:


>>> def make_incrementor(n):
...     return lambda x: x + n
...
>>> f = make_incrementor(42)
>>> f(0)
42
>>> f(1)
43


The above example uses a `lambda` expression to return a function. Another use is to pass a small function as an argument:


>>> pairs = [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')]
>>> pairs.sort(key=lambda pair: pair[1])
>>> pairs
[(4, 'four'), (1, 'one'), (3, 'three'), (2, 'two')]



### ===🗝 4.8.7. Documentation Strings

Here are some conventions about the content and formatting of documentation strings.

The first line should always be a short, concise summary of the object’s purpose. For brevity, it should not explicitly state the object’s name or type, since these are available by other means (except if the name happens to be a verb describing a function’s operation). This line should begin with a capital letter and end with a period.

If there are more lines in the documentation string, the second line should be blank, visually separating the summary from the rest of the description. The following lines should be one or more paragraphs describing the object’s calling conventions, its side effects, etc.

The Python parser does not strip indentation from multi-line string literals in Python, so tools that process documentation have to strip indentation if desired. This is done using the following convention. The `first non-blank` line after the first line of the string determines the amount of indentation for the entire documentation string. (We can’t use the first line since it is generally adjacent to the string’s opening quotes so its indentation is not apparent in the string literal.) Whitespace “equivalent” to this indentation is then stripped from the start of all lines of the string. Lines that are indented less should not occur, but if they occur all their leading whitespace should be `stripped`. Equivalence of whitespace should be tested after expansion of tabs (to 8 spaces, normally).

Here is an example of a multi-line docstring:


>>> def my_function():
...     """Do nothing, but document it.
...
...     No, really, it doesn't do anything.
...     """
...     pass
...
>>> print(my_function.__doc__)
Do nothing, but document it.

    No, really, it doesn't do anything.



### ===🗝 4.8.8. Function Annotations

Function annotations are completely optional metadata information about the types used by user-defined functions (see PEP 3107 and PEP 484 for more information).

Annotations are stored in the __annotations__ attribute of the function as a dictionary and have no effect on any other part of the function. Parameter annotations are defined by a colon after the parameter name, followed by an expression evaluating to the value of the annotation. Return annotations are defined by a literal ->, followed by an expression, between the parameter list and the colon denoting the end of the def statement. The following example has a required argument, an optional argument, and the return value annotated:


>>> def f(ham: str, eggs: str = 'eggs') -> str:
...     print("Annotations:", f.__annotations__)
...     print("Arguments:", ham, eggs)
...     return ham + ' and ' + eggs
...
>>> f('spam')
Annotations: {'ham': <class 'str'>, 'return': <class 'str'>, 'eggs': <class 'str'>}
Arguments: spam eggs
'spam and eggs'



### ===🗝 4.9. Intermezzo: Coding Style

Now that you are about to write longer, more complex pieces of Python, it is a good time to talk about coding style. Most languages can be written (or more concise, formatted) in different styles; some are more readable than others. Making it easy for others to read your code is always a good idea, and adopting a nice coding style helps tremendously for that.

For Python, PEP 8 has emerged as the style guide that most projects adhere to; it promotes a very readable and eye-pleasing coding style. Every Python developer should read it at some point; here are the most important points extracted for you:

↪• Use 4-space indentation, and no tabs.

4 spaces are a good compromise between small indentation (allows greater nesting depth) and large indentation (easier to read). Tabs introduce confusion, and are best left out.


↪• Wrap lines so that they don’t exceed 79 characters.

This helps users with small displays and makes it possible to have several code files side-by-side on larger displays.


↪• Use blank lines to separate functions and classes, and larger blocks of code inside functions.


↪• When possible, put comments on a line of their own.


↪• Use docstrings.


↪• Use spaces around operators and after commas, but not directly inside bracketing constructs: `a = f(1, 2) + g(3, 4)`.


↪• Name your classes and functions consistently; the convention is to use `UpperCamelCase` for classes and `lowercase_with_underscores` for functions and methods. Always use self as the name for the first method argument (see A First Look at Classes for more on classes and methods).


↪• Don’t use fancy encodings if your code is meant to be used in international environments. Python’s default, UTF-8, or even plain ASCII work best in any case.


↪• Likewise, don’t use non-ASCII characters in identifiers if there is only the slightest chance people speaking a different language will read or maintain the code.


Footnotes

[1] Actually, call by object reference would be a better description, since if a mutable object is passed, the caller will see any changes the callee makes to it (items inserted into a list). 



## ==⚡ 5. Data Structures

This chapter describes some things you’ve learned about already in more detail, and adds some new things as well.

### ===🗝 5.1. More on Lists

The list data type has some more methods. Here are all of the methods of list objects:

↪ `list.append(x)` Add an item to the end of the list. Equivalent to a[len(a):] = [x].

↪ `list.extend(iterable)` Extend the list by appending all the items from the iterable. Equivalent to a[len(a):] = iterable.

↪ `list.insert(i, x)` Insert an item at a given position. The first argument is the index of the element before which to insert, so a.insert(0, x) inserts at the front of the list, and a.insert(len(a), x) is equivalent to a.append(x).

↪ `list.remove(x)` Remove the first item from the list whose value is equal to x. It raises a ValueError if there is no such item.

↪ `list.pop([i])` Remove the item at the given position in the list, and return it. If no index is specified, a.pop() removes and returns the last item in the list. (The square brackets around the i in the method signature denote that the parameter is optional, not that you should type square brackets at that position. You will see this notation frequently in the Python Library Reference.)

↪ `list.clear()` Remove all items from the list. Equivalent to del a[:].

↪ `list.index(x[, start[, end]])` Return zero-based index in the list of the first item whose value is equal to x. Raises a ValueError if there is no such item.

The optional arguments start and end are interpreted as in the slice notation and are used to limit the search to a particular subsequence of the list. The returned index is computed relative to the beginning of the full sequence rather than the start argument.

↪ `list.count(x)` Return the number of times x appears in the list.

↪ `list.sort(*, key=None, reverse=False)` Sort the items of the list in place (the arguments can be used for sort customization, see sorted() for their explanation).

↪ `list.reverse()` Reverse the elements of the list in place.

↪ `list.copy()` Return a shallow copy of the list. Equivalent to a[:].

An example that uses most of the list methods:


>>> fruits = ['orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana']
>>> fruits.count('apple')
2
>>> fruits.count('tangerine')
0
>>> fruits.index('banana')
3
>>> fruits.index('banana', 4)  # Find next banana starting a position 4
6
>>> fruits.reverse()
>>> fruits
['banana', 'apple', 'kiwi', 'banana', 'pear', 'apple', 'orange']
>>> fruits.append('grape')
>>> fruits
['banana', 'apple', 'kiwi', 'banana', 'pear', 'apple', 'orange', 'grape']
>>> fruits.sort()
>>> fruits
['apple', 'apple', 'banana', 'banana', 'grape', 'kiwi', 'orange', 'pear']
>>> fruits.pop()
'pear'


You might have noticed that methods like insert, remove or sort that only modify the list have no return value printed – they return the default None. [1] This is a design principle for all mutable data structures in Python.

Another thing you might notice is that not all data can be sorted or compared. For instance, [None, 'hello', 10] doesn’t sort because integers can’t be compared to strings and None can’t be compared to other types. Also, there are some types that don’t have a defined ordering relation. For example, 3+4j < 5+7j isn’t a valid comparison.


#### 5.1.1. Using Lists as Stacks

The list methods make it very easy to use a list as a stack, where the last element added is the first element retrieved (“last-in, first-out”). To add an item to the top of the stack, use append(). To retrieve an item from the top of the stack, use pop() without an explicit index. For example:


>>> stack = [3, 4, 5]
>>> stack.append(6)
>>> stack.append(7)
>>> stack
[3, 4, 5, 6, 7]
>>> stack.pop()
7
>>> stack
[3, 4, 5, 6]
>>> stack.pop()
6
>>> stack.pop()
5
>>> stack
[3, 4]


#### 5.1.2. Using Lists as Queues

It is also possible to use a list as a queue, where the first element added is the first element retrieved (“first-in, first-out”); however, lists are not efficient for this purpose. While appends and pops from the end of list are fast, doing inserts or pops from the beginning of a list is slow (because all of the other elements have to be shifted by one).

To implement a queue, use collections.deque which was designed to have fast appends and pops from both ends. For example:


>>> from collections import deque
>>> queue = deque(["Eric", "John", "Michael"])
>>> queue.append("Terry")           # Terry arrives
>>> queue.append("Graham")          # Graham arrives
>>> queue.popleft()                 # The first to arrive now leaves
'Eric'
>>> queue.popleft()                 # The second to arrive now leaves
'John'
>>> queue                           # Remaining queue in order of arrival
deque(['Michael', 'Terry', 'Graham'])



#### 5.1.3. List Comprehensions

List comprehensions provide a concise way to create lists. Common applications are to make new lists where each element is the result of some operations applied to each member of another sequence or iterable, or to create a subsequence of those elements that satisfy a certain condition.

For example, assume we want to create a list of squares, like:


>>> squares = []
>>> for x in range(10):
...     squares.append(x**2)
...
>>> squares
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]


Note that this creates (or overwrites) a variable named x that still exists after the loop completes. We can calculate the list of squares without any side effects using:


    squares = list(map(lambda x: x**2, range(10)))


or, equivalently:


    squares = [x**2 for x in range(10)]


which is more concise and readable.

A list comprehension consists of brackets containing an expression followed by a for clause, then zero or more for or if clauses. The result will be a new list resulting from evaluating the expression in the context of the for and if clauses which follow it. For example, this listcomp combines the elements of two lists if they are not equal:


>>> [(x, y) for x in [1,2,3] for y in [3,1,4] if x != y]
[(1, 3), (1, 4), (2, 3), (2, 1), (2, 4), (3, 1), (3, 4)]


and it’s equivalent to:


>>> combs = []
>>> for x in [1,2,3]:
...     for y in [3,1,4]:
...         if x != y:
...             combs.append((x, y))
...
>>> combs
[(1, 3), (1, 4), (2, 3), (2, 1), (2, 4), (3, 1), (3, 4)]


Note how the order of the for and if statements is the same in both these snippets.

If the expression is a tuple (e.g. the (x, y) in the previous example), it must be parenthesized.


>>> vec = [-4, -2, 0, 2, 4]
>>> #` create a new list with the values doubled
>>> [x*2 for x in vec]
[-8, -4, 0, 4, 8]
>>> #` filter the list to exclude negative numbers
>>> [x for x in vec if x >= 0]
[0, 2, 4]
>>> #` apply a function to all the elements
>>> [abs(x) for x in vec]
[4, 2, 0, 2, 4]
>>> #` call a method on each element
>>> freshfruit = ['  banana', '  loganberry ', 'passion fruit  ']
>>> [weapon.strip() for weapon in freshfruit]
['banana', 'loganberry', 'passion fruit']
>>> #` create a list of 2-tuples like (number, square)
>>> [(x, x**2) for x in range(6)]
[(0, 0), (1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]
>>> #` the tuple must be parenthesized, otherwise an error is raised
>>> [x, x**2 for x in range(6)]
  File "<stdin>", line 1, in <module>
    [x, x**2 for x in range(6)]
               ^
SyntaxError: invalid syntax
>>> #` flatten a list using a listcomp with two 'for'
>>> vec = [[1,2,3], [4,5,6], [7,8,9]]
>>> [num for elem in vec for num in elem]
[1, 2, 3, 4, 5, 6, 7, 8, 9]


List comprehensions can contain complex expressions and nested functions:


>>> from math import pi
>>> [str(round(pi, i)) for i in range(1, 6)]
['3.1', '3.14', '3.142', '3.1416', '3.14159']



#### 5.1.4. Nested List Comprehensions

The initial expression in a list comprehension can be any arbitrary expression, including another list comprehension.

Consider the following example of a 3x4 matrix implemented as a list of 3 lists of length 4:


>>> matrix = [
...     [1, 2, 3, 4],
...     [5, 6, 7, 8],
...     [9, 10, 11, 12],
... ]


The following list comprehension will transpose rows and columns:


>>> [[row[i] for row in matrix] for i in range(4)]
[[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]]


As we saw in the previous section, the nested listcomp is evaluated in the context of the for that follows it, so this example is equivalent to:


>>> transposed = []
>>> for i in range(4):
...     transposed.append([row[i] for row in matrix])
...
>>> transposed
[[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]]


which, in turn, is the same as:


>>> transposed = []
>>> for i in range(4):
...     # the following 3 lines implement the nested listcomp
...     transposed_row = []
...     for row in matrix:
...         transposed_row.append(row[i])
...     transposed.append(transposed_row)
...
>>> transposed
[[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]]


In the real world, you should prefer built-in functions to complex flow statements. The zip() function would do a great job for this use case:


>>> list(zip(*matrix))
[(1, 5, 9), (2, 6, 10), (3, 7, 11), (4, 8, 12)]


See Unpacking Argument Lists for details on the asterisk in this line.


### ===🗝 5.2. The del statement

There is a way to remove an item from a list given its index instead of its value: the del statement. This differs from the pop() method which returns a value. The del statement can also be used to remove slices from a list or clear the entire list (which we did earlier by assignment of an empty list to the slice). For example:


>>> a = [-1, 1, 66.25, 333, 333, 1234.5]
>>> del a[0]
>>> a
[1, 66.25, 333, 333, 1234.5]
>>> del a[2:4]
>>> a
[1, 66.25, 1234.5]
>>> del a[:]
>>> a
[]


del can also be used to delete entire variables:


>>> del a


Referencing the name a hereafter is an error (at least until another value is assigned to it). We’ll find other uses for del later.


### ===🗝 5.3. Tuples and Sequences

We saw that lists and strings have many common properties, such as indexing and slicing operations. They are two examples of sequence data types (see Sequence Types — list, tuple, range). Since Python is an evolving language, other sequence data types may be added. There is also another standard sequence data type: the tuple.

A tuple consists of a number of values separated by commas, for instance:


>>> t = 12345, 54321, 'hello!'
>>> t[0]
12345
>>> t
(12345, 54321, 'hello!')
>>> #` Tuples may be nested:
... u = t, (1, 2, 3, 4, 5)
>>> u
((12345, 54321, 'hello!'), (1, 2, 3, 4, 5))
>>> #` Tuples are immutable:
... t[0] = 88888
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: 'tuple' object does not support item assignment
>>> #` but they can contain mutable objects:
... v = ([1, 2, 3], [3, 2, 1])
>>> v
([1, 2, 3], [3, 2, 1])


As you see, on output tuples are always enclosed in parentheses, so that nested tuples are interpreted correctly; they may be input with or without surrounding parentheses, although often parentheses are necessary anyway (if the tuple is part of a larger expression). It is not possible to assign to the individual items of a tuple, however it is possible to create tuples which contain mutable objects, such as lists.

Though tuples may seem similar to lists, they are often used in different situations and for different purposes. Tuples are immutable, and usually contain a heterogeneous sequence of elements that are accessed via unpacking (see later in this section) or indexing (or even by attribute in the case of namedtuples). Lists are mutable, and their elements are usually homogeneous and are accessed by iterating over the list.

A special problem is the construction of tuples containing 0 or 1 items: the syntax has some extra quirks to accommodate these. Empty tuples are constructed by an empty pair of parentheses; a tuple with one item is constructed by following a value with a comma (it is not sufficient to enclose a single value in parentheses). Ugly, but effective. For example:


>>> empty = ()
>>> singleton = 'hello',    # <-- note trailing comma
>>> len(empty)
0
>>> len(singleton)
1
>>> singleton
('hello',)


The statement t = 12345, 54321, 'hello!' is an example of tuple packing: the values 12345, 54321 and 'hello!' are packed together in a tuple. The reverse operation is also possible:


>>> x, y, z = t


This is called, appropriately enough, sequence unpacking and works for any sequence on the right-hand side. Sequence unpacking requires that there are as many variables on the left side of the equals sign as there are elements in the sequence. Note that multiple assignment is really just a combination of tuple packing and sequence unpacking.


### ===🗝 5.4. Sets

Python also includes a data type for sets. A set is an unordered collection with no duplicate elements. Basic uses include membership testing and eliminating duplicate entries. Set objects also support mathematical operations like union, intersection, difference, and symmetric difference.

Curly braces or the set() function can be used to create sets. Note: to create an empty set you have to use set(), not {}; the latter creates an empty dictionary, a data structure that we discuss in the next section.

Here is a brief demonstration:


>>> basket = {'apple', 'orange', 'apple', 'pear', 'orange', 'banana'}
>>> print(basket)                      # show that duplicates have been removed
{'orange', 'banana', 'pear', 'apple'}
>>> 'orange' in basket                 # fast membership testing
True
>>> 'crabgrass' in basket
False

>>> #` Demonstrate set operations on unique letters from two words
...
>>> a = set('abracadabra')
>>> b = set('alacazam')
>>> a                                  # unique letters in a
{'a', 'r', 'b', 'c', 'd'}
>>> a - b                              # letters in a but not in b
{'r', 'd', 'b'}
>>> a | b                              # letters in a or b or both
{'a', 'c', 'r', 'd', 'b', 'm', 'z', 'l'}
>>> a & b                              # letters in both a and b
{'a', 'c'}
>>> a ^ b                              # letters in a or b but not both
{'r', 'd', 'b', 'm', 'z', 'l'}


Similarly to list comprehensions, set comprehensions are also supported:


>>> a = {x for x in 'abracadabra' if x not in 'abc'}
>>> a
{'r', 'd'}



### ===🗝 5.5. Dictionaries

Another useful data type built into Python is the dictionary (see Mapping Types — dict). Dictionaries are sometimes found in other languages as “associative memories” or “associative arrays”. Unlike sequences, which are indexed by a range of numbers, dictionaries are indexed by keys, which can be any immutable type; strings and numbers can always be keys. Tuples can be used as keys if they contain only strings, numbers, or tuples; if a tuple contains any mutable object either directly or indirectly, it cannot be used as a key. You can’t use lists as keys, since lists can be modified in place using index assignments, slice assignments, or methods like append() and extend().

It is best to think of a dictionary as a set of key: value pairs, with the requirement that the keys are unique (within one dictionary). A pair of braces creates an empty dictionary: {}. Placing a comma-separated list of key:value pairs within the braces adds initial key:value pairs to the dictionary; this is also the way dictionaries are written on output.

The main operations on a dictionary are storing a value with some key and extracting the value given the key. It is also possible to delete a key:value pair with del. If you store using a key that is already in use, the old value associated with that key is forgotten. It is an error to extract a value using a non-existent key.

Performing list(d) on a dictionary returns a list of all the keys used in the dictionary, in insertion order (if you want it sorted, just use sorted(d) instead). To check whether a single key is in the dictionary, use the in keyword.

Here is a small example using a dictionary:


>>> tel = {'jack': 4098, 'sape': 4139}
>>> tel['guido'] = 4127
>>> tel
{'jack': 4098, 'sape': 4139, 'guido': 4127}
>>> tel['jack']
4098
>>> del tel['sape']
>>> tel['irv'] = 4127
>>> tel
{'jack': 4098, 'guido': 4127, 'irv': 4127}
>>> list(tel)
['jack', 'guido', 'irv']
>>> sorted(tel)
['guido', 'irv', 'jack']
>>> 'guido' in tel
True
>>> 'jack' not in tel
False


The dict() constructor builds dictionaries directly from sequences of key-value pairs:


>>> dict([('sape', 4139), ('guido', 4127), ('jack', 4098)])
{'sape': 4139, 'guido': 4127, 'jack': 4098}


In addition, dict comprehensions can be used to create dictionaries from arbitrary key and value expressions:


>>> {x: x**2 for x in (2, 4, 6)}
{2: 4, 4: 16, 6: 36}


When the keys are simple strings, it is sometimes easier to specify pairs using keyword arguments:


>>> dict(sape=4139, guido=4127, jack=4098)
{'sape': 4139, 'guido': 4127, 'jack': 4098}



5.6. Looping Techniques

When looping through dictionaries, the key and corresponding value can be retrieved at the same time using the items() method.


>>> knights = {'gallahad': 'the pure', 'robin': 'the brave'}
>>> for k, v in knights.items():
...     print(k, v)
...
gallahad the pure
robin the brave


When looping through a sequence, the position index and corresponding value can be retrieved at the same time using the enumerate() function.


>>> for i, v in enumerate(['tic', 'tac', 'toe']):
...     print(i, v)
...
0 tic
1 tac
2 toe


To loop over two or more sequences at the same time, the entries can be paired with the zip() function.


>>> questions = ['name', 'quest', 'favorite color']
>>> answers = ['lancelot', 'the holy grail', 'blue']
>>> for q, a in zip(questions, answers):
...     print('What is your {0}?  It is {1}.'.format(q, a))
...
What is your name?  It is lancelot.
What is your quest?  It is the holy grail.
What is your favorite color?  It is blue.


To loop over a sequence in reverse, first specify the sequence in a forward direction and then call the reversed() function.


>>> for i in reversed(range(1, 10, 2)):
...     print(i)
...
9
7
5
3
1


To loop over a sequence in sorted order, use the sorted() function which returns a new sorted list while leaving the source unaltered.


>>> basket = ['apple', 'orange', 'apple', 'pear', 'orange', 'banana']
>>> for i in sorted(basket):
...     print(i)
...
apple
apple
banana
orange
orange
pear


Using set() on a sequence eliminates duplicate elements. The use of sorted() in combination with set() over a sequence is an idiomatic way to loop over unique elements of the sequence in sorted order.


>>> basket = ['apple', 'orange', 'apple', 'pear', 'orange', 'banana']
>>> for f in sorted(set(basket)):
...     print(f)
...
apple
banana
orange
pear


It is sometimes tempting to change a list while you are looping over it; however, it is often simpler and safer to create a new list instead.


>>> import math
>>> raw_data = [56.2, float('NaN'), 51.7, 55.3, 52.5, float('NaN'), 47.8]
>>> filtered_data = []
>>> for value in raw_data:
...     if not math.isnan(value):
...         filtered_data.append(value)
...
>>> filtered_data
[56.2, 51.7, 55.3, 52.5, 47.8]



5.7. More on Conditions

The conditions used in while and if statements can contain any operators, not just comparisons.

The comparison operators in and not in check whether a value occurs (does not occur) in a sequence. The operators is and is not compare whether two objects are really the same object. All comparison operators have the same priority, which is lower than that of all numerical operators.

Comparisons can be chained. For example, a < b == c tests whether a is less than b and moreover b equals c.

Comparisons may be combined using the Boolean operators and and or, and the outcome of a comparison (or of any other Boolean expression) may be negated with not. These have lower priorities than comparison operators; between them, not has the highest priority and or the lowest, so that A and not B or C is equivalent to (A and (not B)) or C. As always, parentheses can be used to express the desired composition.

The Boolean operators and and or are so-called short-circuit operators: their arguments are evaluated from left to right, and evaluation stops as soon as the outcome is determined. For example, if A and C are true but B is false, A and B and C does not evaluate the expression C. When used as a general value and not as a Boolean, the return value of a short-circuit operator is the last evaluated argument.

It is possible to assign the result of a comparison or other Boolean expression to a variable. For example,


>>> string1, string2, string3 = '', 'Trondheim', 'Hammer Dance'
>>> non_null = string1 or string2 or string3
>>> non_null
'Trondheim'


Note that in Python, unlike C, assignment inside expressions must be done explicitly with the walrus operator :=. This avoids a common class of problems encountered in C programs: typing = in an expression when == was intended.


5.8. Comparing Sequences and Other Types

Sequence objects typically may be compared to other objects with the same sequence type. The comparison uses lexicographical ordering: first the first two items are compared, and if they differ this determines the outcome of the comparison; if they are equal, the next two items are compared, and so on, until either sequence is exhausted. If two items to be compared are themselves sequences of the same type, the lexicographical comparison is carried out recursively. If all items of two sequences compare equal, the sequences are considered equal. If one sequence is an initial sub-sequence of the other, the shorter sequence is the smaller (lesser) one. Lexicographical ordering for strings uses the Unicode code point number to order individual characters. Some examples of comparisons between sequences of the same type:


```py
(1, 2, 3)              < (1, 2, 4)
[1, 2, 3]              < [1, 2, 4]
'ABC' < 'C' < 'Pascal' < 'Python'
(1, 2, 3, 4)           < (1, 2, 4)
(1, 2)                 < (1, 2, -1)
(1, 2, 3)             == (1.0, 2.0, 3.0)
(1, 2, ('aa', 'ab'))   < (1, 2, ('abc', 'a'), 4)
```

Note that comparing objects of different types with < or > is legal provided that the objects have appropriate comparison methods. For example, mixed numeric types are compared according to their numeric value, so 0 equals 0.0, etc. Otherwise, rather than providing an arbitrary ordering, the interpreter will raise a TypeError exception.

Footnotes

[1] Other languages may return the mutated object, which allows method chaining, such as d->insert("a")->remove("b")->sort();. 


## ==⚡ 6. Modules


If you quit from the Python interpreter and enter it again, the definitions you have made (functions and variables) are lost. Therefore, if you want to write a somewhat longer program, you are better off using a text editor to prepare the input for the interpreter and running it with that file as input instead. This is known as creating a script. As your program gets longer, you may want to split it into several files for easier maintenance. You may also want to use a handy function that you’ve written in several programs without copying its definition into each program.

To support this, Python has a way to put definitions in a file and use them in a script or in an interactive instance of the interpreter. Such a file is called a `module`; definitions from a module can be imported into other modules or into the `main module` (the collection of variables that you have access to in a script executed at the top level and in `calculator mode`).

A module is a file containing Python definitions and statements. The file name is the module name with the suffix .py appended. Within a module, the module’s name (as a string) is available as the value of the global variable __name__. For instance, use your favorite text editor to create a file called `fibo.py` in the current directory with the following contents:

```py
# Fibonacci numbers module

def fib(n):    # write Fibonacci series up to n
    a, b = 0, 1
    while a < n:
        print(a, end=' ')
        a, b = b, a+b
    print()

def fib2(n):   # return Fibonacci series up to n
    result = []
    a, b = 0, 1
    while a < n:
        result.append(a)
        a, b = b, a+b
    return result
```

Now enter the Python interpreter and import this module with the following command:


>>> import fibo


This does not enter the names of the functions defined in fibo directly in the current symbol table; it only enters the module name fibo there. Using the module name you can access the functions:


>>> fibo.fib(1000)
0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987
>>> fibo.fib2(100)
[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89]
>>> fibo.__name__
'fibo'


If you intend to use a function often you can assign it to a local name:


>>> fib = fibo.fib
>>> fib(500)
0 1 1 2 3 5 8 13 21 34 55 89 144 233 377



### ===🗝 6.1. More on Modules

A module can contain executable statements as well as function definitions. These statements are intended to initialize the module. They are executed only the first time the module name is encountered in an import statement. [1] (They are also run if the file is executed as a script.)

Each module has its own private symbol table, which is used as the global symbol table by all functions defined in the module. Thus, the author of a module can use global variables in the module without worrying about accidental clashes with a user’s global variables. On the other hand, if you know what you are doing you can touch a module’s global variables with the same notation used to refer to its functions, modname.itemname.

Modules can import other modules. It is customary but not required to place all import statements at the beginning of a module (or script, for that matter). The imported module names are placed in the importing module’s global symbol table.

There is a variant of the import statement that imports names from a module directly into the importing module’s symbol table. For example:


>>> from fibo import fib, fib2
>>> fib(500)
0 1 1 2 3 5 8 13 21 34 55 89 144 233 377


This does not introduce the module name from which the imports are taken in the local symbol table (so in the example, fibo is not defined).

There is even a variant to import all names that a module defines:


>>> from fibo import *
>>> fib(500)
0 1 1 2 3 5 8 13 21 34 55 89 144 233 377


This imports all names except those beginning with an underscore ( _ ). In most cases Python programmers do not use this facility since it introduces an unknown set of names into the interpreter, possibly hiding some things you have already defined.

Note that in general the practice of importing * from a module or package is frowned upon, since it often causes poorly readable code. However, it is okay to use it to save typing in interactive sessions.

If the module name is followed by as, then the name following as is bound directly to the imported module.


>>> import fibo as fib
>>> fib.fib(500)
0 1 1 2 3 5 8 13 21 34 55 89 144 233 377


This is effectively importing the module in the same way that import fibo will do, with the only difference of it being available as fib.

It can also be used when utilising from with similar effects:


>>> from fibo import fib as fibonacci
>>> fibonacci(500)
0 1 1 2 3 5 8 13 21 34 55 89 144 233 377


Note:
 For efficiency reasons, each module is only imported once per interpreter session. Therefore, if you change your modules, you must restart the interpreter – or, if it’s just one module you want to test interactively, use `importlib.reload()`, e.g. import importlib; importlib.reload(modulename).
 


### ===🗝 6.1.1. Executing modules as scripts

When you run a Python module with


    python fibo.py <arguments>


the code in the module will be executed, just as if you imported it, but with the __name__ set to "__main__". That means that by adding this code at the end of your module:


    if __name__ == "__main__":
        import sys
        fib(int(sys.argv[1]))


you can make the file usable as a script as well as an importable module, because the code that parses the command line only runs if the module is executed as the “main” file:


    $ python fibo.py 50
    0 1 1 2 3 5 8 13 21 34


If the module is imported, the code is not run:


>>> import fibo
>>>


This is often used either to provide a convenient user interface to a module, or for testing purposes (running the module as a script executes a test suite).


### ===🗝 6.1.2. The Module Search Path

When a module named spam is imported, the interpreter first searches for a built-in module with that name. If not found, it then searches for a file named spam.py in a list of directories given by the variable `sys.path`. `sys.path` is initialized from these locations:

↪• The directory containing the input script (or the current directory when no file is specified).
↪• `PYTHONPATH` (a list of directory names, with the same syntax as the shell variable `PATH`).
↪• The installation-dependent default.

Note:
 On file systems which support symlinks, the directory containing the input script is calculated after the symlink is followed. In other words the directory containing the symlink is not added to the module search path.
 

After initialization, Python programs can modify `sys.path`. The directory containing the script being run is placed at the beginning of the search path, ahead of the standard library path. This means that scripts in that directory will be loaded instead of modules of the same name in the library directory. This is an error unless the replacement is intended. See section Standard Modules for more information.


### ===🗝 6.1.3. “Compiled” Python files

To speed up loading modules, Python caches the compiled version of each module in the __pycache__ directory under the name `module.version.pyc`, where the version encodes the format of the compiled file; it generally contains the Python version number. For example, in CPython release 3.3 the compiled version of `spam.py` would be cached as `__pycache__/spam.cpython-33.pyc`. This naming convention allows compiled modules from different releases and different versions of Python to coexist.

Python checks the modification date of the source against the compiled version to see if it’s out of date and needs to be recompiled. This is a completely automatic process. Also, the compiled modules are platform-independent, so the same library can be shared among systems with different architectures.

Python does not check the cache in two circumstances. First, it always recompiles and does not store the result for the module that’s loaded directly from the command line. Second, it does not check the cache if there is no source module. To support a non-source (compiled only) distribution, the compiled module must be in the source directory, and there must not be a source module.

Some tips for experts:

↪• You can use the `-O` or `-OO` switches on the Python command to reduce the size of a compiled module. The `-O` switch removes assert statements, the `-OO` switch removes both assert statements and __doc__ strings. Since some programs may rely on having these available, you should only use this option if you know what you’re doing. “Optimized” modules have an opt- tag and are usually smaller. Future releases may change the effects of optimization.
↪• A program doesn’t run any faster when it is read from a `.pyc` file than when it is read from a `.py` file; the only thing that’s faster about `.pyc` files is the speed with which they are loaded.
↪• The module compileall can create `.pyc` files for all modules in a directory.
↪• There is more detail on this process, including a flow chart of the decisions, in PEP 3147.


### ===🗝 6.2. Standard Modules

Python comes with a library of standard modules, described in a separate document, the Python Library Reference (“Library Reference” hereafter). Some modules are built into the interpreter; these provide access to operations that are not part of the core of the language but are nevertheless built in, either for efficiency or to provide access to operating system primitives such as system calls. The set of such modules is a configuration option which also depends on the underlying platform. For example, the winreg module is only provided on Windows systems. One particular module deserves some attention: sys, which is built into every Python interpreter. The variables `sys.ps1` and `sys.ps2` define the strings used as primary and secondary prompts:


>>> import sys
>>> sys.ps1
'>>> '
>>> sys.ps2
'... '
>>> sys.ps1 = 'C> '
C> print('Yuck!')
Yuck!
C>


These two variables are only defined if the interpreter is in interactive mode.

The variable `sys.path` is a list of strings that determines the interpreter’s search path for modules. It is initialized to a default path taken from the environment variable `PYTHONPATH`, or from a built-in default if `PYTHONPATH` is not set. You can modify it using standard list operations:


>>> import sys
>>> `sys.path`.append('/ufs/guido/lib/python')



### ===🗝 6.3. The dir() Function

The built-in function `dir()` is used to find out which names a module defines. It returns a sorted list of strings:


>>> import fibo, sys
>>> dir(fibo)
['__name__', 'fib', 'fib2']
>>> dir(sys)  
['__breakpointhook__', '__displayhook__', '__doc__', '__excepthook__',
 '__interactivehook__', '__loader__', '__name__', '__package__', '__spec__',
 '__stderr__', '__stdin__', '__stdout__', '__unraisablehook__',
 '_clear_type_cache', '_current_frames', '_debugmallocstats', '_framework',
 '_getframe', '_git', '_home', '_xoptions', 'abiflags', 'addaudithook',
 'api_version', 'argv', 'audit', 'base_exec_prefix', 'base_prefix',
 'breakpointhook', 'builtin_module_names', 'byteorder', 'call_tracing',
 'callstats', 'copyright', 'displayhook', 'dont_write_bytecode', 'exc_info',
 'excepthook', 'exec_prefix', 'executable', 'exit', 'flags', 'float_info',
 'float_repr_style', 'get_asyncgen_hooks', 'get_coroutine_origin_tracking_depth',
 'getallocatedblocks', 'getdefaultencoding', 'getdlopenflags',
 'getfilesystemencodeerrors', 'getfilesystemencoding', 'getprofile',
 'getrecursionlimit', 'getrefcount', 'getsizeof', 'getswitchinterval',
 'gettrace', 'hash_info', 'hexversion', 'implementation', 'int_info',
 'intern', 'is_finalizing', 'last_traceback', 'last_type', 'last_value',
 'maxsize', 'maxunicode', 'meta_path', 'modules', 'path', 'path_hooks',
 'path_importer_cache', 'platform', 'prefix', 'ps1', 'ps2', 'pycache_prefix',
 'set_asyncgen_hooks', 'set_coroutine_origin_tracking_depth', 'setdlopenflags',
 'setprofile', 'setrecursionlimit', 'setswitchinterval', 'settrace', 'stderr',
 'stdin', 'stdout', 'thread_info', 'unraisablehook', 'version', 'version_info',
 'warnoptions']


Without arguments, `dir()` lists the names you have defined currently:


>>> a = [1, 2, 3, 4, 5]
>>> import fibo
>>> fib = fibo.fib
>>> dir()
['__builtins__', '__name__', 'a', 'fib', 'fibo', 'sys']


Note that it lists all types of names: variables, modules, functions, etc.

dir() does not list the names of built-in functions and variables. If you want a list of those, they are defined in the standard module builtins:


>>> import builtins
>>> dir(builtins)  
['ArithmeticError', 'AssertionError', 'AttributeError', 'BaseException',
 'BlockingIOError', 'BrokenPipeError', 'BufferError', 'BytesWarning',
 'ChildProcessError', 'ConnectionAbortedError', 'ConnectionError',
 'ConnectionRefusedError', 'ConnectionResetError', 'DeprecationWarning',
 'EOFError', 'Ellipsis', 'EnvironmentError', 'Exception', 'False',
 'FileExistsError', 'FileNotFoundError', 'FloatingPointError',
 'FutureWarning', 'GeneratorExit', 'IOError', 'ImportError',
 'ImportWarning', 'IndentationError', 'IndexError', 'InterruptedError',
 'IsADirectoryError', 'KeyError', 'KeyboardInterrupt', 'LookupError',
 'MemoryError', 'NameError', 'None', 'NotADirectoryError', 'NotImplemented',
 'NotImplementedError', 'OSError', 'OverflowError',
 'PendingDeprecationWarning', 'PermissionError', 'ProcessLookupError',
 'ReferenceError', 'ResourceWarning', 'RuntimeError', 'RuntimeWarning',
 'StopIteration', 'SyntaxError', 'SyntaxWarning', 'SystemError',
 'SystemExit', 'TabError', 'TimeoutError', 'True', 'TypeError',
 'UnboundLocalError', 'UnicodeDecodeError', 'UnicodeEncodeError',
 'UnicodeError', 'UnicodeTranslateError', 'UnicodeWarning', 'UserWarning',
 'ValueError', 'Warning', 'ZeroDivisionError', '_', '__build_class__',
 '__debug__', '__doc__', '__import__', '__name__', '__package__', 'abs',
 'all', 'any', 'ascii', 'bin', 'bool', 'bytearray', 'bytes', 'callable',
 'chr', 'classmethod', 'compile', 'complex', 'copyright', 'credits',
 'delattr', 'dict', 'dir', 'divmod', 'enumerate', 'eval', 'exec', 'exit',
 'filter', 'float', 'format', 'frozenset', 'getattr', 'globals', 'hasattr',
 'hash', 'help', 'hex', 'id', 'input', 'int', 'isinstance', 'issubclass',
 'iter', 'len', 'license', 'list', 'locals', 'map', 'max', 'memoryview',
 'min', 'next', 'object', 'oct', 'open', 'ord', 'pow', 'print', 'property',
 'quit', 'range', 'repr', 'reversed', 'round', 'set', 'setattr', 'slice',
 'sorted', 'staticmethod', 'str', 'sum', 'super', 'tuple', 'type', 'vars',
 'zip']



### ===🗝 6.4. Packages

Packages are a way of structuring Python’s module namespace by using “dotted module names”. For example, the module name A.B designates a submodule named B in a package named A. Just like the use of modules saves the authors of different modules from having to worry about each other’s global variable names, the use of dotted module names saves the authors of multi-module packages like NumPy or Pillow from having to worry about each other’s module names.

Suppose you want to design a collection of modules (a “package”) for the uniform handling of sound files and sound data. There are many different sound file formats (usually recognized by their extension, for example: .wav, .aiff, .au), so you may need to create and maintain a growing collection of modules for the conversion between the various file formats. There are also many different operations you might want to perform on sound data (such as mixing, adding echo, applying an equalizer function, creating an artificial stereo effect), so in addition you will be writing a never-ending stream of modules to perform these operations. Here’s a possible structure for your package (expressed in terms of a hierarchical filesystem):


    sound/                      Top-level package
      __init__.py               Initialize the sound package
      formats/                  Subpackage for file format conversions
              __init__.py
              wavread.py
              wavwrite.py
              aiffread.py
              aiffwrite.py
              auread.py
              auwrite.py
              ...
      effects/                  Subpackage for sound effects
              __init__.py
              echo.py
              surround.py
              reverse.py
              ...
      filters/                  Subpackage for filters
              __init__.py
              equalizer.py
              vocoder.py
              karaoke.py
              ...


When importing the package, Python searches through the directories on `sys.path` looking for the package subdirectory.

The __init__.py files are required to make Python treat directories containing the file as packages. This prevents directories with a common name, such as string, unintentionally hiding valid modules that occur later on the module search path. In the simplest case, __init__.py can just be an empty file, but it can also execute initialization code for the package or set the __all__ variable, described later.

Users of the package can import individual modules from the package, for example:


    import sound.effects.echo


This loads the submodule sound.effects.echo. It must be referenced with its full name.


    sound.effects.echo.echofilter(input, output, delay=0.7, atten=4)


An alternative way of importing the submodule is:


    from sound.effects import echo


This also loads the submodule echo, and makes it available without its package prefix, so it can be used as follows:


    echo.echofilter(input, output, delay=0.7, atten=4)


Yet another variation is to import the desired function or variable directly:


    from sound.effects.echo import echofilter


Again, this loads the submodule echo, but this makes its function echofilter() directly available:


    echofilter(input, output, delay=0.7, atten=4)


Note that when using from package import item, the item can be either a submodule (or subpackage) of the package, or some other name defined in the package, like a function, class or variable. The import statement first tests whether the item is defined in the package; if not, it assumes it is a module and attempts to load it. If it fails to find it, an ImportError exception is raised.

Contrarily, when using syntax like import item.subitem.subsubitem, each item except for the last must be a package; the last item can be a module or a package but can’t be a class or function or variable defined in the previous item.


#### 6.4.1. Importing * From a Package

Now what happens when the user writes `from sound.effects import *` ? Ideally, one would hope that this somehow goes out to the filesystem, finds which submodules are present in the package, and imports them all. This could take a long time and importing sub-modules might have unwanted side-effects that should only happen when the sub-module is explicitly imported.

The only solution is for the package author to provide an explicit index of the package. The import statement uses the following convention: if a package’s __init__.py code defines a list named __all__, it is taken to be the list of module names that should be imported when from package `import *` is encountered. It is up to the package author to keep this list up-to-date when a new version of the package is released. Package authors may also decide not to support it, if they don’t see a use for importing * from their package. For example, the file sound/effects/__init__.py could contain the following code:


    __all__ = ["echo", "surround", "reverse"]


This would mean that from sound.effects import * would import the three named submodules of the sound package.

If __all__ is not defined, the statement from sound.effects import * does not import all submodules from the package sound.effects into the current namespace; it only ensures that the package sound.effects has been imported (possibly running any initialization code in __init__.py) and then imports whatever names are defined in the package. This includes any names defined (and submodules explicitly loaded) by __init__.py. It also includes any submodules of the package that were explicitly loaded by previous import statements. Consider this code:


    import sound.effects.echo
    import sound.effects.surround
    from sound.effects import *


In this example, the echo and surround modules are imported in the current namespace because they are defined in the sound.effects package when the from...import statement is executed. (This also works when __all__ is defined.)

Although certain modules are designed to export only names that follow certain patterns when you use i`mport *`, it is still considered bad practice in production code.

Remember, there is nothing wrong with using from package import specific_submodule! In fact, this is the recommended notation unless the importing module needs to use submodules with the same name from different packages.


#### 6.4.2. Intra-package References

When packages are structured into subpackages (as with the sound package in the example), you can use absolute imports to refer to submodules of siblings packages. For example, if the module sound.filters.vocoder needs to use the echo module in the sound.effects package, it can use from sound.effects import echo.

You can also write relative imports, with the from module import name form of import statement. These imports use leading dots to indicate the current and parent packages involved in the relative import. From the surround module for example, you might use:


    from . import echo
    from .. import formats
    from ..filters import equalizer


Note that relative imports are based on the name of the current module. Since the name of the main module is always "__main__", modules intended for use as the main module of a Python application must always use absolute imports.


#### 6.4.3. Packages in Multiple Directories

Packages support one more special attribute, __path__. This is initialized to be a list containing the name of the directory holding the package’s __init__.py before the code in that file is executed. This variable can be modified; doing so affects future searches for modules and subpackages contained in the package.

While this feature is not often needed, it can be used to extend the set of modules found in a package.

Footnotes

[1] In fact function definitions are also ‘statements’ that are ‘executed’; the execution of a module-level function definition enters the function name in the module’s global symbol table. 


## ==⚡ 7. Input and Output


There are several ways to present the output of a program; data can be printed in a human-readable form, or written to a file for future use. This chapter will discuss some of the possibilities.


### ===🗝 7.1. Fancier Output Formatting

So far we’ve encountered two ways of writing values: expression statements and the `print()` function. (A third way is using the `write()` method of file objects; the standard output file can be referenced as `sys.stdout`. See the Library Reference for more information on this.)

Often you’ll want more control over the formatting of your output than simply printing space-separated values. There are several ways to format output.

• To use formatted string literals, begin a string with `f` or `F` before the opening quotation mark or triple quotation mark. Inside this string, you can write a Python expression between `{ and }` characters that can refer to variables or literal values.


>>> year = 2016
>>> event = 'Referendum'
>>> f'Results of the {year} {event}'
'Results of the 2016 Referendum'



• The `str.format()` method of strings requires more manual effort. You’ll still use `{ and }` to mark where a variable will be substituted and can provide detailed formatting directives, but you’ll also need to provide the information to be formatted.


>>> yes_votes = 42_572_654
>>> no_votes = 43_132_495
>>> percentage = yes_votes / (yes_votes + no_votes)
>>> '{:-9} YES votes  {:2.2%}'.format(yes_votes, percentage)
' 42572654 YES votes  49.67%'



• Finally, you can do all the string handling yourself by using string slicing and concatenation operations to create any layout you can imagine. The string type has some methods that perform useful operations for padding strings to a given column width.


When you don’t need fancy output but just want a quick display of some variables for debugging purposes, you can convert any value to a string with the `repr()` or `str()` functions.

The `str()` function is meant to return representations of values which are fairly human-readable, while `repr()` is meant to generate representations which can be read by the interpreter (or will force a SyntaxError if there is no equivalent syntax). For objects which don’t have a particular representation for human consumption, `str()` will return the same value as `repr()`. Many values, such as numbers or structures like lists and dictionaries, have the same representation using either function. Strings, in particular, have two distinct representations.

Some examples:


>>> s = 'Hello, world.'
>>> str(s)
'Hello, world.'
>>> repr(s)
"'Hello, world.'"
>>> str(1/7)
'0.14285714285714285'
>>> x = 10 * 3.25
>>> y = 200 * 200
>>> s = 'The value of x is ' + repr(x) + ', and y is ' + repr(y) + '...'
>>> print(s)
The value of x is 32.5, and y is 40000...
>>> #` The repr() of a string adds string quotes and backslashes:
... hello = 'hello, world\n'
>>> hellos = repr(hello)
>>> print(hellos)
'hello, world\n'
>>> #` The argument to repr() may be any Python object:
... repr((x, y, ('spam', 'eggs')))
"(32.5, 40000, ('spam', 'eggs'))"


The string module contains a Template class that offers yet another way to substitute values into strings, using placeholders like $x and replacing them with values from a dictionary, but offers much less control of the formatting.


### ===🗝 7.1.1. Formatted String Literals

Formatted string literals (also called f-strings for short) let you include the value of Python expressions inside a string by prefixing the string with f or F and writing expressions as `{expression}`.

An optional format specifier can follow the expression. This allows greater control over how the value is formatted. The following example rounds pi to three places after the decimal:


>>> import math
>>> print(f'The value of pi is approximately {math.pi:.3f}.')
The value of pi is approximately 3.142.


Passing an integer after the ':' will cause that field to be a minimum number of characters wide. This is useful for making columns line up.


>>> table = {'Sjoerd': 4127, 'Jack': 4098, 'Dcab': 7678}
>>> for name, phone in table.items():
...     print(f'{name:10} ==> {phone:10d}')
...
Sjoerd     ==>       4127
Jack       ==>       4098
Dcab       ==>       7678


Other modifiers can be used to convert the value before it is formatted. '!a' applies `ascii()`, '!s' applies `str()`, and '!r' applies `repr()`:


>>> animals = 'eels'
>>> print(f'My hovercraft is full of {animals}.')
My hovercraft is full of eels.
>>> print(f'My hovercraft is full of {animals!r}.')
My hovercraft is full of 'eels'.


For a reference on these format specifications, see the reference guide for the Format Specification Mini-Language.


### ===🗝 7.1.2. The String format() Method

Basic usage of the `str.format()` method looks like this:


>>> print('We are the {} who say "{}!"'.format('knights', 'Ni'))
We are the knights who say "Ni!"


The brackets and characters within them (called format fields) are replaced with the objects passed into the `str.format()` method. A number in the brackets can be used to refer to the position of the object passed into the `str.format()` method.


>>> print('{0} and {1}'.format('spam', 'eggs'))
spam and eggs
>>> print('{1} and {0}'.format('spam', 'eggs'))
eggs and spam


If keyword arguments are used in the `str.format()` method, their values are referred to by using the name of the argument.


>>> print('This {food} is {adjective}.'.format(
...       food='spam', adjective='absolutely horrible'))
This spam is absolutely horrible.


Positional and keyword arguments can be arbitrarily combined:


>>> print('The story of {0}, {1}, and {other}.'.format('Bill', 'Manfred',
                                                       other='Georg'))
The story of Bill, Manfred, and Georg.


If you have a really long format string that you don’t want to split up, it would be nice if you could reference the variables to be formatted by name instead of by position. This can be done by simply passing the dict and using square brackets '[]' to access the keys.


>>> table = {'Sjoerd': 4127, 'Jack': 4098, 'Dcab': 8637678}
>>> print('Jack: {0[Jack]:d}; Sjoerd: {0[Sjoerd]:d}; '
...       'Dcab: {0[Dcab]:d}'.format(table))
Jack: 4098; Sjoerd: 4127; Dcab: 8637678


This could also be done by passing the table as keyword arguments with the ‘`**`’ notation.


>>> table = {'Sjoerd': 4127, 'Jack': 4098, 'Dcab': 8637678}
>>> print('Jack: {Jack:d}; Sjoerd: {Sjoerd:d}; Dcab: {Dcab:d}'.format(**table))
Jack: 4098; Sjoerd: 4127; Dcab: 8637678


This is particularly useful in combination with the built-in function `vars()`, which returns a dictionary containing all local variables.

As an example, the following lines produce a tidily-aligned set of columns giving integers and their squares and cubes:


>>> for x in range(1, 11):
...     print('{0:2d} {1:3d} {2:4d}'.format(x, x*x, x*x*x))
...
 1   1    1
 2   4    8
 3   9   27
 4  16   64
 5  25  125
 6  36  216
 7  49  343
 8  64  512
 9  81  729
10 100 1000


For a complete overview of string formatting with `str.format()`, see Format String Syntax.


### ===🗝 7.1.3. Manual String Formatting

Here’s the same table of squares and cubes, formatted manually:


>>> for x in range(1, 11):
...     print(repr(x).rjust(2), repr(x*x).rjust(3), end=' ')
...     # Note use of 'end' on previous line
...     print(repr(x*x*x).rjust(4))
...
 1   1    1
 2   4    8
 3   9   27
 4  16   64
 5  25  125
 6  36  216
 7  49  343
 8  64  512
 9  81  729
10 100 1000


(Note that the one space between each column was added by the way `print()` works: it always adds spaces between its arguments.)

The `str.rjust()` method of string objects right-justifies a string in a field of a given width by padding it with spaces on the left. There are similar methods `str.ljust()` and `str.center()`. These methods do not write anything, they just return a new string. If the input string is too long, they don’t truncate it, but return it unchanged; this will mess up your column lay-out but that’s usually better than the alternative, which would be lying about a value. (If you really want truncation you can always add a slice operation, as in `x.ljust(n)[:n]`.)

There is another method, `str.zfill()`, which pads a numeric string on the left with zeros. It understands about plus and minus signs:


>>> '12'.zfill(5)
'00012'
>>> '-3.14'.zfill(7)
'-003.14'
>>> '3.14159265359'.zfill(5)
'3.14159265359'



### ===🗝 7.1.4. Old string formatting

The % operator (modulo) can also be used for string formatting. Given `'string' % values`, instances of` %` in string are replaced with zero or more elements of values. This operation is commonly known as string interpolation. For example:


>>> import math
>>> print('The value of pi is approximately %5.3f.' % math.pi)
The value of pi is approximately 3.142.


More information can be found in the printf-style String Formatting section.


### ===🗝 7.2. Reading and Writing Files

`open()` returns a file object, and is most commonly used with two arguments: `open(filename, mode)`.


>>> f = open('workfile', 'w')


The first argument is a string containing the filename. The second argument is another string containing a few characters describing the way in which the file will be used. mode can be `'r'` when the file will only be read, `'w'` for only writing (an existing file with the same name will be erased), and `'a'` opens the file for appending; any data written to the file is automatically added to the end. `'r+'` opens the file for both reading and writing. The mode argument is optional; `'r'` will be assumed if it’s omitted.

Normally, files are opened in text mode, that means, you read and write strings from and to the file, which are encoded in a specific encoding. If encoding is not specified, the default is platform dependent (see `open()`). `'b'` appended to the mode opens the file in binary mode: now the data is read and written in the form of bytes objects. This mode should be used for all files that don’t contain text.

In text mode, the default when reading is to convert platform-specific line endings (`\n` on Unix, `\r\n` on Windows) to just `\n`. When writing in text mode, the default is to convert occurrences of `\n` back to platform-specific line endings. This behind-the-scenes modification to file data is fine for text files, but will corrupt binary data like that in JPEG or EXE files. Be very careful to use binary mode when reading and writing such files.

It is good practice to use the with keyword when dealing with file objects. The advantage is that the file is properly closed after its suite finishes, even if an exception is raised at some point. Using with is also much shorter than writing equivalent try-finally blocks:


>>> with open('workfile') as f:
...     read_data = f.read()

>>> #` We can check that the file has been automatically closed.
>>> f.closed
True


If you’re not using the `with` keyword, then you should call `f.close()` to close the file and immediately free up any system resources used by it.

Warning:
 Calling `f.write()` without using the `with` keyword or calling `f.close()` might result in the arguments of `f.write()` not being completely written to the disk, even if the program exits successfully.
 

After a file object is closed, either by a `with` statement or by calling `f.close()`, attempts to use the file object will automatically fail.


>>> f.close()
>>> f.read()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ValueError: I/O operation on closed file.



### ===🗝 7.2.1. Methods of File Objects

The rest of the examples in this section will assume that a file object called f has already been created.

To read a file’s contents, call `f.read(size)`, which reads some quantity of data and returns it as a string (in text mode) or bytes object (in binary mode). size is an optional numeric argument. When `size` is omitted or negative, the entire contents of the file will be read and returned; it’s your problem if the file is twice as large as your machine’s memory. Otherwise, at most `size` characters (in text mode) or `size` bytes (in binary mode) are read and returned. If the end of the file has been reached, `f.read()` will return an empty string ('').


>>> f.read()
'This is the entire file.\n'
>>> f.read()
''


`f.readline()` reads a single line from the file; a newline character (`\n`) is left at the end of the string, and is only omitted on the last line of the file if the file doesn’t end in a newline. This makes the return value unambiguous; if `f.readline()` returns an empty string, the end of the file has been reached, while a blank line is represented by `'\n'`, a string containing only a single newline.


>>> f.readline()
'This is the first line of the file.\n'
>>> f.readline()
'Second line of the file\n'
>>> f.readline()
''


For reading lines from a file, you can loop over the file object. This is memory efficient, fast, and leads to simple code:


>>> for line in f:
...     print(line, end='')
...
This is the first line of the file.
Second line of the file


If you want to read all the lines of a file in a list you can also use `list(f)` or `f.readlines()`.

`f.write(string)` writes the contents of string to the file, returning the number of characters written.


>>> f.write('This is a test\n')
15


Other types of objects need to be converted – either to a string (in text mode) or a bytes object (in binary mode) – before writing them:


>>> value = ('the answer', 42)
>>> s = str(value)  # convert the tuple to string
>>> f.write(s)
18


`f.tell()` returns an integer giving the file object’s current position in the file represented as number of bytes from the beginning of the file when in binary mode and an opaque number when in text mode.

To change the file object’s position, use `f.seek(offset, whence)`. The position is computed from adding offset to a reference point; the reference point is selected by the whence argument. A whence value of 0 measures from the beginning of the file, 1 uses the current file position, and 2 uses the end of the file as the reference point. whence can be omitted and defaults to 0, using the beginning of the file as the reference point.


>>> f = open('workfile', 'rb+')
>>> f.write(b'0123456789abcdef')
16
>>> f.seek(5)      # Go to the 6th byte in the file
5
>>> f.read(1)
b'5'
>>> f.seek(-3, 2)  # Go to the 3rd byte before the end
13
>>> f.read(1)
b'd'


In text files (those opened without a b in the mode string), only seeks relative to the beginning of the file are allowed (the exception being seeking to the very file end with `seek(0, 2)`) and the only valid offset values are those returned from the` f.tell()`, or zero. Any other offset value produces undefined behaviour.

File objects have some additional methods, such as `isatty()` and `truncate()` which are less frequently used; consult the Library Reference for a complete guide to file objects.


### ===🗝 7.2.2. Saving structured data with json

Strings can easily be written to and read from a file. Numbers take a bit more effort, since the `read()` method only returns strings, which will have to be passed to a function like `int()`, which takes a string like '123' and returns its numeric value 123. When you want to save more complex data types like nested lists and dictionaries, parsing and serializing by hand becomes complicated.

Rather than having users constantly writing and debugging code to save complicated data types to files, Python allows you to use the popular data interchange format called `JSON` (JavaScript Object Notation). The standard module called json can take Python data hierarchies, and convert them to string representations; this process is called `serializing`. Reconstructing the data from the string representation is called `deserializing`. Between `serializing` and `deserializing`, the string representing the object may have been stored in a file or data, or sent over a network connection to some distant machine.

Note:
 The JSON format is commonly used by modern applications to allow for data exchange. Many programmers are already familiar with it, which makes it a good choice for interoperability.
 

If you have an object x, you can view its JSON string representation with a simple line of code:


>>> import json
>>> json.dumps([1, 'simple', 'list'])
'[1, "simple", "list"]'


Another variant of the `dumps()` function, called `dump()`, simply serializes the object to a text file. So if f is a text file object opened for writing, we can do this:


    json.dump(x, f)


To decode the object again, if f is a text file object which has been opened for reading:


    x = json.load(f)


This simple serialization technique can handle lists and dictionaries, but serializing arbitrary class instances in JSON requires a bit of extra effort. The reference for the json module contains an explanation of this.

See also:
 pickle - the pickle module
 
Contrary to JSON, `pickle` is a protocol which allows the serialization of arbitrarily complex Python objects. As such, it is specific to Python and cannot be used to communicate with applications written in other languages. It is also insecure by default: deserializing pickle data coming from an untrusted source can execute arbitrary code, if the data was crafted by a skilled attacker.



## ==⚡ 8. Errors and Exceptions


Until now error messages haven’t been more than mentioned, but if you have tried out the examples you have probably seen some. There are (at least) two distinguishable kinds of errors: syntax errors and exceptions.


### ===🗝 8.1. Syntax Errors

Syntax errors, also known as parsing errors, are perhaps the most common kind of complaint you get while you are still learning Python:


>>> while True print('Hello world')
  File "<stdin>", line 1
    while True print('Hello world')
                   ^
SyntaxError: invalid syntax


The parser repeats the offending line and displays a little ‘arrow’ pointing at the earliest point in the line where the error was detected. The error is caused by (or at least detected at) the token preceding the arrow: in the example, the error is detected at the function `print()`, since a colon (':') is missing before it. File name and line number are printed so you know where to look in case the input came from a script.


### ===🗝 8.2. Exceptions

Even if a statement or expression is syntactically correct, it may cause an error when an attempt is made to execute it. Errors detected during execution are called exceptions and are not unconditionally fatal: you will soon learn how to handle them in Python programs. Most exceptions are not handled by programs, however, and result in error messages as shown here:


>>> 10 * (1/0)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ZeroDivisionError: division by zero
>>> 4 + spam*3
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'spam' is not defined
>>> '2' + 2
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: Can't convert 'int' object to str implicitly


The last line of the error message indicates what happened. Exceptions come in different types, and the type is printed as part of the message: the types in the example are `ZeroDivisionError`, `NameError` and `TypeError`. The string printed as the exception type is the name of the built-in exception that occurred. This is true for all built-in exceptions, but need not be true for user-defined exceptions (although it is a useful convention). Standard exception names are built-in identifiers (not reserved keywords).

The rest of the line provides detail based on the type of exception and what caused it.

The preceding part of the error message shows the context where the exception occurred, in the form of a stack traceback. In general it contains a stack traceback listing source lines; however, it will not display lines read from standard input.

Built-in Exceptions lists the built-in exceptions and their meanings.


### ===🗝 8.3. Handling Exceptions

It is possible to write programs that handle selected exceptions. Look at the following example, which asks the user for input until a valid integer has been entered, but allows the user to interrupt the program (using Control-C or whatever the operating system supports); note that a user-generated interruption is signalled by raising the `KeyboardInterrupt` exception.


>>> while True:
...     try:
...         x = int(input("Please enter a number: "))
...         break
...     except ValueError:
...         print("Oops!  That was no valid number.  Try again...")
...


The `try` statement works as follows.
↪• First, the `try` clause (the statement(s) between the `try` and `except` keywords) is executed.
↪• If no exception occurs, the `except` clause is skipped and execution of the `try` statement is finished.
↪• If an exception occurs during execution of the `try` clause, the rest of the clause is skipped. Then if its type matches the exception named after the except keyword, the `except` clause is executed, and then execution continues after the `try` statement.
↪• If an exception occurs which does not match the exception named in the `except` clause, it is passed on to outer `try` statements; if no handler is found, it is an unhandled exception and execution stops with a message as shown above.

A try statement may have more than one `except` clause, to specify handlers for different exceptions. At most one handler will be executed. Handlers only handle exceptions that occur in the corresponding try clause, not in other handlers of the same try statement. An `except` clause may name multiple exceptions as a parenthesized tuple, for example:

>>>
... except (RuntimeError, TypeError, NameError):
...     pass


A class in an `except` clause is compatible with an exception if it is the same class or a base class thereof (but not the other way around — an `except` clause listing a derived class is not compatible with a base class). For example, the following code will print B, C, D in that order:


```py
class B(Exception):
    pass

class C(B):
    pass

class D(C):
    pass

for cls in [B, C, D]:
    try:
        raise cls()
    except D:
        print("D")
    except C:
        print("C")
    except B:
        print("B")
```


Note that if the `except` clauses were reversed (with `except B` first), it would have printed B, B, B — the first matching `except` clause is triggered.

The last `except` clause may omit the exception name(s), to serve as a wildcard. Use this with extreme caution, since it is easy to mask a real programming error in this way! It can also be used to print an error message and then re-raise the exception (allowing a caller to handle the exception as well):


```py
import sys

try:
    f = open('myfile.txt')
    s = f.readline()
    i = int(s.strip())
except OSError as err:
    print("OS error: {0}".format(err))
except ValueError:
    print("Could not convert data to an integer.")
except:
    print("Unexpected error:", sys.exc_info()[0])
    raise
```


The `try … except` statement has an optional else clause, which, when present, must follow all except clauses. It is useful for code that must be executed if the try clause does not raise an exception. For example:


```py
for arg in sys.argv[1:]:
    try:
        f = open(arg, 'r')
    except OSError:
        print('cannot open', arg)
    else:
        print(arg, 'has', len(f.readlines()), 'lines')
        f.close()
```

The use of the `else` clause is better than adding additional code to the `try` clause because it avoids accidentally catching an exception that wasn’t raised by the code being protected by the `try … except` statement.

When an exception occurs, it may have an associated value, also known as the exception’s argument. The presence and type of the argument depend on the exception type.

The except clause may specify a variable after the exception name. The variable is bound to an exception instance with the arguments stored in instance.args. For convenience, the exception instance defines __str__() so the arguments can be printed directly without having to reference .args. One may also instantiate an exception first before raising it and add any attributes to it as desired.


>>> try:
...     raise Exception('spam', 'eggs')
... except Exception as inst:
...     print(type(inst))    # the exception instance
...     print(inst.args)     # arguments stored in .args
...     print(inst)          # __str__ allows args to be printed directly,
...                          # but may be overridden in exception subclasses
...     x, y = inst.args     # unpack args
...     print('x =', x)
...     print('y =', y)
...
<class 'Exception'>
('spam', 'eggs')
('spam', 'eggs')
x = spam
y = eggs


If an exception has arguments, they are printed as the last part (‘detail’) of the message for unhandled exceptions.

Exception handlers don’t just handle exceptions if they occur immediately in the try clause, but also if they occur inside functions that are called (even indirectly) in the try clause. For example:


>>> def this_fails():
...     x = 1/0
...
>>> try:
...     this_fails()
... except ZeroDivisionError as err:
...     print('Handling run-time error:', err)
...
Handling run-time error: division by zero



### ===🗝 8.4. Raising Exceptions

The `raise` statement allows the programmer to force a specified exception to occur. For example:


>>> raise NameError('HiThere')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: HiThere


The sole argument to `raise` indicates the exception to be raised. This must be either an exception instance or an exception class (a class that derives from Exception). If an exception class is passed, it will be implicitly instantiated by calling its constructor with no arguments:


    raise ValueError  # shorthand for 'raise ValueError()'


If you need to determine whether an exception was raised but don’t intend to handle it, a simpler form of the `raise` statement allows you to re-raise the exception:


>>> try:
...     raise NameError('HiThere')
... except NameError:
...     print('An exception flew by!')
...     raise
...
An exception flew by!
Traceback (most recent call last):
  File "<stdin>", line 2, in <module>
NameError: HiThere



### ===🗝 8.5. Exception Chaining

The `raise` statement allows an optional from which enables chaining exceptions. For example:


>>># exc must be exception instance or None.
raise RuntimeError from exc


This can be useful when you are transforming exceptions. For example:


>>> def func():
...     raise IOError
...
>>> try:
...     func()
... except IOError as exc:
...     raise RuntimeError('Failed to open database') from exc
...
Traceback (most recent call last):
  File "<stdin>", line 2, in <module>
  File "<stdin>", line 2, in func
OSError

The above exception was the direct cause of the following exception:

>>>
Traceback (most recent call last):
  File "<stdin>", line 4, in <module>
RuntimeError: Failed to open database


Exception chaining happens automatically when an exception is raised inside an `except` or `finally` section. Exception chaining can be disabled by using from None idiom:


>>> try:
...     open('database.sqlite')
... except IOError:
...     raise RuntimeError from None
...
Traceback (most recent call last):
  File "<stdin>", line 4, in <module>
RuntimeError


For more information about chaining mechanics, see Built-in Exceptions.


### ===🗝 8.6. User-defined Exceptions

Programs may name their own exceptions by creating a new exception class (see Classes for more about Python classes). Exceptions should typically be derived from the Exception class, either directly or indirectly.

Exception classes can be defined which do anything any other class can do, but are usually kept simple, often only offering a number of attributes that allow information about the error to be extracted by handlers for the exception. When creating a module that can raise several distinct errors, a common practice is to create a base class for exceptions defined by that module, and subclass that to create specific exception classes for different error conditions:

```py
class Error(Exception):
    """Base class for exceptions in this module."""
    pass

class InputError(Error):
    """Exception raised for errors in the input.

    Attributes:
        expression -- input expression in which the error occurred
        message -- explanation of the error
    """

    def __init__(self, expression, message):
        self.expression = expression
        self.message = message

class TransitionError(Error):
    """Raised when an operation attempts a state transition that's not
    allowed.

    Attributes:
        previous -- state at beginning of transition
        next -- attempted new state
        message -- explanation of why the specific transition is not allowed
    """

    def __init__(self, previous, next, message):
        self.previous = previous
        self.next = next
        self.message = message
```


Most exceptions are defined with names that end in “Error”, similar to the naming of the standard exceptions.

Many standard modules define their own exceptions to report errors that may occur in functions they define. More information on classes is presented in chapter Classes.


### ===🗝 8.7. Defining Clean-up Actions

The `try` statement has another optional clause which is intended to define clean-up actions that must be executed under all circumstances. For example:


>>> try:
...     raise KeyboardInterrupt
... finally:
...     print('Goodbye, world!')
...
Goodbye, world!
KeyboardInterrupt
Traceback (most recent call last):
  File "<stdin>", line 2, in <module>


If a `finally` clause is present, the `finally` clause will execute as the last task before the `try` statement completes. The `finally` clause runs whether or not the `try` statement produces an exception. The following points discuss more complex cases when an exception occurs:
↪• If an exception occurs during execution of the `try` clause, the exception may be handled by an except clause. If the exception is not handled by an except clause, the exception is re-raised after the `finally` clause has been executed.
↪• An exception could occur during execution of an except or else clause. Again, the exception is re-raised after the `finally` clause has been executed.
↪• If the `try` statement reaches a break, continue or return statement, the `finally` clause will execute just prior to the break, continue or return statement’s execution.
↪• If a `finally` clause includes a return statement, the returned value will be the one from the `finally` clause’s return statement, not the value from the `try` clause’s return statement.

For example:


>>> def bool_return():
...     try:
...         return True
...     finally:
...         return False
...
>>> bool_return()
False


A more complicated example:


>>> def divide(x, y):
...     try:
...         result = x / y
...     except ZeroDivisionError:
...         print("division by zero!")
...     else:
...         print("result is", result)
...     finally:
...         print("executing finally clause")
...
>>> divide(2, 1)
result is 2.0
executing finally clause
>>> divide(2, 0)
division by zero!
executing finally clause
>>> divide("2", "1")
executing finally clause
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 3, in divide
TypeError: unsupported operand type(s) for /: 'str' and 'str'


As you can see, the `finally` clause is executed in any event. The `TypeError` raised by dividing two strings is not handled by the `except` clause and therefore re-raised after the `finally` clause has been executed.

In real world applications, the `finally` clause is useful for releasing external resources (such as files or network connections), regardless of whether the use of the resource was successful.


### ===🗝 8.8. Predefined Clean-up Actions

Some objects define standard clean-up actions to be undertaken when the object is no longer needed, regardless of whether or not the operation using the object succeeded or failed. Look at the following example, which tries to open a file and print its contents to the screen.

```py
for line in open("myfile.txt"):
    print(line, end="")
```

The problem with this code is that it leaves the file open for an indeterminate amount of time after this part of the code has finished executing. This is not an issue in simple scripts, but can be a problem for larger applications. The with statement allows objects like files to be used in a way that ensures they are always cleaned up promptly and correctly.

```py
with open("myfile.txt") as f:
    for line in f:
        print(line, end="")
```

After the statement is executed, the file f is always closed, even if a problem was encountered while processing the lines. Objects which, like files, provide predefined clean-up actions will indicate this in their documentation.



## ==⚡ 9. Classes


Classes provide a means of bundling data and functionality together. Creating a new class creates a new type of object, allowing new instances of that type to be made. Each class instance can have attributes attached to it for maintaining its state. Class instances can also have methods (defined by its class) for modifying its state.

Compared with other programming languages, Python’s class mechanism adds classes with a minimum of new syntax and semantics. It is a mixture of the class mechanisms found in C++ and Modula-3. Python classes provide all the standard features of Object Oriented Programming: the class inheritance mechanism allows multiple base classes, a derived class can override any methods of its base class or classes, and a method can call the method of a base class with the same name. Objects can contain arbitrary amounts and kinds of data. As is true for modules, classes partake of the dynamic nature of Python: they are created at runtime, and can be modified further after creation.

In C++ terminology, normally class members (including the data members) are public (except see below Private Variables), and all member functions are virtual. As in Modula-3, there are no shorthands for referencing the object’s members from its methods: the method function is declared with an explicit first argument representing the object, which is provided implicitly by the call. As in Smalltalk, classes themselves are objects. This provides semantics for importing and renaming. Unlike C++ and Modula-3, built-in types can be used as base classes for extension by the user. Also, like in C++, most built-in operators with special syntax (arithmetic operators, subscripting etc.) can be redefined for class instances.

(Lacking universally accepted terminology to talk about classes, I will make occasional use of Smalltalk and C++ terms. I would use Modula-3 terms, since its object-oriented semantics are closer to those of Python than C++, but I expect that few readers have heard of it.)


### ===🗝 9.1. A Word About Names and Objects

Objects have individuality, and multiple names (in multiple scopes) can be bound to the same object. This is known as aliasing in other languages. This is usually not appreciated on a first glance at Python, and can be safely ignored when dealing with immutable basic types (numbers, strings, tuples). However, aliasing has a possibly surprising effect on the semantics of Python code involving mutable objects such as lists, dictionaries, and most other types. This is usually used to the benefit of the program, since aliases behave like pointers in some respects. For example, passing an object is cheap since only a pointer is passed by the implementation; and if a function modifies an object passed as an argument, the caller will see the change — this eliminates the need for two different argument passing mechanisms as in Pascal.


### ===🗝 9.2. Python Scopes and Namespaces

Before introducing classes, I first have to tell you something about Python’s scope rules. Class definitions play some neat tricks with namespaces, and you need to know how scopes and namespaces work to fully understand what’s going on. Incidentally, knowledge about this subject is useful for any advanced Python programmer.

Let’s begin with some definitions.

A `namespace` is a mapping from names to objects. Most namespaces are currently implemented as Python dictionaries, but that’s normally not noticeable in any way (except for performance), and it may change in the future. Examples of namespaces are: the set of built-in names (containing functions such as `abs()`, and built-in exception names); the `global` names in a module; and the `local` names in a function invocation. In a sense the set of attributes of an object also form a namespace. The important thing to know about namespaces is that there is absolutely no relation between names in different namespaces; for instance, two different modules may both define a function maximize without confusion — users of the modules must prefix it with the module name.

By the way, I use the word attribute for any name following a dot — for example, in the expression `z.real`, real is an attribute of the object z. Strictly speaking, references to names in modules are attribute references: in the expression `modname.funcname`, modname is a module object and funcname is an attribute of it. In this case there happens to be a straightforward mapping between the module’s attributes and the `global` names defined in the module: they share the same namespace! [1]

Attributes may be read-only or writable. In the latter case, assignment to attributes is possible. Module attributes are writable: you can write `modname.the_answer = 42`. Writable attributes may also be deleted with the del statement. For example, `del modname.the_answer` will remove the attribute the_answer from the object named by modname.

Namespaces are created at different moments and have different lifetimes. The namespace containing the built-in names is created when the Python interpreter starts up, and is never deleted. The `global` namespace for a module is created when the module definition is read in; normally, module namespaces also last until the interpreter quits. The statements executed by the top-level invocation of the interpreter, either read from a script file or interactively, are considered part of a module called __main__, so they have their own `global` namespace. (The built-in names actually also live in a module; this is called builtins.)

The `local` namespace for a function is created when the function is called, and deleted when the function returns or raises an exception that is not handled within the function. (Actually, forgetting would be a better way to describe what actually happens.) Of course, recursive invocations each have their own local namespace.

A `scope` is a textual region of a Python program where a namespace is directly accessible. “Directly accessible” here means that an unqualified reference to a name attempts to find the name in the namespace.

Although scopes are determined statically, they are used dynamically. At any time during execution, there are 3 or 4 nested scopes whose namespaces are directly accessible:

↪• the innermost scope, which is searched first, contains the `local names`
↪• the scopes of any enclosing functions, which are searched starting with the nearest enclosing scope, contains `non-local`, but also `non-global` names
↪• the next-to-last scope contains the current `module’s global names`
↪• the outermost scope (searched last) is the namespace containing `built-in names`

If a name is declared `global`, then all references and assignments go directly to the middle scope containing the module’s global names. To rebind variables found outside of the innermost scope, the `nonlocal` statement can be used; if not declared nonlocal, those variables are read-only (an attempt to write to such a variable will simply create a new local variable in the innermost scope, leaving the identically named outer variable unchanged).

Usually, the local scope references the local names of the (textually) current function. Outside functions, the local scope references the same namespace as the global scope: the module’s namespace. Class definitions place yet another namespace in the local scope.

It is important to realize that scopes are determined textually: the `global scope` of a function defined in a module is that module’s namespace, no matter from where or by what alias the function is called. On the other hand, the actual search for names is done dynamically, at run time — however, the language definition is evolving towards static name resolution, at “compile” time, so don’t rely on dynamic name resolution! (In fact, local variables are already determined statically.)

A special quirk of Python is that – if no `global` or `nonlocal` statement is in effect – assignments to names always go into the innermost scope. Assignments do not copy data — they just bind names to objects. The same is true for deletions: the statement `del x` removes the binding of x from the namespace referenced by the `local scope`. In fact, all operations that introduce new names use the `local scope`: in particular, `import` statements and function definitions bind the module or function name in the `local scope`.

The `global` statement can be used to indicate that particular variables live in the `global scope` and should be rebound there; the `nonlocal` statement indicates that particular variables live in an enclosing scope and should be rebound there.


### ===🗝 9.2.1. Scopes and Namespaces Example

This is an example demonstrating how to reference the different scopes and namespaces, and how `global` and `nonlocal` affect variable binding:


```py
def scope_test():
    def do_local():
        spam = "local spam"

    def do_nonlocal():
        nonlocal spam
        spam = "nonlocal spam"

    def do_global():
        global spam
        spam = "global spam"

    spam = "test spam"
    do_local()
    print("After local assignment:", spam)
    do_nonlocal()
    print("After nonlocal assignment:", spam)
    do_global()
    print("After global assignment:", spam)

scope_test()
print("In global scope:", spam)
```


The output of the example code is:


    After local assignment: test spam
    After nonlocal assignment: nonlocal spam
    After global assignment: nonlocal spam
    In global scope: global spam


Note how the `local` assignment (which is default) didn’t change scope_test’s binding of `spam`. The `nonlocal` assignment changed scope_test’s binding of spam, and the `global` assignment changed the module-level binding.

You can also see that there was no previous binding for spam before the `global` assignment.


### ===🗝 9.3. A First Look at Classes

Classes introduce a little bit of new syntax, three new object types, and some new semantics.


### ===🗝 9.3.1. Class Definition Syntax

The simplest form of class definition looks like this:


    class ClassName:
        <statement-1>
        .
        .
        .
        <statement-N>


Class definitions, like function definitions (`def` statements) must be executed before they have any effect. (You could conceivably place a class definition in a branch of an if statement, or inside a function.)

In practice, the statements inside a class definition will usually be function definitions, but other statements are allowed, and sometimes useful — we’ll come back to this later. The function definitions inside a class normally have a peculiar form of argument list, dictated by the calling conventions for methods — again, this is explained later.

When a class definition is entered, a new namespace is created, and used as the `local scope` — thus, all assignments to local variables go into this new namespace. In particular, function definitions bind the name of the new function here.

When a class definition is left normally (via the end), a class object is created. This is basically a wrapper around the contents of the namespace created by the class definition; we’ll learn more about class objects in the next section. The original local scope (the one in effect just before the class definition was entered) is reinstated, and the class object is bound here to the class name given in the class definition header (`ClassName` in the example).


### ===🗝 9.3.2. Class Objects

Class objects support two kinds of operations: `attribute references` and `instantiation`.

Attribute references use the standard syntax used for all attribute references in Python: obj.name. Valid attribute names are all the names that were in the class’s namespace when the class object was created. So, if the class definition looked like this:


```py
class MyClass:
    """A simple example class"""
    i = 12345

    def f(self):
        return 'hello world'
```


then `MyClass.i` and `MyClass.f` are valid attribute references, returning an integer and a function object, respectively. Class attributes can also be assigned to, so you can change the value of `MyClass.i` by assignment. __doc__ is also a valid attribute, returning the docstring belonging to the class: `"A simple example class"`.

Class instantiation uses function notation. Just pretend that the class object is a parameterless function that returns a new instance of the class. For example (assuming the above class):


    x = MyClass()


creates a new instance of the class and assigns this object to the local variable x.

The instantiation operation (“calling” a class object) creates an empty object. Many classes like to create objects with instances customized to a specific initial state. Therefore a class may define a special method named __init__(), like this:


```py
def __init__(self):
    self.data = []
```


When a class defines an __init__() method, class instantiation automatically invokes __init__() for the newly-created class instance. So in this example, a new, initialized instance can be obtained by:


    x = MyClass()


Of course, the __init__() method may have arguments for greater flexibility. In that case, arguments given to the class instantiation operator are passed on to __init__(). For example,


>>> class Complex:
...     def __init__(self, realpart, imagpart):
...         self.r = realpart
...         self.i = imagpart
...
>>> x = Complex(3.0, -4.5)
>>> x.r, x.i
(3.0, -4.5)



### ===🗝 9.3.3. Instance Objects

Now what can we do with instance objects? The only operations understood by instance objects are attribute references. There are two kinds of valid attribute names: data attributes and methods.

`data attributes` correspond to “`instance variables`” in Smalltalk, and to “`data members`” in C++. Data attributes need not be declared; like local variables, they spring into existence when they are first assigned to. For example, if `x` is the instance of `MyClass` created above, the following piece of code will print the value 16, without leaving a trace:

```py
x.counter = 1
while x.counter < 10:
    x.counter = x.counter * 2
print(x.counter)
del x.counter
```

The other kind of instance attribute reference is a `method`. A `method` is a function that “belongs to” an object. (In Python, the term method is not unique to class instances: other object types can have methods as well. For example, list objects have methods called append, insert, remove, sort, and so on. However, in the following discussion, we’ll use the term method exclusively to mean methods of class instance objects, unless explicitly stated otherwise.)

Valid method names of an instance object depend on its class. By definition, all attributes of a class that are function objects define corresponding methods of its instances. So in our example, `x.f` is a valid method reference, since `MyClass.f` is a function, but `x.i` is not, since` MyClass.i` is not. But `x.f` is not the same thing as `MyClass.f` — it is a `method object`, not a `function object`.


### ===🗝 9.3.4. Method Objects

Usually, a method is called right after it is bound:


    x.f()


In the `MyClass` example, this will return the string 'hello world'. However, it is not necessary to call a method right away: `x.f `is a method object, and can be stored away and called at a later time. For example:


    xf = x.f
    while True:
        print(xf())


will continue to print hello world until the end of time.

What exactly happens when a method is called? You may have noticed that `x.f()` was called without an argument above, even though the function definition for `f()` specified an argument. What happened to the argument? Surely Python raises an exception when a function that requires an argument is called without any — even if the argument isn’t actually used…

Actually, you may have guessed the answer: the special thing about methods is that the `instance object` is passed as the `first argument` of the function. In our example, the call `x.f()` is exactly equivalent to `MyClass.f(x)`. In general, calling a method with a list of n arguments is equivalent to calling the corresponding function with an argument list that is created by inserting the method’s instance object before the first argument.

If you still don’t understand how methods work, a look at the implementation can perhaps clarify matters. When a non-data attribute of an instance is referenced, the instance’s class is searched. If the name denotes a valid class attribute that is a function object, a method object is created by packing (pointers to) the instance object and the function object just found together in an abstract object: this is the method object. When the method object is called with an argument list, a new argument list is constructed from the instance object and the argument list, and the function object is called with this new argument list.


### ===🗝 9.3.5. Class and Instance Variables

Generally speaking, `instance variables` are for data unique to each instance and `class variables` are for attributes and methods shared by all instances of the class:


```py
class Dog:

    kind = 'canine'         # class variable shared by all instances

    def __init__(self, name):
        self.name = name    # instance variable unique to each instance

>>> d = Dog('Fido')
>>> e = Dog('Buddy')
>>> d.kind                  # shared by all dogs
'canine'
>>> e.kind                  # shared by all dogs
'canine'
>>> d.name                  # unique to d
'Fido'
>>> e.name                  # unique to e
'Buddy'
```


As discussed in A Word About `Names` and `Objects`, shared data can have possibly surprising effects with involving mutable objects such as lists and dictionaries. For example, the tricks list in the following code should not be used as a class variable because just a single list would be shared by all Dog instances:


```py
class Dog:

    tricks = []             # mistaken use of a class variable

    def __init__(self, name):
        self.name = name

    def add_trick(self, trick):
        self.tricks.append(trick)

>>> d = Dog('Fido')
>>> e = Dog('Buddy')
>>> d.add_trick('roll over')
>>> e.add_trick('play dead')
>>> d.tricks                # unexpectedly shared by all dogs
['roll over', 'play dead']
```


Correct design of the class should use an instance variable instead:


```py
class Dog:

    def __init__(self, name):
        self.name = name
        self.tricks = []    # creates a new empty list for each dog

    def add_trick(self, trick):
        self.tricks.append(trick)

>>> d = Dog('Fido')
>>> e = Dog('Buddy')
>>> d.add_trick('roll over')
>>> e.add_trick('play dead')
>>> d.tricks
['roll over']
>>> e.tricks
['play dead']
```



### ===🗝 9.4. Random Remarks

If the same attribute name occurs in both an instance and in a class, then attribute lookup prioritizes the instance:


```py
class Warehouse:
    purpose = 'storage'
    region = 'west'

>>> w1 = Warehouse()
>>> print(w1.purpose, w1.region)
storage west
>>> w2 = Warehouse()
>>> w2.region = 'east'
>>> print(w2.purpose, w2.region)
storage east
```


`Data attributes` may be referenced by methods as well as by ordinary users (“clients”) of an object. In other words, classes are not usable to implement pure abstract data types. In fact, nothing in Python makes it possible to enforce data hiding — it is all based upon convention. (On the other hand, the Python implementation, written in C, can completely hide implementation details and control access to an object if necessary; this can be used by extensions to Python written in C.)

Clients should use data attributes with care — clients may mess up invariants maintained by the methods by stamping on their data attributes. Note that clients may add data attributes of their own to an instance object without affecting the validity of the methods, as long as name conflicts are avoided — again, a naming convention can save a lot of headaches here.

There is no shorthand for referencing data attributes (or other methods!) from within methods. I find that this actually increases the readability of methods: there is no chance of confusing `local variables` and `instance variables` when glancing through a method.

Often, the first argument of a method is called `self`. This is nothing more than a convention: the name self has absolutely no special meaning to Python. Note, however, that by not following the convention your code may be less readable to other Python programmers, and it is also conceivable that a class browser program might be written that relies upon such a convention.

Any function object that is a class attribute defines a method for instances of that class. It is not necessary that the function definition is textually enclosed in the class definition: assigning a function object to a local variable in the class is also ok. For example:


```py
# Function defined outside the class
def f1(self, x, y):
    return min(x, x+y)

class C:
    f = f1

    def g(self):
        return 'hello world'

    h = g
```

Now `f`, `g` and `h` are all attributes of class `C` that refer to function objects, and consequently they are all methods of instances of C — `h` being exactly equivalent to `g`. Note that this practice usually only serves to confuse the reader of a program.

Methods may call other methods by using method attributes of the self argument:


```py
class Bag:
    def __init__(self):
        self.data = []

    def add(self, x):
        self.data.append(x)

    def addtwice(self, x):
        self.add(x)
        self.add(x)
```


Methods may reference `global` names in the same way as ordinary functions. The `global` scope associated with a method is the module containing its definition. (A class is never used as a `global scope`.) While one rarely encounters a good reason for using global data in a method, there are many legitimate uses of the `global scope`: for one thing, functions and modules imported into the `global scope` can be used by methods, as well as functions and classes defined in it. Usually, the class containing the method is itself defined in this `global scope`, and in the next section we’ll find some good reasons why a method would want to reference its own class.

Each value is an object, and therefore has a `class` (also called its `type`). It is stored as `object.__class__`.


### ===🗝 9.5. Inheritance

Of course, a language feature would not be worthy of the name “class” without supporting inheritance. The syntax for a derived class definition looks like this:


    class DerivedClassName(BaseClassName):
        <statement-1>
        .
        .
        .
        <statement-N>


The name `BaseClassName` must be defined in a scope containing the derived class definition. In place of a base class name, other arbitrary expressions are also allowed. This can be useful, for example, when the base class is defined in another module:


    class DerivedClassName(modname.BaseClassName):


Execution of a derived class definition proceeds the same as for a base class. When the class object is constructed, the base class is remembered. This is used for resolving attribute references: if a requested attribute is not found in the class, the search proceeds to look in the base class. This rule is applied recursively if the base class itself is derived from some other class.

There’s nothing special about instantiation of derived classes: `DerivedClassName()` creates a new instance of the class. Method references are resolved as follows: the corresponding class attribute is searched, descending down the chain of base classes if necessary, and the method reference is valid if this yields a function object.

Derived classes may override methods of their base classes. Because methods have no special privileges when calling other methods of the same object, a method of a base class that calls another method defined in the same base class may end up calling a method of a derived class that overrides it. (For C++ programmers: all methods in Python are effectively `virtual`.)

An overriding method in a derived class may in fact want to extend rather than simply replace the base class method of the same name. There is a simple way to call the base class method directly: just call `BaseClassName.methodname(self, arguments)`. This is occasionally useful to clients as well. (Note that this only works if the base class is accessible as `BaseClassName` in the `global scope`.)

Python has two built-in functions that work with inheritance:
↪• Use `isinstance()` to check an instance’s type: `isinstance(obj, int)` will be True only if obj.__class__ is int or some class derived from int.
↪• Use `issubclass()` to check class inheritance: `issubclass(bool, int)` is True since bool is a subclass of int. However, `issubclass(float, int)` is False since `float` is not a subclass of `int`.


### ===🗝 9.5.1. Multiple Inheritance

Python supports a form of multiple inheritance as well. A class definition with multiple base classes looks like this:


    class DerivedClassName(Base1, Base2, Base3):
        <statement-1>
        .
        .
        .
        <statement-N>


For most purposes, in the simplest cases, you can think of the search for attributes inherited from a parent class as `depth-first`, `left-to-right`, `not searching twice` in the same class where there is an overlap in the hierarchy. Thus, if an attribute is not found in `DerivedClassName`, it is searched for in `Base1`, then (recursively) in the base classes of `Base1`, and if it was not found there, it was searched for in `Base2`, and so on.

In fact, it is slightly more complex than that; the method resolution order changes dynamically to support cooperative calls to `super()`. This approach is known in some other multiple-inheritance languages as call-next-method and is more powerful than the super call found in single-inheritance languages.

Dynamic ordering is necessary because all cases of multiple inheritance exhibit one or more diamond relationships (where at least one of the parent classes can be accessed through multiple paths from the bottommost class). For example, all classes inherit from object, so any case of multiple inheritance provides more than one path to reach object. To keep the base classes from being accessed more than once, the dynamic algorithm linearizes the search order in a way that preserves the left-to-right ordering specified in each class, that calls each parent only once, and that is monotonic (meaning that a class can be subclassed without affecting the precedence order of its parents). Taken together, these properties make it possible to design reliable and extensible classes with multiple inheritance. For more detail, see https://www.python.org/download/releases/2.3/mro/.


### ===🗝 9.6. Private Variables

“Private” instance variables that cannot be accessed except from inside an object don’t exist in Python. However, there is a convention that is followed by most Python code: a name prefixed with an underscore (e.g. `_spam`) should be treated as a `non-public` part of the API (whether it is a function, a method or a data member). It should be considered an implementation detail and subject to change without notice.

Since there is a valid use-case for class-private members (namely to avoid name clashes of names with names defined by subclasses), there is limited support for such a mechanism, called name mangling. Any identifier of the form `__spam` (at least two leading underscores, at most one trailing underscore) is textually replaced with `_classname__spam`, where classname is the current class name with leading underscore(s) stripped. This mangling is done without regard to the syntactic position of the identifier, as long as it occurs within the definition of a class.

`Name mangling` is helpful for letting subclasses override methods without breaking intraclass method calls. For example:

```py
class Mapping:
    def __init__(self, iterable):
        self.items_list = []
        self.__update(iterable)

    def update(self, iterable):
        for item in iterable:
            self.items_list.append(item)

    __update = update   # private copy of original update() method

class MappingSubclass(Mapping):

    def update(self, keys, values):
        # provides new signature for update()
        # but does not break __init__()
        for item in zip(keys, values):
            self.items_list.append(item)
```


The above example would work even if `MappingSubclass` were to introduce a `__update` identifier since it is replaced with `_Mapping__update` in the Mapping class and `_MappingSubclass__update` in the `MappingSubclass` class respectively.

Note that the mangling rules are designed mostly to avoid accidents; it still is possible to access or modify a variable that is considered private. This can even be useful in special circumstances, such as in the debugger.

Notice that code passed to `exec()` or `eval()` does not consider the classname of the invoking class to be the current class; this is similar to the effect of the global statement, the effect of which is likewise restricted to code that is byte-compiled together. The same restriction applies to `getattr()`, `setattr()` and `delattr()`, as well as when referencing __dict__ directly.


### ===🗝 9.7. Odds and Ends

Sometimes it is useful to have a data type similar to the Pascal “`record`” or C “`struct`”, bundling together a few named data items. An empty class definition will do nicely:


```py
class Employee:
    pass

john = Employee()  # Create an empty employee record

# Fill the fields of the record
john.name = 'John Doe'
john.dept = 'computer lab'
john.salary = 1000

```

A piece of Python code that expects a particular abstract data type can often be passed a class that emulates the methods of that data type instead. For instance, if you have a function that formats some data from a file object, you can define a class with methods `read()` and `readline()` that get the data from a string buffer instead, and pass it as an argument.

Instance method objects have attributes, too: m.__self__ is the instance object with the method m(), and m.__func__ is the function object corresponding to the method.


### ===🗝 9.8. Iterators

By now you have probably noticed that most container objects can be looped over using a for statement:


```py
for element in [1, 2, 3]:
    print(element)
for element in (1, 2, 3):
    print(element)
for key in {'one':1, 'two':2}:
    print(key)
for char in "123":
    print(char)
for line in open("myfile.txt"):
    print(line, end='')
```

This style of access is clear, concise, and convenient. The use of iterators pervades and unifies Python. Behind the scenes, the for statement calls `iter()` on the container object. The function returns an iterator object that defines the method __next__() which accesses elements in the container one at a time. When there are no more elements, __next__() raises a StopIteration exception which tells the for loop to terminate. You can call the __next__() method using the `next()` built-in function; this example shows how it all works:


>>> s = 'abc'
>>> it = iter(s)
>>> it
<iterator object at 0x00A1DB50>
>>> next(it)
'a'
>>> next(it)
'b'
>>> next(it)
'c'
>>> next(it)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
    next(it)
StopIteration


Having seen the mechanics behind the iterator protocol, it is easy to add iterator behavior to your classes. Define an __iter__() method which returns an object with a __next__() method. If the class defines __next__(), then __iter__() can just return self:


```py
class Reverse:
    """Iterator for looping over a sequence backwards."""
    def __init__(self, data):
        self.data = data
        self.index = len(data)

    def __iter__(self):
        return self

    def __next__(self):
        if self.index == 0:
            raise StopIteration
        self.index = self.index - 1
        return self.data[self.index]
```



>>> rev = Reverse('spam')
>>> iter(rev)
<__main__.Reverse object at 0x00A1DB50>
>>> for char in rev:
...     print(char)
...
m
a
p
s



### ===🗝 9.9. Generators

Generators are a simple and powerful tool for creating iterators. They are written like regular functions but use the `yield` statement whenever they want to return data. Each time `next()` is called on it, the generator resumes where it left off (it remembers all the data values and which statement was last executed). An example shows that generators can be trivially easy to create:


```py
def reverse(data):
    for index in range(len(data)-1, -1, -1):
        yield data[index]
```

>>> for char in reverse('golf'):
...     print(char)
...
f
l
o
g


Anything that can be done with generators can also be done with class-based iterators as described in the previous section. What makes generators so compact is that the __iter__() and __next__() methods are created automatically.

Another key feature is that the `local variables` and `execution state` are automatically saved between calls. This made the function easier to write and much more clear than an approach using instance variables like `self.index` and `self.data`.

In addition to automatic method creation and saving program state, when generators terminate, they automatically raise StopIteration. In combination, these features make it easy to create iterators with no more effort than writing a regular function.


### ===🗝 9.10. Generator Expressions

Some simple generators can be coded succinctly as expressions using a syntax similar to `list comprehensions` but with parentheses instead of square brackets. These expressions are designed for situations where the generator is used right away by an enclosing function. Generator expressions are more compact but less versatile than full generator definitions and tend to be more memory friendly than equivalent `list comprehensions`.

Examples:


>>> sum(i*i for i in range(10))                 # sum of squares
285

>>> xvec = [10, 20, 30]
>>> yvec = [7, 5, 3]
>>> sum(x*y for x,y in zip(xvec, yvec))         # dot product
260

>>> unique_words = set(word for line in page  for word in line.split())

>>> valedictorian = max((student.gpa, student.name) for student in graduates)

>>> data = 'golf'
>>> list(data[i] for i in range(len(data)-1, -1, -1))
['f', 'l', 'o', 'g']


Footnotes

[1] Except for one thing. Module objects have a secret read-only attribute called __dict__ which returns the dictionary used to implement the module’s namespace; the name __dict__ is an attribute but not a global name. Obviously, using this violates the abstraction of namespace implementation, and should be restricted to things like post-mortem debuggers. 



## ==⚡ 10. Brief Tour of the Standard Library
- Python 3.10.2 Documentation » The Python Tutorial

### ===🗝 10.1. Operating System Interface

The os module provides dozens of functions for interacting with the operating system:


>>> import os
>>> os.getcwd()      # Return the current working directory
'C:\\Python310'
>>> os.chdir('/server/accesslogs')   # Change current working directory
>>> os.system('mkdir today')   # Run the command mkdir in the system shell
0


Be sure to use the `import os` style instead of `from os import *`. This will keep `os.open()` from shadowing the built-in `open()` function which operates much differently.

The built-in `dir()` and `help()` functions are useful as interactive aids for working with large modules like os:


>>> import os
>>> dir(os)
<returns a list of all module functions>
>>> help(os)
<returns an extensive manual page created from the module's docstrings>


For daily file and directory management tasks, the `shutil` module provides a higher level interface that is easier to use:


>>> import shutil
>>> shutil.copyfile('data.db', 'archive.db')
'archive.db'
>>> shutil.move('/build/executables', 'installdir')
'installdir'



### ===🗝 10.2. File Wildcards

The glob module provides a function for making file lists from directory wildcard searches:


>>> import glob
>>> glob.glob('*.py')
['primes.py', 'random.py', 'quote.py']



### ===🗝 10.3. Command Line Arguments

Common utility scripts often need to process command line arguments. These arguments are stored in the sys module’s argv attribute as a list. For instance the following output results from running python demo.py one two three at the command line:


>>> import sys
>>> print(sys.argv)
['demo.py', 'one', 'two', 'three']


The argparse module provides a more sophisticated mechanism to process command line arguments. The following script extracts one or more filenames and an optional number of lines to be displayed:


```py
import argparse

parser = argparse.ArgumentParser(prog = 'top',
    description = 'Show top lines from each file')
parser.add_argument('filenames', nargs='+')
parser.add_argument('-l', '--lines', type=int, default=10)
args = parser.parse_args()
print(args)
```

When run at the command line with python top.py --lines=5 alpha.txt beta.txt, the script sets args.lines to 5 and args.filenames to ['alpha.txt', 'beta.txt'].


### ===🗝 10.4. Error Output Redirection and Program Termination

The sys module also has attributes for stdin, stdout, and stderr. The latter is useful for emitting warnings and error messages to make them visible even when stdout has been redirected:


>>> sys.stderr.write('Warning, log file not found starting a new one\n')
Warning, log file not found starting a new one


The most direct way to terminate a script is to use `sys.exit()`.


### ===🗝 10.5. String Pattern Matching

The `re` module provides regular expression tools for advanced string processing. For complex matching and manipulation, regular expressions offer succinct, optimized solutions:


>>> import re
>>> re.findall(r'\bf[a-z]*', 'which foot or hand fell fastest')
['foot', 'fell', 'fastest']
>>> re.sub(r'(\b[a-z]+) \1', r'\1', 'cat in the the hat')
'cat in the hat'


When only simple capabilities are needed, string methods are preferred because they are easier to read and debug:


>>> 'tea for too'.replace('too', 'two')
'tea for two'



### ===🗝 10.6. Mathematics

The `math` module gives access to the underlying C library functions for floating point math:


>>> import math
>>> math.cos(math.pi / 4)
0.70710678118654757
>>> math.log(1024, 2)
10.0


The `random` module provides tools for making random selections:


>>> import random
>>> random.choice(['apple', 'pear', 'banana'])
'apple'
>>> random.sample(range(100), 10)   # sampling without replacement
[30, 83, 16, 4, 8, 81, 41, 50, 18, 33]
>>> random.random()    # random float
0.17970987693706186
>>> random.randrange(6)    # random integer chosen from range(6)
4


The `statistics` module calculates basic statistical properties (the mean, median, variance, etc.) of numeric data:


>>> import statistics
>>> data = [2.75, 1.75, 1.25, 0.25, 0.5, 1.25, 3.5]
>>> statistics.mean(data)
1.6071428571428572
>>> statistics.median(data)
1.25
>>> statistics.variance(data)
1.3720238095238095


The SciPy project <https://scipy.org> has many other modules for numerical computations.


### ===🗝 10.7. Internet Access

There are a number of modules for accessing the internet and processing internet protocols. Two of the simplest are `urllib.request` for retrieving data from URLs and `smtplib` for sending mail:


>>> from urllib.request import urlopen
>>> with urlopen('http://worldtimeapi.org/api/timezone/etc/UTC.txt') as response:
...     for line in response:
...         line = line.decode()             # Convert bytes to a str
...         if line.startswith('datetime'):
...             print(line.rstrip())         # Remove trailing newline
...
datetime: 2022-01-01T01:36:47.689215+00:00

>>> import smtplib
>>> server = smtplib.SMTP('localhost')
>>> server.sendmail('soothsayer@example.org', 'jcaesar@example.org',
... """To: jcaesar@example.org
... From: soothsayer@example.org
...
... Beware the Ides of March.
... """)
>>> server.quit()


(Note that the second example needs a mailserver running on localhost.)


### ===🗝 10.8. Dates and Times

The `datetime` module supplies classes for manipulating dates and times in both simple and complex ways. While date and time arithmetic is supported, the focus of the implementation is on efficient member extraction for output formatting and manipulation. The module also supports objects that are timezone aware.


>>> # dates are easily constructed and formatted
>>> from datetime import date
>>> now = date.today()
>>> now
datetime.date(2003, 12, 2)
>>> now.strftime("%m-%d-%y. %d %b %Y is a %A on the %d day of %B.")
'12-02-03. 02 Dec 2003 is a Tuesday on the 02 day of December.'

>>> # dates support calendar arithmetic
>>> birthday = date(1964, 7, 31)
>>> age = now - birthday
>>> age.days
14368



### ===🗝 10.9. Data Compression

Common data archiving and compression formats are directly supported by modules including: `zlib`, `gzip`, `bz2`, `lzma`, `zipfile` and `tarfile`.


>>> import zlib
>>> s = b'witch which has which witches wrist watch'
>>> len(s)
41
>>> t = zlib.compress(s)
>>> len(t)
37
>>> zlib.decompress(t)
b'witch which has which witches wrist watch'
>>> zlib.crc32(s)
226805979



### ===🗝 10.10. Performance Measurement

Some Python users develop a deep interest in knowing the relative performance of different approaches to the same problem. Python provides a measurement tool that answers those questions immediately.

For example, it may be tempting to use the tuple packing and unpacking feature instead of the traditional approach to swapping arguments. The `timeit` module quickly demonstrates a modest performance advantage:


>>> from timeit import Timer
>>> Timer('t=a; a=b; b=t', 'a=1; b=2').timeit()
0.57535828626024577
>>> Timer('a,b = b,a', 'a=1; b=2').timeit()
0.54962537085770791


In contrast to timeit’s fine level of granularity, the `profile` and `pstats` modules provide tools for identifying time critical sections in larger blocks of code.


### ===🗝 10.11. Quality Control

One approach for developing high quality software is to write tests for each function as it is developed and to run those tests frequently during the development process.

The `doctest` module provides a tool for scanning a module and validating tests embedded in a program’s docstrings. Test construction is as simple as cutting-and-pasting a typical call along with its results into the docstring. This improves the documentation by providing the user with an example and it allows the doctest module to make sure the code remains true to the documentation:


```py
def average(values):
    """Computes the arithmetic mean of a list of numbers.

    >>> print(average([20, 30, 70]))
    40.0
    """
    return sum(values) / len(values)

import doctest
doctest.testmod()   # automatically validate the embedded tests
```


The `unittest` module is not as effortless as the `doctest` module, but it allows a more comprehensive set of tests to be maintained in a separate file:


```py
import unittest

class TestStatisticalFunctions(unittest.TestCase):

    def test_average(self):
        self.assertEqual(average([20, 30, 70]), 40.0)
        self.assertEqual(round(average([1, 5, 7]), 1), 4.3)
        with self.assertRaises(ZeroDivisionError):
            average([])
        with self.assertRaises(TypeError):
            average(20, 30, 70)

unittest.main()  # Calling from the command line invokes all tests
```


### ===🗝 10.12. Batteries Included

Python has a “batteries included” philosophy. This is best seen through the sophisticated and robust capabilities of its larger packages. For example:
↪• The `xmlrpc.client` and `xmlrpc.server` modules make implementing remote procedure calls into an almost trivial task. Despite the modules names, no direct knowledge or handling of XML is needed.
↪• The `email` package is a library for managing email messages, including MIME and other RFC 2822-based message documents. Unlike `smtplib` and `poplib` which actually send and receive messages, the `email` package has a complete toolset for building or decoding complex message structures (including attachments) and for implementing internet encoding and header protocols.
↪• The `json` package provides robust support for parsing this popular data interchange format. The `csv` module supports direct reading and writing of files in Comma-Separated Value format, commonly supported by databases and spreadsheets. XML processing is supported by the `xml.etree.ElementTree`, `xml.dom` and `xml.sax` packages. Together, these modules and packages greatly simplify data interchange between Python applications and other tools.
↪• The `sqlite3` module is a wrapper for the `SQLite` database library, providing a persistent database that can be updated and accessed using slightly nonstandard SQL syntax.
↪• Internationalization is supported by a number of modules including `gettext`, `locale`, and the `codecs` package.


## ==⚡ 11. Brief Tour of the Standard Library — Part II
- Python 3.10.2 Documentation » The Python Tutorial

This second tour covers more advanced modules that support professional programming needs. These modules rarely occur in small scripts.


### ===🗝 11.1. Output Formatting

The `reprlib` module provides a version of `repr()` customized for abbreviated displays of large or deeply nested containers:


>>> import reprlib
>>> reprlib.repr(set('supercalifragilisticexpialidocious'))
"{'a', 'c', 'd', 'e', 'f', 'g', ...}"


The `pprint` module offers more sophisticated control over printing both built-in and user defined objects in a way that is readable by the interpreter. When the result is longer than one line, the “pretty printer” adds line breaks and indentation to more clearly reveal data structure:


>>> import pprint
>>> t = [[[['black', 'cyan'], 'white', ['green', 'red']], [['magenta',
...     'yellow'], 'blue']]]
...
>>> pprint.pprint(t, width=30)
[[[['black', 'cyan'],
   'white',
   ['green', 'red']],
  [['magenta', 'yellow'],
   'blue']]]


The `textwrap` module formats paragraphs of text to fit a given screen width:


>>> import textwrap
>>> doc = """The wrap() method is just like fill() except that it returns
... a list of strings instead of one big string with newlines to separate
... the wrapped lines."""
...
>>> print(textwrap.fill(doc, width=40))
The wrap() method is just like fill()
except that it returns a list of strings
instead of one big string with newlines
to separate the wrapped lines.


The `locale` module accesses a database of culture specific data formats. The grouping attribute of locale’s format function provides a direct way of formatting numbers with group separators:


>>> import locale
>>> locale.setlocale(locale.LC_ALL, 'English_United States.1252')
'English_United States.1252'
>>> conv = locale.localeconv()          # get a mapping of conventions
>>> x = 1234567.8
>>> locale.format("%d", x, grouping=True)
'1,234,567'
>>> locale.format_string("%s%.*f", (conv['currency_symbol'],
...                      conv['frac_digits'], x), grouping=True)
'$1,234,567.80'



### ===🗝 11.2. Templating

The `string` module includes a versatile Template class with a simplified syntax suitable for editing by end-users. This allows users to customize their applications without having to alter the application.

The format uses placeholder names formed by `$` with valid Python identifiers (alphanumeric characters and underscores). Surrounding the placeholder with braces allows it to be followed by more alphanumeric letters with no intervening spaces. Writing `$$` creates a single escaped `$`:


>>> from string import Template
>>> t = Template('${village}folk send $$10 to $cause.')
>>> t.substitute(village='Nottingham', cause='the ditch fund')
'Nottinghamfolk send $10 to the ditch fund.'


The `substitute()` method raises a `KeyError` when a placeholder is not supplied in a dictionary or a keyword argument. For mail-merge style applications, user supplied data may be incomplete and the `safe_substitute()` method may be more appropriate — it will leave placeholders unchanged if data is missing:


>>> t = Template('Return the $item to $owner.')
>>> d = dict(item='unladen swallow')
>>> t.substitute(d)
Traceback (most recent call last):
  ...
KeyError: 'owner'
>>> t.safe_substitute(d)
'Return the unladen swallow to $owner.'


`Template` subclasses can specify a custom delimiter. For example, a batch renaming utility for a photo browser may elect to use percent signs for placeholders such as the current date, image sequence number, or file format:


>>> import time, os.path
>>> photofiles = ['img_1074.jpg', 'img_1076.jpg', 'img_1077.jpg']
>>> class BatchRename(Template):
...     delimiter = '%'
>>> fmt = input('Enter rename style (%d-date %n-seqnum %f-format):  ')
Enter rename style (%d-date %n-seqnum %f-format):  Ashley_%n%f

>>> t = BatchRename(fmt)
>>> date = time.strftime('%d%b%y')
>>> for i, filename in enumerate(photofiles):
...     base, ext = os.path.splitext(filename)
...     newname = t.substitute(d=date, n=i, f=ext)
...     print('{0} --> {1}'.format(filename, newname))

img_1074.jpg --> Ashley_0.jpg
img_1076.jpg --> Ashley_1.jpg
img_1077.jpg --> Ashley_2.jpg


Another application for templating is separating program logic from the details of multiple output formats. This makes it possible to substitute custom templates for XML files, plain text reports, and HTML web reports.


### ===🗝 11.3. Working with Binary Data Record Layouts

The `struct` module provides `pack()` and `unpack()` functions for working with variable length binary record formats. The following example shows how to loop through header information in a ZIP file without using the `zipfile` module. Pack codes "H" and "I" represent two and four byte unsigned numbers respectively. The `"<"` indicates that they are standard size and in little-endian byte order:


```py
import struct

with open('myfile.zip', 'rb') as f:
    data = f.read()

start = 0
for i in range(3):                      # show the first 3 file headers
    start += 14
    fields = struct.unpack('<IIIHH', data[start:start+16])
    crc32, comp_size, uncomp_size, filenamesize, extra_size = fields

    start += 16
    filename = data[start:start+filenamesize]
    start += filenamesize
    extra = data[start:start+extra_size]
    print(filename, hex(crc32), comp_size, uncomp_size)

    start += extra_size + comp_size     # skip to the next header
```



### ===🗝 11.4. Multi-threading

Threading is a technique for decoupling tasks which are not sequentially dependent. Threads can be used to improve the responsiveness of applications that accept user input while other tasks run in the background. A related use case is running I/O in parallel with computations in another thread.

The following code shows how the high level threading module can run tasks in background while the main program continues to run:


```py
import threading, zipfile

class AsyncZip(threading.Thread):
    def __init__(self, infile, outfile):
        threading.Thread.__init__(self)
        self.infile = infile
        self.outfile = outfile

    def run(self):
        f = zipfile.ZipFile(self.outfile, 'w', zipfile.ZIP_DEFLATED)
        f.write(self.infile)
        f.close()
        print('Finished background zip of:', self.infile)

background = AsyncZip('mydata.txt', 'myarchive.zip')
background.start()
print('The main program continues to run in foreground.')

background.join()    # Wait for the background task to finish
print('Main program waited until background was done.')
```


The principal challenge of multi-threaded applications is coordinating threads that share data or other resources. To that end, the threading module provides a number of synchronization primitives including locks, events, condition variables, and semaphores.

While those tools are powerful, minor design errors can result in problems that are difficult to reproduce. So, the preferred approach to task coordination is to concentrate all access to a resource in a single thread and then use the queue module to feed that thread with requests from other threads. Applications using Queue objects for inter-thread communication and coordination are easier to design, more readable, and more reliable.


### ===🗝 11.5. Logging

The `logging` module offers a full featured and flexible logging system. At its simplest, log messages are sent to a file or to `sys.stderr`:


```py
import logging
logging.debug('Debugging information')
logging.info('Informational message')
logging.warning('Warning:config file %s not found', 'server.conf')
logging.error('Error occurred')
logging.critical('Critical error -- shutting down')
```


This produces the following output:


WARNING:root:Warning:config file server.conf not found
ERROR:root:Error occurred
CRITICAL:root:Critical error -- shutting down


By default, informational and debugging messages are suppressed and the output is sent to standard error. Other output options include routing messages through email, datagrams, sockets, or to an HTTP Server. New filters can select different routing based on message priority: DEBUG, INFO, WARNING, ERROR, and CRITICAL.

The logging system can be configured directly from Python or can be loaded from a user editable configuration file for customized logging without altering the application.


### ===🗝 11.6. Weak References

Python does automatic memory management (reference counting for most objects and garbage collection to eliminate cycles). The memory is freed shortly after the last reference to it has been eliminated.

This approach works fine for most applications but occasionally there is a need to track objects only as long as they are being used by something else. Unfortunately, just tracking them creates a reference that makes them permanent. The `weakref` module provides tools for tracking objects without creating a reference. When the object is no longer needed, it is automatically removed from a `weakref` table and a callback is triggered for `weakref` objects. Typical applications include caching objects that are expensive to create:


>>> import weakref, gc
>>> class A:
...     def __init__(self, value):
...         self.value = value
...     def __repr__(self):
...         return str(self.value)
...
>>> a = A(10)                   # create a reference
>>> d = weakref.WeakValueDictionary()
>>> d['primary'] = a            # does not create a reference
>>> d['primary']                # fetch the object if it is still alive
10
>>> del a                       # remove the one reference
>>> gc.collect()                # run garbage collection right away
0
>>> d['primary']                # entry was automatically removed
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
    d['primary']                # entry was automatically removed
  File "C:/python310/lib/weakref.py", line 46, in __getitem__
    o = self.data[key]()
KeyError: 'primary'



### ===🗝 11.7. Tools for Working with Lists

Many data structure needs can be met with the built-in `list` type. However, sometimes there is a need for alternative implementations with different performance trade-offs.

The `array` module provides an `array()` object that is like a `list` that stores only homogeneous data and stores it more compactly. The following example shows an array of numbers stored as two byte unsigned binary numbers (typecode "H") rather than the usual 16 bytes per entry for regular lists of Python int objects:


>>> from array import array
>>> a = array('H', [4000, 10, 700, 22222])
>>> sum(a)
26932
>>> a[1:3]
array('H', [10, 700])


The `collections` module provides a `deque()` object that is like a list with faster appends and pops from the left side but slower lookups in the middle. These objects are well suited for implementing queues and breadth first tree searches:


>>> from collections import deque
>>> d = deque(["task1", "task2", "task3"])
>>> d.append("task4")
>>> print("Handling", d.popleft())
Handling task1



```py
unsearched = deque([starting_node])
def breadth_first_search(unsearched):
    node = unsearched.popleft()
    for m in gen_moves(node):
        if is_goal(m):
            return m
        unsearched.append(m)
```


In addition to alternative list implementations, the library also offers other tools such as the `bisect` module with functions for manipulating sorted lists:


>>> import bisect
>>> scores = [(100, 'perl'), (200, 'tcl'), (400, 'lua'), (500, 'python')]
>>> bisect.insort(scores, (300, 'ruby'))
>>> scores
[(100, 'perl'), (200, 'tcl'), (300, 'ruby'), (400, 'lua'), (500, 'python')]


The `heapq` module provides functions for implementing heaps based on regular lists. The lowest valued entry is always kept at position zero. This is useful for applications which repeatedly access the smallest element but do not want to run a full list sort:


>>> from heapq import heapify, heappop, heappush
>>> data = [1, 3, 5, 7, 9, 2, 4, 6, 8, 0]
>>> heapify(data)                      # rearrange the list into heap order
>>> heappush(data, -5)                 # add a new entry
>>> [heappop(data) for i in range(3)]  # fetch the three smallest entries
[-5, 0, 1]



### ===🗝 11.8. Decimal Floating Point Arithmetic

The `decimal` module offers a Decimal datatype for decimal floating point arithmetic. Compared to the built-in float implementation of binary floating point, the class is especially helpful for

• financial applications and other uses which require exact decimal representation,
• control over precision,
• control over rounding to meet legal or regulatory requirements,
• tracking of significant decimal places, or
• applications where the user expects the results to match calculations done by hand.

For example, calculating a 5% tax on a 70 cent phone charge gives different results in decimal floating point and binary floating point. The difference becomes significant if the results are rounded to the nearest cent:


>>> from decimal import *
>>> round(Decimal('0.70') * Decimal('1.05'), 2)
Decimal('0.74')
>>> round(.70 * 1.05, 2)
0.73


The Decimal result keeps a trailing zero, automatically inferring four place significance from multiplicands with two place significance. Decimal reproduces mathematics as done by hand and avoids issues that can arise when binary floating point cannot exactly represent decimal quantities.

Exact representation enables the Decimal class to perform modulo calculations and equality tests that are unsuitable for binary floating point:


>>> Decimal('1.00') % Decimal('.10')
Decimal('0.00')
>>> 1.00 % 0.10
0.09999999999999995

>>> sum([Decimal('0.1')]*10) == Decimal('1.0')
True
>>> sum([0.1]*10) == 1.0
False


The `decimal` module provides arithmetic with as much precision as needed:


>>> getcontext().prec = 36
>>> Decimal(1) / Decimal(7)
Decimal('0.142857142857142857142857142857142857')



## ==⚡ 12. Virtual Environments and Packages

### ===🗝 12.1. Introduction

Python applications will often use packages and modules that don’t come as part of the standard library. Applications will sometimes need a specific version of a library, because the application may require that a particular bug has been fixed or the application may be written using an obsolete version of the library’s interface.

This means it may not be possible for one Python installation to meet the requirements of every application. If application A needs version 1.0 of a particular module but application B needs version 2.0, then the requirements are in conflict and installing either version 1.0 or 2.0 will leave one application unable to run.

The solution for this problem is to create a virtual environment, a self-contained directory tree that contains a Python installation for a particular version of Python, plus a number of additional packages.

Different applications can then use different virtual environments. To resolve the earlier example of conflicting requirements, application A can have its own virtual environment with version 1.0 installed while application B has another virtual environment with version 2.0. If application B requires a library be upgraded to version 3.0, this will not affect application A’s environment.


### ===🗝 12.2. Creating Virtual Environments

The module used to create and manage virtual environments is called `venv`. `venv` will usually install the most recent version of Python that you have available. If you have multiple versions of Python on your system, you can select a specific Python version by running python3 or whichever version you want.

To create a virtual environment, decide upon a directory where you want to place it, and run the `venv` module as a script with the directory path:


    python3 -m venv tutorial-env


This will create the `tutorial-env` directory if it doesn’t exist, and also create directories inside it containing a copy of the Python interpreter and various supporting files.

A common directory location for a virtual environment is `.venv`. This name keeps the directory typically hidden in your shell and thus out of the way while giving it a name that explains why the directory exists. It also prevents clashing with `.env` environment variable definition files that some tooling supports.

Once you’ve created a virtual environment, you may activate it.

On Windows, run:


    tutorial-env\Scripts\activate.bat


On Unix or MacOS, run:


    source tutorial-env/bin/activate


(This script is written for the bash shell. If you use the csh or fish shells, there are alternate activate.csh and activate.fish scripts you should use instead.)

Activating the virtual environment will change your shell’s prompt to show what virtual environment you’re using, and modify the environment so that running python will get you that particular version and installation of Python. For example:


>$ source ~/envs/tutorial-env/bin/activate
(tutorial-env) $ python
Python 3.5.1 (default, May  6 2016, 10:59:36)
  ...
>>> import sys
>>> sys.path
['', '/usr/local/lib/python35.zip', ...,
'~/envs/tutorial-env/lib/python3.5/site-packages']
>>>



### ===🗝 12.3. Managing Packages with pip

You can install, upgrade, and remove packages using a program called pip. By default pip will install packages from the Python Package Index, <https://pypi.org>. You can browse the Python Package Index by going to it in your web browser.

`pip` has a number of subcommands: “install”, “uninstall”, “freeze”, etc. (Consult the Installing Python Modules guide for complete documentation for `pip`.)

You can install the latest version of a package by specifying a package’s name:


```sh
(tutorial-env) $ python -m pip install novas
Collecting novas
  Downloading novas-3.1.1.3.tar.gz (136kB)
Installing collected packages: novas
  Running setup.py install for novas
Successfully installed novas-3.1.1.3
```


You can also install a specific version of a package by giving the package name followed by == and the version number:


```sh
(tutorial-env) $ python -m pip install requests==2.6.0
Collecting requests==2.6.0
  Using cached requests-2.6.0-py2.py3-none-any.whl
Installing collected packages: requests
Successfully installed requests-2.6.0
```


If you re-run this command, pip will notice that the requested version is already installed and do nothing. You can supply a different version number to get that version, or you can run pip install --upgrade to upgrade the package to the latest version:


```sh
(tutorial-env) $ python -m pip install --upgrade requests
Collecting requests
Installing collected packages: requests
  Found existing installation: requests 2.6.0
    Uninstalling requests-2.6.0:
      Successfully uninstalled requests-2.6.0
Successfully installed requests-2.7.0
```


`pip uninstall` followed by one or more package names will remove the packages from the virtual environment.

pip show will display information about a particular package:


```sh
(tutorial-env) $ pip show requests
---
Metadata-Version: 2.0
Name: requests
Version: 2.7.0
Summary: Python HTTP for Humans.
Home-page: http://python-requests.org
Author: Kenneth Reitz
Author-email: me@kennethreitz.com
License: Apache 2.0
Location: /Users/akuchling/envs/tutorial-env/lib/python3.4/site-packages
Requires:
```


pip list will display all of the packages installed in the virtual environment:


```sh
(tutorial-env) $ pip list
novas (3.1.1.3)
numpy (1.9.2)
pip (7.0.3)
requests (2.7.0)
setuptools (16.0)
```


pip freeze will produce a similar list of the installed packages, but the output uses the format that pip install expects. A common convention is to put this list in a requirements.txt file:


```sh
(tutorial-env) $ pip freeze > requirements.txt
(tutorial-env) $ cat requirements.txt
novas==3.1.1.3
numpy==1.9.2
requests==2.7.0
```


The requirements.txt can then be committed to version control and shipped as part of an application. Users can then install all the necessary packages with install -r:


```sh
(tutorial-env) $ python -m pip install -r requirements.txt
Collecting novas==3.1.1.3 (from -r requirements.txt (line 1))
  ...
Collecting numpy==1.9.2 (from -r requirements.txt (line 2))
  ...
Collecting requests==2.7.0 (from -r requirements.txt (line 3))
  ...
Installing collected packages: novas, numpy, requests
  Running setup.py install for novas
Successfully installed novas-3.1.1.3 numpy-1.9.2 requests-2.7.0
```


`pip` has many more options. Consult the Installing Python Modules guide for complete documentation for `pip`. When you’ve written a package and want to make it available on the Python Package Index, consult the Distributing Python Modules guide.



## ==⚡ 13. What Now?

Reading this tutorial has probably reinforced your interest in using Python — you should be eager to apply Python to solving your real-world problems. Where should you go to learn more?

This tutorial is part of Python’s documentation set. Some other documents in the set are:

• The Python Standard Library:

You should browse through this manual, which gives complete (though terse) reference material about types, functions, and the modules in the standard library. The standard Python distribution includes a lot of additional code. There are modules to read Unix mailboxes, retrieve documents via HTTP, generate random numbers, parse command-line options, write CGI programs, compress data, and many other tasks. Skimming through the Library Reference will give you an idea of what’s available.


• Installing Python Modules explains how to install additional modules written by other Python users.


• The Python Language Reference: A detailed explanation of Python’s syntax and semantics. It’s heavy reading, but is useful as a complete guide to the language itself.


More Python resources:

• https://www.python.org: The major Python web site. It contains code, documentation, and pointers to Python-related pages around the web. This web site is mirrored in various places around the world, such as Europe, Japan, and Australia; a mirror may be faster than the main site, depending on your geographical location.
• https://docs.python.org: Fast access to Python’s documentation.
• https://pypi.org: The Python Package Index, previously also nicknamed the Cheese Shop [1], is an index of user-created Python modules that are available for download. Once you begin releasing code, you can register it here so that others can find it.
• https://code.activestate.com/recipes/langs/python/: The Python Cookbook is a sizable collection of code examples, larger modules, and useful scripts. Particularly notable contributions are collected in a book also titled Python Cookbook (O’Reilly & Associates, ISBN 0-596-00797-3.)
• http://www.pyvideo.org collects links to Python-related videos from conferences and user-group meetings.
• https://scipy.org: The Scientific Python project includes modules for fast array computations and manipulations plus a host of packages for such things as linear algebra, Fourier transforms, non-linear solvers, random number distributions, statistical analysis and the like.

For Python-related questions and problem reports, you can post to the newsgroup comp.lang.python, or send them to the mailing list at python-list@python.org. The newsgroup and mailing list are gatewayed, so messages posted to one will automatically be forwarded to the other. There are hundreds of postings a day, asking (and answering) questions, suggesting new features, and announcing new modules. Mailing list archives are available at https://mail.python.org/pipermail/.

Before posting, be sure to check the list of Frequently Asked Questions (also called the FAQ). The FAQ answers many of the questions that come up again and again, and may already contain the solution for your problem.

Footnotes


[1] “Cheese Shop” is a Monty Python’s sketch: a customer enters a cheese shop, but whatever cheese he asks for, the clerk says it’s missing. 


## ==⚡ 14. Interactive Input Editing and History Substitution

Some versions of the Python interpreter support editing of the current input line and history substitution, similar to facilities found in the Korn shell and the GNU Bash shell. This is implemented using the GNU Readline library, which supports various styles of editing. This library has its own documentation which we won’t duplicate here.


14.1. Tab Completion and History Editing

Completion of variable and module names is automatically enabled at interpreter startup so that the Tab key invokes the completion function; it looks at Python statement names, the current local variables, and the available module names. For dotted expressions such as string.a, it will evaluate the expression up to the final '.' and then suggest completions from the attributes of the resulting object. Note that this may execute application-defined code if an object with a __getattr__() method is part of the expression. The default configuration also saves your history into a file named .python_history in your user directory. The history will be available again during the next interactive interpreter session.


14.2. Alternatives to the Interactive Interpreter

This facility is an enormous step forward compared to earlier versions of the interpreter; however, some wishes are left: It would be nice if the proper indentation were suggested on continuation lines (the parser knows if an indent token is required next). The completion mechanism might use the interpreter’s symbol table. A command to check (or even suggest) matching parentheses, quotes, etc., would also be useful.

One alternative enhanced interactive interpreter that has been around for quite some time is IPython, which features tab completion, object exploration and advanced history management. It can also be thoroughly customized and embedded into other applications. Another similar enhanced interactive environment is bpython.


## ==⚡ 15. Floating Point Arithmetic: Issues and Limitations

Floating-point numbers are represented in computer hardware as base 2 (binary) fractions. For example, the decimal fraction


    0.125


has value 1/10 + 2/100 + 5/1000, and in the same way the binary fraction


    0.001


has value 0/2 + 0/4 + 1/8. These two fractions have identical values, the only real difference being that the first is written in base 10 fractional notation, and the second in base 2.

Unfortunately, most decimal fractions cannot be represented exactly as binary fractions. A consequence is that, in general, the decimal floating-point numbers you enter are only approximated by the binary floating-point numbers actually stored in the machine.

The problem is easier to understand at first in base 10. Consider the fraction 1/3. You can approximate that as a base 10 fraction:


    0.3


or, better,


    0.33


or, better,


    0.333


and so on. No matter how many digits you’re willing to write down, the result will never be exactly 1/3, but will be an increasingly better approximation of 1/3.

In the same way, no matter how many base 2 digits you’re willing to use, the decimal value 0.1 cannot be represented exactly as a base 2 fraction. In base 2, 1/10 is the infinitely repeating fraction


0.0001100110011001100110011001100110011001100110011...


Stop at any finite number of bits, and you get an approximation. On most machines today, floats are approximated using a binary fraction with the numerator using the first 53 bits starting with the most significant bit and with the denominator as a power of two. In the case of 1/10, the binary fraction is 3602879701896397 / 2 ** 55 which is close to but not exactly equal to the true value of 1/10.

Many users are not aware of the approximation because of the way values are displayed. Python only prints a decimal approximation to the true decimal value of the binary approximation stored by the machine. On most machines, if Python were to print the true decimal value of the binary approximation stored for 0.1, it would have to display


>>> 0.1
0.1000000000000000055511151231257827021181583404541015625


That is more digits than most people find useful, so Python keeps the number of digits manageable by displaying a rounded value instead


>>> 1 / 10
0.1


Just remember, even though the printed result looks like the exact value of 1/10, the actual stored value is the nearest representable binary fraction.

Interestingly, there are many different decimal numbers that share the same nearest approximate binary fraction. For example, the numbers 0.1 and 0.10000000000000001 and 0.1000000000000000055511151231257827021181583404541015625 are all approximated by 3602879701896397 / 2 ** 55. Since all of these decimal values share the same approximation, any one of them could be displayed while still preserving the invariant `eval(repr(x)) == x`.

Historically, the Python prompt and built-in `repr()` function would choose the one with `17 significant digits`, 0.10000000000000001. Starting with Python 3.1, Python (on most systems) is now able to choose the shortest of these and simply display 0.1.

Note that this is in the very nature of binary floating-point: this is not a bug in Python, and it is not a bug in your code either. You’ll see the same kind of thing in all languages that support your hardware’s floating-point arithmetic (although some languages may not display the difference by default, or in all output modes).

For more pleasant output, you may wish to use string formatting to produce a limited number of significant digits:


>>> format(math.pi, '.12g')  # give 12 significant digits
'3.14159265359'

>>> format(math.pi, '.2f')   # give 2 digits after the point
'3.14'

>>> repr(math.pi)
'3.141592653589793'


It’s important to realize that this is, in a real sense, an illusion: you’re simply rounding the display of the true machine value.

One illusion may beget another. For example, since 0.1 is not exactly 1/10, summing three values of 0.1 may not yield exactly 0.3, either:


>>> .1 + .1 + .1 == .3
False


Also, since the 0.1 cannot get any closer to the exact value of 1/10 and 0.3 cannot get any closer to the exact value of 3/10, then pre-rounding with `round()` function cannot help:


>>> round(.1, 1) + round(.1, 1) + round(.1, 1) == round(.3, 1)
False


Though the numbers cannot be made closer to their intended exact values, the `round()` function can be useful for post-rounding so that results with inexact values become comparable to one another:


>>> round(.1 + .1 + .1, 10) == round(.3, 10)
True


Binary floating-point arithmetic holds many surprises like this. The problem with “0.1” is explained in precise detail below, in the “Representation Error” section. See The Perils of Floating Point for a more complete account of other common surprises.

As that says near the end, “there are no easy answers.” Still, don’t be unduly wary of floating-point! The errors in Python float operations are inherited from the floating-point hardware, and on most machines are on the order of no more than 1 part in 2 ** 53 per operation. That’s more than adequate for most tasks, but you do need to keep in mind that it’s not decimal arithmetic and that every float operation can suffer a new rounding error.

While pathological cases do exist, for most casual use of floating-point arithmetic you’ll see the result you expect in the end if you simply round the display of your final results to the number of decimal digits you expect. `str()` usually suffices, and for finer control see the `str.format()` method’s format specifiers in Format String Syntax.

For use cases which require exact decimal representation, try using the decimal module which implements decimal arithmetic suitable for accounting applications and high-precision applications.

Another form of exact arithmetic is supported by the fractions module which implements arithmetic based on rational numbers (so the numbers like 1/3 can be represented exactly).

If you are a heavy user of floating point operations you should take a look at the NumPy package and many other packages for mathematical and statistical operations supplied by the SciPy project. See <https://scipy.org>.

Python provides tools that may help on those rare occasions when you really do want to know the exact value of a float. The `float.as_integer_ratio()` method expresses the value of a float as a fraction:


>>> x = 3.14159
>>> x.as_integer_ratio()
(3537115888337719, 1125899906842624)


Since the ratio is exact, it can be used to losslessly recreate the original value:


>>> x == 3537115888337719 / 1125899906842624
True


The `float.hex()` method expresses a float in hexadecimal (base 16), again giving the exact value stored by your computer:


>>> x.hex()
'0x1.921f9f01b866ep+1'


This precise hexadecimal representation can be used to reconstruct the float value exactly:


>>> x == float.fromhex('0x1.921f9f01b866ep+1')
True


Since the representation is exact, it is useful for reliably porting values across different versions of Python (platform independence) and exchanging data with other languages that support the same format (such as Java and C99).

Another helpful tool is the `math.fsum()` function which helps mitigate loss-of-precision during summation. It tracks “lost digits” as values are added onto a running total. That can make a difference in overall accuracy so that the errors do not accumulate to the point where they affect the final total:


>>> sum([0.1] * 10) == 1.0
False
>>> math.fsum([0.1] * 10) == 1.0
True



### ===🗝 15.1. Representation Error

This section explains the “0.1” example in detail, and shows how you can perform an exact analysis of cases like this yourself. Basic familiarity with binary floating-point representation is assumed.

Representation error refers to the fact that some (most, actually) decimal fractions cannot be represented exactly as binary (base 2) fractions. This is the chief reason why Python (or Perl, C, C++, Java, Fortran, and many others) often won’t display the exact decimal number you expect.

Why is that? 1/10 is not exactly representable as a binary fraction. Almost all machines today (November 2000) use IEEE-754 floating point arithmetic, and almost all platforms map Python floats to `IEEE-754` “double precision”. 754 doubles contain 53 bits of precision, so on input the computer strives to convert 0.1 to the closest fraction it can of the form J/2 ** N where J is an integer containing exactly 53 bits. Rewriting


    1 / 10 ~= J / (2**N)


as


    J ~= 2**N / 10


and recalling that J has exactly 53 bits (is >= 2**52 but < 2**53), the best value for N is 56:


>>> 2**52 <=  2**56 // 10  < 2**53
True


That is, 56 is the only value for N that leaves J with exactly 53 bits. The best possible value for J is then that quotient rounded:


>>> q, r = divmod(2**56, 10)
>>> r
6


Since the remainder is more than half of 10, the best approximation is obtained by rounding up:


>>> q+1
7205759403792794


Therefore the best possible approximation to 1/10 in 754 double precision is:


7205759403792794 / 2 ** 56


Dividing both the numerator and denominator by two reduces the fraction to:


3602879701896397 / 2 ** 55


Note that since we rounded up, this is actually a little bit larger than 1/10; if we had not rounded up, the quotient would have been a little bit smaller than 1/10. But in no case can it be exactly 1/10!

So the computer never “sees” 1/10: what it sees is the exact fraction given above, the best 754 double approximation it can get:


>>> 0.1 * 2 ** 55
3602879701896397.0


If we multiply that fraction by 10 ** 55, we can see the value out to 55 decimal digits:


>>> 3602879701896397 * 10 ** 55 // 2 ** 55
1000000000000000055511151231257827021181583404541015625


meaning that the exact number stored in the computer is equal to the decimal value 0.1000000000000000055511151231257827021181583404541015625. Instead of displaying the full decimal value, many languages (including older versions of Python), round the result to 17 significant digits:


>>> format(0.1, '.17f')
'0.10000000000000001'


The fractions and decimal modules make these calculations easy:


>>> from decimal import Decimal
>>> from fractions import Fraction

>>> Fraction.from_float(0.1)
Fraction(3602879701896397, 36028797018963968)

>>> (0.1).as_integer_ratio()
(3602879701896397, 36028797018963968)

>>> Decimal.from_float(0.1)
Decimal('0.1000000000000000055511151231257827021181583404541015625')

>>> format(Decimal.from_float(0.1), '.17')
'0.10000000000000001'



## ==⚡ 16. Appendix

16.1. Interactive Mode


### ===🗝 16.1.1. Error Handling

When an error occurs, the interpreter prints an error message and a stack trace. In interactive mode, it then returns to the primary prompt; when input came from a file, it exits with a nonzero exit status after printing the stack trace. (Exceptions handled by an except clause in a try statement are not errors in this context.) Some errors are unconditionally fatal and cause an exit with a nonzero exit; this applies to internal inconsistencies and some cases of running out of memory. All error messages are written to the standard error stream; normal output from executed commands is written to standard output.

Typing the interrupt character (usually Control-C or Delete) to the primary or secondary prompt cancels the input and returns to the primary prompt. [1] Typing an interrupt while a command is executing raises the KeyboardInterrupt exception, which may be handled by a try statement.


### ===🗝 16.1.2. Executable Python Scripts

On BSD’ish Unix systems, Python scripts can be made directly executable, like shell scripts, by putting the line


    #!/usr/bin/env python3.5


(assuming that the interpreter is on the user’s PATH) at the beginning of the script and giving the file an executable mode. The #! must be the first two characters of the file. On some platforms, this first line must end with a Unix-style line ending ('\n'), not a Windows ('\r\n') line ending. Note that the hash, or pound, character, '#', is used to start a comment in Python.

The script can be given an executable mode, or permission, using the chmod command.


    $ chmod +x myscript.py


On Windows systems, there is no notion of an “executable mode”. The Python installer automatically associates .py files with python.exe so that a double-click on a Python file will run it as a script. The extension can also be .pyw, in that case, the console window that normally appears is suppressed.


### ===🗝 16.1.3. The Interactive Startup File

When you use Python interactively, it is frequently handy to have some standard commands executed every time the interpreter is started. You can do this by setting an environment variable named PYTHONSTARTUP to the name of a file containing your start-up commands. This is similar to the .profile feature of the Unix shells.

This file is only read in interactive sessions, not when Python reads commands from a script, and not when /dev/tty is given as the explicit source of commands (which otherwise behaves like an interactive session). It is executed in the same namespace where interactive commands are executed, so that objects that it defines or imports can be used without qualification in the interactive session. You can also change the prompts sys.ps1 and sys.ps2 in this file.

If you want to read an additional start-up file from the current directory, you can program this in the global start-up file using code like if os.path.isfile('.pythonrc.py'): exec(open('.pythonrc.py').read()). If you want to use the startup file in a script, you must do this explicitly in the script:


    import os
    filename = os.environ.get('PYTHONSTARTUP')
    if filename and os.path.isfile(filename):
        with open(filename) as fobj:
            startup_file = fobj.read()
        exec(startup_file)



### ===🗝 16.1.4. The Customization Modules

Python provides two hooks to let you customize it: sitecustomize and usercustomize. To see how it works, you need first to find the location of your user site-packages directory. Start Python and run this code:


>>> import site
>>> site.getusersitepackages()
'/home/user/.local/lib/python3.5/site-packages'


Now you can create a file named usercustomize.py in that directory and put anything you want in it. It will affect every invocation of Python, unless it is started with the -s option to disable the automatic import.

sitecustomize works in the same way, but is typically created by an administrator of the computer in the global site-packages directory, and is imported before usercustomize. See the documentation of the site module for more details.

Footnotes

[1] A problem with the GNU Readline package may prevent this. 



# =🚩 Python HOWTOs Guides
- Python HOWTOs https://docs.python.org/3/howto/index.html

Python HOWTOs are documents that cover a single, specific topic, and attempt to cover it fairly completely. Modelled on the Linux Documentation Project’s HOWTO collection, this collection is an effort to foster documentation that’s more detailed than the Python Library Reference.

Currently, the HOWTOs are:

• Porting Python 2 Code to Python 3
• Porting Extension Modules to Python 3
• Curses Programming with Python
• Descriptor HowTo Guide
• Functional Programming HOWTO
• Logging HOWTO
• Logging Cookbook
• Regular Expression HOWTO
• Socket Programming HOWTO
• Sorting HOW TO
• Unicode HOWTO
• HOWTO Fetch Internet Resources Using The urllib Package
• Argparse Tutorial
• An introduction to the ipaddress module
• Argument Clinic How-To
• Instrumenting CPython with DTrace and SystemTap
• Annotations Best Practices

## ==⚡ • Porting Python 2 Code to Python 3
## ==⚡ • Porting Extension Modules to Python 3
## ==⚡ • Curses Programming with Python
## ==⚡ • Descriptor HowTo Guide
## ==⚡ • Functional Programming HOWTO
## ==⚡ • Logging HOWTO
## ==⚡ • Logging Cookbook
## ==⚡ • Regular Expression HOWTO
## ==⚡ • Socket Programming HOWTO
## ==⚡ • Sorting HOW TO
## ==⚡ • Unicode HOWTO
## ==⚡ • HOWTO Fetch Internet Resources Using The urllib Package
## ==⚡ • Argparse Tutorial
## ==⚡ • An introduction to the ipaddress module
## ==⚡ • Argument Clinic How-To
## ==⚡ • Instrumenting CPython with DTrace and SystemTap
## ==⚡ • Annotations Best Practices


# =🚩 The Python Language Reference 
- Guide to CPython’s Parser https://devguide.python.org/parser/
- Design of CPython’s Compiler https://devguide.python.org/compiler/

The Python Language Reference

This reference manual describes the syntax and “core semantics” of the language. It is terse, but attempts to be exact and complete. The semantics of non-essential built-in object types and of the built-in functions and modules are described in The Python Standard Library. For an informal introduction to the language, see The Python Tutorial. For C or C++ programmers, two additional manuals exist: Extending and Embedding the Python Interpreter describes the high-level picture of how to write a Python extension module, and the Python/C API Reference Manual describes the interfaces available to C/C++ programmers in detail.

• 1. Introduction
◦1.1. Alternate Implementations
◦1.2. Notation

• 2. Lexical analysis
◦2.1. Line structure
◦2.2. Other tokens
◦2.3. Identifiers and keywords
◦2.4. Literals
◦2.5. Operators
◦2.6. Delimiters

• 3. Data model
◦3.1. Objects, values and types
◦3.2. The standard type hierarchy
◦3.3. Special method names
◦3.4. Coroutines

• 4. Execution model
◦4.1. Structure of a program
◦4.2. Naming and binding
◦4.3. Exceptions

• 5. The import system
◦5.1. importlib
◦5.2. Packages
◦5.3. Searching
◦5.4. Loading
◦5.5. The Path Based Finder
◦5.6. Replacing the standard import system
◦5.7. Package Relative Imports
◦5.8. Special considerations for __main__
◦5.9. Open issues
◦5.10. References

• 6. Expressions
◦6.1. Arithmetic conversions
◦6.2. Atoms
◦6.3. Primaries
◦6.4. Await expression
◦6.5. The power operator
◦6.6. Unary arithmetic and bitwise operations
◦6.7. Binary arithmetic operations
◦6.8. Shifting operations
◦6.9. Binary bitwise operations
◦6.10. Comparisons
◦6.11. Boolean operations
◦6.12. Assignment expressions
◦6.13. Conditional expressions
◦6.14. Lambdas
◦6.15. Expression lists
◦6.16. Evaluation order
◦6.17. Operator precedence

• 7. Simple statements
◦7.1. Expression statements
◦7.2. Assignment statements
◦7.3. The assert statement
◦7.4. The pass statement
◦7.5. The del statement
◦7.6. The return statement
◦7.7. The yield statement
◦7.8. The raise statement
◦7.9. The break statement
◦7.10. The continue statement
◦7.11. The import statement
◦7.12. The global statement
◦7.13. The nonlocal statement

• 8. Compound statements
◦8.1. The if statement
◦8.2. The while statement
◦8.3. The for statement
◦8.4. The try statement
◦8.5. The with statement
◦8.6. Function definitions
◦8.7. Class definitions
◦8.8. Coroutines

• 9. Top-level components
◦9.1. Complete Python programs
◦9.2. File input
◦9.3. Interactive input
◦9.4. Expression input

• 10. Full Grammar specification


## ==⚡ 1. Introduction

This reference manual describes the Python programming language. It is not intended as a tutorial.

While I am trying to be as precise as possible, I chose to use English rather than formal specifications for everything except syntax and lexical analysis. This should make the document more understandable to the average reader, but will leave room for ambiguities. Consequently, if you were coming from Mars and tried to re-implement Python from this document alone, you might have to guess things and in fact you would probably end up implementing quite a different language. On the other hand, if you are using Python and wonder what the precise rules about a particular area of the language are, you should definitely be able to find them here. If you would like to see a more formal definition of the language, maybe you could volunteer your time — or invent a cloning machine :-).

It is dangerous to add too many implementation details to a language reference document — the implementation may change, and other implementations of the same language may work differently. On the other hand, CPython is the one Python implementation in widespread use (although alternate implementations continue to gain support), and its particular quirks are sometimes worth being mentioned, especially where the implementation imposes additional limitations. Therefore, you’ll find short “implementation notes” sprinkled throughout the text.

Every Python implementation comes with a number of built-in and standard modules. These are documented in *The Python Standard Library*. A few built-in modules are mentioned when they interact in a significant way with the language definition.


1.1. Alternate Implementations

Though there is one Python implementation which is by far the most popular, there are some alternate implementations which are of particular interest to different audiences.

Known implementations include:
✅ `CPython`
This is the original and most-maintained implementation of Python, written in C. New language features generally appear here first.
✅ `Jython`
Python implemented in Java. This implementation can be used as a scripting language for Java applications, or can be used to create applications using the Java class libraries. It is also often used to create tests for Java libraries. More information can be found at the Jython website.
✅ `Python for .NET`
This implementation actually uses the CPython implementation, but is a managed .NET application and makes .NET libraries available. It was created by Brian Lloyd. For more information, see the Python for .NET home page.
✅ `IronPython`
An alternate Python for .NET. Unlike Python.NET, this is a complete Python implementation that generates IL, and compiles Python code directly to .NET assemblies. It was created by Jim Hugunin, the original creator of Jython. For more information, see the IronPython website.
✅ `PyPy`
An implementation of Python written completely in Python. It supports several advanced features not found in other implementations like stackless support and a Just in Time compiler. One of the goals of the project is to encourage experimentation with the language itself by making it easier to modify the interpreter (since it is written in Python). Additional information is available on the PyPy project’s home page.

Each of these implementations varies in some way from the language as documented in this manual, or introduces specific information beyond what’s covered in the standard Python documentation. Please refer to the implementation-specific documentation to determine what else you need to know about the specific implementation you’re using.


1.2. Notation

The descriptions of lexical analysis and syntax use a modified BNF grammar notation. This uses the following style of definition:

    name      ::=  lc_letter (lc_letter | "_")*
    lc_letter ::=  "a"..."z"

词法中，`( a | b )*` 这样的表达类似正则表达式，竖线表示 or，圆括号表示一个分组，* 号可以表可有可无。所以第一行表示 *lc_letter* 后续还可以有任意个。

The first line says that a name is an *lc_letter* followed by a sequence of zero or more lc_letters and underscores. An *lc_letter* in turn is any of the single characters 'a' through 'z'. (This rule is actually adhered to for the names defined in lexical and grammar rules in this document.)

↪ Each rule begins with a name (which is the name defined by the rule) and `::=`. 
↪ A vertical bar (`|`) is used to separate alternatives; it is the least binding operator in this notation. 
↪ A star (`*`) means zero or more repetitions of the preceding item; likewise, a plus (+) means one or more repetitions。
↪ And a phrase enclosed in square brackets ([ ]) means zero or one occurrences (in other words, the enclosed phrase is optional). 
↪ The * and + operators bind as tightly as possible; parentheses are used for grouping. 
↪ Literal strings are enclosed in quotes. 
↪ White space is only meaningful to separate tokens. 
↪ Rules are normally contained on a single line; rules with many alternatives may be formatted alternatively with each line after the first beginning with a vertical bar.

In lexical definitions (as the example above), two more conventions are used: 

↪ Two literal characters separated by three dots mean a choice of any single character in the given (inclusive) range of ASCII characters.
↪ A phrase between angular brackets (<...>) gives an informal description of the symbol defined; e.g., this could be used to describe the notion of ‘control character’ if needed.

Even though the notation used is almost the same, there is a big difference between the meaning of lexical and syntactic definitions: a lexical definition operates on the individual characters of the input source, while a syntax definition operates on the stream of tokens generated by the lexical analysis. All uses of BNF in the next chapter (“Lexical Analysis”) are lexical definitions; uses in subsequent chapters are syntactic definitions.



## ==⚡ 2. Lexical analysis


A Python program is read by a parser. Input to the parser is a stream of tokens, generated by the lexical analyzer. This chapter describes how the lexical analyzer breaks a file into tokens.

Python reads program text as Unicode code points; the encoding of a source file can be given by an encoding declaration and defaults to UTF-8, see PEP 3120 for details. If the source file cannot be decoded, a *SyntaxError* is raised.


### ===🗝 2.1. Line structure

A Python program is divided into a number of logical lines.


### ===🗝 2.1.1. Logical lines

The end of a logical line is represented by the token NEWLINE. Statements cannot cross logical line boundaries except where NEWLINE is allowed by the syntax (e.g., between statements in compound statements). A logical line is constructed from one or more physical lines by following the explicit or implicit line joining rules.


### ===🗝 2.1.2. Physical lines

A physical line is a sequence of characters terminated by an end-of-line sequence. In source files and strings, any of the standard platform line termination sequences can be used - the Unix form using ASCII LF (linefeed), the Windows form using the ASCII sequence CR LF (return followed by linefeed), or the old Macintosh form using the ASCII CR (return) character. All of these forms can be used equally, regardless of platform. The end of input also serves as an implicit terminator for the final physical line.

When embedding Python, source code strings should be passed to Python APIs using the standard C conventions for newline characters (the \n character, representing ASCII LF, is the line terminator).


### ===🗝 2.1.3. Comments

A comment starts with a hash character (#) that is not part of a string literal, and ends at the end of the physical line. A comment signifies the end of the logical line unless the implicit line joining rules are invoked. Comments are ignored by the syntax.


### ===🗝 2.1.4. Encoding declarations

If a comment in the first or second line of the Python script matches the regular expression `coding[=:]\s*([-\w.]+)`, this comment is processed as an encoding declaration; the first group of this expression names the encoding of the source code file. The encoding declaration must appear on a line of its own. If it is the second line, the first line must also be a comment-only line. The recommended forms of an encoding expression are


    # -*- coding: <encoding-name> -*-


which is recognized also by GNU Emacs, and


    # vim:fileencoding=<encoding-name>


which is recognized by Bram Moolenaar’s VIM.

If no encoding declaration is found, the default encoding is UTF-8. In addition, if the first bytes of the file are the UTF-8 byte-order mark (b'\xef\xbb\xbf'), the declared file encoding is UTF-8 (this is supported, among others, by Microsoft’s notepad).

If an encoding is declared, the encoding name must be recognized by Python. The encoding is used for all lexical analysis, including string literals, comments and identifiers.


### ===🗝 2.1.5. Explicit line joining

Two or more physical lines may be joined into logical lines using backslash characters (\), as follows: when a physical line ends in a backslash that is not part of a string literal or comment, it is joined with the following forming a single logical line, deleting the backslash and the following end-of-line character. For example:


    if 1900 < year < 2100 and 1 <= month <= 12 \
       and 1 <= day <= 31 and 0 <= hour < 24 \
       and 0 <= minute < 60 and 0 <= second < 60:   # Looks like a valid date
            return 1


A line ending in a backslash cannot carry a comment. A backslash does not continue a comment. A backslash does not continue a token except for string literals (i.e., tokens other than string literals cannot be split across physical lines using a backslash). A backslash is illegal elsewhere on a line outside a string literal.


### ===🗝 2.1.6. Implicit line joining

Expressions in parentheses, square brackets or curly braces can be split over more than one physical line without using backslashes. For example:


    month_names = ['Januari', 'Februari', 'Maart',      # These are the
                   'April',   'Mei',      'Juni',       # Dutch names
                   'Juli',    'Augustus', 'September',  # for the months
                   'Oktober', 'November', 'December']   # of the year


Implicitly continued lines can carry comments. The indentation of the continuation lines is not important. Blank continuation lines are allowed. There is no NEWLINE token between implicit continuation lines. Implicitly continued lines can also occur within triple-quoted strings (see below); in that case they cannot carry comments.


### ===🗝 2.1.7. Blank lines

A logical line that contains only spaces, tabs, formfeeds and possibly a comment, is ignored (i.e., no NEWLINE token is generated). During interactive input of statements, handling of a blank line may differ depending on the implementation of the read-eval-print loop. In the standard interactive interpreter, an entirely blank logical line (i.e. one containing not even whitespace or a comment) terminates a multi-line statement.


### ===🗝 2.1.8. Indentation

Leading whitespace (spaces and tabs) at the beginning of a logical line is used to compute the indentation level of the line, which in turn is used to determine the grouping of statements.

Tabs are replaced (from left to right) by one to eight spaces such that the total number of characters up to and including the replacement is a multiple of eight (this is intended to be the same rule as used by Unix). The total number of spaces preceding the first non-blank character then determines the line’s indentation. Indentation cannot be split over multiple physical lines using backslashes; the whitespace up to the first backslash determines the indentation.

Indentation is rejected as inconsistent if a source file mixes tabs and spaces in a way that makes the meaning dependent on the worth of a tab in spaces; a TabError is raised in that case.

Cross-platform compatibility note: because of the nature of text editors on non-UNIX platforms, it is unwise to use a mixture of spaces and tabs for the indentation in a single source file. It should also be noted that different platforms may explicitly limit the maximum indentation level.

A formfeed character may be present at the start of the line; it will be ignored for the indentation calculations above. Formfeed characters occurring elsewhere in the leading whitespace have an undefined effect (for instance, they may reset the space count to zero).

The indentation levels of consecutive lines are used to generate `INDENT` and `DEDENT` tokens, using a stack, as follows.

Before the first line of the file is read, a single zero is pushed on the stack; this will never be popped off again. The numbers pushed on the stack will always be strictly increasing from bottom to top. At the beginning of each logical line, the line’s indentation level is compared to the top of the stack. If it is equal, nothing happens. If it is larger, it is pushed on the stack, and one `INDENT` token is generated. If it is smaller, it must be one of the numbers occurring on the stack; all numbers on the stack that are larger are popped off, and for each number popped off a `DEDENT` token is generated. At the end of the file, a `DEDENT` token is generated for each number remaining on the stack that is larger than zero.

Here is an example of a correctly (though confusingly) indented piece of Python code:


    def perm(l):
            # Compute the list of all permutations of l
        if len(l) <= 1:
                      return [l]
        r = []
        for i in range(len(l)):
                 s = l[:i] + l[i+1:]
                 p = perm(s)
                 for x in p:
                  r.append(l[i:i+1] + x)
        return r


The following example shows various indentation errors:


     def perm(l):                       # error: first line indented
    for i in range(len(l)):             # error: not indented
        s = l[:i] + l[i+1:]
            p = perm(l[:i] + l[i+1:])   # error: unexpected indent
            for x in p:
                    r.append(l[i:i+1] + x)
                return r                # error: inconsistent dedent


(Actually, the first three errors are detected by the parser; only the last error is found by the lexical analyzer — the indentation of return r does not match a level popped off the stack.)


### ===🗝 2.1.9. Whitespace between tokens

Except at the beginning of a logical line or in string literals, the whitespace characters space, tab and formfeed can be used interchangeably to separate tokens. Whitespace is needed between two tokens only if their concatenation could otherwise be interpreted as a different token (e.g., ab is one token, but a b is two tokens).


### ===🗝 2.2. Other tokens

Besides `NEWLINE`, `INDENT` and `DEDENT`, the following categories of tokens exist: identifiers, keywords, literals, operators, and delimiters. Whitespace characters (other than line terminators, discussed earlier) are not tokens, but serve to delimit tokens. Where ambiguity exists, a token comprises the longest possible string that forms a legal token, when read from left to right.


### ===🗝 2.3. Identifiers and keywords

Identifiers (also referred to as names) are described by the following lexical definitions.

The syntax of identifiers in Python is based on the Unicode standard annex UAX-31, with elaboration and changes as defined below; see also PEP 3131 for further details.

Within the ASCII range (U+0001..U+007F), the valid characters for identifiers are the same as in Python 2.x: the uppercase and lowercase letters A through Z, the underscore _ and, except for the first character, the digits 0 through 9.

Python 3.0 introduces additional characters from outside the ASCII range (see PEP 3131). For these characters, the classification uses the version of the Unicode Character Database as included in the unicodedata module.

Identifiers are unlimited in length. Case is significant.

    identifier   ::=  xid_start xid_continue*
    id_start     ::=  <all characters in general categories Lu, Ll, Lt, Lm, Lo, Nl, the underscore, and characters with the Other_ID_Start property>
    id_continue  ::=  <all characters in id_start, plus characters in the categories Mn, Mc, Nd, Pc and others with the Other_ID_Continue property>
    xid_start    ::=  <all characters in id_start whose NFKC normalization is in "id_start xid_continue*">
    xid_continue ::=  <all characters in id_continue whose NFKC normalization is in "id_continue*">

The Unicode category codes mentioned above stand for:

1.  •Lu - uppercase letters
2.  •Ll - lowercase letters
3.  •Lt - titlecase letters
4.  •Lm - modifier letters
5.  •Lo - other letters
6.  •Nl - letter numbers
7.  •Mn - nonspacing marks
8.  •Mc - spacing combining marks
9.  •Nd - decimal numbers
10. •Pc - connector punctuations
11. •Other_ID_Start - explicit list of characters in PropList.txt to support backwards compatibility
12. •Other_ID_Continue - likewise

All identifiers are converted into the normal form NFKC while parsing; comparison of identifiers is based on NFKC.

A non-normative HTML file listing all valid identifier characters for Unicode 4.1 can be found at https://www.unicode.org/Public/13.0.0/ucd/DerivedCoreProperties.txt


### ===🗝 2.3.1. Keywords

The following identifiers are used as reserved words, or keywords of the language, and cannot be used as ordinary identifiers. They must be spelled exactly as written here:


    False      await      else       import     pass
    None       break      except     in         raise
    True       class      finally    is         return
    and        continue   for        lambda     try
    as         def        from       nonlocal   while
    assert     del        global     not        with
    async      elif       if         or         yield



### ===🗝 2.3.2. Reserved classes of identifiers

Certain classes of identifiers (besides keywords) have special meanings. These classes are identified by the patterns of leading and trailing underscore characters:
➡ `_*`
Not imported by `from module import *`. The special identifier _ is used in the interactive interpreter to store the result of the last evaluation; it is stored in the builtins module. When not in interactive mode, _ has no special meaning and is not defined. See section The import statement.

Note:
 The name _ is often used in conjunction with internationalization; refer to the documentation for the gettext module for more information on this convention.
 
➡ `__*__`
System-defined names, informally known as “dunder” names. These names are defined by the interpreter and its implementation (including the standard library). Current system names are discussed in the Special method names section and elsewhere. More will likely be defined in future versions of Python. Any use of __*__ names, in any context, that does not follow explicitly documented use, is subject to breakage without warning.
➡ `__*`
Class-private names. Names in this category, when used within the context of a class definition, are re-written to use a mangled form to help avoid name clashes between “private” attributes of base and derived classes. See section Identifiers (Names).

### ===🗝 2.4. Literals

Literals are notations for constant values of some built-in types.


### ===🗝 2.4.1. String and Bytes literals

String literals are described by the following lexical definitions:

    stringliteral   ::=  [stringprefix](shortstring | longstring)
    stringprefix    ::=  "r" | "u" | "R" | "U" | "f" | "F"
                         | "fr" | "Fr" | "fR" | "FR" | "rf" | "rF" | "Rf" | "RF"
    shortstring     ::=  "'" shortstringitem* "'" | '"' shortstringitem* '"'
    longstring      ::=  "'''" longstringitem* "'''" | '"""' longstringitem* '"""'
    shortstringitem ::=  shortstringchar | stringescapeseq
    longstringitem  ::=  longstringchar | stringescapeseq
    shortstringchar ::=  <any source character except "\" or newline or the quote>
    longstringchar  ::=  <any source character except "\">
    stringescapeseq ::=  "\" <any source character>

    bytesliteral   ::=  bytesprefix(shortbytes | longbytes)
    bytesprefix    ::=  "b" | "B" | "br" | "Br" | "bR" | "BR" | "rb" | "rB" | "Rb" | "RB"
    shortbytes     ::=  "'" shortbytesitem* "'" | '"' shortbytesitem* '"'
    longbytes      ::=  "'''" longbytesitem* "'''" | '"""' longbytesitem* '"""'
    shortbytesitem ::=  shortbyteschar | bytesescapeseq
    longbytesitem  ::=  longbyteschar | bytesescapeseq
    shortbyteschar ::=  <any ASCII character except "\" or newline or the quote>
    longbyteschar  ::=  <any ASCII character except "\">
    bytesescapeseq ::=  "\" <any ASCII character>

One syntactic restriction not indicated by these productions is that whitespace is not allowed between the stringprefix or bytesprefix and the rest of the literal. The source character set is defined by the encoding declaration; it is UTF-8 if no encoding declaration is given in the source file; see section Encoding declarations.

In plain English: Both types of literals can be enclosed in matching single quotes (') or double quotes ("). They can also be enclosed in matching groups of three single or double quotes (these are generally referred to as triple-quoted strings). The backslash (\) character is used to escape characters that otherwise have a special meaning, such as newline, backslash itself, or the quote character.

Bytes literals are always prefixed with 'b' or 'B'; they produce an instance of the bytes type instead of the str type. They may only contain ASCII characters; bytes with a numeric value of 128 or greater must be expressed with escapes.

Both string and bytes literals may optionally be prefixed with a letter 'r' or 'R'; such strings are called raw strings and treat backslashes as literal characters. As a result, in string literals, '\U' and '\u' escapes in raw strings are not treated specially. Given that Python 2.x’s raw unicode literals behave differently than Python 3.x’s the 'ur' syntax is not supported.


New in version 3.3: The 'rb' prefix of raw bytes literals has been added as a synonym of 'br'.

New in version 3.3: Support for the unicode legacy literal (u'value') was reintroduced to simplify the maintenance of dual Python 2.x and 3.x codebases. See PEP 414 for more information.

A string literal with 'f' or 'F' in its prefix is a formatted string literal; see Formatted string literals. The 'f' may be combined with 'r', but not with 'b' or 'u', therefore raw formatted strings are possible, but formatted bytes literals are not.

In triple-quoted literals, unescaped newlines and quotes are allowed (and are retained), except that three unescaped quotes in a row terminate the literal. (A “quote” is the character used to open the literal, i.e. either ' or ".)

Unless an 'r' or 'R' prefix is present, escape sequences in string and bytes literals are interpreted according to rules similar to those used by Standard C. The recognized escape sequences are:

| Escape Sequence |            Meaning             | Notes |
|-----------------|--------------------------------|-------|
| \newline        | Backslash and newline ignored  |       |
| \\              | Backslash (\)                  |       |
| \'              | Single quote (')               |       |
| \"              | Double quote (")               |       |
| \a              | ASCII Bell (BEL)               |       |
| \b              | ASCII Backspace (BS)           |       |
| \f              | ASCII Formfeed (FF)            |       |
| \n              | ASCII Linefeed (LF)            |       |
| \r              | ASCII Carriage Return (CR)     |       |
| \t              | ASCII Horizontal Tab (TAB)     |       |
| \v              | ASCII Vertical Tab (VT)        |       |
| \ooo            | Character with octal value ooo | (1,3) |
| \xhh            | Character with hex value hh    | (2,3) |

Escape sequences only recognized in string literals are:

| Escape Sequence |                   Meaning                    | Notes |
|-----------------|----------------------------------------------|-------|
| \N{name}        | Character named name in the Unicode database | (4)   |
| \uxxxx          | Character with 16-bit hex value xxxx         | (5)   |
| \Uxxxxxxxx      | Character with 32-bit hex value xxxxxxxx     | (6)   |

Notes:

1.As in Standard C, up to three octal digits are accepted.
2.Unlike in Standard C, exactly two hex digits are required.
3.In a bytes literal, hexadecimal and octal escapes denote the byte with the given value. In a string literal, these escapes denote a Unicode character with the given value.
4.Changed in version 3.3: Support for name aliases [1] has been added.
5.Exactly four hex digits are required.
6.Any Unicode character can be encoded this way. Exactly eight hex digits are required.

Unlike Standard C, all unrecognized escape sequences are left in the string unchanged, i.e., the backslash is left in the result. (This behavior is useful when debugging: if an escape sequence is mistyped, the resulting output is more easily recognized as broken.) It is also important to note that the escape sequences only recognized in string literals fall into the category of unrecognized escapes for bytes literals.

Changed in version 3.6: Unrecognized escape sequences produce a DeprecationWarning. In a future Python version they will be a SyntaxWarning and eventually a SyntaxError.

Even in a raw literal, quotes can be escaped with a backslash, but the backslash remains in the result; for example, r"\"" is a valid string literal consisting of two characters: a backslash and a double quote; r"\" is not a valid string literal (even a raw string cannot end in an odd number of backslashes). Specifically, a raw literal cannot end in a single backslash (since the backslash would escape the following quote character). Note also that a single backslash followed by a newline is interpreted as those two characters as part of the literal, not as a line continuation.


### ===🗝 2.4.2. String literal concatenation

Multiple adjacent string or bytes literals (delimited by whitespace), possibly using different quoting conventions, are allowed, and their meaning is the same as their concatenation. Thus, "hello" 'world' is equivalent to "helloworld". This feature can be used to reduce the number of backslashes needed, to split long strings conveniently across long lines, or even to add comments to parts of strings, for example:


    re.compile("[A-Za-z_]"       # letter or underscore
               "[A-Za-z0-9_]*"   # letter, digit or underscore
              )


Note that this feature is defined at the syntactical level, but implemented at compile time. The ‘+’ operator must be used to concatenate string expressions at run time. Also note that literal concatenation can use different quoting styles for each component (even mixing raw strings and triple quoted strings), and formatted string literals may be concatenated with plain string literals.


### ===🗝 2.4.3. Formatted string literals


New in version 3.6.

A formatted string literal or f-string is a string literal that is prefixed with 'f' or 'F'. These strings may contain replacement fields, which are expressions delimited by curly braces {}. While other string literals always have a constant value, formatted strings are really expressions evaluated at run time.

Escape sequences are decoded like in ordinary string literals (except when a literal is also marked as a raw string). After decoding, the grammar for the contents of the string is:

    f_string          ::=  (literal_char | "{{" | "}}" | replacement_field)*
    replacement_field ::=  "{" f_expression ["="] ["!" conversion] [":" format_spec] "}"
    f_expression      ::=  (conditional_expression | "*" or_expr)
                             ("," conditional_expression | "," "*" or_expr)* [","]
                           | yield_expression
    conversion        ::=  "s" | "r" | "a"
    format_spec       ::=  (literal_char | NULL | replacement_field)*
    literal_char      ::=  <any code point except "{", "}" or NULL>

The parts of the string outside curly braces are treated literally, except that any doubled curly braces '{{' or '}}' are replaced with the corresponding single curly brace. A single opening curly bracket '{' marks a replacement field, which starts with a Python expression. To display both the expression text and its value after evaluation, (useful in debugging), an equal sign '=' may be added after the expression. A conversion field, introduced by an exclamation point '!' may follow. A format specifier may also be appended, introduced by a colon ':'. A replacement field ends with a closing curly bracket '}'.

Expressions in formatted string literals are treated like regular Python expressions surrounded by parentheses, with a few exceptions. An empty expression is not allowed, and both lambda and assignment expressions := must be surrounded by explicit parentheses. Replacement expressions can contain line breaks (e.g. in triple-quoted strings), but they cannot contain comments. Each expression is evaluated in the context where the formatted string literal appears, in order from left to right.


Changed in version 3.7: Prior to Python 3.7, an await expression and comprehensions containing an async for clause were illegal in the expressions in formatted string literals due to a problem with the implementation.

When the equal sign '=' is provided, the output will have the expression text, the '=' and the evaluated value. Spaces after the opening brace '{', within the expression and after the '=' are all retained in the output. By default, the '=' causes the repr() of the expression to be provided, unless there is a format specified. When a format is specified it defaults to the str() of the expression unless a conversion '!r' is declared.


New in version 3.8: The equal sign '='.

If a conversion is specified, the result of evaluating the expression is converted before formatting. Conversion '!s' calls str() on the result, '!r' calls repr(), and '!a' calls ascii().

The result is then formatted using the format() protocol. The format specifier is passed to the __format__() method of the expression or conversion result. An empty string is passed when the format specifier is omitted. The formatted result is then included in the final value of the whole string.

Top-level format specifiers may include nested replacement fields. These nested fields may include their own conversion fields and format specifiers, but may not include more deeply-nested replacement fields. The format specifier mini-language is the same as that used by the str.format() method.

Formatted string literals may be concatenated, but replacement fields cannot be split across literals.

Some examples of formatted string literals:


>>> name = "Fred"
>>> f"He said his name is {name!r}."
"He said his name is 'Fred'."
>>> f"He said his name is {repr(name)}."  # repr() is equivalent to !r
"He said his name is 'Fred'."
>>> width = 10
>>> precision = 4
>>> value = decimal.Decimal("12.34567")
>>> f"result: {value:{width}.{precision}}"  # nested fields
'result:      12.35'
>>> today = datetime(year=2017, month=1, day=27)
>>> f"{today:%B %d, %Y}"  # using date format specifier
'January 27, 2017'
>>> f"{today=:%B %d, %Y}" # using date format specifier and debugging
'today=January 27, 2017'
>>> number = 1024
>>> f"{number:#0x}"  # using integer format specifier
'0x400'
>>> foo = "bar"
>>> f"{ foo = }" # preserves whitespace
" foo = 'bar'"
>>> line = "The mill's closed"
>>> f"{line = }"
'line = "The mill\'s closed"'
>>> f"{line = :20}"
"line = The mill's closed   "
>>> f"{line = !r:20}"
'line = "The mill\'s closed" '


A consequence of sharing the same syntax as regular string literals is that characters in the replacement fields must not conflict with the quoting used in the outer formatted string literal:


    f"abc {a["x"]} def"    # error: outer string literal ended prematurely
    f"abc {a['x']} def"    # workaround: use different quoting


Backslashes are not allowed in format expressions and will raise an error:


    f"newline: {ord('\n')}"  # raises SyntaxError


To include a value in which a backslash escape is required, create a temporary variable.


>>> newline = ord('\n')
>>> f"newline: {newline}"
'newline: 10'


Formatted string literals cannot be used as docstrings, even if they do not include expressions.


>>> def foo():
...     f"Not a docstring"
...
>>> foo.__doc__ is None
True


See also PEP 498 for the proposal that added formatted string literals, and str.format(), which uses a related format string mechanism.


### ===🗝 2.4.4. Numeric literals

There are three types of numeric literals: integers, floating point numbers, and imaginary numbers. There are no complex literals (complex numbers can be formed by adding a real number and an imaginary number).

Note that numeric literals do not include a sign; a phrase like -1 is actually an expression composed of the unary operator ‘-’ and the literal 1.


### ===🗝 2.4.5. Integer literals

Integer literals are described by the following lexical definitions:

    integer      ::=  decinteger | bininteger | octinteger | hexinteger
    decinteger   ::=  nonzerodigit (["_"] digit)* | "0"+ (["_"] "0")*
    bininteger   ::=  "0" ("b" | "B") (["_"] bindigit)+
    octinteger   ::=  "0" ("o" | "O") (["_"] octdigit)+
    hexinteger   ::=  "0" ("x" | "X") (["_"] hexdigit)+
    nonzerodigit ::=  "1"..."9"
    digit        ::=  "0"..."9"
    bindigit     ::=  "0" | "1"
    octdigit     ::=  "0"..."7"
    hexdigit     ::=  digit | "a"..."f" | "A"..."F"

There is no limit for the length of integer literals apart from what can be stored in available memory.

Underscores are ignored for determining the numeric value of the literal. They can be used to group digits for enhanced readability. One underscore can occur between digits, and after base specifiers like 0x.

Note that leading zeros in a non-zero decimal number are not allowed. This is for disambiguation with C-style octal literals, which Python used before version 3.0.

Some examples of integer literals:


    7     2147483647                        0o177    0b100110111
    3     79228162514264337593543950336     0o377    0xdeadbeef
          100_000_000_000                   0b_1110_0101

Changed in version 3.6: Underscores are now allowed for grouping purposes in literals.


### ===🗝 2.4.6. Floating point literals

Floating point literals are described by the following lexical definitions:

    floatnumber   ::=  pointfloat | exponentfloat
    pointfloat    ::=  [digitpart] fraction | digitpart "."
    exponentfloat ::=  (digitpart | pointfloat) exponent
    digitpart     ::=  digit (["_"] digit)*
    fraction      ::=  "." digitpart
    exponent      ::=  ("e" | "E") ["+" | "-"] digitpart

Note that the integer and exponent parts are always interpreted using radix 10. For example, 077e010 is legal, and denotes the same number as 77e10. The allowed range of floating point literals is implementation-dependent. As in integer literals, underscores are supported for digit grouping.

Some examples of floating point literals:


    3.14    10.    .001    1e100    3.14e-10    0e0    3.14_15_93

Changed in version 3.6: Underscores are now allowed for grouping purposes in literals.


### ===🗝 2.4.7. Imaginary literals

Imaginary literals are described by the following lexical definitions:

    imagnumber ::=  (floatnumber | digitpart) ("j" | "J")

An imaginary literal yields a complex number with a real part of 0.0. Complex numbers are represented as a pair of floating point numbers and have the same restrictions on their range. To create a complex number with a nonzero real part, add a floating point number to it, e.g., (3+4j). Some examples of imaginary literals:


    3.14j   10.j    10j     .001j   1e100j   3.14e-10j   3.14_15_93j



### ===🗝 2.5. Operators

The following tokens are operators:


    +       -       *       **      /       //      %      @
    <<      >>      &       |       ^       ~       :=
    <       >       <=      >=      ==      !=



### ===🗝 2.6. Delimiters

The following tokens serve as delimiters in the grammar:


    (       )       [       ]       {       }
    ,       :       .       ;       @       =       ->
    +=      -=      *=      /=      //=     %=      @=
    &=      |=      ^=      >>=     <<=     **=


The period can also occur in floating-point and imaginary literals. A sequence of three periods has a special meaning as an ellipsis literal. The second half of the list, the augmented assignment operators, serve lexically as delimiters, but also perform an operation.

The following printing ASCII characters have special meaning as part of other tokens or are otherwise significant to the lexical analyzer:


    '       "       #       \


The following printing ASCII characters are not used in Python. Their occurrence outside string literals and comments is an unconditional error:


    $       ?       `


Footnotes

[1] https://www.unicode.org/Public/11.0.0/ucd/NameAliases.txt 



## ==⚡ 3. Data model


### ===🗝 3.1. Objects, values and types

Objects are Python’s abstraction for data. All data in a Python program is represented by objects or by relations between objects. (In a sense, and in conformance to Von Neumann’s model of a “stored program computer”, code is also represented by objects.)

Every object has an identity, a type and a value. An object’s identity never changes once it has been created; you may think of it as the object’s address in memory. The ‘is’ operator compares the identity of two objects; the `id()` function returns an integer representing its identity.


CPython implementation detail: For CPython, `id(x)` is the memory address where x is stored.

An object’s type determines the operations that the object supports (e.g., “does it have a length?”) and also defines the possible values for objects of that type. The `type()` function returns an object’s type (which is an object itself). Like its identity, an object’s type is also unchangeable. [1]

The value of some objects can change. Objects whose value can change are said to be `mutable`; objects whose value is unchangeable once they are created are called `immutable`. (The value of an immutable container object that contains a reference to a mutable object can change when the latter’s value is changed; however the container is still considered immutable, because the collection of objects it contains cannot be changed. So, immutability is not strictly the same as having an unchangeable value, it is more subtle.) An object’s mutability is determined by its type; for instance, numbers, strings and tuples are immutable, while dictionaries and lists are mutable.

Objects are never explicitly destroyed; however, when they become unreachable they may be garbage-collected. An implementation is allowed to postpone garbage collection or omit it altogether — it is a matter of implementation quality how garbage collection is implemented, as long as no objects are collected that are still reachable.


CPython implementation detail: CPython currently uses a reference-counting scheme with (optional) delayed detection of cyclically linked garbage, which collects most objects as soon as they become unreachable, but is not guaranteed to collect garbage containing circular references. See the documentation of the gc module for information on controlling the collection of cyclic garbage. Other implementations act differently and CPython may change. Do not depend on immediate finalization of objects when they become unreachable (so you should always close files explicitly).

Note that the use of the implementation’s tracing or debugging facilities may keep objects alive that would normally be collectable. Also note that catching an exception with a ‘`try…except`’ statement may keep objects alive.

Some objects contain references to “`external`” resources such as open files or windows. It is understood that these resources are freed when the object is garbage-collected, but since garbage collection is not guaranteed to happen, such objects also provide an explicit way to release the external resource, usually a `close()` method. Programs are strongly recommended to explicitly close such objects. The ‘`try…finally`’ statement and the ‘with’ statement provide convenient ways to do this.

Some objects contain references to other objects; these are called containers. Examples of containers are tuples, lists and dictionaries. The references are part of a container’s value. In most cases, when we talk about the value of a container, we imply the values, not the identities of the contained objects; however, when we talk about the mutability of a container, only the identities of the immediately contained objects are implied. So, if an immutable container (like a tuple) contains a reference to a mutable object, its value changes if that mutable object is changed.

Types affect almost all aspects of object behavior. Even the importance of object identity is affected in some sense: for immutable types, operations that compute new values may actually return a reference to any existing object with the same type and value, while for mutable objects this is not allowed. E.g., after a = 1; b = 1, a and b may or may not refer to the same object with the value one, depending on the implementation, but after c = []; d = [], c and d are guaranteed to refer to two different, unique, newly created empty lists. (Note that c = d = [] assigns the same object to both c and d.)


### ===🗝 3.2. The standard type hierarchy

Below is a list of the types that are built into Python. Extension modules (written in C, Java, or other languages, depending on the implementation) can define additional types. Future versions of Python may add types to the type hierarchy (e.g., rational numbers, efficiently stored arrays of integers, etc.), although such additions will often be provided via the standard library instead.

Some of the type descriptions below contain a paragraph listing ‘special attributes.’ These are attributes that provide access to the implementation and are not intended for general use. Their definition may change in the future.

#### None
This type has a single value. There is a single object with this value. This object is accessed through the built-in name None. It is used to signify the absence of a value in many situations, e.g., it is returned from functions that don’t explicitly return anything. Its truth value is false.

#### NotImplemented
This type has a single value. There is a single object with this value. This object is accessed through the built-in name NotImplemented. Numeric methods and rich comparison methods should return this value if they do not implement the operation for the operands provided. (The interpreter will then try the reflected operation, or some other fallback, depending on the operator.) It should not be evaluated in a boolean context.

See Implementing the arithmetic operations for more details.


Changed in version 3.9: Evaluating NotImplemented in a boolean context is deprecated. While it currently evaluates as true, it will emit a DeprecationWarning. It will raise a TypeError in a future version of Python.

#### Ellipsis
This type has a single value. There is a single object with this value. This object is accessed through the literal ... or the built-in name Ellipsis. Its truth value is true.


#### Numbers

➡ `numbers.Number`
These are created by numeric literals and returned as results by arithmetic operators and arithmetic built-in functions. Numeric objects are immutable; once created their value never changes. Python numbers are of course strongly related to mathematical numbers, but subject to the limitations of numerical representation in computers.

The string representations of the numeric classes, computed by __repr__() and __str__(), have the following properties:
•They are valid numeric literals which, when passed to their class constructor, produce an object having the value of the original numeric.
•The representation is in base 10, when possible.
•Leading zeros, possibly excepting a single zero before a decimal point, are not shown.
•Trailing zeros, possibly excepting a single zero after a decimal point, are not shown.
•A sign is shown only when the number is negative.

Python distinguishes between integers, floating point numbers, and complex numbers:
➡ `numbers.Integral`
These represent elements from the mathematical set of integers (positive and negative).

There are two types of integers:
✅ `Integers (int)`
These represent numbers in an unlimited range, subject to available (virtual) memory only. For the purpose of shift and mask operations, a binary representation is assumed, and negative numbers are represented in a variant of 2’s complement which gives the illusion of an infinite string of sign bits extending to the left.
✅ `Booleans (bool)`
These represent the truth values False and True. The two objects representing the values False and True are the only Boolean objects. The Boolean type is a subtype of the integer type, and Boolean values behave like the values 0 and 1, respectively, in almost all contexts, the exception being that when converted to a string, the strings "False" or "True" are returned, respectively.

The rules for integer representation are intended to give the most meaningful interpretation of shift and mask operations involving negative integers.
➡ `numbers.Real (float)`
These represent machine-level double precision floating point numbers. You are at the mercy of the underlying machine architecture (and C or Java implementation) for the accepted range and handling of overflow. Python does not support single-precision floating point numbers; the savings in processor and memory usage that are usually the reason for using these are dwarfed by the overhead of using objects in Python, so there is no reason to complicate the language with two kinds of floating point numbers.
➡ `numbers.Complex (complex)`
These represent complex numbers as a pair of machine-level double precision floating point numbers. The same caveats apply as for floating point numbers. The real and imaginary parts of a complex number z can be retrieved through the read-only attributes z.real and z.imag.

#### Sequences
These represent finite ordered sets indexed by non-negative numbers. The built-in function `len()` returns the number of items of a sequence. When the length of a sequence is n, the index set contains the numbers 0, 1, …, n-1. Item i of sequence a is selected by `a[i]`.

Sequences also support slicing: `a[i:j]` selects all items with index k such that i <= k < j. When used as an expression, a slice is a sequence of the same type. This implies that the index set is renumbered so that it starts at 0.

Some sequences also support “extended slicing” with a third “step” parameter: `a[i:j:k]` selects all items of a with index x where x = i + n * k, n >= 0 and i <= x < j.

Sequences are distinguished according to their mutability:

#### Immutable sequences
An object of an immutable sequence type cannot change once it is created. (If the object contains references to other objects, these other objects may be mutable and may be changed; however, the collection of objects directly referenced by an immutable object cannot change.)

The following types are immutable sequences:
➡ `Strings`
A string is a sequence of values that represent Unicode code points. All the code points in the range U+0000 - U+10FFFF can be represented in a string. Python doesn’t have a char type; instead, every code point in the string is represented as a string object with length 1. The built-in function `ord()` converts a code point from its string form to an integer in the range 0 - 10FFFF; `chr()` converts an integer in the range 0 - 10FFFF to the corresponding length 1 string object. `str.encode()` can be used to convert a `str` to `bytes` using the given text encoding, and `bytes.decode()` can be used to achieve the opposite.
➡ `Tuples`
The items of a `tuple` are arbitrary Python objects. Tuples of two or more items are formed by comma-separated lists of expressions. A tuple of one item (a ‘singleton’) can be formed by affixing a comma to an expression (an expression by itself does not create a tuple, since parentheses must be usable for grouping of expressions). An `empty tuple` can be formed by an empty pair of parentheses.

    type((()))  # <class 'tuple'>
    type(())    # <class 'tuple'>
    type((1))   # <class 'int'>
    type((1,))  # <class 'tuple'>

➡ `Bytes`
A bytes object is an immutable array. The items are 8-bit bytes, represented by integers in the range 0 <= x < 256. Bytes literals (like b'abc') and the built-in `bytes() `constructor can be used to create bytes objects. Also, bytes objects can be decoded to strings via the `decode()` method.

#### Mutable sequences
Mutable sequences can be changed after they are created. The subscription and slicing notations can be used as the target of assignment and del (delete) statements.

There are currently two intrinsic mutable sequence types:
➡ `Lists`
The items of a list are arbitrary Python objects. Lists are formed by placing a comma-separated list of expressions in square brackets. (Note that there are no special cases needed to form lists of length 0 or 1.)
➡ `Byte Arrays`
A bytearray object is a mutable array. They are created by the built-in bytearray() constructor. Aside from being mutable (and hence unhashable), byte arrays otherwise provide the same interface and functionality as immutable bytes objects.

The extension module array provides an additional example of a mutable sequence type, as does the collections module.

#### Set types
These represent unordered, finite sets of unique, immutable objects. As such, they cannot be indexed by any subscript. However, they can be iterated over, and the built-in function `len()` returns the number of items in a set. Common uses for sets are fast membership testing, removing duplicates from a sequence, and computing mathematical operations such as intersection, union, difference, and symmetric difference.

For set elements, the same immutability rules apply as for dictionary keys. Note that numeric types obey the normal rules for numeric comparison: if two numbers compare equal (e.g., 1 and 1.0), only one of them can be contained in a set.

    A = {1,2,2.0}   # {1, 2}
    B = {1,1.0,2}   # {1, 2}
    A == B          # True
    C = {"a", "b"}
    D = {"a", "b"}
    D == D          # True

There are currently two intrinsic set types:

✅ `Sets`
These represent a mutable set. They are created by the built-in `set()` constructor and can be modified afterwards by several methods, such as `add()`.
✅ `Frozen sets`
These represent an immutable set. They are created by the built-in `frozenset()` constructor. As a frozenset is immutable and hashable, it can be used again as an element of another set, or as a dictionary key.

#### Mappings - dict
These represent finite sets of objects indexed by arbitrary index sets. The subscript notation `a[k]` selects the item indexed by `k` from the mapping `a`; this can be used in expressions and as the target of assignments or `del` statements. The built-in function `len()` returns the number of items in a mapping.

There is currently a single intrinsic mapping type:

➡ ` Dictionaries`
These represent finite sets of objects indexed by nearly arbitrary values. The only types of values not acceptable as keys are values containing lists or dictionaries or other mutable types that are compared by value rather than by object identity, the reason being that the efficient implementation of dictionaries requires a key’s hash value to remain constant. Numeric types used for keys obey the normal rules for numeric comparison: if two numbers compare equal (e.g., 1 and 1.0) then they can be used interchangeably to index the same dictionary entry.

Dictionaries preserve insertion order, meaning that keys will be produced in the same order they were added sequentially over the dictionary. Replacing an existing key does not change the order, however removing a key and re-inserting it will add it to the end instead of keeping its old place.

Dictionaries are mutable; they can be created by the `{...}` notation (see section Dictionary displays).

The extension modules `dbm.ndbm` and `dbm.gnu` provide additional examples of mapping types, as does the collections module.


Changed in version 3.7: Dictionaries did not preserve insertion order in versions of Python before 3.6. In CPython 3.6, insertion order was preserved, but it was considered an implementation detail at that time rather than a language guarantee.

#### Callable types
These are the types to which the function call operation (see section Calls) can be applied:

➡ `User-defined functions`
A user-defined function object is created by a function definition (see section Function definitions). It should be called with an argument list containing the same number of items as the function’s formal parameter list.

Special attributes: 

↪ `__doc__`
    Writable  The function’s documentation string, or None if unavailable; not inherited by subclasses. 
↪ `__name__`
    Writable  The function’s name. 
↪ `__qualname__` 
    Writable New in version 3.3. The function’s qualified name.
↪ `__module__`
    Writable  The name of the module the function was defined in, or None if unavailable. 
↪ `__defaults__`
    Writable  A tuple containing default argument values for those arguments that have defaults, or None if no arguments have a default value. 
↪ `__code__`
    Writable  The code object representing the compiled function body. 
↪ `__globals__ `
     A reference to the dictionary that holds the function’s global variables — the global namespace of the module in which the function was defined.
↪ `__dict__`
    Writable  The namespace supporting arbitrary function attributes. 
↪ `__closure__ `
     None or a tuple of cells that contain bindings for the function’s free variables. See below for information on the `cell_contents` attribute.
↪ `__annotations__`
    Writable  A dict containing annotations of parameters. The keys of the dict are the parameter names, and 'return' for the return annotation, if provided. 
↪ `__kwdefaults__`
    Writable  A dict containing defaults for keyword-only parameters. 

Most of the attributes labelled “Writable” check the type of the assigned value.

Function objects also support getting and setting arbitrary attributes, which can be used, for example, to attach metadata to functions. Regular attribute dot-notation is used to get and set such attributes. Note that the current implementation only supports function attributes on user-defined functions. Function attributes on built-in functions may be supported in the future.

A cell object has the attribute `cell_contents`. This can be used to get the value of the cell, as well as set the value.

Additional information about a function’s definition can be retrieved from its code object; see the description of internal types below. The cell type can be accessed in the types module.

➡ `Instance methods`
An instance method object combines a class, a class instance and any callable object (normally a user-defined function).

Special read-only attributes: __self__ is the class instance object, __func__ is the function object; __doc__ is the method’s documentation (same as __func__.__doc__); __name__ is the method name (same as __func__.__name__); __module__ is the name of the module the method was defined in, or None if unavailable.

Methods also support accessing (but not setting) the arbitrary function attributes on the underlying function object.

User-defined method objects may be created when getting an attribute of a class (perhaps via an instance of that class), if that attribute is a user-defined function object or a class method object.

When an instance method object is created by retrieving a user-defined function object from a class via one of its instances, its __self__ attribute is the instance, and the method object is said to be bound. The new method’s __func__ attribute is the original function object.

When an instance method object is created by retrieving a class method object from a class or instance, its __self__ attribute is the class itself, and its __func__ attribute is the function object underlying the class method.

When an instance method object is called, the underlying function (__func__) is called, inserting the class instance (__self__) in front of the argument list. For instance, when C is a class which contains a definition for a function f(), and x is an instance of C, calling x.f(1) is equivalent to calling C.f(x, 1).

When an instance method object is derived from a class method object, the “class instance” stored in __self__ will actually be the class itself, so that calling either x.f(1) or C.f(1) is equivalent to calling f(C,1) where f is the underlying function.

Note that the transformation from function object to instance method object happens each time the attribute is retrieved from the instance. In some cases, a fruitful optimization is to assign the attribute to a local variable and call that local variable. Also notice that this transformation only happens for user-defined functions; other callable objects (and all non-callable objects) are retrieved without transformation. It is also important to note that user-defined functions which are attributes of a class instance are not converted to bound methods; this only happens when the function is an attribute of the class.

➡ `Generator functions`
A function or method which uses the yield statement (see section The yield statement) is called a generator function. Such a function, when called, always returns an iterator object which can be used to execute the body of the function: calling the iterator’s iterator.__next__() method will cause the function to execute until it provides a value using the yield statement. When the function executes a return statement or falls off the end, a StopIteration exception is raised and the iterator will have reached the end of the set of values to be returned.

➡ `Coroutine functions`
A function or method which is defined using async def is called a coroutine function. Such a function, when called, returns a coroutine object. It may contain await expressions, as well as async with and async for statements. See also the Coroutine Objects section.
Asynchronous generator functions
A function or method which is defined using async def and which uses the yield statement is called a asynchronous generator function. Such a function, when called, returns an asynchronous iterator object which can be used in an async for statement to execute the body of the function.

Calling the asynchronous iterator’s aiterator.__anext__() method will return an awaitable which when awaited will execute until it provides a value using the yield expression. When the function executes an empty return statement or falls off the end, a StopAsyncIteration exception is raised and the asynchronous iterator will have reached the end of the set of values to be yielded.

➡ `Built-in functions`
A built-in function object is a wrapper around a C function. Examples of built-in functions are `len()` and math.sin() (math is a standard built-in module). The number and type of the arguments are determined by the C function. Special read-only attributes: __doc__ is the function’s documentation string, or None if unavailable; __name__ is the function’s name; __self__ is set to None (but see the next item); __module__ is the name of the module the function was defined in or None if unavailable.

➡ `Built-in methods`
This is really a different disguise of a built-in function, this time containing an object passed to the C function as an implicit extra argument. An example of a built-in method is alist.append(), assuming alist is a list object. In this case, the special read-only attribute __self__ is set to the object denoted by alist.
ClassesClasses are callable. These objects normally act as factories for new instances of themselves, but variations are possible for class types that override __new__(). The arguments of the call are passed to __new__() and, in the typical case, to __init__() to initialize the new instance.Class InstancesInstances of arbitrary classes can be made callable by defining a __call__() method in their class.

#### Modules
Modules are a basic organizational unit of Python code, and are created by the import system as invoked either by the import statement, or by calling functions such as `importlib.import_module()` and built-in `__import__()`. A module object has a namespace implemented by a dictionary object (this is the dictionary referenced by the __globals__ attribute of functions defined in the module). Attribute references are translated to lookups in this dictionary, e.g., m.x is equivalent to `m.__dict__["x"]`. A module object does not contain the code object used to initialize the module (since it isn’t needed once the initialization is done).

Attribute assignment updates the module’s namespace dictionary, e.g., m.x = 1 is equivalent to `m.__dict__["x"] = 1`.

Predefined (writable) attributes: __name__ is the module’s name; __doc__ is the module’s documentation string, or None if unavailable; __annotations__ (optional) is a dictionary containing variable annotations collected during module body execution; __file__ is the pathname of the file from which the module was loaded, if it was loaded from a file. The __file__ attribute may be missing for certain types of modules, such as C modules that are statically linked into the interpreter; for extension modules loaded dynamically from a shared library, it is the pathname of the shared library file.

Special read-only attribute: __dict__ is the module’s namespace as a dictionary object.


CPython implementation detail: Because of the way CPython clears module dictionaries, the module dictionary will be cleared when the module falls out of scope even if the dictionary still has live references. To avoid this, copy the dictionary or keep the module around while using its dictionary directly.

#### Custom classes
Custom class types are typically created by class definitions (see section Class definitions). A class has a namespace implemented by a dictionary object. Class attribute references are translated to lookups in this dictionary, e.g., C.x is translated to `C.__dict__["x"]` (although there are a number of hooks which allow for other means of locating attributes). When the attribute name is not found there, the attribute search continues in the base classes. This search of the base classes uses the C3 method resolution order which behaves correctly even in the presence of ‘diamond’ inheritance structures where there are multiple inheritance paths leading back to a common ancestor. Additional details on the *C3 MRO* used by Python can be found in the documentation accompanying the 2.3 release at https://www.python.org/download/releases/2.3/mro/.


When a class attribute reference (for class C, say) would yield a class method object, it is transformed into an instance method object whose __self__ attribute is C. When it would yield a static method object, it is transformed into the object wrapped by the static method object. See section Implementing Descriptors for another way in which attributes retrieved from a class may differ from those actually contained in its __dict__.

Class attribute assignments update the class’s dictionary, never the dictionary of a base class.

A class object can be called (see above) to yield a class instance (see below).

Special attributes: __name__ is the class name; __module__ is the module name in which the class was defined; __dict__ is the dictionary containing the class’s namespace; __bases__ is a tuple containing the base classes, in the order of their occurrence in the base class list; __doc__ is the class’s documentation string, or None if undefined; __annotations__ (optional) is a dictionary containing variable annotations collected during class body execution.

#### Class instances
A class instance is created by calling a class object (see above). A class instance has a namespace implemented as a dictionary which is the first place in which attribute references are searched. When an attribute is not found there, and the instance’s class has an attribute by that name, the search continues with the class attributes. If a class attribute is found that is a user-defined function object, it is transformed into an instance method object whose __self__ attribute is the instance. Static method and class method objects are also transformed; see above under “Classes”. See section Implementing Descriptors for another way in which attributes of a class retrieved via its instances may differ from the objects actually stored in the class’s __dict__. If no class attribute is found, and the object’s class has a __getattr__() method, that is called to satisfy the lookup.

Attribute assignments and deletions update the instance’s dictionary, never a class’s dictionary. If the class has a __setattr__() or __delattr__() method, this is called instead of updating the instance dictionary directly.

Class instances can pretend to be numbers, sequences, or mappings if they have methods with certain special names. See section Special method names.

Special attributes: __dict__ is the attribute dictionary; __class__ is the instance’s class.

#### I/O objects (also known as file objects)
A file object represents an open file. Various shortcuts are available to create file objects: the `open()` built-in function, and also `os.popen()`, `os.fdopen()`, and the `makefile()` method of socket objects (and perhaps by other functions or methods provided by extension modules).

The objects sys.stdin, sys.stdout and sys.stderr are initialized to file objects corresponding to the interpreter’s standard input, output and error streams; they are all open in text mode and therefore follow the interface defined by the io.TextIOBase abstract class.

#### Internal types
A few types used internally by the interpreter are exposed to the user. Their definitions may change with future versions of the interpreter, but they are mentioned here for completeness.

✅ `Code objects`
Code objects represent byte-compiled executable Python code, or bytecode. The difference between a code object and a function object is that the function object contains an explicit reference to the function’s globals (the module in which it was defined), while a code object contains no context; also the default argument values are stored in the function object, not in the code object (because they represent values calculated at run-time). Unlike function objects, code objects are immutable and contain no references (directly or indirectly) to mutable objects.

Special read-only attributes: 

↪ `co_name` gives the function name;
↪ `co_argcount` is the total number of positional arguments (including positional-only arguments and arguments with default values);
↪ `co_posonlyargcount` is the number of positional-only arguments (including arguments with default values); 
↪ `co_kwonlyargcount` is the number of keyword-only arguments (including arguments with default values); 
↪ `co_nlocals` is the number of local variables used by the function (including arguments); 
↪ `co_varnames` is a tuple containing the names of the local variables (starting with the argument names);
↪ `co_cellvars` is a tuple containing the names of local variables that are referenced by nested functions;
↪ `co_freevars` is a tuple containing the names of free variables;
↪ `co_code` is a string representing the sequence of bytecode instructions;
↪ `co_consts` is a tuple containing the literals used by the bytecode;
↪ `co_names` is a tuple containing the names used by the bytecode;
↪ `co_filename` is the filename from which the code was compiled;
↪ `co_firstlineno` is the first line number of the function;
↪ `co_lnotab` is a string encoding the mapping from bytecode offsets to line numbers (for details see the source code of the interpreter);
↪ `co_stacksize` is the required stack size;
↪ `co_flags` is an integer encoding a number of flags for the interpreter.

The following flag bits are defined for ↪ `co_flags` : bit 0x04 is set if the function uses the `*arguments` syntax to accept an arbitrary number of positional arguments; bit 0x08 is set if the function uses the `**keywords` syntax to accept arbitrary keyword arguments; bit 0x20 is set if the function is a generator.

Future feature declarations (from __future__ import division) also use bits in ↪ `co_flags` to indicate whether a code object was compiled with a particular feature enabled: bit 0x2000 is set if the function was compiled with future division enabled; bits 0x10 and 0x1000 were used in earlier versions of Python.

Other bits in ↪ `co_flags` are reserved for internal use.

If a code object represents a function, the first item in ↪ `co_consts` is the documentation string of the function, or `None` if undefined.

✅ `Frame objects`
Frame objects represent execution frames. They may occur in traceback objects (see below), and are also passed to registered trace functions.

Special read-only attributes: 
↪ `f_back` is to the previous stack frame (towards the caller), or `None` if this is the bottom stack frame; 
↪ `f_code` is the code object being executed in this frame;
↪ `f_locals` is the dictionary used to look up local variables; 
↪ `f_globals` is used for global variables; 
↪ `f_builtins` is used for built-in (intrinsic) names; 
↪ `f_lasti` gives the precise instruction (this is an index into the bytecode string of the code object).

Accessing f_code raises an auditing event `object.__getattr__` with arguments obj and "f_code".

Special writable attributes: `f_trace`, if not `None`, is a function called for various events during code execution (this is used by the debugger). Normally an event is triggered for each new source line - this can be disabled by setting f_trace_lines to False.

Implementations may allow per-opcode events to be requested by setting f_trace_opcodes to True. Note that this may lead to undefined interpreter behaviour if exceptions raised by the trace function escape to the function being traced.

`f_lineno` is the current line number of the frame — writing to this from within a trace function jumps to the given line (only for the bottom-most frame). A debugger can implement a Jump command (aka Set Next Statement) by writing to `f_lineno`.

Frame objects support one method:

➡ `frame.clear()`
This method clears all references to local variables held by the frame. Also, if the frame belonged to a generator, the generator is finalized. This helps break reference cycles involving frame objects (for example when catching an exception and storing its traceback for later use).

`RuntimeError` is raised if the frame is currently executing.

New in version 3.4.

✅ `Traceback objects`
Traceback objects represent a stack trace of an exception. A traceback object is implicitly created when an exception occurs, and may also be explicitly created by calling types.TracebackType.

For implicitly created tracebacks, when the search for an exception handler unwinds the execution stack, at each unwound level a traceback object is inserted in front of the current traceback. When an exception handler is entered, the stack trace is made available to the program. (See section The try statement.) It is accessible as the third item of the tuple returned by sys.exc_info(), and as the __traceback__ attribute of the caught exception.

When the program contains no suitable handler, the stack trace is written (nicely formatted) to the standard error stream; if the interpreter is interactive, it is also made available to the user as sys.last_traceback.

For explicitly created tracebacks, it is up to the creator of the traceback to determine how the tb_next attributes should be linked to form a full stack trace.

Special read-only attributes: tb_frame points to the execution frame of the current level; tb_lineno gives the line number where the exception occurred; tb_lasti indicates the precise instruction. The line number and last instruction in the traceback may differ from the line number of its frame object if the exception occurred in a try statement with no matching except clause or with a finally clause.

Accessing tb_frame raises an auditing event object.__getattr__ with arguments obj and "tb_frame".

Special writable attribute: tb_next is the next level in the stack trace (towards the frame where the exception occurred), or None if there is no next level.

Changed in version 3.7: Traceback objects can now be explicitly instantiated from Python code, and the tb_next attribute of existing instances can be updated.

✅ `Slice objects`
Slice objects are used to represent slices for __getitem__() methods. They are also created by the built-in slice() function.

Special read-only attributes: start is the lower bound; stop is the upper bound; step is the step value; each is None if omitted. These attributes can have any type.

Slice objects support one method:
slice.indices(self, length)
This method takes a single integer argument length and computes information about the slice that the slice object would describe if applied to a sequence of length items. It returns a tuple of three integers; respectively these are the start and stop indices and the step or stride length of the slice. Missing or out-of-bounds indices are handled in a manner consistent with regular slices.

✅ `Static method objects`
Static method objects provide a way of defeating the transformation of function objects to method objects described above. A static method object is a wrapper around any other object, usually a user-defined method object. When a static method object is retrieved from a class or a class instance, the object actually returned is the wrapped object, which is not subject to any further transformation. Static method objects are not themselves callable, although the objects they wrap usually are. Static method objects are created by the built-in staticmethod() constructor.

✅ `Class method objects`
A class method object, like a static method object, is a wrapper around another object that alters the way in which that object is retrieved from classes and class instances. The behaviour of class method objects upon such retrieval is described above, under “User-defined methods”. Class method objects are created by the built-in classmethod() constructor.

### ===🗝 3.3. Special method names - Magic Methods

A class can implement certain operations that are invoked by special syntax (such as arithmetic operations or subscripting and slicing) by defining methods with special names. This is Python’s approach to operator overloading, allowing classes to define their own behavior with respect to language operators. For instance, if a class defines a method named __getitem__(), and x is an instance of this class, then `x[i]` is roughly equivalent to `type(x).__getitem__(x, i)`. Except where mentioned, attempts to execute an operation raise an exception when no appropriate method is defined (typically AttributeError or TypeError).

Setting a special method to None indicates that the corresponding operation is not available. For example, if a class sets __iter__() to None, the class is not iterable, so calling `iter()` on its instances will raise a TypeError (without falling back to __getitem__()). [2]

When implementing a class that emulates any built-in type, it is important that the emulation only be implemented to the degree that it makes sense for the object being modelled. For example, some sequences may work well with retrieval of individual elements, but extracting a slice may not make sense. (One example of this is the NodeList interface in the W3C’s Document Object Model.)


#### 3.3.1. Basic customization

✅ `object.__new__(cls[, ...])`
Called to create a new instance of class cls. __new__() is a static method (special-cased so you need not declare it as such) that takes the class of which an instance was requested as its first argument. The remaining arguments are those passed to the object constructor expression (the call to the class). The return value of __new__() should be the new object instance (usually an instance of cls).

Typical implementations create a new instance of the class by invoking the superclass’s __new__() method using `super().__new__(cls[, ...])` with appropriate arguments and then modifying the newly-created instance as necessary before returning it.

If __new__() is invoked during object construction and it returns an instance or subclass of cls, then the new instance’s __init__() method will be invoked like `__init__(self[, ...])`, where self is the new instance and the remaining arguments are the same as were passed to the object constructor.

If __new__() does not return an instance of cls, then the new instance’s __init__() method will not be invoked.

__new__() is intended mainly to allow subclasses of immutable types (like int, str, or tuple) to customize instance creation. It is also commonly overridden in custom metaclasses in order to customize class creation.

✅ `object.__init__(self[, ...])`
Called after the instance has been created (by __new__()), but before it is returned to the caller. The arguments are those passed to the class constructor expression. If a base class has an __init__() method, the derived class’s __init__() method, if any, must explicitly call it to ensure proper initialization of the base class part of the instance; for example: `super().__init__([args...])`.

Because __new__() and __init__() work together in constructing objects (__new__() to create it, and __init__() to customize it), no non-None value may be returned by __init__(); doing so will cause a TypeError to be raised at runtime.

✅ `object.__del__(self)`
Called when the instance is about to be destroyed. This is also called a finalizer or (improperly) a destructor. If a base class has a __del__() method, the derived class’s __del__() method, if any, must explicitly call it to ensure proper deletion of the base class part of the instance.

It is possible (though not recommended!) for the __del__() method to postpone destruction of the instance by creating a new reference to it. This is called object resurrection. It is implementation-dependent whether __del__() is called a second time when a resurrected object is about to be destroyed; the current CPython implementation only calls it once.

It is not guaranteed that __del__() methods are called for objects that still exist when the interpreter exits.

Note:
 del x doesn’t directly call x.__del__() — the former decrements the reference count for x by one, and the latter is only called when x’s reference count reaches zero.
 
CPython implementation detail: It is possible for a reference cycle to prevent the reference count of an object from going to zero. In this case, the cycle will be later detected and deleted by the cyclic garbage collector. A common cause of reference cycles is when an exception has been caught in a local variable. The frame’s locals then reference the exception, which references its own traceback, which references the locals of all frames caught in the traceback.

See also:  Documentation for the gc module.
 

Warning:

 Due to the precarious circumstances under which __del__() methods are invoked, exceptions that occur during their execution are ignored, and a warning is printed to sys.stderr instead. In particular:

•__del__() can be invoked when arbitrary code is being executed, including from any arbitrary thread. If __del__() needs to take a lock or invoke any other blocking resource, it may deadlock as the resource may already be taken by the code that gets interrupted to execute __del__().
•__del__() can be executed during interpreter shutdown. As a consequence, the global variables it needs to access (including other modules) may already have been deleted or set to None. Python guarantees that globals whose name begins with a single underscore are deleted from their module before other globals are deleted; if no other references to such globals exist, this may help in assuring that imported modules are still available at the time when the __del__() method is called.

✅ `object.__repr__(self)`
Called by the `repr()` built-in function to compute the “official” string representation of an object. If at all possible, this should look like a valid Python expression that could be used to recreate an object with the same value (given an appropriate environment). If this is not possible, a string of the form `<...some useful description...>` should be returned. The return value must be a string object. If a class defines __repr__() but not __str__(), then __repr__() is also used when an “informal” string representation of instances of that class is required.

This is typically used for debugging, so it is important that the representation is information-rich and unambiguous.

✅ `object.__str__(self)`
Called by `str(object)` and the built-in functions `format()` and `print()` to compute the “informal” or nicely printable string representation of an object. The return value must be a string object.

This method differs from object.__repr__() in that there is no expectation that __str__() return a valid Python expression: a more convenient or concise representation can be used.

The default implementation defined by the built-in type object calls object.__repr__().

✅ `object.__bytes__(self)`
Called by bytes to compute a byte-string representation of an object. This should return a bytes object.

✅ `object.__format__(self, format_spec)`
Called by the `format()` built-in function, and by extension, evaluation of formatted string literals and the `str.format()` method, to produce a “formatted” string representation of an object. The format_spec argument is a string that contains a description of the formatting options desired. The interpretation of the format_spec argument is up to the type implementing __format__(), however most classes will either delegate formatting to one of the built-in types, or use a similar formatting option syntax.

See Format Specification Mini-Language for a description of the standard formatting syntax.

The return value must be a string object.


Changed in version 3.4: The __format__ method of object itself raises a TypeError if passed any non-empty string.


Changed in version 3.7: `object.__format__(x, '')` is now equivalent to `str(x)` rather than `format(str(x), '')`.

✅ `object.__lt__(self, other)`
✅ `object.__le__(self, other)`
✅ `object.__eq__(self, other)`
✅ `object.__ne__(self, other)`
✅ `object.__gt__(self, other)`
✅ `object.__ge__(self, other)`
These are the so-called “rich comparison” methods. The correspondence between operator symbols and method names is as follows: 

1. x < y calls x.__lt__(y), 
2. x<=y calls x.__le__(y), 
3. x==y calls x.__eq__(y), 
4. x!=y calls x.__ne__(y), 
5. x>y calls x.__gt__(y), 
6. and x>=y calls x.__ge__(y).

A rich comparison method may return the singleton NotImplemented if it does not implement the operation for a given pair of arguments. By convention, False and True are returned for a successful comparison. However, these methods can return any value, so if the comparison operator is used in a Boolean context (e.g., in the condition of an if statement), Python will call bool() on the value to determine if the result is true or false.

By default, object implements __eq__() by using is, returning NotImplemented in the case of a false comparison: True if x is y else NotImplemented. For __ne__(), by default it delegates to __eq__() and inverts the result unless it is NotImplemented. There are no other implied relationships among the comparison operators or default implementations; for example, the truth of (x < y or x==y) does not imply x<=y. To automatically generate ordering operations from a single root operation, see `functools.total_ordering()`.

See the paragraph on __hash__() for some important notes on creating hashable objects which support custom comparison operations and are usable as dictionary keys.

There are no swapped-argument versions of these methods (to be used when the left argument does not support the operation but the right argument does); rather, __lt__() and __gt__() are each other’s reflection, __le__() and __ge__() are each other’s reflection, and __eq__() and __ne__() are their own reflection. If the operands are of different types, and right operand’s type is a direct or indirect subclass of the left operand’s type, the reflected method of the right operand has priority, otherwise the left operand’s method has priority. Virtual subclassing is not considered.

✅ `object.__hash__(self)`
Called by built-in function `hash()` and for operations on members of hashed collections including set, frozenset, and dict. __hash__() should return an integer. The only required property is that objects which compare equal have the same hash value; it is advised to mix together the hash values of the components of the object that also play a part in comparison of objects by packing them into a tuple and hashing the tuple. Example:


    def __hash__(self):
        return hash((self.name, self.nick, self.color))


Note:
 hash() truncates the value returned from an object’s custom __hash__() method to the size of a `Py_ssize_t`. This is typically 8 bytes on 64-bit builds and 4 bytes on 32-bit builds. If an object’s __hash__() must interoperate on builds of different bit sizes, be sure to check the width on all supported builds. An easy way to do this is with `python -c "import sys; print(sys.hash_info.width)"`.
 

If a class does not define an __eq__() method it should not define a __hash__() operation either; if it defines __eq__() but not __hash__(), its instances will not be usable as items in hashable collections. If a class defines mutable objects and implements an __eq__() method, it should not implement __hash__(), since the implementation of hashable collections requires that a key’s hash value is immutable (if the object’s hash value changes, it will be in the wrong hash bucket).

User-defined classes have __eq__() and __hash__() methods by default; with them, all objects compare unequal (except with themselves) and x.__hash__() returns an appropriate value such that x == y implies both that x is y and hash(x) == hash(y).

A class that overrides __eq__() and does not define __hash__() will have its __hash__() implicitly set to None. When the __hash__() method of a class is None, instances of the class will raise an appropriate TypeError when a program attempts to retrieve their hash value, and will also be correctly identified as unhashable when checking isinstance(obj, collections.abc.Hashable).

If a class that overrides __eq__() needs to retain the implementation of __hash__() from a parent class, the interpreter must be told this explicitly by setting __hash__ = <ParentClass>.__hash__.

If a class that does not override __eq__() wishes to suppress hash support, it should include __hash__ = None in the class definition. A class which defines its own __hash__() that explicitly raises a TypeError would be incorrectly identified as hashable by an isinstance(obj, collections.abc.Hashable) call.

Note:
 By default, the __hash__() values of str and bytes objects are “salted” with an unpredictable random value. Although they remain constant within an individual Python process, they are not predictable between repeated invocations of Python.
 
This is intended to provide protection against a denial-of-service caused by carefully-chosen inputs that exploit the worst case performance of a dict insertion, O(n^2) complexity. See http://www.ocert.org/advisories/ocert-2011-003.html for details.

Changing hash values affects the iteration order of sets. Python has never made guarantees about this ordering (and it typically varies between 32-bit and 64-bit builds).

See also PYTHONHASHSEED.

Changed in version 3.3: Hash randomization is enabled by default.


✅ `object.__bool__(self)`
Called to implement truth value testing and the built-in operation bool(); should return False or True. When this method is not defined, __len__() is called, if it is defined, and the object is considered true if its result is nonzero. If a class defines neither __len__() nor __bool__(), all its instances are considered true.


#### 3.3.2. Customizing attribute access

The following methods can be defined to customize the meaning of attribute access (use of, assignment to, or deletion of x.name) for class instances.

✅ `object.__getattr__(self, name)`
Called when the default attribute access fails with an AttributeError (either __getattribute__() raises an AttributeError because name is not an instance attribute or an attribute in the class tree for self; or __get__() of a name property raises AttributeError). This method should either return the (computed) attribute value or raise an AttributeError exception.

Note that if the attribute is found through the normal mechanism, __getattr__() is not called. (This is an intentional asymmetry between __getattr__() and __setattr__().) This is done both for efficiency reasons and because otherwise __getattr__() would have no way to access other attributes of the instance. Note that at least for instance variables, you can fake total control by not inserting any values in the instance attribute dictionary (but instead inserting them in another object). See the __getattribute__() method below for a way to actually get total control over attribute access.

✅ `object.__getattribute__(self, name)`
Called unconditionally to implement attribute accesses for instances of the class. If the class also defines __getattr__(), the latter will not be called unless __getattribute__() either calls it explicitly or raises an AttributeError. This method should return the (computed) attribute value or raise an AttributeError exception. In order to avoid infinite recursion in this method, its implementation should always call the base class method with the same name to access any attributes it needs, for example, object.__getattribute__(self, name).

Note:
 This method may still be bypassed when looking up special methods as the result of implicit invocation via language syntax or built-in functions. See Special method lookup.
 

For certain sensitive attribute accesses, raises an auditing event object.__getattr__ with arguments obj and name.

✅ `object.__setattr__(self, name, value)`
Called when an attribute assignment is attempted. This is called instead of the normal mechanism (i.e. store the value in the instance dictionary). name is the attribute name, value is the value to be assigned to it.

If __setattr__() wants to assign to an instance attribute, it should call the base class method with the same name, for example, object.__setattr__(self, name, value).

For certain sensitive attribute assignments, raises an auditing event object.__setattr__ with arguments obj, name, value.

✅ `object.__delattr__(self, name)`
Like __setattr__() but for attribute deletion instead of assignment. This should only be implemented if del obj.name is meaningful for the object.

For certain sensitive attribute deletions, raises an auditing event object.__delattr__ with arguments obj and name.

✅ `object.__dir__(self)`
Called when dir() is called on the object. A sequence must be returned. dir() converts the returned sequence to a list and sorts it.


#### 3.3.2.1. Customizing module attribute access

Special names __getattr__ and __dir__ can be also used to customize access to module attributes. The __getattr__ function at the module level should accept one argument which is the name of an attribute and return the computed value or raise an AttributeError. If an attribute is not found on a module object through the normal lookup, i.e. object.__getattribute__(), then __getattr__ is searched in the module __dict__ before raising an AttributeError. If found, it is called with the attribute name and the result is returned.

The __dir__ function should accept no arguments, and return a sequence of strings that represents the names accessible on module. If present, this function overrides the standard dir() search on a module.

For a more fine grained customization of the module behavior (setting attributes, properties, etc.), one can set the __class__ attribute of a module object to a subclass of `types.ModuleType`. For example:


    import sys
    from types import ModuleType

    class VerboseModule(ModuleType):
        def __repr__(self):
            return f'Verbose {self.__name__}'

        def __setattr__(self, attr, value):
            print(f'Setting {attr}...')
            super().__setattr__(attr, value)

    sys.modules[__name__].__class__ = VerboseModule


Note:
 Defining module __getattr__ and setting module __class__ only affect lookups made using the attribute access syntax – directly accessing the module globals (whether by code within the module, or via a reference to the module’s globals dictionary) is unaffected.
 

Changed in version 3.5: __class__ module attribute is now writable.

New in version 3.7: __getattr__ and __dir__ module attributes.

See also:
 PEP 562 - Module __getattr__ and __dir__Describes the __getattr__ and __dir__ functions on modules.

#### 3.3.2.2. Implementing Descriptors

The following methods only apply when an instance of the class containing the method (a so-called descriptor class) appears in an owner class (the descriptor must be in either the owner’s class dictionary or in the class dictionary for one of its parents). In the examples below, “the attribute” refers to the attribute whose name is the key of the property in the owner class’ __dict__.

✅ `object.__get__(self, instance, owner=None)`
Called to get the attribute of the owner class (class attribute access) or of an instance of that class (instance attribute access). The optional owner argument is the owner class, while instance is the instance that the attribute was accessed through, or None when the attribute is accessed through the owner.

This method should return the computed attribute value or raise an AttributeError exception.

PEP 252 specifies that __get__() is callable with one or two arguments. Python’s own built-in descriptors support this specification; however, it is likely that some third-party tools have descriptors that require both arguments. Python’s own __getattribute__() implementation always passes in both arguments whether they are required or not.

✅ `object.__set__(self, instance, value)`
Called to set the attribute on an instance instance of the owner class to a new value, value.

Note, adding __set__() or __delete__() changes the kind of descriptor to a “data descriptor”. See Invoking Descriptors for more details.

✅ `object.__delete__(self, instance)`
Called to delete the attribute on an instance instance of the owner class.

✅ `object.__set_name__(self, owner, name)`
Called at the time the owning class owner is created. The descriptor has been assigned to name.

Note:
 __set_name__() is only called implicitly as part of the type constructor, so it will need to be called explicitly with the appropriate parameters when a descriptor is added to a class after initial creation:
 

    class A:
       pass
    descr = custom_descriptor()
    A.attr = descr
    descr.__set_name__(A, 'attr')


See Creating the class object for more details.


New in version 3.6.

The attribute __objclass__ is interpreted by the inspect module as specifying the class where this object was defined (setting this appropriately can assist in runtime introspection of dynamic class attributes). For callables, it may indicate that an instance of the given type (or a subclass) is expected or required as the first positional argument (for example, CPython sets this attribute for unbound methods that are implemented in C).


#### 3.3.2.3. Invoking Descriptors

In general, a descriptor is an object attribute with “binding behavior”, one whose attribute access has been overridden by methods in the descriptor protocol: __get__(), __set__(), and __delete__(). If any of those methods are defined for an object, it is said to be a descriptor.

The default behavior for attribute access is to get, set, or delete the attribute from an object’s dictionary. For instance, a.x has a lookup chain starting with a.__dict__['x'], then type(a).__dict__['x'], and continuing through the base classes of type(a) excluding metaclasses.

However, if the looked-up value is an object defining one of the descriptor methods, then Python may override the default behavior and invoke the descriptor method instead. Where this occurs in the precedence chain depends on which descriptor methods were defined and how they were called.

The starting point for descriptor invocation is a binding, a.x. How the arguments are assembled depends on a:
➡ `Direct Call`
The simplest and least common call is when user code directly invokes a descriptor method: x.__get__(a).
➡ `Instance Binding`
If binding to an object instance, a.x is transformed into the call: type(a).__dict__['x'].__get__(a, type(a)).
➡ `Class Binding`
If binding to a class, A.x is transformed into the call: A.__dict__['x'].__get__(None, A).
➡ `Super Binding`
If a is an instance of super, then the binding super(B, obj).m() searches obj.__class__.__mro__ for the base class A immediately preceding B and then invokes the descriptor with the call: A.__dict__['m'].__get__(obj, obj.__class__).

For instance bindings, the precedence of descriptor invocation depends on which descriptor methods are defined. A descriptor can define any combination of __get__(), __set__() and __delete__(). If it does not define __get__(), then accessing the attribute will return the descriptor object itself unless there is a value in the object’s instance dictionary. If the descriptor defines __set__() and/or __delete__(), it is a data descriptor; if it defines neither, it is a non-data descriptor. Normally, data descriptors define both __get__() and __set__(), while non-data descriptors have just the __get__() method. Data descriptors with __get__() and __set__() (and/or __delete__()) defined always override a redefinition in an instance dictionary. In contrast, non-data descriptors can be overridden by instances.

Python methods (including staticmethod() and classmethod()) are implemented as non-data descriptors. Accordingly, instances can redefine and override methods. This allows individual instances to acquire behaviors that differ from other instances of the same class.

The property() function is implemented as a data descriptor. Accordingly, instances cannot override the behavior of a property.


#### 3.3.2.4. __slots__

__slots__ allow us to explicitly declare data members (like properties) and deny the creation of __dict__ and __weakref__ (unless explicitly declared in __slots__ or available in a parent.)

The space saved over using __dict__ can be significant. Attribute lookup speed can be significantly improved as well.

✅ `object.__slots__`
This class variable can be assigned a string, iterable, or sequence of strings with variable names used by instances. __slots__ reserves space for the declared variables and prevents the automatic creation of __dict__ and __weakref__ for each instance.



➡ `Notes on using __slots__`

↪ • When inheriting from a class without __slots__, the __dict__ and __weakref__ attribute of the instances will always be accessible.
↪ • Without a __dict__ variable, instances cannot be assigned new variables not listed in the __slots__ definition. Attempts to assign to an unlisted variable name raises AttributeError. If dynamic assignment of new variables is desired, then add '__dict__' to the sequence of strings in the __slots__ declaration.
↪ • Without a __weakref__ variable for each instance, classes defining __slots__ do not support weak references to its instances. If weak reference support is needed, then add '__weakref__' to the sequence of strings in the __slots__ declaration.
↪ • __slots__ are implemented at the class level by creating descriptors (Implementing Descriptors) for each variable name. As a result, class attributes cannot be used to set default values for instance variables defined by __slots__; otherwise, the class attribute would overwrite the descriptor assignment.
↪ • The action of a __slots__ declaration is not limited to the class where it is defined. __slots__ declared in parents are available in child classes. However, child subclasses will get a __dict__ and __weakref__ unless they also define __slots__ (which should only contain names of any additional slots).
↪ • If a class defines a slot also defined in a base class, the instance variable defined by the base class slot is inaccessible (except by retrieving its descriptor directly from the base class). This renders the meaning of the program undefined. In the future, a check may be added to prevent this.
↪ • Nonempty __slots__ does not work for classes derived from “variable-length” built-in types such as int, bytes and tuple.
↪ • Any non-string iterable may be assigned to __slots__. Mappings may also be used; however, in the future, special meaning may be assigned to the values corresponding to each key.
↪ • __class__ assignment works only if both classes have the same __slots__.
↪ • Multiple inheritance with multiple slotted parent classes can be used, but only one parent is allowed to have attributes created by slots (the other bases must have empty slot layouts) - violations raise TypeError.
↪ • If an iterator is used for __slots__ then a descriptor is created for each of the iterator’s values. However, the __slots__ attribute will be an empty iterator.


### ===🗝 3.3.3. Customizing class creation

Whenever a class inherits from another class, __init_subclass__ is called on that class. This way, it is possible to write classes which change the behavior of subclasses. This is closely related to class decorators, but where class decorators only affect the specific class they’re applied to, __init_subclass__ solely applies to future subclasses of the class defining the method.

➡ `classmethod object.__init_subclass__(cls)`
This method is called whenever the containing class is subclassed. cls is then the new subclass. If defined as a normal instance method, this method is implicitly converted to a class method.

Keyword arguments which are given to a new class are passed to the parent’s class __init_subclass__. For compatibility with other classes using __init_subclass__, one should take out the needed keyword arguments and pass the others over to the base class, as in:


    class Philosopher:
        def __init_subclass__(cls, /, default_name, **kwargs):
            super().__init_subclass__(**kwargs)
            cls.default_name = default_name

    class AustralianPhilosopher(Philosopher, default_name="Bruce"):
        pass


The default implementation object.__init_subclass__ does nothing, but raises an error if it is called with any arguments.

Note:
 The metaclass hint metaclass is consumed by the rest of the type machinery, and is never passed to __init_subclass__ implementations. The actual metaclass (rather than the explicit hint) can be accessed as type(cls).
 
New in version 3.6.


### ===🗝 3.3.3.1. Metaclasses

By default, classes are constructed using `type()`. The class body is executed in a new namespace and the class name is bound locally to the result of `type(name, bases, namespace)`.

The class creation process can be customized by passing the metaclass keyword argument in the class definition line, or by inheriting from an existing class that included such an argument. In the following example, both `MyClass` and `MySubclass` are instances of `Meta`:


    class Meta(type):
        pass

    class MyClass(metaclass=Meta):
        pass

    class MySubclass(MyClass):
        pass

Any other keyword arguments that are specified in the class definition are passed through to all metaclass operations described below.

When a class definition is executed, the following steps occur:

• MRO entries are resolved;
• the appropriate metaclass is determined;
• the class namespace is prepared;
• the class body is executed;
• the class object is created.


### ===🗝 3.3.3.2. Resolving MRO entries

If a base that appears in class definition is not an instance of type, then an __mro_entries__ method is searched on it. If found, it is called with the original bases tuple. This method must return a tuple of classes that will be used instead of this base. The tuple may be empty, in such case the original base is ignored.

See also:
 PEP 560 - Core support for typing module and generic types https://www.python.org/dev/peps/pep-3115/
 

### ===🗝 3.3.3.3. Determining the appropriate metaclass

The appropriate metaclass for a class definition is determined as follows:
•if no bases and no explicit metaclass are given, then `type()` is used;
•if an explicit metaclass is given and it is not an instance of `type()`, then it is used directly as the metaclass;
•if an instance of `type()` is given as the explicit metaclass, or bases are defined, then the most derived metaclass is used.

The most derived metaclass is selected from the explicitly specified metaclass (if any) and the metaclasses (i.e. type(cls)) of all specified base classes. The most derived metaclass is one which is a subtype of all of these candidate metaclasses. If none of the candidate metaclasses meets that criterion, then the class definition will fail with TypeError.


### ===🗝 3.3.3.4. Preparing the class namespace

Once the appropriate metaclass has been identified, then the class namespace is prepared. If the metaclass has a __prepare__ attribute, it is called as `namespace = metaclass.__prepare__(name, bases, **kwds)` (where the additional keyword arguments, if any, come from the class definition). The __prepare__ method should be implemented as a `classmethod()`. The namespace returned by __prepare__ is passed in to __new__, but when the final class object is created the namespace is copied into a new dict.

If the metaclass has no __prepare__ attribute, then the class namespace is initialised as an empty ordered mapping.

See also:
 PEP 3115 - Metaclasses in Python 3000 Introduced the __prepare__ namespace hook
 - PEP 3115 - Metaclasses in Python 3000 https://www.python.org/dev/peps/pep-3115

### ===🗝 3.3.3.5. Executing the class body

The class body is executed (approximately) as `exec(body, globals(), namespace)`. The key difference from a normal call to `exec()` is that lexical scoping allows the class body (including any methods) to reference names from the current and outer scopes when the class definition occurs inside a function.

However, even when the class definition occurs inside the function, methods defined inside the class still cannot see names defined at the class scope. Class variables must be accessed through the first parameter of instance or class methods, or through the implicit lexically scoped __class__ reference described in the next section.


### ===🗝 3.3.3.6. Creating the class object

Once the class namespace has been populated by executing the class body, the class object is created by calling `metaclass(name, bases, namespace, **kwds)` (the additional keywords passed here are the same as those passed to __prepare__).

This class object is the one that will be referenced by the zero-argument form of super(). __class__ is an implicit closure reference created by the compiler if any methods in a class body refer to either __class__ or super. This allows the zero argument form of `super()` to correctly identify the class being defined based on lexical scoping, while the class or instance that was used to make the current call is identified based on the first argument passed to the method.


CPython implementation detail: In CPython 3.6 and later, the __class__ cell is passed to the metaclass as a __classcell__ entry in the class namespace. If present, this must be propagated up to the type.__new__ call in order for the class to be initialised correctly. Failing to do so will result in a RuntimeError in Python 3.8.

When using the default metaclass type, or any metaclass that ultimately calls type.__new__, the following additional customisation steps are invoked after creating the class object:

• first, type.__new__ collects all of the descriptors in the class namespace that define a __set_name__() method;
• second, all of these __set_name__ methods are called with the class being defined and the assigned name of that particular descriptor;
• finally, the __init_subclass__() hook is called on the immediate parent of the new class in its method resolution order.

After the class object is created, it is passed to the class decorators included in the class definition (if any) and the resulting object is bound in the local namespace as the defined class.

When a new class is created by type.__new__, the object provided as the namespace parameter is copied to a new ordered mapping and the original object is discarded. The new copy is wrapped in a read-only proxy, which becomes the __dict__ attribute of the class object.

See also:
 PEP 3135 - New superDescribes the implicit __class__ closure reference

### ===🗝 3.3.3.7. Uses for metaclasses

The potential uses for metaclasses are boundless. Some ideas that have been explored include enum, logging, interface checking, automatic delegation, automatic property creation, proxies, frameworks, and automatic resource locking/synchronization.


### ===🗝 3.3.4. Customizing instance and subclass checks

The following methods are used to override the default behavior of the isinstance() and issubclass() built-in functions.

In particular, the metaclass abc.ABCMeta implements these methods in order to allow the addition of Abstract Base Classes (ABCs) as “virtual base classes” to any class or type (including built-in types), including other ABCs.
➡ `class.__instancecheck__(self, instance)`
Return true if instance should be considered a (direct or indirect) instance of class. If defined, called to implement isinstance(instance, class).
➡ `class.__subclasscheck__(self, subclass)`
Return true if subclass should be considered a (direct or indirect) subclass of class. If defined, called to implement issubclass(subclass, class).

Note that these methods are looked up on the type (metaclass) of a class. They cannot be defined as class methods in the actual class. This is consistent with the lookup of special methods that are called on instances, only in this case the instance is itself a class.

See also:
 PEP 3119 - Introducing Abstract Base ClassesIncludes the specification for customizing `isinstance()` and `issubclass()` behavior through __instancecheck__() and __subclasscheck__(), with motivation for this functionality in the context of adding Abstract Base Classes (see the abc module) to the language.

### ===🗝 3.3.5. Emulating generic types

One can implement the generic class syntax as specified by PEP 484 (for example `List[int]`) by defining a special method:
➡ `classmethod object.__class_getitem__(cls, key)`
Return an object representing the specialization of a generic class by type arguments found in key.

This method is looked up on the class object itself, and when defined in the class body, this method is implicitly a class method. Note, this mechanism is primarily reserved for use with static type hints, other usage is discouraged.

See also:
 PEP 560 - Core support for typing module and generic types
 


### ===🗝 3.3.6. Emulating callable objects

✅ `object.__call__(self[, args...])`
Called when the instance is “called” as a function; if this method is defined, `x(arg1, arg2, ...)` roughly translates to `type(x).__call__(x, arg1, ...)`.


### ===🗝 3.3.7. Emulating container types

The following methods can be defined to implement container objects. Containers usually are sequences (such as lists or tuples) or mappings (like dictionaries), but can represent other containers as well. The first set of methods is used either to emulate a sequence or to emulate a mapping; the difference is that for a sequence, the allowable keys should be the integers k for which `0 <= k < N` where N is the length of the sequence, or slice objects, which define a range of items. 

It is also recommended that mappings provide the methods `keys()`, `values()`, `items()`, `get()`, `clear()`, `setdefault()`, `pop()`, `popitem()`, `copy()`, and `update()` behaving similar to those for Python’s standard dictionary objects. 

The `collections.abc` module provides a `MutableMapping` abstract base class to help create those methods from a base set of __getitem__(), __setitem__(), __delitem__(), and `keys()`. 

Mutable sequences should provide methods `append()`, `count()`, `index()`, `extend()`, `insert()`, `pop()`, `remove()`, `reverse()` and `sort()`, like Python standard list objects. Finally, sequence types should implement addition (meaning concatenation) and multiplication (meaning repetition) by defining the methods __add__(), __radd__(), __iadd__(), __mul__(), __rmul__() and __imul__() described below; they should not define other numerical operators. 

It is recommended that both mappings and sequences implement the __contains__() method to allow efficient use of the in operator; for mappings, in should search the mapping’s keys; for sequences, it should search through the values. 

It is further recommended that both mappings and sequences implement the __iter__() method to allow efficient iteration through the container; for mappings, __iter__() should iterate through the object’s keys; for sequences, it should iterate through the values.

✅ `object.__len__(self)`
Called to implement the built-in function `len()`. Should return the length of the object, an integer >= 0. Also, an object that doesn’t define a __bool__() method and whose __len__() method returns zero is considered to be false in a Boolean context.


CPython implementation detail: In CPython, the length is required to be at most sys.maxsize. If the length is larger than sys.maxsize some features (such as `len()`) may raise OverflowError. To prevent raising OverflowError by truth value testing, an object must define a __bool__() method.

✅ `object.__length_hint__(self)`
Called to implement operator.length_hint(). Should return an estimated length for the object (which may be greater or less than the actual length). The length must be an integer >= 0. The return value may also be NotImplemented, which is treated the same as if the __length_hint__ method didn’t exist at all. This method is purely an optimization and is never required for correctness.


New in version 3.4.

Note:
 Slicing is done exclusively with the following three methods. A call like
 

    a[1:2] = b

is translated to

    a[slice(1, 2, None)] = b


and so forth. Missing slice items are always filled in with `None`.


✅ `object.__getitem__(self, key)`
Called to implement evaluation of `self[key]`. For sequence types, the accepted keys should be integers and slice objects. Note that the special interpretation of negative indexes (if the class wishes to emulate a sequence type) is up to the __getitem__() method. If key is of an inappropriate type, TypeError may be raised; if of a value outside the set of indexes for the sequence (after any special interpretation of negative values), IndexError should be raised. For mapping types, if key is missing (not in the container), KeyError should be raised.

Note:
 for loops expect that an IndexError will be raised for illegal indexes to allow proper detection of the end of the sequence.
 

✅ `object.__setitem__(self, key, value)`
Called to implement assignment to `self[key]`. Same note as for __getitem__(). This should only be implemented for mappings if the objects support changes to the values for keys, or if new keys can be added, or for sequences if elements can be replaced. The same exceptions should be raised for improper `key` values as for the __getitem__() method.

✅ `object.__delitem__(self, key)`
Called to implement deletion of `self[key]`. Same note as for __getitem__(). This should only be implemented for mappings if the objects support removal of keys, or for sequences if elements can be removed from the sequence. The same exceptions should be raised for improper `key` values as for the __getitem__() method.

✅ `object.__missing__(self, key)`
Called by dict.__getitem__() to implement `self[key]` for dict subclasses when `key` is not in the dictionary.

✅ `object.__iter__(self)`
This method is called when an iterator is required for a container. This method should return a new iterator object that can iterate over all the objects in the container. For mappings, it should iterate over the keys of the container.

Iterator objects also need to implement this method; they are required to return themselves. For more information on iterator objects, see Iterator Types.

✅ `object.__reversed__(self)`
Called (if present) by the `reversed()` built-in to implement reverse iteration. It should return a new iterator object that iterates over all the objects in the container in reverse order.

If the __reversed__() method is not provided, the `reversed()` built-in will fall back to using the sequence protocol (__len__() and __getitem__()). Objects that support the sequence protocol should only provide __reversed__() if they can provide an implementation that is more efficient than the one provided by `reversed()`.

The membership test operators (in and not in) are normally implemented as an iteration through a container. However, container objects can supply the following special method with a more efficient implementation, which also does not require the object be iterable.

✅ `object.__contains__(self, item)`
Called to implement membership test operators. Should return true if item is in self, false otherwise. For mapping objects, this should consider the keys of the mapping rather than the values or the key-item pairs.

For objects that don’t define __contains__(), the membership test first tries iteration via __iter__(), then the old sequence iteration protocol via __getitem__(), see this section in the language reference.


### ===🗝 3.3.8. Emulating numeric types

The following methods can be defined to emulate numeric objects. Methods corresponding to operations that are not supported by the particular kind of number implemented (e.g., bitwise operations for non-integral numbers) should be left undefined.

✅ `object.__add__(self, other)`            <---| +
✅ `object.__sub__(self, other)`            <---| -
✅ `object.__mul__(self, other)`            <---| *
✅ `object.__matmul__(self, other)`         <---| @
✅ `object.__truediv__(self, other)`        <---| /
✅ `object.__floordiv__(self, other)`       <---| //
✅ `object.__mod__(self, other)`            <---| %
✅ `object.__divmod__(self, other)`         <---| divmod()
✅ `object.__pow__(self, other[, modulo])`  <---| pow() **
✅ `object.__lshift__(self, other)`         <---| <<
✅ `object.__rshift__(self, other)`         <---| >>
✅ `object.__and__(self, other)`            <---| &
✅ `object.__xor__(self, other)`            <---| ^
✅ `object.__or__(self, other)`             <---| |

These methods are called to implement the binary arithmetic operations.

For instance, to evaluate the expression x + y, where x is an instance of a class that has an __add__() method, x.__add__(y) is called. The __divmod__() method should be the equivalent to using __floordiv__() and __mod__(); it should not be related to __truediv__(). Note that __pow__() should be defined to accept an optional third argument if the ternary version of the built-in pow() function is to be supported.

If one of those methods does not support the operation with the supplied arguments, it should return `NotImplemented`.

✅ `object.__radd__(self, other)`           <---| +
✅ `object.__rsub__(self, other)`           <---| -
✅ `object.__rmul__(self, other)`           <---| *
✅ `object.__rmatmul__(self, other)`        <---| @
✅ `object.__rtruediv__(self, other)`       <---| /
✅ `object.__rfloordiv__(self, other)`      <---| //
✅ `object.__rmod__(self, other)`           <---| %
✅ `object.__rdivmod__(self, other)`        <---| divmod()
✅ `object.__rpow__(self, other[, modulo])` <---| pow() **
✅ `object.__rlshift__(self, other)`        <---| <<
✅ `object.__rrshift__(self, other)`        <---| >>
✅ `object.__rand__(self, other)`           <---| &
✅ `object.__rxor__(self, other)`           <---| ^
✅ `object.__ror__(self, other)`            <---| |

These methods are called to implement the binary arithmetic operations with reflected (swapped) operands. These functions are only called if the left operand does not support the corresponding operation [3] and the operands are of different types. [4] For instance, to evaluate the expression x - y, where y is an instance of a class that has an __rsub__() method, y.__rsub__(x) is called if x.__sub__(y) returns `NotImplemented`.

Note that ternary pow() will not try calling __rpow__() (the coercion rules would become too complicated).

Note:
 If the right operand’s type is a subclass of the left operand’s type and that subclass provides a different implementation of the reflected method for the operation, this method will be called before the left operand’s non-reflected method. This behavior allows subclasses to override their ancestors’ operations.
 

✅ `object.__iadd__(self, other)`           <---| +=
✅ `object.__isub__(self, other)`           <---| -=
✅ `object.__imul__(self, other)`           <---| *=
✅ `object.__imatmul__(self, other)`        <---| @=
✅ `object.__itruediv__(self, other)`       <---| /=
✅ `object.__ifloordiv__(self, other)`      <---| //=
✅ `object.__imod__(self, other)`           <---| %=
✅ `object.__ipow__(self, other[, modulo])` <---| **=
✅ `object.__ilshift__(self, other)`        <---| <<=
✅ `object.__irshift__(self, other)`        <---| >>=
✅ `object.__iand__(self, other)`           <---| &=
✅ `object.__ixor__(self, other)`           <---| ^=
✅ `object.__ior__(self, other)`            <---| |=

These methods are called to implement the augmented arithmetic assignments. These methods should attempt to do the operation in-place (modifying self) and return the result (which could be, but does not have to be, self). If a specific method is not defined, the augmented assignment falls back to the normal methods. For instance, if x is an instance of a class with an __iadd__() method, `x += y` is equivalent to `x = x.__iadd__(y)` . Otherwise, `x.__add__(y)` and `y.__radd__(x)` are considered, as with the evaluation of `x + y`. In certain situations, augmented assignment can result in unexpected errors (see Why does `a_tuple[i] += ['item']` raise an exception when the addition works?), but this behavior is in fact part of the data model.

Note:
 Due to a bug in the dispatching mechanism for `**=`, a class that defines __ipow__() but returns `NotImplemented` would fail to fall back to x.__pow__(y) and y.__rpow__(x). This bug is fixed in Python 3.10.
 

✅ `object.__neg__(self)`
✅ `object.__pos__(self)`
✅ `object.__abs__(self)`
✅ `object.__invert__(self)`

Called to implement the unary arithmetic operations (-, +, abs() and ~ ).

✅ `object.__complex__(self)`
✅ `object.__int__(self)`
✅ `object.__float__(self)`

Called to implement the built-in functions `complex()`, `int()` and `float()`. Should return a value of the appropriate type.

✅ `object.__index__(self)`

Called to implement `operator.index()`, and whenever Python needs to losslessly convert the numeric object to an integer object (such as in slicing, or in the built-in `bin()`, `hex()` and `oct()` functions). Presence of this method indicates that the numeric object is an integer type. Must return an integer.

If __int__(), __float__() and __complex__() are not defined then corresponding built-in functions int(), float() and complex() fall back to __index__().

✅ `object.__round__(self[, ndigits])`
✅ `object.__trunc__(self)`
✅ `object.__floor__(self)`
✅ `object.__ceil__(self)`

Called to implement the built-in function `round()` and math functions `trunc()`, `floor()` and `ceil()`. Unless ndigits is passed to __round__() all these methods should return the value of the object truncated to an Integral (typically an int).

If __int__() is not defined then the built-in function `int()` falls back to __trunc__().


### ===🗝 3.3.9. With Statement Context Managers

A context manager is an object that defines the runtime context to be established when executing a with statement. The context manager handles the entry into, and the exit from, the desired runtime context for the execution of the block of code. Context managers are normally invoked using the with statement (described in section The with statement), but can also be used by directly invoking their methods.

Typical uses of context managers include saving and restoring various kinds of global state, locking and unlocking resources, closing opened files, etc.

For more information on context managers, see Context Manager Types.

✅ `object.__enter__(self)`
Enter the runtime context related to this object. The with statement will bind this method’s return value to the target(s) specified in the as clause of the statement, if any.
✅ `object.__exit__(self, exc_type, exc_value, traceback)`
Exit the runtime context related to this object. The parameters describe the exception that caused the context to be exited. If the context was exited without an exception, all three arguments will be None.

If an exception is supplied, and the method wishes to suppress the exception (i.e., prevent it from being propagated), it should return a true value. Otherwise, the exception will be processed normally upon exit from this method.

Note that __exit__() methods should not reraise the passed-in exception; this is the caller’s responsibility.

See also:
 PEP 343 - The “with” statementThe specification, background, and examples for the Python with statement.

### ===🗝 3.3.10. Special method lookup

For custom classes, implicit invocations of special methods are only guaranteed to work correctly if defined on an object’s type, not in the object’s instance dictionary. That behaviour is the reason why the following code raises an exception:


>>> class C:
...     pass
...
>>> c = C()
>>> c.__len__ = lambda: 5
>>> len(c)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: object of type 'C' has no len()


The rationale behind this behaviour lies with a number of special methods such as __hash__() and __repr__() that are implemented by all objects, including type objects. If the implicit lookup of these methods used the conventional lookup process, they would fail when invoked on the type object itself:


>>> 1 .__hash__() == hash(1)
True
>>> int.__hash__() == hash(int)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: descriptor '__hash__' of 'int' object needs an argument


Incorrectly attempting to invoke an unbound method of a class in this way is sometimes referred to as ‘metaclass confusion’, and is avoided by bypassing the instance when looking up special methods:


>>> type(1).__hash__(1) == hash(1)
True
>>> type(int).__hash__(int) == hash(int)
True


In addition to bypassing any instance attributes in the interest of correctness, implicit special method lookup generally also bypasses the __getattribute__() method even of the object’s metaclass:


>>> class Meta(type):
...     def __getattribute__(*args):
...         print("Metaclass getattribute invoked")
...         return type.__getattribute__(*args)
...
>>> class C(object, metaclass=Meta):
...     def __len__(self):
...         return 10
...     def __getattribute__(*args):
...         print("Class getattribute invoked")
...         return object.__getattribute__(*args)
...
>>> c = C()
>>> c.__len__()                 # Explicit lookup via instance
Class getattribute invoked
10
>>> type(c).__len__(c)          # Explicit lookup via type
Metaclass getattribute invoked
10
>>> len(c)                      # Implicit lookup
10


Bypassing the __getattribute__() machinery in this fashion provides significant scope for speed optimisations within the interpreter, at the cost of some flexibility in the handling of special methods (the special method must be set on the class object itself in order to be consistently invoked by the interpreter).


### ===🗝 3.4. Coroutines


#### 3.4.1. Awaitable Objects

An awaitable object generally implements an __await__() method. Coroutine objects returned from `async` def functions are awaitable.

Note:
 The generator iterator objects returned from generators decorated with `types.coroutine()` or `asyncio.coroutine()` are also awaitable, but they do not implement __await__().
 
✅ `object.__await__(self)`
Must return an iterator. Should be used to implement awaitable objects. For instance, `asyncio.Future` implements this method to be compatible with the await expression.


New in version 3.5.

See also:
 PEP 492 for additional information about awaitable objects.
 


#### 3.4.2. Coroutine Objects

Coroutine objects are awaitable objects. A coroutine’s execution can be controlled by calling __await__() and iterating over the result. When the coroutine has finished executing and returns, the iterator raises `StopIteration`, and the exception’s value attribute holds the return value. If the coroutine raises an exception, it is propagated by the iterator. Coroutines should not directly raise unhandled `StopIteration` exceptions.

Coroutines also have the methods listed below, which are analogous to those of generators (see Generator-iterator methods). However, unlike generators, coroutines do not directly support iteration.

Changed in version 3.5.2: It is a RuntimeError to await on a coroutine more than once.

↪ `coroutine.send(value)`
Starts or resumes execution of the coroutine. If value is None, this is equivalent to advancing the iterator returned by __await__(). If value is not None, this method delegates to the send() method of the iterator that caused the coroutine to suspend. The result (return value, `StopIteration`, or other exception) is the same as when iterating over the __await__() return value, described above.
↪ `coroutine.throw(type[, value[, traceback]])`
Raises the specified exception in the coroutine. This method delegates to the `throw()` method of the iterator that caused the coroutine to suspend, if it has such a method. Otherwise, the exception is raised at the suspension point. The result (return value, `StopIteration`, or other exception) is the same as when iterating over the __await__() return value, described above. If the exception is not caught in the coroutine, it propagates back to the caller.
↪ `coroutine.close()`
Causes the coroutine to clean itself up and exit. If the coroutine is suspended, this method first delegates to the `close()` method of the iterator that caused the coroutine to suspend, if it has such a method. Then it raises GeneratorExit at the suspension point, causing the coroutine to immediately clean itself up. Finally, the coroutine is marked as having finished executing, even if it was never started.

Coroutine objects are automatically closed using the above process when they are about to be destroyed.


#### 3.4.3. Asynchronous Iterators

An asynchronous iterator can call asynchronous code in its __anext__ method.

Asynchronous iterators can be used in an async for statement.
✅ `object.__aiter__(self)`
Must return an asynchronous iterator object.
✅ `object.__anext__(self)`
Must return an awaitable resulting in a next value of the iterator. Should raise a StopAsyncIteration error when the iteration is over.

An example of an asynchronous iterable object:


    class Reader:
        async def readline(self):
            ...

        def __aiter__(self):
            return self

        async def __anext__(self):
            val = await self.readline()
            if val == b'':
                raise StopAsyncIteration
            return val



New in version 3.5.


Changed in version 3.7: Prior to Python 3.7, __aiter__ could return an awaitable that would resolve to an asynchronous iterator.

Starting with Python 3.7, __aiter__ must return an asynchronous iterator object. Returning anything else will result in a TypeError error.


#### 3.4.4. Asynchronous Context Managers

An asynchronous context manager is a context manager that is able to suspend execution in its __aenter__ and __aexit__ methods.

Asynchronous context managers can be used in an async with statement.

✅ `object.__aenter__(self)`
Semantically similar to __enter__(), the only difference being that it must return an awaitable.
✅ `object.__aexit__(self, exc_type, exc_value, traceback)`
Semantically similar to __exit__(), the only difference being that it must return an awaitable.

An example of an asynchronous context manager class:


    class AsyncContextManager:
        async def __aenter__(self):
            await log('entering context')

        async def __aexit__(self, exc_type, exc, tb):
            await log('exiting context')



New in version 3.5.

Footnotes

[1] It is possible in some cases to change an object’s type, under certain controlled conditions. It generally isn’t a good idea though, since it can lead to some very strange behaviour if it is handled incorrectly. 

[2] The __hash__(), __iter__(), __reversed__(), and __contains__() methods have special handling for this; others will still raise a TypeError, but may do so by relying on the behavior that None is not callable. 

[3] “Does not support” here means that the class has no such method, or the method returns NotImplemented. Do not set the method to None if you want to force fallback to the right operand’s reflected method—that will instead have the opposite effect of explicitly blocking such fallback. 

[4] For operands of the same type, it is assumed that if the non-reflected method – such as __add__() – fails then the overall operation is not supported, which is why the reflected method is not called. 



## ==⚡ 4. Execution model


### ===🗝 4.1. Structure of a program

A Python program is constructed from code blocks. A block is a piece of Python program text that is executed as a unit. 

The following are blocks: 

1. a module, 
2. a function body, 
3. and a class definition. 

Each command typed interactively is a block. A script file (a file given as standard input to the interpreter or specified as a command line argument -c "codetext" to the interpreter) is a code block. A script command (a command specified on the interpreter command line with the `-c` option) is a code block. A module run as a top level script (as module __main__) from the command line using a `-m` argument is also a code block. The string argument passed to the built-in functions eval() and exec() is a code block.

A code block is executed in an execution frame. A frame contains some administrative information (used for debugging) and determines where and how execution continues after the code block’s execution has completed.


### ===🗝 4.2. Naming and binding


#### 4.2.1. Binding of names

Names refer to objects. Names are introduced by name binding operations.

The following constructs bind names: 

- formal parameters to functions, 
- import statements, 
- class and function definitions (these bind the class or function name in the defining block), 
- and targets that are identifiers if occurring in an assignment, 
- for loop header, 
- or after as in a with statement or except clause. 

The import statement of the form `from ... import *` binds all names defined in the imported module, except those beginning with an underscore. This form may only be used at the module level.

A target occurring in a `del` statement is also considered bound for this purpose (though the actual semantics are to unbind the name).

Each assignment or import statement occurs within a block defined by a class or function definition or at the module level (the top-level code block).

If a name is bound in a block, it is a local variable of that block, unless declared as `nonlocal` or `global`. If a name is bound at the module level, it is a global variable. (The variables of the module code block are local and global.) If a variable is used in a code block but not defined there, it is a `free variable`.

Each occurrence of a name in the program text refers to the binding of that name established by the following name resolution rules.


#### 4.2.2. Resolution of names

A scope defines the visibility of a name within a block. If a local variable is defined in a block, its scope includes that block. If the definition occurs in a function block, the scope extends to any blocks contained within the defining one, unless a contained block introduces a different binding for the name.

When a name is used in a code block, it is resolved using the nearest enclosing scope. The set of all such scopes visible to a code block is called the block’s environment.

When a name is not found at all, a NameError exception is raised. If the current scope is a function scope, and the name refers to a local variable that has not yet been bound to a value at the point where the name is used, an `UnboundLocalError` exception is raised. `UnboundLocalError` is a subclass of `NameError`.

If a name binding operation occurs anywhere within a code block, all uses of the name within the block are treated as references to the current block. This can lead to errors when a name is used within a block before it is bound. This rule is subtle. Python lacks declarations and allows name binding operations to occur anywhere within a code block. The local variables of a code block can be determined by scanning the entire text of the block for name binding operations.

If the `global` statement occurs within a block, all uses of the name specified in the statement refer to the binding of that name in the top-level namespace. Names are resolved in the top-level namespace by searching the `global` namespace, i.e. the namespace of the module containing the code block, and the builtins namespace, the namespace of the module builtins. The `global` namespace is searched first. If the name is not found there, the builtins namespace is searched. The `global` statement must precede all uses of the name.

The `global` statement has the same scope as a name binding operation in the same block. If the nearest enclosing scope for a free variable contains a `global` statement, the free variable is treated as a `global`.

The `nonlocal` statement causes corresponding names to refer to previously bound variables in the nearest enclosing function scope. `SyntaxError` is raised at compile time if the given name does not exist in any enclosing function scope.

The namespace for a module is automatically created the first time a module is imported. The main module for a script is always called __main__.

Class definition blocks and arguments to `exec()` and `eval()` are special in the context of name resolution. A class definition is an executable statement that may use and define names. These references follow the normal rules for name resolution with an exception that unbound local variables are looked up in the global namespace. The namespace of the class definition becomes the attribute dictionary of the class. The scope of names defined in a class block is limited to the class block; it does not extend to the code blocks of methods – this includes comprehensions and generator expressions since they are implemented using a function scope. This means that the following will fail:


    class A:
        a = 42
        b = list(a + i for i in range(10))



#### 4.2.3. Builtins and restricted execution


CPython implementation detail: Users should not touch __builtins__; it is strictly an implementation detail. Users wanting to override values in the builtins namespace should import the builtins module and modify its attributes appropriately.

The builtins namespace associated with the execution of a code block is actually found by looking up the name __builtins__ in its global namespace; this should be a dictionary or a module (in the latter case the module’s dictionary is used). By default, when in the __main__ module, __builtins__ is the built-in module builtins; when in any other module, __builtins__ is an alias for the dictionary of the builtins module itself.


#### 4.2.4. Interaction with dynamic features

Name resolution of free variables occurs at runtime, not at compile time. This means that the following code will print 42:


    i = 10
    def f():
        print(i)
    i = 42
    f()


The `eval()` and `exec()` functions do not have access to the full environment for resolving names. Names may be resolved in the local and global namespaces of the caller. Free variables are not resolved in the nearest enclosing namespace, but in the global namespace. [1] The `exec()` and `eval()` functions have optional arguments to override the global and local namespace. If only one namespace is specified, it is used for both.


### ===🗝 4.3. Exceptions

Exceptions are a means of breaking out of the normal flow of control of a code block in order to handle errors or other exceptional conditions. An exception is raised at the point where the error is detected; it may be handled by the surrounding code block or by any code block that directly or indirectly invoked the code block where the error occurred.

The Python interpreter raises an exception when it detects a run-time error (such as division by zero). A Python program can also explicitly raise an exception with the raise statement. Exception handlers are specified with the `try … except` statement. The `finally` clause of such a statement can be used to specify cleanup code which does not handle the exception, but is executed whether an exception occurred or not in the preceding code.

Python uses the “`termination`” model of error handling: an exception handler can find out what happened and continue execution at an outer level, but it cannot repair the cause of the error and retry the failing operation (except by re-entering the offending piece of code from the top).

When an exception is not handled at all, the interpreter terminates execution of the program, or returns to its interactive main loop. In either case, it prints a stack traceback, except when the exception is `SystemExit`.

Exceptions are identified by class instances. The except clause is selected depending on the class of the instance: it must reference the class of the instance or a base class thereof. The instance can be received by the handler and can carry additional information about the exceptional condition.

Note:
 Exception messages are not part of the Python API. Their contents may change from one version of Python to the next without warning and should not be relied on by code which will run under multiple versions of the interpreter.
 

See also the description of the `try` statement in section The `try` statement and `raise` statement in section The raise statement.

Footnotes

[1] This limitation occurs because the code that is executed by these operations is not available at the time the module is compiled. 



## ==⚡ 5. The import system



Python code in one module gains access to the code in another module by the process of importing it. The `import` statement is the most common way of invoking the import machinery, but it is not the only way. Functions such as `importlib.import_module()` and built-in __import__() can also be used to invoke the import machinery.

The `import` statement combines two operations; it searches for the named module, then it binds the results of that search to a name in the local scope. The search operation of the `import` statement is defined as a call to the __import__() function, with the appropriate arguments. The return value of __import__() is used to perform the name binding operation of the `import` statement. See the `import` statement for the exact details of that name binding operation.

A direct call to __import__() performs only the module search and, if found, the module creation operation. While certain side-effects may occur, such as the importing of parent packages, and the updating of various caches (including `sys.modules`), only the `import` statement performs a name binding operation.

When an `import` statement is executed, the standard builtin __import__() function is called. Other mechanisms for invoking the import system (such as `importlib.import_module()`) may choose to bypass __import__() and use their own solutions to implement import semantics.

When a module is first imported, Python searches for the module and if found, it creates a module object [1], initializing it. If the named module cannot be found, a `ModuleNotFoundError` is raised. Python implements various strategies to search for the named module when the import machinery is invoked. These strategies can be modified and extended by using various hooks described in the sections below.


Changed in version 3.3: The import system has been updated to fully implement the second phase of PEP 302. There is no longer any implicit import machinery - the full import system is exposed through `sys.meta_path`. In addition, native namespace package support has been implemented (see PEP 420).


### ===🗝 5.1. importlib

The importlib module provides a rich API for interacting with the import system. For example `importlib.import_module()` provides a recommended, simpler API than built-in __import__() for invoking the import machinery. Refer to the importlib library documentation for additional detail.


### ===🗝 5.2. Packages

Python has only one type of module object, and all modules are of this type, regardless of whether the module is implemented in Python, C, or something else. To help organize modules and provide a naming hierarchy, Python has a concept of packages.

You can think of `packages` as the directories on a file system and modules as files within directories, but don’t take this analogy too literally since packages and modules need not originate from the file system. For the purposes of this documentation, we’ll use this convenient analogy of directories and files. Like file system directories, packages are organized hierarchically, and packages may themselves contain subpackages, as well as regular modules.

It’s important to keep in mind that all packages are modules, but not all modules are packages. Or put another way, packages are just a special kind of module. Specifically, any module that contains a __path__ attribute is considered a package.

All modules have a name. Subpackage names are separated from their parent package name by a dot, akin to Python’s standard attribute access syntax. Thus you might have a module called sys and a package called email, which in turn has a subpackage called email.mime and a module within that subpackage called email.mime.text.


### ===🗝 5.2.1. Regular packages

Python defines two types of packages, `regular packages` and `namespace packages`. Regular packages are traditional packages as they existed in Python 3.2 and earlier. A regular package is typically implemented as a directory containing an __init__.py file. When a regular package is imported, this __init__.py file is implicitly executed, and the objects it defines are bound to names in the package’s namespace. The __init__.py file can contain the same Python code that any other module can contain, and Python will add some additional attributes to the module when it is imported.

For example, the following file system layout defines a top level parent package with three subpackages:


    parent/
        __init__.py
        one/
            __init__.py
        two/
            __init__.py
        three/
            __init__.py


Importing `parent.one` will implicitly execute `parent/__init__.py` and `parent/one/__init__.py`. Subsequent imports of `parent.two` or `parent.three` will execute `parent/two/__init__.py` and `parent/three/__init__.py` respectively.


### ===🗝 5.2.2. Namespace packages

A namespace package is a composite of various portions, where each portion contributes a subpackage to the parent package. Portions may reside in different locations on the file system. Portions may also be found in zip files, on the network, or anywhere else that Python searches during import. Namespace packages may or may not correspond directly to objects on the file system; they may be virtual modules that have no concrete representation.

Namespace packages do not use an ordinary list for their __path__ attribute. They instead use a custom iterable type which will automatically perform a new search for package portions on the next import attempt within that package if the path of their parent package (or `sys.path` for a top level package) changes.

With namespace packages, there is no `parent/__init__.py` file. In fact, there may be multiple parent directories found during import search, where each one is provided by a different portion. Thus `parent/one` may not be physically located next to `parent/two`. In this case, Python will create a namespace package for the top-level parent package whenever it or one of its subpackages is imported.

See also PEP 420 for the namespace package specification.
https://www.python.org/dev/peps/pep-0420

### ===🗝 5.3. Searching

To begin the search, Python needs the fully qualified name of the module (or package, but for the purposes of this discussion, the difference is immaterial) being imported. This name may come from various arguments to the `import` statement, or from the parameters to the `importlib.import_module()` or __import__() functions.

This name will be used in various phases of the import search, and it may be the dotted path to a submodule, e.g. foo.bar.baz. In this case, Python first tries to import foo, then foo.bar, and finally foo.bar.baz. If any of the intermediate imports fail, a `ModuleNotFoundError` is raised.


### ===🗝 5.3.1. The module cache

The first place checked during import search is `sys.modules`. This mapping serves as a cache of all modules that have been previously imported, including the intermediate paths. So if foo.bar.baz was previously imported, `sys.modules` will contain entries for foo, foo.bar, and foo.bar.baz. Each key will have as its value the corresponding module object.

During import, the module name is looked up in `sys.modules` and if present, the associated value is the module satisfying the import, and the process completes. However, if the value is None, then a `ModuleNotFoundError` is raised. If the module name is missing, Python will continue searching for the module.

sys.modules is writable. Deleting a key may not destroy the associated module (as other modules may hold references to it), but it will invalidate the cache entry for the named module, causing Python to search anew for the named module upon its next import. The key can also be assigned to None, forcing the next import of the module to result in a `ModuleNotFoundError`.

Beware though, as if you keep a reference to the module object, invalidate its cache entry in `sys.modules`, and then re-import the named module, the two module objects will not be the same. By contrast, `importlib.reload()` will reuse the same module object, and simply reinitialise the module contents by rerunning the module’s code.


### ===🗝 5.3.2. Finders and loaders

If the named module is not found in `sys.modules`, then Python’s import protocol is invoked to find and load the module. This protocol consists of two conceptual objects, `finders` and `loaders`. A finder’s job is to determine whether it can find the named module using whatever strategy it knows about. Objects that implement both of these interfaces are referred to as `importers` - they return themselves when they find that they can load the requested module.

Python includes a number of default finders and importers. The first one knows how to locate built-in modules, and the second knows how to locate frozen modules. A third default finder searches an `import path` for modules. The `import path` is a list of locations that may name file system paths or zip files. It can also be extended to search for any locatable resource, such as those identified by URLs.

The import machinery is extensible, so new finders can be added to extend the range and scope of module searching.

Finders do not actually load modules. If they can find the named module, they return a module spec, an encapsulation of the module’s import-related information, which the import machinery then uses when loading the module.

The following sections describe the protocol for finders and loaders in more detail, including how you can create and register new ones to extend the import machinery.


Changed in version 3.4: In previous versions of Python, finders returned loaders directly, whereas now they return module specs which contain loaders. Loaders are still used during import but have fewer responsibilities.


### ===🗝 5.3.3. Import hooks

The import machinery is designed to be extensible; the primary mechanism for this are the import hooks. There are two types of import hooks: `meta hooks` and `import path hooks`.

Meta hooks are called at the start of import processing, before any other import processing has occurred, other than `sys.modules` cache look up. This allows meta hooks to override `sys.path` processing, frozen modules, or even built-in modules. Meta hooks are registered by adding new finder objects to `sys.meta_path`, as described below.

Import path hooks are called as part of `sys.path` (or package.__path__) processing, at the point where their associated path item is encountered. Import path hooks are registered by adding new callables to `sys.path_hooks` as described below.


### ===🗝 5.3.4. The meta path

When the named module is not found in `sys.modules`, Python next searches `sys.meta_path`, which contains a list of meta path finder objects. These finders are queried in order to see if they know how to handle the named module. Meta path finders must implement a method called `find_spec()` which takes three arguments: a name, an import path, and (optionally) a target module. The meta path finder can use any strategy it wants to determine whether it can handle the named module or not.

If the meta path finder knows how to handle the named module, it returns a spec object. If it cannot handle the named module, it returns None. If `sys.meta_path` processing reaches the end of its list without returning a spec, then a `ModuleNotFoundError` is raised. Any other exceptions raised are simply propagated up, aborting the import process.

The `find_spec()` method of meta path finders is called with two or three arguments. The first is the fully qualified name of the module being imported, for example foo.bar.baz. The second argument is the path entries to use for the module search. For top-level modules, the second argument is None, but for submodules or subpackages, the second argument is the value of the parent package’s __path__ attribute. If the appropriate __path__ attribute cannot be accessed, a `ModuleNotFoundError` is raised. The third argument is an existing module object that will be the target of loading later. The import system passes in a target module only during reload.

The meta path may be traversed multiple times for a single import request. For example, assuming none of the modules involved has already been cached, importing foo.bar.baz will first perform a top level import, calling `mpf.find_spec("foo", None, None)` on each meta path finder (mpf). After foo has been imported, foo.bar will be imported by traversing the meta path a second time, calling `mpf.find_spec("foo.bar", foo.__path__, None)`. Once foo.bar has been imported, the final traversal will call `mpf.find_spec("foo.bar.baz", foo.bar.__path__, None)`.

Some meta path finders only support top level imports. These importers will always return None when anything other than None is passed as the second argument.

Python’s default `sys.meta_path` has three meta path finders, one that knows how to import built-in modules, one that knows how to import frozen modules, and one that knows how to import modules from an import path (i.e. the path based finder).


Changed in version 3.4: The `find_spec()` method of meta path finders replaced `find_module()`, which is now deprecated. While it will continue to work without change, the import machinery will try it only if the finder does not implement `find_spec()`.


### ===🗝 5.4. Loading

If and when a module spec is found, the import machinery will use it (and the loader it contains) when loading the module. Here is an approximation of what happens during the loading portion of import:

```py
module = None
if spec.loader is not None and hasattr(spec.loader, 'create_module'):
    # It is assumed 'exec_module' will also be defined on the loader.
    module = spec.loader.create_module(spec)
if module is None:
    module = ModuleType(spec.name)
# The import-related module attributes get set here:
_init_module_attrs(spec, module)

if spec.loader is None:
    # unsupported
    raise ImportError
if spec.origin is None and spec.submodule_search_locations is not None:
    # namespace package
    `sys.modules`[spec.name] = module
elif not hasattr(spec.loader, 'exec_module'):
    module = spec.loader.load_module(spec.name)
    # Set __loader__ and __package__ if missing.
else:
    `sys.modules`[spec.name] = module
    try:
        spec.loader.exec_module(module)
    except BaseException:
        try:
            del `sys.modules`[spec.name]
        except KeyError:
            pass
        raise
return `sys.modules`[spec.name]
```


Note the following details:


• If there is an existing module object with the given name in `sys.modules`, import will have already returned it.
• The module will exist in `sys.modules` before the loader executes the module code. This is crucial because the module code may (directly or indirectly) import itself; adding it to `sys.modules` beforehand prevents unbounded recursion in the worst case and multiple loading in the best.
• If loading fails, the failing module – and only the failing module – gets removed from `sys.modules`. Any module already in the `sys.modules` cache, and any module that was successfully loaded as a side-effect, must remain in the cache. This contrasts with reloading where even the failing module is left in `sys.modules`.
• After the module is created but before execution, the import machinery sets the import-related module attributes (“`_init_module_attrs`” in the pseudo-code example above), as summarized in a later section.
• Module execution is the key moment of loading in which the module’s namespace gets populated. Execution is entirely delegated to the loader, which gets to decide what gets populated and how.
• The module created during loading and passed to `exec_module()` may not be the one returned at the end of import [2].


Changed in version 3.4: The import system has taken over the boilerplate responsibilities of loaders. These were previously performed by the `importlib.abc.Loader.load_module()` method.


### ===🗝 5.4.1. Loaders

Module loaders provide the critical function of loading: `module execution`. The import machinery calls the `importlib.abc.Loader.exec_module()` method with a single argument, the module object to execute. Any value returned from `exec_module()` is ignored.

Loaders must satisfy the following requirements:

• If the module is a Python module (as opposed to a built-in module or a dynamically loaded extension), the loader should execute the module’s code in the module’s global name space (module.__dict__).
• If the loader cannot execute the module, it should raise an `ImportError`, although any other exception raised during `exec_module()` will be propagated.

In many cases, the finder and loader can be the same object; in such cases the `find_spec()` method would just return a spec with the loader set to self.

Module loaders may opt in to creating the module object during loading by implementing a `create_module()` method. It takes one argument, the module spec, and returns the new module object to use during loading. `create_module()` does not need to set any attributes on the module object. If the method returns None, the import machinery will create the new module itself.


New in version 3.4: The `create_module()` method of loaders.

Changed in version 3.4: The `load_module()` method was replaced by `exec_module()` and the import machinery assumed all the boilerplate responsibilities of loading.


For compatibility with existing loaders, the import machinery will use the `load_module()` method of loaders if it exists and the loader does not also implement `exec_module()`. However, `load_module()` has been deprecated and loaders should implement `exec_module()` instead.

The `load_module()` method must implement all the boilerplate loading functionality described above in addition to executing the module. All the same constraints apply, with some additional clarification:


• If there is an existing module object with the given name in `sys.modules`, the loader must use that existing module. (Otherwise, `importlib.reload()` will not work correctly.) If the named module does not exist in `sys.modules`, the loader must create a new module object and add it to `sys.modules`.
• The module must exist in `sys.modules` before the loader executes the module code, to prevent unbounded recursion or multiple loading.
• If loading fails, the loader must remove any modules it has inserted into `sys.modules`, but it must remove only the failing module(s), and only if the loader itself has loaded the module(s) explicitly.


Changed in version 3.5: A DeprecationWarning is raised when `exec_module()` is defined but `create_module()` is not.


Changed in version 3.6: An `ImportError` is raised when `exec_module()` is defined but `create_module()` is not.


### ===🗝 5.4.2. Submodules

When a submodule is loaded using any mechanism (e.g. importlib APIs, the import or import-from statements, or built-in __import__()) a binding is placed in the parent module’s namespace to the submodule object. For example, if package spam has a submodule foo, after importing spam.foo, spam will have an attribute foo which is bound to the submodule. Let’s say you have the following directory structure:


    spam/
        __init__.py
        foo.py
        bar.py


and `spam/__init__.py` has the following lines in it:


    from .foo import Foo
    from .bar import Bar


then executing the following puts a name binding to foo and bar in the spam module:


>>> import spam
>>> spam.foo
<module 'spam.foo' from '/tmp/imports/spam/foo.py'>
>>> spam.bar
<module 'spam.bar' from '/tmp/imports/spam/bar.py'>


Given Python’s familiar name binding rules this might seem surprising, but it’s actually a fundamental feature of the import system. The invariant holding is that if you have `sys.modules['spam']` and `sys.modules['spam.foo']` (as you would after the above import), the latter must appear as the foo attribute of the former.


### ===🗝 5.4.3. Module spec

The import machinery uses a variety of information about each module during import, especially before loading. Most of the information is common to all modules. The purpose of a module’s spec is to encapsulate this import-related information on a per-module basis.

Using a spec during import allows state to be transferred between import system components, e.g. between the finder that creates the module spec and the loader that executes it. Most importantly, it allows the import machinery to perform the boilerplate operations of loading, whereas without a module spec the loader had that responsibility.

The module’s spec is exposed as the __spec__ attribute on a module object. See `ModuleSpec` for details on the contents of the module spec.

New in version 3.4.


### ===🗝 5.4.4. Import-related module attributes

The import machinery fills in these attributes on each module object during loading, based on the module’s spec, before the loader executes the module.

➡ `__name__`
The __name__ attribute must be set to the fully-qualified name of the module. This name is used to uniquely identify the module in the import system.

➡ `__loader__`
The __loader__ attribute must be set to the loader object that the import machinery used when loading the module. This is mostly for introspection, but can be used for additional loader-specific functionality, for example getting data associated with a loader.

➡ `__package__`
The module’s __package__ attribute must be set. Its value must be a string, but it can be the same value as its __name__. When the module is a package, its __package__ value should be set to its __name__. When the module is not a package, __package__ should be set to the empty string for top-level modules, or for submodules, to the parent package’s name. See PEP 366 for further details.

This attribute is used instead of __name__ to calculate explicit relative imports for main modules, as defined in PEP 366. It is expected to have the same value as __spec__.parent.


Changed in version 3.6: The value of __package__ is expected to be the same as __spec__.parent.

➡ `__spec__`
The __spec__ attribute must be set to the module spec that was used when importing the module. Setting __spec__ appropriately applies equally to modules initialized during interpreter startup. The one exception is __main__, where __spec__ is set to None in some cases.

When __package__ is not defined, __spec__.parent is used as a fallback.


New in version 3.4.


Changed in version 3.6: __spec__.parent is used as a fallback when __package__ is not defined.

➡ `__path__`
If the module is a package (either regular or namespace), the module object’s __path__ attribute must be set. The value must be iterable, but may be empty if __path__ has no further significance. If __path__ is not empty, it must produce strings when iterated over. More details on the semantics of __path__ are given below.

Non-package modules should not have a __path__ attribute.

➡ `__file__`
➡ `__cached__`
__file__ is optional. If set, this attribute’s value must be a string. The import system may opt to leave __file__ unset if it has no semantic meaning (e.g. a module loaded from a database).

If __file__ is set, it may also be appropriate to set the __cached__ attribute which is the path to any compiled version of the code (e.g. byte-compiled file). The file does not need to exist to set this attribute; the path can simply point to where the compiled file would exist (see PEP 3147).

It is also appropriate to set __cached__ when __file__ is not set. However, that scenario is quite atypical. Ultimately, the loader is what makes use of __file__ and/or __cached__. So if a loader can load from a cached module but otherwise does not load from a file, that atypical scenario may be appropriate.


### ===🗝 5.4.5. module.__path__

By definition, if a module has a __path__ attribute, it is a package.

A package’s __path__ attribute is used during imports of its subpackages. Within the import machinery, it functions much the same as sys.path, i.e. providing a list of locations to search for modules during import. However, __path__ is typically much more constrained than sys.path.

__path__ must be an iterable of strings, but it may be empty. The same rules used for `sys.path` also apply to a package’s __path__, and `sys.path_hooks` (described below) are consulted when traversing a package’s __path__.

A package’s __init__.py file may set or alter the package’s __path__ attribute, and this was typically the way namespace packages were implemented prior to PEP 420. With the adoption of PEP 420, namespace packages no longer need to supply __init__.py files containing only __path__ manipulation code; the import machinery automatically sets __path__ correctly for the namespace package.


### ===🗝 5.4.6. Module reprs

By default, all modules have a usable repr, however depending on the attributes set above, and in the module’s spec, you can more explicitly control the repr of module objects.

If the module has a spec (__spec__), the import machinery will try to generate a repr from it. If that fails or there is no spec, the import system will craft a default repr using whatever information is available on the module. It will try to use the module.__name__, module.__file__, and module.__loader__ as input into the repr, with defaults for whatever information is missing.

Here are the exact rules used:


• If the module has a __spec__ attribute, the information in the spec is used to generate the repr. The “name”, “loader”, “origin”, and “has_location” attributes are consulted.
• If the module has a __file__ attribute, this is used as part of the module’s repr.
• If the module has no __file__ but does have a __loader__ that is not None, then the loader’s repr is used as part of the module’s repr.
• Otherwise, just use the module’s __name__ in the repr.


Changed in version 3.4: Use of loader.module_repr() has been deprecated and the module spec is now used by the import machinery to generate a module repr.

For backward compatibility with Python 3.3, the module repr will be generated by calling the loader’s module_repr() method, if defined, before trying either approach described above. However, the method is deprecated.


### ===🗝 5.4.7. Cached bytecode invalidation

Before Python loads cached bytecode from a .pyc file, it checks whether the cache is up-to-date with the source .py file. By default, Python does this by storing the source’s last-modified timestamp and size in the cache file when writing it. At runtime, the import system then validates the cache file by checking the stored metadata in the cache file against the source’s metadata.

Python also supports “hash-based” cache files, which store a hash of the source file’s contents rather than its metadata. There are two variants of hash-based .pyc files: checked and unchecked. For checked hash-based .pyc files, Python validates the cache file by hashing the source file and comparing the resulting hash with the hash in the cache file. If a checked hash-based cache file is found to be invalid, Python regenerates it and writes a new checked hash-based cache file. For unchecked hash-based .pyc files, Python simply assumes the cache file is valid if it exists. Hash-based .pyc files validation behavior may be overridden with the --check-hash-based-pycs flag.


Changed in version 3.7: Added hash-based .pyc files. Previously, Python only supported timestamp-based invalidation of bytecode caches.


### ===🗝 5.5. The Path Based Finder

As mentioned previously, Python comes with several default meta path finders. One of these, called the path based finder (`PathFinder`), searches an import path, which contains a list of path entries. Each path entry names a location to search for modules.

The path based finder itself doesn’t know how to import anything. Instead, it traverses the individual path entries, associating each of them with a path entry finder that knows how to handle that particular kind of path.

The default set of path entry finders implement all the semantics for finding modules on the file system, handling special file types such as Python source code (.py files), Python byte code (.pyc files) and shared libraries (e.g. .so files). When supported by the `zipimport` module in the standard library, the default path entry finders also handle loading all of these file types (other than shared libraries) from zipfiles.

Path entries need not be limited to file system locations. They can refer to URLs, database queries, or any other location that can be specified as a string.

The path based finder provides additional hooks and protocols so that you can extend and customize the types of searchable path entries. For example, if you wanted to support path entries as network URLs, you could write a hook that implements HTTP semantics to find modules on the web. This hook (a callable) would return a path entry finder supporting the protocol described below, which was then used to get a loader for the module from the web.

A word of warning: this section and the previous both use the term finder, distinguishing between them by using the terms meta path finder and path entry finder. These two types of finders are very similar, support similar protocols, and function in similar ways during the import process, but it’s important to keep in mind that they are subtly different. In particular, meta path finders operate at the beginning of the import process, as keyed off the `sys.meta_path` traversal.

By contrast, path entry finders are in a sense an implementation detail of the path based finder, and in fact, if the path based finder were to be removed from `sys.meta_path`, none of the path entry finder semantics would be invoked.


### ===🗝 5.5.1. Path entry finders

The path based finder is responsible for finding and loading Python modules and packages whose location is specified with a string path entry. Most path entries name locations in the file system, but they need not be limited to this.

As a meta path finder, the path based finder implements the `find_spec()` protocol previously described, however it exposes additional hooks that can be used to customize how modules are found and loaded from the import path.

Three variables are used by the path based finder, `sys.path`, `sys.path_hooks` and `sys.path_importer_cache`. The __path__ attributes on package objects are also used. These provide additional ways that the import machinery can be customized.

`sys.path` contains a list of strings providing search locations for modules and packages. It is initialized from the PYTHONPATH environment variable and various other installation- and implementation-specific defaults. Entries in `sys.path` can name directories on the file system, zip files, and potentially other “locations” (see the site module) that should be searched for modules, such as URLs, or database queries. Only strings and bytes should be present on `sys.path`; all other data types are ignored. The encoding of bytes entries is determined by the individual path entry finders.

The path based finder is a meta path finder, so the import machinery begins the import path search by calling the path based finder’s `find_spec()` method as described previously. When the path argument to `find_spec()` is given, it will be a list of string paths to traverse - typically a package’s __path__ attribute for an import within that package. If the path argument is `None`, this indicates a top level import and `sys.path` is used.

The path based finder iterates over every entry in the search path, and for each of these, looks for an appropriate path entry finder (`PathEntryFinder`) for the path entry. Because this can be an expensive operation (e.g. there may be `stat()` call overheads for this search), the path based finder maintains a cache mapping path entries to path entry finders. This cache is maintained in `sys.path_importer_cache` (despite the name, this cache actually stores finder objects rather than being limited to importer objects). In this way, the expensive search for a particular path entry location’s path entry finder need only be done once. User code is free to remove cache entries from `sys.path_importer_cache` forcing the path based finder to perform the path entry search again [3].

If the path entry is not present in the cache, the path based finder iterates over every callable in `sys.path_hooks`. Each of the path entry hooks in this list is called with a single argument, the path entry to be searched. This callable may either return a path entry finder that can handle the path entry, or it may raise `ImportError`. An `ImportError` is used by the path based finder to signal that the hook cannot find a path entry finder for that path entry. The exception is ignored and import path iteration continues. The hook should expect either a string or bytes object; the encoding of bytes objects is up to the hook (e.g. it may be a file system encoding, UTF-8, or something else), and if the hook cannot decode the argument, it should raise `ImportError`.

If `sys.path_hooks` iteration ends with no path entry finder being returned, then the path based finder’s `find_spec()` method will store None in `sys.path_importer_cache` (to indicate that there is no finder for this path entry) and return None, indicating that this meta path finder could not find the module.

If a path entry finder is returned by one of the path entry hook callables on `sys.path_hooks`, then the following protocol is used to ask the finder for a module spec, which is then used when loading the module.

The current working directory – denoted by an empty string – is handled slightly differently from other entries on sys.path. First, if the current working directory is found to not exist, no value is stored in `sys.path_importer_cache`. Second, the value for the current working directory is looked up fresh for each module lookup. Third, the path used for `sys.path_importer_cache` and returned by `importlib.machinery.PathFinder.find_spec()` will be the actual current working directory and not the empty string.


### ===🗝 5.5.2. Path entry finder protocol

In order to support imports of modules and initialized packages and also to contribute portions to namespace packages, path entry finders must implement the `find_spec()` method.

`find_spec()` takes two arguments: the fully qualified name of the module being imported, and the (optional) target module. `find_spec()` returns a fully populated spec for the module. This spec will always have “loader” set (with one exception).

To indicate to the import machinery that the spec represents a namespace portion, the path entry finder sets “submodule_search_locations” to a list containing the portion.


Changed in version 3.4: `find_spec()` replaced `find_loader()` and `find_module()`, both of which are now deprecated, but will be used if `find_spec()` is not defined.

Older path entry finders may implement one of these two deprecated methods instead of `find_spec()`. The methods are still respected for the sake of backward compatibility. However, if `find_spec()` is implemented on the path entry finder, the legacy methods are ignored.

`find_loader()` takes one argument, the fully qualified name of the module being imported. `find_loader()` returns a 2-tuple where the first item is the loader and the second item is a namespace portion.

For backwards compatibility with other implementations of the import protocol, many path entry finders also support the same, traditional `find_module()` method that meta path finders support. However path entry finder `find_module()` methods are never called with a path argument (they are expected to record the appropriate path information from the initial call to the path hook).

The `find_module()` method on path entry finders is deprecated, as it does not allow the path entry finder to contribute portions to namespace packages. If both `find_loader()` and `find_module()` exist on a path entry finder, the import system will always call `find_loader()` in preference to `find_module()`.


### ===🗝 5.6. Replacing the standard import system

The most reliable mechanism for replacing the entire import system is to delete the default contents of `sys.meta_path`, replacing them entirely with a custom meta path hook.

If it is acceptable to only alter the behaviour of `import` statements without affecting other APIs that access the import system, then replacing the builtin __import__() function may be sufficient. This technique may also be employed at the module level to only alter the behaviour of `import` statements within that module.

To selectively prevent the import of some modules from a hook early on the meta path (rather than disabling the standard import system entirely), it is sufficient to raise `ModuleNotFoundError` directly from `find_spec()` instead of returning None. The latter indicates that the meta path search should continue, while raising an exception terminates it immediately.


### ===🗝 5.7. Package Relative Imports

Relative imports use leading dots. A single leading dot indicates a relative import, starting with the current package. Two or more leading dots indicate a relative import to the parent(s) of the current package, one level per dot after the first. For example, given the following package layout:


    package/
        __init__.py
        subpackage1/
            __init__.py
            moduleX.py
            moduleY.py
        subpackage2/
            __init__.py
            moduleZ.py
        moduleA.py


In either subpackage1/moduleX.py or subpackage1/__init__.py, the following are valid relative imports:


    from .moduleY import spam
    from .moduleY import spam as ham
    from . import moduleY
    from ..subpackage1 import moduleY
    from ..subpackage2.moduleZ import eggs
    from ..moduleA import foo


Absolute imports may use either the `import <>` or `from <> import <>` syntax, but relative imports may only use the second form; the reason for this is that:


    import XXX.YYY.ZZZ


should expose XXX.YYY.ZZZ as a usable expression, but .moduleY is not a valid expression.


### ===🗝 5.8. Special considerations for __main__

The __main__ module is a special case relative to Python’s import system. As noted elsewhere, the __main__ module is directly initialized at interpreter startup, much like sys and builtins. However, unlike those two, it doesn’t strictly qualify as a built-in module. This is because the manner in which __main__ is initialized depends on the flags and other options with which the interpreter is invoked.


#### 5.8.1. __main__.__spec__

Depending on how __main__ is initialized, __main__.__spec__ gets set appropriately or to `None`.

When Python is started with the `-m` option, __spec__ is set to the module spec of the corresponding module or package. __spec__ is also populated when the __main__ module is loaded as part of executing a directory, zipfile or other `sys.path` entry.

In the remaining cases __main__.__spec__ is set to `None`, as the code used to populate the __main__ does not correspond directly with an importable module:

• interactive prompt
• -c option
• running from stdin
• running directly from a source or bytecode file

Note that __main__.__spec__ is always `None` in the last case, even if the file could technically be imported directly as a module instead. Use the `-m` switch if valid module metadata is desired in __main__.

Note also that even when __main__ corresponds with an importable module and __main__.__spec__ is set accordingly, they’re still considered distinct modules. This is due to the fact that blocks guarded by if __name__ == "__main__": checks only execute when the module is used to populate the __main__ namespace, and not during normal import.


### ===🗝 5.9. Open issues

XXX It would be really nice to have a diagram.

XXX * (import_machinery.rst) how about a section devoted just to the attributes of modules and packages, perhaps expanding upon or supplanting the related entries in the data model reference page?

XXX runpy, pkgutil, et al in the library manual should all get “See Also” links at the top pointing to the new import system section.

XXX Add more explanation regarding the different ways in which __main__ is initialized?

XXX Add more info on __main__ quirks/pitfalls (i.e. copy from PEP 395).


### ===🗝 5.10. References

The import machinery has evolved considerably since Python’s early days. The original specification for packages is still available to read, although some details have changed since the writing of that document.

The original specification for `sys.meta_path` was PEP 302, with subsequent extension in PEP 420.

PEP 420 introduced namespace packages for Python 3.3. PEP 420 also introduced the `find_loader()` protocol as an alternative to `find_module()`.

PEP 366 describes the addition of the __package__ attribute for explicit relative imports in main modules.

PEP 328 introduced absolute and explicit relative imports and initially proposed __name__ for semantics PEP 366 would eventually specify for __package__.

PEP 338 defines executing modules as scripts.

PEP 451 adds the encapsulation of per-module import state in spec objects. It also off-loads most of the boilerplate responsibilities of loaders back onto the import machinery. These changes allow the deprecation of several APIs in the import system and also addition of new methods to finders and loaders.

Footnotes

[1] See types.ModuleType. 

[2] The importlib implementation avoids using the return value directly. Instead, it gets the module object by looking the module name up in `sys.modules`. The indirect effect of this is that an imported module may replace itself in `sys.modules`. This is implementation-specific behavior that is not guaranteed to work in other Python implementations. 

[3] In legacy code, it is possible to find instances of imp.NullImporter in the `sys.path_importer_cache`. It is recommended that code be changed to use None instead. See Porting Python code for more details. 


## ==⚡ 6. Expressions

This chapter explains the meaning of the elements of expressions in Python.

Syntax Notes: In this and the following chapters, extended BNF notation will be used to describe syntax, not lexical analysis. When (one alternative of) a syntax rule has the form

    name ::=  othername

and no semantics are given, the semantics of this form of name are the same as for othername.


### ===🗝 6.1. Arithmetic conversions

When a description of an arithmetic operator below uses the phrase “the numeric arguments are converted to a common type”, this means that the operator implementation for built-in types works as follows:

•If either argument is a complex number, the other is converted to complex;
•otherwise, if either argument is a floating point number, the other is converted to floating point;
•otherwise, both must be integers and no conversion is necessary.

Some additional rules apply for certain operators (e.g., a string as a left argument to the ‘%’ operator). Extensions must define their own conversion behavior.


### ===🗝 6.2. Expressions Atoms

Atoms are the most basic elements of expressions. The simplest atoms are identifiers or literals. Forms enclosed in parentheses, brackets or braces are also categorized syntactically as atoms. The syntax for atoms is:

    atom      ::=  identifier | literal | enclosure
    enclosure ::=  parenth_form | list_display | dict_display | set_display
                   | generator_expression | yield_atom


#### 6.2.1. Identifiers (Names)

An identifier occurring as an atom is a name. See section Identifiers and keywords for lexical definition and section Naming and binding for documentation of naming and binding.

When the name is bound to an object, evaluation of the atom yields that object. When a name is not bound, an attempt to evaluate it raises a NameError exception.

Private name mangling: When an identifier that textually occurs in a class definition begins with two or more underscore characters and does not end in two or more underscores, it is considered a private name of that class. Private names are transformed to a longer form before code is generated for them. The transformation inserts the class name, with leading underscores removed and a single underscore inserted, in front of the name. For example, the identifier `__spam` occurring in a class named Ham will be transformed to `_Ham__spam`. This transformation is independent of the syntactical context in which the identifier is used. If the transformed name is extremely long (longer than 255 characters), implementation defined truncation may happen. If the class name consists only of underscores, no transformation is done.


#### 6.2.2. Literals

Python supports string and bytes literals and various numeric literals:

    literal ::=  stringliteral | bytesliteral
                 | integer | floatnumber | imagnumber


Evaluation of a literal yields an object of the given type (string, bytes, integer, floating point number, complex number) with the given value. The value may be approximated in the case of floating point and imaginary (complex) literals. See section Literals for details.

All literals correspond to immutable data types, and hence the object’s identity is less important than its value. Multiple evaluations of literals with the same value (either the same occurrence in the program text or a different occurrence) may obtain the same object or a different object with the same value.


#### 6.2.3. Parenthesized forms

A parenthesized form is an optional expression list enclosed in parentheses:

    parenth_form ::=  "(" [starred_expression] ")"


A parenthesized expression list yields whatever that expression list yields: if the list contains at least one comma, it yields a tuple; otherwise, it yields the single expression that makes up the expression list.

An empty pair of parentheses yields an empty tuple object. Since tuples are immutable, the same rules as for literals apply (i.e., two occurrences of the empty tuple may or may not yield the same object).

Note that tuples are not formed by the parentheses, but rather by use of the comma operator. The exception is the empty tuple, for which parentheses are required — allowing unparenthesized “nothing” in expressions would cause ambiguities and allow common typos to pass uncaught.


#### 6.2.4. Displays for lists, sets and dictionaries

For constructing a list, a set or a dictionary Python provides special syntax called “displays”, each of them in two flavors:
•either the container contents are listed explicitly, or
•they are computed via a set of looping and filtering instructions, called a comprehension.

Common syntax elements for comprehensions are:

    comprehension ::=  assignment_expression comp_for
    comp_for      ::=  ["async"] "for" target_list "in" or_test [comp_iter]
    comp_iter     ::=  comp_for | comp_if
    comp_if       ::=  "if" or_test [comp_iter]


The comprehension consists of a single expression followed by at least one for clause and zero or more for or if clauses. In this case, the elements of the new container are those that would be produced by considering each of the for or if clauses a block, nesting from left to right, and evaluating the expression to produce an element each time the innermost block is reached.

However, aside from the iterable expression in the leftmost for clause, the comprehension is executed in a separate implicitly nested scope. This ensures that names assigned to in the target list don’t “leak” into the enclosing scope.

The iterable expression in the leftmost for clause is evaluated directly in the enclosing scope and then passed as an argument to the implicitly nested scope. Subsequent for clauses and any filter condition in the leftmost for clause cannot be evaluated in the enclosing scope as they may depend on the values obtained from the leftmost iterable. For example: [x*y for x in range(10) for y in range(x, x+10)].

To ensure the comprehension always results in a container of the appropriate type, yield and yield from expressions are prohibited in the implicitly nested scope.

Since Python 3.6, in an async def function, an async for clause may be used to iterate over a asynchronous iterator. A comprehension in an async def function may consist of either a for or async for clause following the leading expression, may contain additional for or async for clauses, and may also use await expressions. If a comprehension contains either async for clauses or await expressions it is called an asynchronous comprehension. An asynchronous comprehension may suspend the execution of the coroutine function in which it appears. See also PEP 530.


New in version 3.6: Asynchronous comprehensions were introduced.


Changed in version 3.8: yield and yield from prohibited in the implicitly nested scope.


#### 6.2.5. List displays

A list display is a possibly empty series of expressions enclosed in square brackets:

    list_display ::=  "[" [starred_list | comprehension] "]"


A list display yields a new list object, the contents being specified by either a list of expressions or a comprehension. When a comma-separated list of expressions is supplied, its elements are evaluated from left to right and placed into the list object in that order. When a comprehension is supplied, the list is constructed from the elements resulting from the comprehension.


#### 6.2.6. Set displays

A set display is denoted by curly braces and distinguishable from dictionary displays by the lack of colons separating keys and values:

    set_display ::=  "{" (starred_list | comprehension) "}"

A set display yields a new mutable set object, the contents being specified by either a sequence of expressions or a comprehension. When a comma-separated list of expressions is supplied, its elements are evaluated from left to right and added to the set object. When a comprehension is supplied, the set is constructed from the elements resulting from the comprehension.

An empty set cannot be constructed with {}; this literal constructs an empty dictionary.


#### 6.2.7. Dictionary displays

A dictionary display is a possibly empty series of key/datum pairs enclosed in curly braces:

    dict_display       ::=  "{" [key_datum_list | dict_comprehension] "}"
    key_datum_list     ::=  key_datum ("," key_datum)* [","]
    key_datum          ::=  expression ":" expression | "**" or_expr
    dict_comprehension ::=  expression ":" expression comp_for

A dictionary display yields a new dictionary object.

If a comma-separated sequence of key/datum pairs is given, they are evaluated from left to right to define the entries of the dictionary: each key object is used as a key into the dictionary to store the corresponding datum. This means that you can specify the same key multiple times in the key/datum list, and the final dictionary’s value for that key will be the last one given.

A double asterisk ** denotes dictionary unpacking. Its operand must be a mapping. Each mapping item is added to the new dictionary. Later values replace values already set by earlier key/datum pairs and earlier dictionary unpackings.


New in version 3.5: Unpacking into dictionary displays, originally proposed by PEP 448.

A dict comprehension, in contrast to list and set comprehensions, needs two expressions separated with a colon followed by the usual “for” and “if” clauses. When the comprehension is run, the resulting key and value elements are inserted in the new dictionary in the order they are produced.

Restrictions on the types of the key values are listed earlier in section The standard type hierarchy. (To summarize, the key type should be hashable, which excludes all mutable objects.) Clashes between duplicate keys are not detected; the last datum (textually rightmost in the display) stored for a given key value prevails.


Changed in version 3.8: Prior to Python 3.8, in dict comprehensions, the evaluation order of key and value was not well-defined. In CPython, the value was evaluated before the key. Starting with 3.8, the key is evaluated before the value, as proposed by PEP 572.


#### 6.2.8. Generator expressions

A generator expression is a compact generator notation in parentheses:

    generator_expression ::=  "(" expression comp_for ")"

A generator expression yields a new generator object. Its syntax is the same as for comprehensions, except that it is enclosed in parentheses instead of brackets or curly braces.

Variables used in the generator expression are evaluated lazily when the __next__() method is called for the generator object (in the same fashion as normal generators). However, the iterable expression in the leftmost for clause is immediately evaluated, so that an error produced by it will be emitted at the point where the generator expression is defined, rather than at the point where the first value is retrieved. Subsequent for clauses and any filter condition in the leftmost for clause cannot be evaluated in the enclosing scope as they may depend on the values obtained from the leftmost iterable. For example: 

    (x*y for x in range(10) for y in range(x, x+10))

The parentheses can be omitted on calls with only one argument. See section Calls for details.

To avoid interfering with the expected operation of the generator expression itself, yield and yield from expressions are prohibited in the implicitly defined generator.

If a generator expression contains either async for clauses or await expressions it is called an asynchronous generator expression. An asynchronous generator expression returns a new asynchronous generator object, which is an asynchronous iterator (see Asynchronous Iterators).


New in version 3.6: Asynchronous generator expressions were introduced.

Changed in version 3.7: Prior to Python 3.7, asynchronous generator expressions could only appear in async def coroutines. Starting with 3.7, any function can use asynchronous generator expressions.

Changed in version 3.8: yield and yield from prohibited in the implicitly nested scope.


#### 6.2.9. Yield expressions

    yield_atom       ::=  "(" yield_expression ")"
    yield_expression ::=  "yield" [expression_list | "from" expression]


The yield expression is used when defining a generator function or an asynchronous generator function and thus can only be used in the body of a function definition. Using a yield expression in a function’s body causes that function to be a generator, and using it in an async def function’s body causes that coroutine function to be an asynchronous generator. For example:


    def gen():  # defines a generator function
        yield 123

    async def agen(): # defines an asynchronous generator function
        yield 123


Due to their side effects on the containing scope, yield expressions are not permitted as part of the implicitly defined scopes used to implement comprehensions and generator expressions.


Changed in version 3.8: Yield expressions prohibited in the implicitly nested scopes used to implement comprehensions and generator expressions.

Generator functions are described below, while asynchronous generator functions are described separately in section Asynchronous generator functions.

When a generator function is called, it returns an iterator known as a generator. That generator then controls the execution of the generator function. The execution starts when one of the generator’s methods is called. At that time, the execution proceeds to the first yield expression, where it is suspended again, returning the value of expression_list to the generator’s caller. By suspended, we mean that all local state is retained, including the current bindings of local variables, the instruction pointer, the internal evaluation stack, and the state of any exception handling. When the execution is resumed by calling one of the generator’s methods, the function can proceed exactly as if the yield expression were just another external call. The value of the yield expression after resuming depends on the method which resumed the execution. If __next__() is used (typically via either a for or the next() builtin) then the result is None. Otherwise, if send() is used, then the result will be the value passed in to that method.

All of this makes generator functions quite similar to coroutines; they yield multiple times, they have more than one entry point and their execution can be suspended. The only difference is that a generator function cannot control where the execution should continue after it yields; the control is always transferred to the generator’s caller.

Yield expressions are allowed anywhere in a try construct. If the generator is not resumed before it is finalized (by reaching a zero reference count or by being garbage collected), the generator-iterator’s close() method will be called, allowing any pending finally clauses to execute.

When yield from <expr> is used, the supplied expression must be an iterable. The values produced by iterating that iterable are passed directly to the caller of the current generator’s methods. Any values passed in with send() and any exceptions passed in with throw() are passed to the underlying iterator if it has the appropriate methods. If this is not the case, then send() will raise AttributeError or TypeError, while throw() will just raise the passed in exception immediately.

When the underlying iterator is complete, the value attribute of the raised StopIteration instance becomes the value of the yield expression. It can be either set explicitly when raising StopIteration, or automatically when the subiterator is a generator (by returning a value from the subgenerator).

Changed in version 3.3: Added yield from <expr> to delegate control flow to a subiterator.

The parentheses may be omitted when the yield expression is the sole expression on the right hand side of an assignment statement.

See also:
 PEP 255 - Simple GeneratorsThe proposal for adding generators and the yield statement to Python.PEP 342 - Coroutines via Enhanced GeneratorsThe proposal to enhance the API and syntax of generators, making them usable as simple coroutines.PEP 380 - Syntax for Delegating to a SubgeneratorThe proposal to introduce the yield_from syntax, making delegation to subgenerators easy.PEP 525 - Asynchronous GeneratorsThe proposal that expanded on PEP 492 by adding generator capabilities to coroutine functions.

#### 6.2.9.1. Generator-iterator methods

This subsection describes the methods of a generator iterator. They can be used to control the execution of a generator function.

Note that calling any of the generator methods below when the generator is already executing raises a ValueError exception.
 generator.__next__()
Starts the execution of a generator function or resumes it at the last executed yield expression. When a generator function is resumed with a __next__() method, the current yield expression always evaluates to None. The execution then continues to the next yield expression, where the generator is suspended again, and the value of the expression_list is returned to __next__()’s caller. If the generator exits without yielding another value, a StopIteration exception is raised.

This method is normally called implicitly, e.g. by a for loop, or by the built-in next() function.
generator.send(value)
Resumes the execution and “sends” a value into the generator function. The value argument becomes the result of the current yield expression. The send() method returns the next value yielded by the generator, or raises StopIteration if the generator exits without yielding another value. When send() is called to start the generator, it must be called with None as the argument, because there is no yield expression that could receive the value.
generator.throw(type[, value[, traceback]])
Raises an exception of type type at the point where the generator was paused, and returns the next value yielded by the generator function. If the generator exits without yielding another value, a StopIteration exception is raised. If the generator function does not catch the passed-in exception, or raises a different exception, then that exception propagates to the caller.
 generator.close()
Raises a GeneratorExit at the point where the generator function was paused. If the generator function then exits gracefully, is already closed, or raises GeneratorExit (by not catching the exception), close returns to its caller. If the generator yields a value, a RuntimeError is raised. If the generator raises any other exception, it is propagated to the caller. close() does nothing if the generator has already exited due to an exception or normal exit.


#### 6.2.9.2. Examples

Here is a simple example that demonstrates the behavior of generators and generator functions:


>>> def echo(value=None):
...     print("Execution starts when 'next()' is called for the first time.")
...     try:
...         while True:
...             try:
...                 value = (yield value)
...             except Exception as e:
...                 value = e
...     finally:
...         print("Don't forget to clean up when 'close()' is called.")
...
>>> generator = echo(1)
>>> print(next(generator))
Execution starts when 'next()' is called for the first time.
1
>>> print(next(generator))
None
>>> print(generator.send(2))
2
>>> generator.throw(TypeError, "spam")
TypeError('spam',)
>>> generator.close()
Don't forget to clean up when 'close()' is called.


For examples using yield from, see PEP 380: Syntax for Delegating to a Subgenerator in “What’s New in Python.”


#### 6.2.9.3. Asynchronous generator functions

The presence of a yield expression in a function or method defined using async def further defines the function as an asynchronous generator function.

When an asynchronous generator function is called, it returns an asynchronous iterator known as an asynchronous generator object. That object then controls the execution of the generator function. An asynchronous generator object is typically used in an async for statement in a coroutine function analogously to how a generator object would be used in a for statement.

Calling one of the asynchronous generator’s methods returns an awaitable object, and the execution starts when this object is awaited on. At that time, the execution proceeds to the first yield expression, where it is suspended again, returning the value of expression_list to the awaiting coroutine. As with a generator, suspension means that all local state is retained, including the current bindings of local variables, the instruction pointer, the internal evaluation stack, and the state of any exception handling. When the execution is resumed by awaiting on the next object returned by the asynchronous generator’s methods, the function can proceed exactly as if the yield expression were just another external call. The value of the yield expression after resuming depends on the method which resumed the execution. If __anext__() is used then the result is None. Otherwise, if asend() is used, then the result will be the value passed in to that method.

In an asynchronous generator function, yield expressions are allowed anywhere in a try construct. However, if an asynchronous generator is not resumed before it is finalized (by reaching a zero reference count or by being garbage collected), then a yield expression within a try construct could result in a failure to execute pending finally clauses. In this case, it is the responsibility of the event loop or scheduler running the asynchronous generator to call the asynchronous generator-iterator’s aclose() method and run the resulting coroutine object, thus allowing any pending finally clauses to execute.

To take care of finalization, an event loop should define a finalizer function which takes an asynchronous generator-iterator and presumably calls aclose() and executes the coroutine. This finalizer may be registered by calling sys.set_asyncgen_hooks(). When first iterated over, an asynchronous generator-iterator will store the registered finalizer to be called upon finalization. For a reference example of a finalizer method see the implementation of asyncio.Loop.shutdown_asyncgens in Lib/asyncio/base_events.py.

The expression yield from <expr> is a syntax error when used in an asynchronous generator function.


#### 6.2.9.4. Asynchronous generator-iterator methods

This subsection describes the methods of an asynchronous generator iterator, which are used to control the execution of a generator function.
➡ `coroutine agen.__anext__()`
Returns an awaitable which when run starts to execute the asynchronous generator or resumes it at the last executed yield expression. When an asynchronous generator function is resumed with an __anext__() method, the current yield expression always evaluates to None in the returned awaitable, which when run will continue to the next yield expression. The value of the expression_list of the yield expression is the value of the StopIteration exception raised by the completing coroutine. If the asynchronous generator exits without yielding another value, the awaitable instead raises a StopAsyncIteration exception, signalling that the asynchronous iteration has completed.

This method is normally called implicitly by a async for loop.
➡ `coroutine agen.asend(value)`
Returns an awaitable which when run resumes the execution of the asynchronous generator. As with the send() method for a generator, this “sends” a value into the asynchronous generator function, and the value argument becomes the result of the current yield expression. The awaitable returned by the asend() method will return the next value yielded by the generator as the value of the raised StopIteration, or raises StopAsyncIteration if the asynchronous generator exits without yielding another value. When asend() is called to start the asynchronous generator, it must be called with None as the argument, because there is no yield expression that could receive the value.
➡ `coroutine agen.athrow(type[, value[, traceback]])`
Returns an awaitable that raises an exception of type type at the point where the asynchronous generator was paused, and returns the next value yielded by the generator function as the value of the raised StopIteration exception. If the asynchronous generator exits without yielding another value, a StopAsyncIteration exception is raised by the awaitable. If the generator function does not catch the passed-in exception, or raises a different exception, then when the awaitable is run that exception propagates to the caller of the awaitable.
➡ `coroutine agen.aclose()`
Returns an awaitable that when run will throw a GeneratorExit into the asynchronous generator function at the point where it was paused. If the asynchronous generator function then exits gracefully, is already closed, or raises GeneratorExit (by not catching the exception), then the returned awaitable will raise a StopIteration exception. Any further awaitables returned by subsequent calls to the asynchronous generator will raise a StopAsyncIteration exception. If the asynchronous generator yields a value, a RuntimeError is raised by the awaitable. If the asynchronous generator raises any other exception, it is propagated to the caller of the awaitable. If the asynchronous generator has already exited due to an exception or normal exit, then further calls to aclose() will return an awaitable that does nothing.


### ===🗝 6.3. Primaries

Primaries represent the most tightly bound operations of the language. Their syntax is:

    primary ::=  atom | attributeref | subscription | slicing | call


#### 6.3.1. Attribute references

An attribute reference is a primary followed by a period and a name:

    attributeref ::=  primary "." identifier

The primary must evaluate to an object of a type that supports attribute references, which most objects do. This object is then asked to produce the attribute whose name is the identifier. This production can be customized by overriding the __getattr__() method. If this attribute is not available, the exception AttributeError is raised. Otherwise, the type and value of the object produced is determined by the object. Multiple evaluations of the same attribute reference may yield different objects.


#### 6.3.2. Subscriptions

Subscription of a sequence (string, tuple or list) or mapping (dictionary) object usually selects an item from the collection:

    subscription ::=  primary "[" expression_list "]"

The primary must evaluate to an object that supports subscription (lists or dictionaries for example). User-defined objects can support subscription by defining a __getitem__() method.

For built-in objects, there are two types of objects that support subscription:

If the primary is a mapping, the expression list must evaluate to an object whose value is one of the keys of the mapping, and the subscription selects the value in the mapping that corresponds to that key. (The expression list is a tuple except if it has exactly one item.)

If the primary is a sequence, the expression list must evaluate to an integer or a slice (as discussed in the following section).

The formal syntax makes no special provision for negative indices in sequences; however, built-in sequences all provide a __getitem__() method that interprets negative indices by adding the length of the sequence to the index (so that x[-1] selects the last item of x). The resulting value must be a nonnegative integer less than the number of items in the sequence, and the subscription selects the item whose index is that value (counting from zero). Since the support for negative indices and slicing occurs in the object’s __getitem__() method, subclasses overriding this method will need to explicitly add that support.

A string’s items are characters. A character is not a separate data type but a string of exactly one character.

Subscription of certain classes or types creates a generic alias. In this case, user-defined classes can support subscription by providing a __class_getitem__() classmethod.


#### 6.3.3. Slicings

A slicing selects a range of items in a sequence object (e.g., a string, tuple or list). Slicings may be used as expressions or as targets in assignment or del statements. The syntax for a slicing:

    slicing      ::=  primary "[" slice_list "]"
    slice_list   ::=  slice_item ("," slice_item)* [","]
    slice_item   ::=  expression | proper_slice
    proper_slice ::=  [lower_bound] ":" [upper_bound] [ ":" [stride] ]
    lower_bound  ::=  expression
    upper_bound  ::=  expression
    stride       ::=  expression

There is ambiguity in the formal syntax here: anything that looks like an expression list also looks like a slice list, so any subscription can be interpreted as a slicing. Rather than further complicating the syntax, this is disambiguated by defining that in this case the interpretation as a subscription takes priority over the interpretation as a slicing (this is the case if the slice list contains no proper slice).

The semantics for a slicing are as follows. The primary is indexed (using the same __getitem__() method as normal subscription) with a key that is constructed from the slice list, as follows. If the slice list contains at least one comma, the key is a tuple containing the conversion of the slice items; otherwise, the conversion of the lone slice item is the key. The conversion of a slice item that is an expression is that expression. The conversion of a proper slice is a slice object (see section The standard type hierarchy) whose start, stop and step attributes are the values of the expressions given as lower bound, upper bound and stride, respectively, substituting None for missing expressions.


#### 6.3.4. Calls

A call calls a callable object (e.g., a function) with a possibly empty series of arguments:

    call                 ::=  primary "(" [argument_list [","] | comprehension] ")"
    argument_list        ::=  positional_arguments ["," starred_and_keywords]
                                ["," keywords_arguments]
                              | starred_and_keywords ["," keywords_arguments]
                              | keywords_arguments
    positional_arguments ::=  positional_item ("," positional_item)*
    positional_item      ::=  assignment_expression | "*" expression
    starred_and_keywords ::=  ("*" expression | keyword_item)
                              ("," "*" expression | "," keyword_item)*
    keywords_arguments   ::=  (keyword_item | "**" expression)
                              ("," keyword_item | "," "**" expression)*
    keyword_item         ::=  identifier "=" expression

An optional trailing comma may be present after the positional and keyword arguments but does not affect the semantics.

The primary must evaluate to a callable object (user-defined functions, built-in functions, methods of built-in objects, class objects, methods of class instances, and all objects having a __call__() method are callable). All argument expressions are evaluated before the call is attempted. Please refer to section Function definitions for the syntax of formal parameter lists.

If keyword arguments are present, they are first converted to positional arguments, as follows. First, a list of unfilled slots is created for the formal parameters. If there are N positional arguments, they are placed in the first N slots. Next, for each keyword argument, the identifier is used to determine the corresponding slot (if the identifier is the same as the first formal parameter name, the first slot is used, and so on). If the slot is already filled, a TypeError exception is raised. Otherwise, the value of the argument is placed in the slot, filling it (even if the expression is None, it fills the slot). When all arguments have been processed, the slots that are still unfilled are filled with the corresponding default value from the function definition. (Default values are calculated, once, when the function is defined; thus, a mutable object such as a list or dictionary used as default value will be shared by all calls that don’t specify an argument value for the corresponding slot; this should usually be avoided.) If there are any unfilled slots for which no default value is specified, a TypeError exception is raised. Otherwise, the list of filled slots is used as the argument list for the call.


CPython implementation detail: An implementation may provide built-in functions whose positional parameters do not have names, even if they are ‘named’ for the purpose of documentation, and which therefore cannot be supplied by keyword. In CPython, this is the case for functions implemented in C that use `PyArg_ParseTuple()` to parse their arguments.

If there are more positional arguments than there are formal parameter slots, a TypeError exception is raised, unless a formal parameter using the syntax `*identifier` is present; in this case, that formal parameter receives a tuple containing the excess positional arguments (or an empty tuple if there were no excess positional arguments).

If any keyword argument does not correspond to a formal parameter name, a TypeError exception is raised, unless a formal parameter using the syntax `**identifier` is present; in this case, that formal parameter receives a dictionary containing the excess keyword arguments (using the keywords as keys and the argument values as corresponding values), or a (new) empty dictionary if there were no excess keyword arguments.

If the syntax `*expression` appears in the function call, expression must evaluate to an iterable. Elements from these iterables are treated as if they were additional positional arguments. For the call `f(x1, x2, *y, x3, x4)`, if y evaluates to a sequence y1, …, yM, this is equivalent to a call with M+4 positional arguments x1, x2, y1, …, yM, x3, x4.

A consequence of this is that although the `*expression` syntax may appear after explicit keyword arguments, it is processed before the keyword arguments (and any **expression arguments – see below). So:


>>> def f(a, b):
...     print(a, b)
...
>>> f(b=1, * (2,))
2 1
>>> f(a=1, * (2,))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: f() got multiple values for keyword argument 'a'
>>> f(1, * (2,))
1 2


It is unusual for both keyword arguments and the `*expression` syntax to be used in the same call, so in practice this confusion does not arise.

If the syntax `**expression` appears in the function call, expression must evaluate to a mapping, the contents of which are treated as additional keyword arguments. If a keyword is already present (as an explicit keyword argument, or from another unpacking), a TypeError exception is raised.

Formal parameters using the syntax `*identifier` or `**identifier` cannot be used as positional argument slots or as keyword argument names.


Changed in version 3.5: Function calls accept any number of * and ** unpackings, positional arguments may follow iterable unpackings ( * ), and keyword arguments may follow dictionary unpackings ( ** ). Originally proposed by PEP 448.

A call always returns some value, possibly None, unless it raises an exception. How this value is computed depends on the type of the callable object.

If it is—
↪ `a user-defined function:`
The code block for the function is executed, passing it the argument list. The first thing the code block will do is bind the formal parameters to the arguments; this is described in section Function definitions. When the code block executes a return statement, this specifies the return value of the function call.
↪ `a built-in function or method:`
The result is up to the interpreter; see Built-in Functions for the descriptions of built-in functions and methods.
↪ `a class object:`
A new instance of that class is returned.
↪ `a class instance method:`
The corresponding user-defined function is called, with an argument list that is one longer than the argument list of the call: the instance becomes the first argument.
↪ `a class instance:`
The class must define a __call__() method; the effect is then the same as if that method was called.


### ===🗝 6.4. Await expression

Suspend the execution of coroutine on an awaitable object. Can only be used inside a coroutine function.

    await_expr ::=  "await" primary


New in version 3.5.


### ===🗝 6.5. The power operator

The power operator binds more tightly than unary operators on its left; it binds less tightly than unary operators on its right. The syntax is:

    power ::=  (await_expr | primary) ["**" u_expr]


Thus, in an unparenthesized sequence of power and unary operators, the operators are evaluated from right to left (this does not constrain the evaluation order for the operands): `-1**2` results in -1.

The power operator has the same semantics as the built-in pow() function, when called with two arguments: it yields its left argument raised to the power of its right argument. The numeric arguments are first converted to a common type, and the result is of that type.

For int operands, the result has the same type as the operands unless the second argument is negative; in that case, all arguments are converted to float and a float result is delivered. For example, 10**2 returns 100, but 10**-2 returns 0.01.

Raising 0.0 to a negative power results in a ZeroDivisionError. Raising a negative number to a fractional power results in a complex number. (In earlier versions it raised a ValueError.)


### ===🗝 6.6. Unary arithmetic and bitwise operations

All unary arithmetic and bitwise operations have the same priority:

    u_expr ::=  power | "-" u_expr | "+" u_expr | "~" u_expr

The unary - (minus) operator yields the negation of its numeric argument.

The unary + (plus) operator yields its numeric argument unchanged.

The unary ~ (invert) operator yields the bitwise inversion of its integer argument. The bitwise inversion of x is defined as -(x+1). It only applies to integral numbers.

In all three cases, if the argument does not have the proper type, a TypeError exception is raised.


### ===🗝 6.7. Binary arithmetic operations

The binary arithmetic operations have the conventional priority levels. Note that some of these operations also apply to certain non-numeric types. Apart from the power operator, there are only two levels, one for multiplicative operators and one for additive operators:

    m_expr ::=  u_expr | m_expr "*" u_expr | m_expr "@" m_expr |
                m_expr "//" u_expr | m_expr "/" u_expr |
                m_expr "%" u_expr
    a_expr ::=  m_expr | a_expr "+" m_expr | a_expr "-" m_expr


The * (multiplication) operator yields the product of its arguments. The arguments must either both be numbers, or one argument must be an integer and the other must be a sequence. In the former case, the numbers are converted to a common type and then multiplied together. In the latter case, sequence repetition is performed; a negative repetition factor yields an empty sequence.

The @ (at) operator is intended to be used for matrix multiplication. No builtin Python types implement this operator.


New in version 3.5.

The / (division) and // (floor division) operators yield the quotient of their arguments. The numeric arguments are first converted to a common type. Division of integers yields a float, while floor division of integers results in an integer; the result is that of mathematical division with the ‘floor’ function applied to the result. Division by zero raises the ZeroDivisionError exception.

The % (modulo) operator yields the remainder from the division of the first argument by the second. The numeric arguments are first converted to a common type. A zero right argument raises the ZeroDivisionError exception. The arguments may be floating point numbers, e.g., 3.14%0.7 equals 0.34 (since 3.14 equals 4 * 0.7 + 0.34.) The modulo operator always yields a result with the same sign as its second operand (or zero); the absolute value of the result is strictly smaller than the absolute value of the second operand [1].

The floor division and modulo operators are connected by the following identity: x == (x//y) * y + (x%y). Floor division and modulo are also connected with the built-in function divmod(): divmod(x, y) == (x//y, x%y). [2].

In addition to performing the modulo operation on numbers, the % operator is also overloaded by string objects to perform old-style string formatting (also known as interpolation). The syntax for string formatting is described in the Python Library Reference, section printf-style String Formatting.

The floor division operator, the modulo operator, and the divmod() function are not defined for complex numbers. Instead, convert to a floating point number using the abs() function if appropriate.

The + (addition) operator yields the sum of its arguments. The arguments must either both be numbers or both be sequences of the same type. In the former case, the numbers are converted to a common type and then added together. In the latter case, the sequences are concatenated.

The - (subtraction) operator yields the difference of its arguments. The numeric arguments are first converted to a common type.


### ===🗝 6.8. Shifting operations

The shifting operations have lower priority than the arithmetic operations:

    shift_expr ::=  a_expr | shift_expr ("<<" | ">>") a_expr

These operators accept integers as arguments. They shift the first argument to the left or right by the number of bits given by the second argument.

A right shift by n bits is defined as floor division by pow(2,n). A left shift by n bits is defined as multiplication with pow(2,n).


### ===🗝 6.9. Binary bitwise operations

Each of the three bitwise operations has a different priority level:

    and_expr ::=  shift_expr | and_expr "&" shift_expr
    xor_expr ::=  and_expr | xor_expr "^" and_expr
    or_expr  ::=  xor_expr | or_expr "|" xor_expr

The & operator yields the bitwise AND of its arguments, which must be integers.

The ^ operator yields the bitwise XOR (exclusive OR) of its arguments, which must be integers.

The | operator yields the bitwise (inclusive) OR of its arguments, which must be integers.


### ===🗝 6.10. Comparisons

Unlike C, all comparison operations in Python have the same priority, which is lower than that of any arithmetic, shifting or bitwise operation. Also unlike C, expressions like a < b < c have the interpretation that is conventional in mathematics:

    comparison    ::=  or_expr (comp_operator or_expr)*
    comp_operator ::=  "<" | ">" | "==" | ">=" | "<=" | "!="
                       | "is" ["not"] | ["not"] "in"

Comparisons yield boolean values: True or False.

Comparisons can be chained arbitrarily, e.g., x < y <= z is equivalent to x < y and y <= z, except that y is evaluated only once (but in both cases z is not evaluated at all when x < y is found to be false).

Formally, if a, b, c, …, y, z are expressions and op1, op2, …, opN are comparison operators, then a op1 b op2 c ... y opN z is equivalent to a op1 b and b op2 c and ... y opN z, except that each expression is evaluated at most once.

Note that a op1 b op2 c doesn’t imply any kind of comparison between a and c, so that, e.g., x < y > z is perfectly legal (though perhaps not pretty).


#### 6.10.1. Value comparisons

The operators <, >, ==, >=, <=, and != compare the values of two objects. The objects do not need to have the same type.

Chapter Objects, values and types states that objects have a value (in addition to type and identity). The value of an object is a rather abstract notion in Python: For example, there is no canonical access method for an object’s value. Also, there is no requirement that the value of an object should be constructed in a particular way, e.g. comprised of all its data attributes. Comparison operators implement a particular notion of what the value of an object is. One can think of them as defining the value of an object indirectly, by means of their comparison implementation.

Because all types are (direct or indirect) subtypes of object, they inherit the default comparison behavior from object. Types can customize their comparison behavior by implementing rich comparison methods like __lt__(), described in Basic customization.

The default behavior for equality comparison (== and !=) is based on the identity of the objects. Hence, equality comparison of instances with the same identity results in equality, and equality comparison of instances with different identities results in inequality. A motivation for this default behavior is the desire that all objects should be reflexive (i.e. x is y implies x == y).

A default order comparison (<, >, <=, and >=) is not provided; an attempt raises TypeError. A motivation for this default behavior is the lack of a similar invariant as for equality.

The behavior of the default equality comparison, that instances with different identities are always unequal, may be in contrast to what types will need that have a sensible definition of object value and value-based equality. Such types will need to customize their comparison behavior, and in fact, a number of built-in types have done that.

✅ The following list describes the comparison behavior of the most important built-in types.

↪ •Numbers of built-in numeric types (Numeric Types — int, float, complex) and of the standard library types fractions.Fraction and decimal.Decimal can be compared within and across their types, with the restriction that complex numbers do not support order comparison. Within the limits of the types involved, they compare mathematically (algorithmically) correct without loss of precision.

The not-a-number values float('NaN') and decimal.Decimal('NaN') are special. Any ordered comparison of a number to a not-a-number value is false. A counter-intuitive implication is that not-a-number values are not equal to themselves. For example, if x = float('NaN'), 3 < x, x < 3 and x == x are all false, while x != x is true. This behavior is compliant with IEEE 754.


↪ •None and NotImplemented are singletons. PEP 8 advises that comparisons for singletons should always be done with is or is not, never the equality operators.


↪ •Binary sequences (instances of bytes or bytearray) can be compared within and across their types. They compare lexicographically using the numeric values of their elements.


↪ •Strings (instances of str) compare lexicographically using the numerical Unicode code points (the result of the built-in function ord()) of their characters. [3]

Strings and binary sequences cannot be directly compared.


↪ •Sequences (instances of tuple, list, or range) can be compared only within each of their types, with the restriction that ranges do not support order comparison. Equality comparison across these types results in inequality, and ordering comparison across these types raises TypeError.

Sequences compare lexicographically using comparison of corresponding elements. The built-in containers typically assume identical objects are equal to themselves. That lets them bypass equality tests for identical objects to improve performance and to maintain their internal invariants.

Lexicographical comparison between built-in collections works as follows:
◦For two collections to compare equal, they must be of the same type, have the same length, and each pair of corresponding elements must compare equal (for example, [1,2] == (1,2) is false because the type is not the same).
◦Collections that support order comparison are ordered the same as their first unequal elements (for example, [1,2,x] <= [1,2,y] has the same value as x <= y). If a corresponding element does not exist, the shorter collection is ordered first (for example, [1,2] < [1,2,3] is true).


↪ •Mappings (instances of dict) compare equal if and only if they have equal (key, value) pairs. Equality comparison of the keys and values enforces reflexivity.

Order comparisons (<, >, <=, and >=) raise TypeError.


↪ •Sets (instances of set or frozenset) can be compared within and across their types.

They define order comparison operators to mean subset and superset tests. Those relations do not define total orderings (for example, the two sets {1,2} and {2,3} are not equal, nor subsets of one another, nor supersets of one another). Accordingly, sets are not appropriate arguments for functions which depend on total ordering (for example, min(), max(), and sorted() produce undefined results given a list of sets as inputs).

Comparison of sets enforces reflexivity of its elements.


↪ •Most other built-in types have no comparison methods implemented, so they inherit the default comparison behavior.


✅ User-defined classes that customize their comparison behavior should follow some consistency rules, if possible:

↪•Equality comparison should be reflexive. In other words, identical objects should compare equal:



    x is y implies x == y


↪ •Comparison should be symmetric. In other words, the following expressions should have the same result:


    x == y and y == x
    x != y and y != x
    x < y and y > x
    x <= y and y >= x


↪ •Comparison should be transitive. The following (non-exhaustive) examples illustrate that:

    x > y and y > z implies x > z
    x < y and y <= z implies x < z


↪ •Inverse comparison should result in the boolean negation. In other words, the following expressions should have the same result:


    x == y and not x != y
    x < y and not x >= y (for total ordering)
    x > y and not x <= y (for total ordering)

The last two expressions apply to totally ordered collections (e.g. to sequences, but not to sets or mappings). See also the total_ordering() decorator.


↪ •The hash() result should be consistent with equality. Objects that are equal should either have the same hash value, or be marked as unhashable.


Python does not enforce these consistency rules. In fact, the not-a-number values are an example for not following these rules.


#### 6.10.2. Membership test operations

The operators in and not in test for membership. x in s evaluates to True if x is a member of s, and False otherwise. x not in s returns the negation of x in s. All built-in sequences and set types support this as well as dictionary, for which in tests whether the dictionary has a given key. For container types such as list, tuple, set, frozenset, dict, or collections.deque, the expression x in y is equivalent to any(x is e or x == e for e in y).

For the string and bytes types, x in y is True if and only if x is a substring of y. An equivalent test is y.find(x) != -1. Empty strings are always considered to be a substring of any other string, so "" in "abc" will return True.

For user-defined classes which define the __contains__() method, x in y returns True if y.__contains__(x) returns a true value, and False otherwise.

For user-defined classes which do not define __contains__() but do define __iter__(), x in y is True if some value z, for which the expression x is z or x == z is true, is produced while iterating over y. If an exception is raised during the iteration, it is as if in raised that exception.

Lastly, the old-style iteration protocol is tried: if a class defines __getitem__(), x in y is True if and only if there is a non-negative integer index i such that x is y[i] or x == y[i], and no lower integer index raises the IndexError exception. (If any other exception is raised, it is as if in raised that exception).

The operator not in is defined to have the inverse truth value of in.


#### 6.10.3. Identity comparisons

The operators is and is not test for an object’s identity: x is y is true if and only if x and y are the same object. An Object’s identity is determined using the id() function. x is not y yields the inverse truth value. [4]


### ===🗝 6.11. Boolean operations

    or_test  ::=  and_test | or_test "or" and_test
    and_test ::=  not_test | and_test "and" not_test
    not_test ::=  comparison | "not" not_test


In the context of Boolean operations, and also when expressions are used by control flow statements, the following values are interpreted as false: False, None, numeric zero of all types, and empty strings and containers (including strings, tuples, lists, dictionaries, sets and frozensets). All other values are interpreted as true. User-defined objects can customize their truth value by providing a __bool__() method.

The operator not yields True if its argument is false, False otherwise.

The expression x and y first evaluates x; if x is false, its value is returned; otherwise, y is evaluated and the resulting value is returned.

The expression x or y first evaluates x; if x is true, its value is returned; otherwise, y is evaluated and the resulting value is returned.

Note that neither and nor or restrict the value and type they return to False and True, but rather return the last evaluated argument. This is sometimes useful, e.g., if s is a string that should be replaced by a default value if it is empty, the expression s or 'foo' yields the desired value. Because not has to create a new value, it returns a boolean value regardless of the type of its argument (for example, not 'foo' produces False rather than ''.)


### ===🗝 6.12. Assignment expressions

    assignment_expression ::=  [identifier ":="] expression

An assignment expression (sometimes also called a “named expression” or “walrus”) assigns an expression to an identifier, while also returning the value of the expression.

One common use case is when handling matched regular expressions:


    if matching := pattern.search(data):
        do_something(matching)


Or, when processing a file stream in chunks:


    while chunk := file.read(9000):
        process(chunk)


New in version 3.8: See PEP 572 for more details about assignment expressions.



### ===🗝 6.13. Conditional expressions

    conditional_expression ::=  or_test ["if" or_test "else" expression]
    expression             ::=  conditional_expression | lambda_expr

Conditional expressions (sometimes called a “ternary operator”) have the lowest priority of all Python operations.

The expression x if C else y first evaluates the condition, C rather than x. If C is true, x is evaluated and its value is returned; otherwise, y is evaluated and its value is returned.

See PEP 308 for more details about conditional expressions.


### ===🗝 6.14. Lambdas

    lambda_expr ::=  "lambda" [parameter_list] ":" expression

Lambda expressions (sometimes called lambda forms) are used to create anonymous functions. The expression lambda parameters: expression yields a function object. The unnamed object behaves like a function object defined with:


    def <lambda>(parameters):
        return expression


See section Function definitions for the syntax of parameter lists. Note that functions created with lambda expressions cannot contain statements or annotations.


### ===🗝 6.15. Expression lists

    expression_list    ::=  expression ("," expression)* [","]
    starred_list       ::=  starred_item ("," starred_item)* [","]
    starred_expression ::=  expression | (starred_item ",")* [starred_item]
    starred_item       ::=  assignment_expression | "*" or_expr

Except when part of a list or set display, an expression list containing at least one comma yields a tuple. The length of the tuple is the number of expressions in the list. The expressions are evaluated from left to right.

An asterisk * denotes iterable unpacking. Its operand must be an iterable. The iterable is expanded into a sequence of items, which are included in the new tuple, list, or set, at the site of the unpacking.


New in version 3.5: Iterable unpacking in expression lists, originally proposed by PEP 448.

The trailing comma is required only to create a single tuple (a.k.a. a singleton); it is optional in all other cases. A single expression without a trailing comma doesn’t create a tuple, but rather yields the value of that expression. (To create an empty tuple, use an empty pair of parentheses: ().)


### ===🗝 6.16. Evaluation order

Python evaluates expressions from left to right. Notice that while evaluating an assignment, the right-hand side is evaluated before the left-hand side.

In the following lines, expressions will be evaluated in the arithmetic order of their suffixes:


    expr1, expr2, expr3, expr4
    (expr1, expr2, expr3, expr4)
    {expr1: expr2, expr3: expr4}
    expr1 + expr2 * (expr3 - expr4)
    expr1(expr2, expr3, *expr4, **expr5)
    expr3, expr4 = expr1, expr2



### ===🗝 6.17. Operator precedence

The following table summarizes the operator precedence in Python, from highest precedence (most binding) to lowest precedence (least binding). Operators in the same box have the same precedence. Unless the syntax is explicitly given, operators are binary. Operators in the same box group left to right (except for exponentiation, which groups from right to left).

Note that comparisons, membership tests, and identity tests, all have the same precedence and have a left-to-right chaining feature as described in the Comparisons section.


    | Operator | ↪ Description

    (expressions...),
    [expressions...], {key: value...}, {expressions...}
                ↪ Binding or parenthesized expression, list display, dictionary display, set display 
    x[index], x[index:index], x(arguments...), x.attribute 
                ↪ Subscription, slicing, call, attribute reference 
    await x     ↪ Await expression 
    **          ↪ Exponentiation [5] 
    +x, -x, ~x  ↪ Positive, negative, bitwise NOT 
    *, @, /, //, % 
                ↪ Multiplication, matrix multiplication, division, floor division, remainder [6] 
    +, -        ↪ Addition and subtraction 
    <<, >>      ↪ Shifts 
    &           ↪ Bitwise AND 
    ^           ↪ Bitwise XOR 
    |           ↪ Bitwise OR 
    in, not in, is, is not, <, <=, >, >=, !=, == 
                ↪ Comparisons, including membership tests and identity tests 
    not x       ↪ Boolean NOT 
    and         ↪ Boolean AND 
    or          ↪ Boolean OR 
    if – else   ↪ Conditional expression 
    lambda      ↪ Lambda expression 
    :=          ↪ Assignment expression 

Footnotes

[1] While abs(x%y) < abs(y) is true mathematically, for floats it may not be true numerically due to roundoff. For example, and assuming a platform on which a Python float is an IEEE 754 double-precision number, in order that -1e-100 % 1e100 have the same sign as 1e100, the computed result is -1e-100 + 1e100, which is numerically exactly equal to 1e100. The function math.fmod() returns a result whose sign matches the sign of the first argument instead, and so returns -1e-100 in this case. Which approach is more appropriate depends on the application. 

[2] If x is very close to an exact integer multiple of y, it’s possible for x//y to be one larger than (x-x%y)//y due to rounding. In such cases, Python returns the latter result, in order to preserve that divmod(x,y)[0] * y + x % y be very close to x. 

[3] The Unicode standard distinguishes between code points (e.g. U+0041) and abstract characters (e.g. “LATIN CAPITAL LETTER A”). While most abstract characters in Unicode are only represented using one code point, there is a number of abstract characters that can in addition be represented using a sequence of more than one code point. For example, the abstract character “LATIN CAPITAL LETTER C WITH CEDILLA” can be represented as a single precomposed character at code position U+00C7, or as a sequence of a base character at code position U+0043 (LATIN CAPITAL LETTER C), followed by a combining character at code position U+0327 (COMBINING CEDILLA).

The comparison operators on strings compare at the level of Unicode code points. This may be counter-intuitive to humans. For example, "\u00C7" == "\u0043\u0327" is False, even though both strings represent the same abstract character “LATIN CAPITAL LETTER C WITH CEDILLA”.

To compare strings at the level of abstract characters (that is, in a way intuitive to humans), use unicodedata.normalize().

[4] Due to automatic garbage-collection, free lists, and the dynamic nature of descriptors, you may notice seemingly unusual behaviour in certain uses of the is operator, like those involving comparisons between instance methods, or constants. Check their documentation for more info. 
[5] The power operator ** binds less tightly than an arithmetic or bitwise unary operator on its right, that is, 2**-1 is 0.5. 
[6] The % operator is also used for string formatting; the same precedence applies. 


## ==⚡ 7. Simple statements
- The Python Language Reference » 7. Simple statements

A simple statement is comprised within a single logical line. Several simple statements may occur on a single line separated by semicolons. The syntax for simple statements is:

    simple_stmt ::=  expression_stmt
                     | assert_stmt
                     | assignment_stmt
                     | augmented_assignment_stmt
                     | annotated_assignment_stmt
                     | pass_stmt
                     | del_stmt
                     | return_stmt
                     | yield_stmt
                     | raise_stmt
                     | break_stmt
                     | continue_stmt
                     | import_stmt
                     | future_stmt
                     | global_stmt
                     | nonlocal_stmt


### ===🗝 7.1. Expression statements

Expression statements are used (mostly interactively) to compute and write a value, or (usually) to call a procedure (a function that returns no meaningful result; in Python, procedures return the value None). Other uses of expression statements are allowed and occasionally useful. The syntax for an expression statement is:

    expression_stmt ::=  starred_expression

An expression statement evaluates the expression list (which may be a single expression).

In interactive mode, if the value is not None, it is converted to a string using the built-in repr() function and the resulting string is written to standard output on a line by itself (except if the result is None, so that procedure calls do not cause any output.)


#### 7.2. Assignment statements

Assignment statements are used to (re)bind names to values and to modify attributes or items of mutable objects:

    assignment_stmt ::=  (target_list "=")+ (starred_expression | yield_expression)
    target_list     ::=  target ("," target)* [","]
    target          ::=  identifier
                         | "(" [target_list] ")"
                         | "[" [target_list] "]"
                         | attributeref
                         | subscription
                         | slicing
                         | "*" target

(See section Primaries for the syntax definitions for attributeref, subscription, and slicing.)

An assignment statement evaluates the expression list (remember that this can be a single expression or a comma-separated list, the latter yielding a tuple) and assigns the single resulting object to each of the target lists, from left to right.

Assignment is defined recursively depending on the form of the target (list). When a target is part of a mutable object (an attribute reference, subscription or slicing), the mutable object must ultimately perform the assignment and decide about its validity, and may raise an exception if the assignment is unacceptable. The rules observed by various types and the exceptions raised are given with the definition of the object types (see section The standard type hierarchy).

Assignment of an object to a target list, optionally enclosed in parentheses or square brackets, is recursively defined as follows.
•If the target list is a single target with no trailing comma, optionally in parentheses, the object is assigned to that target.
•Else: The object must be an iterable with the same number of items as there are targets in the target list, and the items are assigned, from left to right, to the corresponding targets.◦If the target list contains one target prefixed with an asterisk, called a “starred” target: The object must be an iterable with at least as many items as there are targets in the target list, minus one. The first items of the iterable are assigned, from left to right, to the targets before the starred target. The final items of the iterable are assigned to the targets after the starred target. A list of the remaining items in the iterable is then assigned to the starred target (the list can be empty).
◦Else: The object must be an iterable with the same number of items as there are targets in the target list, and the items are assigned, from left to right, to the corresponding targets.


Assignment of an object to a single target is recursively defined as follows.

•If the target is an identifier (name):
◦If the name does not occur in a global or nonlocal statement in the current code block: the name is bound to the object in the current local namespace.
◦Otherwise: the name is bound to the object in the global namespace or the outer namespace determined by nonlocal, respectively.

The name is rebound if it was already bound. This may cause the reference count for the object previously bound to the name to reach zero, causing the object to be deallocated and its destructor (if it has one) to be called.


•If the target is an attribute reference: The primary expression in the reference is evaluated. It should yield an object with assignable attributes; if this is not the case, TypeError is raised. That object is then asked to assign the assigned object to the given attribute; if it cannot perform the assignment, it raises an exception (usually but not necessarily AttributeError).

Note: If the object is a class instance and the attribute reference occurs on both sides of the assignment operator, the right-hand side expression, a.x can access either an instance attribute or (if no instance attribute exists) a class attribute. The left-hand side target a.x is always set as an instance attribute, creating it if necessary. Thus, the two occurrences of a.x do not necessarily refer to the same attribute: if the right-hand side expression refers to a class attribute, the left-hand side creates a new instance attribute as the target of the assignment:


    class Cls:
        x = 3             # class variable
    inst = Cls()
    inst.x = inst.x + 1   # writes inst.x as 4 leaving Cls.x as 3


This description does not necessarily apply to descriptor attributes, such as properties created with property().


•If the target is a subscription: The primary expression in the reference is evaluated. It should yield either a mutable sequence object (such as a list) or a mapping object (such as a dictionary). Next, the subscript expression is evaluated.

If the primary is a mutable sequence object (such as a list), the subscript must yield an integer. If it is negative, the sequence’s length is added to it. The resulting value must be a nonnegative integer less than the sequence’s length, and the sequence is asked to assign the assigned object to its item with that index. If the index is out of range, IndexError is raised (assignment to a subscripted sequence cannot add new items to a list).

If the primary is a mapping object (such as a dictionary), the subscript must have a type compatible with the mapping’s key type, and the mapping is then asked to create a key/datum pair which maps the subscript to the assigned object. This can either replace an existing key/value pair with the same key value, or insert a new key/value pair (if no key with the same value existed).

For user-defined objects, the __setitem__() method is called with appropriate arguments.


•If the target is a slicing: The primary expression in the reference is evaluated. It should yield a mutable sequence object (such as a list). The assigned object should be a sequence object of the same type. Next, the lower and upper bound expressions are evaluated, insofar they are present; defaults are zero and the sequence’s length. The bounds should evaluate to integers. If either bound is negative, the sequence’s length is added to it. The resulting bounds are clipped to lie between zero and the sequence’s length, inclusive. Finally, the sequence object is asked to replace the slice with the items of the assigned sequence. The length of the slice may be different from the length of the assigned sequence, thus changing the length of the target sequence, if the target sequence allows it.



CPython implementation detail: In the current implementation, the syntax for targets is taken to be the same as for expressions, and invalid syntax is rejected during the code generation phase, causing less detailed error messages.

Although the definition of assignment implies that overlaps between the left-hand side and the right-hand side are ‘simultaneous’ (for example a, b = b, a swaps two variables), overlaps within the collection of assigned-to variables occur left-to-right, sometimes resulting in confusion. For instance, the following program prints [0, 2]:


    x = [0, 1]
    i = 0
    i, x[i] = 1, 2         # i is updated, then x[i] is updated
    print(x)


See also:
 PEP 3132 - Extended Iterable UnpackingThe specification for the *target feature.

#### 7.2.1. Augmented assignment statements

Augmented assignment is the combination, in a single statement, of a binary operation and an assignment statement:

    augmented_assignment_stmt ::=  augtarget augop (expression_list | yield_expression)
    augtarget                 ::=  identifier | attributeref | subscription | slicing
    augop                     ::=  "+=" | "-=" | "*=" | "@=" | "/=" | "//=" | "%=" | "**="
                                   | ">>=" | "<<=" | "&=" | "^=" | "|="

(See section Primaries for the syntax definitions of the last three symbols.)

Python 3.5 引入 PEP0465 @ 运算符号，执行矩阵乘法。它们的目的是清晰区分现有的乘法运算符号，避免之间存在的混淆。简化形式 `A @= B`，与 `A *= B`。

An augmented assignment evaluates the target (which, unlike normal assignment statements, cannot be an unpacking) and the expression list, performs the binary operation specific to the type of assignment on the two operands, and assigns the result to the original target. The target is only evaluated once.

An augmented assignment expression like x += 1 can be rewritten as x = x + 1 to achieve a similar, but not exactly equal effect. In the augmented version, x is only evaluated once. Also, when possible, the actual operation is performed in-place, meaning that rather than creating a new object and assigning that to the target, the old object is modified instead.

Unlike normal assignments, augmented assignments evaluate the left-hand side before evaluating the right-hand side. For example, a[i] += f(x) first looks-up a[i], then it evaluates f(x) and performs the addition, and lastly, it writes the result back to a[i].

With the exception of assigning to tuples and multiple targets in a single statement, the assignment done by augmented assignment statements is handled the same way as normal assignments. Similarly, with the exception of the possible in-place behavior, the binary operation performed by augmented assignment is the same as the normal binary operations.

For targets which are attribute references, the same caveat about class and instance attributes applies as for regular assignments.


#### 7.2.2. Annotated assignment statements

Annotation assignment is the combination, in a single statement, of a variable or attribute annotation and an optional assignment statement:

    annotated_assignment_stmt ::=  augtarget ":" expression
                                   ["=" (starred_expression | yield_expression)]

The difference from normal Assignment statements is that only single target is allowed.

For simple names as assignment targets, if in class or module scope, the annotations are evaluated and stored in a special class or module attribute __annotations__ that is a dictionary mapping from variable names (mangled if private) to evaluated annotations. This attribute is writable and is automatically created at the start of class or module body execution, if annotations are found statically.

For expressions as assignment targets, the annotations are evaluated if in class or module scope, but not stored.

If a name is annotated in a function scope, then this name is local for that scope. Annotations are never evaluated and stored in function scopes.

If the right hand side is present, an annotated assignment performs the actual assignment before evaluating annotations (where applicable). If the right hand side is not present for an expression target, then the interpreter evaluates the target except for the last __setitem__() or __setattr__() call.

See also:
 PEP 526 - Syntax for Variable AnnotationsThe proposal that added syntax for annotating the types of variables (including class variables and instance variables), instead of expressing them through comments.PEP 484 - Type hintsThe proposal that added the typing module to provide a standard syntax for type annotations that can be used in static analysis tools and IDEs.

Changed in version 3.8: Now annotated assignments allow same expressions in the right hand side as the regular assignments. Previously, some expressions (like un-parenthesized tuple expressions) caused a syntax error.


### ===🗝 7.3. The assert statement

Assert statements are a convenient way to insert debugging assertions into a program:

    assert_stmt ::=  "assert" expression ["," expression]

The simple form, assert expression, is equivalent to


    if __debug__:
        if not expression: raise AssertionError


The extended form, assert expression1, expression2, is equivalent to


    if __debug__:
        if not expression1: raise AssertionError(expression2)


These equivalences assume that __debug__ and AssertionError refer to the built-in variables with those names. In the current implementation, the built-in variable __debug__ is True under normal circumstances, False when optimization is requested (command line option -O). The current code generator emits no code for an assert statement when optimization is requested at compile time. Note that it is unnecessary to include the source code for the expression that failed in the error message; it will be displayed as part of the stack trace.

Assignments to __debug__ are illegal. The value for the built-in variable is determined when the interpreter starts.


### ===🗝 7.4. The pass statement

    pass_stmt ::=  "pass"

pass is a null operation — when it is executed, nothing happens. It is useful as a placeholder when a statement is required syntactically, but no code needs to be executed, for example:


    def f(arg): pass    # a function that does nothing (yet)
    class C: pass       # a class with no methods (yet)

pass do nothing

    if( 1 ):
        pass
    print("pass do nothing really?");


### ===🗝 7.5. The del statement

    del_stmt ::=  "del" target_list

Deletion is recursively defined very similar to the way assignment is defined. Rather than spelling it out in full details, here are some hints.

Deletion of a target list recursively deletes each target, from left to right.

Deletion of a name removes the binding of that name from the local or global namespace, depending on whether the name occurs in a global statement in the same code block. If the name is unbound, a NameError exception will be raised.

Deletion of attribute references, subscriptions and slicings is passed to the primary object involved; deletion of a slicing is in general equivalent to assignment of an empty slice of the right type (but even this is determined by the sliced object).


Changed in version 3.2: Previously it was illegal to delete a name from the local namespace if it occurs as a free variable in a nested block.


### ===🗝 7.6. The return statement

    return_stmt ::=  "return" [expression_list]

return may only occur syntactically nested in a function definition, not within a nested class definition.

If an expression list is present, it is evaluated, else None is substituted.

return leaves the current function call with the expression list (or None) as return value.

When return passes control out of a try statement with a finally clause, that finally clause is executed before really leaving the function.

In a generator function, the return statement indicates that the generator is done and will cause StopIteration to be raised. The returned value (if any) is used as an argument to construct StopIteration and becomes the StopIteration.value attribute.

In an asynchronous generator function, an empty return statement indicates that the asynchronous generator is done and will cause StopAsyncIteration to be raised. A non-empty return statement is a syntax error in an asynchronous generator function.


### ===🗝 7.7. The yield statement

    yield_stmt ::=  yield_expression

A yield statement is semantically equivalent to a yield expression. The yield statement can be used to omit the parentheses that would otherwise be required in the equivalent yield expression statement. For example, the yield statements


    yield <expr>
    yield from <expr>


are equivalent to the yield expression statements


    (yield <expr>)
    (yield from <expr>)


Yield expressions and statements are only used when defining a generator function, and are only used in the body of the generator function. Using yield in a function definition is sufficient to cause that definition to create a generator function instead of a normal function.

For full details of yield semantics, refer to the Yield expressions section.


### ===🗝 7.8. The raise statement

    raise_stmt ::=  "raise" [expression ["from" expression]]

If no expressions are present, raise re-raises the last exception that was active in the current scope. If no exception is active in the current scope, a RuntimeError exception is raised indicating that this is an error.

Otherwise, raise evaluates the first expression as the exception object. It must be either a subclass or an instance of BaseException. If it is a class, the exception instance will be obtained when needed by instantiating the class with no arguments.

The type of the exception is the exception instance’s class, the value is the instance itself.

A traceback object is normally created automatically when an exception is raised and attached to it as the __traceback__ attribute, which is writable. You can create an exception and set your own traceback in one step using the with_traceback() exception method (which returns the same exception instance, with its traceback set to its argument), like so:


    raise Exception("foo occurred").with_traceback(tracebackobj)


The from clause is used for exception chaining: if given, the second expression must be another exception class or instance. If the second expression is an exception instance, it will be attached to the raised exception as the __cause__ attribute (which is writable). If the expression is an exception class, the class will be instantiated and the resulting exception instance will be attached to the raised exception as the __cause__ attribute. If the raised exception is not handled, both exceptions will be printed:


>>> try:
...     print(1 / 0)
... except Exception as exc:
...     raise RuntimeError("Something bad happened") from exc
...
Traceback (most recent call last):
  File "<stdin>", line 2, in <module>
ZeroDivisionError: division by zero

The above exception was the direct cause of the following exception:

    Traceback (most recent call last):
      File "<stdin>", line 4, in <module>
    RuntimeError: Something bad happened


A similar mechanism works implicitly if an exception is raised inside an exception handler or a finally clause: the previous exception is then attached as the new exception’s __context__ attribute:


>>> try:
...     print(1 / 0)
... except:
...     raise RuntimeError("Something bad happened")
...
Traceback (most recent call last):
  File "<stdin>", line 2, in <module>
ZeroDivisionError: division by zero

During handling of the above exception, another exception occurred:

    Traceback (most recent call last):
      File "<stdin>", line 4, in <module>
    RuntimeError: Something bad happened


Exception chaining can be explicitly suppressed by specifying None in the from clause:


>>> try:
...     print(1 / 0)
... except:
...     raise RuntimeError("Something bad happened") from None
...
Traceback (most recent call last):
  File "<stdin>", line 4, in <module>
RuntimeError: Something bad happened


Additional information on exceptions can be found in section Exceptions, and information about handling exceptions is in section The try statement.


Changed in version 3.3: None is now permitted as Y in raise X from Y.
New in version 3.3: The __suppress_context__ attribute to suppress automatic display of the exception context.


### ===🗝 7.9. The break statement

    break_stmt ::=  "break"

break may only occur syntactically nested in a for or while loop, but not nested in a function or class definition within that loop.

It terminates the nearest enclosing loop, skipping the optional else clause if the loop has one.

If a for loop is terminated by break, the loop control target keeps its current value.

When break passes control out of a try statement with a finally clause, that finally clause is executed before really leaving the loop.


### ===🗝 7.10. The continue statement

    continue_stmt ::=  "continue"

continue may only occur syntactically nested in a for or while loop, but not nested in a function or class definition within that loop. It continues with the next cycle of the nearest enclosing loop.

When continue passes control out of a try statement with a finally clause, that finally clause is executed before really starting the next loop cycle.


### ===🗝 7.11. The import statement

    import_stmt     ::=  "import" module ["as" identifier] ("," module ["as" identifier])*
                         | "from" relative_module "import" identifier ["as" identifier]
                         ("," identifier ["as" identifier])*
                         | "from" relative_module "import" "(" identifier ["as" identifier]
                         ("," identifier ["as" identifier])* [","] ")"
                         | "from" module "import" "*"
    module          ::=  (identifier ".")* identifier
    relative_module ::=  "."* module | "."+

The basic import statement (no from clause) is executed in two steps:
1.find a module, loading and initializing it if necessary
2.define a name or names in the local namespace for the scope where the import statement occurs.

When the statement contains multiple clauses (separated by commas) the two steps are carried out separately for each clause, just as though the clauses had been separated out into individual import statements.

The details of the first step, finding and loading modules are described in greater detail in the section on the import system, which also describes the various types of packages and modules that can be imported, as well as all the hooks that can be used to customize the import system. Note that failures in this step may indicate either that the module could not be located, or that an error occurred while initializing the module, which includes execution of the module’s code.

If the requested module is retrieved successfully, it will be made available in the local namespace in one of three ways:
•If the module name is followed by as, then the name following as is bound directly to the imported module.
•If no other name is specified, and the module being imported is a top level module, the module’s name is bound in the local namespace as a reference to the imported module
•If the module being imported is not a top level module, then the name of the top level package that contains the module is bound in the local namespace as a reference to the top level package. The imported module must be accessed using its full qualified name rather than directly

The from form uses a slightly more complex process:
1.find the module specified in the from clause, loading and initializing it if necessary;
2.for each of the identifiers specified in the import clauses:1.check if the imported module has an attribute by that name
2.if not, attempt to import a submodule with that name and then check the imported module again for that attribute
3.if the attribute is not found, ImportError is raised.
4.otherwise, a reference to that value is stored in the local namespace, using the name in the as clause if it is present, otherwise using the attribute name


Examples:

    import foo                 # foo imported and bound locally
    import foo.bar.baz         # foo.bar.baz imported, foo bound locally
    import foo.bar.baz as fbb  # foo.bar.baz imported and bound as fbb
    from foo.bar import baz    # foo.bar.baz imported and bound as baz
    from foo import attr       # foo imported and foo.attr bound as attr


If the list of identifiers is replaced by a star ( * ), all public names defined in the module are bound in the local namespace for the scope where the import statement occurs.

The public names defined by a module are determined by checking the module’s namespace for a variable named __all__; if defined, it must be a sequence of strings which are names defined or imported by that module. The names given in __all__ are all considered public and are required to exist. If __all__ is not defined, the set of public names includes all names found in the module’s namespace which do not begin with an underscore character ( _ ). __all__ should contain the entire public API. It is intended to avoid accidentally exporting items that are not part of the API (such as library modules which were imported and used within the module).

The wild card form of import — `from module import *` — is only allowed at the module level. Attempting to use it in class or function definitions will raise a `SyntaxError`.

When specifying what module to import you do not have to specify the absolute name of the module. When a module or package is contained within another package it is possible to make a relative import within the same top package without having to mention the package name. By using leading dots in the specified module or package after `from` you can specify how high to traverse up the current package hierarchy without specifying exact names. One leading dot means the current package where the module making the import exists. Two dots means up one package level. Three dots is up two levels, etc. So if you execute `from . import mod` from a module in the pkg package then you will end up importing pkg.mod. If you execute `from ..subpkg2 import mod` from within pkg.subpkg1 you will import pkg.subpkg2.mod. The specification for relative imports is contained in the Package Relative Imports section.

`importlib.import_module()` is provided to support applications that determine dynamically the modules to be loaded.

Raises an auditing event import with arguments module, filename, sys.path, sys.meta_path, sys.path_hooks.


### ===🗝 7.11.1. Future statements

A future statement is a directive to the compiler that a particular module should be compiled using syntax or semantics that will be available in a specified future release of Python where the feature becomes standard.

The future statement is intended to ease migration to future versions of Python that introduce incompatible changes to the language. It allows use of the new features on a per-module basis before the release in which the feature becomes standard.

    future_stmt ::=  "from" "__future__" "import" feature ["as" identifier]
                     ("," feature ["as" identifier])*
                     | "from" "__future__" "import" "(" feature ["as" identifier]
                     ("," feature ["as" identifier])* [","] ")"
    feature     ::=  identifier

A future statement must appear near the top of the module. The only lines that can appear before a future statement are:
•the module docstring (if any),
•comments,
•blank lines, and
•other future statements.

The only feature that requires using the future statement is annotations (see PEP 563).

All historical features enabled by the future statement are still recognized by Python 3. The list includes absolute_import, division, generators, generator_stop, unicode_literals, print_function, nested_scopes and with_statement. They are all redundant because they are always enabled, and only kept for backwards compatibility.

A future statement is recognized and treated specially at compile time: Changes to the semantics of core constructs are often implemented by generating different code. It may even be the case that a new feature introduces new incompatible syntax (such as a new reserved word), in which case the compiler may need to parse the module differently. Such decisions cannot be pushed off until runtime.

For any given release, the compiler knows which feature names have been defined, and raises a compile-time error if a future statement contains a feature not known to it.

The direct runtime semantics are the same as for any import statement: there is a standard module __future__, described later, and it will be imported in the usual way at the time the future statement is executed.

The interesting runtime semantics depend on the specific feature enabled by the future statement.

Note that there is nothing special about the statement:

    import __future__ [as name]


That is not a future statement; it’s an ordinary import statement with no special semantics or syntax restrictions.

Code compiled by calls to the built-in functions exec() and compile() that occur in a module M containing a future statement will, by default, use the new syntax or semantics associated with the future statement. This can be controlled by optional arguments to compile() — see the documentation of that function for details.

A future statement typed at an interactive interpreter prompt will take effect for the rest of the interpreter session. If an interpreter is started with the -i option, is passed a script name to execute, and the script includes a future statement, it will be in effect in the interactive session started after the script is executed.

See also:
 PEP 236 - Back to the __future__The original proposal for the __future__ mechanism.

### ===🗝 7.12. The global statement

    global_stmt ::=  "global" identifier ("," identifier)*

The global statement is a declaration which holds for the entire current code block. It means that the listed identifiers are to be interpreted as globals. It would be impossible to assign to a global variable without global, although free variables may refer to globals without being declared global.

Names listed in a global statement must not be used in the same code block textually preceding that global statement.

Names listed in a global statement must not be defined as formal parameters or in a for loop control target, class definition, function definition, import statement, or variable annotation.


CPython implementation detail: The current implementation does not enforce some of these restrictions, but programs should not abuse this freedom, as future implementations may enforce them or silently change the meaning of the program.

Programmer’s note: global is a directive to the parser. It applies only to code parsed at the same time as the global statement. In particular, a global statement contained in a string or code object supplied to the built-in exec() function does not affect the code block containing the function call, and code contained in such a string is unaffected by global statements in the code containing the function call. The same applies to the eval() and compile() functions.


### ===🗝 7.13. The nonlocal statement

    nonlocal_stmt ::=  "nonlocal" identifier ("," identifier)*

The nonlocal statement causes the listed identifiers to refer to previously bound variables in the nearest enclosing scope excluding globals. This is important because the default behavior for binding is to search the local namespace first. The statement allows encapsulated code to rebind variables outside of the local scope besides the global (module) scope.

Names listed in a nonlocal statement, unlike those listed in a global statement, must refer to pre-existing bindings in an enclosing scope (the scope in which a new binding should be created cannot be determined unambiguously).

Names listed in a nonlocal statement must not collide with pre-existing bindings in the local scope.

See also:
 PEP 3104 - Access to Names in Outer ScopesThe specification for the nonlocal statement.


 
## ==⚡ 8. Compound statements
- The Python Language Reference » 8. Compound statements

Compound statements contain (groups of) other statements; they affect or control the execution of those other statements in some way. In general, compound statements span multiple lines, although in simple incarnations a whole compound statement may be contained in one line.

The if, while and for statements implement traditional control flow constructs. try specifies exception handlers and/or cleanup code for a group of statements, while the with statement allows the execution of initialization and finalization code around a block of code. Function and class definitions are also syntactically compound statements.

A compound statement consists of one or more ‘clauses.’ A clause consists of a header and a ‘suite.’ The clause headers of a particular compound statement are all at the same indentation level. Each clause header begins with a uniquely identifying keyword and ends with a colon. A suite is a group of statements controlled by a clause. A suite can be one or more semicolon-separated simple statements on the same line as the header, following the header’s colon, or it can be one or more indented statements on subsequent lines. Only the latter form of a suite can contain nested compound statements; the following is illegal, mostly because it wouldn’t be clear to which if clause a following else clause would belong:


    if test1: if test2: print(x)


Also note that the semicolon binds tighter than the colon in this context, so that in the following example, either all or none of the print() calls are executed:


    if x < y < z: print(x); print(y); print(z)


    Summarizing:
    compound_stmt ::=  if_stmt
                       | while_stmt
                       | for_stmt
                       | try_stmt
                       | with_stmt
                       | funcdef
                       | classdef
                       | async_with_stmt
                       | async_for_stmt
                       | async_funcdef
    suite         ::=  stmt_list NEWLINE | NEWLINE INDENT statement+ DEDENT
    statement     ::=  stmt_list NEWLINE | compound_stmt
    stmt_list     ::=  simple_stmt (";" simple_stmt)* [";"]


Note that statements always end in a NEWLINE possibly followed by a DEDENT. Also note that optional continuation clauses always begin with a keyword that cannot start a statement, thus there are no ambiguities (the ‘dangling else’ problem is solved in Python by requiring nested if statements to be indented).

The formatting of the grammar rules in the following sections places each clause on a separate line for clarity.


### ===🗝 8.1. The if statement

The if statement is used for conditional execution:
if_stmt ::=  "if" assignment_expression ":" suite
             ("elif" assignment_expression ":" suite)*
             ["else" ":" suite]


It selects exactly one of the suites by evaluating the expressions one by one until one is found to be true (see section Boolean operations for the definition of true and false); then that suite is executed (and no other part of the if statement is executed or evaluated). If all expressions are false, the suite of the else clause, if present, is executed.


### ===🗝 8.2. The while statement

The while statement is used for repeated execution as long as an expression is true:
while_stmt ::=  "while" assignment_expression ":" suite
                ["else" ":" suite]


This repeatedly tests the expression and, if it is true, executes the first suite; if the expression is false (which may be the first time it is tested) the suite of the else clause, if present, is executed and the loop terminates.

A break statement executed in the first suite terminates the loop without executing the else clause’s suite. A continue statement executed in the first suite skips the rest of the suite and goes back to testing the expression.


### ===🗝 8.3. The for statement

The for statement is used to iterate over the elements of a sequence (such as a string, tuple or list) or other iterable object:

    for_stmt ::=  "for" target_list "in" expression_list ":" suite
                  ["else" ":" suite]

The expression list is evaluated once; it should yield an iterable object. An iterator is created for the result of the expression_list. The suite is then executed once for each item provided by the iterator, in the order returned by the iterator. Each item in turn is assigned to the target list using the standard rules for assignments (see Assignment statements), and then the suite is executed. When the items are exhausted (which is immediately when the sequence is empty or an iterator raises a StopIteration exception), the suite in the else clause, if present, is executed, and the loop terminates.

A break statement executed in the first suite terminates the loop without executing the else clause’s suite. A continue statement executed in the first suite skips the rest of the suite and continues with the next item, or with the else clause if there is no next item.

The for-loop makes assignments to the variables in the target list. This overwrites all previous assignments to those variables including those made in the suite of the for-loop:


    for i in range(10):
        print(i)
        i = 5             # this will not affect the for-loop
                          # because i will be overwritten with the next
                          # index in the range


Names in the target list are not deleted when the loop is finished, but if the sequence is empty, they will not have been assigned to at all by the loop. Hint: the built-in function range() returns an iterator of integers suitable to emulate the effect of Pascal’s for i := a to b do; e.g., list(range(3)) returns the list [0, 1, 2].

Note:
 There is a subtlety when the sequence is being modified by the loop (this can only occur for mutable sequences, e.g. lists). An internal counter is used to keep track of which item is used next, and this is incremented on each iteration. When this counter has reached the length of the sequence the loop terminates. This means that if the suite deletes the current (or a previous) item from the sequence, the next item will be skipped (since it gets the index of the current item which has already been treated). Likewise, if the suite inserts an item in the sequence before the current item, the current item will be treated again the next time through the loop. This can lead to nasty bugs that can be avoided by making a temporary copy using a slice of the whole sequence, e.g.,
 

    for x in a[:]:
        if x < 0: a.remove(x)


>>>a = list(range(1,9))
for x in a:
    print(x)
    # if x<5: a.remove(x)
    a.remove(x) if x < 5 else None
print(a)
Python 3.7.9
1
3
5
6
7
8
[2, 4, 5, 6, 7, 8]


### ===🗝 8.4. The try statement

The try statement specifies exception handlers and/or cleanup code for a group of statements:

    try_stmt  ::=  try1_stmt | try2_stmt
    try1_stmt ::=  "try" ":" suite
                   ("except" [expression ["as" identifier]] ":" suite)+
                   ["else" ":" suite]
                   ["finally" ":" suite]
    try2_stmt ::=  "try" ":" suite
                   "finally" ":" suite

The except clause(s) specify one or more exception handlers. When no exception occurs in the try clause, no exception handler is executed. When an exception occurs in the try suite, a search for an exception handler is started. This search inspects the except clauses in turn until one is found that matches the exception. An expression-less except clause, if present, must be last; it matches any exception. For an except clause with an expression, that expression is evaluated, and the clause matches the exception if the resulting object is “compatible” with the exception. An object is compatible with an exception if it is the class or a base class of the exception object, or a tuple containing an item that is the class or a base class of the exception object.

If no except clause matches the exception, the search for an exception handler continues in the surrounding code and on the invocation stack. [1]

If the evaluation of an expression in the header of an except clause raises an exception, the original search for a handler is canceled and a search starts for the new exception in the surrounding code and on the call stack (it is treated as if the entire try statement raised the exception).

When a matching except clause is found, the exception is assigned to the target specified after the as keyword in that except clause, if present, and the except clause’s suite is executed. All except clauses must have an executable block. When the end of this block is reached, execution continues normally after the entire try statement. (This means that if two nested handlers exist for the same exception, and the exception occurs in the try clause of the inner handler, the outer handler will not handle the exception.)

When an exception has been assigned using as target, it is cleared at the end of the except clause. This is as if


    except E as N:
        foo


was translated to


    except E as N:
        try:
            foo
        finally:
            del N


This means the exception must be assigned to a different name to be able to refer to it after the except clause. Exceptions are cleared because with the traceback attached to them, they form a reference cycle with the stack frame, keeping all locals in that frame alive until the next garbage collection occurs.

Before an except clause’s suite is executed, details about the exception are stored in the sys module and can be accessed via sys.exc_info(). sys.exc_info() returns a 3-tuple consisting of the exception class, the exception instance and a traceback object (see section The standard type hierarchy) identifying the point in the program where the exception occurred. sys.exc_info() values are restored to their previous values (before the call) when returning from a function that handled an exception.

The optional else clause is executed if the control flow leaves the try suite, no exception was raised, and no return, continue, or break statement was executed. Exceptions in the else clause are not handled by the preceding except clauses.

If finally is present, it specifies a ‘cleanup’ handler. The try clause is executed, including any except and else clauses. If an exception occurs in any of the clauses and is not handled, the exception is temporarily saved. The finally clause is executed. If there is a saved exception it is re-raised at the end of the finally clause. If the finally clause raises another exception, the saved exception is set as the context of the new exception. If the finally clause executes a return, break or continue statement, the saved exception is discarded:


>>> def f():
...     try:
...         1/0
...     finally:
...         return 42
...
>>> f()
42


The exception information is not available to the program during execution of the finally clause.

When a return, break or continue statement is executed in the try suite of a try…finally statement, the finally clause is also executed ‘on the way out.’

The return value of a function is determined by the last return statement executed. Since the finally clause always executes, a return statement executed in the finally clause will always be the last one executed:


>>> def foo():
...     try:
...         return 'try'
...     finally:
...         return 'finally'
...
>>> foo()
'finally'


Additional information on exceptions can be found in section Exceptions, and information on using the raise statement to generate exceptions may be found in section The raise statement.


Changed in version 3.8: Prior to Python 3.8, a continue statement was illegal in the finally clause due to a problem with the implementation.


### ===🗝 8.5. The with statement

The with statement is used to wrap the execution of a block with methods defined by a context manager (see section With Statement Context Managers). This allows common try…except…finally usage patterns to be encapsulated for convenient reuse.
with_stmt ::=  "with" with_item ("," with_item)* ":" suite
with_item ::=  expression ["as" target]


The execution of the with statement with one “item” proceeds as follows:

1.The context expression (the expression given in the with_item) is evaluated to obtain a context manager.
2.The context manager’s __enter__() is loaded for later use.
3.The context manager’s __exit__() is loaded for later use.
4.The context manager’s __enter__() method is invoked.
5.If a target was included in the with statement, the return value from __enter__() is assigned to it.

Note:
 The with statement guarantees that if the __enter__() method returns without an error, then __exit__() will always be called. Thus, if an error occurs during the assignment to the target list, it will be treated the same as an error occurring within the suite would be. See step 6 below.
 
6.The suite is executed.
7.The context manager’s __exit__() method is invoked. If an exception caused the suite to be exited, its type, value, and traceback are passed as arguments to __exit__(). Otherwise, three None arguments are supplied.

If the suite was exited due to an exception, and the return value from the __exit__() method was false, the exception is reraised. If the return value was true, the exception is suppressed, and execution continues with the statement following the with statement.

If the suite was exited for any reason other than an exception, the return value from __exit__() is ignored, and execution proceeds at the normal location for the kind of exit that was taken.


The following code:


    with EXPRESSION as TARGET:
        SUITE


is semantically equivalent to:


    manager = (EXPRESSION)
    enter = type(manager).__enter__
    exit = type(manager).__exit__
    value = enter(manager)
    hit_except = False

    try:
        TARGET = value
        SUITE
    except:
        hit_except = True
        if not exit(manager, *sys.exc_info()):
            raise
    finally:
        if not hit_except:
            exit(manager, None, None, None)


With more than one item, the context managers are processed as if multiple with statements were nested:


    with A() as a, B() as b:
        SUITE


is semantically equivalent to:


    with A() as a:
        with B() as b:
            SUITE

Changed in version 3.1: Support for multiple context expressions.

See also:
 PEP 343 - The “with” statementThe specification, background, and examples for the Python with statement.

### ===🗝 8.6. Function definitions

A function definition defines a user-defined function object (see section The standard type hierarchy):

    funcdef                   ::=  [decorators] "def" funcname "(" [parameter_list] ")"
                                   ["->" expression] ":" suite
    decorators                ::=  decorator+
    decorator                 ::=  "@" assignment_expression NEWLINE
    parameter_list            ::=  defparameter ("," defparameter)* "," "/" ["," [parameter_list_no_posonly]]
                                     | parameter_list_no_posonly
    parameter_list_no_posonly ::=  defparameter ("," defparameter)* ["," [parameter_list_starargs]]
                                   | parameter_list_starargs
    parameter_list_starargs   ::=  "*" [parameter] ("," defparameter)* ["," ["**" parameter [","]]]
                                   | "**" parameter [","]
    parameter                 ::=  identifier [":" expression]
    defparameter              ::=  parameter ["=" expression]
    funcname                  ::=  identifier

A function definition is an executable statement. Its execution binds the function name in the current local namespace to a function object (a wrapper around the executable code for the function). This function object contains a reference to the current global namespace as the global namespace to be used when the function is called.

The function definition does not execute the function body; this gets executed only when the function is called. [2]

A function definition may be wrapped by one or more decorator expressions. Decorator expressions are evaluated when the function is defined, in the scope that contains the function definition. The result must be a callable, which is invoked with the function object as the only argument. The returned value is bound to the function name instead of the function object. Multiple decorators are applied in nested fashion. For example, the following code


    @f1(arg)
    @f2
    def func(): pass


is roughly equivalent to


    def func(): pass
    func = f1(arg)(f2(func))


except that the original function is not temporarily bound to the name func.


Changed in version 3.9: Functions may be decorated with any valid assignment_expression. Previously, the grammar was much more restrictive; see PEP 614 for details.

When one or more parameters have the form parameter = expression, the function is said to have “default parameter values.” For a parameter with a default value, the corresponding argument may be omitted from a call, in which case the parameter’s default value is substituted. If a parameter has a default value, all following parameters up until the * must also have a default value — this is a syntactic restriction that is not expressed by the grammar.

Default parameter values are evaluated from left to right when the function definition is executed. This means that the expression is evaluated once, when the function is defined, and that the same “pre-computed” value is used for each call. This is especially important to understand when a default parameter is a mutable object, such as a list or a dictionary: if the function modifies the object (e.g. by appending an item to a list), the default value is in effect modified. This is generally not what was intended. A way around this is to use None as the default, and explicitly test for it in the body of the function, e.g.:


```py
def whats_on_the_telly(penguin=None):
    if penguin is None:
        penguin = []
    penguin.append("property of the zoo")
    return penguin
```


Function call semantics are described in more detail in section Calls. A function call always assigns values to all parameters mentioned in the parameter list, either from positional arguments, from keyword arguments, or from default values. If the form “`*identifier`” is present, it is initialized to a tuple receiving any excess positional parameters, defaulting to the empty tuple. If the form “`**identifier`” is present, it is initialized to a new ordered mapping receiving any excess keyword arguments, defaulting to a new empty mapping of the same type. Parameters after * or “`*identifier`” are keyword-only parameters and may only be passed by keyword arguments. Parameters before “/” are positional-only parameters and may only be passed by positional arguments.


Changed in version 3.8: The / function parameter syntax may be used to indicate positional-only parameters. See PEP 570 for details.

Parameters may have an annotation of the form “`: expression`” following the parameter name. Any parameter may have an annotation, even those of the form `*identifier` or `**identifier`. Functions may have “return” annotation of the form “`-> expression`” after the parameter list. These annotations can be any valid Python expression. The presence of annotations does not change the semantics of a function. The annotation values are available as values of a dictionary keyed by the parameters’ names in the __annotations__ attribute of the function object. If the annotations import from __future__ is used, annotations are preserved as strings at runtime which enables postponed evaluation. Otherwise, they are evaluated when the function definition is executed. In this case annotations may be evaluated in a different order than they appear in the source code.

It is also possible to create anonymous functions (functions not bound to a name), for immediate use in expressions. This uses lambda expressions, described in section Lambdas. Note that the lambda expression is merely a shorthand for a simplified function definition; a function defined in a “`def`” statement can be passed around or assigned to another name just like a function defined by a lambda expression. The “`def`” form is actually more powerful since it allows the execution of multiple statements and annotations.

Programmer’s note: Functions are first-class objects. A “`def`” statement executed inside a function definition defines a local function that can be returned or passed around. Free variables used in the nested function can access the local variables of the function containing the def. See section Naming and binding for details.

See also:
↪ PEP 3107 - Function
 Annotations The original specification for function annotations.
↪ PEP 484 - Type Hints
 Definition of a standard meaning for annotations: type hints.
↪ PEP 526 - Syntax for Variable Annotations
 Ability to type hint variable declarations, including class variables and instance variables
↪ PEP 563 - Postponed Evaluation of Annotations
 Support for forward references within annotations by preserving annotations in a string form at runtime instead of eager evaluation.

### ===🗝 8.7. Class definitions

A class definition defines a class object (see section The standard type hierarchy):

    classdef    ::=  [decorators] "class" classname [inheritance] ":" suite
    inheritance ::=  "(" [argument_list] ")"
    classname   ::=  identifier

A class definition is an executable statement. The inheritance list usually gives a list of base classes (see Metaclasses for more advanced uses), so each item in the list should evaluate to a class object which allows subclassing. Classes without an inheritance list inherit, by default, from the base class object; hence,


    class Foo:
        pass


is equivalent to


    class Foo(object):
        pass


The class’s suite is then executed in a new execution frame (see Naming and binding), using a newly created local namespace and the original global namespace. (Usually, the suite contains mostly function definitions.) When the class’s suite finishes execution, its execution frame is discarded but its local namespace is saved. [3] A class object is then created using the inheritance list for the base classes and the saved local namespace for the attribute dictionary. The class name is bound to this class object in the original local namespace.

The order in which attributes are defined in the class body is preserved in the new class’s __dict__. Note that this is reliable only right after the class is created and only for classes that were defined using the definition syntax.

Class creation can be customized heavily using metaclasses.

Classes can also be decorated: just like when decorating functions,


    @f1(arg)
    @f2
    class Foo: pass


is roughly equivalent to


    class Foo: pass
    Foo = f1(arg)(f2(Foo))


The evaluation rules for the decorator expressions are the same as for function decorators. The result is then bound to the class name.


Changed in version 3.9: Classes may be decorated with any valid assignment_expression. Previously, the grammar was much more restrictive; see PEP 614 for details.

Programmer’s note: Variables defined in the class definition are class attributes; they are shared by instances. Instance attributes can be set in a method with self.name = value. Both class and instance attributes are accessible through the notation “self.name”, and an instance attribute hides a class attribute with the same name when accessed in this way. Class attributes can be used as defaults for instance attributes, but using mutable values there can lead to unexpected results. Descriptors can be used to create instance variables with different implementation details.

See also:
 PEP 3115 - Metaclasses in Python 3000The proposal that changed the declaration of metaclasses to the current syntax, and the semantics for how classes with metaclasses are constructed.PEP 3129 - Class DecoratorsThe proposal that added class decorators. Function and method decorators were introduced in PEP 318.

### ===🗝 8.8. Coroutines

New in version 3.5.


#### 8.8.1. Coroutine function definition

    async_funcdef ::=  [decorators] "async" "def" funcname "(" [parameter_list] ")"
                       ["->" expression] ":" suite

Execution of Python coroutines can be suspended and resumed at many points (see coroutine). Inside the body of a coroutine function, await and async identifiers become reserved keywords; await expressions, async for and async with can only be used in coroutine function bodies.

Functions defined with async def syntax are always coroutine functions, even if they do not contain await or async keywords.

It is a SyntaxError to use a yield from expression inside the body of a coroutine function.

An example of a coroutine function:


    async def func(param1, param2):
        do_stuff()
        await some_coroutine()



#### 8.8.2. The async for statement

    async_for_stmt ::=  "async" for_stmt

An asynchronous iterable provides an __aiter__ method that directly returns an asynchronous iterator, which can call asynchronous code in its __anext__ method.

The async for statement allows convenient iteration over asynchronous iterables.

The following code:


    async for TARGET in ITER:
        SUITE
    else:
        SUITE2


Is semantically equivalent to:


    iter = (ITER)
    iter = type(iter).__aiter__(iter)
    running = True

    while running:
        try:
            TARGET = await type(iter).__anext__(iter)
        except StopAsyncIteration:
            running = False
        else:
            SUITE
    else:
        SUITE2


See also __aiter__() and __anext__() for details.

It is a SyntaxError to use an async for statement outside the body of a coroutine function.


#### 8.8.3. The async with statement

    async_with_stmt ::=  "async" with_stmt

An asynchronous context manager is a context manager that is able to suspend execution in its enter and exit methods.

The following code:


    async with EXPRESSION as TARGET:
        SUITE


is semantically equivalent to:


    manager = (EXPRESSION)
    aenter = type(manager).__aenter__
    aexit = type(manager).__aexit__
    value = await aenter(manager)
    hit_except = False

    try:
        TARGET = value
        SUITE
    except:
        hit_except = True
        if not await aexit(manager, *sys.exc_info()):
            raise
    finally:
        if not hit_except:
            await aexit(manager, None, None, None)


See also __aenter__() and __aexit__() for details.

It is a SyntaxError to use an async with statement outside the body of a coroutine function.

See also:
 PEP 492 - Coroutines with async and await syntaxThe proposal that made coroutines a proper standalone concept in Python, and added supporting syntax.

Footnotes

[1] The exception is propagated to the invocation stack unless there is a finally clause which happens to raise another exception. That new exception causes the old one to be lost. 

[2] A string literal appearing as the first statement in the function body is transformed into the function’s __doc__ attribute and therefore the function’s docstring. 

[3] A string literal appearing as the first statement in the class body is transformed into the namespace’s __doc__ item and therefore the class’s docstring. 



## ==⚡ 9. Top-level components
- The Python Language Reference » 9. Top-level components

The Python interpreter can get its input from a number of sources: from a script passed to it as standard input or as program argument, typed in interactively, from a module source file, etc. This chapter gives the syntax used in these cases.


### ===🗝 9.1. Complete Python programs

While a language specification need not prescribe how the language interpreter is invoked, it is useful to have a notion of a complete Python program. A complete Python program is executed in a minimally initialized environment: all built-in and standard modules are available, but none have been initialized, except for sys (various system services), builtins (built-in functions, exceptions and None) and __main__. The latter is used to provide the local and global namespace for execution of the complete program.

The syntax for a complete Python program is that for file input, described in the next section.

The interpreter may also be invoked in interactive mode; in this case, it does not read and execute a complete program but reads and executes one statement (possibly compound) at a time. The initial environment is identical to that of a complete program; each statement is executed in the namespace of __main__.

A complete program can be passed to the interpreter in three forms: with the -c string command line option, as a file passed as the first command line argument, or as standard input. If the file or standard input is a tty device, the interpreter enters interactive mode; otherwise, it executes the file as a complete program.


### ===🗝 9.2. File input

All input read from non-interactive files has the same form:

    file_input ::=  (NEWLINE | statement)*

This syntax is used in the following situations:
•when parsing a complete Python program (from a file or from a string);
•when parsing a module;
•when parsing a string passed to the exec() function;


### ===🗝 9.3. Interactive input

Input in interactive mode is parsed using the following grammar:

    interactive_input ::=  [stmt_list] NEWLINE | compound_stmt NEWLINE

Note that a (top-level) compound statement must be followed by a blank line in interactive mode; this is needed to help the parser detect the end of the input.


### ===🗝 9.4. Expression input

eval() is used for expression input. It ignores leading whitespace. The string argument to eval() must have the following form:

    eval_input ::=  expression_list NEWLINE*


## ==⚡ 10. Full Grammar specification
- The Python Language Reference » 10. Full Grammar specification

This is the full Python grammar, derived directly from the grammar used to generate the CPython parser (see Grammar/python.gram). The version here omits details related to code generation and error recovery.

The notation is a mixture of EBNF and PEG. In particular, & followed by a symbol, token or parenthesized group indicates a positive lookahead (i.e., is required to match but not consumed), while ! indicates a negative lookahead (i.e., is required _not_ to match). We use the | separator to mean PEG’s “ordered choice” (written as / in traditional PEG grammars).


    # PEG grammar for Python

    file: [statements] ENDMARKER 
    interactive: statement_newline 
    eval: expressions NEWLINE* ENDMARKER 
    func_type: '(' [type_expressions] ')' '->' expression NEWLINE* ENDMARKER 
    fstring: star_expressions

    # type_expressions allow */** but ignore them
    type_expressions:
        | ','.expression+ ',' '*' expression ',' '**' expression 
        | ','.expression+ ',' '*' expression 
        | ','.expression+ ',' '**' expression 
        | '*' expression ',' '**' expression 
        | '*' expression 
        | '**' expression 
        | ','.expression+

    statements: statement+ 
    statement: compound_stmt  | simple_stmt
    statement_newline:
        | compound_stmt NEWLINE 
        | simple_stmt
        | NEWLINE 
        | ENDMARKER 
    simple_stmt:
        | small_stmt !';' NEWLINE  # Not needed, there for speedup
        | ';'.small_stmt+ [';'] NEWLINE 
    # NOTE: assignment MUST precede expression, else parsing a simple assignment
    # will throw a SyntaxError.
    small_stmt:
        | assignment
        | star_expressions 
        | return_stmt
        | import_stmt
        | raise_stmt
        | 'pass' 
        | del_stmt
        | yield_stmt
        | assert_stmt
        | 'break' 
        | 'continue' 
        | global_stmt
        | nonlocal_stmt
    compound_stmt:
        | function_def
        | if_stmt
        | class_def
        | with_stmt
        | for_stmt
        | try_stmt
        | while_stmt

    # NOTE: annotated_rhs may start with 'yield'; yield_expr must start with 'yield'
    assignment:
        | NAME ':' expression ['=' annotated_rhs ] 
        | ('(' single_target ')' 
             | single_subscript_attribute_target) ':' expression ['=' annotated_rhs ] 
        | (star_targets '=' )+ (yield_expr | star_expressions) !'=' [TYPE_COMMENT] 
        | single_target augassign ~ (yield_expr | star_expressions) 
    augassign:
        | '+=' 
        | '-=' 
        | '*=' 
        | '@=' 
        | '/=' 
        | '%=' 
        | '&=' 
        | '|=' 
        | '^=' 
        | '<<=' 
        | '>>=' 
        | '**=' 
        | '//=' 

    global_stmt: 'global' ','.NAME+ 
    nonlocal_stmt: 'nonlocal' ','.NAME+ 

    yield_stmt: yield_expr 

    assert_stmt: 'assert' expression [',' expression ] 

    del_stmt:
        | 'del' del_targets &(';' | NEWLINE) 
    import_stmt: import_name | import_from
    import_name: 'import' dotted_as_names 
    # note below: the ('.' | '...') is necessary because '...' is tokenized as ELLIPSIS
    import_from:
        | 'from' ('.' | '...')* dotted_name 'import' import_from_targets 
        | 'from' ('.' | '...')+ 'import' import_from_targets 
    import_from_targets:
        | '(' import_from_as_names [','] ')' 
        | import_from_as_names !','
        | '*' 
    import_from_as_names:
        | ','.import_from_as_name+ 
    import_from_as_name:
        | NAME ['as' NAME ] 
    dotted_as_names:
        | ','.dotted_as_name+ 
    dotted_as_name:
        | dotted_name ['as' NAME ] 
    dotted_name:
        | dotted_name '.' NAME 
        | NAME

    if_stmt:
        | 'if' named_expression ':' block elif_stmt 
        | 'if' named_expression ':' block [else_block] 
    elif_stmt:
        | 'elif' named_expression ':' block elif_stmt 
        | 'elif' named_expression ':' block [else_block] 
    else_block: 'else' ':' block 

    while_stmt:
        | 'while' named_expression ':' block [else_block] 

    for_stmt:
        | 'for' star_targets 'in' ~ star_expressions ':' [TYPE_COMMENT] block [else_block] 
        | ASYNC 'for' star_targets 'in' ~ star_expressions ':' [TYPE_COMMENT] block [else_block] 
    with_stmt:
        | 'with' '(' ','.with_item+ ','? ')' ':' block 
        | 'with' ','.with_item+ ':' [TYPE_COMMENT] block 
        | ASYNC 'with' '(' ','.with_item+ ','? ')' ':' block 
        | ASYNC 'with' ','.with_item+ ':' [TYPE_COMMENT] block 
    with_item:
        | expression 'as' star_target &(',' | ')' | ':') 
        | expression 

    try_stmt:
        | 'try' ':' block finally_block 
        | 'try' ':' block except_block+ [else_block] [finally_block] 
    except_block:
        | 'except' expression ['as' NAME ] ':' block 
        | 'except' ':' block 
    finally_block: 'finally' ':' block 

    return_stmt:
        | 'return' [star_expressions] 

    raise_stmt:
        | 'raise' expression ['from' expression ] 
        | 'raise' 

    function_def:
        | decorators function_def_raw 
        | function_def_raw

    function_def_raw:
        | 'def' NAME '(' [params] ')' ['->' expression ] ':' [func_type_comment] block 
        | ASYNC 'def' NAME '(' [params] ')' ['->' expression ] ':' [func_type_comment] block 
    func_type_comment:
        | NEWLINE TYPE_COMMENT &(NEWLINE INDENT)   # Must be followed by indented block
        | TYPE_COMMENT

    params:
        | parameters

    parameters:
        | slash_no_default param_no_default* param_with_default* [star_etc] 
        | slash_with_default param_with_default* [star_etc] 
        | param_no_default+ param_with_default* [star_etc] 
        | param_with_default+ [star_etc] 
        | star_etc 

    # Some duplication here because we can't write (',' | &')'),
    # which is because we don't support empty alternatives (yet).
    #
    slash_no_default:
        | param_no_default+ '/' ',' 
        | param_no_default+ '/' &')' 
    slash_with_default:
        | param_no_default* param_with_default+ '/' ',' 
        | param_no_default* param_with_default+ '/' &')' 

    star_etc:
        | '*' param_no_default param_maybe_default* [kwds] 
        | '*' ',' param_maybe_default+ [kwds] 
        | kwds 
    kwds: '**' param_no_default 

    # One parameter.  This *includes* a following comma and type comment.
    #
    # There are three styles:
    # - No default
    # - With default
    # - Maybe with default
    #
    # There are two alternative forms of each, to deal with type comments:
    # - Ends in a comma followed by an optional type comment
    # - No comma, optional type comment, must be followed by close paren
    # The latter form is for a final parameter without trailing comma.
    #
    param_no_default:
        | param ',' TYPE_COMMENT? 
        | param TYPE_COMMENT? &')' 
    param_with_default:
        | param default ',' TYPE_COMMENT? 
        | param default TYPE_COMMENT? &')' 
    param_maybe_default:
        | param default? ',' TYPE_COMMENT? 
        | param default? TYPE_COMMENT? &')' 
    param: NAME annotation? 

    annotation: ':' expression 
    default: '=' expression 

    decorators: ('@' named_expression NEWLINE )+ 

    class_def:
        | decorators class_def_raw 
        | class_def_raw
    class_def_raw:
        | 'class' NAME ['(' [arguments] ')' ] ':' block 

    block:
        | NEWLINE INDENT statements DEDENT 
        | simple_stmt
    star_expressions:
        | star_expression (',' star_expression )+ [','] 
        | star_expression ',' 
        | star_expression
    star_expression:
        | '*' bitwise_or 
        | expression

    star_named_expressions: ','.star_named_expression+ [','] 
    star_named_expression:
        | '*' bitwise_or 
        | named_expression
    named_expression:
        | NAME ':=' ~ expression 
        | expression !':='
    annotated_rhs: yield_expr | star_expressions

    expressions:
        | expression (',' expression )+ [','] 
        | expression ',' 
        | expression
    expression:
        | disjunction 'if' disjunction 'else' expression 
        | disjunction
        | lambdef

    lambdef:
        | 'lambda' [lambda_params] ':' expression 

    lambda_params:
        | lambda_parameters

    # lambda_parameters etc. duplicates parameters but without annotations
    # or type comments, and if there's no comma after a parameter, we expect
    # a colon, not a close parenthesis.  (For more, see parameters above.)
    #
    lambda_parameters:
        | lambda_slash_no_default lambda_param_no_default* lambda_param_with_default* [lambda_star_etc] 
        | lambda_slash_with_default lambda_param_with_default* [lambda_star_etc] 
        | lambda_param_no_default+ lambda_param_with_default* [lambda_star_etc] 
        | lambda_param_with_default+ [lambda_star_etc] 
        | lambda_star_etc 

    lambda_slash_no_default:
        | lambda_param_no_default+ '/' ',' 
        | lambda_param_no_default+ '/' &':' 
    lambda_slash_with_default:
        | lambda_param_no_default* lambda_param_with_default+ '/' ',' 
        | lambda_param_no_default* lambda_param_with_default+ '/' &':' 

    lambda_star_etc:
        | '*' lambda_param_no_default lambda_param_maybe_default* [lambda_kwds] 
        | '*' ',' lambda_param_maybe_default+ [lambda_kwds] 
        | lambda_kwds 
    lambda_kwds: '**' lambda_param_no_default 

    lambda_param_no_default:
        | lambda_param ',' 
        | lambda_param &':' 
    lambda_param_with_default:
        | lambda_param default ',' 
        | lambda_param default &':' 
    lambda_param_maybe_default:
        | lambda_param default? ',' 
        | lambda_param default? &':' 
    lambda_param: NAME 

    disjunction:
        | conjunction ('or' conjunction )+ 
        | conjunction
    conjunction:
        | inversion ('and' inversion )+ 
        | inversion
    inversion:
        | 'not' inversion 
        | comparison
    comparison:
        | bitwise_or compare_op_bitwise_or_pair+ 
        | bitwise_or
    compare_op_bitwise_or_pair:
        | eq_bitwise_or
        | noteq_bitwise_or
        | lte_bitwise_or
        | lt_bitwise_or
        | gte_bitwise_or
        | gt_bitwise_or
        | notin_bitwise_or
        | in_bitwise_or
        | isnot_bitwise_or
        | is_bitwise_or
    eq_bitwise_or: '==' bitwise_or 
    noteq_bitwise_or:
        | ('!=' ) bitwise_or 
    lte_bitwise_or: '<=' bitwise_or 
    lt_bitwise_or: '<' bitwise_or 
    gte_bitwise_or: '>=' bitwise_or 
    gt_bitwise_or: '>' bitwise_or 
    notin_bitwise_or: 'not' 'in' bitwise_or 
    in_bitwise_or: 'in' bitwise_or 
    isnot_bitwise_or: 'is' 'not' bitwise_or 
    is_bitwise_or: 'is' bitwise_or 

    bitwise_or:
        | bitwise_or '|' bitwise_xor 
        | bitwise_xor
    bitwise_xor:
        | bitwise_xor '^' bitwise_and 
        | bitwise_and
    bitwise_and:
        | bitwise_and '&' shift_expr 
        | shift_expr
    shift_expr:
        | shift_expr '<<' sum 
        | shift_expr '>>' sum 
        | sum

    sum:
        | sum '+' term 
        | sum '-' term 
        | term
    term:
        | term '*' factor 
        | term '/' factor 
        | term '//' factor 
        | term '%' factor 
        | term '@' factor 
        | factor
    factor:
        | '+' factor 
        | '-' factor 
        | '~' factor 
        | power
    power:
        | await_primary '**' factor 
        | await_primary
    await_primary:
        | AWAIT primary 
        | primary
    primary:
        | invalid_primary  # must be before 'primay genexp' because of invalid_genexp
        | primary '.' NAME 
        | primary genexp 
        | primary '(' [arguments] ')' 
        | primary '[' slices ']' 
        | atom

    slices:
        | slice !',' 
        | ','.slice+ [','] 
    slice:
        | [expression] ':' [expression] [':' [expression] ] 
        | expression 
    atom:
        | NAME
        | 'True' 
        | 'False' 
        | 'None' 
        | '__peg_parser__' 
        | strings
        | NUMBER
        | (tuple | group | genexp)
        | (list | listcomp)
        | (dict | set | dictcomp | setcomp)
        | '...' 

    strings: STRING+ 
    list:
        | '[' [star_named_expressions] ']' 
    listcomp:
        | '[' named_expression ~ for_if_clauses ']' 
    tuple:
        | '(' [star_named_expression ',' [star_named_expressions]  ] ')' 
    group:
        | '(' (yield_expr | named_expression) ')' 
    genexp:
        | '(' named_expression ~ for_if_clauses ')' 
    set: '{' star_named_expressions '}' 
    setcomp:
        | '{' named_expression ~ for_if_clauses '}' 
    dict:
        | '{' [double_starred_kvpairs] '}' 
    dictcomp:
        | '{' kvpair for_if_clauses '}' 
    double_starred_kvpairs: ','.double_starred_kvpair+ [','] 
    double_starred_kvpair:
        | '**' bitwise_or 
        | kvpair
    kvpair: expression ':' expression 
    for_if_clauses:
        | for_if_clause+
    for_if_clause:
        | ASYNC 'for' star_targets 'in' ~ disjunction ('if' disjunction )* 
        | 'for' star_targets 'in' ~ disjunction ('if' disjunction )* 
    yield_expr:
        | 'yield' 'from' expression 
        | 'yield' [star_expressions] 

    arguments:
        | args [','] &')' 
    args:
        | ','.(starred_expression | named_expression !'=')+ [',' kwargs ] 
        | kwargs 
    kwargs:
        | ','.kwarg_or_starred+ ',' ','.kwarg_or_double_starred+ 
        | ','.kwarg_or_starred+
        | ','.kwarg_or_double_starred+
    starred_expression:
        | '*' expression 
    kwarg_or_starred:
        | NAME '=' expression 
        | starred_expression 
    kwarg_or_double_starred:
        | NAME '=' expression 
        | '**' expression 
    # NOTE: star_targets may contain *bitwise_or, targets may not.
    star_targets:
        | star_target !',' 
        | star_target (',' star_target )* [','] 
    star_targets_list_seq: ','.star_target+ [','] 
    star_targets_tuple_seq:
        | star_target (',' star_target )+ [','] 
        | star_target ',' 
    star_target:
        | '*' (!'*' star_target) 
        | target_with_star_atom
    target_with_star_atom:
        | t_primary '.' NAME !t_lookahead 
        | t_primary '[' slices ']' !t_lookahead 
        | star_atom
    star_atom:
        | NAME 
        | '(' target_with_star_atom ')' 
        | '(' [star_targets_tuple_seq] ')' 
        | '[' [star_targets_list_seq] ']' 

    single_target:
        | single_subscript_attribute_target
        | NAME 
        | '(' single_target ')' 
    single_subscript_attribute_target:
        | t_primary '.' NAME !t_lookahead 
        | t_primary '[' slices ']' !t_lookahead 

    del_targets: ','.del_target+ [','] 
    del_target:
        | t_primary '.' NAME !t_lookahead 
        | t_primary '[' slices ']' !t_lookahead 
        | del_t_atom
    del_t_atom:
        | NAME 
        | '(' del_target ')' 
        | '(' [del_targets] ')' 
        | '[' [del_targets] ']' 

    targets: ','.target+ [','] 
    target:
        | t_primary '.' NAME !t_lookahead 
        | t_primary '[' slices ']' !t_lookahead 
        | t_atom
    t_primary:
        | t_primary '.' NAME &t_lookahead 
        | t_primary '[' slices ']' &t_lookahead 
        | t_primary genexp &t_lookahead 
        | t_primary '(' [arguments] ')' &t_lookahead 
        | atom &t_lookahead 
    t_lookahead: '(' | '[' | '.'
    t_atom:
        | NAME 
        | '(' target ')' 
        | '(' [targets] ')' 
        | '[' [targets] ']' 


# =🚩 What’s New in Python
- Python 3.10.2 Documentation » What’s New in Python
- Python Changelog https://docs.python.org/3/whatsnew/changelog.html

The “What’s New in Python” series of essays takes tours through the most important changes between major Python versions. They are a “must read” for anyone wishing to stay up-to-date after a new release.


The “Changelog” is an HTML version of the file built from the contents of the Misc/NEWS.d directory tree, which contains all nontrivial changes to Python for the current version.


## ==⚡ • What’s New In Python 3.10
◦ Summary – Release highlights
◦ New Features
◦ New Features Related to Type Hints
◦ Other Language Changes
◦ New Modules
◦ Improved Modules
◦ Optimizations
◦ Deprecated
◦ Removed
◦ Porting to Python 3.10
◦ CPython bytecode changes
◦ Build Changes
◦ C API Changes

This article explains the new features in Python 3.10, compared to 3.9.

For full details, see the changelog.


### ===🗝 Summary – Release highlights

New syntax features:

• PEP 634, Structural Pattern Matching: Specification https://www.python.org/dev/peps/pep-0634
• PEP 635, Structural Pattern Matching: Motivation and Rationale https://www.python.org/dev/peps/pep-0635
• PEP 636, Structural Pattern Matching: Tutorial https://www.python.org/dev/peps/pep-0636
• bpo-12782, Parenthesized context managers are now officially allowed. https://bugs.python.org/issue12782

New features in the standard library:

• PEP 618, Add Optional Length-Checking To zip. https://www.python.org/dev/peps/pep-0618

Interpreter improvements:

• PEP 626, Precise line numbers for debugging and other tools. https://www.python.org/dev/peps/pep-0626

New typing features:
• PEP 604, Allow writing union types as X | Y https://www.python.org/dev/peps/pep-0604
• PEP 613, Explicit Type Aliases https://www.python.org/dev/peps/pep-0613
• PEP 612, Parameter Specification Variables https://www.python.org/dev/peps/pep-0612

Important deprecations, removals or restrictions:
• PEP 644, Require OpenSSL 1.1.1 or newer https://www.python.org/dev/peps/pep-0644
• PEP 632, Deprecate distutils module https://www.python.org/dev/peps/pep-0632
• PEP 623, Deprecate and prepare for the removal of the wstr member in PyUnicodeObject https://www.python.org/dev/peps/pep-0623
• PEP 624, Remove Py_UNICODE encoder APIs https://www.python.org/dev/peps/pep-0624
• PEP 597, Add optional EncodingWarning https://www.python.org/dev/peps/pep-0597


### ===🗝 New Features


#### Parenthesized context managers

Using enclosing parentheses for continuation across multiple lines in context managers is now supported. This allows formatting a long collection of context managers in multiple lines in a similar way as it was previously possible with import statements. For instance, all these examples are now valid:


```py
with (CtxManager() as example):
    ...

with (
    CtxManager1(),
    CtxManager2()
):
    ...

with (CtxManager1() as example,
      CtxManager2()):
    ...

with (CtxManager1(),
      CtxManager2() as example):
    ...

with (
    CtxManager1() as example1,
    CtxManager2() as example2
):
    ...
```


it is also possible to use a trailing comma at the end of the enclosed group:


```py
with (
    CtxManager1() as example1,
    CtxManager2() as example2,
    CtxManager3() as example3,
):
    ...
```


This new syntax uses the non LL(1) capacities of the new parser. Check PEP 617 for more details.

(Contributed by Guido van Rossum, Pablo Galindo and Lysandros Nikolaou in bpo-12782 and bpo-40334.)


#### Better error messages


SyntaxErrors

When parsing code that contains unclosed parentheses or brackets the interpreter now includes the location of the unclosed bracket of parentheses instead of displaying SyntaxError: unexpected EOF while parsing or pointing to some incorrect location. For instance, consider the following code (notice the unclosed ‘{‘):


>expected = {9: 1, 18: 2, 19: 2, 27: 3, 28: 3, 29: 3, 36: 4, 37: 4,
            38: 4, 39: 4, 45: 5, 46: 5, 47: 5, 48: 5, 49: 5, 54: 6,
some_other_code = foo()


Previous versions of the interpreter reported confusing places as the location of the syntax error:


>File "example.py", line 3
    some_other_code = foo()
                    ^
SyntaxError: invalid syntax


but in Python 3.10 a more informative error is emitted:


>File "example.py", line 1
    expected = {9: 1, 18: 2, 19: 2, 27: 3, 28: 3, 29: 3, 36: 4, 37: 4,
               ^
SyntaxError: '{' was never closed


In a similar way, errors involving unclosed string literals (single and triple quoted) now point to the start of the string instead of reporting EOF/EOL.

These improvements are inspired by previous work in the PyPy interpreter.

(Contributed by Pablo Galindo in bpo-42864 and Batuhan Taskaya in bpo-40176.)

SyntaxError exceptions raised by the interpreter will now highlight the full error range of the expression that constitutes the syntax error itself, instead of just where the problem is detected. In this way, instead of displaying (before Python 3.10):


>>> foo(x, z for z in range(10), t, w)
  File "<stdin>", line 1
    foo(x, z for z in range(10), t, w)
           ^
SyntaxError: Generator expression must be parenthesized


now Python 3.10 will display the exception as:


>>> foo(x, z for z in range(10), t, w)
  File "<stdin>", line 1
    foo(x, z for z in range(10), t, w)
           ^^^^^^^^^^^^^^^^^^^^
SyntaxError: Generator expression must be parenthesized


This improvement was contributed by Pablo Galindo in bpo-43914.

A considerable amount of new specialized messages for SyntaxError exceptions have been incorporated. Some of the most notable ones are as follows:

• Missing : before blocks:


>>> if rocket.position > event_horizon
  File "<stdin>", line 1
    if rocket.position > event_horizon
                                      ^
SyntaxError: expected ':'


(Contributed by Pablo Galindo in bpo-42997)

• Unparenthesised tuples in comprehensions targets:


>>> {x,y for x,y in zip('abcd', '1234')}
  File "<stdin>", line 1
    {x,y for x,y in zip('abcd', '1234')}
     ^
SyntaxError: did you forget parentheses around the comprehension target?


(Contributed by Pablo Galindo in bpo-43017)


• Missing commas in collection literals and between expressions:


>>> items = {
... x: 1,
... y: 2
... z: 3,
  File "<stdin>", line 3
    y: 2
       ^
SyntaxError: invalid syntax. Perhaps you forgot a comma?


(Contributed by Pablo Galindo in bpo-43822)


• Multiple Exception types without parentheses:



>>> try:
...     build_dyson_sphere()
... except NotEnoughScienceError, NotEnoughResourcesError:
  File "<stdin>", line 3
    except NotEnoughScienceError, NotEnoughResourcesError:
           ^
SyntaxError: multiple exception types must be parenthesized


(Contributed by Pablo Galindo in bpo-43149)


• Missing : and values in dictionary literals:



>>> values = {
... x: 1,
... y: 2,
... z:
... }
  File "<stdin>", line 4
    z:
     ^
SyntaxError: expression expected after dictionary key and ':'

>>> values = {x:1, y:2, z w:3}
  File "<stdin>", line 1
    values = {x:1, y:2, z w:3}
                        ^
SyntaxError: ':' expected after dictionary key


(Contributed by Pablo Galindo in bpo-43823)


• try blocks without except or finally blocks:



>>> try:
...     x = 2
... something = 3
  File "<stdin>", line 3
    something  = 3
    ^^^^^^^^^
SyntaxError: expected 'except' or 'finally' block


(Contributed by Pablo Galindo in bpo-44305)


• Usage of = instead of == in comparisons:



>>> if rocket.position = event_horizon:
  File "<stdin>", line 1
    if rocket.position = event_horizon:
                       ^
SyntaxError: cannot assign to attribute here. Maybe you meant '==' instead of '='?


(Contributed by Pablo Galindo in bpo-43797)


• Usage of * in f-strings:




>>> f"Black holes {*all_black_holes} and revelations"
  File "<stdin>", line 1
    (*all_black_holes)
     ^
SyntaxError: f-string: cannot use starred expression here


(Contributed by Pablo Galindo in bpo-41064)



IndentationErrors

Many IndentationError exceptions now have more context regarding what kind of block was expecting an indentation, including the location of the statement:


>>> def foo():
...    if lel:
...    x = 2
  File "<stdin>", line 3
    x = 2
    ^
IndentationError: expected an indented block after 'if' statement in line 2



AttributeErrors

When printing AttributeError, PyErr_Display() will offer suggestions of similar attribute names in the object that the exception was raised from:


>>> collections.namedtoplo
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: module 'collections' has no attribute 'namedtoplo'. Did you mean: namedtuple?


(Contributed by Pablo Galindo in bpo-38530.)



Warning:
 Notice this won’t work if PyErr_Display() is not called to display the error which can happen if some other custom error display function is used. This is a common scenario in some REPLs like IPython.
 


NameErrors

When printing NameError raised by the interpreter, PyErr_Display() will offer suggestions of similar variable names in the function that the exception was raised from:


>>> schwarzschild_black_hole = None
>>> schwarschild_black_hole
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'schwarschild_black_hole' is not defined. Did you mean: schwarzschild_black_hole?


(Contributed by Pablo Galindo in bpo-38530.)



Warning:
 Notice this won’t work if PyErr_Display() is not called to display the error, which can happen if some other custom error display function is used. This is a common scenario in some REPLs like IPython.
 


#### PEP 626: Precise line numbers for debugging and other tools

PEP 626 brings more precise and reliable line numbers for debugging, profiling and coverage tools. Tracing events, with the correct line number, are generated for all lines of code executed and only for lines of code that are executed.

The f_lineno attribute of frame objects will always contain the expected line number.

The co_lnotab attribute of code objects is deprecated and will be removed in 3.12. Code that needs to convert from offset to line number should use the new co_lines() method instead.


#### PEP 634: Structural Pattern Matching

Structural pattern matching has been added in the form of a match statement and case statements of patterns with associated actions. Patterns consist of sequences, mappings, primitive data types as well as class instances. Pattern matching enables programs to extract information from complex data types, branch on the structure of data, and apply specific actions based on different forms of data.


Syntax and operations

The generic syntax of pattern matching is:


```py
match subject:
    case <pattern_1>:
        <action_1>
    case <pattern_2>:
        <action_2>
    case <pattern_3>:
        <action_3>
    case _:
        <action_wildcard>
```


A match statement takes an expression and compares its value to successive patterns given as one or more case blocks. Specifically, pattern matching operates by:


1. using data with type and shape (the subject)
2. evaluating the subject in the match statement
3. comparing the subject with each pattern in a case statement from top to bottom until a  match is confirmed.
4. executing the action associated with the pattern of the confirmed match
5. If an exact match is not confirmed, the last case, a wildcard `_`, if provided, will be used as the matching case. If an exact match is not confirmed and a wildcard case does not exist, the entire match block is a no-op.


Declarative approach

Readers may be aware of pattern matching through the simple example of matching a subject (data object) to a literal (pattern) with the switch statement found in C, Java or JavaScript (and many other languages). Often the switch statement is used for comparison of an object/expression with case statements containing literals.

More powerful examples of pattern matching can be found in languages such as Scala and Elixir. With structural pattern matching, the approach is “declarative” and explicitly states the conditions (the patterns) for data to match.

While an “imperative” series of instructions using nested “if” statements could be used to accomplish something similar to structural pattern matching, it is less clear than the “declarative” approach. Instead the “declarative” approach states the conditions to meet for a match and is more readable through its explicit patterns. While structural pattern matching can be used in its simplest form comparing a variable to a literal in a case statement, its true value for Python lies in its handling of the subject’s type and shape.


Simple pattern: match to a literal

Let’s look at this example as pattern matching in its simplest form: a value, the subject, being matched to several literals, the patterns. In the example below, status is the subject of the match statement. The patterns are each of the case statements, where literals represent request status codes. The associated action to the case is executed after a match:


def http_error(status):
    match status:
        case 400:
            return "Bad request"
        case 404:
            return "Not found"
        case 418:
            return "I'm a teapot"
        case _:
            return "Something's wrong with the internet"


If the above function is passed a status of 418, “I’m a teapot” is returned. If the above function is passed a status of 500, the case statement with `_` will match as a wildcard, and “Something’s wrong with the internet” is returned. Note the last block: the variable name, `_`, acts as a wildcard and insures the subject will always match. The use of `_` is optional.

You can combine several literals in a single pattern using | (“or”):


case 401 | 403 | 404:
    return "Not allowed"



Behavior without the wildcard

If we modify the above example by removing the last case block, the example becomes:


```py
def http_error(status):
    match status:
        case 400:
            return "Bad request"
        case 404:
            return "Not found"
        case 418:
            return "I'm a teapot"
```


Without the use of `_` in a case statement, a match may not exist. If no match exists, the behavior is a no-op. For example, if status of 500 is passed, a no-op occurs.


Patterns with a literal and variable

Patterns can look like unpacking assignments, and a pattern may be used to bind variables. In this example, a data point can be unpacked to its x-coordinate and y-coordinate:


```py
# point is an (x, y) tuple
match point:
    case (0, 0):
        print("Origin")
    case (0, y):
        print(f"Y={y}")
    case (x, 0):
        print(f"X={x}")
    case (x, y):
        print(f"X={x}, Y={y}")
    case _:
        raise ValueError("Not a point")
```


The first pattern has two literals, (0, 0), and may be thought of as an extension of the literal pattern shown above. The next two patterns combine a literal and a variable, and the variable binds a value from the subject (point). The fourth pattern captures two values, which makes it conceptually similar to the unpacking assignment (x, y) = point.


Patterns and classes

If you are using classes to structure your data, you can use as a pattern the class name followed by an argument list resembling a constructor. This pattern has the ability to capture class attributes into variables:


```py
class Point:
    x: int
    y: int

def location(point):
    match point:
        case Point(x=0, y=0):
            print("Origin is the point's location.")
        case Point(x=0, y=y):
            print(f"Y={y} and the point is on the y-axis.")
        case Point(x=x, y=0):
            print(f"X={x} and the point is on the x-axis.")
        case Point():
            print("The point is located somewhere else on the plane.")
        case _:
            print("Not a point")
```



Patterns with positional parameters

You can use positional parameters with some builtin classes that provide an ordering for their attributes (e.g. dataclasses). You can also define a specific position for attributes in patterns by setting the __match_args__ special attribute in your classes. If it’s set to (“x”, “y”), the following patterns are all equivalent (and all bind the y attribute to the var variable):


```py
Point(1, var)
Point(1, y=var)
Point(x=1, y=var)
Point(y=var, x=1)
```



Nested patterns

Patterns can be arbitrarily nested. For example, if our data is a short list of points, it could be matched like this:


```py
match points:
    case []:
        print("No points in the list.")
    case [Point(0, 0)]:
        print("The origin is the only point in the list.")
    case [Point(x, y)]:
        print(f"A single point {x}, {y} is in the list.")
    case [Point(0, y1), Point(0, y2)]:
        print(f"Two points on the Y axis at {y1}, {y2} are in the list.")
    case _:
        print("Something else is found in the list.")
```



Complex patterns and the wildcard

To this point, the examples have used _ alone in the last case statement. A wildcard can be used in more complex patterns, such as `('error', code, _)`. For example:


```py
match test_variable:
    case ('warning', code, 40):
        print("A warning has been received.")
    case ('error', code, _):
        print(f"An error {code} occurred.")
```


In the above case, test_variable will match for (‘error’, code, 100) and (‘error’, code, 800).


Guard

We can add an if clause to a pattern, known as a “guard”. If the guard is false, match goes on to try the next case block. Note that value capture happens before the guard is evaluated:


```py
match point:
    case Point(x, y) if x == y:
        print(f"The point is located on the diagonal Y=X at {x}.")
    case Point(x, y):
        print(f"Point is not on the diagonal.")
```



Other Key Features

Several other key features:

• Like unpacking assignments, tuple and list patterns have exactly the same meaning and actually match arbitrary sequences. Technically, the subject must be a sequence. Therefore, an important exception is that patterns don’t match iterators. Also, to prevent a common mistake, sequence patterns don’t match strings.


• Sequence patterns support wildcards: `[x, y, *rest]` and `(x, y, *rest)` work similar to wildcards in unpacking assignments. The name after * may also be `_`, so `(x, y, *_)` matches a sequence of at least two items without binding the remaining items.


• Mapping patterns: {"bandwidth": b, "latency": l} captures the "bandwidth" and "latency" values from a dict. Unlike sequence patterns, extra keys are ignored. A wildcard `**rest` is also supported. (But `**_` would be redundant, so is not allowed.)


• Subpatterns may be captured using the as keyword:


>case (Point(x1, y1), Point(x2, y2) as p2): ...


This binds x1, y1, x2, y2 like you would expect without the as clause, and p2 to the entire second item of the subject.


• Most literals are compared by equality. However, the singletons True, False and None are compared by identity.


• Named constants may be used in patterns. These named constants must be dotted names to prevent the constant from being interpreted as a capture variable:


```py
from enum import Enum
class Color(Enum):
    RED = 0
    GREEN = 1
    BLUE = 2

match color:
    case Color.RED:
        print("I see red!")
    case Color.GREEN:
        print("Grass is green")
    case Color.BLUE:
        print("I'm feeling the blues :(")
```


For the full specification see PEP 634. Motivation and rationale are in PEP 635, and a longer tutorial is in PEP 636.


Optional EncodingWarning and encoding="locale" option

The default encoding of TextIOWrapper and open() is platform and locale dependent. Since UTF-8 is used on most Unix platforms, omitting encoding option when opening UTF-8 files (e.g. JSON, YAML, TOML, Markdown) is a very common bug. For example:


```py
# BUG: "rb" mode or encoding="utf-8" should be used.
with open("data.json") as f:
    data = json.load(f)
```


To find this type of bug, an optional EncodingWarning is added. It is emitted when sys.flags.warn_default_encoding is true and locale-specific default encoding is used.

-X warn_default_encoding option and PYTHONWARNDEFAULTENCODING are added to enable the warning.

See Text Encoding for more information.


### ===🗝 New Features Related to Type Hints

This section covers major changes affecting PEP 484 type hints and the typing module.


#### PEP 604: New Type Union Operator

A new type union operator was introduced which enables the syntax X | Y. This provides a cleaner way of expressing ‘either type X or type Y’ instead of using typing.Union, especially in type hints.

In previous versions of Python, to apply a type hint for functions accepting arguments of multiple types, typing.Union was used:


def square(number: Union[int, float]) -> Union[int, float]:
    return number ** 2


Type hints can now be written in a more succinct manner:


def square(number: int | float) -> int | float:
    return number ** 2


This new syntax is also accepted as the second argument to isinstance() and issubclass():


>>> isinstance(1, int | str)
True


See Union Type and PEP 604 for more details.

(Contributed by Maggie Moss and Philippe Prados in bpo-41428, with additions by Yurii Karabas and Serhiy Storchaka in bpo-44490.)


#### PEP 612: Parameter Specification Variables

Two new options to improve the information provided to static type checkers for PEP 484’s Callable have been added to the typing module.

The first is the parameter specification variable. They are used to forward the parameter types of one callable to another callable – a pattern commonly found in higher order functions and decorators. Examples of usage can be found in typing.ParamSpec. Previously, there was no easy way to type annotate dependency of parameter types in such a precise manner.

The second option is the new Concatenate operator. It’s used in conjunction with parameter specification variables to type annotate a higher order callable which adds or removes parameters of another callable. Examples of usage can be found in typing.Concatenate.

See typing.Callable, typing.ParamSpec, typing.Concatenate, typing.ParamSpecArgs, typing.ParamSpecKwargs, and PEP 612 for more details.

(Contributed by Ken Jin in bpo-41559, with minor enhancements by Jelle Zijlstra in bpo-43783. PEP written by Mark Mendoza.)


#### PEP 613: TypeAlias

PEP 484 introduced the concept of type aliases, only requiring them to be top-level unannotated assignments. This simplicity sometimes made it difficult for type checkers to distinguish between type aliases and ordinary assignments, especially when forward references or invalid types were involved. Compare:


```py
StrCache = 'Cache[str]'  # a type alias
LOG_PREFIX = 'LOG[DEBUG]'  # a module constant
```


Now the typing module has a special value TypeAlias which lets you declare type aliases more explicitly:


```py
StrCache: TypeAlias = 'Cache[str]'  # a type alias
LOG_PREFIX = 'LOG[DEBUG]'  # a module constant
```


See PEP 613 for more details.

(Contributed by Mikhail Golubev in bpo-41923.)


#### PEP 647: User-Defined Type Guards

TypeGuard has been added to the typing module to annotate type guard functions and improve information provided to static type checkers during type narrowing. For more information, please see TypeGuard’s documentation, and PEP 647.

(Contributed by Ken Jin and Guido van Rossum in bpo-43766. PEP written by Eric Traut.)


### ===🗝 Other Language Changes


• The int type has a new method int.bit_count(), returning the number of ones in the binary expansion of a given integer, also known as the population count. (Contributed by Niklas Fiekas in bpo-29882.)

• The views returned by dict.keys(), dict.values() and dict.items() now all have a mapping attribute that gives a types.MappingProxyType object wrapping the original dictionary. (Contributed by Dennis Sweeney in bpo-40890.)

• PEP 618: The zip() function now has an optional strict flag, used to require that all the iterables have an equal length.

• Builtin and extension functions that take integer arguments no longer accept Decimals, Fractions and other objects that can be converted to integers only with a loss (e.g. that have the __int__() method but do not have the __index__() method). (Contributed by Serhiy Storchaka in bpo-37999.)

• If object.__ipow__() returns NotImplemented, the operator will correctly fall back to object.__pow__() and object.__rpow__() as expected. (Contributed by Alex Shkop in bpo-38302.)

• Assignment expressions can now be used unparenthesized within set literals and set comprehensions, as well as in sequence indexes (but not slices).

• Functions have a new __builtins__ attribute which is used to look for builtin symbols when a function is executed, instead of looking into __globals__['__builtins__']. The attribute is initialized from __globals__["__builtins__"] if it exists, else from the current builtins. (Contributed by Mark Shannon in bpo-42990.)

• Two new builtin functions – aiter() and anext() have been added to provide asynchronous counterparts to iter() and next(), respectively. (Contributed by Joshua Bronson, Daniel Pope, and Justin Wang in bpo-31861.)

• Static methods (@staticmethod) and class methods (@classmethod) now inherit the method attributes (__module__, __name__, __qualname__, __doc__, __annotations__) and have a new __wrapped__ attribute. Moreover, static methods are now callable as regular functions. (Contributed by Victor Stinner in bpo-43682.)

• Annotations for complex targets (everything beside simple name targets defined by PEP 526) no longer cause any runtime effects with from __future__ import annotations. (Contributed by Batuhan Taskaya in bpo-42737.)

• Class and module objects now lazy-create empty annotations dicts on demand. The annotations dicts are stored in the object’s __dict__ for backwards compatibility. This improves the best practices for working with __annotations__; for more information, please see Annotations Best Practices. (Contributed by Larry Hastings in bpo-43901.)

• Annotations consist of yield, yield from, await or named expressions are now forbidden under from __future__ import annotations due to their side effects. (Contributed by Batuhan Taskaya in bpo-42725.)

• Usage of unbound variables, super() and other expressions that might alter the processing of symbol table as annotations are now rendered effectless under from __future__ import annotations. (Contributed by Batuhan Taskaya in bpo-42725.)

• Hashes of NaN values of both float type and decimal.Decimal type now depend on object identity. Formerly, they always hashed to 0 even though NaN values are not equal to one another. This caused potentially quadratic runtime behavior due to excessive hash collisions when creating dictionaries and sets containing multiple NaNs. (Contributed by Raymond Hettinger in bpo-43475.)

• A SyntaxError (instead of a NameError) will be raised when deleting the __debug__ constant. (Contributed by Dong-hee Na in bpo-45000.)

• SyntaxError exceptions now have end_lineno and end_offset attributes. They will be None if not determined. (Contributed by Pablo Galindo in bpo-43914.)


### ===🗝 New Modules
• None yet.


### ===🗝 Improved Modules


asyncio

Add missing connect_accepted_socket() method. (Contributed by Alex Grönholm in bpo-41332.)


argparse

Misleading phrase “optional arguments” was replaced with “options” in argparse help. Some tests might require adaptation if they rely on exact output match. (Contributed by Raymond Hettinger in bpo-9694.)


array

The index() method of array.array now has optional start and stop parameters. (Contributed by Anders Lorentsen and Zackery Spytz in bpo-31956.)


asynchat, asyncore, smtpd

These modules have been marked as deprecated in their module documentation since Python 3.6. An import-time DeprecationWarning has now been added to all three of these modules.


base64

Add base64.b32hexencode() and base64.b32hexdecode() to support the Base32 Encoding with Extended Hex Alphabet.


bdb

Add clearBreakpoints() to reset all set breakpoints. (Contributed by Irit Katriel in bpo-24160.)


bisect

Added the possibility of providing a key function to the APIs in the bisect module. (Contributed by Raymond Hettinger in bpo-4356.)


codecs

Add a codecs.unregister() function to unregister a codec search function. (Contributed by Hai Shi in bpo-41842.)


collections.abc

The __args__ of the parameterized generic for collections.abc.Callable are now consistent with typing.Callable. collections.abc.Callable generic now flattens type parameters, similar to what typing.Callable currently does. This means that collections.abc.Callable[[int, str], str] will have __args__ of (int, str, str); previously this was ([int, str], str). To allow this change, types.GenericAlias can now be subclassed, and a subclass will be returned when subscripting the collections.abc.Callable type. Note that a TypeError may be raised for invalid forms of parameterizing collections.abc.Callable which may have passed silently in Python 3.9. (Contributed by Ken Jin in bpo-42195.)


contextlib

Add a contextlib.aclosing() context manager to safely close async generators and objects representing asynchronously released resources. (Contributed by Joongi Kim and John Belmonte in bpo-41229.)

Add asynchronous context manager support to contextlib.nullcontext(). (Contributed by Tom Gringauz in bpo-41543.)

Add AsyncContextDecorator, for supporting usage of async context managers as decorators.


curses

The extended color functions added in ncurses 6.1 will be used transparently by curses.color_content(), curses.init_color(), curses.init_pair(), and curses.pair_content(). A new function, curses.has_extended_color_support(), indicates whether extended color support is provided by the underlying ncurses library. (Contributed by Jeffrey Kintscher and Hans Petter Jansson in bpo-36982.)

The BUTTON5_* constants are now exposed in the curses module if they are provided by the underlying curses library. (Contributed by Zackery Spytz in bpo-39273.)


dataclasses


__slots__

Added slots parameter in dataclasses.dataclass() decorator. (Contributed by Yurii Karabas in bpo-42269)


Keyword-only fields

dataclasses now supports fields that are keyword-only in the generated __init__ method. There are a number of ways of specifying keyword-only fields.

You can say that every field is keyword-only:


```py
from dataclasses import dataclass

@dataclass(kw_only=True)
class Birthday:
    name: str
    birthday: datetime.date
```


Both name and birthday are keyword-only parameters to the generated __init__ method.

You can specify keyword-only on a per-field basis:


```py
from dataclasses import dataclass

@dataclass
class Birthday:
    name: str
    birthday: datetime.date = field(kw_only=True)

```

Here only birthday is keyword-only. If you set kw_only on individual fields, be aware that there are rules about re-ordering fields due to keyword-only fields needing to follow non-keyword-only fields. See the full dataclasses documentation for details.

You can also specify that all fields following a KW_ONLY marker are keyword-only. This will probably be the most common usage:


```py
from dataclasses import dataclass, KW_ONLY

@dataclass
class Point:
    x: float
    y: float
    _: KW_ONLY
    z: float = 0.0
    t: float = 0.0
```


Here, z and t are keyword-only parameters, while x and y are not. (Contributed by Eric V. Smith in bpo-43532)


distutils

The entire distutils package is deprecated, to be removed in Python 3.12. Its functionality for specifying package builds has already been completely replaced by third-party packages setuptools and packaging, and most other commonly used APIs are available elsewhere in the standard library (such as platform, shutil, subprocess or sysconfig). There are no plans to migrate any other functionality from distutils, and applications that are using other functions should plan to make private copies of the code. Refer to PEP 632 for discussion.

The bdist_wininst command deprecated in Python 3.8 has been removed. The bdist_wheel command is now recommended to distribute binary packages on Windows. (Contributed by Victor Stinner in bpo-42802.)


doctest

When a module does not define __loader__, fall back to __spec__.loader. (Contributed by Brett Cannon in bpo-42133.)


encodings

encodings.normalize_encoding() now ignores non-ASCII characters. (Contributed by Hai Shi in bpo-39337.)


fileinput

Add encoding and errors parameters in fileinput.input() and fileinput.FileInput. (Contributed by Inada Naoki in bpo-43712.)

fileinput.hook_compressed() now returns TextIOWrapper object when mode is “r” and file is compressed, like uncompressed files. (Contributed by Inada Naoki in bpo-5758.)


faulthandler

The faulthandler module now detects if a fatal error occurs during a garbage collector collection. (Contributed by Victor Stinner in bpo-44466.)


gc

Add audit hooks for gc.get_objects(), gc.get_referrers() and gc.get_referents(). (Contributed by Pablo Galindo in bpo-43439.)


glob

Add the root_dir and dir_fd parameters in glob() and iglob() which allow to specify the root directory for searching. (Contributed by Serhiy Storchaka in bpo-38144.)


hashlib

The hashlib module requires OpenSSL 1.1.1 or newer. (Contributed by Christian Heimes in PEP 644 and bpo-43669.)

The hashlib module has preliminary support for OpenSSL 3.0.0. (Contributed by Christian Heimes in bpo-38820 and other issues.)

The pure-Python fallback of pbkdf2_hmac() is deprecated. In the future PBKDF2-HMAC will only be available when Python has been built with OpenSSL support. (Contributed by Christian Heimes in bpo-43880.)


hmac

The hmac module now uses OpenSSL’s HMAC implementation internally. (Contributed by Christian Heimes in bpo-40645.)


IDLE and idlelib

Make IDLE invoke sys.excepthook() (when started without ‘-n’). User hooks were previously ignored. (Patch by Ken Hilton in bpo-43008.)

This change was backported to a 3.9 maintenance release.

Add a Shell sidebar. Move the primary prompt (‘>>>’) to the sidebar. Add secondary prompts (’…’) to the sidebar. Left click and optional drag selects one or more lines of text, as with the editor line number sidebar. Right click after selecting text lines displays a context menu with ‘copy with prompts’. This zips together prompts from the sidebar with lines from the selected text. This option also appears on the context menu for the text. (Contributed by Tal Einat in bpo-37903.)

Use spaces instead of tabs to indent interactive code. This makes interactive code entries ‘look right’. Making this feasible was a major motivation for adding the shell sidebar. Contributed by Terry Jan Reedy in bpo-37892.)

We expect to backport these shell changes to a future 3.9 maintenance release.

Highlight the new soft keywords match, case, and _ in pattern-matching statements. However, this highlighting is not perfect and will be incorrect in some rare cases, including some _-s in case patterns. (Contributed by Tal Einat in bpo-44010.)


importlib.metadata

Feature parity with importlib_metadata 4.6 (history).

importlib.metadata entry points now provide a nicer experience for selecting entry points by group and name through a new importlib.metadata.EntryPoints class. See the Compatibility Note in the docs for more info on the deprecation and usage.

Added importlib.metadata.packages_distributions() for resolving top-level Python modules and packages to their importlib.metadata.Distribution.


inspect

When a module does not define __loader__, fall back to __spec__.loader. (Contributed by Brett Cannon in bpo-42133.)

Add inspect.get_annotations(), which safely computes the annotations defined on an object. It works around the quirks of accessing the annotations on various types of objects, and makes very few assumptions about the object it examines. inspect.get_annotations() can also correctly un-stringize stringized annotations. inspect.get_annotations() is now considered best practice for accessing the annotations dict defined on any Python object; for more information on best practices for working with annotations, please see Annotations Best Practices. Relatedly, inspect.signature(), inspect.Signature.from_callable(), and inspect.Signature.from_function() now call inspect.get_annotations() to retrieve annotations. This means inspect.signature() and inspect.Signature.from_callable() can also now un-stringize stringized annotations. (Contributed by Larry Hastings in bpo-43817.)


linecache

When a module does not define __loader__, fall back to __spec__.loader. (Contributed by Brett Cannon in bpo-42133.)


os

Add os.cpu_count() support for VxWorks RTOS. (Contributed by Peixing Xin in bpo-41440.)

Add a new function os.eventfd() and related helpers to wrap the eventfd2 syscall on Linux. (Contributed by Christian Heimes in bpo-41001.)

Add os.splice() that allows to move data between two file descriptors without copying between kernel address space and user address space, where one of the file descriptors must refer to a pipe. (Contributed by Pablo Galindo in bpo-41625.)

Add O_EVTONLY, O_FSYNC, O_SYMLINK and O_NOFOLLOW_ANY for macOS. (Contributed by Dong-hee Na in bpo-43106.)


os.path

os.path.realpath() now accepts a strict keyword-only argument. When set to True, OSError is raised if a path doesn’t exist or a symlink loop is encountered. (Contributed by Barney Gale in bpo-43757.)


pathlib

Add slice support to PurePath.parents. (Contributed by Joshua Cannon in bpo-35498)

Add negative indexing support to PurePath.parents. (Contributed by Yaroslav Pankovych in bpo-21041)

Add Path.hardlink_to method that supersedes link_to(). The new method has the same argument order as symlink_to(). (Contributed by Barney Gale in bpo-39950.)

pathlib.Path.stat() and chmod() now accept a follow_symlinks keyword-only argument for consistency with corresponding functions in the os module. (Contributed by Barney Gale in bpo-39906.)


platform

Add platform.freedesktop_os_release() to retrieve operation system identification from freedesktop.org os-release standard file. (Contributed by Christian Heimes in bpo-28468)


pprint

pprint.pprint() now accepts a new underscore_numbers keyword argument. (Contributed by sblondon in bpo-42914.)

pprint can now pretty-print dataclasses.dataclass instances. (Contributed by Lewis Gaul in bpo-43080.)


py_compile

Add --quiet option to command-line interface of py_compile. (Contributed by Gregory Schevchenko in bpo-38731.)


pyclbr

Add an end_lineno attribute to the Function and Class objects in the tree returned by pyclbr.readline() and pyclbr.readline_ex(). It matches the existing (start) lineno. (Contributed by Aviral Srivastava in bpo-38307.)


shelve

The shelve module now uses pickle.DEFAULT_PROTOCOL by default instead of pickle protocol 3 when creating shelves. (Contributed by Zackery Spytz in bpo-34204.)


statistics

Add covariance(), Pearson’s correlation(), and simple linear_regression() functions. (Contributed by Tymoteusz Wołodźko in bpo-38490.)


site

When a module does not define __loader__, fall back to __spec__.loader. (Contributed by Brett Cannon in bpo-42133.)


socket

The exception socket.timeout is now an alias of TimeoutError. (Contributed by Christian Heimes in bpo-42413.)

Add option to create MPTCP sockets with IPPROTO_MPTCP (Contributed by Rui Cunha in bpo-43571.)

Add IP_RECVTOS option to receive the type of service (ToS) or DSCP/ECN fields (Contributed by Georg Sauthoff in bpo-44077.)


ssl

The ssl module requires OpenSSL 1.1.1 or newer. (Contributed by Christian Heimes in PEP 644 and bpo-43669.)

The ssl module has preliminary support for OpenSSL 3.0.0 and new option OP_IGNORE_UNEXPECTED_EOF. (Contributed by Christian Heimes in bpo-38820, bpo-43794, bpo-43788, bpo-43791, bpo-43799, bpo-43920, bpo-43789, and bpo-43811.)

Deprecated function and use of deprecated constants now result in a DeprecationWarning. ssl.SSLContext.options has OP_NO_SSLv2 and OP_NO_SSLv3 set by default and therefore cannot warn about setting the flag again. The deprecation section has a list of deprecated features. (Contributed by Christian Heimes in bpo-43880.)

The ssl module now has more secure default settings. Ciphers without forward secrecy or SHA-1 MAC are disabled by default. Security level 2 prohibits weak RSA, DH, and ECC keys with less than 112 bits of security. SSLContext defaults to minimum protocol version TLS 1.2. Settings are based on Hynek Schlawack’s research. (Contributed by Christian Heimes in bpo-43998.)

The deprecated protocols SSL 3.0, TLS 1.0, and TLS 1.1 are no longer officially supported. Python does not block them actively. However OpenSSL build options, distro configurations, vendor patches, and cipher suites may prevent a successful handshake.

Add a timeout parameter to the ssl.get_server_certificate() function. (Contributed by Zackery Spytz in bpo-31870.)

The ssl module uses heap-types and multi-phase initialization. (Contributed by Christian Heimes in bpo-42333.)

A new verify flag VERIFY_X509_PARTIAL_CHAIN has been added. (Contributed by l0x in bpo-40849.)


sqlite3

Add audit events for connect/handle(), enable_load_extension(), and load_extension(). (Contributed by Erlend E. Aasland in bpo-43762.)


sys

Add sys.orig_argv attribute: the list of the original command line arguments passed to the Python executable. (Contributed by Victor Stinner in bpo-23427.)

Add sys.stdlib_module_names, containing the list of the standard library module names. (Contributed by Victor Stinner in bpo-42955.)


_thread

_thread.interrupt_main() now takes an optional signal number to simulate (the default is still signal.SIGINT). (Contributed by Antoine Pitrou in bpo-43356.)


threading

Add threading.gettrace() and threading.getprofile() to retrieve the functions set by threading.settrace() and threading.setprofile() respectively. (Contributed by Mario Corchero in bpo-42251.)

Add threading.__excepthook__ to allow retrieving the original value of threading.excepthook() in case it is set to a broken or a different value. (Contributed by Mario Corchero in bpo-42308.)


traceback

The format_exception(), format_exception_only(), and print_exception() functions can now take an exception object as a positional-only argument. (Contributed by Zackery Spytz and Matthias Bussonnier in bpo-26389.)


types

Reintroduce the types.EllipsisType, types.NoneType and types.NotImplementedType classes, providing a new set of types readily interpretable by type checkers. (Contributed by Bas van Beek in bpo-41810.)


typing

For major changes, see New Features Related to Type Hints.

The behavior of typing.Literal was changed to conform with PEP 586 and to match the behavior of static type checkers specified in the PEP.

1.Literal now de-duplicates parameters.


2.Equality comparisons between Literal objects are now order independent.


3.Literal comparisons now respect types. For example, Literal[0] == Literal[False] previously evaluated to True. It is now False. To support this change, the internally used type cache now supports differentiating types.


4.Literal objects will now raise a TypeError exception during equality comparisons if any of their parameters are not hashable. Note that declaring Literal with unhashable parameters will not throw an error:


>>> from typing import Literal
>>> Literal[{0}]
>>> Literal[{0}] == Literal[{False}]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: unhashable type: 'set'



(Contributed by Yurii Karabas in bpo-42345.)

Add new function typing.is_typeddict() to introspect if an annotation is a typing.TypedDict. (Contributed by Patrick Reader in bpo-41792)

Subclasses of typing.Protocol which only have data variables declared will now raise a TypeError when checked with isinstance unless they are decorated with runtime_checkable(). Previously, these checks passed silently. Users should decorate their subclasses with the runtime_checkable() decorator if they want runtime protocols. (Contributed by Yurii Karabas in bpo-38908)

Importing from the typing.io and typing.re submodules will now emit DeprecationWarning. These submodules have been deprecated since Python 3.8 and will be removed in a future version of Python. Anything belonging to those submodules should be imported directly from typing instead. (Contributed by Sebastian Rittau in bpo-38291)


unittest

Add new method assertNoLogs() to complement the existing assertLogs(). (Contributed by Kit Yan Choi in bpo-39385.)


urllib.parse

Python versions earlier than Python 3.10 allowed using both ; and & as query parameter separators in urllib.parse.parse_qs() and urllib.parse.parse_qsl(). Due to security concerns, and to conform with newer W3C recommendations, this has been changed to allow only a single separator key, with & as the default. This change also affects cgi.parse() and cgi.parse_multipart() as they use the affected functions internally. For more details, please see their respective documentation. (Contributed by Adam Goldschmidt, Senthil Kumaran and Ken Jin in bpo-42967.)

The presence of newline or tab characters in parts of a URL allows for some forms of attacks. Following the WHATWG specification that updates RFC 3986, ASCII newline \n, \r and tab \t characters are stripped from the URL by the parser in urllib.parse preventing such attacks. The removal characters are controlled by a new module level variable urllib.parse._UNSAFE_URL_BYTES_TO_REMOVE. (See bpo-43882)


xml

Add a LexicalHandler class to the xml.sax.handler module. (Contributed by Jonathan Gossage and Zackery Spytz in bpo-35018.)


zipimport

Add methods related to PEP 451: find_spec(), zipimport.zipimporter.create_module(), and zipimport.zipimporter.exec_module(). (Contributed by Brett Cannon in bpo-42131.)

Add invalidate_caches() method. (Contributed by Desmond Cheong in bpo-14678.)


### ===🗝 Optimizations

• Constructors str(), bytes() and bytearray() are now faster (around 30–40% for small objects). (Contributed by Serhiy Storchaka in bpo-41334.)

• The runpy module now imports fewer modules. The python3 -m module-name command startup time is 1.4x faster in average. On Linux, python3 -I -m module-name imports 69 modules on Python 3.9, whereas it only imports 51 modules (-18) on Python 3.10. (Contributed by Victor Stinner in bpo-41006 and bpo-41718.)

• The LOAD_ATTR instruction now uses new “per opcode cache” mechanism. It is about 36% faster now for regular attributes and 44% faster for slots. (Contributed by Pablo Galindo and Yury Selivanov in bpo-42093 and Guido van Rossum in bpo-42927, based on ideas implemented originally in PyPy and MicroPython.)

• When building Python with --enable-optimizations now -fno-semantic-interposition is added to both the compile and link line. This speeds builds of the Python interpreter created with --enable-shared with gcc by up to 30%. See this article for more details. (Contributed by Victor Stinner and Pablo Galindo in bpo-38980.)

• Use a new output buffer management code for bz2 / lzma / zlib modules, and add .readall() function to `_compression.DecompressReader` class. bz2 decompression is now 1.09x ~ 1.17x faster, lzma decompression 1.20x ~ 1.32x faster, GzipFile.read(-1) 1.11x ~ 1.18x faster. (Contributed by Ma Lin, reviewed by Gregory P. Smith, in bpo-41486)

• When using stringized annotations, annotations dicts for functions are no longer created when the function is created. Instead, they are stored as a tuple of strings, and the function object lazily converts this into the annotations dict on demand. This optimization cuts the CPU time needed to define an annotated function by half. (Contributed by Yurii Karabas and Inada Naoki in bpo-42202)

• Substring search functions such as str1 in str2 and str2.find(str1) now sometimes use Crochemore & Perrin’s “Two-Way” string searching algorithm to avoid quadratic behavior on long strings. (Contributed by Dennis Sweeney in bpo-41972)

• Add micro-optimizations to `_PyType_Lookup()` to improve type attribute cache lookup performance in the common case of cache hits. This makes the interpreter 1.04 times faster on average. (Contributed by Dino Viehland in bpo-43452)

• The following built-in functions now support the faster PEP 590 vectorcall calling convention: map(), filter(), reversed(), bool() and float(). (Contributed by Dong-hee Na and Jeroen Demeyer in bpo-43575, bpo-43287, bpo-41922, bpo-41873 and bpo-41870)

• BZ2File performance is improved by removing internal RLock. This makes BZ2File thread unsafe in the face of multiple simultaneous readers or writers, just like its equivalent classes in gzip and lzma have always been. (Contributed by Inada Naoki in bpo-43785).



### ===🗝 Deprecated

• Currently Python accepts numeric literals immediately followed by keywords, for example 0in x, 1or x, 0if 1else 2. It allows confusing and ambiguous expressions like [0x1for x in y] (which can be interpreted as [0x1 for x in y] or [0x1f or x in y]). Starting in this release, a deprecation warning is raised if the numeric literal is immediately followed by one of keywords and, else, for, if, in, is and or. In future releases it will be changed to syntax warning, and finally to syntax error. (Contributed by Serhiy Storchaka in bpo-43833).


• Starting in this release, there will be a concerted effort to begin cleaning up old import semantics that were kept for Python 2.7 compatibility. Specifically, find_loader()/find_module() (superseded by find_spec()), load_module() (superseded by exec_module()), module_repr() (which the import system takes care of for you), the __package__ attribute (superseded by __spec__.parent), the __loader__ attribute (superseded by __spec__.loader), and the __cached__ attribute (superseded by __spec__.cached) will slowly be removed (as well as other classes and methods in importlib). ImportWarning and/or DeprecationWarning will be raised as appropriate to help identify code which needs updating during this transition.


• The entire distutils namespace is deprecated, to be removed in Python 3.12. Refer to the module changes section for more information.


• Non-integer arguments to random.randrange() are deprecated. The ValueError is deprecated in favor of a TypeError. (Contributed by Serhiy Storchaka and Raymond Hettinger in bpo-37319.)


• The various load_module() methods of importlib have been documented as deprecated since Python 3.6, but will now also trigger a DeprecationWarning. Use exec_module() instead. (Contributed by Brett Cannon in bpo-26131.)


• zimport.zipimporter.load_module() has been deprecated in preference for exec_module(). (Contributed by Brett Cannon in bpo-26131.)


• The use of load_module() by the import system now triggers an ImportWarning as exec_module() is preferred. (Contributed by Brett Cannon in bpo-26131.)


• The use of importlib.abc.MetaPathFinder.find_module() and importlib.abc.PathEntryFinder.find_module() by the import system now trigger an ImportWarning as importlib.abc.MetaPathFinder.find_spec() and importlib.abc.PathEntryFinder.find_spec() are preferred, respectively. You can use importlib.util.spec_from_loader() to help in porting. (Contributed by Brett Cannon in bpo-42134.)


• The use of importlib.abc.PathEntryFinder.find_loader() by the import system now triggers an ImportWarning as importlib.abc.PathEntryFinder.find_spec() is preferred. You can use importlib.util.spec_from_loader() to help in porting. (Contributed by Brett Cannon in bpo-43672.)


• The various implementations of importlib.abc.MetaPathFinder.find_module() ( importlib.machinery.BuiltinImporter.find_module(), importlib.machinery.FrozenImporter.find_module(), importlib.machinery.WindowsRegistryFinder.find_module(), importlib.machinery.PathFinder.find_module(), importlib.abc.MetaPathFinder.find_module() ), importlib.abc.PathEntryFinder.find_module() ( importlib.machinery.FileFinder.find_module() ), and importlib.abc.PathEntryFinder.find_loader() ( importlib.machinery.FileFinder.find_loader() ) now raise DeprecationWarning and are slated for removal in Python 3.12 (previously they were documented as deprecated in Python 3.4). (Contributed by Brett Cannon in bpo-42135.)


• importlib.abc.Finder is deprecated (including its sole method, find_module()). Both importlib.abc.MetaPathFinder and importlib.abc.PathEntryFinder no longer inherit from the class. Users should inherit from one of these two classes as appropriate instead. (Contributed by Brett Cannon in bpo-42135.)


• The deprecations of imp, importlib.find_loader(), importlib.util.set_package_wrapper(), importlib.util.set_loader_wrapper(), importlib.util.module_for_loader(), pkgutil.ImpImporter, and pkgutil.ImpLoader have all been updated to list Python 3.12 as the slated version of removal (they began raising DeprecationWarning in previous versions of Python). (Contributed by Brett Cannon in bpo-43720.)


• The import system now uses the __spec__ attribute on modules before falling back on module_repr() for a module’s __repr__() method. Removal of the use of module_repr() is scheduled for Python 3.12. (Contributed by Brett Cannon in bpo-42137.)


• importlib.abc.Loader.module_repr(), importlib.machinery.FrozenLoader.module_repr(), and importlib.machinery.BuiltinLoader.module_repr() are deprecated and slated for removal in Python 3.12. (Contributed by Brett Cannon in bpo-42136.)


• sqlite3.OptimizedUnicode has been undocumented and obsolete since Python 3.3, when it was made an alias to str. It is now deprecated, scheduled for removal in Python 3.12. (Contributed by Erlend E. Aasland in bpo-42264.)


• asyncio.get_event_loop() now emits a deprecation warning if there is no running event loop. In the future it will be an alias of get_running_loop(). asyncio functions which implicitly create Future or Task objects now emit a deprecation warning if there is no running event loop and no explicit loop argument is passed: ensure_future(), wrap_future(), gather(), shield(), as_completed() and constructors of Future, Task, StreamReader, StreamReaderProtocol. (Contributed by Serhiy Storchaka in bpo-39529.)


• The undocumented built-in function sqlite3.enable_shared_cache is now deprecated, scheduled for removal in Python 3.12. Its use is strongly discouraged by the SQLite3 documentation. See the SQLite3 docs for more details. If a shared cache must be used, open the database in URI mode using the cache=shared query parameter. (Contributed by Erlend E. Aasland in bpo-24464.)


• The following threading methods are now deprecated:
◦ threading.currentThread => threading.current_thread()
◦ threading.activeCount => threading.active_count()
◦ threading.Condition.notifyAll => threading.Condition.notify_all()
◦ threading.Event.isSet => threading.Event.is_set()
◦ threading.Thread.setName => threading.Thread.name
◦ threading.thread.getName => threading.Thread.name
◦ threading.Thread.isDaemon => threading.Thread.daemon
◦ threading.Thread.setDaemon => threading.Thread.daemon

(Contributed by Jelle Zijlstra in bpo-21574.)


• pathlib.Path.link_to() is deprecated and slated for removal in Python 3.12. Use pathlib.Path.hardlink_to() instead. (Contributed by Barney Gale in bpo-39950.)


• cgi.log() is deprecated and slated for removal in Python 3.12. (Contributed by Inada Naoki in bpo-41139.)


• The following ssl features have been deprecated since Python 3.6, Python 3.7, or OpenSSL 1.1.0 and will be removed in 3.11:
◦ OP_NO_SSLv2, OP_NO_SSLv3, OP_NO_TLSv1, OP_NO_TLSv1_1, OP_NO_TLSv1_2, and OP_NO_TLSv1_3 are replaced by sslSSLContext.minimum_version and sslSSLContext.maximum_version.
◦ PROTOCOL_SSLv2, PROTOCOL_SSLv3, PROTOCOL_SSLv23, PROTOCOL_TLSv1, PROTOCOL_TLSv1_1, PROTOCOL_TLSv1_2, and PROTOCOL_TLS are deprecated in favor of PROTOCOL_TLS_CLIENT and PROTOCOL_TLS_SERVER
◦ wrap_socket() is replaced by ssl.SSLContext.wrap_socket()
◦ match_hostname()
◦ RAND_pseudo_bytes(), RAND_egd()
◦ NPN features like ssl.SSLSocket.selected_npn_protocol() and ssl.SSLContext.set_npn_protocols() are replaced by ALPN.


• The threading debug (PYTHONTHREADDEBUG environment variable) is deprecated in Python 3.10 and will be removed in Python 3.12. This feature requires a debug build of Python. (Contributed by Victor Stinner in bpo-44584.)


• Importing from the typing.io and typing.re submodules will now emit DeprecationWarning. These submodules will be removed in a future version of Python. Anything belonging to these submodules should be imported directly from typing instead. (Contributed by Sebastian Rittau in bpo-38291)



### ===🗝 Removed

• Removed special methods __int__, __float__, __floordiv__, __mod__, __divmod__, __rfloordiv__, __rmod__ and __rdivmod__ of the complex class. They always raised a TypeError. (Contributed by Serhiy Storchaka in bpo-41974.)


• The ParserBase.error() method from the private and undocumented _markupbase module has been removed. html.parser.HTMLParser is the only subclass of ParserBase and its error() implementation was already removed in Python 3.5. (Contributed by Berker Peksag in bpo-31844.)


• Removed the unicodedata.ucnhash_CAPI attribute which was an internal PyCapsule object. The related private _PyUnicode_Name_CAPI structure was moved to the internal C API. (Contributed by Victor Stinner in bpo-42157.)


• Removed the parser module, which was deprecated in 3.9 due to the switch to the new PEG parser, as well as all the C source and header files that were only being used by the old parser, including node.h, parser.h, graminit.h and grammar.h.


• Removed the Public C API functions PyParser_SimpleParseStringFlags, PyParser_SimpleParseStringFlagsFilename, PyParser_SimpleParseFileFlags and PyNode_Compile that were deprecated in 3.9 due to the switch to the new PEG parser.


• Removed the formatter module, which was deprecated in Python 3.4. It is somewhat obsolete, little used, and not tested. It was originally scheduled to be removed in Python 3.6, but such removals were delayed until after Python 2.7 EOL. Existing users should copy whatever classes they use into their code. (Contributed by Dong-hee Na and Terry J. Reedy in bpo-42299.)


• Removed the PyModule_GetWarningsModule() function that was useless now due to the `_warnings` module was converted to a builtin module in 2.6. (Contributed by Hai Shi in bpo-42599.)


• Remove deprecated aliases to Collections Abstract Base Classes from the collections module. (Contributed by Victor Stinner in bpo-37324.)


• The loop parameter has been removed from most of asyncio’s high-level API following deprecation in Python 3.8. The motivation behind this change is multifold:
1.This simplifies the high-level API.
2.The functions in the high-level API have been implicitly getting the current thread’s running event loop since Python 3.7. There isn’t a need to pass the event loop to the API in most normal use cases.
3.Event loop passing is error-prone especially when dealing with loops running in different threads.

Note that the low-level API will still accept loop. See Changes in the Python API for examples of how to replace existing code.

(Contributed by Yurii Karabas, Andrew Svetlov, Yury Selivanov and Kyle Stanley in bpo-42392.)



### ===🗝 Porting to Python 3.10

This section lists previously described changes and other bugfixes that may require changes to your code.


Changes in the Python syntax

• Deprecation warning is now emitted when compiling previously valid syntax if the numeric literal is immediately followed by a keyword (like in 0in x). In future releases it will be changed to syntax warning, and finally to a syntax error. To get rid of the warning and make the code compatible with future releases just add a space between the numeric literal and the following keyword. (Contributed by Serhiy Storchaka in bpo-43833).


Changes in the Python API

• The etype parameters of the format_exception(), format_exception_only(), and print_exception() functions in the traceback module have been renamed to exc. (Contributed by Zackery Spytz and Matthias Bussonnier in bpo-26389.)


• atexit: At Python exit, if a callback registered with atexit.register() fails, its exception is now logged. Previously, only some exceptions were logged, and the last exception was always silently ignored. (Contributed by Victor Stinner in bpo-42639.)


• collections.abc.Callable generic now flattens type parameters, similar to what typing.Callable currently does. This means that collections.abc.Callable[[int, str], str] will have __args__ of (int, str, str); previously this was ([int, str], str). Code which accesses the arguments via typing.get_args() or __args__ need to account for this change. Furthermore, TypeError may be raised for invalid forms of parameterizing collections.abc.Callable which may have passed silently in Python 3.9. (Contributed by Ken Jin in bpo-42195.)


• socket.htons() and socket.ntohs() now raise OverflowError instead of DeprecationWarning if the given parameter will not fit in a 16-bit unsigned integer. (Contributed by Erlend E. Aasland in bpo-42393.)


• The loop parameter has been removed from most of asyncio’s high-level API following deprecation in Python 3.8.

A coroutine that currently looks like this:


```py
async def foo(loop):
    await asyncio.sleep(1, loop=loop)


Should be replaced with this:


async def foo():
    await asyncio.sleep(1)
```


If foo() was specifically designed not to run in the current thread’s running event loop (e.g. running in another thread’s event loop), consider using asyncio.run_coroutine_threadsafe() instead.

(Contributed by Yurii Karabas, Andrew Svetlov, Yury Selivanov and Kyle Stanley in bpo-42392.)


• The types.FunctionType constructor now inherits the current builtins if the globals dictionary has no "__builtins__" key, rather than using {"None": None} as builtins: same behavior as eval() and exec() functions. Defining a function with def function(...): ... in Python is not affected, globals cannot be overridden with this syntax: it also inherits the current builtins. (Contributed by Victor Stinner in bpo-42990.)



### ===🗝 Changes in the C API

• The C API functions PyParser_SimpleParseStringFlags, PyParser_SimpleParseStringFlagsFilename, PyParser_SimpleParseFileFlags, PyNode_Compile and the type used by these functions, struct `_node`, were removed due to the switch to the new PEG parser.

Source should be now be compiled directly to a code object using, for example, Py_CompileString(). The resulting code object can then be evaluated using, for example, PyEval_EvalCode().

Specifically:

◦ A call to PyParser_SimpleParseStringFlags followed by PyNode_Compile can be replaced by calling Py_CompileString().


◦ There is no direct replacement for PyParser_SimpleParseFileFlags. To compile code from a FILE * argument, you will need to read the file in C and pass the resulting buffer to Py_CompileString().


◦ To compile a file given a char * filename, explicitly open the file, read it and compile the result. One way to do this is using the io module with PyImport_ImportModule(), PyObject_CallMethod(), PyBytes_AsString() and Py_CompileString(), as sketched below. (Declarations and error handling are omitted.)


io_module = Import_ImportModule("io");
fileobject = PyObject_CallMethod(io_module, "open", "ss", filename, "rb");
source_bytes_object = PyObject_CallMethod(fileobject, "read", "");
result = PyObject_CallMethod(fileobject, "close", "");
source_buf = PyBytes_AsString(source_bytes_object);
code = Py_CompileString(source_buf, filename, Py_file_input);



◦ For FrameObject objects, the f_lasti member now represents a wordcode offset instead of a simple offset into the bytecode string. This means that this number needs to be multiplied by 2 to be used with APIs that expect a byte offset instead (like PyCode_Addr2Line() for example). Notice as well that the f_lasti member of FrameObject objects is not considered stable: please use PyFrame_GetLineNumber() instead.



### ===🗝 CPython bytecode changes

• The MAKE_FUNCTION instruction now accepts either a dict or a tuple of strings as the function’s annotations. (Contributed by Yurii Karabas and Inada Naoki in bpo-42202)


### ===🗝 Build Changes

• PEP 644: Python now requires OpenSSL 1.1.1 or newer. OpenSSL 1.0.2 is no longer supported. (Contributed by Christian Heimes in bpo-43669.)


• The C99 functions snprintf() and vsnprintf() are now required to build Python. (Contributed by Victor Stinner in bpo-36020.)


• sqlite3 requires SQLite 3.7.15 or higher. (Contributed by Sergey Fedoseev and Erlend E. Aasland in bpo-40744 and bpo-40810.)


• The atexit module must now always be built as a built-in module. (Contributed by Victor Stinner in bpo-42639.)


• Add --disable-test-modules option to the configure script: don’t build nor install test modules. (Contributed by Xavier de Gaye, Thomas Petazzoni and Peixing Xin in bpo-27640.)


• Add --with-wheel-pkg-dir=PATH option to the ./configure script. If specified, the ensurepip module looks for setuptools and pip wheel packages in this directory: if both are present, these wheel packages are used instead of ensurepip bundled wheel packages.

Some Linux distribution packaging policies recommend against bundling dependencies. For example, Fedora installs wheel packages in the /usr/share/python-wheels/ directory and don’t install the `ensurepip._bundled` package.

(Contributed by Victor Stinner in bpo-42856.)


• Add a new configure --without-static-libpython option to not build the libpythonMAJOR.MINOR.a static library and not install the python.o object file.

(Contributed by Victor Stinner in bpo-43103.)


• The configure script now uses the pkg-config utility, if available, to detect the location of Tcl/Tk headers and libraries. As before, those locations can be explicitly specified with the --with-tcltk-includes and --with-tcltk-libs configuration options. (Contributed by Manolis Stamatogiannakis in bpo-42603.)


• Add --with-openssl-rpath option to configure script. The option simplifies building Python with a custom OpenSSL installation, e.g. ./configure --with-openssl=/path/to/openssl --with-openssl-rpath=auto. (Contributed by Christian Heimes in bpo-43466.)



### ===🗝 C API Changes


PEP 652: Maintaining the Stable ABI

The Stable ABI (Application Binary Interface) for extension modules or embedding Python is now explicitly defined. C API Stability describes C API and ABI stability guarantees along with best practices for using the Stable ABI.

(Contributed by Petr Viktorin in PEP 652 and bpo-43795.)


New Features

• The result of PyNumber_Index() now always has exact type int. Previously, the result could have been an instance of a subclass of int. (Contributed by Serhiy Storchaka in bpo-40792.)


• Add a new orig_argv member to the PyConfig structure: the list of the original command line arguments passed to the Python executable. (Contributed by Victor Stinner in bpo-23427.)


• The PyDateTime_DATE_GET_TZINFO() and PyDateTime_TIME_GET_TZINFO() macros have been added for accessing the tzinfo attributes of datetime.datetime and datetime.time objects. (Contributed by Zackery Spytz in bpo-30155.)


• Add a PyCodec_Unregister() function to unregister a codec search function. (Contributed by Hai Shi in bpo-41842.)


• The PyIter_Send() function was added to allow sending value into iterator without raising StopIteration exception. (Contributed by Vladimir Matveev in bpo-41756.)


• Add PyUnicode_AsUTF8AndSize() to the limited C API. (Contributed by Alex Gaynor in bpo-41784.)


• Add PyModule_AddObjectRef() function: similar to PyModule_AddObject() but don’t steal a reference to the value on success. (Contributed by Victor Stinner in bpo-1635741.)


• Add Py_NewRef() and Py_XNewRef() functions to increment the reference count of an object and return the object. (Contributed by Victor Stinner in bpo-42262.)


• The PyType_FromSpecWithBases() and PyType_FromModuleAndSpec() functions now accept a single class as the bases argument. (Contributed by Serhiy Storchaka in bpo-42423.)


• The PyType_FromModuleAndSpec() function now accepts NULL tp_doc slot. (Contributed by Hai Shi in bpo-41832.)


• The PyType_GetSlot() function can accept static types. (Contributed by Hai Shi and Petr Viktorin in bpo-41073.)


• Add a new PySet_CheckExact() function to the C-API to check if an object is an instance of set but not an instance of a subtype. (Contributed by Pablo Galindo in bpo-43277.)


• Add PyErr_SetInterruptEx() which allows passing a signal number to simulate. (Contributed by Antoine Pitrou in bpo-43356.)


• The limited C API is now supported if Python is built in debug mode (if the Py_DEBUG macro is defined). In the limited C API, the Py_INCREF() and Py_DECREF() functions are now implemented as opaque function calls, rather than accessing directly the PyObject.ob_refcnt member, if Python is built in debug mode and the Py_LIMITED_API macro targets Python 3.10 or newer. It became possible to support the limited C API in debug mode because the PyObject structure is the same in release and debug mode since Python 3.8 (see bpo-36465).

The limited C API is still not supported in the --with-trace-refs special build (Py_TRACE_REFS macro). (Contributed by Victor Stinner in bpo-43688.)


• Add the Py_Is(x, y) function to test if the x object is the y object, the same as x is y in Python. Add also the Py_IsNone(), Py_IsTrue(), Py_IsFalse() functions to test if an object is, respectively, the None singleton, the True singleton or the False singleton. (Contributed by Victor Stinner in bpo-43753.)


• Add new functions to control the garbage collector from C code: PyGC_Enable(), PyGC_Disable(), PyGC_IsEnabled(). These functions allow to activate, deactivate and query the state of the garbage collector from C code without having to import the gc module.


• Add a new Py_TPFLAGS_DISALLOW_INSTANTIATION type flag to disallow creating type instances. (Contributed by Victor Stinner in bpo-43916.)


• Add a new Py_TPFLAGS_IMMUTABLETYPE type flag for creating immutable type objects: type attributes cannot be set nor deleted. (Contributed by Victor Stinner and Erlend E. Aasland in bpo-43908.)



Porting to Python 3.10

• The PY_SSIZE_T_CLEAN macro must now be defined to use PyArg_ParseTuple() and Py_BuildValue() formats which use #: es#, et#, s#, u#, y#, z#, U# and Z#. See Parsing arguments and building values and the PEP 353. (Contributed by Victor Stinner in bpo-40943.)


• Since Py_REFCNT() is changed to the inline static function, Py_REFCNT(obj) = new_refcnt must be replaced with Py_SET_REFCNT(obj, new_refcnt): see Py_SET_REFCNT() (available since Python 3.9). For backward compatibility, this macro can be used:


```py
#if PY_VERSION_HEX < 0x030900A4
#  define Py_SET_REFCNT(obj, refcnt) ((Py_REFCNT(obj) = (refcnt)), (void)0)
#endif
```


(Contributed by Victor Stinner in bpo-39573.)


• Calling PyDict_GetItem() without GIL held had been allowed for historical reason. It is no longer allowed. (Contributed by Victor Stinner in bpo-40839.)


• PyUnicode_FromUnicode(NULL, size) and PyUnicode_FromStringAndSize(NULL, size) raise DeprecationWarning now. Use PyUnicode_New() to allocate Unicode object without initial data. (Contributed by Inada Naoki in bpo-36346.)


• The private `_PyUnicode_Name_CAPI` structure of the PyCapsule API unicodedata.ucnhash_CAPI has been moved to the internal C API. (Contributed by Victor Stinner in bpo-42157.)


• Py_GetPath(), Py_GetPrefix(), Py_GetExecPrefix(), Py_GetProgramFullPath(), Py_GetPythonHome() and Py_GetProgramName() functions now return NULL if called before Py_Initialize() (before Python is initialized). Use the new Python Initialization Configuration API to get the Python Path Configuration.. (Contributed by Victor Stinner in bpo-42260.)


• PyList_SET_ITEM(), PyTuple_SET_ITEM() and PyCell_SET() macros can no longer be used as l-value or r-value. For example, x = PyList_SET_ITEM(a, b, c) and PyList_SET_ITEM(a, b, c) = x now fail with a compiler error. It prevents bugs like if (PyList_SET_ITEM (a, b, c) < 0) ... test. (Contributed by Zackery Spytz and Victor Stinner in bpo-30459.)


• The non-limited API files odictobject.h, parser_interface.h, picklebufobject.h, pyarena.h, pyctype.h, pydebug.h, pyfpe.h, and pytime.h have been moved to the Include/cpython directory. These files must not be included directly, as they are already included in Python.h: Include Files. If they have been included directly, consider including Python.h instead. (Contributed by Nicholas Sim in bpo-35134)


• Use the Py_TPFLAGS_IMMUTABLETYPE type flag to create immutable type objects. Do not rely on Py_TPFLAGS_HEAPTYPE to decide if a type object is mutable or not; check if Py_TPFLAGS_IMMUTABLETYPE is set instead. (Contributed by Victor Stinner and Erlend E. Aasland in bpo-43908.)


• The undocumented function Py_FrozenMain has been removed from the limited API. The function is mainly useful for custom builds of Python. (Contributed by Petr Viktorin in bpo-26241)



Deprecated
• The PyUnicode_InternImmortal() function is now deprecated and will be removed in Python 3.12: use PyUnicode_InternInPlace() instead. (Contributed by Victor Stinner in bpo-41692.)


Removed

• Removed Py_UNICODE_str* functions manipulating Py_UNICODE* strings. (Contributed by Inada Naoki in bpo-41123.)


◦ Py_UNICODE_strlen: use PyUnicode_GetLength() or PyUnicode_GET_LENGTH
◦ Py_UNICODE_strcat: use PyUnicode_CopyCharacters() or PyUnicode_FromFormat()
◦ Py_UNICODE_strcpy, Py_UNICODE_strncpy: use PyUnicode_CopyCharacters() or PyUnicode_Substring()
◦ Py_UNICODE_strcmp: use PyUnicode_Compare()
◦ Py_UNICODE_strncmp: use PyUnicode_Tailmatch()
◦ Py_UNICODE_strchr, Py_UNICODE_strrchr: use PyUnicode_FindChar()


• Removed PyUnicode_GetMax(). Please migrate to new (PEP 393) APIs. (Contributed by Inada Naoki in bpo-41103.)


• Removed PyLong_FromUnicode(). Please migrate to PyLong_FromUnicodeObject(). (Contributed by Inada Naoki in bpo-41103.)


• Removed PyUnicode_AsUnicodeCopy(). Please use PyUnicode_AsUCS4Copy() or PyUnicode_AsWideCharString() (Contributed by Inada Naoki in bpo-41103.)


• Removed `_Py_CheckRecursionLimit` variable: it has been replaced by ceval.recursion_limit of the PyInterpreterState structure. (Contributed by Victor Stinner in bpo-41834.)


• Removed undocumented macros Py_ALLOW_RECURSION and Py_END_ALLOW_RECURSION and the recursion_critical field of the PyInterpreterState structure. (Contributed by Serhiy Storchaka in bpo-41936.)


• Removed the undocumented PyOS_InitInterrupts() function. Initializing Python already implicitly installs signal handlers: see PyConfig.install_signal_handlers. (Contributed by Victor Stinner in bpo-41713.)


• Remove the PyAST_Validate() function. It is no longer possible to build a AST object (mod_ty type) with the public C API. The function was already excluded from the limited C API (PEP 384). (Contributed by Victor Stinner in bpo-43244.)


• Remove the symtable.h header file and the undocumented functions:
◦ PyST_GetScope()
◦ PySymtable_Build()
◦ PySymtable_BuildObject()
◦ PySymtable_Free()
◦ Py_SymtableString()
◦ Py_SymtableStringObject()

The Py_SymtableString() function was part the stable ABI by mistake but it could not be used, because the symtable.h header file was excluded from the limited C API.

Use Python symtable module instead. (Contributed by Victor Stinner in bpo-43244.)


• Remove PyOS_ReadlineFunctionPointer() from the limited C API headers and from python3.dll, the library that provides the stable ABI on Windows. Since the function takes a FILE* argument, its ABI stability cannot be guaranteed. (Contributed by Petr Viktorin in bpo-43868.)


• Remove ast.h, asdl.h, and Python-ast.h header files. These functions were undocumented and excluded from the limited C API. Most names defined by these header files were not prefixed by Py and so could create names conflicts. For example, Python-ast.h defined a Yield macro which was conflict with the Yield name used by the Windows <winbase.h> header. Use the Python ast module instead. (Contributed by Victor Stinner in bpo-43244.)


• Remove the compiler and parser functions using struct `_mod` type, because the public AST C API was removed:
◦ PyAST_Compile()
◦ PyAST_CompileEx()
◦ PyAST_CompileObject()
◦ PyFuture_FromAST()
◦ PyFuture_FromASTObject()
◦ PyParser_ASTFromFile()
◦ PyParser_ASTFromFileObject()
◦ PyParser_ASTFromFilename()
◦ PyParser_ASTFromString()
◦ PyParser_ASTFromStringObject()

These functions were undocumented and excluded from the limited C API. (Contributed by Victor Stinner in bpo-43244.)


• Remove the pyarena.h header file with functions:
◦ PyArena_New()
◦ PyArena_Free()
◦ PyArena_Malloc()
◦ PyArena_AddPyObject()

These functions were undocumented, excluded from the limited C API, and were only used internally by the compiler. (Contributed by Victor Stinner in bpo-43244.)


• The PyThreadState.use_tracing member has been removed to optimize Python. (Contributed by Mark Shannon in bpo-43760.)


## ==⚡ • What’s New In Python 3.9
◦ Summary – Release highlights
◦ You should check for DeprecationWarning in your code
◦ New Features
◦ Other Language Changes
◦ New Modules
◦ Improved Modules
◦ Optimizations
◦ Deprecated
◦ Removed
◦ Porting to Python 3.9
◦ Build Changes
◦ C API Changes
◦ Notable changes in Python 3.9.1
◦ Notable changes in Python 3.9.2


### ===🗝 Summary – Release highlights

New syntax features:

• PEP 584, union operators added to dict;
• PEP 585, type hinting generics in standard collections;
• PEP 614, relaxed grammar restrictions on decorators.

New built-in features:

• PEP 616, string methods to remove prefixes and suffixes.

New features in the standard library:

• PEP 593, flexible function and variable annotations;
• os.pidfd_open() added that allows process management without races and signals.

Interpreter improvements:

• PEP 573, fast access to module state from methods of C extension types;
• PEP 617, CPython now uses a new parser based on PEG;
• a number of Python builtins (range, tuple, set, frozenset, list, dict) are now sped up using PEP 590 vectorcall;
• garbage collection does not block on resurrected objects;
• a number of Python modules (`_abc, audioop, _bz2, _codecs, _contextvars, _crypt, _functools, _json, _locale, math, operator, resource, time, _weakref`) now use multiphase initialization as defined by PEP 489;
• a number of standard library modules (`audioop, ast, grp, _hashlib, pwd, _posixsubprocess, random, select, struct, termios, zlib`) are now using the stable ABI defined by PEP 384.

New library modules:

• PEP 615, the IANA Time Zone Database is now present in the standard library in the zoneinfo module;
• an implementation of a topological sort of a graph is now provided in the new graphlib module.

Release process changes:

• PEP 602, CPython adopts an annual release cycle.


## ==⚡ • What’s New In Python 3.8
◦ Summary – Release highlights
◦ New Features
  Assignment expressions
  Positional-only parameters
  Parallel filesystem cache for compiled bytecode files
  Debug build uses the same ABI as release build
  f-strings support `=` for self-documenting expressions and debugging
  PEP 578: Python Runtime Audit Hooks
  PEP 587: Python Initialization Configuration
  PEP 590: Vectorcall: a fast calling protocol for CPython
  Pickle protocol 5 with out-of-band data buffers
◦ Other Language Changes
◦ New Modules
◦ Improved Modules
◦ Optimizations
◦ Build and C API Changes
◦ Deprecated
◦ API and Feature Removals
◦ Porting to Python 3.8
◦ Notable changes in Python 3.8.1
◦ Notable changes in Python 3.8.8
◦ Notable changes in Python 3.8.12

## ==⚡ • What’s New In Python 3.7
◦ Summary – Release Highlights
◦ New Features
◦ Other Language Changes
◦ New Modules
◦ Improved Modules
◦ C API Changes
◦ Build Changes
◦ Optimizations
◦ Other CPython Implementation Changes
◦ Deprecated Python Behavior
◦ Deprecated Python modules, functions and methods
◦ Deprecated functions and types of the C API
◦ Platform Support Removals
◦ API and Feature Removals
◦ Module Removals
◦ Windows-only Changes
◦ Porting to Python 3.7
◦ Notable changes in Python 3.7.1
◦ Notable changes in Python 3.7.2
◦ Notable changes in Python 3.7.6
◦ Notable changes in Python 3.7.10

### ===🗝 Summary – Release Highlights

New syntax features:
• PEP 563, postponed evaluation of type annotations.

Backwards incompatible syntax changes:
• async and await are now reserved keywords.

New library modules:
• contextvars: PEP 567 – Context Variables
• dataclasses: PEP 557 – Data Classes
• importlib.resources

New built-in features:
• PEP 553, the new breakpoint() function.

Python data model improvements:
• PEP 562, customization of access to module attributes.
• PEP 560, core support for typing module and generic types.
• the insertion-order preservation nature of dict objects has been declared to be an official part of the Python language spec.

Significant improvements in the standard library:
• The asyncio module has received new features, significant usability and performance improvements.
• The time module gained support for functions with nanosecond resolution.

CPython implementation improvements:
• Avoiding the use of ASCII as a default text encoding:◦PEP 538, legacy C locale coercion
◦PEP 540, forced UTF-8 runtime mode

• PEP 552, deterministic .pycs
• New Python Development Mode
• PEP 565, improved DeprecationWarning handling

C API improvements:
• PEP 539, new C API for thread-local storage

Documentation improvements:
• PEP 545, Python documentation translations
• New documentation translations: Japanese, French, and Korean.

This release features notable performance improvements in many areas. The Optimizations section lists them in detail.

For a list of changes that may affect compatibility with previous Python releases please refer to the Porting to Python 3.7 section.


### ===🗝 New Features


#### PEP 563: Postponed Evaluation of Annotations

The advent of type hints in Python uncovered two glaring usability issues with the functionality of annotations added in PEP 3107 and refined further in PEP 526:

• annotations could only use names which were already available in the current scope, in other words they didn’t support forward references of any kind; and
• annotating source code had adverse effects on startup time of Python programs.

Both of these issues are fixed by postponing the evaluation of annotations. Instead of compiling code which executes expressions in annotations at their definition time, the compiler stores the annotation in a string form equivalent to the AST of the expression in question. If needed, annotations can be resolved at runtime using `typing.get_type_hints()`. In the common case where this is not required, the annotations are cheaper to store (since short strings are interned by the interpreter) and make startup time faster.

Usability-wise, annotations now support forward references, making the following syntax valid:


```py
class C:
    @classmethod
    def from_string(cls, source: str) -> C:
        ...

    def validate_b(self, obj: B) -> bool:
        ...

class B:
    ...
```


Since this change breaks compatibility, the new behavior needs to be enabled on a per-module basis in Python 3.7 using a __future__ import:


from __future__ import annotations


It will become the default in Python 3.10.

See also:
 PEP 563 – Postponed evaluation of annotationsPEP written and implemented by Łukasz Langa.

#### PEP 538: Legacy C Locale Coercion

An ongoing challenge within the Python 3 series has been determining a sensible default strategy for handling the “7-bit ASCII” text encoding assumption currently implied by the use of the default C or POSIX locale on non-Windows platforms.

PEP 538 updates the default interpreter command line interface to automatically coerce that locale to an available UTF-8 based locale as described in the documentation of the new PYTHONCOERCECLOCALE environment variable. Automatically setting LC_CTYPE this way means that both the core interpreter and locale-aware C extensions (such as readline) will assume the use of UTF-8 as the default text encoding, rather than ASCII.

The platform support definition in PEP 11 has also been updated to limit full text handling support to suitably configured non-ASCII based locales.

As part of this change, the default error handler for stdin and stdout is now surrogateescape (rather than strict) when using any of the defined coercion target locales (currently C.UTF-8, C.utf8, and UTF-8). The default error handler for stderr continues to be backslashreplace, regardless of locale.

Locale coercion is silent by default, but to assist in debugging potentially locale related integration problems, explicit warnings (emitted directly on stderr) can be requested by setting PYTHONCOERCECLOCALE=warn. This setting will also cause the Python runtime to emit a warning if the legacy C locale remains active when the core interpreter is initialized.

While PEP 538’s locale coercion has the benefit of also affecting extension modules (such as GNU readline), as well as child processes (including those running non-Python applications and older versions of Python), it has the downside of requiring that a suitable target locale be present on the running system. To better handle the case where no suitable target locale is available (as occurs on RHEL/CentOS 7, for example), Python 3.7 also implements PEP 540: Forced UTF-8 Runtime Mode.

See also:
 PEP 538 – Coercing the legacy C locale to a UTF-8 based localePEP written and implemented by Nick Coghlan.

#### PEP 540: Forced UTF-8 Runtime Mode

The new -X utf8 command line option and PYTHONUTF8 environment variable can be used to enable the Python UTF-8 Mode.

When in UTF-8 mode, CPython ignores the locale settings, and uses the UTF-8 encoding by default. The error handlers for sys.stdin and sys.stdout streams are set to surrogateescape.

The forced UTF-8 mode can be used to change the text handling behavior in an embedded Python interpreter without changing the locale settings of an embedding application.

While PEP 540’s UTF-8 mode has the benefit of working regardless of which locales are available on the running system, it has the downside of having no effect on extension modules (such as GNU readline), child processes running non-Python applications, and child processes running older versions of Python. To reduce the risk of corrupting text data when communicating with such components, Python 3.7 also implements PEP 540: Forced UTF-8 Runtime Mode).

The UTF-8 mode is enabled by default when the locale is C or POSIX, and the PEP 538 locale coercion feature fails to change it to a UTF-8 based alternative (whether that failure is due to PYTHONCOERCECLOCALE=0 being set, LC_ALL being set, or the lack of a suitable target locale).

See also:
 PEP 540 – Add a new UTF-8 modePEP written and implemented by Victor Stinner

#### PEP 553: Built-in breakpoint()

Python 3.7 includes the new built-in `breakpoint()` function as an easy and consistent way to enter the Python debugger.

Built-in `breakpoint()` calls `sys.breakpointhook()`. By default, the latter imports pdb and then calls pdb.set_trace(), but by binding `sys.breakpointhook()` to the function of your choosing, `breakpoint()` can enter any debugger. Additionally, the environment variable `PYTHONBREAKPOINT` can be set to the callable of your debugger of choice. Set PYTHONBREAKPOINT=0 to completely disable built-in `breakpoint()`.

See also:
 PEP 553 – Built-in breakpoint()PEP written and implemented by Barry Warsaw

#### PEP 539: New C API for Thread-Local Storage

While Python provides a C API for thread-local storage support; the existing Thread Local Storage (TLS) API has used int to represent TLS keys across all platforms. This has not generally been a problem for officially-support platforms, but that is neither POSIX-compliant, nor portable in any practical sense.

PEP 539 changes this by providing a new Thread Specific Storage (TSS) API to CPython which supersedes use of the existing TLS API within the CPython interpreter, while deprecating the existing API. The TSS API uses a new type Py_tss_t instead of int to represent TSS keys–an opaque type the definition of which may depend on the underlying TLS implementation. Therefore, this will allow to build CPython on platforms where the native TLS key is defined in a way that cannot be safely cast to int.

Note that on platforms where the native TLS key is defined in a way that cannot be safely cast to int, all functions of the existing TLS API will be no-op and immediately return failure. This indicates clearly that the old API is not supported on platforms where it cannot be used reliably, and that no effort will be made to add such support.

See also:
 PEP 539 – A New C-API for Thread-Local Storage in CPythonPEP written by Erik M. Bray; implementation by Masayuki Yamamoto.

#### PEP 562: Customization of Access to Module Attributes

Python 3.7 allows defining __getattr__() on modules and will call it whenever a module attribute is otherwise not found. Defining __dir__() on modules is now also allowed.

A typical example of where this may be useful is module attribute deprecation and lazy loading.

See also:
 PEP 562 – Module `__getattr__` and `__dir__PEP` written and implemented by Ivan Levkivskyi

#### PEP 564: New Time Functions With Nanosecond Resolution

The resolution of clocks in modern systems can exceed the limited precision of a floating point number returned by the `time.time()` function and its variants. To avoid loss of precision, PEP 564 adds six new “nanosecond” variants of the existing timer functions to the time module:

• time.clock_gettime_ns()
• time.clock_settime_ns()
• time.monotonic_ns()
• time.perf_counter_ns()
• time.process_time_ns()
• time.time_ns()

The new functions return the number of nanoseconds as an integer value.

Measurements show that on Linux and Windows the resolution of `time.time_ns()` is approximately 3 times better than that of `time.time()`.

See also:
 PEP 564 – Add new time functions with nanosecond resolutionPEP written and implemented by Victor Stinner

#### PEP 565: Show DeprecationWarning in __main__

The default handling of DeprecationWarning has been changed such that these warnings are once more shown by default, but only when the code triggering them is running directly in the __main__ module. As a result, developers of single file scripts and those using Python interactively should once again start seeing deprecation warnings for the APIs they use, but deprecation warnings triggered by imported application, library and framework modules will continue to be hidden by default.

As a result of this change, the standard library now allows developers to choose between three different deprecation warning behaviours:

• `FutureWarning`: always displayed by default, recommended for warnings intended to be seen by application end users (e.g. for deprecated application configuration settings).

• `DeprecationWarning`: displayed by default only in __main__ and when running tests, recommended for warnings intended to be seen by other Python developers where a version upgrade may result in changed behaviour or an error.

• `PendingDeprecationWarning`: displayed by default only when running tests, intended for cases where a future version upgrade will change the warning category to DeprecationWarning or FutureWarning.


Previously both DeprecationWarning and PendingDeprecationWarning were only visible when running tests, which meant that developers primarily writing single file scripts or using Python interactively could be surprised by breaking changes in the APIs they used.

See also:
 PEP 565 – Show DeprecationWarning in `__main__PEP` written and implemented by Nick Coghlan

#### PEP 560: Core Support for typing module and Generic Types

Initially PEP 484 was designed in such way that it would not introduce any changes to the core CPython interpreter. Now type hints and the typing module are extensively used by the community, so this restriction is removed. The PEP introduces two special methods __class_getitem__() and __mro_entries__, these methods are now used by most classes and special constructs in typing. As a result, the speed of various operations with types increased up to 7 times, the generic types can be used without metaclass conflicts, and several long standing bugs in typing module are fixed.

See also:
 PEP 560 – Core support for typing module and generic typesPEP written and implemented by Ivan Levkivskyi

#### PEP 552: Hash-based .pyc Files

Python has traditionally checked the up-to-dateness of bytecode cache files (i.e., .pyc files) by comparing the source metadata (last-modified timestamp and size) with source metadata saved in the cache file header when it was generated. While effective, this invalidation method has its drawbacks. When filesystem timestamps are too coarse, Python can miss source updates, leading to user confusion. Additionally, having a timestamp in the cache file is problematic for build reproducibility and content-based build systems.

PEP 552 extends the pyc format to allow the hash of the source file to be used for invalidation instead of the source timestamp. Such .pyc files are called “hash-based”. By default, Python still uses timestamp-based invalidation and does not generate hash-based .pyc files at runtime. Hash-based .pyc files may be generated with py_compile or compileall.

Hash-based .pyc files come in two variants: checked and unchecked. Python validates checked hash-based .pyc files against the corresponding source files at runtime but doesn’t do so for unchecked hash-based pycs. Unchecked hash-based .pyc files are a useful performance optimization for environments where a system external to Python (e.g., the build system) is responsible for keeping .pyc files up-to-date.

See Cached bytecode invalidation for more information.

See also:
 PEP 552 – Deterministic pycsPEP written and implemented by Benjamin Peterson

#### PEP 545: Python Documentation Translations

PEP 545 describes the process of creating and maintaining Python documentation translations.

Three new translations have been added:
• Japanese: https://docs.python.org/ja/
• French: https://docs.python.org/fr/
• Korean: https://docs.python.org/ko/

See also:
 PEP 545 – Python Documentation TranslationsPEP written and implemented by Julien Palard, Inada Naoki, and Victor Stinner.

#### Python Development Mode (-X dev)

The new -X dev command line option or the new PYTHONDEVMODE environment variable can be used to enable Python Development Mode. When in development mode, Python performs additional runtime checks that are too expensive to be enabled by default. See Python Development Mode documentation for the full description.


### ===🗝 Other Language Changes

• An await expression and comprehensions containing an async for clause were illegal in the expressions in formatted string literals due to a problem with the implementation. In Python 3.7 this restriction was lifted.

• More than 255 arguments can now be passed to a function, and a function can now have more than 255 parameters. (Contributed by Serhiy Storchaka in bpo-12844 and bpo-18896.)

• bytes.fromhex() and bytearray.fromhex() now ignore all ASCII whitespace, not only spaces. (Contributed by Robert Xiao in bpo-28927.)

• str, bytes, and bytearray gained support for the new isascii() method, which can be used to test if a string or bytes contain only the ASCII characters. (Contributed by INADA Naoki in bpo-32677.)

• ImportError now displays module name and module __file__ path when from ... import ... fails. (Contributed by Matthias Bussonnier in bpo-29546.)

• Circular imports involving absolute imports with binding a submodule to a name are now supported. (Contributed by Serhiy Storchaka in bpo-30024.)

• object.__format__(x, '') is now equivalent to str(x) rather than format(str(self), ''). (Contributed by Serhiy Storchaka in bpo-28974.)

• In order to better support dynamic creation of stack traces, types.TracebackType can now be instantiated from Python code, and the tb_next attribute on tracebacks is now writable. (Contributed by Nathaniel J. Smith in bpo-30579.)

• When using the -m switch, sys.path[0] is now eagerly expanded to the full starting directory path, rather than being left as the empty directory (which allows imports from the current working directory at the time when an import occurs) (Contributed by Nick Coghlan in bpo-33053.)

• The new -X importtime option or the PYTHONPROFILEIMPORTTIME environment variable can be used to show the timing of each module import. (Contributed by Victor Stinner in bpo-31415.)


### ===🗝 New Modules


#### contextvars

The new contextvars module and a set of new C APIs introduce support for context variables. Context variables are conceptually similar to thread-local variables. Unlike TLS, context variables support asynchronous code correctly.

The asyncio and decimal modules have been updated to use and support context variables out of the box. Particularly the active decimal context is now stored in a context variable, which allows decimal operations to work with the correct context in asynchronous code.

See also:
 PEP 567 – Context VariablesPEP written and implemented by Yury Selivanov

#### dataclasses

The new dataclass() decorator provides a way to declare data classes. A data class describes its attributes using class variable annotations. Its constructor and other magic methods, such as __repr__(), __eq__(), and __hash__() are generated automatically.

Example:


```py
@dataclass
class Point:
    x: float
    y: float
    z: float = 0.0

p = Point(1.5, 2.5)
print(p)   # produces "Point(x=1.5, y=2.5, z=0.0)"
```


See also:
 PEP 557 – Data ClassesPEP written and implemented by Eric V. Smith

#### importlib.resources

The new importlib.resources module provides several new APIs and one new ABC for access to, opening, and reading resources inside packages. Resources are roughly similar to files inside packages, but they needn’t be actual files on the physical file system. Module loaders can provide a get_resource_reader() function which returns a importlib.abc.ResourceReader instance to support this new API. Built-in file path loaders and zip file loaders both support this.

Contributed by Barry Warsaw and Brett Cannon in bpo-32248.

See also:
 importlib_resources – a PyPI backport for earlier Python versions.
 


### ===🗝 Improved Modules


argparse

The new ArgumentParser.parse_intermixed_args() method allows intermixing options and positional arguments. (Contributed by paul.j3 in bpo-14191.)


asyncio

The asyncio module has received many new features, usability and performance improvements. Notable changes include:

• The new provisional asyncio.run() function can be used to run a coroutine from synchronous code by automatically creating and destroying the event loop. (Contributed by Yury Selivanov in bpo-32314.)


• asyncio gained support for contextvars. loop.call_soon(), loop.call_soon_threadsafe(), loop.call_later(), loop.call_at(), and Future.add_done_callback() have a new optional keyword-only context parameter. Tasks now track their context automatically. See PEP 567 for more details. (Contributed by Yury Selivanov in bpo-32436.)


• The new asyncio.create_task() function has been added as a shortcut to asyncio.get_event_loop().create_task(). (Contributed by Andrew Svetlov in bpo-32311.)


• The new loop.start_tls() method can be used to upgrade an existing connection to TLS. (Contributed by Yury Selivanov in bpo-23749.)


• The new loop.sock_recv_into() method allows reading data from a socket directly into a provided buffer making it possible to reduce data copies. (Contributed by Antoine Pitrou in bpo-31819.)


• The new asyncio.current_task() function returns the currently running Task instance, and the new asyncio.all_tasks() function returns a set of all existing Task instances in a given loop. The Task.current_task() and Task.all_tasks() methods have been deprecated. (Contributed by Andrew Svetlov in bpo-32250.)


• The new provisional BufferedProtocol class allows implementing streaming protocols with manual control over the receive buffer. (Contributed by Yury Selivanov in bpo-32251.)


• The new asyncio.get_running_loop() function returns the currently running loop, and raises a RuntimeError if no loop is running. This is in contrast with asyncio.get_event_loop(), which will create a new event loop if none is running. (Contributed by Yury Selivanov in bpo-32269.)


• The new StreamWriter.wait_closed() coroutine method allows waiting until the stream writer is closed. The new StreamWriter.is_closing() method can be used to determine if the writer is closing. (Contributed by Andrew Svetlov in bpo-32391.)


• The new loop.sock_sendfile() coroutine method allows sending files using os.sendfile when possible. (Contributed by Andrew Svetlov in bpo-32410.)


• The new Future.get_loop() and Task.get_loop() methods return the instance of the loop on which a task or a future were created. Server.get_loop() allows doing the same for asyncio.Server objects. (Contributed by Yury Selivanov in bpo-32415 and Srinivas Reddy Thatiparthy in bpo-32418.)


• It is now possible to control how instances of asyncio.Server begin serving. Previously, the server would start serving immediately when created. The new start_serving keyword argument to loop.create_server() and loop.create_unix_server(), as well as Server.start_serving(), and Server.serve_forever() can be used to decouple server instantiation and serving. The new Server.is_serving() method returns True if the server is serving. Server objects are now asynchronous context managers:


```py
srv = await loop.create_server(...)

async with srv:
    # some code

# At this point, srv is closed and no longer accepts new connections.
```


(Contributed by Yury Selivanov in bpo-32662.)


• Callback objects returned by loop.call_later() gained the new when() method which returns an absolute scheduled callback timestamp. (Contributed by Andrew Svetlov in bpo-32741.)


• The loop.create_datagram_endpoint()  method gained support for Unix sockets. (Contributed by Quentin Dawans in bpo-31245.)


• The asyncio.open_connection(), asyncio.start_server() functions, loop.create_connection(), loop.create_server(), loop.create_accepted_socket() methods and their corresponding UNIX socket variants now accept the ssl_handshake_timeout keyword argument. (Contributed by Neil Aspinall in bpo-29970.)


• The new Handle.cancelled() method returns True if the callback was cancelled. (Contributed by Marat Sharafutdinov in bpo-31943.)


• The asyncio source has been converted to use the async/await syntax. (Contributed by Andrew Svetlov in bpo-32193.)


• The new ReadTransport.is_reading() method can be used to determine the reading state of the transport. Additionally, calls to ReadTransport.resume_reading() and ReadTransport.pause_reading() are now idempotent. (Contributed by Yury Selivanov in bpo-32356.)


• Loop methods which accept socket paths now support passing path-like objects. (Contributed by Yury Selivanov in bpo-32066.)


• In asyncio TCP sockets on Linux are now created with TCP_NODELAY flag set by default. (Contributed by Yury Selivanov and Victor Stinner in bpo-27456.)


• Exceptions occurring in cancelled tasks are no longer logged. (Contributed by Yury Selivanov in bpo-30508.)


• New WindowsSelectorEventLoopPolicy and WindowsProactorEventLoopPolicy classes. (Contributed by Yury Selivanov in bpo-33792.)


Several asyncio APIs have been deprecated.


binascii

The b2a_uu() function now accepts an optional backtick keyword argument. When it’s true, zeros are represented by '`' instead of spaces. (Contributed by Xiang Zhang in bpo-30103.)


calendar

The HTMLCalendar class has new class attributes which ease the customization of CSS classes in the produced HTML calendar. (Contributed by Oz Tiram in bpo-30095.)


collections

collections.namedtuple() now supports default values. (Contributed by Raymond Hettinger in bpo-32320.)


compileall

compileall.compile_dir() learned the new invalidation_mode parameter, which can be used to enable hash-based .pyc invalidation. The invalidation mode can also be specified on the command line using the new --invalidation-mode argument. (Contributed by Benjamin Peterson in bpo-31650.)


concurrent.futures

ProcessPoolExecutor and ThreadPoolExecutor now support the new initializer and initargs constructor arguments. (Contributed by Antoine Pitrou in bpo-21423.)

The ProcessPoolExecutor can now take the multiprocessing context via the new mp_context argument. (Contributed by Thomas Moreau in bpo-31540.)


contextlib

The new nullcontext() is a simpler and faster no-op context manager than ExitStack. (Contributed by Jesse-Bakker in bpo-10049.)

The new asynccontextmanager(), AbstractAsyncContextManager, and AsyncExitStack have been added to complement their synchronous counterparts. (Contributed by Jelle Zijlstra in bpo-29679 and bpo-30241, and by Alexander Mohr and Ilya Kulakov in bpo-29302.)


cProfile

The cProfile command line now accepts -m module_name as an alternative to script path. (Contributed by Sanyam Khurana in bpo-21862.)


crypt

The crypt module now supports the Blowfish hashing method. (Contributed by Serhiy Storchaka in bpo-31664.)

The mksalt() function now allows specifying the number of rounds for hashing. (Contributed by Serhiy Storchaka in bpo-31702.)


datetime

The new datetime.fromisoformat() method constructs a datetime object from a string in one of the formats output by datetime.isoformat(). (Contributed by Paul Ganssle in bpo-15873.)

The tzinfo class now supports sub-minute offsets. (Contributed by Alexander Belopolsky in bpo-5288.)


dbm

dbm.dumb now supports reading read-only files and no longer writes the index file when it is not changed.


decimal

The decimal module now uses context variables to store the decimal context. (Contributed by Yury Selivanov in bpo-32630.)


dis

The dis() function is now able to disassemble nested code objects (the code of comprehensions, generator expressions and nested functions, and the code used for building nested classes). The maximum depth of disassembly recursion is controlled by the new depth parameter. (Contributed by Serhiy Storchaka in bpo-11822.)


distutils

README.rst is now included in the list of distutils standard READMEs and therefore included in source distributions. (Contributed by Ryan Gonzalez in bpo-11913.)


enum

The Enum learned the new _ignore_ class property, which allows listing the names of properties which should not become enum members. (Contributed by Ethan Furman in bpo-31801.)

In Python 3.8, attempting to check for non-Enum objects in Enum classes will raise a TypeError (e.g. 1 in Color); similarly, attempting to check for non-Flag objects in a Flag member will raise TypeError (e.g. 1 in Perm.RW); currently, both operations return False instead and are deprecated. (Contributed by Ethan Furman in bpo-33217.)


functools

functools.singledispatch() now supports registering implementations using type annotations. (Contributed by Łukasz Langa in bpo-32227.)


gc

The new gc.freeze() function allows freezing all objects tracked by the garbage collector and excluding them from future collections. This can be used before a POSIX fork() call to make the GC copy-on-write friendly or to speed up collection. The new gc.unfreeze() functions reverses this operation. Additionally, gc.get_freeze_count() can be used to obtain the number of frozen objects. (Contributed by Li Zekun in bpo-31558.)


hmac

The hmac module now has an optimized one-shot digest() function, which is up to three times faster than HMAC(). (Contributed by Christian Heimes in bpo-32433.)


http.client

HTTPConnection and HTTPSConnection now support the new blocksize argument for improved upload throughput. (Contributed by Nir Soffer in bpo-31945.)


http.server

SimpleHTTPRequestHandler now supports the HTTP If-Modified-Since header. The server returns the 304 response status if the target file was not modified after the time specified in the header. (Contributed by Pierre Quentel in bpo-29654.)

SimpleHTTPRequestHandler accepts the new directory argument, in addition to the new --directory command line argument. With this parameter, the server serves the specified directory, by default it uses the current working directory. (Contributed by Stéphane Wirtel and Julien Palard in bpo-28707.)

The new ThreadingHTTPServer class uses threads to handle requests using ThreadingMixin. It is used when http.server is run with -m. (Contributed by Julien Palard in bpo-31639.)


idlelib and IDLE

Multiple fixes for autocompletion. (Contributed by Louie Lu in bpo-15786.)

Module Browser (on the File menu, formerly called Class Browser), now displays nested functions and classes in addition to top-level functions and classes. (Contributed by Guilherme Polo, Cheryl Sabella, and Terry Jan Reedy in bpo-1612262.)

The Settings dialog (Options, Configure IDLE) has been partly rewritten to improve both appearance and function. (Contributed by Cheryl Sabella and Terry Jan Reedy in multiple issues.)

The font sample now includes a selection of non-Latin characters so that users can better see the effect of selecting a particular font. (Contributed by Terry Jan Reedy in bpo-13802.) The sample can be edited to include other characters. (Contributed by Serhiy Storchaka in bpo-31860.)

The IDLE features formerly implemented as extensions have been reimplemented as normal features. Their settings have been moved from the Extensions tab to other dialog tabs. (Contributed by Charles Wohlganger and Terry Jan Reedy in bpo-27099.)

Editor code context option revised. Box displays all context lines up to maxlines. Clicking on a context line jumps the editor to that line. Context colors for custom themes is added to Highlights tab of Settings dialog. (Contributed by Cheryl Sabella and Terry Jan Reedy in bpo-33642, bpo-33768, and bpo-33679.)

On Windows, a new API call tells Windows that tk scales for DPI. On Windows 8.1+ or 10, with DPI compatibility properties of the Python binary unchanged, and a monitor resolution greater than 96 DPI, this should make text and lines sharper. It should otherwise have no effect. (Contributed by Terry Jan Reedy in bpo-33656.)

New in 3.7.1:

Output over N lines (50 by default) is squeezed down to a button. N can be changed in the PyShell section of the General page of the Settings dialog. Fewer, but possibly extra long, lines can be squeezed by right clicking on the output. Squeezed output can be expanded in place by double-clicking the button or into the clipboard or a separate window by right-clicking the button. (Contributed by Tal Einat in bpo-1529353.)

The changes above have been backported to 3.6 maintenance releases.

NEW in 3.7.4:

Add “Run Customized” to the Run menu to run a module with customized settings. Any command line arguments entered are added to sys.argv. They re-appear in the box for the next customized run. One can also suppress the normal Shell main module restart. (Contributed by Cheryl Sabella, Terry Jan Reedy, and others in bpo-5680 and bpo-37627.)

New in 3.7.5:

Add optional line numbers for IDLE editor windows. Windows open without line numbers unless set otherwise in the General tab of the configuration dialog. Line numbers for an existing window are shown and hidden in the Options menu. (Contributed by Tal Einat and Saimadhav Heblikar in bpo-17535.)


importlib

The importlib.abc.ResourceReader ABC was introduced to support the loading of resources from packages. See also importlib.resources. (Contributed by Barry Warsaw, Brett Cannon in bpo-32248.)

importlib.reload() now raises ModuleNotFoundError if the module lacks a spec. (Contributed by Garvit Khatri in bpo-29851.)

importlib.find_spec() now raises ModuleNotFoundError instead of AttributeError if the specified parent module is not a package (i.e. lacks a __path__ attribute). (Contributed by Milan Oberkirch in bpo-30436.)

The new importlib.source_hash() can be used to compute the hash of the passed source. A hash-based .pyc file embeds the value returned by this function.


io

The new TextIOWrapper.reconfigure() method can be used to reconfigure the text stream with the new settings. (Contributed by Antoine Pitrou in bpo-30526 and INADA Naoki in bpo-15216.)


ipaddress

The new subnet_of() and supernet_of() methods of ipaddress.IPv6Network and ipaddress.IPv4Network can be used for network containment tests. (Contributed by Michel Albert and Cheryl Sabella in bpo-20825.)


itertools

itertools.islice() now accepts integer-like objects as start, stop, and slice arguments. (Contributed by Will Roberts in bpo-30537.)


locale

The new monetary argument to locale.format_string() can be used to make the conversion use monetary thousands separators and grouping strings. (Contributed by Garvit in bpo-10379.)

The locale.getpreferredencoding() function now always returns 'UTF-8' on Android or when in the forced UTF-8 mode.


logging

Logger instances can now be pickled. (Contributed by Vinay Sajip in bpo-30520.)

The new StreamHandler.setStream() method can be used to replace the logger stream after handler creation. (Contributed by Vinay Sajip in bpo-30522.)

It is now possible to specify keyword arguments to handler constructors in configuration passed to logging.config.fileConfig(). (Contributed by Preston Landers in bpo-31080.)


math

The new math.remainder() function implements the IEEE 754-style remainder operation. (Contributed by Mark Dickinson in bpo-29962.)


mimetypes

The MIME type of .bmp has been changed from 'image/x-ms-bmp' to 'image/bmp'. (Contributed by Nitish Chandra in bpo-22589.)


msilib

The new Database.Close() method can be used to close the MSI database. (Contributed by Berker Peksag in bpo-20486.)


multiprocessing

The new Process.close() method explicitly closes the process object and releases all resources associated with it. ValueError is raised if the underlying process is still running. (Contributed by Antoine Pitrou in bpo-30596.)

The new Process.kill() method can be used to terminate the process using the SIGKILL signal on Unix. (Contributed by Vitor Pereira in bpo-30794.)

Non-daemonic threads created by Process are now joined on process exit. (Contributed by Antoine Pitrou in bpo-18966.)


os

os.fwalk() now accepts the path argument as bytes. (Contributed by Serhiy Storchaka in bpo-28682.)

os.scandir() gained support for file descriptors. (Contributed by Serhiy Storchaka in bpo-25996.)

The new register_at_fork() function allows registering Python callbacks to be executed at process fork. (Contributed by Antoine Pitrou in bpo-16500.)

Added os.preadv() (combine the functionality of os.readv() and os.pread()) and os.pwritev() functions (combine the functionality of os.writev() and os.pwrite()). (Contributed by Pablo Galindo in bpo-31368.)

The mode argument of os.makedirs() no longer affects the file permission bits of newly-created intermediate-level directories. (Contributed by Serhiy Storchaka in bpo-19930.)

os.dup2() now returns the new file descriptor. Previously, None was always returned. (Contributed by Benjamin Peterson in bpo-32441.)

The structure returned by os.stat() now contains the st_fstype attribute on Solaris and its derivatives. (Contributed by Jesús Cea Avión in bpo-32659.)


pathlib

The new Path.is_mount() method is now available on POSIX systems and can be used to determine whether a path is a mount point. (Contributed by Cooper Ry Lees in bpo-30897.)


pdb

pdb.set_trace() now takes an optional header keyword-only argument. If given, it is printed to the console just before debugging begins. (Contributed by Barry Warsaw in bpo-31389.)

pdb command line now accepts -m module_name as an alternative to script file. (Contributed by Mario Corchero in bpo-32206.)


py_compile

py_compile.compile() – and by extension, compileall – now respects the SOURCE_DATE_EPOCH environment variable by unconditionally creating .pyc files for hash-based validation. This allows for guaranteeing reproducible builds of .pyc files when they are created eagerly. (Contributed by Bernhard M. Wiedemann in bpo-29708.)


pydoc

The pydoc server can now bind to an arbitrary hostname specified by the new -n command-line argument. (Contributed by Feanil Patel in bpo-31128.)


queue

The new SimpleQueue class is an unbounded FIFO queue. (Contributed by Antoine Pitrou in bpo-14976.)


re

The flags re.ASCII, re.LOCALE and re.UNICODE can be set within the scope of a group. (Contributed by Serhiy Storchaka in bpo-31690.)

re.split() now supports splitting on a pattern like r'\b', '^$' or (?=-) that matches an empty string. (Contributed by Serhiy Storchaka in bpo-25054.)

Regular expressions compiled with the re.LOCALE flag no longer depend on the locale at compile time. Locale settings are applied only when the compiled regular expression is used. (Contributed by Serhiy Storchaka in bpo-30215.)

FutureWarning is now emitted if a regular expression contains character set constructs that will change semantically in the future, such as nested sets and set operations. (Contributed by Serhiy Storchaka in bpo-30349.)

Compiled regular expression and match objects can now be copied using copy.copy() and copy.deepcopy(). (Contributed by Serhiy Storchaka in bpo-10076.)


signal

The new warn_on_full_buffer argument to the signal.set_wakeup_fd() function makes it possible to specify whether Python prints a warning on stderr when the wakeup buffer overflows. (Contributed by Nathaniel J. Smith in bpo-30050.)


socket

The new socket.getblocking() method returns True if the socket is in blocking mode and False otherwise. (Contributed by Yury Selivanov in bpo-32373.)

The new socket.close() function closes the passed socket file descriptor. This function should be used instead of os.close() for better compatibility across platforms. (Contributed by Christian Heimes in bpo-32454.)

The socket module now exposes the socket.TCP_CONGESTION (Linux 2.6.13), socket.TCP_USER_TIMEOUT (Linux 2.6.37), and socket.TCP_NOTSENT_LOWAT (Linux 3.12) constants. (Contributed by Omar Sandoval in bpo-26273 and Nathaniel J. Smith in bpo-29728.)

Support for socket.AF_VSOCK sockets has been added to allow communication between virtual machines and their hosts. (Contributed by Cathy Avery in bpo-27584.)

Sockets now auto-detect family, type and protocol from file descriptor by default. (Contributed by Christian Heimes in bpo-28134.)


socketserver

socketserver.ThreadingMixIn.server_close() now waits until all non-daemon threads complete. socketserver.ForkingMixIn.server_close() now waits until all child processes complete.

Add a new socketserver.ForkingMixIn.block_on_close class attribute to socketserver.ForkingMixIn and socketserver.ThreadingMixIn classes. Set the class attribute to False to get the pre-3.7 behaviour.


sqlite3

sqlite3.Connection now exposes the backup() method when the underlying SQLite library is at version 3.6.11 or higher. (Contributed by Lele Gaifax in bpo-27645.)

The database argument of sqlite3.connect() now accepts any path-like object, instead of just a string. (Contributed by Anders Lorentsen in bpo-31843.)


ssl

The ssl module now uses OpenSSL’s builtin API instead of match_hostname() to check a host name or an IP address. Values are validated during TLS handshake. Any certificate validation error including failing the host name check now raises SSLCertVerificationError and aborts the handshake with a proper TLS Alert message. The new exception contains additional information. Host name validation can be customized with SSLContext.hostname_checks_common_name. (Contributed by Christian Heimes in bpo-31399.)

Note:
 The improved host name check requires a libssl implementation compatible with OpenSSL 1.0.2 or 1.1. Consequently, OpenSSL 0.9.8 and 1.0.1 are no longer supported (see Platform Support Removals for more details). The ssl module is mostly compatible with LibreSSL 2.7.2 and newer.
 

The ssl module no longer sends IP addresses in SNI TLS extension. (Contributed by Christian Heimes in bpo-32185.)

match_hostname() no longer supports partial wildcards like `www*.example.org`. (Contributed by Mandeep Singh in bpo-23033 and Christian Heimes in bpo-31399.)

The default cipher suite selection of the ssl module now uses a blacklist approach rather than a hard-coded whitelist. Python no longer re-enables ciphers that have been blocked by OpenSSL security updates. Default cipher suite selection can be configured at compile time. (Contributed by Christian Heimes in bpo-31429.)

Validation of server certificates containing internationalized domain names (IDNs) is now supported. As part of this change, the SSLSocket.server_hostname attribute now stores the expected hostname in A-label form ("xn--pythn-mua.org"), rather than the U-label form ("pythön.org"). (Contributed by Nathaniel J. Smith and Christian Heimes in bpo-28414.)

The ssl module has preliminary and experimental support for TLS 1.3 and OpenSSL 1.1.1. At the time of Python 3.7.0 release, OpenSSL 1.1.1 is still under development and TLS 1.3 hasn’t been finalized yet. The TLS 1.3 handshake and protocol behaves slightly differently than TLS 1.2 and earlier, see TLS 1.3. (Contributed by Christian Heimes in bpo-32947, bpo-20995, bpo-29136, bpo-30622 and bpo-33618)

SSLSocket and SSLObject no longer have a public constructor. Direct instantiation was never a documented and supported feature. Instances must be created with SSLContext methods wrap_socket() and wrap_bio(). (Contributed by Christian Heimes in bpo-32951)

OpenSSL 1.1 APIs for setting the minimum and maximum TLS protocol version are available as SSLContext.minimum_version and SSLContext.maximum_version. Supported protocols are indicated by several new flags, such as HAS_TLSv1_1. (Contributed by Christian Heimes in bpo-32609.)


string

string.Template now lets you to optionally modify the regular expression pattern for braced placeholders and non-braced placeholders separately. (Contributed by Barry Warsaw in bpo-1198569.)


subprocess

The subprocess.run() function accepts the new capture_output keyword argument. When true, stdout and stderr will be captured. This is equivalent to passing subprocess.PIPE as stdout and stderr arguments. (Contributed by Bo Bayles in bpo-32102.)

The subprocess.run function and the subprocess.Popen constructor now accept the text keyword argument as an alias to universal_newlines. (Contributed by Andrew Clegg in bpo-31756.)

On Windows the default for close_fds was changed from False to True when redirecting the standard handles. It’s now possible to set close_fds to true when redirecting the standard handles. See subprocess.Popen. This means that close_fds now defaults to True on all supported platforms. (Contributed by Segev Finer in bpo-19764.)

The subprocess module is now more graceful when handling KeyboardInterrupt during subprocess.call(), subprocess.run(), or in a Popen context manager. It now waits a short amount of time for the child to exit, before continuing the handling of the KeyboardInterrupt exception. (Contributed by Gregory P. Smith in bpo-25942.)


sys

The new `sys.breakpointhook()` hook function is called by the built-in breakpoint(). (Contributed by Barry Warsaw in bpo-31353.)

On Android, the new sys.getandroidapilevel() returns the build-time Android API version. (Contributed by Victor Stinner in bpo-28740.)

The new sys.get_coroutine_origin_tracking_depth() function returns the current coroutine origin tracking depth, as set by the new sys.set_coroutine_origin_tracking_depth(). asyncio has been converted to use this new API instead of the deprecated sys.set_coroutine_wrapper(). (Contributed by Nathaniel J. Smith in bpo-32591.)


time

PEP 564 adds six new functions with nanosecond resolution to the time module:
• time.clock_gettime_ns()
• time.clock_settime_ns()
• time.monotonic_ns()
• time.perf_counter_ns()
• time.process_time_ns()
• time.time_ns()

New clock identifiers have been added:
• time.CLOCK_BOOTTIME (Linux): Identical to time.CLOCK_MONOTONIC, except it also includes any time that the system is suspended.
• time.CLOCK_PROF (FreeBSD, NetBSD and OpenBSD): High-resolution per-process CPU timer.
• time.CLOCK_UPTIME (FreeBSD, OpenBSD): Time whose absolute value is the time the system has been running and not suspended, providing accurate uptime measurement.

The new time.thread_time() and time.thread_time_ns() functions can be used to get per-thread CPU time measurements. (Contributed by Antoine Pitrou in bpo-32025.)

The new time.pthread_getcpuclockid() function returns the clock ID of the thread-specific CPU-time clock.


tkinter

The new tkinter.ttk.Spinbox class is now available. (Contributed by Alan Moore in bpo-32585.)


tracemalloc

tracemalloc.Traceback behaves more like regular tracebacks, sorting the frames from oldest to most recent. Traceback.format() now accepts negative limit, truncating the result to the abs(limit) oldest frames. To get the old behaviour, use the new most_recent_first argument to Traceback.format(). (Contributed by Jesse Bakker in bpo-32121.)


types

The new WrapperDescriptorType, MethodWrapperType, MethodDescriptorType, and ClassMethodDescriptorType classes are now available. (Contributed by Manuel Krebber and Guido van Rossum in bpo-29377, and Serhiy Storchaka in bpo-32265.)

The new types.resolve_bases() function resolves MRO entries dynamically as specified by PEP 560. (Contributed by Ivan Levkivskyi in bpo-32717.)


unicodedata

The internal unicodedata database has been upgraded to use Unicode 11. (Contributed by Benjamin Peterson.)


unittest

The new -k command-line option allows filtering tests by a name substring or a Unix shell-like pattern. For example, python -m unittest -k foo runs foo_tests.SomeTest.test_something, bar_tests.SomeTest.test_foo, but not bar_tests.FooTest.test_something. (Contributed by Jonas Haag in bpo-32071.)


unittest.mock

The sentinel attributes now preserve their identity when they are copied or pickled. (Contributed by Serhiy Storchaka in bpo-20804.)

The new seal() function allows sealing Mock instances, which will disallow further creation of attribute mocks. The seal is applied recursively to all attributes that are themselves mocks. (Contributed by Mario Corchero in bpo-30541.)


urllib.parse

urllib.parse.quote() has been updated from RFC 2396 to RFC 3986, adding ~ to the set of characters that are never quoted by default. (Contributed by Christian Theune and Ratnadeep Debnath in bpo-16285.)


uu

The uu.encode() function now accepts an optional backtick keyword argument. When it’s true, zeros are represented by '`' instead of spaces. (Contributed by Xiang Zhang in bpo-30103.)


uuid

The new UUID.is_safe attribute relays information from the platform about whether generated UUIDs are generated with a multiprocessing-safe method. (Contributed by Barry Warsaw in bpo-22807.)

uuid.getnode() now prefers universally administered MAC addresses over locally administered MAC addresses. This makes a better guarantee for global uniqueness of UUIDs returned from uuid.uuid1(). If only locally administered MAC addresses are available, the first such one found is returned. (Contributed by Barry Warsaw in bpo-32107.)


warnings

The initialization of the default warnings filters has changed as follows:

• warnings enabled via command line options (including those for -b and the new CPython-specific -X dev option) are always passed to the warnings machinery via the sys.warnoptions attribute.


• warnings filters enabled via the command line or the environment now have the following order of precedence:


◦the BytesWarning filter for -b (or -bb)
◦any filters specified with the -W option
◦any filters specified with the PYTHONWARNINGS environment variable
◦any other CPython specific filters (e.g. the default filter added for the new -X dev mode)
◦any implicit filters defined directly by the warnings machinery


• in CPython debug builds, all warnings are now displayed by default (the implicit filter list is empty)


(Contributed by Nick Coghlan and Victor Stinner in bpo-20361, bpo-32043, and bpo-32230.)

Deprecation warnings are once again shown by default in single-file scripts and at the interactive prompt. See PEP 565: Show DeprecationWarning in __main__ for details. (Contributed by Nick Coghlan in bpo-31975.)


xml.etree

ElementPath predicates in the find() methods can now compare text of the current node with [. = "text"], not only text in children. Predicates also allow adding spaces for better readability. (Contributed by Stefan Behnel in bpo-31648.)


xmlrpc.server

SimpleXMLRPCDispatcher.register_function can now be used as a decorator. (Contributed by Xiang Zhang in bpo-7769.)


zipapp

Function create_archive() now accepts an optional filter argument to allow the user to select which files should be included in the archive. (Contributed by Irmen de Jong in bpo-31072.)

Function create_archive() now accepts an optional compressed argument to generate a compressed archive. A command line option --compress has also been added to support compression. (Contributed by Zhiming Wang in bpo-31638.)


zipfile

ZipFile now accepts the new compresslevel parameter to control the compression level. (Contributed by Bo Bayles in bpo-21417.)

Subdirectories in archives created by ZipFile are now stored in alphabetical order. (Contributed by Bernhard M. Wiedemann in bpo-30693.)


### ===🗝 C API Changes

A new API for thread-local storage has been implemented. See PEP 539: New C API for Thread-Local Storage for an overview and Thread Specific Storage (TSS) API for a complete reference. (Contributed by Masayuki Yamamoto in bpo-25658.)

The new context variables functionality exposes a number of new C APIs.

The new PyImport_GetModule() function returns the previously imported module with the given name. (Contributed by Eric Snow in bpo-28411.)

The new Py_RETURN_RICHCOMPARE macro eases writing rich comparison functions. (Contributed by Petr Victorin in bpo-23699.)

The new Py_UNREACHABLE macro can be used to mark unreachable code paths. (Contributed by Barry Warsaw in bpo-31338.)

The tracemalloc now exposes a C API through the new PyTraceMalloc_Track() and PyTraceMalloc_Untrack() functions. (Contributed by Victor Stinner in bpo-30054.)

The new import__find__load__start() and import__find__load__done() static markers can be used to trace module imports. (Contributed by Christian Heimes in bpo-31574.)

The fields name and doc of structures PyMemberDef, PyGetSetDef, PyStructSequence_Field, PyStructSequence_Desc, and wrapperbase are now of type `const char *` rather of `char *`. (Contributed by Serhiy Storchaka in bpo-28761.)

The result of PyUnicode_AsUTF8AndSize() and PyUnicode_AsUTF8() is now of type `const char *` rather of `char *`. (Contributed by Serhiy Storchaka in bpo-28769.)

The result of PyMapping_Keys(), PyMapping_Values() and PyMapping_Items() is now always a list, rather than a list or a tuple. (Contributed by Oren Milman in bpo-28280.)

Added functions PySlice_Unpack() and PySlice_AdjustIndices(). (Contributed by Serhiy Storchaka in bpo-27867.)

PyOS_AfterFork() is deprecated in favour of the new functions PyOS_BeforeFork(), PyOS_AfterFork_Parent() and PyOS_AfterFork_Child(). (Contributed by Antoine Pitrou in bpo-16500.)

The PyExc_RecursionErrorInst singleton that was part of the public API has been removed as its members being never cleared may cause a segfault during finalization of the interpreter. Contributed by Xavier de Gaye in bpo-22898 and bpo-30697.

Added C API support for timezones with timezone constructors PyTimeZone_FromOffset() and PyTimeZone_FromOffsetAndName(), and access to the UTC singleton with PyDateTime_TimeZone_UTC. Contributed by Paul Ganssle in bpo-10381.

The type of results of PyThread_start_new_thread() and PyThread_get_thread_ident(), and the id parameter of PyThreadState_SetAsyncExc() changed from long to unsigned long. (Contributed by Serhiy Storchaka in bpo-6532.)

PyUnicode_AsWideCharString() now raises a ValueError if the second argument is NULL and the wchar_t* string contains null characters. (Contributed by Serhiy Storchaka in bpo-30708.)

Changes to the startup sequence and the management of dynamic memory allocators mean that the long documented requirement to call Py_Initialize() before calling most C API functions is now relied on more heavily, and failing to abide by it may lead to segfaults in embedding applications. See the Porting to Python 3.7 section in this document and the Before Python Initialization section in the C API documentation for more details.

The new PyInterpreterState_GetID() returns the unique ID for a given interpreter. (Contributed by Eric Snow in bpo-29102.)

Py_DecodeLocale(), Py_EncodeLocale() now use the UTF-8 encoding when the UTF-8 mode is enabled. (Contributed by Victor Stinner in bpo-29240.)

PyUnicode_DecodeLocaleAndSize() and PyUnicode_EncodeLocale() now use the current locale encoding for surrogateescape error handler. (Contributed by Victor Stinner in bpo-29240.)

The start and end parameters of PyUnicode_FindChar() are now adjusted to behave like string slices. (Contributed by Xiang Zhang in bpo-28822.)


### ===🗝 Build Changes

Support for building --without-threads has been removed. The threading module is now always available. (Contributed by Antoine Pitrou in bpo-31370.).

A full copy of libffi is no longer bundled for use when building the `_ctypes` module on non-OSX UNIX platforms. An installed copy of libffi is now required when building `_ctypes` on such platforms. (Contributed by Zachary Ware in bpo-27979.)

The Windows build process no longer depends on Subversion to pull in external sources, a Python script is used to download zipfiles from GitHub instead. If Python 3.6 is not found on the system (via py -3.6), NuGet is used to download a copy of 32-bit Python for this purpose. (Contributed by Zachary Ware in bpo-30450.)

The ssl module requires OpenSSL 1.0.2 or 1.1 compatible libssl. OpenSSL 1.0.1 has reached end of lifetime on 2016-12-31 and is no longer supported. LibreSSL is temporarily not supported as well. LibreSSL releases up to version 2.6.4 are missing required OpenSSL 1.0.2 APIs.


### ===🗝 Optimizations

The overhead of calling many methods of various standard library classes implemented in C has been significantly reduced by porting more code to use the METH_FASTCALL convention. (Contributed by Victor Stinner in bpo-29300, bpo-29507, bpo-29452, and bpo-29286.)

Various optimizations have reduced Python startup time by 10% on Linux and up to 30% on macOS. (Contributed by Victor Stinner, INADA Naoki in bpo-29585, and Ivan Levkivskyi in bpo-31333.)

Method calls are now up to 20% faster due to the bytecode changes which avoid creating bound method instances. (Contributed by Yury Selivanov and INADA Naoki in bpo-26110.)

The asyncio module received a number of notable optimizations for commonly used functions:
• The asyncio.get_event_loop() function has been reimplemented in C to make it up to 15 times faster. (Contributed by Yury Selivanov in bpo-32296.)
• asyncio.Future callback management has been optimized. (Contributed by Yury Selivanov in bpo-32348.)
• asyncio.gather() is now up to 15% faster. (Contributed by Yury Selivanov in bpo-32355.)
• asyncio.sleep() is now up to 2 times faster when the delay argument is zero or negative. (Contributed by Andrew Svetlov in bpo-32351.)
• The performance overhead of asyncio debug mode has been reduced. (Contributed by Antoine Pitrou in bpo-31970.)

As a result of PEP 560 work, the import time of typing has been reduced by a factor of 7, and many typing operations are now faster. (Contributed by Ivan Levkivskyi in bpo-32226.)

sorted() and list.sort() have been optimized for common cases to be up to 40-75% faster. (Contributed by Elliot Gorokhovsky in bpo-28685.)

dict.copy() is now up to 5.5 times faster. (Contributed by Yury Selivanov in bpo-31179.)

hasattr() and getattr() are now about 4 times faster when name is not found and obj does not override object.__getattr__() or object.__getattribute__(). (Contributed by INADA Naoki in bpo-32544.)

Searching for certain Unicode characters (like Ukrainian capital “Є”) in a string was up to 25 times slower than searching for other characters. It is now only 3 times slower in the worst case. (Contributed by Serhiy Storchaka in bpo-24821.)

The collections.namedtuple() factory has been reimplemented to make the creation of named tuples 4 to 6 times faster. (Contributed by Jelle Zijlstra with further improvements by INADA Naoki, Serhiy Storchaka, and Raymond Hettinger in bpo-28638.)

date.fromordinal() and date.fromtimestamp() are now up to 30% faster in the common case. (Contributed by Paul Ganssle in bpo-32403.)

The os.fwalk() function is now up to 2 times faster thanks to the use of os.scandir(). (Contributed by Serhiy Storchaka in bpo-25996.)

The speed of the shutil.rmtree() function has been improved by 20–40% thanks to the use of the os.scandir() function. (Contributed by Serhiy Storchaka in bpo-28564.)

Optimized case-insensitive matching and searching of regular expressions. Searching some patterns can now be up to 20 times faster. (Contributed by Serhiy Storchaka in bpo-30285.)

re.compile() now converts flags parameter to int object if it is RegexFlag. It is now as fast as Python 3.5, and faster than Python 3.6 by about 10% depending on the pattern. (Contributed by INADA Naoki in bpo-31671.)

The modify() methods of classes selectors.EpollSelector, selectors.PollSelector and selectors.DevpollSelector may be around 10% faster under heavy loads. (Contributed by Giampaolo Rodola’ in bpo-30014)

Constant folding has been moved from the peephole optimizer to the new AST optimizer, which is able perform optimizations more consistently. (Contributed by Eugene Toder and INADA Naoki in bpo-29469 and bpo-11549.)

Most functions and methods in abc have been rewritten in C. This makes creation of abstract base classes, and calling isinstance() and issubclass() on them 1.5x faster. This also reduces Python start-up time by up to 10%. (Contributed by Ivan Levkivskyi and INADA Naoki in bpo-31333)

Significant speed improvements to alternate constructors for datetime.date and datetime.datetime by using fast-path constructors when not constructing subclasses. (Contributed by Paul Ganssle in bpo-32403)

The speed of comparison of array.array instances has been improved considerably in certain cases. It is now from 10x to 70x faster when comparing arrays holding values of the same integer type. (Contributed by Adrian Wielgosik in bpo-24700.)

The math.erf() and math.erfc() functions now use the (faster) C library implementation on most platforms. (Contributed by Serhiy Storchaka in bpo-26121.)


### ===🗝 Other CPython Implementation Changes

• Trace hooks may now opt out of receiving the line and opt into receiving the opcode events from the interpreter by setting the corresponding new f_trace_lines and f_trace_opcodes attributes on the frame being traced. (Contributed by Nick Coghlan in bpo-31344.)

• Fixed some consistency problems with namespace package module attributes. Namespace module objects now have an __file__ that is set to None (previously unset), and their `__spec__.origin` is also set to None (previously the string "namespace"). See bpo-32305. Also, the namespace module object’s __spec__.loader is set to the same value as __loader__ (previously, the former was set to None). See bpo-32303.

• The locals() dictionary now displays in the lexical order that variables were defined. Previously, the order was undefined. (Contributed by Raymond Hettinger in bpo-32690.)

• The distutils upload command no longer tries to change CR end-of-line characters to CRLF. This fixes a corruption issue with sdists that ended with a byte equivalent to CR. (Contributed by Bo Bayles in bpo-32304.)



### ===🗝 Deprecated Python Behavior

Yield expressions (both yield and yield from clauses) are now deprecated in comprehensions and generator expressions (aside from the iterable expression in the leftmost for clause). This ensures that comprehensions always immediately return a container of the appropriate type (rather than potentially returning a generator iterator object), while generator expressions won’t attempt to interleave their implicit output with the output from any explicit yield expressions. In Python 3.7, such expressions emit DeprecationWarning when compiled, in Python 3.8 this will be a SyntaxError. (Contributed by Serhiy Storchaka in bpo-10544.)

Returning a subclass of complex from object.__complex__() is deprecated and will be an error in future Python versions. This makes __complex__() consistent with object.__int__() and object.__float__(). (Contributed by Serhiy Storchaka in bpo-28894.)


### ===🗝 Deprecated Python modules, functions and methods


aifc

aifc.openfp() has been deprecated and will be removed in Python 3.9. Use aifc.open() instead. (Contributed by Brian Curtin in bpo-31985.)


asyncio

Support for directly await-ing instances of asyncio.Lock and other asyncio synchronization primitives has been deprecated. An asynchronous context manager must be used in order to acquire and release the synchronization resource. (Contributed by Andrew Svetlov in bpo-32253.)

The asyncio.Task.current_task() and asyncio.Task.all_tasks() methods have been deprecated. (Contributed by Andrew Svetlov in bpo-32250.)


collections

In Python 3.8, the abstract base classes in collections.abc will no longer be exposed in the regular collections module. This will help create a clearer distinction between the concrete classes and the abstract base classes. (Contributed by Serhiy Storchaka in bpo-25988.)


dbm

dbm.dumb now supports reading read-only files and no longer writes the index file when it is not changed. A deprecation warning is now emitted if the index file is missing and recreated in the 'r' and 'w' modes (this will be an error in future Python releases). (Contributed by Serhiy Storchaka in bpo-28847.)


enum

In Python 3.8, attempting to check for non-Enum objects in Enum classes will raise a TypeError (e.g. 1 in Color); similarly, attempting to check for non-Flag objects in a Flag member will raise TypeError (e.g. 1 in Perm.RW); currently, both operations return False instead. (Contributed by Ethan Furman in bpo-33217.)


gettext

Using non-integer value for selecting a plural form in gettext is now deprecated. It never correctly worked. (Contributed by Serhiy Storchaka in bpo-28692.)


importlib

Methods MetaPathFinder.find_module() (replaced by MetaPathFinder.find_spec()) and PathEntryFinder.find_loader() (replaced by PathEntryFinder.find_spec()) both deprecated in Python 3.4 now emit DeprecationWarning. (Contributed by Matthias Bussonnier in bpo-29576)

The importlib.abc.ResourceLoader ABC has been deprecated in favour of importlib.abc.ResourceReader.


locale

locale.format() has been deprecated, use locale.format_string() instead. (Contributed by Garvit in bpo-10379.)


macpath

The macpath is now deprecated and will be removed in Python 3.8. (Contributed by Chi Hsuan Yen in bpo-9850.)


threading

dummy_threading and `_dummy_thread` have been deprecated. It is no longer possible to build Python with threading disabled. Use threading instead. (Contributed by Antoine Pitrou in bpo-31370.)


socket

The silent argument value truncation in socket.htons() and socket.ntohs() has been deprecated. In future versions of Python, if the passed argument is larger than 16 bits, an exception will be raised. (Contributed by Oren Milman in bpo-28332.)


ssl

ssl.wrap_socket() is deprecated. Use ssl.SSLContext.wrap_socket() instead. (Contributed by Christian Heimes in bpo-28124.)


sunau

sunau.openfp() has been deprecated and will be removed in Python 3.9. Use sunau.open() instead. (Contributed by Brian Curtin in bpo-31985.)


sys

Deprecated sys.set_coroutine_wrapper() and sys.get_coroutine_wrapper().

The undocumented sys.callstats() function has been deprecated and will be removed in a future Python version. (Contributed by Victor Stinner in bpo-28799.)


wave

wave.openfp() has been deprecated and will be removed in Python 3.9. Use wave.open() instead. (Contributed by Brian Curtin in bpo-31985.)


### ===🗝 Deprecated functions and types of the C API

Function PySlice_GetIndicesEx() is deprecated and replaced with a macro if Py_LIMITED_API is not set or set to a value in the range between 0x03050400 and 0x03060000 (not inclusive), or is 0x03060100 or higher. (Contributed by Serhiy Storchaka in bpo-27867.)

PyOS_AfterFork() has been deprecated. Use PyOS_BeforeFork(), PyOS_AfterFork_Parent() or PyOS_AfterFork_Child() instead. (Contributed by Antoine Pitrou in bpo-16500.)


### ===🗝 Platform Support Removals

• FreeBSD 9 and older are no longer officially supported.


• For full Unicode support, including within extension modules, `*nix` platforms are now expected to provide at least one of C.UTF-8 (full locale), C.utf8 (full locale) or UTF-8 (LC_CTYPE-only locale) as an alternative to the legacy ASCII-based C locale.


• OpenSSL 0.9.8 and 1.0.1 are no longer supported, which means building CPython 3.7 with SSL/TLS support on older platforms still using these versions requires custom build options that link to a more recent version of OpenSSL.

Notably, this issue affects the Debian 8 (aka “jessie”) and Ubuntu 14.04 (aka “Trusty”) LTS Linux distributions, as they still use OpenSSL 1.0.1 by default.

Debian 9 (“stretch”) and Ubuntu 16.04 (“xenial”), as well as recent releases of other LTS Linux releases (e.g. RHEL/CentOS 7.5, SLES 12-SP3), use OpenSSL 1.0.2 or later, and remain supported in the default build configuration.

CPython’s own CI configuration file provides an example of using the SSL compatibility testing infrastructure in CPython’s test suite to build and link against OpenSSL 1.1.0 rather than an outdated system provided OpenSSL.



### ===🗝 API and Feature Removals

The following features and APIs have been removed from Python 3.7:

• The os.stat_float_times() function has been removed. It was introduced in Python 2.3 for backward compatibility with Python 2.2, and was deprecated since Python 3.1.

• Unknown escapes consisting of '\' and an ASCII letter in replacement templates for re.sub() were deprecated in Python 3.5, and will now cause an error.

• Removed support of the exclude argument in tarfile.TarFile.add(). It was deprecated in Python 2.7 and 3.2. Use the filter argument instead.

• The splitunc() function in the ntpath module was deprecated in Python 3.1, and has now been removed. Use the splitdrive() function instead.

• collections.namedtuple() no longer supports the verbose parameter or `_source` attribute which showed the generated source code for the named tuple class. This was part of an optimization designed to speed-up class creation. (Contributed by Jelle Zijlstra with further improvements by INADA Naoki, Serhiy Storchaka, and Raymond Hettinger in bpo-28638.)

• Functions bool(), float(), list() and tuple() no longer take keyword arguments. The first argument of int() can now be passed only as positional argument.

• Removed previously deprecated in Python 2.4 classes Plist, Dict and `_InternalDict` in the plistlib module. Dict values in the result of functions readPlist() and readPlistFromBytes() are now normal dicts. You no longer can use attribute access to access items of these dictionaries.

• The asyncio.windows_utils.socketpair() function has been removed. Use the socket.socketpair() function instead, it is available on all platforms since Python 3.5. asyncio.windows_utils.socketpair was just an alias to socket.socketpair on Python 3.5 and newer.

• asyncio no longer exports the selectors and `_overlapped` modules as asyncio.selectors and `asyncio._overlapped`. Replace from asyncio import selectors with import selectors.

• Direct instantiation of ssl.SSLSocket and ssl.SSLObject objects is now prohibited. The constructors were never documented, tested, or designed as public constructors. Users were supposed to use ssl.wrap_socket() or ssl.SSLContext. (Contributed by Christian Heimes in bpo-32951.)

• The unused distutils install_misc command has been removed. (Contributed by Eric N. Vander Weele in bpo-29218.)


### ===🗝 Module Removals

The fpectl module has been removed. It was never enabled by default, never worked correctly on x86-64, and it changed the Python ABI in ways that caused unexpected breakage of C extensions. (Contributed by Nathaniel J. Smith in bpo-29137.)


### ===🗝 Windows-only Changes

The python launcher, (py.exe), can accept 32 & 64 bit specifiers without having to specify a minor version as well. So py -3-32 and py -3-64 become valid as well as py -3.7-32, also the -m-64 and -m.n-64 forms are now accepted to force 64 bit python even if 32 bit would have otherwise been used. If the specified version is not available py.exe will error exit. (Contributed by Steve Barnes in bpo-30291.)

The launcher can be run as py -0 to produce a list of the installed pythons, with default marked with an asterisk. Running py -0p will include the paths. If py is run with a version specifier that cannot be matched it will also print the short form list of available specifiers. (Contributed by Steve Barnes in bpo-30362.)


### ===🗝 Porting to Python 3.7

This section lists previously described changes and other bugfixes that may require changes to your code.


#### Changes in Python Behavior

• async and await names are now reserved keywords. Code using these names as identifiers will now raise a SyntaxError. (Contributed by Jelle Zijlstra in bpo-30406.)


• PEP 479 is enabled for all code in Python 3.7, meaning that StopIteration exceptions raised directly or indirectly in coroutines and generators are transformed into RuntimeError exceptions. (Contributed by Yury Selivanov in bpo-32670.)


• object.__aiter__() methods can no longer be declared as asynchronous. (Contributed by Yury Selivanov in bpo-31709.)


• Due to an oversight, earlier Python versions erroneously accepted the following syntax:


f(1 for x in [1],)

class C(1 for x in [1]):
    pass


Python 3.7 now correctly raises a SyntaxError, as a generator expression always needs to be directly inside a set of parentheses and cannot have a comma on either side, and the duplication of the parentheses can be omitted only on calls. (Contributed by Serhiy Storchaka in bpo-32012 and bpo-32023.)


• When using the -m switch, the initial working directory is now added to sys.path, rather than an empty string (which dynamically denoted the current working directory at the time of each import). Any programs that are checking for the empty string, or otherwise relying on the previous behaviour, will need to be updated accordingly (e.g. by also checking for os.getcwd() or os.path.dirname(__main__.__file__), depending on why the code was checking for the empty string in the first place).



#### Changes in the Python API

• socketserver.ThreadingMixIn.server_close() now waits until all non-daemon threads complete. Set the new socketserver.ThreadingMixIn.block_on_close class attribute to False to get the pre-3.7 behaviour. (Contributed by Victor Stinner in bpo-31233 and bpo-33540.)


• socketserver.ForkingMixIn.server_close() now waits until all child processes complete. Set the new socketserver.ForkingMixIn.block_on_close class attribute to False to get the pre-3.7 behaviour. (Contributed by Victor Stinner in bpo-31151 and bpo-33540.)


• The locale.localeconv() function now temporarily sets the LC_CTYPE locale to the value of LC_NUMERIC in some cases. (Contributed by Victor Stinner in bpo-31900.)


• pkgutil.walk_packages() now raises a ValueError if path is a string. Previously an empty list was returned. (Contributed by Sanyam Khurana in bpo-24744.)


• A format string argument for string.Formatter.format() is now positional-only. Passing it as a keyword argument was deprecated in Python 3.5. (Contributed by Serhiy Storchaka in bpo-29193.)


• Attributes key, value and coded_value of class http.cookies.Morsel are now read-only. Assigning to them was deprecated in Python 3.5. Use the set() method for setting them. (Contributed by Serhiy Storchaka in bpo-29192.)


• The mode argument of os.makedirs() no longer affects the file permission bits of newly-created intermediate-level directories. To set their file permission bits you can set the umask before invoking makedirs(). (Contributed by Serhiy Storchaka in bpo-19930.)


• The struct.Struct.format type is now str instead of bytes. (Contributed by Victor Stinner in bpo-21071.)


• parse_multipart() now accepts the encoding and errors arguments and returns the same results as FieldStorage: for non-file fields, the value associated to a key is a list of strings, not bytes. (Contributed by Pierre Quentel in bpo-29979.)


• Due to internal changes in socket, calling socket.fromshare() on a socket created by socket.share in older Python versions is not supported.


• repr for BaseException has changed to not include the trailing comma. Most exceptions are affected by this change. (Contributed by Serhiy Storchaka in bpo-30399.)


• repr for datetime.timedelta has changed to include the keyword arguments in the output. (Contributed by Utkarsh Upadhyay in bpo-30302.)


• Because shutil.rmtree() is now implemented using the os.scandir() function, the user specified handler onerror is now called with the first argument os.scandir instead of os.listdir when listing the directory is failed.


• Support for nested sets and set operations in regular expressions as in Unicode Technical Standard #18 might be added in the future. This would change the syntax. To facilitate this future change a FutureWarning will be raised in ambiguous cases for the time being. That include sets starting with a literal '[' or containing literal character sequences '--', '&&', '~~', and '||'. To avoid a warning, escape them with a backslash. (Contributed by Serhiy Storchaka in bpo-30349.)


• The result of splitting a string on a regular expression that could match an empty string has been changed. For example splitting on `r'\s*'` will now split not only on whitespaces as it did previously, but also on empty strings before all non-whitespace characters and just before the end of the string. The previous behavior can be restored by changing the pattern to r'\s+'. A FutureWarning was emitted for such patterns since Python 3.5.

For patterns that match both empty and non-empty strings, the result of searching for all matches may also be changed in other cases. For example in the string 'a\n\n', the pattern `r'(?m)^\s*?$'` will not only match empty strings at positions 2 and 3, but also the string '\n' at positions 2–3. To match only blank lines, the pattern should be rewritten as `r'(?m)^[^\S\n]*$'`.

re.sub() now replaces empty matches adjacent to a previous non-empty match. For example `re.sub('x*', '-', 'abxd')` returns now '-a-b--d-' instead of '-a-b-d-' (the first minus between ‘b’ and ‘d’ replaces ‘x’, and the second minus replaces an empty string between ‘x’ and ‘d’).

(Contributed by Serhiy Storchaka in bpo-25054 and bpo-32308.)


• Change re.escape() to only escape regex special characters instead of escaping all characters other than ASCII letters, numbers, and `'_'`. (Contributed by Serhiy Storchaka in bpo-29995.)


• tracemalloc.Traceback frames are now sorted from oldest to most recent to be more consistent with traceback. (Contributed by Jesse Bakker in bpo-32121.)


• On OSes that support socket.SOCK_NONBLOCK or socket.SOCK_CLOEXEC bit flags, the socket.type no longer has them applied. Therefore, checks like if sock.type == socket.SOCK_STREAM work as expected on all platforms. (Contributed by Yury Selivanov in bpo-32331.)


• On Windows the default for the close_fds argument of subprocess.Popen was changed from False to True when redirecting the standard handles. If you previously depended on handles being inherited when using subprocess.Popen with standard io redirection, you will have to pass close_fds=False to preserve the previous behaviour, or use STARTUPINFO.lpAttributeList.


• importlib.machinery.PathFinder.invalidate_caches() – which implicitly affects importlib.invalidate_caches() – now deletes entries in sys.path_importer_cache which are set to None. (Contributed by Brett Cannon in bpo-33169.)


• In asyncio, loop.sock_recv(), loop.sock_sendall(), loop.sock_accept(), loop.getaddrinfo(), loop.getnameinfo() have been changed to be proper coroutine methods to match their documentation. Previously, these methods returned asyncio.Future instances. (Contributed by Yury Selivanov in bpo-32327.)


• asyncio.Server.sockets now returns a copy of the internal list of server sockets, instead of returning it directly. (Contributed by Yury Selivanov in bpo-32662.)


• Struct.format is now a str instance instead of a bytes instance. (Contributed by Victor Stinner in bpo-21071.)


• argparse subparsers can now be made mandatory by passing required=True to ArgumentParser.add_subparsers(). (Contributed by Anthony Sottile in bpo-26510.)


• ast.literal_eval() is now stricter. Addition and subtraction of arbitrary numbers are no longer allowed. (Contributed by Serhiy Storchaka in bpo-31778.)


• Calendar.itermonthdates will now consistently raise an exception when a date falls outside of the 0001-01-01 through 9999-12-31 range. To support applications that cannot tolerate such exceptions, the new Calendar.itermonthdays3 and Calendar.itermonthdays4 can be used. The new methods return tuples and are not restricted by the range supported by datetime.date. (Contributed by Alexander Belopolsky in bpo-28292.)


• collections.ChainMap now preserves the order of the underlying mappings. (Contributed by Raymond Hettinger in bpo-32792.)


• The submit() method of concurrent.futures.ThreadPoolExecutor and concurrent.futures.ProcessPoolExecutor now raises a RuntimeError if called during interpreter shutdown. (Contributed by Mark Nemec in bpo-33097.)


• The configparser.ConfigParser constructor now uses read_dict() to process the default values, making its behavior consistent with the rest of the parser. Non-string keys and values in the defaults dictionary are now being implicitly converted to strings. (Contributed by James Tocknell in bpo-23835.)


• Several undocumented internal imports were removed. One example is that os.errno is no longer available; use import errno directly instead. Note that such undocumented internal imports may be removed any time without notice, even in micro version releases.



####  Changes in the C API

The function PySlice_GetIndicesEx() is considered unsafe for resizable sequences. If the slice indices are not instances of int, but objects that implement the __index__() method, the sequence can be resized after passing its length to PySlice_GetIndicesEx(). This can lead to returning indices out of the length of the sequence. For avoiding possible problems use new functions PySlice_Unpack() and PySlice_AdjustIndices(). (Contributed by Serhiy Storchaka in bpo-27867.)


####  CPython bytecode changes

There are two new opcodes: LOAD_METHOD and CALL_METHOD. (Contributed by Yury Selivanov and INADA Naoki in bpo-26110.)

The STORE_ANNOTATION opcode has been removed. (Contributed by Mark Shannon in bpo-32550.)


####  Windows-only Changes

The file used to override sys.path is now called `<python-executable>._pth` instead of 'sys.path'. See Finding modules for more information. (Contributed by Steve Dower in bpo-28137.)


####  Other CPython implementation changes

In preparation for potential future changes to the public CPython runtime initialization API (see PEP 432 for an initial, but somewhat outdated, draft), CPython’s internal startup and configuration management logic has been significantly refactored. While these updates are intended to be entirely transparent to both embedding applications and users of the regular CPython CLI, they’re being mentioned here as the refactoring changes the internal order of various operations during interpreter startup, and hence may uncover previously latent defects, either in embedding applications, or in CPython itself. (Initially contributed by Nick Coghlan and Eric Snow as part of bpo-22257, and further updated by Nick, Eric, and Victor Stinner in a number of other issues). Some known details affected:
• PySys_AddWarnOptionUnicode() is not currently usable by embedding applications due to the requirement to create a Unicode object prior to calling Py_Initialize. Use PySys_AddWarnOption() instead.
• warnings filters added by an embedding application with PySys_AddWarnOption() should now more consistently take precedence over the default filters set by the interpreter

Due to changes in the way the default warnings filters are configured, setting Py_BytesWarningFlag to a value greater than one is no longer sufficient to both emit BytesWarning messages and have them converted to exceptions. Instead, the flag must be set (to cause the warnings to be emitted in the first place), and an explicit error::BytesWarning warnings filter added to convert them to exceptions.

Due to a change in the way docstrings are handled by the compiler, the implicit return None in a function body consisting solely of a docstring is now marked as occurring on the same line as the docstring, not on the function’s header line.

The current exception state has been moved from the frame object to the co-routine. This simplified the interpreter and fixed a couple of obscure bugs caused by having swap exception state when entering or exiting a generator. (Contributed by Mark Shannon in bpo-25612.)


### ===🗝 Notable changes in Python 3.7.1

Starting in 3.7.1, Py_Initialize() now consistently reads and respects all of the same environment settings as Py_Main() (in earlier Python versions, it respected an ill-defined subset of those environment variables, while in Python 3.7.0 it didn’t read any of them due to bpo-34247). If this behavior is unwanted, set Py_IgnoreEnvironmentFlag to 1 before calling Py_Initialize().

In 3.7.1 the C API for Context Variables was updated to use PyObject pointers. See also bpo-34762.

In 3.7.1 the tokenize module now implicitly emits a NEWLINE token when provided with input that does not have a trailing new line. This behavior now matches what the C tokenizer does internally. (Contributed by Ammar Askar in bpo-33899.)


### ===🗝 Notable changes in Python 3.7.2

In 3.7.2, venv on Windows no longer copies the original binaries, but creates redirector scripts named python.exe and pythonw.exe instead. This resolves a long standing issue where all virtual environments would have to be upgraded or recreated with each Python update. However, note that this release will still require recreation of virtual environments in order to get the new scripts.


### ===🗝 Notable changes in Python 3.7.6

Due to significant security concerns, the reuse_address parameter of asyncio.loop.create_datagram_endpoint() is no longer supported. This is because of the behavior of the socket option SO_REUSEADDR in UDP. For more details, see the documentation for loop.create_datagram_endpoint(). (Contributed by Kyle Stanley, Antoine Pitrou, and Yury Selivanov in bpo-37228.)


### ===🗝 Notable changes in Python 3.7.10

Earlier Python versions allowed using both ; and & as query parameter separators in urllib.parse.parse_qs() and urllib.parse.parse_qsl(). Due to security concerns, and to conform with newer W3C recommendations, this has been changed to allow only a single separator key, with & as the default. This change also affects cgi.parse() and cgi.parse_multipart() as they use the affected functions internally. For more details, please see their respective documentation. (Contributed by Adam Goldschmidt, Senthil Kumaran and Ken Jin in bpo-42967.)


## ==⚡ • What’s New In Python 3.6
◦ Summary – Release highlights
◦ New Features
  PEP 498: Formatted string literals
  PEP 526: Syntax for variable annotations
  PEP 515: Underscores in Numeric Literals
  PEP 525: Asynchronous Generators
  PEP 530: Asynchronous Comprehensions
  PEP 487: Simpler customization of class creation
  PEP 487: Descriptor Protocol Enhancements
  PEP 519: Adding a file system path protocol
  PEP 495: Local Time Disambiguation
  PEP 529: Change Windows filesystem encoding to UTF-8
  PEP 528: Change Windows console encoding to UTF-8
  PEP 520: Preserving Class Attribute Definition Order
  PEP 468: Preserving Keyword Argument Order
  PEP 523: Adding a frame evaluation API to CPython
◦ Other Language Changes
◦ New Modules
◦ Improved Modules
◦ Optimizations
◦ Build and C API Changes
◦ Other Improvements
◦ Deprecated
◦ Removed
◦ Porting to Python 3.6
◦ Notable changes in Python 3.6.2
◦ Notable changes in Python 3.6.4
◦ Notable changes in Python 3.6.5
◦ Notable changes in Python 3.6.7
◦ Notable changes in Python 3.6.10
◦ Notable changes in Python 3.6.13

### ===🗝 Summary – Release highlights

New syntax features:
• PEP 498, formatted string literals.
• PEP 515, underscores in numeric literals.
• PEP 526, syntax for variable annotations.
• PEP 525, asynchronous generators.
• PEP 530: asynchronous comprehensions.

New library modules:
• secrets: PEP 506 – Adding A Secrets Module To The Standard Library.

CPython implementation improvements:
• The dict type has been reimplemented to use a more compact representation based on a proposal by Raymond Hettinger and similar to the PyPy dict implementation. This resulted in dictionaries using 20% to 25% less memory when compared to Python 3.5.
• Customization of class creation has been simplified with the new protocol.
• The class attribute definition order is now preserved.
• The order of elements in `**kwargs` now corresponds to the order in which keyword arguments were passed to the function.
• DTrace and SystemTap probing support has been added.
• The new PYTHONMALLOC environment variable can now be used to debug the interpreter memory allocation and access errors.

Significant improvements in the standard library:
• The asyncio module has received new features, significant usability and performance improvements, and a fair amount of bug fixes. Starting with Python 3.6 the asyncio module is no longer provisional and its API is considered stable.
• A new file system path protocol has been implemented to support path-like objects. All standard library functions operating on paths have been updated to work with the new protocol.
• The datetime module has gained support for Local Time Disambiguation.
• The typing module received a number of improvements.
• The tracemalloc module has been significantly reworked and is now used to provide better output for ResourceWarning as well as provide better diagnostics for memory allocation errors. See the PYTHONMALLOC section for more information.

Security improvements:
• The new secrets module has been added to simplify the generation of cryptographically strong pseudo-random numbers suitable for managing secrets such as account authentication, tokens, and similar.
• On Linux, os.urandom() now blocks until the system urandom entropy pool is initialized to increase the security. See the PEP 524 for the rationale.
• The hashlib and ssl modules now support OpenSSL 1.1.0.
• The default settings and feature set of the ssl module have been improved.
• The hashlib module received support for the BLAKE2, SHA-3 and SHAKE hash algorithms and the scrypt() key derivation function.

Windows improvements:
• PEP 528 and PEP 529, Windows filesystem and console encoding changed to UTF-8.
• The py.exe launcher, when used interactively, no longer prefers Python 2 over Python 3 when the user doesn’t specify a version (via command line arguments or a config file). Handling of shebang lines remains unchanged - “python” refers to Python 2 in that case.
• python.exe and pythonw.exe have been marked as long-path aware, which means that the 260 character path limit may no longer apply. See removing the MAX_PATH limitation for details.
• A `._pth` file can be added to force isolated mode and fully specify all search paths to avoid registry and environment lookup. See the documentation for more information.
• A python36.zip file now works as a landmark to infer PYTHONHOME. See the documentation for more information.

## ==⚡ • What’s New In Python 3.5
◦ Summary – Release highlights
◦ New Features

PEP 492 - Coroutines with async and await syntax
PEP 465 - A dedicated infix operator for matrix multiplication
PEP 448 - Additional Unpacking Generalizations
PEP 461 - percent formatting support for bytes and bytearray
PEP 484 - Type Hints
PEP 471 - os.scandir() function – a better and faster directory iterator
PEP 475: Retry system calls failing with EINTR
PEP 479: Change StopIteration handling inside generators
PEP 485: A function for testing approximate equality
PEP 486: Make the Python Launcher aware of virtual environments
PEP 488: Elimination of PYO files
PEP 489: Multi-phase extension module initialization

◦ Other Language Changes
◦ New Modules
◦ Improved Modules
◦ Other module-level changes
◦ Optimizations
◦ Build and C API Changes
◦ Deprecated
◦ Removed
◦ Porting to Python 3.5
◦ Notable changes in Python 3.5.4

What’s New In Python 3.5
Editors:
Elvis Pranskevichus <elvis@magic.io>, Yury Selivanov <yury@magic.io> 

This article explains the new features in Python 3.5, compared to 3.4. Python 3.5 was released on September 13, 2015.  See the changelog for a full list of changes.

See also:
 PEP 478 - Python 3.5 Release Schedule
 

### ===🗝 Summary – Release highlights

New syntax features:

• PEP 492, coroutines with async and await syntax.
• PEP 465, a new matrix multiplication operator: a @ b.
• PEP 448, additional unpacking generalizations.

New library modules:

• typing: PEP 484 – Type Hints.
• zipapp: PEP 441 Improving Python ZIP Application Support.

New built-in features:

• bytes % args, bytearray % args: PEP 461 – Adding % formatting to bytes and bytearray.

• New bytes.hex(), bytearray.hex() and memoryview.hex() methods. (Contributed by Arnon Yaari in bpo-9951.)

• memoryview now supports tuple indexing (including multi-dimensional). (Contributed by Antoine Pitrou in bpo-23632.)

• Generators have a new gi_yieldfrom attribute, which returns the object being iterated by yield from expressions. (Contributed by Benno Leslie and Yury Selivanov in bpo-24450.)

• A new RecursionError exception is now raised when maximum recursion depth is reached. (Contributed by Georg Brandl in bpo-19235.)

CPython implementation improvements:

• When the LC_TYPE locale is the POSIX locale (C locale), sys.stdin and sys.stdout now use the surrogateescape error handler, instead of the strict error handler. (Contributed by Victor Stinner in bpo-19977.)

• .pyo files are no longer used and have been replaced by a more flexible scheme that includes the optimization level explicitly in .pyc name. (See PEP 488 overview.)

• Builtin and extension modules are now initialized in a multi-phase process, which is similar to how Python modules are loaded. (See PEP 489 overview.)

Significant improvements in the standard library:

• collections.OrderedDict is now implemented in C, which makes it 4 to 100 times faster.

• The ssl module gained support for Memory BIO, which decouples SSL protocol handling from network IO.

• The new os.scandir() function provides a better and significantly faster way of directory traversal.

• functools.lru_cache() has been mostly reimplemented in C, yielding much better performance.

• The new subprocess.run() function provides a streamlined way to run subprocesses.

• The traceback module has been significantly enhanced for improved performance and developer convenience.

Security improvements:

• SSLv3 is now disabled throughout the standard library. It can still be enabled by instantiating a ssl.SSLContext manually. (See bpo-22638 for more details; this change was backported to CPython 3.4 and 2.7.)

• HTTP cookie parsing is now stricter, in order to protect against potential injection attacks. (Contributed by Antoine Pitrou in bpo-22796.)

Windows improvements:

• A new installer for Windows has replaced the old MSI. See Using Python on Windows for more information.

• Windows builds now use Microsoft Visual C++ 14.0, and extension modules should use the same.

Please read on for a comprehensive list of user-facing changes, including many other smaller improvements, CPython optimizations, deprecations, and potential porting issues.


### ===🗝 New Features


#### PEP 492 - Coroutines with async and await syntax

PEP 492 greatly improves support for asynchronous programming in Python by adding awaitable objects, coroutine functions, asynchronous iteration, and asynchronous context managers.

Coroutine functions are declared using the new async def syntax:


>>> async def coro():
...     return 'spam'


Inside a coroutine function, the new await expression can be used to suspend coroutine execution until the result is available. Any object can be awaited, as long as it implements the awaitable protocol by defining the __await__() method.

PEP 492 also adds async for statement for convenient iteration over asynchronous iterables.

An example of a rudimentary HTTP client written using the new syntax:


```py
import asyncio

async def http_get(domain):
    reader, writer = await asyncio.open_connection(domain, 80)

    writer.write(b'\r\n'.join([
        b'GET / HTTP/1.1',
        b'Host: %b' % domain.encode('latin-1'),
        b'Connection: close',
        b'', b''
    ]))

    async for line in reader:
        print('>>>', line)

    writer.close()

loop = asyncio.get_event_loop()
try:
    loop.run_until_complete(http_get('example.com'))
finally:
    loop.close()

```

Similarly to asynchronous iteration, there is a new syntax for asynchronous context managers. The following script:


```py
import asyncio

async def coro(name, lock):
    print('coro {}: waiting for lock'.format(name))
    async with lock:
        print('coro {}: holding the lock'.format(name))
        await asyncio.sleep(1)
        print('coro {}: releasing the lock'.format(name))

loop = asyncio.get_event_loop()
lock = asyncio.Lock()
coros = asyncio.gather(coro(1, lock), coro(2, lock))
try:
    loop.run_until_complete(coros)
finally:
    loop.close()

```

will output:

>
coro 2: waiting for lock
coro 2: holding the lock
coro 1: waiting for lock
coro 2: releasing the lock
coro 1: holding the lock
coro 1: releasing the lock


Note that both async for and async with can only be used inside a coroutine function declared with async def.

Coroutine functions are intended to be run inside a compatible event loop, such as the asyncio loop.

Note:
 

Changed in version 3.5.2: Starting with CPython 3.5.2, __aiter__ can directly return asynchronous iterators. Returning an awaitable object will result in a PendingDeprecationWarning.

See more details in the Asynchronous Iterators documentation section.

See also:
 PEP 492 – Coroutines with async and await syntaxPEP written and implemented by Yury Selivanov.

#### PEP 465 - A dedicated infix operator for matrix multiplication

PEP 465 adds the @ infix operator for matrix multiplication. Currently, no builtin Python types implement the new operator, however, it can be implemented by defining __matmul__(), __rmatmul__(), and __imatmul__() for regular, reflected, and in-place matrix multiplication. The semantics of these methods is similar to that of methods defining other infix arithmetic operators.

Matrix multiplication is a notably common operation in many fields of mathematics, science, engineering, and the addition of @ allows writing cleaner code:


S = (H @ beta - r).T @ inv(H @ V @ H.T) @ (H @ beta - r)


instead of:


S = dot((dot(H, beta) - r).T,
        dot(inv(dot(dot(H, V), H.T)), dot(H, beta) - r))


NumPy 1.10 has support for the new operator:


>>> import numpy

>>> x = numpy.ones(3)
>>> x
array([ 1., 1., 1.])

>>> m = numpy.eye(3)
>>> m
array([[ 1., 0., 0.],
       [ 0., 1., 0.],
       [ 0., 0., 1.]])

>>> x @ m
array([ 1., 1., 1.])


See also:
 PEP 465 – A dedicated infix operator for matrix multiplicationPEP written by Nathaniel J. Smith; implemented by Benjamin Peterson.

#### PEP 448 - Additional Unpacking Generalizations

PEP 448 extends the allowed uses of the * iterable unpacking operator and ** dictionary unpacking operator. It is now possible to use an arbitrary number of unpackings in function calls:


>>> print(*[1], *[2], 3, *[4, 5])
1 2 3 4 5
>>> def fn(a, b, c, d):
...     print(a, b, c, d)
...

>>> fn(**{'a': 1, 'c': 3}, **{'b': 2, 'd': 4})
1 2 3 4


Similarly, tuple, list, set, and dictionary displays allow multiple unpackings (see Expression lists and Dictionary displays):


>>> *range(4), 4
(0, 1, 2, 3, 4)
>>> [* range(4), 4]
[0, 1, 2, 3, 4]
>>> {* range(4), 4, *(5, 6, 7)}
{0, 1, 2, 3, 4, 5, 6, 7}
>>> {'x': 1, ** {'y': 2}}
{'x': 1, 'y': 2}


See also:
 PEP 448 – Additional Unpacking GeneralizationsPEP written by Joshua Landau; implemented by Neil Girdhar, Thomas Wouters, and Joshua Landau.

#### PEP 461 - percent formatting support for bytes and bytearray

PEP 461 adds support for the % interpolation operator to bytes and bytearray.

While interpolation is usually thought of as a string operation, there are cases where interpolation on bytes or bytearrays makes sense, and the work needed to make up for this missing functionality detracts from the overall readability of the code. This issue is particularly important when dealing with wire format protocols, which are often a mixture of binary and ASCII compatible text.

Examples:


>>> b'Hello %b!' % b'World'
b'Hello World!'

>>> b'x=%i y=%f' % (1, 2.5)
b'x=1 y=2.500000'


Unicode is not allowed for %b, but it is accepted by %a (equivalent of repr(obj).encode('ascii', 'backslashreplace')):


>>> b'Hello %b!' % 'World'
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: %b requires bytes, or an object that implements __bytes__, not 'str'

>>> b'price: %a' % '10€'
b"price: '10\\u20ac'"


Note that %s and %r conversion types, although supported, should only be used in codebases that need compatibility with Python 2.

See also:
 PEP 461 – Adding % formatting to bytes and bytearrayPEP written by Ethan Furman; implemented by Neil Schemenauer and Ethan Furman.

#### PEP 484 - Type Hints

Function annotation syntax has been a Python feature since version 3.0 (PEP 3107), however the semantics of annotations has been left undefined.

Experience has shown that the majority of function annotation uses were to provide type hints to function parameters and return values. It became evident that it would be beneficial for Python users, if the standard library included the base definitions and tools for type annotations.

PEP 484 introduces a provisional module to provide these standard definitions and tools, along with some conventions for situations where annotations are not available.

For example, here is a simple function whose argument and return type are declared in the annotations:


def greeting(name: str) -> str:
    return 'Hello ' + name


While these annotations are available at runtime through the usual __annotations__ attribute, no automatic type checking happens at runtime. Instead, it is assumed that a separate off-line type checker (e.g. mypy) will be used for on-demand source code analysis.

The type system supports unions, generic types, and a special type named Any which is consistent with (i.e. assignable to and from) all types.

See also:

• typing module documentation

• PEP 484 – Type HintsPEP written by Guido van Rossum, Jukka Lehtosalo, and Łukasz Langa; implemented by Guido van Rossum.

• PEP 483 – The Theory of Type HintsPEP written by Guido van Rossum


#### PEP 471 - os.scandir() function – a better and faster directory iterator

PEP 471 adds a new directory iteration function, os.scandir(), to the standard library. Additionally, os.walk() is now implemented using scandir, which makes it 3 to 5 times faster on POSIX systems and 7 to 20 times faster on Windows systems. This is largely achieved by greatly reducing the number of calls to os.stat() required to walk a directory tree.

Additionally, scandir returns an iterator, as opposed to returning a list of file names, which improves memory efficiency when iterating over very large directories.

The following example shows a simple use of os.scandir() to display all the files (excluding directories) in the given path that don’t start with '.'. The entry.is_file() call will generally not make an additional system call:


for entry in os.scandir(path):
    if not entry.name.startswith('.') and entry.is_file():
        print(entry.name)


See also:
 PEP 471 – os.scandir() function – a better and faster directory iteratorPEP written and implemented by Ben Hoyt with the help of Victor Stinner.

#### PEP 475: Retry system calls failing with EINTR

An errno.EINTR error code is returned whenever a system call, that is waiting for I/O, is interrupted by a signal. Previously, Python would raise InterruptedError in such cases. This meant that, when writing a Python application, the developer had two choices:
1.Ignore the InterruptedError.
2.Handle the InterruptedError and attempt to restart the interrupted system call at every call site.

The first option makes an application fail intermittently. The second option adds a large amount of boilerplate that makes the code nearly unreadable. Compare:


```py
print("Hello World")

```

and:


```py
while True:
    try:
        print("Hello World")
        break
    except InterruptedError:
        continue
```


PEP 475 implements automatic retry of system calls on EINTR. This removes the burden of dealing with EINTR or InterruptedError in user code in most situations and makes Python programs, including the standard library, more robust. Note that the system call is only retried if the signal handler does not raise an exception.

Below is a list of functions which are now retried when interrupted by a signal:

• open() and io.open();

• functions of the faulthandler module;

• os functions: fchdir(), fchmod(), fchown(), fdatasync(), fstat(), fstatvfs(), fsync(), ftruncate(), mkfifo(), mknod(), open(), posix_fadvise(), posix_fallocate(), pread(), pwrite(), read(), readv(), sendfile(), wait3(), wait4(), wait(), waitid(), waitpid(), write(), writev();

• special cases: os.close() and os.dup2() now ignore EINTR errors; the syscall is not retried (see the PEP for the rationale);

• select functions: devpoll.poll(), epoll.poll(), kqueue.control(), poll.poll(), select();

• methods of the socket class: accept(), connect() (except for non-blocking sockets), recv(), recvfrom(), recvmsg(), send(), sendall(), sendmsg(), sendto();

• signal.sigtimedwait() and signal.sigwaitinfo();

• time.sleep().

See also:
 PEP 475 – Retry system calls failing with EINTRPEP and implementation written by Charles-François Natali and Victor Stinner, with the help of Antoine Pitrou (the French connection).

#### PEP 479: Change StopIteration handling inside generators

The interaction of generators and StopIteration in Python 3.4 and earlier was sometimes surprising, and could conceal obscure bugs. Previously, StopIteration raised accidentally inside a generator function was interpreted as the end of the iteration by the loop construct driving the generator.

PEP 479 changes the behavior of generators: when a StopIteration exception is raised inside a generator, it is replaced with a RuntimeError before it exits the generator frame. The main goal of this change is to ease debugging in the situation where an unguarded next() call raises StopIteration and causes the iteration controlled by the generator to terminate silently. This is particularly pernicious in combination with the yield from construct.

This is a backwards incompatible change, so to enable the new behavior, a __future__ import is necessary:


>>> from __future__ import generator_stop

>>> def gen():
...     next(iter([]))
...     yield
...
>>> next(gen())
Traceback (most recent call last):
  File "<stdin>", line 2, in gen
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: generator raised StopIteration


Without a __future__ import, a PendingDeprecationWarning will be raised whenever a StopIteration exception is raised inside a generator.

See also:
 PEP 479 – Change StopIteration handling inside generatorsPEP written by Chris Angelico and Guido van Rossum. Implemented by Chris Angelico, Yury Selivanov and Nick Coghlan.

#### PEP 485: A function for testing approximate equality

PEP 485 adds the math.isclose() and cmath.isclose() functions which tell whether two values are approximately equal or “close” to each other. Whether or not two values are considered close is determined according to given absolute and relative tolerances. Relative tolerance is the maximum allowed difference between isclose arguments, relative to the larger absolute value:


>>> import math
>>> a = 5.0
>>> b = 4.99998
>>> math.isclose(a, b, rel_tol=1e-5)
True
>>> math.isclose(a, b, rel_tol=1e-6)
False


It is also possible to compare two values using absolute tolerance, which must be a non-negative value:


>>> import math
>>> a = 5.0
>>> b = 4.99998
>>> math.isclose(a, b, abs_tol=0.00003)
True
>>> math.isclose(a, b, abs_tol=0.00001)
False


See also:
 PEP 485 – A function for testing approximate equalityPEP written by Christopher Barker; implemented by Chris Barker and Tal Einat.

#### PEP 486: Make the Python Launcher aware of virtual environments

PEP 486 makes the Windows launcher (see PEP 397) aware of an active virtual environment. When the default interpreter would be used and the VIRTUAL_ENV environment variable is set, the interpreter in the virtual environment will be used.

See also:
 PEP 486 – Make the Python Launcher aware of virtual environmentsPEP written and implemented by Paul Moore.

#### PEP 488: Elimination of PYO files

PEP 488 does away with the concept of .pyo files. This means that .pyc files represent both unoptimized and optimized bytecode. To prevent the need to constantly regenerate bytecode files, .pyc files now have an optional opt- tag in their name when the bytecode is optimized. This has the side-effect of no more bytecode file name clashes when running under either -O or -OO. Consequently, bytecode files generated from -O, and -OO may now exist simultaneously. importlib.util.cache_from_source() has an updated API to help with this change.

See also:
 PEP 488 – Elimination of PYO filesPEP written and implemented by Brett Cannon.

#### PEP 489: Multi-phase extension module initialization

PEP 489 updates extension module initialization to take advantage of the two step module loading mechanism introduced by PEP 451 in Python 3.4.

This change brings the import semantics of extension modules that opt-in to using the new mechanism much closer to those of Python source and bytecode modules, including the ability to use any valid identifier as a module name, rather than being restricted to ASCII.

See also:
 PEP 489 – Multi-phase extension module initializationPEP written by Petr Viktorin, Stefan Behnel, and Nick Coghlan; implemented by Petr Viktorin.

### ===🗝 Other Language Changes

Some smaller changes made to the core Python language are:

• Added the "namereplace" error handlers. The "backslashreplace" error handlers now work with decoding and translating. (Contributed by Serhiy Storchaka in bpo-19676 and bpo-22286.)

• The -b option now affects comparisons of bytes with int. (Contributed by Serhiy Storchaka in bpo-23681.)

• New Kazakh kz1048 and Tajik koi8_t codecs. (Contributed by Serhiy Storchaka in bpo-22682 and bpo-22681.)

• Property docstrings are now writable. This is especially useful for collections.namedtuple() docstrings. (Contributed by Berker Peksag in bpo-24064.)

• Circular imports involving relative imports are now supported. (Contributed by Brett Cannon and Antoine Pitrou in bpo-17636.)


### ===🗝 New Modules


typing

The new typing provisional module provides standard definitions and tools for function type annotations. See Type Hints for more information.


zipapp

The new zipapp module (specified in PEP 441) provides an API and command line tool for creating executable Python Zip Applications, which were introduced in Python 2.6 in bpo-1739468, but which were not well publicized, either at the time or since.

With the new module, bundling your application is as simple as putting all the files, including a __main__.py file, into a directory myapp and running:


$ python -m zipapp myapp
$ python myapp.pyz


The module implementation has been contributed by Paul Moore in bpo-23491.

See also:
 PEP 441 – Improving Python ZIP Application Support
 


### ===🗝 Improved Modules


argparse

The ArgumentParser class now allows disabling abbreviated usage of long options by setting allow_abbrev to False. (Contributed by Jonathan Paugh, Steven Bethard, paul j3 and Daniel Eriksson in bpo-14910.)


asyncio

Since the asyncio module is provisional, all changes introduced in Python 3.5 have also been backported to Python 3.4.x.

Notable changes in the asyncio module since Python 3.4.0:

• New debugging APIs: loop.set_debug() and loop.get_debug() methods. (Contributed by Victor Stinner.)

• The proactor event loop now supports SSL. (Contributed by Antoine Pitrou and Victor Stinner in bpo-22560.)

• A new loop.is_closed() method to check if the event loop is closed. (Contributed by Victor Stinner in bpo-21326.)

• A new loop.create_task() to conveniently create and schedule a new Task for a coroutine. The create_task method is also used by all asyncio functions that wrap coroutines into tasks, such as asyncio.wait(), asyncio.gather(), etc. (Contributed by Victor Stinner.)

• A new transport.get_write_buffer_limits() method to inquire for high- and low- water limits of the flow control. (Contributed by Victor Stinner.)

• The async() function is deprecated in favor of ensure_future(). (Contributed by Yury Selivanov.)

• New loop.set_task_factory() and loop.get_task_factory() methods to customize the task factory that loop.create_task() method uses. (Contributed by Yury Selivanov.)

• New Queue.join() and Queue.task_done() queue methods. (Contributed by Victor Stinner.)

• The JoinableQueue class was removed, in favor of the asyncio.Queue class. (Contributed by Victor Stinner.)

Updates in 3.5.1:

• The ensure_future() function and all functions that use it, such as loop.run_until_complete(), now accept all kinds of awaitable objects. (Contributed by Yury Selivanov.)

• New run_coroutine_threadsafe() function to submit coroutines to event loops from other threads. (Contributed by Vincent Michel.)

• New Transport.is_closing() method to check if the transport is closing or closed. (Contributed by Yury Selivanov.)

• The loop.create_server() method can now accept a list of hosts. (Contributed by Yann Sionneau.)

Updates in 3.5.2:

• New loop.create_future() method to create Future objects. This allows alternative event loop implementations, such as uvloop, to provide a faster asyncio.Future implementation. (Contributed by Yury Selivanov.)

• New loop.get_exception_handler() method to get the current exception handler. (Contributed by Yury Selivanov.)

• New StreamReader.readuntil() method to read data from the stream until a separator bytes sequence appears. (Contributed by Mark Korenberg.)

• The loop.create_connection() and loop.create_server() methods are optimized to avoid calling the system getaddrinfo function if the address is already resolved. (Contributed by A. Jesse Jiryu Davis.)

• The loop.sock_connect(sock, address) no longer requires the address to be resolved prior to the call. (Contributed by A. Jesse Jiryu Davis.)


bz2

The BZ2Decompressor.decompress method now accepts an optional max_length argument to limit the maximum size of decompressed data. (Contributed by Nikolaus Rath in bpo-15955.)


cgi

The FieldStorage class now supports the context manager protocol. (Contributed by Berker Peksag in bpo-20289.)


cmath

A new function isclose() provides a way to test for approximate equality. (Contributed by Chris Barker and Tal Einat in bpo-24270.)


code

The InteractiveInterpreter.showtraceback() method now prints the full chained traceback, just like the interactive interpreter. (Contributed by Claudiu Popa in bpo-17442.)


collections

The OrderedDict class is now implemented in C, which makes it 4 to 100 times faster. (Contributed by Eric Snow in bpo-16991.)

OrderedDict.items(), OrderedDict.keys(), OrderedDict.values() views now support reversed() iteration. (Contributed by Serhiy Storchaka in bpo-19505.)

The deque class now defines index(), insert(), and copy(), and supports the + and * operators. This allows deques to be recognized as a MutableSequence and improves their substitutability for lists. (Contributed by Raymond Hettinger in bpo-23704.)

Docstrings produced by namedtuple() can now be updated:


Point = namedtuple('Point', ['x', 'y'])
Point.__doc__ += ': Cartesian coodinate'
Point.x.__doc__ = 'abscissa'
Point.y.__doc__ = 'ordinate'


(Contributed by Berker Peksag in bpo-24064.)

The UserString class now implements the __getnewargs__(), __rmod__(), casefold(), format_map(), isprintable(), and maketrans() methods to match the corresponding methods of str. (Contributed by Joe Jevnik in bpo-22189.)


collections.abc

The Sequence.index() method now accepts start and stop arguments to match the corresponding methods of tuple, list, etc. (Contributed by Devin Jeanpierre in bpo-23086.)

A new Generator abstract base class. (Contributed by Stefan Behnel in bpo-24018.)

New Awaitable, Coroutine, AsyncIterator, and AsyncIterable abstract base classes. (Contributed by Yury Selivanov in bpo-24184.)

For earlier Python versions, a backport of the new ABCs is available in an external PyPI package.


compileall

A new compileall option, -j N, allows running N workers simultaneously to perform parallel bytecode compilation. The compile_dir() function has a corresponding workers parameter. (Contributed by Claudiu Popa in bpo-16104.)

Another new option, -r, allows controlling the maximum recursion level for subdirectories. (Contributed by Claudiu Popa in bpo-19628.)

The -q command line option can now be specified more than once, in which case all output, including errors, will be suppressed. The corresponding quiet parameter in compile_dir(), compile_file(), and compile_path() can now accept an integer value indicating the level of output suppression. (Contributed by Thomas Kluyver in bpo-21338.)


concurrent.futures

The Executor.map() method now accepts a chunksize argument to allow batching of tasks to improve performance when ProcessPoolExecutor() is used. (Contributed by Dan O’Reilly in bpo-11271.)

The number of workers in the ThreadPoolExecutor constructor is optional now. The default value is 5 times the number of CPUs. (Contributed by Claudiu Popa in bpo-21527.)


configparser

configparser now provides a way to customize the conversion of values by specifying a dictionary of converters in the ConfigParser constructor, or by defining them as methods in ConfigParser subclasses. Converters defined in a parser instance are inherited by its section proxies.

Example:


>>> import configparser
>>> conv = {}
>>> conv['list'] = lambda v: [e.strip() for e in v.split() if e.strip()]
>>> cfg = configparser.ConfigParser(converters=conv)
>>> cfg.read_string("""
... [s]
... list = a b c d e f g
... """)
>>> cfg.get('s', 'list')
'a b c d e f g'
>>> cfg.getlist('s', 'list')
['a', 'b', 'c', 'd', 'e', 'f', 'g']
>>> section = cfg['s']
>>> section.getlist('list')
['a', 'b', 'c', 'd', 'e', 'f', 'g']


(Contributed by Łukasz Langa in bpo-18159.)


contextlib

The new redirect_stderr() context manager (similar to redirect_stdout()) makes it easier for utility scripts to handle inflexible APIs that write their output to sys.stderr and don’t provide any options to redirect it:


>>> import contextlib, io, logging
>>> f = io.StringIO()
>>> with contextlib.redirect_stderr(f):
...     logging.warning('warning')
...
>>> f.getvalue()
'WARNING:root:warning\n'


(Contributed by Berker Peksag in bpo-22389.)


csv

The writerow() method now supports arbitrary iterables, not just sequences. (Contributed by Serhiy Storchaka in bpo-23171.)


curses

The new update_lines_cols() function updates the LINES and COLS environment variables. This is useful for detecting manual screen resizing. (Contributed by Arnon Yaari in bpo-4254.)


dbm

dumb.open always creates a new database when the flag has the value "n". (Contributed by Claudiu Popa in bpo-18039.)


difflib

The charset of HTML documents generated by HtmlDiff.make_file() can now be customized by using a new charset keyword-only argument. The default charset of HTML document changed from "ISO-8859-1" to "utf-8". (Contributed by Berker Peksag in bpo-2052.)

The diff_bytes() function can now compare lists of byte strings. This fixes a regression from Python 2. (Contributed by Terry J. Reedy and Greg Ward in bpo-17445.)


distutils

Both the build and build_ext commands now accept a -j option to enable parallel building of extension modules. (Contributed by Antoine Pitrou in bpo-5309.)

The distutils module now supports xz compression, and can be enabled by passing xztar as an argument to bdist --format. (Contributed by Serhiy Storchaka in bpo-16314.)


doctest

The DocTestSuite() function returns an empty unittest.TestSuite if module contains no docstrings, instead of raising ValueError. (Contributed by Glenn Jones in bpo-15916.)


email

A new policy option Policy.mangle_from_ controls whether or not lines that start with "From " in email bodies are prefixed with a ">" character by generators. The default is True for compat32 and False for all other policies. (Contributed by Milan Oberkirch in bpo-20098.)

A new Message.get_content_disposition() method provides easy access to a canonical value for the Content-Disposition header. (Contributed by Abhilash Raj in bpo-21083.)

A new policy option EmailPolicy.utf8 can be set to True to encode email headers using the UTF-8 charset instead of using encoded words. This allows Messages to be formatted according to RFC 6532 and used with an SMTP server that supports the RFC 6531 SMTPUTF8 extension. (Contributed by R. David Murray in bpo-24211.)

The mime.text.MIMEText constructor now accepts a charset.Charset instance. (Contributed by Claude Paroz and Berker Peksag in bpo-16324.)


enum

The Enum callable has a new parameter start to specify the initial number of enum values if only names are provided:


>>> Animal = enum.Enum('Animal', 'cat dog', start=10)
>>> Animal.cat
<Animal.cat: 10>
>>> Animal.dog
<Animal.dog: 11>


(Contributed by Ethan Furman in bpo-21706.)


faulthandler

The enable(), register(), dump_traceback() and dump_traceback_later() functions now accept file descriptors in addition to file-like objects. (Contributed by Wei Wu in bpo-23566.)


functools

Most of the lru_cache() machinery is now implemented in C, making it significantly faster. (Contributed by Matt Joiner, Alexey Kachayev, and Serhiy Storchaka in bpo-14373.)


glob

The iglob() and glob() functions now support recursive search in subdirectories, using the "**" pattern. (Contributed by Serhiy Storchaka in bpo-13968.)


gzip

The mode argument of the GzipFile constructor now accepts "x" to request exclusive creation. (Contributed by Tim Heaney in bpo-19222.)


heapq

Element comparison in merge() can now be customized by passing a key function in a new optional key keyword argument, and a new optional reverse keyword argument can be used to reverse element comparison:


>>> import heapq
>>> a = ['9', '777', '55555']
>>> b = ['88', '6666']
>>> list(heapq.merge(a, b, key=len))
['9', '88', '777', '6666', '55555']
>>> list(heapq.merge(reversed(a), reversed(b), key=len, reverse=True))
['55555', '6666', '777', '88', '9']


(Contributed by Raymond Hettinger in bpo-13742.)


http

A new HTTPStatus enum that defines a set of HTTP status codes, reason phrases and long descriptions written in English. (Contributed by Demian Brecht in bpo-21793.)


http.client

HTTPConnection.getresponse() now raises a RemoteDisconnected exception when a remote server connection is closed unexpectedly. Additionally, if a ConnectionError (of which RemoteDisconnected is a subclass) is raised, the client socket is now closed automatically, and will reconnect on the next request:


import http.client
conn = http.client.HTTPConnection('www.python.org')
for retries in range(3):
    try:
        conn.request('GET', '/')
        resp = conn.getresponse()
    except http.client.RemoteDisconnected:
        pass


(Contributed by Martin Panter in bpo-3566.)


idlelib and IDLE

Since idlelib implements the IDLE shell and editor and is not intended for import by other programs, it gets improvements with every release. See Lib/idlelib/NEWS.txt for a cumulative list of changes since 3.4.0, as well as changes made in future 3.5.x releases. This file is also available from the IDLE Help ‣ About IDLE dialog.


imaplib

The IMAP4 class now supports the context manager protocol. When used in a with statement, the IMAP4 LOGOUT command will be called automatically at the end of the block. (Contributed by Tarek Ziadé and Serhiy Storchaka in bpo-4972.)

The imaplib module now supports RFC 5161 (ENABLE Extension) and RFC 6855 (UTF-8 Support) via the IMAP4.enable() method. A new IMAP4.utf8_enabled attribute tracks whether or not RFC 6855 support is enabled. (Contributed by Milan Oberkirch, R. David Murray, and Maciej Szulik in bpo-21800.)

The imaplib module now automatically encodes non-ASCII string usernames and passwords using UTF-8, as recommended by the RFCs. (Contributed by Milan Oberkirch in bpo-21800.)


imghdr

The what() function now recognizes the OpenEXR format (contributed by Martin Vignali and Claudiu Popa in bpo-20295), and the WebP format (contributed by Fabrice Aneche and Claudiu Popa in bpo-20197.)


importlib

The util.LazyLoader class allows for lazy loading of modules in applications where startup time is important. (Contributed by Brett Cannon in bpo-17621.)

The abc.InspectLoader.source_to_code() method is now a static method. This makes it easier to initialize a module object with code compiled from a string by running exec(code, module.__dict__). (Contributed by Brett Cannon in bpo-21156.)

The new util.module_from_spec() function is now the preferred way to create a new module. As opposed to creating a types.ModuleType instance directly, this new function will set the various import-controlled attributes based on the passed-in spec object. (Contributed by Brett Cannon in bpo-20383.)


inspect

Both the Signature and Parameter classes are now picklable and hashable. (Contributed by Yury Selivanov in bpo-20726 and bpo-20334.)

A new BoundArguments.apply_defaults() method provides a way to set default values for missing arguments:


>>> def foo(a, b='ham', *args): pass
>>> ba = inspect.signature(foo).bind('spam')
>>> ba.apply_defaults()
>>> ba.arguments
OrderedDict([('a', 'spam'), ('b', 'ham'), ('args', ())])


(Contributed by Yury Selivanov in bpo-24190.)

A new class method Signature.from_callable() makes subclassing of Signature easier. (Contributed by Yury Selivanov and Eric Snow in bpo-17373.)

The signature() function now accepts a follow_wrapped optional keyword argument, which, when set to False, disables automatic following of __wrapped__ links. (Contributed by Yury Selivanov in bpo-20691.)

A set of new functions to inspect coroutine functions and coroutine objects has been added: iscoroutine(), iscoroutinefunction(), isawaitable(), getcoroutinelocals(), and getcoroutinestate(). (Contributed by Yury Selivanov in bpo-24017 and bpo-24400.)

The stack(), trace(), getouterframes(), and getinnerframes() functions now return a list of named tuples. (Contributed by Daniel Shahaf in bpo-16808.)


io

A new BufferedIOBase.readinto1() method, that uses at most one call to the underlying raw stream’s RawIOBase.read() or RawIOBase.readinto() methods. (Contributed by Nikolaus Rath in bpo-20578.)


ipaddress

Both the IPv4Network and IPv6Network classes now accept an (address, netmask) tuple argument, so as to easily construct network objects from existing addresses:


>>> import ipaddress
>>> ipaddress.IPv4Network(('127.0.0.0', 8))
IPv4Network('127.0.0.0/8')
>>> ipaddress.IPv4Network(('127.0.0.0', '255.0.0.0'))
IPv4Network('127.0.0.0/8')


(Contributed by Peter Moody and Antoine Pitrou in bpo-16531.)

A new reverse_pointer attribute for the IPv4Network and IPv6Network classes returns the name of the reverse DNS PTR record:


>>> import ipaddress
>>> addr = ipaddress.IPv4Address('127.0.0.1')
>>> addr.reverse_pointer
'1.0.0.127.in-addr.arpa'
>>> addr6 = ipaddress.IPv6Address('::1')
>>> addr6.reverse_pointer
'1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa'


(Contributed by Leon Weber in bpo-20480.)


json

The json.tool command line interface now preserves the order of keys in JSON objects passed in input. The new --sort-keys option can be used to sort the keys alphabetically. (Contributed by Berker Peksag in bpo-21650.)

JSON decoder now raises JSONDecodeError instead of ValueError to provide better context information about the error. (Contributed by Serhiy Storchaka in bpo-19361.)


linecache

A new lazycache() function can be used to capture information about a non-file-based module to permit getting its lines later via getline(). This avoids doing I/O until a line is actually needed, without having to carry the module globals around indefinitely. (Contributed by Robert Collins in bpo-17911.)


locale

A new delocalize() function can be used to convert a string into a normalized number string, taking the LC_NUMERIC settings into account:


>>> import locale
>>> locale.setlocale(locale.LC_NUMERIC, 'de_DE.UTF-8')
'de_DE.UTF-8'
>>> locale.delocalize('1.234,56')
'1234.56'
>>> locale.setlocale(locale.LC_NUMERIC, 'en_US.UTF-8')
'en_US.UTF-8'
>>> locale.delocalize('1,234.56')
'1234.56'


(Contributed by Cédric Krier in bpo-13918.)


logging

All logging methods (Logger log(), exception(), critical(), debug(), etc.), now accept exception instances as an exc_info argument, in addition to boolean values and exception tuples:


>>> import logging
>>> try:
...     1/0
... except ZeroDivisionError as ex:
...     logging.error('exception', exc_info=ex)
ERROR:root:exception


(Contributed by Yury Selivanov in bpo-20537.)

The handlers.HTTPHandler class now accepts an optional ssl.SSLContext instance to configure SSL settings used in an HTTP connection. (Contributed by Alex Gaynor in bpo-22788.)

The handlers.QueueListener class now takes a respect_handler_level keyword argument which, if set to True, will pass messages to handlers taking handler levels into account. (Contributed by Vinay Sajip.)


lzma

The LZMADecompressor.decompress() method now accepts an optional max_length argument to limit the maximum size of decompressed data. (Contributed by Martin Panter in bpo-15955.)


math

Two new constants have been added to the math module: inf and nan. (Contributed by Mark Dickinson in bpo-23185.)

A new function isclose() provides a way to test for approximate equality. (Contributed by Chris Barker and Tal Einat in bpo-24270.)

A new gcd() function has been added. The fractions.gcd() function is now deprecated. (Contributed by Mark Dickinson and Serhiy Storchaka in bpo-22486.)


multiprocessing

sharedctypes.synchronized() objects now support the context manager protocol. (Contributed by Charles-François Natali in bpo-21565.)


operator

attrgetter(), itemgetter(), and methodcaller() objects now support pickling. (Contributed by Josh Rosenberg and Serhiy Storchaka in bpo-22955.)

New matmul() and imatmul() functions to perform matrix multiplication. (Contributed by Benjamin Peterson in bpo-21176.)


os

The new scandir() function returning an iterator of DirEntry objects has been added. If possible, scandir() extracts file attributes while scanning a directory, removing the need to perform subsequent system calls to determine file type or attributes, which may significantly improve performance. (Contributed by Ben Hoyt with the help of Victor Stinner in bpo-22524.)

On Windows, a new stat_result.st_file_attributes attribute is now available. It corresponds to the dwFileAttributes member of the BY_HANDLE_FILE_INFORMATION structure returned by GetFileInformationByHandle(). (Contributed by Ben Hoyt in bpo-21719.)

The urandom() function now uses the getrandom() syscall on Linux 3.17 or newer, and getentropy() on OpenBSD 5.6 and newer, removing the need to use /dev/urandom and avoiding failures due to potential file descriptor exhaustion. (Contributed by Victor Stinner in bpo-22181.)

New get_blocking() and set_blocking() functions allow getting and setting a file descriptor’s blocking mode (O_NONBLOCK.) (Contributed by Victor Stinner in bpo-22054.)

The truncate() and ftruncate() functions are now supported on Windows. (Contributed by Steve Dower in bpo-23668.)

There is a new os.path.commonpath() function returning the longest common sub-path of each passed pathname. Unlike the os.path.commonprefix() function, it always returns a valid path:


>>> os.path.commonprefix(['/usr/lib', '/usr/local/lib'])
'/usr/l'

>>> os.path.commonpath(['/usr/lib', '/usr/local/lib'])
'/usr'


(Contributed by Rafik Draoui and Serhiy Storchaka in bpo-10395.)


pathlib

The new Path.samefile() method can be used to check whether the path points to the same file as another path, which can be either another Path object, or a string:


>>> import pathlib
>>> p1 = pathlib.Path('/etc/hosts')
>>> p2 = pathlib.Path('/etc/../etc/hosts')
>>> p1.samefile(p2)
True


(Contributed by Vajrasky Kok and Antoine Pitrou in bpo-19775.)

The Path.mkdir() method now accepts a new optional exist_ok argument to match mkdir -p and os.makedirs() functionality. (Contributed by Berker Peksag in bpo-21539.)

There is a new Path.expanduser() method to expand ~ and ~user prefixes. (Contributed by Serhiy Storchaka and Claudiu Popa in bpo-19776.)

A new Path.home() class method can be used to get a Path instance representing the user’s home directory. (Contributed by Victor Salgado and Mayank Tripathi in bpo-19777.)

New Path.write_text(), Path.read_text(), Path.write_bytes(), Path.read_bytes() methods to simplify read/write operations on files.

The following code snippet will create or rewrite existing file ~/spam42:


>>> import pathlib
>>> p = pathlib.Path('~/spam42')
>>> p.expanduser().write_text('ham')
3


(Contributed by Christopher Welborn in bpo-20218.)


pickle

Nested objects, such as unbound methods or nested classes, can now be pickled using pickle protocols older than protocol version 4. Protocol version 4 already supports these cases. (Contributed by Serhiy Storchaka in bpo-23611.)


poplib

A new POP3.utf8() command enables RFC 6856 (Internationalized Email) support, if a POP server supports it. (Contributed by Milan OberKirch in bpo-21804.)


re

References and conditional references to groups with fixed length are now allowed in lookbehind assertions:


>>> import re
>>> pat = re.compile(r'(a|b).(?<=\1)c')
>>> pat.match('aac')
<_sre.SRE_Match object; span=(0, 3), match='aac'>
>>> pat.match('bbc')
<_sre.SRE_Match object; span=(0, 3), match='bbc'>


(Contributed by Serhiy Storchaka in bpo-9179.)

The number of capturing groups in regular expressions is no longer limited to 100. (Contributed by Serhiy Storchaka in bpo-22437.)

The sub() and subn() functions now replace unmatched groups with empty strings instead of raising an exception. (Contributed by Serhiy Storchaka in bpo-1519638.)

The re.error exceptions have new attributes, msg, pattern, pos, lineno, and colno, that provide better context information about the error:


>>> re.compile("""
...     (?x)
...     .++
... """)
Traceback (most recent call last):
   ...
sre_constants.error: multiple repeat at position 16 (line 3, column 7)


(Contributed by Serhiy Storchaka in bpo-22578.)


readline

A new append_history_file() function can be used to append the specified number of trailing elements in history to the given file. (Contributed by Bruno Cauet in bpo-22940.)


selectors

The new DevpollSelector supports efficient /dev/poll polling on Solaris. (Contributed by Giampaolo Rodola’ in bpo-18931.)


shutil

The move() function now accepts a copy_function argument, allowing, for example, the copy() function to be used instead of the default copy2() if there is a need to ignore file metadata when moving. (Contributed by Claudiu Popa in bpo-19840.)

The make_archive() function now supports the xztar format. (Contributed by Serhiy Storchaka in bpo-5411.)


signal

On Windows, the set_wakeup_fd() function now also supports socket handles. (Contributed by Victor Stinner in bpo-22018.)

Various SIG* constants in the signal module have been converted into Enums. This allows meaningful names to be printed during debugging, instead of integer “magic numbers”. (Contributed by Giampaolo Rodola’ in bpo-21076.)


smtpd

Both the SMTPServer and SMTPChannel classes now accept a decode_data keyword argument to determine if the DATA portion of the SMTP transaction is decoded using the "utf-8" codec or is instead provided to the SMTPServer.process_message() method as a byte string. The default is True for backward compatibility reasons, but will change to False in Python 3.6. If decode_data is set to False, the process_message method must be prepared to accept keyword arguments. (Contributed by Maciej Szulik in bpo-19662.)

The SMTPServer class now advertises the 8BITMIME extension (RFC 6152) if decode_data has been set True. If the client specifies BODY=8BITMIME on the MAIL command, it is passed to SMTPServer.process_message() via the mail_options keyword. (Contributed by Milan Oberkirch and R. David Murray in bpo-21795.)

The SMTPServer class now also supports the SMTPUTF8 extension (RFC 6531: Internationalized Email). If the client specified SMTPUTF8 BODY=8BITMIME on the MAIL command, they are passed to SMTPServer.process_message() via the mail_options keyword. It is the responsibility of the process_message method to correctly handle the SMTPUTF8 data. (Contributed by Milan Oberkirch in bpo-21725.)

It is now possible to provide, directly or via name resolution, IPv6 addresses in the SMTPServer constructor, and have it successfully connect. (Contributed by Milan Oberkirch in bpo-14758.)


smtplib

A new SMTP.auth() method provides a convenient way to implement custom authentication mechanisms. (Contributed by Milan Oberkirch in bpo-15014.)

The SMTP.set_debuglevel() method now accepts an additional debuglevel (2), which enables timestamps in debug messages. (Contributed by Gavin Chappell and Maciej Szulik in bpo-16914.)

Both the SMTP.sendmail() and SMTP.send_message() methods now support RFC 6531 (SMTPUTF8). (Contributed by Milan Oberkirch and R. David Murray in bpo-22027.)


sndhdr

The what() and whathdr() functions now return a namedtuple(). (Contributed by Claudiu Popa in bpo-18615.)


socket

Functions with timeouts now use a monotonic clock, instead of a system clock. (Contributed by Victor Stinner in bpo-22043.)

A new socket.sendfile() method allows sending a file over a socket by using the high-performance os.sendfile() function on UNIX, resulting in uploads being from 2 to 3 times faster than when using plain socket.send(). (Contributed by Giampaolo Rodola’ in bpo-17552.)

The socket.sendall() method no longer resets the socket timeout every time bytes are received or sent. The socket timeout is now the maximum total duration to send all data. (Contributed by Victor Stinner in bpo-23853.)

The backlog argument of the socket.listen() method is now optional. By default it is set to SOMAXCONN or to 128, whichever is less. (Contributed by Charles-François Natali in bpo-21455.)


ssl


#### Memory BIO Support

(Contributed by Geert Jansen in bpo-21965.)

The new SSLObject class has been added to provide SSL protocol support for cases when the network I/O capabilities of SSLSocket are not necessary or are suboptimal. SSLObject represents an SSL protocol instance, but does not implement any network I/O methods, and instead provides a memory buffer interface. The new MemoryBIO class can be used to pass data between Python and an SSL protocol instance.

The memory BIO SSL support is primarily intended to be used in frameworks implementing asynchronous I/O for which SSLSocket’s readiness model (“select/poll”) is inefficient.

A new SSLContext.wrap_bio() method can be used to create a new SSLObject instance.


Application-Layer Protocol Negotiation Support

(Contributed by Benjamin Peterson in bpo-20188.)

Where OpenSSL support is present, the ssl module now implements the Application-Layer Protocol Negotiation TLS extension as described in RFC 7301.

The new SSLContext.set_alpn_protocols() can be used to specify which protocols a socket should advertise during the TLS handshake.

The new SSLSocket.selected_alpn_protocol() returns the protocol that was selected during the TLS handshake. The HAS_ALPN flag indicates whether ALPN support is present.


#### Other Changes

There is a new SSLSocket.version() method to query the actual protocol version in use. (Contributed by Antoine Pitrou in bpo-20421.)

The SSLSocket class now implements a SSLSocket.sendfile() method. (Contributed by Giampaolo Rodola’ in bpo-17552.)

The SSLSocket.send() method now raises either the ssl.SSLWantReadError or ssl.SSLWantWriteError exception on a non-blocking socket if the operation would block. Previously, it would return 0. (Contributed by Nikolaus Rath in bpo-20951.)

The cert_time_to_seconds() function now interprets the input time as UTC and not as local time, per RFC 5280. Additionally, the return value is always an int. (Contributed by Akira Li in bpo-19940.)

New SSLObject.shared_ciphers() and SSLSocket.shared_ciphers() methods return the list of ciphers sent by the client during the handshake. (Contributed by Benjamin Peterson in bpo-23186.)

The SSLSocket.do_handshake(), SSLSocket.read(), SSLSocket.shutdown(), and SSLSocket.write() methods of the SSLSocket class no longer reset the socket timeout every time bytes are received or sent. The socket timeout is now the maximum total duration of the method. (Contributed by Victor Stinner in bpo-23853.)

The match_hostname() function now supports matching of IP addresses. (Contributed by Antoine Pitrou in bpo-23239.)


sqlite3

The Row class now fully supports the sequence protocol, in particular reversed() iteration and slice indexing. (Contributed by Claudiu Popa in bpo-10203; by Lucas Sinclair, Jessica McKellar, and Serhiy Storchaka in bpo-13583.)


subprocess

The new run() function has been added. It runs the specified command and returns a CompletedProcess object, which describes a finished process. The new API is more consistent and is the recommended approach to invoking subprocesses in Python code that does not need to maintain compatibility with earlier Python versions. (Contributed by Thomas Kluyver in bpo-23342.)

Examples:


>>> subprocess.run(["ls", "-l"])  # doesn't capture output
CompletedProcess(args=['ls', '-l'], returncode=0)

>>> subprocess.run("exit 1", shell=True, check=True)
Traceback (most recent call last):
  ...
subprocess.CalledProcessError: Command 'exit 1' returned non-zero exit status 1

>>> subprocess.run(["ls", "-l", "/dev/null"], stdout=subprocess.PIPE)
CompletedProcess(args=['ls', '-l', '/dev/null'], returncode=0,
stdout=b'crw-rw-rw- 1 root root 1, 3 Jan 23 16:23 /dev/null\n')



sys

A new set_coroutine_wrapper() function allows setting a global hook that will be called whenever a coroutine object is created by an async def function. A corresponding get_coroutine_wrapper() can be used to obtain a currently set wrapper. Both functions are provisional, and are intended for debugging purposes only. (Contributed by Yury Selivanov in bpo-24017.)

A new is_finalizing() function can be used to check if the Python interpreter is shutting down. (Contributed by Antoine Pitrou in bpo-22696.)


sysconfig

The name of the user scripts directory on Windows now includes the first two components of the Python version. (Contributed by Paul Moore in bpo-23437.)


tarfile

The mode argument of the open() function now accepts "x" to request exclusive creation. (Contributed by Berker Peksag in bpo-21717.)

The TarFile.extractall() and TarFile.extract() methods now take a keyword argument numeric_owner. If set to True, the extracted files and directories will be owned by the numeric uid and gid from the tarfile. If set to False (the default, and the behavior in versions prior to 3.5), they will be owned by the named user and group in the tarfile. (Contributed by Michael Vogt and Eric Smith in bpo-23193.)

The TarFile.list() now accepts an optional members keyword argument that can be set to a subset of the list returned by TarFile.getmembers(). (Contributed by Serhiy Storchaka in bpo-21549.)


threading

Both the Lock.acquire() and RLock.acquire() methods now use a monotonic clock for timeout management. (Contributed by Victor Stinner in bpo-22043.)


time

The monotonic() function is now always available. (Contributed by Victor Stinner in bpo-22043.)


timeit

A new command line option -u or --unit=U can be used to specify the time unit for the timer output. Supported options are usec, msec, or sec. (Contributed by Julian Gindi in bpo-18983.)

The timeit() function has a new globals parameter for specifying the namespace in which the code will be running. (Contributed by Ben Roberts in bpo-2527.)


tkinter

The `tkinter._fix` module used for setting up the Tcl/Tk environment on Windows has been replaced by a private function in the `_tkinter` module which makes no permanent changes to environment variables. (Contributed by Zachary Ware in bpo-20035.)


traceback

New walk_stack() and walk_tb() functions to conveniently traverse frame and traceback objects. (Contributed by Robert Collins in bpo-17911.)

New lightweight classes: TracebackException, StackSummary, and FrameSummary. (Contributed by Robert Collins in bpo-17911.)

Both the print_tb() and print_stack() functions now support negative values for the limit argument. (Contributed by Dmitry Kazakov in bpo-22619.)


types

A new coroutine() function to transform generator and generator-like objects into awaitables. (Contributed by Yury Selivanov in bpo-24017.)

A new type called CoroutineType, which is used for coroutine objects created by async def functions. (Contributed by Yury Selivanov in bpo-24400.)


unicodedata

The unicodedata module now uses data from Unicode 8.0.0.


unittest

The TestLoader.loadTestsFromModule() method now accepts a keyword-only argument pattern which is passed to load_tests as the third argument. Found packages are now checked for load_tests regardless of whether their path matches pattern, because it is impossible for a package name to match the default pattern. (Contributed by Robert Collins and Barry A. Warsaw in bpo-16662.)

Unittest discovery errors now are exposed in the TestLoader.errors attribute of the TestLoader instance. (Contributed by Robert Collins in bpo-19746.)

A new command line option --locals to show local variables in tracebacks. (Contributed by Robert Collins in bpo-22936.)


unittest.mock

The Mock class has the following improvements:

• The class constructor has a new unsafe parameter, which causes mock objects to raise AttributeError on attribute names starting with "assert". (Contributed by Kushal Das in bpo-21238.)

• A new Mock.assert_not_called() method to check if the mock object was called. (Contributed by Kushal Das in bpo-21262.)

The MagicMock class now supports __truediv__(), __divmod__() and __matmul__() operators. (Contributed by Johannes Baiter in bpo-20968, and Håkan Lövdahl in bpo-23581 and bpo-23568.)

It is no longer necessary to explicitly pass create=True to the patch() function when patching builtin names. (Contributed by Kushal Das in bpo-17660.)


urllib

A new request.HTTPPasswordMgrWithPriorAuth class allows HTTP Basic Authentication credentials to be managed so as to eliminate unnecessary 401 response handling, or to unconditionally send credentials on the first request in order to communicate with servers that return a 404 response instead of a 401 if the Authorization header is not sent. (Contributed by Matej Cepl in bpo-19494 and Akshit Khurana in bpo-7159.)

A new quote_via argument for the parse.urlencode() function provides a way to control the encoding of query parts if needed. (Contributed by Samwyse and Arnon Yaari in bpo-13866.)

The request.urlopen() function accepts an ssl.SSLContext object as a context argument, which will be used for the HTTPS connection. (Contributed by Alex Gaynor in bpo-22366.)

The parse.urljoin() was updated to use the RFC 3986 semantics for the resolution of relative URLs, rather than RFC 1808 and RFC 2396. (Contributed by Demian Brecht and Senthil Kumaran in bpo-22118.)


wsgiref

The headers argument of the headers.Headers class constructor is now optional. (Contributed by Pablo Torres Navarrete and SilentGhost in bpo-5800.)


xmlrpc

The client.ServerProxy class now supports the context manager protocol. (Contributed by Claudiu Popa in bpo-20627.)

The client.ServerProxy constructor now accepts an optional ssl.SSLContext instance. (Contributed by Alex Gaynor in bpo-22960.)


xml.sax

SAX parsers now support a character stream of the xmlreader.InputSource object. (Contributed by Serhiy Storchaka in bpo-2175.)

parseString() now accepts a str instance. (Contributed by Serhiy Storchaka in bpo-10590.)


zipfile

ZIP output can now be written to unseekable streams. (Contributed by Serhiy Storchaka in bpo-23252.)

The mode argument of ZipFile.open() method now accepts "x" to request exclusive creation. (Contributed by Serhiy Storchaka in bpo-21717.)


### ===🗝 Other module-level changes

Many functions in the mmap, ossaudiodev, socket, ssl, and codecs modules now accept writable bytes-like objects. (Contributed by Serhiy Storchaka in bpo-23001.)


### ===🗝 Optimizations

The os.walk() function has been sped up by 3 to 5 times on POSIX systems, and by 7 to 20 times on Windows. This was done using the new os.scandir() function, which exposes file information from the underlying readdir or FindFirstFile/FindNextFile system calls. (Contributed by Ben Hoyt with help from Victor Stinner in bpo-23605.)

Construction of bytes(int) (filled by zero bytes) is faster and uses less memory for large objects. calloc() is used instead of malloc() to allocate memory for these objects. (Contributed by Victor Stinner in bpo-21233.)

Some operations on ipaddress IPv4Network and IPv6Network have been massively sped up, such as subnets(), supernet(), summarize_address_range(), collapse_addresses(). The speed up can range from 3 to 15 times. (Contributed by Antoine Pitrou, Michel Albert, and Markus in bpo-21486, bpo-21487, bpo-20826, bpo-23266.)

Pickling of ipaddress objects was optimized to produce significantly smaller output. (Contributed by Serhiy Storchaka in bpo-23133.)

Many operations on io.BytesIO are now 50% to 100% faster. (Contributed by Serhiy Storchaka in bpo-15381 and David Wilson in bpo-22003.)

The marshal.dumps() function is now faster: 65–85% with versions 3 and 4, 20–25% with versions 0 to 2 on typical data, and up to 5 times in best cases. (Contributed by Serhiy Storchaka in bpo-20416 and bpo-23344.)

The UTF-32 encoder is now 3 to 7 times faster. (Contributed by Serhiy Storchaka in bpo-15027.)

Regular expressions are now parsed up to 10% faster. (Contributed by Serhiy Storchaka in bpo-19380.)

The json.dumps() function was optimized to run with ensure_ascii=False as fast as with ensure_ascii=True. (Contributed by Naoki Inada in bpo-23206.)

The PyObject_IsInstance() and PyObject_IsSubclass() functions have been sped up in the common case that the second argument has type as its metaclass. (Contributed Georg Brandl by in bpo-22540.)

Method caching was slightly improved, yielding up to 5% performance improvement in some benchmarks. (Contributed by Antoine Pitrou in bpo-22847.)

Objects from the random module now use 50% less memory on 64-bit builds. (Contributed by Serhiy Storchaka in bpo-23488.)

The property() getter calls are up to 25% faster. (Contributed by Joe Jevnik in bpo-23910.)

Instantiation of fractions.Fraction is now up to 30% faster. (Contributed by Stefan Behnel in bpo-22464.)

String methods find(), rfind(), split(), partition() and the in string operator are now significantly faster for searching 1-character substrings. (Contributed by Serhiy Storchaka in bpo-23573.)


### ===🗝 Build and C API Changes

New calloc functions were added:

• PyMem_RawCalloc(),

• PyMem_Calloc(),

• PyObject_Calloc().

(Contributed by Victor Stinner in bpo-21233.)

New encoding/decoding helper functions:

• Py_DecodeLocale() (replaced `_Py_char2wchar()`),

• Py_EncodeLocale() (replaced `_Py_wchar2char()`).

(Contributed by Victor Stinner in bpo-18395.)

A new PyCodec_NameReplaceErrors() function to replace the unicode encode error with \N{...} escapes. (Contributed by Serhiy Storchaka in bpo-19676.)

A new PyErr_FormatV() function similar to PyErr_Format(), but accepts a va_list argument. (Contributed by Antoine Pitrou in bpo-18711.)

A new PyExc_RecursionError exception. (Contributed by Georg Brandl in bpo-19235.)

New PyModule_FromDefAndSpec(), PyModule_FromDefAndSpec2(), and PyModule_ExecDef() functions introduced by PEP 489 – multi-phase extension module initialization. (Contributed by Petr Viktorin in bpo-24268.)

New PyNumber_MatrixMultiply() and PyNumber_InPlaceMatrixMultiply() functions to perform matrix multiplication. (Contributed by Benjamin Peterson in bpo-21176. See also PEP 465 for details.)

The PyTypeObject.tp_finalize slot is now part of the stable ABI.

Windows builds now require Microsoft Visual C++ 14.0, which is available as part of Visual Studio 2015.

Extension modules now include a platform information tag in their filename on some platforms (the tag is optional, and CPython will import extensions without it, although if the tag is present and mismatched, the extension won’t be loaded):

• On Linux, extension module filenames end with .cpython-<major><minor>m-<architecture>-<os>.pyd:◦<major> is the major number of the Python version; for Python 3.5 this is 3.
◦<minor> is the minor number of the Python version; for Python 3.5 this is 5.
◦<architecture> is the hardware architecture the extension module was built to run on. It’s most commonly either i386 for 32-bit Intel platforms or x86_64 for 64-bit Intel (and AMD) platforms.
◦<os> is always linux-gnu, except for extensions built to talk to the 32-bit ABI on 64-bit platforms, in which case it is linux-gnu32 (and <architecture> will be x86_64).


• On Windows, extension module filenames end with <debug>.cp<major><minor>-<platform>.pyd:◦<major> is the major number of the Python version; for Python 3.5 this is 3.
◦<minor> is the minor number of the Python version; for Python 3.5 this is 5.
◦<platform> is the platform the extension module was built for, either win32 for Win32, win_amd64 for Win64, win_ia64 for Windows Itanium 64, and win_arm for Windows on ARM.
◦If built in debug mode, <debug> will be _d, otherwise it will be blank.


• On OS X platforms, extension module filenames now end with -darwin.so.

• On all other platforms, extension module filenames are the same as they were with Python 3.4.


### ===🗝 Deprecated


New Keywords

async and await are not recommended to be used as variable, class, function or module names. Introduced by PEP 492 in Python 3.5, they will become proper keywords in Python 3.7.


Deprecated Python Behavior

Raising the StopIteration exception inside a generator will now generate a silent PendingDeprecationWarning, which will become a non-silent deprecation warning in Python 3.6 and will trigger a RuntimeError in Python 3.7. See PEP 479: Change StopIteration handling inside generators for details.


Unsupported Operating Systems

Windows XP is no longer supported by Microsoft, thus, per PEP 11, CPython 3.5 is no longer officially supported on this OS.


Deprecated Python modules, functions and methods

The `formatter` module has now graduated to full deprecation and is still slated for removal in Python 3.6.

The asyncio.async() function is deprecated in favor of ensure_future().

The smtpd module has in the past always decoded the DATA portion of email messages using the utf-8 codec. This can now be controlled by the new decode_data keyword to SMTPServer. The default value is True, but this default is deprecated. Specify the decode_data keyword with an appropriate value to avoid the deprecation warning.

Directly assigning values to the key, value and coded_value of http.cookies.Morsel objects is deprecated. Use the set() method instead. In addition, the undocumented LegalChars parameter of set() is deprecated, and is now ignored.

Passing a format string as keyword argument format_string to the format() method of the string.Formatter class has been deprecated. (Contributed by Serhiy Storchaka in bpo-23671.)

The platform.dist() and platform.linux_distribution() functions are now deprecated. Linux distributions use too many different ways of describing themselves, so the functionality is left to a package. (Contributed by Vajrasky Kok and Berker Peksag in bpo-1322.)

The previously undocumented from_function and from_builtin methods of inspect.Signature are deprecated. Use the new Signature.from_callable() method instead. (Contributed by Yury Selivanov in bpo-24248.)

The inspect.getargspec() function is deprecated and scheduled to be removed in Python 3.6. (See bpo-20438 for details.)

The inspect getfullargspec(), getcallargs(), and formatargspec() functions are deprecated in favor of the inspect.signature() API. (Contributed by Yury Selivanov in bpo-20438.)

getargvalues() and formatargvalues() functions were inadvertently marked as deprecated with the release of Python 3.5.0.

Use of re.LOCALE flag with str patterns or re.ASCII is now deprecated. (Contributed by Serhiy Storchaka in bpo-22407.)

Use of unrecognized special sequences consisting of '\' and an ASCII letter in regular expression patterns and replacement patterns now raises a deprecation warning and will be forbidden in Python 3.6. (Contributed by Serhiy Storchaka in bpo-23622.)

The undocumented and unofficial use_load_tests default argument of the unittest.TestLoader.loadTestsFromModule() method now is deprecated and ignored. (Contributed by Robert Collins and Barry A. Warsaw in bpo-16662.)


### ===🗝 Removed


API and Feature Removals

The following obsolete and previously deprecated APIs and features have been removed:

• The __version__ attribute has been dropped from the email package. The email code hasn’t been shipped separately from the stdlib for a long time, and the __version__ string was not updated in the last few releases.

• The internal Netrc class in the ftplib module was deprecated in 3.4, and has now been removed. (Contributed by Matt Chaput in bpo-6623.)

• The concept of .pyo files has been removed.

• The JoinableQueue class in the provisional asyncio module was deprecated in 3.4.4 and is now removed. (Contributed by A. Jesse Jiryu Davis in bpo-23464.)


### ===🗝 Porting to Python 3.5

This section lists previously described changes and other bugfixes that may require changes to your code.


Changes in Python behavior


• Due to an oversight, earlier Python versions erroneously accepted the following syntax:


f(1 for x in [1], *args)
f(1 for x in [1], **kwargs)


Python 3.5 now correctly raises a SyntaxError, as generator expressions must be put in parentheses if not a sole argument to a function.



#### Changes in the Python API

• PEP 475: System calls are now retried when interrupted by a signal instead of raising InterruptedError if the Python signal handler does not raise an exception.

• Before Python 3.5, a datetime.time object was considered to be false if it represented midnight in UTC. This behavior was considered obscure and error-prone and has been removed in Python 3.5. See bpo-13936 for full details.

• The ssl.SSLSocket.send() method now raises either ssl.SSLWantReadError or ssl.SSLWantWriteError on a non-blocking socket if the operation would block. Previously, it would return 0. (Contributed by Nikolaus Rath in bpo-20951.)

• The __name__ attribute of generators is now set from the function name, instead of being set from the code name. Use gen.gi_code.co_name to retrieve the code name. Generators also have a new __qualname__ attribute, the qualified name, which is now used for the representation of a generator (repr(gen)). (Contributed by Victor Stinner in bpo-21205.)

• The deprecated “strict” mode and argument of HTMLParser, HTMLParser.error(), and the HTMLParserError exception have been removed. (Contributed by Ezio Melotti in bpo-15114.) The convert_charrefs argument of HTMLParser is now True by default. (Contributed by Berker Peksag in bpo-21047.)

• Although it is not formally part of the API, it is worth noting for porting purposes (ie: fixing tests) that error messages that were previously of the form “‘sometype’ does not support the buffer protocol” are now of the form “a bytes-like object is required, not ‘sometype’”. (Contributed by Ezio Melotti in bpo-16518.)

• If the current directory is set to a directory that no longer exists then FileNotFoundError will no longer be raised and instead find_spec() will return None without caching None in sys.path_importer_cache, which is different than the typical case (bpo-22834).

• HTTP status code and messages from http.client and http.server were refactored into a common HTTPStatus enum. The values in http.client and http.server remain available for backwards compatibility. (Contributed by Demian Brecht in bpo-21793.)

• When an import loader defines importlib.machinery.Loader.exec_module() it is now expected to also define create_module() (raises a DeprecationWarning now, will be an error in Python 3.6). If the loader inherits from importlib.abc.Loader then there is nothing to do, else simply define create_module() to return None. (Contributed by Brett Cannon in bpo-23014.)

• The re.split() function always ignored empty pattern matches, so the "x*" pattern worked the same as "x+", and the "\b" pattern never worked. Now re.split() raises a warning if the pattern could match an empty string. For compatibility, use patterns that never match an empty string (e.g. "x+" instead of "x*"). Patterns that could only match an empty string (such as "\b") now raise an error. (Contributed by Serhiy Storchaka in bpo-22818.)

• The http.cookies.Morsel dict-like interface has been made self consistent: morsel comparison now takes the key and value into account, copy() now results in a Morsel instance rather than a dict, and update() will now raise an exception if any of the keys in the update dictionary are invalid. In addition, the undocumented LegalChars parameter of set() is deprecated and is now ignored. (Contributed by Demian Brecht in bpo-2211.)

• PEP 488 has removed .pyo files from Python and introduced the optional opt- tag in .pyc file names. The importlib.util.cache_from_source() has gained an optimization parameter to help control the opt- tag. Because of this, the debug_override parameter of the function is now deprecated. .pyo files are also no longer supported as a file argument to the Python interpreter and thus serve no purpose when distributed on their own (i.e. sourceless code distribution). Due to the fact that the magic number for bytecode has changed in Python 3.5, all old .pyo files from previous versions of Python are invalid regardless of this PEP.

• The socket module now exports the CAN_RAW_FD_FRAMES constant on linux 3.6 and greater.

• The ssl.cert_time_to_seconds() function now interprets the input time as UTC and not as local time, per RFC 5280. Additionally, the return value is always an int. (Contributed by Akira Li in bpo-19940.)

• The pygettext.py Tool now uses the standard +NNNN format for timezones in the POT-Creation-Date header.

• The smtplib module now uses sys.stderr instead of the previous module-level stderr variable for debug output. If your (test) program depends on patching the module-level variable to capture the debug output, you will need to update it to capture sys.stderr instead.

• The str.startswith() and str.endswith() methods no longer return True when finding the empty string and the indexes are completely out of range. (Contributed by Serhiy Storchaka in bpo-24284.)

• The inspect.getdoc() function now returns documentation strings inherited from base classes. Documentation strings no longer need to be duplicated if the inherited documentation is appropriate. To suppress an inherited string, an empty string must be specified (or the documentation may be filled in). This change affects the output of the pydoc module and the help() function. (Contributed by Serhiy Storchaka in bpo-15582.)

• Nested functools.partial() calls are now flattened. If you were relying on the previous behavior, you can now either add an attribute to a functools.partial() object or you can create a subclass of functools.partial(). (Contributed by Alexander Belopolsky in bpo-7830.)


#### Changes in the C API

• The undocumented format member of the (non-public) PyMemoryViewObject structure has been removed. All extensions relying on the relevant parts in memoryobject.h must be rebuilt.

• The PyMemAllocator structure was renamed to PyMemAllocatorEx and a new calloc field was added.

• Removed non-documented macro PyObject_REPR which leaked references. Use format character %R in PyUnicode_FromFormat()-like functions to format the repr() of the object. (Contributed by Serhiy Storchaka in bpo-22453.)

• Because the lack of the __module__ attribute breaks pickling and introspection, a deprecation warning is now raised for builtin types without the __module__ attribute. This would be an AttributeError in the future. (Contributed by Serhiy Storchaka in bpo-20204.)

• As part of the PEP 492 implementation, the tp_reserved slot of PyTypeObject was replaced with a tp_as_async slot. Refer to Coroutine Objects for new types, structures and functions.


### ===🗝 Notable changes in Python 3.5.4


New make regen-all build target

To simplify cross-compilation, and to ensure that CPython can reliably be compiled without requiring an existing version of Python to already be available, the autotools-based build system no longer attempts to implicitly recompile generated files based on file modification times.

Instead, a new make regen-all command has been added to force regeneration of these files when desired (e.g. after an initial version of Python has already been built based on the pregenerated versions).

More selective regeneration targets are also defined - see Makefile.pre.in for details.

(Contributed by Victor Stinner in bpo-23404.)


New in version 3.5.4.


Removal of make touch build target

The make touch build target previously used to request implicit regeneration of generated files by updating their modification times has been removed.

It has been replaced by the new make regen-all target.

(Contributed by Victor Stinner in bpo-23404.)


Changed in version 3.5.4.


## ==⚡ • What’s New In Python 3.4
◦ Summary – Release Highlights
◦ New Features
◦ New Modules
◦ Improved Modules
◦ CPython Implementation Changes
◦ Deprecated
◦ Removed
◦ Porting to Python 3.4
◦ Changed in 3.4.3


What’s New In Python 3.4
Author:
R. David Murray <rdmurray@bitdance.com> (Editor) 

This article explains the new features in Python 3.4, compared to 3.3. Python 3.4 was released on March 16, 2014. For full details, see the changelog.


### ===🗝 Summary – Release Highlights

New syntax features:
• No new syntax features were added in Python 3.4.

Other new features:
• pip should always be available (PEP 453).
• Newly created file descriptors are non-inheritable (PEP 446).
• command line option for isolated mode (bpo-16499).
• improvements in the handling of codecs that are not text encodings (multiple issues).
• A ModuleSpec Type for the Import System (PEP 451). (Affects importer authors.)
• The marshal format has been made more compact and efficient (bpo-16475).

New library modules:
• asyncio: New provisional API for asynchronous IO (PEP 3156).
• ensurepip: Bootstrapping the pip installer (PEP 453).
• enum: Support for enumeration types (PEP 435).
• pathlib: Object-oriented filesystem paths (PEP 428).
• selectors: High-level and efficient I/O multiplexing, built upon the select module primitives (part of PEP 3156).
• statistics: A basic numerically stable statistics library (PEP 450).
• tracemalloc: Trace Python memory allocations (PEP 454).

Significantly improved library modules:
• Single-dispatch generic functions in functools (PEP 443).
• New pickle protocol 4 (PEP 3154).
• multiprocessing now has an option to avoid using os.fork on Unix (bpo-8713).
• email has a new submodule, contentmanager, and a new Message subclass (EmailMessage) that simplify MIME handling (bpo-18891).
• The inspect and pydoc modules are now capable of correct introspection of a much wider variety of callable objects, which improves the output of the Python help() system.
• The ipaddress module API has been declared stable

Security improvements:
• Secure and interchangeable hash algorithm (PEP 456).
• Make newly created file descriptors non-inheritable (PEP 446) to avoid leaking file descriptors to child processes.
• New command line option for isolated mode, (bpo-16499).
• multiprocessing now has an option to avoid using os.fork on Unix. spawn and forkserver are more secure because they avoid sharing data with child processes.
• multiprocessing child processes on Windows no longer inherit all of the parent’s inheritable handles, only the necessary ones.
• A new hashlib.pbkdf2_hmac() function provides the PKCS#5 password-based key derivation function 2.
• TLSv1.1 and TLSv1.2 support for ssl.
• Retrieving certificates from the Windows system cert store support for ssl.
• Server-side SNI (Server Name Indication) support for ssl.
• The ssl.SSLContext class has a lot of improvements.
• All modules in the standard library that support SSL now support server certificate verification, including hostname matching (ssl.match_hostname()) and CRLs (Certificate Revocation lists, see ssl.SSLContext.load_verify_locations()).

CPython implementation improvements:
• Safe object finalization (PEP 442).
• Leveraging PEP 442, in most cases module globals are no longer set to None during finalization (bpo-18214).
• Configurable memory allocators (PEP 445).
• Argument Clinic (PEP 436).

Please read on for a comprehensive list of user-facing changes, including many other smaller improvements, CPython optimizations, deprecations, and potential porting issues.


## ==⚡ • What’s New In Python 3.3
◦ Summary – Release highlights
◦ PEP 405: Virtual Environments
◦ PEP 420: Implicit Namespace Packages
◦ PEP 3118: New memoryview implementation and buffer protocol documentation
◦ PEP 393: Flexible String Representation
◦ PEP 397: Python Launcher for Windows
◦ PEP 3151: Reworking the OS and IO exception hierarchy
◦ PEP 380: Syntax for Delegating to a Subgenerator
◦ PEP 409: Suppressing exception context
◦ PEP 414: Explicit Unicode literals
◦ PEP 3155: Qualified name for classes and functions
◦ PEP 412: Key-Sharing Dictionary
◦ PEP 362: Function Signature Object
◦ PEP 421: Adding sys.implementation
◦ Using importlib as the Implementation of Import
◦ Other Language Changes
◦ A Finer-Grained Import Lock
◦ Builtin functions and types
◦ New Modules
◦ Improved Modules
◦ Optimizations
◦ Build and C API Changes
◦ Deprecated
◦ Porting to Python 3.3

This article explains the new features in Python 3.3, compared to 3.2. Python 3.3 was released on September 29, 2012. For full details, see the changelog.

### ===🗝 Summary – Release highlights

New syntax features:
• New `yield` from expression for generator delegation.
• The u'unicode' syntax is accepted again for str objects.

New library modules:
• `faulthandler` (helps debugging low-level crashes)
• `ipaddress` (high-level objects representing IP addresses and masks)
• `lzma` (compress data using the XZ / LZMA algorithm)
• `unittest.mock` (replace parts of your system under test with mock objects)
• `venv` (Python virtual environments, as in the popular virtualenv package)

New built-in features:
• Reworked I/O exception hierarchy.

Implementation improvements:
• Rewritten import machinery based on importlib.
• More compact unicode strings.
• More compact attribute dictionaries.

Significantly Improved Library Modules:
• C Accelerator for the decimal module.
• Better unicode handling in the email module (provisional).

Security improvements:
• Hash randomization is switched on by default.

Please read on for a comprehensive list of user-facing changes.


## ==⚡ • What’s New In Python 3.2
◦ PEP 384: Defining a Stable ABI
◦ PEP 389: Argparse Command Line Parsing Module
◦ PEP 391: Dictionary Based Configuration for Logging
◦ PEP 3148: The concurrent.futures module
◦ PEP 3147: PYC Repository Directories
◦ PEP 3149: ABI Version Tagged .so Files
◦ PEP 3333: Python Web Server Gateway Interface v1.0.1
◦ Other Language Changes
◦ New, Improved, and Deprecated Modules
◦ Multi-threading
◦ Optimizations
◦ Unicode
◦ Codecs
◦ Documentation
◦ IDLE
◦ Code Repository
◦ Build and C API Changes
◦ Porting to Python 3.2

## ==⚡ • What’s New In Python 3.1
◦ PEP 372: Ordered Dictionaries
◦ PEP 378: Format Specifier for Thousands Separator
◦ Other Language Changes
◦ New, Improved, and Deprecated Modules
◦ Optimizations
◦ IDLE
◦ Build and C API Changes
◦ Porting to Python 3.1

## ==⚡ • What’s New In Python 3.0
◦ Common Stumbling Blocks
◦ Overview Of Syntax Changes
◦ Changes Already Present In Python 2.6
◦ Library Changes
◦ PEP 3101: A New Approach To String Formatting
◦ Changes To Exceptions
◦ Miscellaneous Other Changes
◦ Build and C API Changes
◦ Performance
◦ Porting To Python 3.0

What’s New In Python 3.0
Author:
Guido van Rossum 

This article explains the new features in Python 3.0, compared to 2.6. Python 3.0, also known as “`Python 3000`” or “Py3K”, is the first ever intentionally backwards incompatible Python release. There are more changes than in a typical release, and more that are important for all Python users. Nevertheless, after digesting the changes, you’ll find that Python really hasn’t changed all that much – by and large, we’re mostly fixing well-known annoyances and warts, and removing a lot of old cruft.

This article doesn’t attempt to provide a complete specification of all new features, but instead tries to give a convenient overview. For full details, you should refer to the documentation for Python 3.0, and/or the many PEPs referenced in the text. If you want to understand the complete implementation and design rationale for a particular feature, PEPs usually have more details than the regular documentation; but note that PEPs usually are not kept up-to-date once a feature has been fully implemented.

Due to time constraints this document is not as complete as it should have been. As always for a new release, the Misc/NEWS file in the source distribution contains a wealth of detailed information about every small thing that was changed.


### ===🗝 Common Stumbling Blocks

This section lists those few changes that are most likely to trip you up if you’re used to Python 2.5.


#### Print Is A Function

The print statement has been replaced with a print() function, with keyword arguments to replace most of the special syntax of the old print statement (PEP 3105). Examples:


```py
Old: print "The answer is", 2*2
New: print("The answer is", 2*2)

Old: print x,           # Trailing comma suppresses newline
New: print(x, end=" ")  # Appends a space instead of a newline

Old: print              # Prints a newline
New: print()            # You must call the function!

Old: print >>sys.stderr, "fatal error"
New: print("fatal error", file=sys.stderr)

Old: print (x, y)       # prints repr((x, y))
New: print((x, y))      # Not the same as print(x, y)!
```


You can also customize the separator between items, e.g.:


```py
print("There are <", 2**32, "> possibilities!", sep="")
```


which produces:


There are <4294967296> possibilities!


Note:

• The print() function doesn’t support the “softspace” feature of the old print statement. For example, in Python 2.x, print "A\n", "B" would write "A\nB\n"; but in Python 3.0, print("A\n", "B") writes "A\n B\n".

• Initially, you’ll be finding yourself typing the old print x a lot in interactive mode. Time to retrain your fingers to type print(x) instead!

• When using the 2to3 source-to-source conversion tool, all print statements are automatically converted to print() function calls, so this is mostly a non-issue for larger projects.

#### Views And Iterators Instead Of Lists

Some well-known APIs no longer return lists:

• dict methods dict.keys(), dict.items() and dict.values() return “views” instead of lists. For example, this no longer works: k = d.keys(); k.sort(). Use k = sorted(d) instead (this works in Python 2.5 too and is just as efficient).

• Also, the dict.iterkeys(), dict.iteritems() and dict.itervalues() methods are no longer supported.

• map() and filter() return iterators. If you really need a list and the input sequences are all of equal length, a quick fix is to wrap map() in list(), e.g. list(map(...)), but a better fix is often to use a list comprehension (especially when the original code uses lambda), or rewriting the code so it doesn’t need a list at all. Particularly tricky is map() invoked for the side effects of the function; the correct transformation is to use a regular for loop (since creating a list would just be wasteful).


If the input sequences are not of equal length, map() will stop at the termination of the shortest of the sequences. For full compatibility with map() from Python 2.x, also wrap the sequences in `itertools.zip_longest()`, e.g. `map(func, *sequences)` becomes `list(map(func, itertools.zip_longest(*sequences)))`.

• range() now behaves like xrange() used to behave, except it works with values of arbitrary size. The latter no longer exists.

• zip() now returns an iterator.


#### Ordering Comparisons

Python 3.0 has simplified the rules for ordering comparisons:
• The ordering comparison operators (<, <=, >=, >) raise a TypeError exception when the operands don’t have a meaningful natural ordering. Thus, expressions like 1 < '', 0 > None or len <= len are no longer valid, and e.g. None < None raises TypeError instead of returning False. A corollary is that sorting a heterogeneous list no longer makes sense – all the elements must be comparable to each other. Note that this does not apply to the == and != operators: objects of different incomparable types always compare unequal to each other.

• builtin.sorted() and list.sort() no longer accept the cmp argument providing a comparison function. Use the key argument instead. N.B. the key and reverse arguments are now “keyword-only”.

• The cmp() function should be treated as gone, and the __cmp__() special method is no longer supported. Use __lt__() for sorting, __eq__() with __hash__(), and other rich comparisons as needed. (If you really need the cmp() functionality, you could use the expression (a > b) - (a < b) as the equivalent for cmp(a, b).)

#### Integers

• PEP 237: Essentially, long renamed to int. That is, there is only one built-in integral type, named int; but it behaves mostly like the old long type.

• PEP 238: An expression like 1/2 returns a float. Use 1//2 to get the truncating behavior. (The latter syntax has existed for years, at least since Python 2.2.)

• The sys.maxint constant was removed, since there is no longer a limit to the value of integers. However, sys.maxsize can be used as an integer larger than any practical list or string index. It conforms to the implementation’s “natural” integer size and is typically the same as sys.maxint in previous releases on the same platform (assuming the same build options).

• The repr() of a long integer doesn’t include the trailing L anymore, so code that unconditionally strips that character will chop off the last digit instead. (Use str() instead.)

• Octal literals are no longer of the form 0720; use 0o720 instead.

#### Text Vs. Data Instead Of Unicode Vs. 8-bit

Everything you thought you knew about binary data and Unicode has changed.

• Python 3.0 uses the concepts of text and (binary) data instead of Unicode strings and 8-bit strings. All text is Unicode; however encoded Unicode is represented as binary data. The type used to hold text is str, the type used to hold data is bytes. The biggest difference with the 2.x situation is that any attempt to mix text and data in Python 3.0 raises TypeError, whereas if you were to mix Unicode and 8-bit strings in Python 2.x, it would work if the 8-bit string happened to contain only 7-bit (ASCII) bytes, but you would get UnicodeDecodeError if it contained non-ASCII values. This value-specific behavior has caused numerous sad faces over the years.

• As a consequence of this change in philosophy, pretty much all code that uses Unicode, encodings or binary data most likely has to change. The change is for the better, as in the 2.x world there were numerous bugs having to do with mixing encoded and unencoded text. To be prepared in Python 2.x, start using unicode for all unencoded text, and str for binary or encoded data only. Then the 2to3 tool will do most of the work for you.

• You can no longer use u"..." literals for Unicode text. However, you must use b"..." literals for binary data.

• As the str and bytes types cannot be mixed, you must always explicitly convert between them. Use str.encode() to go from str to bytes, and bytes.decode() to go from bytes to str. You can also use bytes(s, encoding=...) and str(b, encoding=...), respectively.

• Like str, the bytes type is immutable. There is a separate mutable type to hold buffered binary data, bytearray. Nearly all APIs that accept bytes also accept bytearray. The mutable API is based on collections.MutableSequence.

• All backslashes in raw string literals are interpreted literally. This means that '\U' and '\u' escapes in raw strings are not treated specially. For example, r'\u20ac' is a string of 6 characters in Python 3.0, whereas in 2.6, ur'\u20ac' was the single “euro” character. (Of course, this change only affects raw string literals; the euro character is '\u20ac' in Python 3.0.)

• The built-in basestring abstract type was removed. Use str instead. The str and bytes types don’t have functionality enough in common to warrant a shared base class. The 2to3 tool (see below) replaces every occurrence of basestring with str.

• Files opened as text files (still the default mode for open()) always use an encoding to map between strings (in memory) and bytes (on disk). Binary files (opened with a b in the mode argument) always use bytes in memory. This means that if a file is opened using an incorrect mode or encoding, I/O will likely fail loudly, instead of silently producing incorrect data. It also means that even Unix users will have to specify the correct mode (text or binary) when opening a file. There is a platform-dependent default encoding, which on Unixy platforms can be set with the LANG environment variable (and sometimes also with some other platform-specific locale-related environment variables). In many cases, but not all, the system default is UTF-8; you should never count on this default. Any application reading or writing more than pure ASCII text should probably have a way to override the encoding. There is no longer any need for using the encoding-aware streams in the codecs module.

• The initial values of sys.stdin, sys.stdout and sys.stderr are now unicode-only text files (i.e., they are instances of io.TextIOBase). To read and write bytes data with these streams, you need to use their io.TextIOBase.buffer attribute.

• Filenames are passed to and returned from APIs as (Unicode) strings. This can present platform-specific problems because on some platforms filenames are arbitrary byte strings. (On the other hand, on Windows filenames are natively stored as Unicode.) As a work-around, most APIs (e.g. open() and many functions in the os module) that take filenames accept bytes objects as well as strings, and a few APIs have a way to ask for a bytes return value. Thus, os.listdir() returns a list of bytes instances if the argument is a bytes instance, and os.getcwdb() returns the current working directory as a bytes instance. Note that when os.listdir() returns a list of strings, filenames that cannot be decoded properly are omitted rather than raising UnicodeError.

• Some system APIs like os.environ and sys.argv can also present problems when the bytes made available by the system is not interpretable using the default encoding. Setting the LANG variable and rerunning the program is probably the best approach.

• PEP 3138: The repr() of a string no longer escapes non-ASCII characters. It still escapes control characters and code points with non-printable status in the Unicode standard, however.

• PEP 3120: The default source encoding is now UTF-8.

• PEP 3131: Non-ASCII letters are now allowed in identifiers. (However, the standard library remains ASCII-only with the exception of contributor names in comments.)

• The StringIO and cStringIO modules are gone. Instead, import the io module and use io.StringIO or io.BytesIO for text and data respectively.

• See also the Unicode HOWTO, which was updated for Python 3.0.

### ===🗝 Overview Of Syntax Changes

This section gives a brief overview of every syntactic change in Python 3.0.


#### New Syntax

• PEP 3107: Function argument and return value annotations. This provides a standardized way of annotating a function’s parameters and return value. There are no semantics attached to such annotations except that they can be introspected at runtime using the __annotations__ attribute. The intent is to encourage experimentation through metaclasses, decorators or frameworks.

• PEP 3102: Keyword-only arguments. Named parameters occurring after `*args` in the parameter list must be specified using keyword syntax in the call. You can also use a bare * in the parameter list to indicate that you don’t accept a variable-length argument list, but you do have keyword-only arguments.

• Keyword arguments are allowed after the list of base classes in a class definition. This is used by the new convention for specifying a metaclass (see next section), but can be used for other purposes as well, as long as the metaclass supports it.

• PEP 3104: nonlocal statement. Using nonlocal x you can now assign directly to a variable in an outer (but non-global) scope. nonlocal is a new reserved word.

• PEP 3132: Extended Iterable Unpacking. You can now write things like a, b, `*rest = some_sequence`. And even `*rest`, a = stuff. The rest object is always a (possibly empty) list; the right-hand side may be any iterable. Example:

    (a, *rest, b) = range(5)

This sets a to 0, b to 4, and rest to [1, 2, 3].


• Dictionary comprehensions: {k: v for k, v in stuff} means the same thing as dict(stuff) but is more flexible. (This is PEP 274 vindicated. :-)

• Set literals, e.g. {1, 2}. Note that {} is an empty dictionary; use set() for an empty set. Set comprehensions are also supported; e.g., {x for x in stuff} means the same thing as set(stuff) but is more flexible.

• New octal literals, e.g. 0o720 (already in 2.6). The old octal literals (0720) are gone.

• New binary literals, e.g. 0b1010 (already in 2.6), and there is a new corresponding built-in function, bin().

• Bytes literals are introduced with a leading b or B, and there is a new corresponding built-in function, bytes().

Changed Syntax

• PEP 3109 and PEP 3134: new raise statement syntax: raise [expr [from expr]]. See below.

• as and with are now reserved words. (Since 2.6, actually.)

• True, False, and None are reserved words. (2.6 partially enforced the restrictions on None already.)

• Change from except exc, var to except exc as var. See PEP 3110.

• PEP 3115: New Metaclass Syntax. Instead of:

    class C:
        __metaclass__ = M
        ...


you must now use:


    class C(metaclass=M):
        ...


The module-global __metaclass__ variable is no longer supported. (It was a crutch to make it easier to default to new-style classes without deriving every class from object.)

• List comprehensions no longer support the syntactic form [... for var in item1, item2, ...]. Use [... for var in (item1, item2, ...)] instead. Also note that list comprehensions have different semantics: they are closer to syntactic sugar for a generator expression inside a list() constructor, and in particular the loop control variables are no longer leaked into the surrounding scope.

• The ellipsis (...) can be used as an atomic expression anywhere. (Previously it was only allowed in slices.) Also, it must now be spelled as .... (Previously it could also be spelled as . . ., by a mere accident of the grammar.)


#### Removed Syntax

• PEP 3113: Tuple parameter unpacking removed. You can no longer write def foo(a, (b, c)): .... Use def foo(a, b_c): b, c = b_c instead.

• Removed backticks (use repr() instead).

• Removed <> (use != instead).

• Removed keyword: exec() is no longer a keyword; it remains as a function. (Fortunately the function syntax was also accepted in 2.x.) Also note that exec() no longer takes a stream argument; instead of exec(f) you can use exec(f.read()).

• Integer literals no longer support a trailing l or L.

• String literals no longer support a leading u or U.

• The from module import * syntax is only allowed at the module level, no longer inside functions.

• The only acceptable syntax for relative imports is from .[module] import name. All import forms not starting with . are interpreted as absolute imports. (PEP 328)

• Classic classes are gone.


### ===🗝 Changes Already Present In Python 2.6

Since many users presumably make the jump straight from Python 2.5 to Python 3.0, this section reminds the reader of new features that were originally designed for Python 3.0 but that were back-ported to Python 2.6. The corresponding sections in What’s New in Python 2.6 should be consulted for longer descriptions.
• PEP 343: The ‘with’ statement. The with statement is now a standard feature and no longer needs to be imported from the __future__. Also check out Writing Context Managers and The contextlib module.

• PEP 366: Explicit Relative Imports From a Main Module. This enhances the usefulness of the -m option when the referenced module lives in a package.

• PEP 370: Per-user site-packages Directory.

• PEP 371: The multiprocessing Package.

• PEP 3101: Advanced String Formatting. Note: the 2.6 description mentions the format() method for both 8-bit and Unicode strings. In 3.0, only the str type (text strings with Unicode support) supports this method; the bytes type does not. The plan is to eventually make this the only API for string formatting, and to start deprecating the % operator in Python 3.1.

• PEP 3105: print As a Function. This is now a standard feature and no longer needs to be imported from __future__. More details were given above.

• PEP 3110: Exception-Handling Changes. The except exc as var syntax is now standard and except exc, var is no longer supported. (Of course, the as var part is still optional.)

• PEP 3112: Byte Literals. The b"..." string literal notation (and its variants like b'...', b"""...""", and br"...") now produces a literal of type bytes.

• PEP 3116: New I/O Library. The io module is now the standard way of doing file I/O. The built-in open() function is now an alias for io.open() and has additional keyword arguments encoding, errors, newline and closefd. Also note that an invalid mode argument now raises ValueError, not IOError. The binary file object underlying a text file object can be accessed as f.buffer (but beware that the text object maintains a buffer of itself in order to speed up the encoding and decoding operations).

• PEP 3118: Revised Buffer Protocol. The old builtin buffer() is now really gone; the new builtin memoryview() provides (mostly) similar functionality.

• PEP 3119: Abstract Base Classes. The abc module and the ABCs defined in the collections module plays a somewhat more prominent role in the language now, and built-in collection types like dict and list conform to the collections.MutableMapping and collections.MutableSequence ABCs, respectively.

• PEP 3127: Integer Literal Support and Syntax. As mentioned above, the new octal literal notation is the only one supported, and binary literals have been added.

• PEP 3129: Class Decorators.

• PEP 3141: A Type Hierarchy for Numbers. The numbers module is another new use of ABCs, defining Python’s “numeric tower”. Also note the new fractions module which implements numbers.Rational.

### ===🗝 Library Changes

Due to time constraints, this document does not exhaustively cover the very extensive changes to the standard library. PEP 3108 is the reference for the major changes to the library. Here’s a capsule review:

• Many old modules were removed. Some, like gopherlib (no longer used) and md5 (replaced by hashlib), were already deprecated by PEP 4. Others were removed as a result of the removal of support for various platforms such as Irix, BeOS and Mac OS 9 (see PEP 11). Some modules were also selected for removal in Python 3.0 due to lack of use or because a better replacement exists. See PEP 3108 for an exhaustive list.

• The bsddb3 package was removed because its presence in the core standard library has proved over time to be a particular burden for the core developers due to testing instability and Berkeley DB’s release schedule. However, the package is alive and well, externally maintained at https://www.jcea.es/programacion/pybsddb.htm.

• Some modules were renamed because their old name disobeyed PEP 8, or for various other reasons. Here’s the list:

|      Old Name     |   New Name   |
|-------------------|--------------|
| _winreg           | winreg       |
| ConfigParser      | configparser |
| copy_reg          | copyreg      |
| Queue             | queue        |
| SocketServer      | socketserver |
| markupbase        | _markupbase  |
| repr              | reprlib      |
| test.test_support | test.support |


• A common pattern in Python 2.x is to have one version of a module implemented in pure Python, with an optional accelerated version implemented as a C extension; for example, pickle and cPickle. This places the burden of importing the accelerated version and falling back on the pure Python version on each user of these modules. In Python 3.0, the accelerated versions are considered implementation details of the pure Python versions. Users should always import the standard version, which attempts to import the accelerated version and falls back to the pure Python version. The pickle / cPickle pair received this treatment. The profile module is on the list for 3.1. The StringIO module has been turned into a class in the io module.

• Some related modules have been grouped into packages, and usually the submodule names have been simplified. The resulting new packages are:

◦ dbm (anydbm, dbhash, dbm, dumbdbm, gdbm, whichdb).

◦ html (HTMLParser, htmlentitydefs).

◦ http (httplib, BaseHTTPServer, CGIHTTPServer, SimpleHTTPServer, Cookie, cookielib).

◦ tkinter (all Tkinter-related modules except turtle). The target audience of turtle doesn’t really care about tkinter. Also note that as of Python 2.6, the functionality of turtle has been greatly enhanced.

◦ urllib (urllib, urllib2, urlparse, robotparse).

◦ xmlrpc (xmlrpclib, DocXMLRPCServer, SimpleXMLRPCServer).

Some other changes to standard library modules, not covered by PEP 3108:

• Killed sets. Use the built-in set() class.

• Cleanup of the sys module: removed sys.exitfunc(), sys.exc_clear(), sys.exc_type, sys.exc_value, sys.exc_traceback. (Note that sys.last_type etc. remain.)

• Cleanup of the array.array type: the read() and write() methods are gone; use fromfile() and tofile() instead. Also, the 'c' typecode for array is gone – use either 'b' for bytes or 'u' for Unicode characters.

• Cleanup of the operator module: removed sequenceIncludes() and isCallable().

• Cleanup of the thread module: acquire_lock() and release_lock() are gone; use acquire() and release() instead.

• Cleanup of the random module: removed the jumpahead() API.

• The new module is gone.

• The functions os.tmpnam(), os.tempnam() and os.tmpfile() have been removed in favor of the tempfile module.

• The tokenize module has been changed to work with bytes. The main entry point is now tokenize.tokenize(), instead of generate_tokens.

• string.letters and its friends (string.lowercase and string.uppercase) are gone. Use string.ascii_letters etc. instead. (The reason for the removal is that string.letters and friends had locale-specific behavior, which is a bad idea for such attractively-named global “constants”.)

• Renamed module __builtin__ to builtins (removing the underscores, adding an ‘s’). The __builtins__ variable found in most global namespaces is unchanged. To modify a builtin, you should use builtins, not __builtins__!


### ===🗝 PEP 3101: A New Approach To String Formatting

• A new system for built-in string formatting operations replaces the % string formatting operator. (However, the % operator is still supported; it will be deprecated in Python 3.1 and removed from the language at some later time.) Read PEP 3101 for the full scoop.

### ===🗝 Changes To Exceptions

The APIs for raising and catching exception have been cleaned up and new powerful features added:

• PEP 352: All exceptions must be derived (directly or indirectly) from BaseException. This is the root of the exception hierarchy. This is not new as a recommendation, but the requirement to inherit from BaseException is new. (Python 2.6 still allowed classic classes to be raised, and placed no restriction on what you can catch.) As a consequence, string exceptions are finally truly and utterly dead.

• Almost all exceptions should actually derive from Exception; BaseException should only be used as a base class for exceptions that should only be handled at the top level, such as SystemExit or KeyboardInterrupt. The recommended idiom for handling all exceptions except for this latter category is to use except Exception.

• StandardError was removed.

• Exceptions no longer behave as sequences. Use the args attribute instead.

• PEP 3109: Raising exceptions. You must now use raise Exception(args) instead of raise Exception, args. Additionally, you can no longer explicitly specify a traceback; instead, if you have to do this, you can assign directly to the __traceback__ attribute (see below).

• PEP 3110: Catching exceptions. You must now use except SomeException as variable instead of except SomeException, variable. Moreover, the variable is explicitly deleted when the except block is left.

• PEP 3134: Exception chaining. There are two cases: implicit chaining and explicit chaining. Implicit chaining happens when an exception is raised in an except or finally handler block. This usually happens due to a bug in the handler block; we call this a secondary exception. In this case, the original exception (that was being handled) is saved as the __context__ attribute of the secondary exception. Explicit chaining is invoked with this syntax:

    raise SecondaryException() from primary_exception


(where primary_exception is any expression that produces an exception object, probably an exception that was previously caught). In this case, the primary exception is stored on the __cause__ attribute of the secondary exception. The traceback printed when an unhandled exception occurs walks the chain of __cause__ and __context__ attributes and prints a separate traceback for each component of the chain, with the primary exception at the top. (Java users may recognize this behavior.)


• PEP 3134: Exception objects now store their traceback as the __traceback__ attribute. This means that an exception object now contains all the information pertaining to an exception, and there are fewer reasons to use sys.exc_info() (though the latter is not removed).

• A few exception messages are improved when Windows fails to load an extension module. For example, error code 193 is now %1 is not a valid Win32 application. Strings now deal with non-English locales.


### ===🗝 Miscellaneous Other Changes


#### Operators And Special Methods

• != now returns the opposite of ==, unless == returns NotImplemented.

• The concept of “unbound methods” has been removed from the language. When referencing a method as a class attribute, you now get a plain function object.

• __getslice__(), __setslice__() and __delslice__() were killed. The syntax a[i:j] now translates to a.__getitem__(slice(i, j)) (or __setitem__() or __delitem__(), when used as an assignment or deletion target, respectively).

• PEP 3114: the standard next() method has been renamed to __next__().

• The __oct__() and __hex__() special methods are removed – oct() and hex() use __index__() now to convert the argument to an integer.

• Removed support for __members__ and __methods__.

• The function attributes named func_X have been renamed to use the __X__ form, freeing up these names in the function attribute namespace for user-defined attributes. To wit, func_closure, func_code, func_defaults, func_dict, func_doc, func_globals, func_name were renamed to __closure__, __code__, __defaults__, __dict__, __doc__, __globals__, __name__, respectively.

• __nonzero__() is now __bool__().

#### Builtins

• PEP 3135: New super(). You can now invoke super() without arguments and (assuming this is in a regular instance method defined inside a class statement) the right class and instance will automatically be chosen. With arguments, the behavior of super() is unchanged.

• PEP 3111: raw_input() was renamed to input(). That is, the new input() function reads a line from sys.stdin and returns it with the trailing newline stripped. It raises EOFError if the input is terminated prematurely. To get the old behavior of input(), use eval(input()).

• A new built-in function next() was added to call the __next__() method on an object.

• The round() function rounding strategy and return type have changed. Exact halfway cases are now rounded to the nearest even result instead of away from zero. (For example, round(2.5) now returns 2 rather than 3.) round(x[, n]) now delegates to x.__round__([n]) instead of always returning a float. It generally returns an integer when called with a single argument and a value of the same type as x when called with two arguments.

• Moved intern() to sys.intern().

• Removed: apply(). Instead of apply(f, args) use `f(*args)`.

• Removed callable(). Instead of callable(f) you can use isinstance(f, collections.Callable). The operator.isCallable() function is also gone.

• Removed coerce(). This function no longer serves a purpose now that classic classes are gone.

• Removed execfile(). Instead of execfile(fn) use exec(open(fn).read()).

• Removed the file type. Use open(). There are now several different kinds of streams that open can return in the io module.

• Removed reduce(). Use functools.reduce() if you really need it; however, 99 percent of the time an explicit for loop is more readable.

• Removed reload(). Use imp.reload().

• Removed. dict.has_key() – use the in operator instead.


### ===🗝 Build and C API Changes

Due to time constraints, here is a very incomplete list of changes to the C API.

• Support for several platforms was dropped, including but not limited to Mac OS 9, BeOS, RISCOS, Irix, and Tru64.

• PEP 3118: New Buffer API.

• PEP 3121: Extension Module Initialization & Finalization.

• PEP 3123: Making PyObject_HEAD conform to standard C.

• No more C API support for restricted execution.

• PyNumber_Coerce(), PyNumber_CoerceEx(), PyMember_Get(), and PyMember_Set() C APIs are removed.

• New C API PyImport_ImportModuleNoBlock(), works like PyImport_ImportModule() but won’t block on the import lock (returning an error instead).

• Renamed the boolean conversion C-level slot and method: nb_nonzero is now nb_bool.

• Removed METH_OLDARGS and WITH_CYCLE_GC from the C API.

### ===🗝 Performance

The net result of the 3.0 generalizations is that Python 3.0 runs the pystone benchmark around 10% slower than Python 2.5. Most likely the biggest cause is the removal of special-casing for small integers. There’s room for improvement, but it will happen after 3.0 is released!


### ===🗝 Porting To Python 3.0

For porting existing Python 2.5 or 2.6 source code to Python 3.0, the best strategy is the following:

0. (Prerequisite:) Start with excellent test coverage.
1.Port to Python 2.6. This should be no more work than the average port from Python 2.x to Python 2.(x+1). Make sure all your tests pass.

2. (Still using 2.6:) Turn on the -3 command line switch. This enables warnings about features that will be removed (or change) in 3.0. Run your test suite again, and fix code that you get warnings about until there are no warnings left, and all your tests still pass.

3. Run the 2to3 source-to-source translator over your source code tree. (See 2to3 - Automated Python 2 to 3 code translation for more on this tool.) Run the result of the translation under Python 3.0. Manually fix up any remaining issues, fixing problems until all tests pass again.

It is not recommended to try to write source code that runs unchanged under both Python 2.6 and 3.0; you’d have to use a very contorted coding style, e.g. avoiding print statements, metaclasses, and much more. If you are maintaining a library that needs to support both Python 2.6 and Python 3.0, the best approach is to modify step 3 above by editing the 2.6 version of the source code and running the 2to3 translator again, rather than editing the 3.0 version of the source code.

For porting C extensions to Python 3.0, please see Porting Extension Modules to Python 3.


## ==⚡ • What’s New in Python 2.7
◦ The Future for Python 2.x
◦ Changes to the Handling of Deprecation Warnings
◦ Python 3.1 Features
◦ PEP 372: Adding an Ordered Dictionary to collections
◦ PEP 378: Format Specifier for Thousands Separator
◦ PEP 389: The argparse Module for Parsing Command Lines
◦ PEP 391: Dictionary-Based Configuration For Logging
◦ PEP 3106: Dictionary Views
◦ PEP 3137: The memoryview Object
◦ Other Language Changes
◦ New and Improved Modules
◦ Build and C API Changes
◦ Other Changes and Fixes
◦ Porting to Python 2.7
◦ New Features Added to Python 2.7 Maintenance Releases
◦ Acknowledgements

## ==⚡ • What’s New in Python 2.6
◦ Python 3.0
◦ Changes to the Development Process
◦ PEP 343: The ‘with’ statement
◦ PEP 366: Explicit Relative Imports From a Main Module
◦ PEP 370: Per-user site-packages Directory
◦ PEP 371: The multiprocessing Package
◦ PEP 3101: Advanced String Formatting
◦ PEP 3105: print As a Function
◦ PEP 3110: Exception-Handling Changes
◦ PEP 3112: Byte Literals
◦ PEP 3116: New I/O Library
◦ PEP 3118: Revised Buffer Protocol
◦ PEP 3119: Abstract Base Classes
◦ PEP 3127: Integer Literal Support and Syntax
◦ PEP 3129: Class Decorators
◦ PEP 3141: A Type Hierarchy for Numbers
◦ Other Language Changes
◦ New and Improved Modules
◦ Deprecations and Removals
◦ Build and C API Changes
◦ Porting to Python 2.6
◦ Acknowledgements

## ==⚡ • What’s New in Python 2.5
◦ PEP 308: Conditional Expressions
◦ PEP 309: Partial Function Application
◦ PEP 314: Metadata for Python Software Packages v1.1
◦ PEP 328: Absolute and Relative Imports
◦ PEP 338: Executing Modules as Scripts
◦ PEP 341: Unified try/except/finally
◦ PEP 342: New Generator Features
◦ PEP 343: The ‘with’ statement
◦ PEP 352: Exceptions as New-Style Classes
◦ PEP 353: Using ssize_t as the index type
◦ PEP 357: The ‘__index__’ method
◦ Other Language Changes
◦ New, Improved, and Removed Modules
◦ Build and C API Changes
◦ Porting to Python 2.5
◦ Acknowledgements

## ==⚡ • What’s New in Python 2.4
◦ PEP 218: Built-In Set Objects
◦ PEP 237: Unifying Long Integers and Integers
◦ PEP 289: Generator Expressions
◦ PEP 292: Simpler String Substitutions
◦ PEP 318: Decorators for Functions and Methods
◦ PEP 322: Reverse Iteration
◦ PEP 324: New subprocess Module
◦ PEP 327: Decimal Data Type
◦ PEP 328: Multi-line Imports
◦ PEP 331: Locale-Independent Float/String Conversions
◦ Other Language Changes
◦ New, Improved, and Deprecated Modules
◦ Build and C API Changes
◦ Porting to Python 2.4
◦ Acknowledgements

## ==⚡ • What’s New in Python 2.3
◦ PEP 218: A Standard Set Datatype
◦ PEP 255: Simple Generators
◦ PEP 263: Source Code Encodings
◦ PEP 273: Importing Modules from ZIP Archives
◦ PEP 277: Unicode file name support for Windows NT
◦ PEP 278: Universal Newline Support
◦ PEP 279: enumerate()
◦ PEP 282: The logging Package
◦ PEP 285: A Boolean Type
◦ PEP 293: Codec Error Handling Callbacks
◦ PEP 301: Package Index and Metadata for Distutils
◦ PEP 302: New Import Hooks
◦ PEP 305: Comma-separated Files
◦ PEP 307: Pickle Enhancements
◦ Extended Slices
◦ Other Language Changes
◦ New, Improved, and Deprecated Modules
◦ Pymalloc: A Specialized Object Allocator
◦ Build and C API Changes
◦ Other Changes and Fixes
◦ Porting to Python 2.3
◦ Acknowledgements

## ==⚡ • What’s New in Python 2.2
◦ Introduction
◦ PEPs 252 and 253: Type and Class Changes
◦ PEP 234: Iterators
◦ PEP 255: Simple Generators
◦ PEP 237: Unifying Long Integers and Integers
◦ PEP 238: Changing the Division Operator
◦ Unicode Changes
◦ PEP 227: Nested Scopes
◦ New and Improved Modules
◦ Interpreter Changes and Fixes
◦ Other Changes and Fixes
◦ Acknowledgements

What’s New in Python 2.2
Author:
A.M. Kuchling 


### ===🗝 Introduction

This article explains the new features in Python 2.2.2, released on October 14, 2002. Python 2.2.2 is a bugfix release of Python 2.2, originally released on December 21, 2001.

Python 2.2 can be thought of as the “cleanup release”. There are some features such as generators and iterators that are completely new, but most of the changes, significant and far-reaching though they may be, are aimed at cleaning up irregularities and dark corners of the language design.

This article doesn’t attempt to provide a complete specification of the new features, but instead provides a convenient overview. For full details, you should refer to the documentation for Python 2.2, such as the Python Library Reference and the Python Reference Manual. If you want to understand the complete implementation and design rationale for a change, refer to the PEP for a particular new feature.


### ===🗝 PEPs 252 and 253: Type and Class Changes

The largest and most far-reaching changes in Python 2.2 are to Python’s model of objects and classes. The changes should be backward compatible, so it’s likely that your code will continue to run unchanged, but the changes provide some amazing new capabilities. Before beginning this, the longest and most complicated section of this article, I’ll provide an overview of the changes and offer some comments.

A long time ago I wrote a web page listing flaws in Python’s design. One of the most significant flaws was that it’s impossible to subclass Python types implemented in C. In particular, it’s not possible to subclass built-in types, so you can’t just subclass, say, lists in order to add a single useful method to them. The UserList module provides a class that supports all of the methods of lists and that can be subclassed further, but there’s lots of C code that expects a regular Python list and won’t accept a UserList instance.

Python 2.2 fixes this, and in the process adds some exciting new capabilities. A brief summary:

• You can subclass built-in types such as lists and even integers, and your subclasses should work in every place that requires the original type.

• It’s now possible to define static and class methods, in addition to the instance methods available in previous versions of Python.

• It’s also possible to automatically call methods on accessing or setting an instance attribute by using a new mechanism called properties. Many uses of __getattr__() can be rewritten to use properties instead, making the resulting code simpler and faster. As a small side benefit, attributes can now have docstrings, too.

• The list of legal attributes for an instance can be limited to a particular set using slots, making it possible to safeguard against typos and perhaps make more optimizations possible in future versions of Python.

Some users have voiced concern about all these changes. Sure, they say, the new features are neat and lend themselves to all sorts of tricks that weren’t possible in previous versions of Python, but they also make the language more complicated. Some people have said that they’ve always recommended Python for its simplicity, and feel that its simplicity is being lost.

Personally, I think there’s no need to worry. Many of the new features are quite esoteric, and you can write a lot of Python code without ever needed to be aware of them. Writing a simple class is no more difficult than it ever was, so you don’t need to bother learning or teaching them unless they’re actually needed. Some very complicated tasks that were previously only possible from C will now be possible in pure Python, and to my mind that’s all for the better.

I’m not going to attempt to cover every single corner case and small change that were required to make the new features work. Instead this section will paint only the broad strokes. See section Related Links, “Related Links”, for further sources of information about Python 2.2’s new object model.


### ===🗝 Old and New Classes

First, you should know that Python 2.2 really has two kinds of classes: classic or old-style classes, and new-style classes. The old-style class model is exactly the same as the class model in earlier versions of Python. All the new features described in this section apply only to new-style classes. This divergence isn’t intended to last forever; eventually old-style classes will be dropped, possibly in Python 3.0.

So how do you define a new-style class? You do it by subclassing an existing new-style class. Most of Python’s built-in types, such as integers, lists, dictionaries, and even files, are new-style classes now. A new-style class named object, the base class for all built-in types, has also been added so if no built-in type is suitable, you can just subclass object:


```py
class C(object):
    def __init__ (self):
        ...
    ...
```


This means that class statements that don’t have any base classes are always classic classes in Python 2.2. (Actually you can also change this by setting a module-level variable named __metaclass__ — see PEP 253 for the details — but it’s easier to just subclass object.)

The type objects for the built-in types are available as built-ins, named using a clever trick. Python has always had built-in functions named int(), float(), and str(). In 2.2, they aren’t functions any more, but type objects that behave as factories when called.


>>> int
<type 'int'>
>>> int('123')
123


To make the set of types complete, new type objects such as dict() and file() have been added. Here’s a more interesting example, adding a lock() method to file objects:


```py
class LockableFile(file):
    def lock (self, operation, length=0, start=0, whence=0):
        import fcntl
        return fcntl.lockf(self.fileno(), operation,
                           length, start, whence)
```


The now-obsolete posixfile module contained a class that emulated all of a file object’s methods and also added a lock() method, but this class couldn’t be passed to internal functions that expected a built-in file, something which is possible with our new LockableFile.


#### Descriptors

In previous versions of Python, there was no consistent way to discover what attributes and methods were supported by an object. There were some informal conventions, such as defining __members__ and __methods__ attributes that were lists of names, but often the author of an extension type or a class wouldn’t bother to define them. You could fall back on inspecting the __dict__ of an object, but when class inheritance or an arbitrary __getattr__() hook were in use this could still be inaccurate.

The one big idea underlying the new class model is that an API for describing the attributes of an object using descriptors has been formalized. Descriptors specify the value of an attribute, stating whether it’s a method or a field. With the descriptor API, static methods and class methods become possible, as well as more exotic constructs.

Attribute descriptors are objects that live inside class objects, and have a few attributes of their own:

• __name__ is the attribute’s name.

• __doc__ is the attribute’s docstring.

• __get__(object) is a method that retrieves the attribute value from object.

• __set__(object, value) sets the attribute on object to value.

• __delete__(object, value) deletes the value attribute of object.

For example, when you write obj.x, the steps that Python actually performs are:


descriptor = obj.__class__.x
descriptor.__get__(obj)


For methods, descriptor.__get__() returns a temporary object that’s callable, and wraps up the instance and the method to be called on it. This is also why static methods and class methods are now possible; they have descriptors that wrap up just the method, or the method and the class. As a brief explanation of these new kinds of methods, static methods aren’t passed the instance, and therefore resemble regular functions. Class methods are passed the class of the object, but not the object itself. Static and class methods are defined like this:


```py
class C(object):
    def f(arg1, arg2):
        ...
    f = staticmethod(f)

    def g(cls, arg1, arg2):
        ...
    g = classmethod(g)
```


The staticmethod() function takes the function f(), and returns it wrapped up in a descriptor so it can be stored in the class object. You might expect there to be special syntax for creating such methods (def static f, defstatic f(), or something like that) but no such syntax has been defined yet; that’s been left for future versions of Python.

More new features, such as slots and properties, are also implemented as new kinds of descriptors, and it’s not difficult to write a descriptor class that does something novel. For example, it would be possible to write a descriptor class that made it possible to write Eiffel-style preconditions and postconditions for a method. A class that used this feature might be defined like this:


```py
from eiffel import eiffelmethod

class C(object):
    def f(self, arg1, arg2):
        # The actual function
        ...
    def pre_f(self):
        # Check preconditions
        ...
    def post_f(self):
        # Check postconditions
        ...

    f = eiffelmethod(f, pre_f, post_f)
```


Note that a person using the new eiffelmethod() doesn’t have to understand anything about descriptors. This is why I think the new features don’t increase the basic complexity of the language. There will be a few wizards who need to know about it in order to write eiffelmethod() or the ZODB or whatever, but most users will just write code on top of the resulting libraries and ignore the implementation details.


#### Multiple Inheritance: The Diamond Rule

Multiple inheritance has also been made more useful through changing the rules under which names are resolved. Consider this set of classes (diagram taken from PEP 253 by Guido van Rossum):


          class A:
            ^ ^  def save(self): ...
           /   \
          /     \
         /       \
        /         \
    class B     class C:
        ^         ^  def save(self): ...
         \       /
          \     /
           \   /
            \ /
          class D


The lookup rule for classic classes is simple but not very smart; the base classes are searched depth-first, going from left to right. A reference to D.save() will search the classes D, B, and then A, where save() would be found and returned. C.save() would never be found at all. This is bad, because if C’s save() method is saving some internal state specific to C, not calling it will result in that state never getting saved.

New-style classes follow a different algorithm that’s a bit more complicated to explain, but does the right thing in this situation. (Note that Python 2.3 changes this algorithm to one that produces the same results in most cases, but produces more useful results for really complicated inheritance graphs.)

1. List all the base classes, following the classic lookup rule and include a class multiple times if it’s visited repeatedly. In the above example, the list of visited classes is [D, B, A, C, A].

2. Scan the list for duplicated classes. If any are found, remove all but one occurrence, leaving the last one in the list. In the above example, the list becomes [D, B, C, A] after dropping duplicates.

Following this rule, referring to D.save() will return C.save(), which is the behaviour we’re after. This lookup rule is the same as the one followed by Common Lisp. A new built-in function, super(), provides a way to get at a class’s superclasses without having to reimplement Python’s algorithm. The most commonly used form will be super(class, obj), which returns a bound superclass object (not the actual class object). This form will be used in methods to call a method in the superclass; for example, D’s save() method would look like this:


```py
class D (B,C):
    def save (self):
        # Call superclass .save()
        super(D, self).save()
        # Save D's private information here
        ...
```


super() can also return unbound superclass objects when called as super(class) or super(class1, class2), but this probably won’t often be useful.


#### Attribute Access

A fair number of sophisticated Python classes define hooks for attribute access using __getattr__(); most commonly this is done for convenience, to make code more readable by automatically mapping an attribute access such as obj.parent into a method call such as obj.get_parent. Python 2.2 adds some new ways of controlling attribute access.

First, __getattr__(attr_name) is still supported by new-style classes, and nothing about it has changed. As before, it will be called when an attempt is made to access obj.foo and no attribute named foo is found in the instance’s dictionary.

New-style classes also support a new method, __getattribute__(attr_name). The difference between the two methods is that __getattribute__() is always called whenever any attribute is accessed, while the old __getattr__() is only called if foo isn’t found in the instance’s dictionary.

However, Python 2.2’s support for properties will often be a simpler way to trap attribute references. Writing a __getattr__() method is complicated because to avoid recursion you can’t use regular attribute accesses inside them, and instead have to mess around with the contents of __dict__. __getattr__() methods also end up being called by Python when it checks for other methods such as __repr__() or __coerce__(), and so have to be written with this in mind. Finally, calling a function on every attribute access results in a sizable performance loss.

property is a new built-in type that packages up three functions that get, set, or delete an attribute, and a docstring. For example, if you want to define a size attribute that’s computed, but also settable, you could write:


```py
class C(object):
    def get_size (self):
        result = ... computation ...
        return result
    def set_size (self, size):
        ... compute something based on the size
        and set internal state appropriately ...

    # Define a property.  The 'delete this attribute'
    # method is defined as None, so the attribute
    # can't be deleted.
    size = property(get_size, set_size,
                    None,
                    "Storage size of this instance")

```

That is certainly clearer and easier to write than a pair of __getattr__()/__setattr__() methods that check for the size attribute and handle it specially while retrieving all other attributes from the instance’s __dict__. Accesses to size are also the only ones which have to perform the work of calling a function, so references to other attributes run at their usual speed.

Finally, it’s possible to constrain the list of attributes that can be referenced on an object using the new __slots__ class attribute. Python objects are usually very dynamic; at any time it’s possible to define a new attribute on an instance by just doing obj.new_attr=1. A new-style class can define a class attribute named __slots__ to limit the legal attributes to a particular set of names. An example will make this clear:


>>> class C(object):
...     __slots__ = ('template', 'name')
...
>>> obj = C()
>>> print obj.template
None
>>> obj.template = 'Test'
>>> print obj.template
Test
>>> obj.newattr = None
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
AttributeError: 'C' object has no attribute 'newattr'


Note how you get an AttributeError on the attempt to assign to an attribute not listed in __slots__.


Related Links

This section has just been a quick overview of the new features, giving enough of an explanation to start you programming, but many details have been simplified or ignored. Where should you go to get a more complete picture?

https://docs.python.org/dev/howto/descriptor.html is a lengthy tutorial introduction to the descriptor features, written by Guido van Rossum. If my description has whetted your appetite, go read this tutorial next, because it goes into much more detail about the new features while still remaining quite easy to read.

Next, there are two relevant PEPs, PEP 252 and PEP 253. PEP 252 is titled “Making Types Look More Like Classes”, and covers the descriptor API. PEP 253 is titled “Subtyping Built-in Types”, and describes the changes to type objects that make it possible to subtype built-in objects. PEP 253 is the more complicated PEP of the two, and at a few points the necessary explanations of types and meta-types may cause your head to explode. Both PEPs were written and implemented by Guido van Rossum, with substantial assistance from the rest of the Zope Corp. team.

Finally, there’s the ultimate authority: the source code. Most of the machinery for the type handling is in Objects/typeobject.c, but you should only resort to it after all other avenues have been exhausted, including posting a question to python-list or python-dev.


### ===🗝 PEP 234: Iterators

Another significant addition to 2.2 is an iteration interface at both the C and Python levels. Objects can define how they can be looped over by callers.

In Python versions up to 2.1, the usual way to make for item in obj work is to define a __getitem__() method that looks something like this:


def __getitem__(self, index):
    return <next item>


__getitem__() is more properly used to define an indexing operation on an object so that you can write obj[5] to retrieve the sixth element. It’s a bit misleading when you’re using this only to support for loops. Consider some file-like object that wants to be looped over; the index parameter is essentially meaningless, as the class probably assumes that a series of __getitem__() calls will be made with index incrementing by one each time. In other words, the presence of the __getitem__() method doesn’t mean that using file[5] to randomly access the sixth element will work, though it really should.

In Python 2.2, iteration can be implemented separately, and __getitem__() methods can be limited to classes that really do support random access. The basic idea of iterators is simple. A new built-in function, iter(obj) or iter(C, sentinel), is used to get an iterator. iter(obj) returns an iterator for the object obj, while iter(C, sentinel) returns an iterator that will invoke the callable object C until it returns sentinel to signal that the iterator is done.

Python classes can define an __iter__() method, which should create and return a new iterator for the object; if the object is its own iterator, this method can just return self. In particular, iterators will usually be their own iterators. Extension types implemented in C can implement a tp_iter function in order to return an iterator, and extension types that want to behave as iterators can define a tp_iternext function.

So, after all this, what do iterators actually do? They have one required method, next(), which takes no arguments and returns the next value. When there are no more values to be returned, calling next() should raise the StopIteration exception.


>>> L = [1,2,3]
>>> i = iter(L)
>>> print i
<iterator object at 0x8116870>
>>> i.next()
1
>>> i.next()
2
>>> i.next()
3
>>> i.next()
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
StopIteration
>>>


In 2.2, Python’s for statement no longer expects a sequence; it expects something for which iter() will return an iterator. For backward compatibility and convenience, an iterator is automatically constructed for sequences that don’t implement __iter__() or a tp_iter slot, so for i in [1,2,3] will still work. Wherever the Python interpreter loops over a sequence, it’s been changed to use the iterator protocol. This means you can do things like this:


>>> L = [1,2,3]
>>> i = iter(L)
>>> a,b,c = i
>>> a,b,c
(1, 2, 3)


Iterator support has been added to some of Python’s basic types. Calling iter() on a dictionary will return an iterator which loops over its keys:


>>> m = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
...      'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}
>>> for key in m: print key, m[key]
...
Mar 3
Feb 2
Aug 8
Sep 9
May 5
Jun 6
Jul 7
Jan 1
Apr 4
Nov 11
Dec 12
Oct 10


That’s just the default behaviour. If you want to iterate over keys, values, or key/value pairs, you can explicitly call the iterkeys(), itervalues(), or iteritems() methods to get an appropriate iterator. In a minor related change, the in operator now works on dictionaries, so key in dict is now equivalent to dict.has_key(key).

Files also provide an iterator, which calls the readline() method until there are no more lines in the file. This means you can now read each line of a file using code like this:


    for line in file:
        # do something for each line
        ...


Note that you can only go forward in an iterator; there’s no way to get the previous element, reset the iterator, or make a copy of it. An iterator object could provide such additional capabilities, but the iterator protocol only requires a next() method.

See also:
 PEP 234 - IteratorsWritten by Ka-Ping Yee and GvR; implemented by the Python Labs crew, mostly by GvR and Tim Peters.

### ===🗝 PEP 255: Simple Generators

Generators are another new feature, one that interacts with the introduction of iterators.

You’re doubtless familiar with how function calls work in Python or C. When you call a function, it gets a private namespace where its local variables are created. When the function reaches a return statement, the local variables are destroyed and the resulting value is returned to the caller. A later call to the same function will get a fresh new set of local variables. But, what if the local variables weren’t thrown away on exiting a function? What if you could later resume the function where it left off? This is what generators provide; they can be thought of as resumable functions.

Here’s the simplest example of a generator function:


def generate_ints(N):
    for i in range(N):
        yield i


A new keyword, yield, was introduced for generators. Any function containing a yield statement is a generator function; this is detected by Python’s bytecode compiler which compiles the function specially as a result. Because a new keyword was introduced, generators must be explicitly enabled in a module by including a from __future__ import generators statement near the top of the module’s source code. In Python 2.3 this statement will become unnecessary.

When you call a generator function, it doesn’t return a single value; instead it returns a generator object that supports the iterator protocol. On executing the yield statement, the generator outputs the value of i, similar to a return statement. The big difference between yield and a return statement is that on reaching a yield the generator’s state of execution is suspended and local variables are preserved. On the next call to the generator’s next() method, the function will resume executing immediately after the yield statement. (For complicated reasons, the yield statement isn’t allowed inside the try block of a try…finally statement; read PEP 255 for a full explanation of the interaction between yield and exceptions.)

Here’s a sample usage of the generate_ints() generator:


>>> gen = generate_ints(3)
>>> gen
<generator object at 0x8117f90>
>>> gen.next()
0
>>> gen.next()
1
>>> gen.next()
2
>>> gen.next()
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
  File "<stdin>", line 2, in generate_ints
StopIteration


You could equally write for i in generate_ints(5), or a,b,c = generate_ints(3).

Inside a generator function, the return statement can only be used without a value, and signals the end of the procession of values; afterwards the generator cannot return any further values. return with a value, such as return 5, is a syntax error inside a generator function. The end of the generator’s results can also be indicated by raising StopIteration manually, or by just letting the flow of execution fall off the bottom of the function.

You could achieve the effect of generators manually by writing your own class and storing all the local variables of the generator as instance variables. For example, returning a list of integers could be done by setting self.count to 0, and having the next() method increment self.count and return it. However, for a moderately complicated generator, writing a corresponding class would be much messier. Lib/test/test_generators.py contains a number of more interesting examples. The simplest one implements an in-order traversal of a tree using generators recursively.


```py
# A recursive generator that generates Tree leaves in in-order.
def inorder(t):
    if t:
        for x in inorder(t.left):
            yield x
        yield t.label
        for x in inorder(t.right):
            yield x
```


Two other examples in Lib/test/test_generators.py produce solutions for the N-Queens problem (placing $N$ queens on an $NxN$ chess board so that no queen threatens another) and the Knight’s Tour (a route that takes a knight to every square of an $NxN$ chessboard without visiting any square twice).

The idea of generators comes from other programming languages, especially Icon (https://www.cs.arizona.edu/icon/), where the idea of generators is central. In Icon, every expression and function call behaves like a generator. One example from “An Overview of the Icon Programming Language” at https://www.cs.arizona.edu/icon/docs/ipd266.htm gives an idea of what this looks like:


sentence := "Store it in the neighboring harbor"
if (i := find("or", sentence)) > 5 then write(i)


In Icon the find() function returns the indexes at which the substring “or” is found: 3, 23, 33. In the if statement, i is first assigned a value of 3, but 3 is less than 5, so the comparison fails, and Icon retries it with the second value of 23. 23 is greater than 5, so the comparison now succeeds, and the code prints the value 23 to the screen.

Python doesn’t go nearly as far as Icon in adopting generators as a central concept. Generators are considered a new part of the core Python language, but learning or using them isn’t compulsory; if they don’t solve any problems that you have, feel free to ignore them. One novel feature of Python’s interface as compared to Icon’s is that a generator’s state is represented as a concrete object (the iterator) that can be passed around to other functions or stored in a data structure.

See also:
 PEP 255 - Simple GeneratorsWritten by Neil Schemenauer, Tim Peters, Magnus Lie Hetland. Implemented mostly by Neil Schemenauer and Tim Peters, with other fixes from the Python Labs crew.

### ===🗝 PEP 237: Unifying Long Integers and Integers

In recent versions, the distinction between regular integers, which are 32-bit values on most machines, and long integers, which can be of arbitrary size, was becoming an annoyance. For example, on platforms that support files larger than `2**32` bytes, the tell() method of file objects has to return a long integer. However, there were various bits of Python that expected plain integers and would raise an error if a long integer was provided instead. For example, in Python 1.5, only regular integers could be used as a slice index, and 'abc'[1L:] would raise a TypeError exception with the message ‘slice index must be int’.

Python 2.2 will shift values from short to long integers as required. The ‘L’ suffix is no longer needed to indicate a long integer literal, as now the compiler will choose the appropriate type. (Using the ‘L’ suffix will be discouraged in future 2.x versions of Python, triggering a warning in Python 2.4, and probably dropped in Python 3.0.) Many operations that used to raise an OverflowError will now return a long integer as their result. For example:


>>> 1234567890123
1234567890123L
>>> 2 ** 64
18446744073709551616L


In most cases, integers and long integers will now be treated identically. You can still distinguish them with the type() built-in function, but that’s rarely needed.

See also:
 PEP 237 - Unifying Long Integers and IntegersWritten by Moshe Zadka and Guido van Rossum. Implemented mostly by Guido van Rossum.

### ===🗝 PEP 238: Changing the Division Operator

The most controversial change in Python 2.2 heralds the start of an effort to fix an old design flaw that’s been in Python from the beginning. Currently Python’s division operator, /, behaves like C’s division operator when presented with two integer arguments: it returns an integer result that’s truncated down when there would be a fractional part. For example, 3/2 is 1, not 1.5, and (-1)/2 is -1, not -0.5. This means that the results of division can vary unexpectedly depending on the type of the two operands and because Python is dynamically typed, it can be difficult to determine the possible types of the operands.

(The controversy is over whether this is really a design flaw, and whether it’s worth breaking existing code to fix this. It’s caused endless discussions on python-dev, and in July 2001 erupted into a storm of acidly sarcastic postings on comp.lang.python. I won’t argue for either side here and will stick to describing what’s implemented in 2.2. Read PEP 238 for a summary of arguments and counter-arguments.)

Because this change might break code, it’s being introduced very gradually. Python 2.2 begins the transition, but the switch won’t be complete until Python 3.0.

First, I’ll borrow some terminology from PEP 238. “True division” is the division that most non-programmers are familiar with: 3/2 is 1.5, 1/4 is 0.25, and so forth. “Floor division” is what Python’s / operator currently does when given integer operands; the result is the floor of the value returned by true division. “Classic division” is the current mixed behaviour of /; it returns the result of floor division when the operands are integers, and returns the result of true division when one of the operands is a floating-point number.

Here are the changes 2.2 introduces:


• A new operator, //, is the floor division operator. (Yes, we know it looks like C++’s comment symbol.) // always performs floor division no matter what the types of its operands are, so 1 // 2 is 0 and 1.0 // 2.0 is also 0.0.

// is always available in Python 2.2; you don’t need to enable it using a __future__ statement.



• By including a from __future__ import division in a module, the / operator will be changed to return the result of true division, so 1/2 is 0.5. Without the __future__ statement, / still means classic division. The default meaning of / will not change until Python 3.0.



• Classes can define methods called __truediv__() and __floordiv__() to overload the two division operators. At the C level, there are also slots in the PyNumberMethods structure so extension types can define the two operators.



• Python 2.2 supports some command-line arguments for testing whether code will work with the changed division semantics. Running python with -Q warn will cause a warning to be issued whenever division is applied to two integers. You can use this to find code that’s affected by the change and fix it. By default, Python 2.2 will simply perform classic division without a warning; the warning will be turned on by default in Python 2.3.


See also:
 PEP 238 - Changing the Division OperatorWritten by Moshe Zadka and Guido van Rossum. Implemented by Guido van Rossum..

### ===🗝 Unicode Changes

Python’s Unicode support has been enhanced a bit in 2.2. Unicode strings are usually stored as UCS-2, as 16-bit unsigned integers. Python 2.2 can also be compiled to use UCS-4, 32-bit unsigned integers, as its internal encoding by supplying --enable-unicode=ucs4 to the configure script. (It’s also possible to specify --disable-unicode to completely disable Unicode support.)

When built to use UCS-4 (a “wide Python”), the interpreter can natively handle Unicode characters from U+000000 to U+110000, so the range of legal values for the unichr() function is expanded accordingly. Using an interpreter compiled to use UCS-2 (a “narrow Python”), values greater than 65535 will still cause unichr() to raise a ValueError exception. This is all described in PEP 261, “Support for ‘wide’ Unicode characters”; consult it for further details.

Another change is simpler to explain. Since their introduction, Unicode strings have supported an encode() method to convert the string to a selected encoding such as UTF-8 or Latin-1. A symmetric decode([*encoding*]) method has been added to 8-bit strings (though not to Unicode strings) in 2.2. decode() assumes that the string is in the specified encoding and decodes it, returning whatever is returned by the codec.

Using this new feature, codecs have been added for tasks not directly related to Unicode. For example, codecs have been added for uu-encoding, MIME’s base64 encoding, and compression with the zlib module:


>>> s = """Here is a lengthy piece of redundant, overly verbose,
... and repetitive text.
... """
>>> data = s.encode('zlib')
>>> data
'x\x9c\r\xc9\xc1\r\x80 \x10\x04\xc0?Ul...'
>>> data.decode('zlib')
'Here is a lengthy piece of redundant, overly verbose,\nand repetitive text.\n'
>>> print s.encode('uu')
begin 666 <data>
M2&5R92!I<R!A(&QE;F=T:'D@<&EE8V4@;V8@<F5D=6YD86YT+"!O=F5R;'D@
>=F5R8F]S92P*86YD(')E<&5T:71I=F4@=&5X="X*

end
>>> "sheesh".encode('rot-13')
'furrfu'


To convert a class instance to Unicode, a __unicode__() method can be defined by a class, analogous to __str__().

encode(), decode(), and __unicode__() were implemented by Marc-André Lemburg. The changes to support using UCS-4 internally were implemented by Fredrik Lundh and Martin von Löwis.

See also:
 PEP 261 - Support for ‘wide’ Unicode charactersWritten by Paul Prescod.

### ===🗝 PEP 227: Nested Scopes

In Python 2.1, statically nested scopes were added as an optional feature, to be enabled by a from __future__ import nested_scopes directive. In 2.2 nested scopes no longer need to be specially enabled, and are now always present. The rest of this section is a copy of the description of nested scopes from my “What’s New in Python 2.1” document; if you read it when 2.1 came out, you can skip the rest of this section.

The largest change introduced in Python 2.1, and made complete in 2.2, is to Python’s scoping rules. In Python 2.0, at any given time there are at most three namespaces used to look up variable names: local, module-level, and the built-in namespace. This often surprised people because it didn’t match their intuitive expectations. For example, a nested recursive function definition doesn’t work:


```py
def f():
    ...
    def g(value):
        ...
        return g(value-1) + 1
    ...
```


The function g() will always raise a NameError exception, because the binding of the name g isn’t in either its local namespace or in the module-level namespace. This isn’t much of a problem in practice (how often do you recursively define interior functions like this?), but this also made using the lambda expression clumsier, and this was a problem in practice. In code which uses lambda you can often find local variables being copied by passing them as the default values of arguments.


```py
def find(self, name):
    "Return list of any entries equal to 'name'"
    L = filter(lambda x, name=name: x == name,
               self.list_attribute)
    return L
```


The readability of Python code written in a strongly functional style suffers greatly as a result.

The most significant change to Python 2.2 is that static scoping has been added to the language to fix this problem. As a first effect, the name=name default argument is now unnecessary in the above example. Put simply, when a given variable name is not assigned a value within a function (by an assignment, or the def, class, or import statements), references to the variable will be looked up in the local namespace of the enclosing scope. A more detailed explanation of the rules, and a dissection of the implementation, can be found in the PEP.

This change may cause some compatibility problems for code where the same variable name is used both at the module level and as a local variable within a function that contains further function definitions. This seems rather unlikely though, since such code would have been pretty confusing to read in the first place.

One side effect of the change is that the from module import * and exec statements have been made illegal inside a function scope under certain conditions. The Python reference manual has said all along that from module import * is only legal at the top level of a module, but the CPython interpreter has never enforced this before. As part of the implementation of nested scopes, the compiler which turns Python source into bytecodes has to generate different code to access variables in a containing scope. from module import * and exec make it impossible for the compiler to figure this out, because they add names to the local namespace that are unknowable at compile time. Therefore, if a function contains function definitions or lambda expressions with free variables, the compiler will flag this by raising a SyntaxError exception.

To make the preceding explanation a bit clearer, here’s an example:


```py
x = 1
def f():
    # The next line is a syntax error
    exec 'x=2'
    def g():
        return x
```


Line 4 containing the exec statement is a syntax error, since exec would define a new local variable named x whose value should be accessed by g().

This shouldn’t be much of a limitation, since exec is rarely used in most Python code (and when it is used, it’s often a sign of a poor design anyway).

See also:
 PEP 227 - Statically Nested ScopesWritten and implemented by Jeremy Hylton.


### ===🗝 New and Improved Modules


• The xmlrpclib module was contributed to the standard library by Fredrik Lundh, providing support for writing XML-RPC clients. XML-RPC is a simple remote procedure call protocol built on top of HTTP and XML. For example, the following snippet retrieves a list of RSS channels from the O’Reilly Network, and then lists the recent headlines for one channel:


```py
import xmlrpclib
s = xmlrpclib.Server(
      'http://www.oreillynet.com/meerkat/xml-rpc/server.php')
channels = s.meerkat.getChannels()
# channels is a list of dictionaries, like this:
# [{'id': 4, 'title': 'Freshmeat Daily News'}
#  {'id': 190, 'title': '32Bits Online'},
#  {'id': 4549, 'title': '3DGamers'}, ... ]

# Get the items for one channel
items = s.meerkat.getItems( {'channel': 4} )

# 'items' is another list of dictionaries, like this:
# [{'link': 'http://freshmeat.net/releases/52719/',
#   'description': 'A utility which converts HTML to XSL FO.',
#   'title': 'html2fo 0.3 (Default)'}, ... ]
```


The SimpleXMLRPCServer module makes it easy to create straightforward XML-RPC servers. See http://xmlrpc.scripting.com/ for more information about XML-RPC.



• The new hmac module implements the HMAC algorithm described by RFC 2104. (Contributed by Gerhard Häring.)



• Several functions that originally returned lengthy tuples now return pseudo-sequences that still behave like tuples but also have mnemonic attributes such as memberst_mtime or tm_year. The enhanced functions include stat(), fstat(), statvfs(), and fstatvfs() in the os module, and localtime(), gmtime(), and strptime() in the time module.

For example, to obtain a file’s size using the old tuples, you’d end up writing something like file_size = os.stat(filename)[stat.ST_SIZE], but now this can be written more clearly as file_size = os.stat(filename).st_size.

The original patch for this feature was contributed by Nick Mathewson.



• The Python profiler has been extensively reworked and various errors in its output have been corrected. (Contributed by Fred L. Drake, Jr. and Tim Peters.)



• The socket module can be compiled to support IPv6; specify the --enable-ipv6 option to Python’s configure script. (Contributed by Jun-ichiro “itojun” Hagino.)



• Two new format characters were added to the struct module for 64-bit integers on platforms that support the C long long type. q is for a signed 64-bit integer, and Q is for an unsigned one. The value is returned in Python’s long integer type. (Contributed by Tim Peters.)



• In the interpreter’s interactive mode, there’s a new built-in function help() that uses the pydoc module introduced in Python 2.1 to provide interactive help. help(object) displays any available help text about object. help() with no argument puts you in an online help utility, where you can enter the names of functions, classes, or modules to read their help text. (Contributed by Guido van Rossum, using Ka-Ping Yee’s pydoc module.)



• Various bugfixes and performance improvements have been made to the SRE engine underlying the re module. For example, the re.sub() and re.split() functions have been rewritten in C. Another contributed patch speeds up certain Unicode character ranges by a factor of two, and a new finditer() method that returns an iterator over all the non-overlapping matches in a given string. (SRE is maintained by Fredrik Lundh. The BIGCHARSET patch was contributed by Martin von Löwis.)



• The smtplib module now supports RFC 2487, “Secure SMTP over TLS”, so it’s now possible to encrypt the SMTP traffic between a Python program and the mail transport agent being handed a message. smtplib also supports SMTP authentication. (Contributed by Gerhard Häring.)



• The imaplib module, maintained by Piers Lauder, has support for several new extensions: the NAMESPACE extension defined in RFC 2342, SORT, GETACL and SETACL. (Contributed by Anthony Baxter and Michel Pelletier.)



• The rfc822 module’s parsing of email addresses is now compliant with RFC 2822, an update to RFC 822. (The module’s name is not going to be changed to rfc2822.) A new package, email, has also been added for parsing and generating e-mail messages. (Contributed by Barry Warsaw, and arising out of his work on Mailman.)



• The difflib module now contains a new Differ class for producing human-readable lists of changes (a “delta”) between two sequences of lines of text. There are also two generator functions, ndiff() and restore(), which respectively return a delta from two sequences, or one of the original sequences from a delta. (Grunt work contributed by David Goodger, from ndiff.py code by Tim Peters who then did the generatorization.)



• New constants ascii_letters, ascii_lowercase, and ascii_uppercase were added to the string module. There were several modules in the standard library that used string.letters to mean the ranges A-Za-z, but that assumption is incorrect when locales are in use, because string.letters varies depending on the set of legal characters defined by the current locale. The buggy modules have all been fixed to use ascii_letters instead. (Reported by an unknown person; fixed by Fred L. Drake, Jr.)



• The mimetypes module now makes it easier to use alternative MIME-type databases by the addition of a MimeTypes class, which takes a list of filenames to be parsed. (Contributed by Fred L. Drake, Jr.)



• A Timer class was added to the threading module that allows scheduling an activity to happen at some future time. (Contributed by Itamar Shtull-Trauring.)



### ===🗝 Interpreter Changes and Fixes

Some of the changes only affect people who deal with the Python interpreter at the C level because they’re writing Python extension modules, embedding the interpreter, or just hacking on the interpreter itself. If you only write Python code, none of the changes described here will affect you very much.


• Profiling and tracing functions can now be implemented in C, which can operate at much higher speeds than Python-based functions and should reduce the overhead of profiling and tracing. This will be of interest to authors of development environments for Python. Two new C functions were added to Python’s API, PyEval_SetProfile() and PyEval_SetTrace(). The existing sys.setprofile() and sys.settrace() functions still exist, and have simply been changed to use the new C-level interface. (Contributed by Fred L. Drake, Jr.)



• Another low-level API, primarily of interest to implementors of Python debuggers and development tools, was added. PyInterpreterState_Head() and PyInterpreterState_Next() let a caller walk through all the existing interpreter objects; PyInterpreterState_ThreadHead() and PyThreadState_Next() allow looping over all the thread states for a given interpreter. (Contributed by David Beazley.)



• The C-level interface to the garbage collector has been changed to make it easier to write extension types that support garbage collection and to debug misuses of the functions. Various functions have slightly different semantics, so a bunch of functions had to be renamed. Extensions that use the old API will still compile but will not participate in garbage collection, so updating them for 2.2 should be considered fairly high priority.

To upgrade an extension module to the new API, perform the following steps:



• Rename Py_TPFLAGS_GC() to PyTPFLAGS_HAVE_GC().


• Use PyObject_GC_New() or PyObject_GC_NewVar() to allocate
objects, and PyObject_GC_Del() to deallocate them.


• Rename PyObject_GC_Init() to PyObject_GC_Track() and
PyObject_GC_Fini() to PyObject_GC_UnTrack().



• Remove PyGC_HEAD_SIZE() from object size calculations.



• Remove calls to PyObject_AS_GC() and PyObject_FROM_GC().



• A new et format sequence was added to PyArg_ParseTuple(); et takes both a parameter and an encoding name, and converts the parameter to the given encoding if the parameter turns out to be a Unicode string, or leaves it alone if it’s an 8-bit string, assuming it to already be in the desired encoding. This differs from the es format character, which assumes that 8-bit strings are in Python’s default ASCII encoding and converts them to the specified new encoding. (Contributed by M.-A. Lemburg, and used for the MBCS support on Windows described in the following section.)



• A different argument parsing function, PyArg_UnpackTuple(), has been added that’s simpler and presumably faster. Instead of specifying a format string, the caller simply gives the minimum and maximum number of arguments expected, and a set of pointers to PyObject* variables that will be filled in with argument values.



• Two new flags METH_NOARGS and METH_O are available in method definition tables to simplify implementation of methods with no arguments or a single untyped argument. Calling such methods is more efficient than calling a corresponding method that uses METH_VARARGS. Also, the old METH_OLDARGS style of writing C methods is now officially deprecated.



• Two new wrapper functions, PyOS_snprintf() and PyOS_vsnprintf() were added to provide cross-platform implementations for the relatively new snprintf() and vsnprintf() C lib APIs. In contrast to the standard sprintf() and vsprintf() functions, the Python versions check the bounds of the buffer used to protect against buffer overruns. (Contributed by M.-A. Lemburg.)



• The `_PyTuple_Resize()` function has lost an unused parameter, so now it takes 2 parameters instead of 3. The third argument was never used, and can simply be discarded when porting code from earlier versions to Python 2.2.



### ===🗝 Other Changes and Fixes

As usual there were a bunch of other improvements and bugfixes scattered throughout the source tree. A search through the CVS change logs finds there were 527 patches applied and 683 bugs fixed between Python 2.1 and 2.2; 2.2.1 applied 139 patches and fixed 143 bugs; 2.2.2 applied 106 patches and fixed 82 bugs. These figures are likely to be underestimates.

Some of the more notable changes are:


• The code for the MacOS port for Python, maintained by Jack Jansen, is now kept in the main Python CVS tree, and many changes have been made to support MacOS X.

The most significant change is the ability to build Python as a framework, enabled by supplying the --enable-framework option to the configure script when compiling Python. According to Jack Jansen, “This installs a self-contained Python installation plus the OS X framework “glue” into /Library/Frameworks/Python.framework (or another location of choice). For now there is little immediate added benefit to this (actually, there is the disadvantage that you have to change your PATH to be able to find Python), but it is the basis for creating a full-blown Python application, porting the MacPython IDE, possibly using Python as a standard OSA scripting language and much more.”

Most of the MacPython toolbox modules, which interface to MacOS APIs such as windowing, QuickTime, scripting, etc. have been ported to OS X, but they’ve been left commented out in setup.py. People who want to experiment with these modules can uncomment them manually.



• Keyword arguments passed to built-in functions that don’t take them now cause a TypeError exception to be raised, with the message “function takes no keyword arguments”.



• Weak references, added in Python 2.1 as an extension module, are now part of the core because they’re used in the implementation of new-style classes. The ReferenceError exception has therefore moved from the weakref module to become a built-in exception.



• A new script, Tools/scripts/cleanfuture.py by Tim Peters, automatically removes obsolete __future__ statements from Python source code.



• An additional flags argument has been added to the built-in function compile(), so the behaviour of __future__ statements can now be correctly observed in simulated shells, such as those presented by IDLE and other development environments. This is described in PEP 264. (Contributed by Michael Hudson.)



• The new license introduced with Python 1.6 wasn’t GPL-compatible. This is fixed by some minor textual changes to the 2.2 license, so it’s now legal to embed Python inside a GPLed program again. Note that Python itself is not GPLed, but instead is under a license that’s essentially equivalent to the BSD license, same as it always was. The license changes were also applied to the Python 2.0.1 and 2.1.1 releases.



• When presented with a Unicode filename on Windows, Python will now convert it to an MBCS encoded string, as used by the Microsoft file APIs. As MBCS is explicitly used by the file APIs, Python’s choice of ASCII as the default encoding turns out to be an annoyance. On Unix, the locale’s character set is used if locale.nl_langinfo(CODESET) is available. (Windows support was contributed by Mark Hammond with assistance from Marc-André Lemburg. Unix support was added by Martin von Löwis.)



• Large file support is now enabled on Windows. (Contributed by Tim Peters.)



• The Tools/scripts/ftpmirror.py script now parses a .netrc file, if you have one. (Contributed by Mike Romberg.)



• Some features of the object returned by the xrange() function are now deprecated, and trigger warnings when they’re accessed; they’ll disappear in Python 2.3. xrange objects tried to pretend they were full sequence types by supporting slicing, sequence multiplication, and the in operator, but these features were rarely used and therefore buggy. The tolist() method and the start, stop, and step attributes are also being deprecated. At the C level, the fourth argument to the PyRange_New() function, repeat, has also been deprecated.



• There were a bunch of patches to the dictionary implementation, mostly to fix potential core dumps if a dictionary contains objects that sneakily changed their hash value, or mutated the dictionary they were contained in. For a while python-dev fell into a gentle rhythm of Michael Hudson finding a case that dumped core, Tim Peters fixing the bug, Michael finding another case, and round and round it went.



• On Windows, Python can now be compiled with Borland C thanks to a number of patches contributed by Stephen Hansen, though the result isn’t fully functional yet. (But this is progress…)



• Another Windows enhancement: Wise Solutions generously offered PythonLabs use of their InstallerMaster 8.1 system. Earlier PythonLabs Windows installers used Wise 5.0a, which was beginning to show its age. (Packaged up by Tim Peters.)



• Files ending in .pyw can now be imported on Windows. .pyw is a Windows-only thing, used to indicate that a script needs to be run using PYTHONW.EXE instead of PYTHON.EXE in order to prevent a DOS console from popping up to display the output. This patch makes it possible to import such scripts, in case they’re also usable as modules. (Implemented by David Bolen.)



• On platforms where Python uses the C dlopen() function to load extension modules, it’s now possible to set the flags used by dlopen() using the sys.getdlopenflags() and sys.setdlopenflags() functions. (Contributed by Bram Stolk.)



• The pow() built-in function no longer supports 3 arguments when floating-point numbers are supplied. pow(x, y, z) returns `(x**y) % z`, but this is never useful for floating point numbers, and the final result varies unpredictably depending on the platform. A call such as pow(2.0, 8.0, 7.0) will now raise a TypeError exception.



### ===🗝 Acknowledgements

The author would like to thank the following people for offering suggestions, corrections and assistance with various drafts of this article: Fred Bremmer, Keith Briggs, Andrew Dalke, Fred L. Drake, Jr., Carel Fellinger, David Goodger, Mark Hammond, Stephen Hansen, Michael Hudson, Jack Jansen, Marc-André Lemburg, Martin von Löwis, Fredrik Lundh, Michael McLay, Nick Mathewson, Paul Moore, Gustavo Niemeyer, Don O’Donnell, Joonas Paalasma, Tim Peters, Jens Quade, Tom Reinhardt, Neil Schemenauer, Guido van Rossum, Greg Ward, Edward Welbourne.


## ==⚡ • What’s New in Python 2.1
◦ Introduction
◦ PEP 227: Nested Scopes
◦ PEP 236: __future__ Directives
◦ PEP 207: Rich Comparisons
◦ PEP 230: Warning Framework
◦ PEP 229: New Build System
◦ PEP 205: Weak References
◦ PEP 232: Function Attributes
◦ PEP 235: Importing Modules on Case-Insensitive Platforms
◦ PEP 217: Interactive Display Hook
◦ PEP 208: New Coercion Model
◦ PEP 241: Metadata in Python Packages
◦ New and Improved Modules
◦ Other Changes and Fixes
◦ Acknowledgements

What’s New in Python 2.1
Author:
A.M. Kuchling 


### ===🗝 Introduction

This article explains the new features in Python 2.1. While there aren’t as many changes in 2.1 as there were in Python 2.0, there are still some pleasant surprises in store. 2.1 is the first release to be steered through the use of Python Enhancement Proposals, or PEPs, so most of the sizable changes have accompanying PEPs that provide more complete documentation and a design rationale for the change. This article doesn’t attempt to document the new features completely, but simply provides an overview of the new features for Python programmers. Refer to the Python 2.1 documentation, or to the specific PEP, for more details about any new feature that particularly interests you.

One recent goal of the Python development team has been to accelerate the pace of new releases, with a new release coming every 6 to 9 months. 2.1 is the first release to come out at this faster pace, with the first alpha appearing in January, 3 months after the final version of 2.0 was released.

The final release of Python 2.1 was made on April 17, 2001.


### ===🗝 PEP 227: Nested Scopes

The largest change in Python 2.1 is to Python’s scoping rules. In Python 2.0, at any given time there are at most three namespaces used to look up variable names: local, module-level, and the built-in namespace. This often surprised people because it didn’t match their intuitive expectations. For example, a nested recursive function definition doesn’t work:


```py
def f():
    ...
    def g(value):
        ...
        return g(value-1) + 1
    ...
```


The function g() will always raise a NameError exception, because the binding of the name g isn’t in either its local namespace or in the module-level namespace. This isn’t much of a problem in practice (how often do you recursively define interior functions like this?), but this also made using the lambda expression clumsier, and this was a problem in practice. In code which uses lambda you can often find local variables being copied by passing them as the default values of arguments.


```py
def find(self, name):
    "Return list of any entries equal to 'name'"
    L = filter(lambda x, name=name: x == name,
               self.list_attribute)
    return L
```


The readability of Python code written in a strongly functional style suffers greatly as a result.

The most significant change to Python 2.1 is that static scoping has been added to the language to fix this problem. As a first effect, the name=name default argument is now unnecessary in the above example. Put simply, when a given variable name is not assigned a value within a function (by an assignment, or the def, class, or import statements), references to the variable will be looked up in the local namespace of the enclosing scope. A more detailed explanation of the rules, and a dissection of the implementation, can be found in the PEP.

This change may cause some compatibility problems for code where the same variable name is used both at the module level and as a local variable within a function that contains further function definitions. This seems rather unlikely though, since such code would have been pretty confusing to read in the first place.

One side effect of the change is that the from module import * and exec statements have been made illegal inside a function scope under certain conditions. The Python reference manual has said all along that from module import * is only legal at the top level of a module, but the CPython interpreter has never enforced this before. As part of the implementation of nested scopes, the compiler which turns Python source into bytecodes has to generate different code to access variables in a containing scope. from module import * and exec make it impossible for the compiler to figure this out, because they add names to the local namespace that are unknowable at compile time. Therefore, if a function contains function definitions or lambda expressions with free variables, the compiler will flag this by raising a SyntaxError exception.

To make the preceding explanation a bit clearer, here’s an example:


```py
x = 1
def f():
    # The next line is a syntax error
    exec 'x=2'
    def g():
        return x
```


Line 4 containing the exec statement is a syntax error, since exec would define a new local variable named x whose value should be accessed by g().

This shouldn’t be much of a limitation, since exec is rarely used in most Python code (and when it is used, it’s often a sign of a poor design anyway).

Compatibility concerns have led to nested scopes being introduced gradually; in Python 2.1, they aren’t enabled by default, but can be turned on within a module by using a future statement as described in PEP 236. (See the following section for further discussion of PEP 236.) In Python 2.2, nested scopes will become the default and there will be no way to turn them off, but users will have had all of 2.1’s lifetime to fix any breakage resulting from their introduction.

See also:
 PEP 227 - Statically Nested ScopesWritten and implemented by Jeremy Hylton.


### ===🗝 PEP 236: __future__ Directives

The reaction to nested scopes was widespread concern about the dangers of breaking code with the 2.1 release, and it was strong enough to make the Pythoneers take a more conservative approach. This approach consists of introducing a convention for enabling optional functionality in release N that will become compulsory in release N+1.

The syntax uses a from...import statement using the reserved module name __future__. Nested scopes can be enabled by the following statement:


    from __future__ import nested_scopes


While it looks like a normal import statement, it’s not; there are strict rules on where such a future statement can be put. They can only be at the top of a module, and must precede any Python code or regular import statements. This is because such statements can affect how the Python bytecode compiler parses code and generates bytecode, so they must precede any statement that will result in bytecodes being produced.

See also:
 PEP 236 - Back to the `__future__Written` by Tim Peters, and primarily implemented by Jeremy Hylton.


### ===🗝 PEP 207: Rich Comparisons

In earlier versions, Python’s support for implementing comparisons on user-defined classes and extension types was quite simple. Classes could implement a __cmp__() method that was given two instances of a class, and could only return 0 if they were equal or +1 or -1 if they weren’t; the method couldn’t raise an exception or return anything other than a Boolean value. Users of Numeric Python often found this model too weak and restrictive, because in the number-crunching programs that numeric Python is used for, it would be more useful to be able to perform elementwise comparisons of two matrices, returning a matrix containing the results of a given comparison for each element. If the two matrices are of different sizes, then the compare has to be able to raise an exception to signal the error.

In Python 2.1, rich comparisons were added in order to support this need. Python classes can now individually overload each of the <, <=, >, >=, ==, and != operations. The new magic method names are:

| Operation | Method name |
|-----------|-------------|
| <         | __lt__()    |
| <=        | __le__()    |
| >         | __gt__()    |
| >=        | __ge__()    |
| ==        | __eq__()    |
| !=        | __ne__()    |

(The magic methods are named after the corresponding Fortran operators .LT.. .LE., &c. Numeric programmers are almost certainly quite familiar with these names and will find them easy to remember.)

Each of these magic methods is of the form method(self, other), where self will be the object on the left-hand side of the operator, while other will be the object on the right-hand side. For example, the expression A < B will cause A.__lt__(B) to be called.

Each of these magic methods can return anything at all: a Boolean, a matrix, a list, or any other Python object. Alternatively they can raise an exception if the comparison is impossible, inconsistent, or otherwise meaningless.

The built-in cmp(A,B) function can use the rich comparison machinery, and now accepts an optional argument specifying which comparison operation to use; this is given as one of the strings "<", "<=", ">", ">=", "==", or "!=". If called without the optional third argument, cmp() will only return -1, 0, or +1 as in previous versions of Python; otherwise it will call the appropriate method and can return any Python object.

There are also corresponding changes of interest to C programmers; there’s a new slot tp_richcmp in type objects and an API for performing a given rich comparison. I won’t cover the C API here, but will refer you to PEP 207, or to 2.1’s C API documentation, for the full list of related functions.

See also:
 PEP 207 - Rich ComparisonsWritten by Guido van Rossum, heavily based on earlier work by David Ascher, and implemented by Guido van Rossum.

### ===🗝 PEP 230: Warning Framework

Over its 10 years of existence, Python has accumulated a certain number of obsolete modules and features along the way. It’s difficult to know when a feature is safe to remove, since there’s no way of knowing how much code uses it — perhaps no programs depend on the feature, or perhaps many do. To enable removing old features in a more structured way, a warning framework was added. When the Python developers want to get rid of a feature, it will first trigger a warning in the next version of Python. The following Python version can then drop the feature, and users will have had a full release cycle to remove uses of the old feature.

Python 2.1 adds the warning framework to be used in this scheme. It adds a warnings module that provide functions to issue warnings, and to filter out warnings that you don’t want to be displayed. Third-party modules can also use this framework to deprecate old features that they no longer wish to support.

For example, in Python 2.1 the regex module is deprecated, so importing it causes a warning to be printed:


>>> import regex
__main__:1: DeprecationWarning: the regex module
         is deprecated; please use the re module
>>>


Warnings can be issued by calling the warnings.warn() function:


warnings.warn("feature X no longer supported")


The first parameter is the warning message; an additional optional parameters can be used to specify a particular warning category.

Filters can be added to disable certain warnings; a regular expression pattern can be applied to the message or to the module name in order to suppress a warning. For example, you may have a program that uses the regex module and not want to spare the time to convert it to use the re module right now. The warning can be suppressed by calling


```py
import warnings
warnings.filterwarnings(action = 'ignore',
                        message='.*regex module is deprecated',
                        category=DeprecationWarning,
                        module = '__main__')
```

This adds a filter that will apply only to warnings of the class DeprecationWarning triggered in the __main__ module, and applies a regular expression to only match the message about the regex module being deprecated, and will cause such warnings to be ignored. Warnings can also be printed only once, printed every time the offending code is executed, or turned into exceptions that will cause the program to stop (unless the exceptions are caught in the usual way, of course).

Functions were also added to Python’s C API for issuing warnings; refer to PEP 230 or to Python’s API documentation for the details.

See also:
 PEP 5 - Guidelines for Language EvolutionWritten by Paul Prescod, to specify procedures to be followed when removing old features from Python. The policy described in this PEP hasn’t been officially adopted, but the eventual policy probably won’t be too different from Prescod’s proposal.PEP 230 - Warning FrameworkWritten and implemented by Guido van Rossum.

### ===🗝 PEP 229: New Build System

When compiling Python, the user had to go in and edit the Modules/Setup file in order to enable various additional modules; the default set is relatively small and limited to modules that compile on most Unix platforms. This means that on Unix platforms with many more features, most notably Linux, Python installations often don’t contain all useful modules they could.

Python 2.0 added the Distutils, a set of modules for distributing and installing extensions. In Python 2.1, the Distutils are used to compile much of the standard library of extension modules, autodetecting which ones are supported on the current machine. It’s hoped that this will make Python installations easier and more featureful.

Instead of having to edit the Modules/Setup file in order to enable modules, a setup.py script in the top directory of the Python source distribution is run at build time, and attempts to discover which modules can be enabled by examining the modules and header files on the system. If a module is configured in Modules/Setup, the setup.py script won’t attempt to compile that module and will defer to the Modules/Setup file’s contents. This provides a way to specific any strange command-line flags or libraries that are required for a specific platform.

In another far-reaching change to the build mechanism, Neil Schemenauer restructured things so Python now uses a single makefile that isn’t recursive, instead of makefiles in the top directory and in each of the Python/, Parser/, Objects/, and Modules/ subdirectories. This makes building Python faster and also makes hacking the Makefiles clearer and simpler.

See also:
 PEP 229 - Using Distutils to Build PythonWritten and implemented by A.M. Kuchling.

### ===🗝 PEP 205: Weak References

Weak references, available through the weakref module, are a minor but useful new data type in the Python programmer’s toolbox.

Storing a reference to an object (say, in a dictionary or a list) has the side effect of keeping that object alive forever. There are a few specific cases where this behaviour is undesirable, object caches being the most common one, and another being circular references in data structures such as trees.

For example, consider a memoizing function that caches the results of another function f(x) by storing the function’s argument and its result in a dictionary:


```py
_cache = {}
def memoize(x):
    if _cache.has_key(x):
        return _cache[x]

    retval = f(x)

    # Cache the returned object
    _cache[x] = retval

    return retval
```


This version works for simple things such as integers, but it has a side effect; the `_cache` dictionary holds a reference to the return values, so they’ll never be deallocated until the Python process exits and cleans up. This isn’t very noticeable for integers, but if f() returns an object, or a data structure that takes up a lot of memory, this can be a problem.

Weak references provide a way to implement a cache that won’t keep objects alive beyond their time. If an object is only accessible through weak references, the object will be deallocated and the weak references will now indicate that the object it referred to no longer exists. A weak reference to an object obj is created by calling wr = weakref.ref(obj). The object being referred to is returned by calling the weak reference as if it were a function: wr(). It will return the referenced object, or None if the object no longer exists.

This makes it possible to write a memoize() function whose cache doesn’t keep objects alive, by storing weak references in the cache.


```py
_cache = {}
def memoize(x):
    if _cache.has_key(x):
        obj = _cache[x]()
        # If weak reference object still exists,
        # return it
        if obj is not None: return obj

    retval = f(x)

    # Cache a weak reference
    _cache[x] = weakref.ref(retval)

    return retval
```


The weakref module also allows creating proxy objects which behave like weak references — an object referenced only by proxy objects is deallocated – but instead of requiring an explicit call to retrieve the object, the proxy transparently forwards all operations to the object as long as the object still exists. If the object is deallocated, attempting to use a proxy will cause a weakref.ReferenceError exception to be raised.


```py
proxy = weakref.proxy(obj)
proxy.attr   # Equivalent to obj.attr
proxy.meth() # Equivalent to obj.meth()
del obj
proxy.attr   # raises weakref.ReferenceError
```


See also:
 PEP 205 - Weak ReferencesWritten and implemented by Fred L. Drake, Jr.

### ===🗝 PEP 232: Function Attributes

In Python 2.1, functions can now have arbitrary information attached to them. People were often using docstrings to hold information about functions and methods, because the __doc__ attribute was the only way of attaching any information to a function. For example, in the Zope web application server, functions are marked as safe for public access by having a docstring, and in John Aycock’s SPARK parsing framework, docstrings hold parts of the BNF grammar to be parsed. This overloading is unfortunate, since docstrings are really intended to hold a function’s documentation; for example, it means you can’t properly document functions intended for private use in Zope.

Arbitrary attributes can now be set and retrieved on functions using the regular Python syntax:


```py
def f(): pass

f.publish = 1
f.secure = 1
f.grammar = "A ::= B (C D)*"

```

The dictionary containing attributes can be accessed as the function’s __dict__. Unlike the __dict__ attribute of class instances, in functions you can actually assign a new dictionary to __dict__, though the new value is restricted to a regular Python dictionary; you can’t be tricky and set it to a UserDict instance, or any other random object that behaves like a mapping.

See also:
 PEP 232 - Function AttributesWritten and implemented by Barry Warsaw.

### ===🗝 PEP 235: Importing Modules on Case-Insensitive Platforms

Some operating systems have filesystems that are case-insensitive, MacOS and Windows being the primary examples; on these systems, it’s impossible to distinguish the filenames FILE.PY and file.py, even though they do store the file’s name in its original case (they’re case-preserving, too).

In Python 2.1, the import statement will work to simulate case-sensitivity on case-insensitive platforms. Python will now search for the first case-sensitive match by default, raising an ImportError if no such file is found, so import file will not import a module named FILE.PY. Case-insensitive matching can be requested by setting the PYTHONCASEOK environment variable before starting the Python interpreter.


### ===🗝 PEP 217: Interactive Display Hook

When using the Python interpreter interactively, the output of commands is displayed using the built-in repr() function. In Python 2.1, the variable sys.displayhook() can be set to a callable object which will be called instead of repr(). For example, you can set it to a special pretty-printing function:


>>> # Create a recursive data structure
... L = [1,2,3]
>>> L.append(L)
>>> L # Show Python's default output
[1, 2, 3, [...]]
>>> # Use pprint.pprint() as the display function
... import sys, pprint
>>> sys.displayhook = pprint.pprint
>>> L
[1, 2, 3,  <Recursion on list with id=135143996>]
>>>


See also:
 PEP 217 - Display Hook for Interactive UseWritten and implemented by Moshe Zadka.

### ===🗝 PEP 208: New Coercion Model

How numeric coercion is done at the C level was significantly modified. This will only affect the authors of C extensions to Python, allowing them more flexibility in writing extension types that support numeric operations.

Extension types can now set the type flag Py_TPFLAGS_CHECKTYPES in their PyTypeObject structure to indicate that they support the new coercion model. In such extension types, the numeric slot functions can no longer assume that they’ll be passed two arguments of the same type; instead they may be passed two arguments of differing types, and can then perform their own internal coercion. If the slot function is passed a type it can’t handle, it can indicate the failure by returning a reference to the Py_NotImplemented singleton value. The numeric functions of the other type will then be tried, and perhaps they can handle the operation; if the other type also returns Py_NotImplemented, then a TypeError will be raised. Numeric methods written in Python can also return Py_NotImplemented, causing the interpreter to act as if the method did not exist (perhaps raising a TypeError, perhaps trying another object’s numeric methods).

See also:
 PEP 208 - Reworking the Coercion ModelWritten and implemented by Neil Schemenauer, heavily based upon earlier work by Marc-André Lemburg. Read this to understand the fine points of how numeric operations will now be processed at the C level.

### ===🗝 PEP 241: Metadata in Python Packages

A common complaint from Python users is that there’s no single catalog of all the Python modules in existence. T. Middleton’s Vaults of Parnassus at http://www.vex.net/parnassus/ are the largest catalog of Python modules, but registering software at the Vaults is optional, and many people don’t bother.

As a first small step toward fixing the problem, Python software packaged using the Distutils sdist command will include a file named PKG-INFO containing information about the package such as its name, version, and author (metadata, in cataloguing terminology). PEP 241 contains the full list of fields that can be present in the PKG-INFO file. As people began to package their software using Python 2.1, more and more packages will include metadata, making it possible to build automated cataloguing systems and experiment with them. With the result experience, perhaps it’ll be possible to design a really good catalog and then build support for it into Python 2.2. For example, the Distutils sdist and bdist_* commands could support an upload option that would automatically upload your package to a catalog server.

You can start creating packages containing PKG-INFO even if you’re not using Python 2.1, since a new release of the Distutils will be made for users of earlier Python versions. Version 1.0.2 of the Distutils includes the changes described in PEP 241, as well as various bugfixes and enhancements. It will be available from the Distutils SIG at https://www.python.org/community/sigs/current/distutils-sig/.

See also:
 PEP 241 - Metadata for Python Software PackagesWritten and implemented by A.M. Kuchling.PEP 243 - Module Repository Upload MechanismWritten by Sean Reifschneider, this draft PEP describes a proposed mechanism for uploading Python packages to a central server.

### ===🗝 New and Improved Modules

• Ka-Ping Yee contributed two new modules: inspect.py, a module for getting information about live Python code, and pydoc.py, a module for interactively converting docstrings to HTML or text. As a bonus, Tools/scripts/pydoc, which is now automatically installed, uses pydoc.py to display documentation given a Python module, package, or class name. For example, pydoc xml.dom displays the following:


Python Library Documentation: package xml.dom in xml

NAME
    xml.dom - W3C Document Object Model implementation for Python.

FILE
    /usr/local/lib/python2.1/xml/dom/__init__.pyc

DESCRIPTION
    The Python mapping of the Document Object Model is documented in the
    Python Library Reference in the section on the xml.dom package.

    This package contains the following modules:
      ...


pydoc also includes a Tk-based interactive help browser. pydoc quickly becomes addictive; try it out!


• Two different modules for unit testing were added to the standard library. The `doctest` module, contributed by Tim Peters, provides a testing framework based on running embedded examples in docstrings and comparing the results against the expected output. `PyUnit`, contributed by Steve Purcell, is a unit testing framework inspired by JUnit, which was in turn an adaptation of Kent Beck’s Smalltalk testing framework. See http://pyunit.sourceforge.net/ for more information about PyUnit.


• The `difflib` module contains a class, SequenceMatcher, which compares two sequences and computes the changes required to transform one sequence into the other. For example, this module can be used to write a tool similar to the Unix diff program, and in fact the sample program Tools/scripts/ndiff.py demonstrates how to write such a script.


• curses.panel, a wrapper for the panel library, part of ncurses and of SYSV curses, was contributed by Thomas Gellekum. The panel library provides windows with the additional feature of depth. Windows can be moved higher or lower in the depth ordering, and the panel library figures out where panels overlap and which sections are visible.


• The PyXML package has gone through a few releases since Python 2.0, and Python 2.1 includes an updated version of the xml package. Some of the noteworthy changes include support for Expat 1.2 and later versions, the ability for Expat parsers to handle files in any encoding supported by Python, and various bugfixes for SAX, DOM, and the minidom module.


• Ping also contributed another hook for handling uncaught exceptions. sys.excepthook() can be set to a callable object. When an exception isn’t caught by any try…except blocks, the exception will be passed to sys.excepthook(), which can then do whatever it likes. At the Ninth Python Conference, Ping demonstrated an application for this hook: printing an extended traceback that not only lists the stack frames, but also lists the function arguments and the local variables for each frame.


• Various functions in the time module, such as asctime() and localtime(), require a floating point argument containing the time in seconds since the epoch. The most common use of these functions is to work with the current time, so the floating point argument has been made optional; when a value isn’t provided, the current time will be used. For example, log file entries usually need a string containing the current time; in Python 2.1, time.asctime() can be used, instead of the lengthier time.asctime(time.localtime(time.time())) that was previously required.

This change was proposed and implemented by Thomas Wouters.


• The ftplib module now defaults to retrieving files in passive mode, because passive mode is more likely to work from behind a firewall. This request came from the Debian bug tracking system, since other Debian packages use ftplib to retrieve files and then don’t work from behind a firewall. It’s deemed unlikely that this will cause problems for anyone, because Netscape defaults to passive mode and few people complain, but if passive mode is unsuitable for your application or network setup, call set_pasv(0) on FTP objects to disable passive mode.


• Support for raw socket access has been added to the socket module, contributed by Grant Edwards.


• The pstats module now contains a simple interactive statistics browser for displaying timing profiles for Python programs, invoked when the module is run as a script. Contributed by Eric S. Raymond.


• A new implementation-dependent function, `sys._getframe([depth])`, has been added to return a given frame object from the current call stack. · returns the frame at the top of the call stack; if the optional integer argument depth is supplied, the function returns the frame that is depth calls below the top of the stack. For example, `sys._getframe(1)` returns the caller’s frame object.

This function is only present in CPython, not in Jython or the .NET implementation. Use it for debugging, and resist the temptation to put it into production code.



Other Changes and Fixes

There were relatively few smaller changes made in Python 2.1 due to the shorter release cycle. A search through the CVS change logs turns up 117 patches applied, and 136 bugs fixed; both figures are likely to be underestimates. Some of the more notable changes are:

• A specialized object allocator is now optionally available, that should be faster than the system malloc() and have less memory overhead. The allocator uses C’s malloc() function to get large pools of memory, and then fulfills smaller memory requests from these pools. It can be enabled by providing the --with-pymalloc option to the configure script; see Objects/obmalloc.c for the implementation details.

Authors of C extension modules should test their code with the object allocator enabled, because some incorrect code may break, causing core dumps at runtime. There are a bunch of memory allocation functions in Python’s C API that have previously been just aliases for the C library’s malloc() and free(), meaning that if you accidentally called mismatched functions, the error wouldn’t be noticeable. When the object allocator is enabled, these functions aren’t aliases of malloc() and free() any more, and calling the wrong function to free memory will get you a core dump. For example, if memory was allocated using PyMem_New(), it has to be freed using PyMem_Del(), not free(). A few modules included with Python fell afoul of this and had to be fixed; doubtless there are more third-party modules that will have the same problem.

The object allocator was contributed by Vladimir Marangozov.


• The speed of line-oriented file I/O has been improved because people often complain about its lack of speed, and because it’s often been used as a naïve benchmark. The readline() method of file objects has therefore been rewritten to be much faster. The exact amount of the speedup will vary from platform to platform depending on how slow the C library’s getc() was, but is around 66%, and potentially much faster on some particular operating systems. Tim Peters did much of the benchmarking and coding for this change, motivated by a discussion in comp.lang.python.

A new module and method for file objects was also added, contributed by Jeff Epler. The new method, xreadlines(), is similar to the existing xrange() built-in. xreadlines() returns an opaque sequence object that only supports being iterated over, reading a line on every iteration but not reading the entire file into memory as the existing readlines() method does. You’d use it like this:


```py
for line in sys.stdin.xreadlines():
    # ... do something for each line ...
    ...
```


For a fuller discussion of the line I/O changes, see the python-dev summary for January 1–15, 2001 at https://mail.python.org/pipermail/python-dev/2001-January/.


• A new method, popitem(), was added to dictionaries to enable destructively iterating through the contents of a dictionary; this can be faster for large dictionaries because there’s no need to construct a list containing all the keys or values. D.popitem() removes a random (key, value) pair from the dictionary D and returns it as a 2-tuple. This was implemented mostly by Tim Peters and Guido van Rossum, after a suggestion and preliminary patch by Moshe Zadka.


• Modules can now control which names are imported when from module import * is used, by defining an __all__ attribute containing a list of names that will be imported. One common complaint is that if the module imports other modules such as sys or string, from module import * will add them to the importing module’s namespace. To fix this, simply list the public names in __all__:


```py
# List public names
__all__ = ['Database', 'open']
```


A stricter version of this patch was first suggested and implemented by Ben Wolfson, but after some python-dev discussion, a weaker final version was checked in.


• Applying repr() to strings previously used octal escapes for non-printable characters; for example, a newline was '\012'. This was a vestigial trace of Python’s C ancestry, but today octal is of very little practical use. Ka-Ping Yee suggested using hex escapes instead of octal ones, and using the \n, \t, \r escapes for the appropriate characters, and implemented this new formatting.


• Syntax errors detected at compile-time can now raise exceptions containing the filename and line number of the error, a pleasant side effect of the compiler reorganization done by Jeremy Hylton.


• C extensions which import other modules have been changed to use PyImport_ImportModule(), which means that they will use any import hooks that have been installed. This is also encouraged for third-party extensions that need to import some other module from C code.


• The size of the Unicode character database was shrunk by another 340K thanks to Fredrik Lundh.


• Some new ports were contributed: MacOS X (by Steven Majewski), Cygwin (by Jason Tishler); RISCOS (by Dietmar Schwertberger); Unixware 7 (by Billy G. Allie).


And there’s the usual list of minor bugfixes, minor memory leaks, docstring edits, and other tweaks, too lengthy to be worth itemizing; see the CVS logs for the full details if you want them.


### ===🗝 Acknowledgements

The author would like to thank the following people for offering suggestions on various drafts of this article: Graeme Cross, David Goodger, Jay Graves, Michael Hudson, Marc-André Lemburg, Fredrik Lundh, Neil Schemenauer, Thomas Wouters.

## ==⚡ • What’s New in Python 2.0
◦ Introduction
◦ What About Python 1.6?
◦ New Development Process
◦ Unicode
◦ List Comprehensions
◦ Augmented Assignment
◦ String Methods
◦ Garbage Collection of Cycles
◦ Other Core Changes
◦ Porting to 2.0
◦ Extending/Embedding Changes
◦ Distutils: Making Modules Easy to Install
◦ XML Modules
◦ Module changes
◦ New modules
◦ IDLE Improvements
◦ Deleted and Deprecated Modules
◦ Acknowledgements

What’s New in Python 2.0
Author:
A.M. Kuchling and Moshe Zadka 


### ===🗝 Introduction

A new release of Python, version 2.0, was released on October 16, 2000. This article covers the exciting new features in 2.0, highlights some other useful changes, and points out a few incompatible changes that may require rewriting code.

Python’s development never completely stops between releases, and a steady flow of bug fixes and improvements are always being submitted. A host of minor fixes, a few optimizations, additional docstrings, and better error messages went into 2.0; to list them all would be impossible, but they’re certainly significant. Consult the publicly-available CVS logs if you want to see the full list. This progress is due to the five developers working for PythonLabs are now getting paid to spend their days fixing bugs, and also due to the improved communication resulting from moving to SourceForge.


### ===🗝 What About Python 1.6?

Python 1.6 can be thought of as the Contractual Obligations Python release. After the core development team left CNRI in May 2000, CNRI requested that a 1.6 release be created, containing all the work on Python that had been performed at CNRI. Python 1.6 therefore represents the state of the CVS tree as of May 2000, with the most significant new feature being Unicode support. Development continued after May, of course, so the 1.6 tree received a few fixes to ensure that it’s forward-compatible with Python 2.0. 1.6 is therefore part of Python’s evolution, and not a side branch.

So, should you take much interest in Python 1.6? Probably not. The 1.6final and 2.0beta1 releases were made on the same day (September 5, 2000), the plan being to finalize Python 2.0 within a month or so. If you have applications to maintain, there seems little point in breaking things by moving to 1.6, fixing them, and then having another round of breakage within a month by moving to 2.0; you’re better off just going straight to 2.0. Most of the really interesting features described in this document are only in 2.0, because a lot of work was done between May and September.


### ===🗝 New Development Process

The most important change in Python 2.0 may not be to the code at all, but to how Python is developed: in May 2000 the Python developers began using the tools made available by SourceForge for storing source code, tracking bug reports, and managing the queue of patch submissions. To report bugs or submit patches for Python 2.0, use the bug tracking and patch manager tools available from Python’s project page, located at https://sourceforge.net/projects/python/.

The most important of the services now hosted at SourceForge is the Python CVS tree, the version-controlled repository containing the source code for Python. Previously, there were roughly 7 or so people who had write access to the CVS tree, and all patches had to be inspected and checked in by one of the people on this short list. Obviously, this wasn’t very scalable. By moving the CVS tree to SourceForge, it became possible to grant write access to more people; as of September 2000 there were 27 people able to check in changes, a fourfold increase. This makes possible large-scale changes that wouldn’t be attempted if they’d have to be filtered through the small group of core developers. For example, one day Peter Schneider-Kamp took it into his head to drop K&R C compatibility and convert the C source for Python to ANSI C. After getting approval on the python-dev mailing list, he launched into a flurry of checkins that lasted about a week, other developers joined in to help, and the job was done. If there were only 5 people with write access, probably that task would have been viewed as “nice, but not worth the time and effort needed” and it would never have gotten done.

The shift to using SourceForge’s services has resulted in a remarkable increase in the speed of development. Patches now get submitted, commented on, revised by people other than the original submitter, and bounced back and forth between people until the patch is deemed worth checking in. Bugs are tracked in one central location and can be assigned to a specific person for fixing, and we can count the number of open bugs to measure progress. This didn’t come without a cost: developers now have more e-mail to deal with, more mailing lists to follow, and special tools had to be written for the new environment. For example, SourceForge sends default patch and bug notification e-mail messages that are completely unhelpful, so Ka-Ping Yee wrote an HTML screen-scraper that sends more useful messages.

The ease of adding code caused a few initial growing pains, such as code was checked in before it was ready or without getting clear agreement from the developer group. The approval process that has emerged is somewhat similar to that used by the Apache group. Developers can vote +1, +0, -0, or -1 on a patch; +1 and -1 denote acceptance or rejection, while +0 and -0 mean the developer is mostly indifferent to the change, though with a slight positive or negative slant. The most significant change from the Apache model is that the voting is essentially advisory, letting Guido van Rossum, who has Benevolent Dictator For Life status, know what the general opinion is. He can still ignore the result of a vote, and approve or reject a change even if the community disagrees with him.

Producing an actual patch is the last step in adding a new feature, and is usually easy compared to the earlier task of coming up with a good design. Discussions of new features can often explode into lengthy mailing list threads, making the discussion hard to follow, and no one can read every posting to python-dev. Therefore, a relatively formal process has been set up to write Python Enhancement Proposals (PEPs), modelled on the internet RFC process. PEPs are draft documents that describe a proposed new feature, and are continually revised until the community reaches a consensus, either accepting or rejecting the proposal. Quoting from the introduction to PEP 1, “PEP Purpose and Guidelines”:

PEP stands for Python Enhancement Proposal. A PEP is a design document providing information to the Python community, or describing a new feature for Python. The PEP should provide a concise technical specification of the feature and a rationale for the feature.

We intend PEPs to be the primary mechanisms for proposing new features, for collecting community input on an issue, and for documenting the design decisions that have gone into Python. The PEP author is responsible for building consensus within the community and documenting dissenting opinions.

Read the rest of PEP 1 for the details of the PEP editorial process, style, and format. PEPs are kept in the Python CVS tree on SourceForge, though they’re not part of the Python 2.0 distribution, and are also available in HTML form from https://www.python.org/dev/peps/. As of September 2000, there are 25 PEPS, ranging from PEP 201, “Lockstep Iteration”, to PEP 225, “Elementwise/Objectwise Operators”.


### ===🗝 Unicode

The largest new feature in Python 2.0 is a new fundamental data type: Unicode strings. Unicode uses 16-bit numbers to represent characters instead of the 8-bit number used by ASCII, meaning that 65,536 distinct characters can be supported.

The final interface for Unicode support was arrived at through countless often-stormy discussions on the python-dev mailing list, and mostly implemented by Marc-André Lemburg, based on a Unicode string type implementation by Fredrik Lundh. A detailed explanation of the interface was written up as PEP 100, “Python Unicode Integration”. This article will simply cover the most significant points about the Unicode interfaces.

In Python source code, Unicode strings are written as u"string". Arbitrary Unicode characters can be written using a new escape sequence, \uHHHH, where HHHH is a 4-digit hexadecimal number from 0000 to FFFF. The existing \xHHHH escape sequence can also be used, and octal escapes can be used for characters up to U+01FF, which is represented by \777.

Unicode strings, just like regular strings, are an immutable sequence type. They can be indexed and sliced, but not modified in place. Unicode strings have an encode( [encoding] ) method that returns an 8-bit string in the desired encoding. Encodings are named by strings, such as 'ascii', 'utf-8', 'iso-8859-1', or whatever. A codec API is defined for implementing and registering new encodings that are then available throughout a Python program. If an encoding isn’t specified, the default encoding is usually 7-bit ASCII, though it can be changed for your Python installation by calling the sys.setdefaultencoding(encoding) function in a customized version of site.py.

Combining 8-bit and Unicode strings always coerces to Unicode, using the default ASCII encoding; the result of 'a' + u'bc' is u'abc'.

New built-in functions have been added, and existing built-ins modified to support Unicode:

• unichr(ch) returns a Unicode string 1 character long, containing the character ch.

• ord(u), where u is a 1-character regular or Unicode string, returns the number of the character as an integer.

• unicode(string [, encoding]  [, errors] ) creates a Unicode string from an 8-bit string. encoding is a string naming the encoding to use. The errors parameter specifies the treatment of characters that are invalid for the current encoding; passing 'strict' as the value causes an exception to be raised on any encoding error, while 'ignore' causes errors to be silently ignored and 'replace' uses U+FFFD, the official replacement character, in case of any problems.

• The exec statement, and various built-ins such as eval(), getattr(), and setattr() will also accept Unicode strings as well as regular strings. (It’s possible that the process of fixing this missed some built-ins; if you find a built-in function that accepts strings but doesn’t accept Unicode strings at all, please report it as a bug.)

A new module, unicodedata, provides an interface to Unicode character properties. For example, unicodedata.category(u'A') returns the 2-character string ‘Lu’, the ‘L’ denoting it’s a letter, and ‘u’ meaning that it’s uppercase. unicodedata.bidirectional(u'\u0660') returns ‘AN’, meaning that U+0660 is an Arabic number.

The codecs module contains functions to look up existing encodings and register new ones. Unless you want to implement a new encoding, you’ll most often use the codecs.lookup(encoding) function, which returns a 4-element tuple: (encode_func, decode_func, stream_reader, stream_writer).

• encode_func is a function that takes a Unicode string, and returns a 2-tuple (string, length). string is an 8-bit string containing a portion (perhaps all) of the Unicode string converted into the given encoding, and length tells you how much of the Unicode string was converted.

• decode_func is the opposite of encode_func, taking an 8-bit string and returning a 2-tuple (ustring, length), consisting of the resulting Unicode string ustring and the integer length telling how much of the 8-bit string was consumed.

• stream_reader is a class that supports decoding input from a stream. stream_reader(file_obj) returns an object that supports the read(), readline(), and readlines() methods. These methods will all translate from the given encoding and return Unicode strings.

• stream_writer, similarly, is a class that supports encoding output to a stream. stream_writer(file_obj) returns an object that supports the write() and writelines() methods. These methods expect Unicode strings, translating them to the given encoding on output.

For example, the following code writes a Unicode string into a file, encoding it as UTF-8:


```py
import codecs

unistr = u'\u0660\u2000ab ...'

(UTF8_encode, UTF8_decode,
 UTF8_streamreader, UTF8_streamwriter) = codecs.lookup('UTF-8')

output = UTF8_streamwriter( open( '/tmp/output', 'wb') )
output.write( unistr )
output.close()
```


The following code would then read UTF-8 input from the file:


```py
input = UTF8_streamreader( open( '/tmp/output', 'rb') )
print repr(input.read())
input.close()
```


Unicode-aware regular expressions are available through the re module, which has a new underlying implementation called SRE written by Fredrik Lundh of Secret Labs AB.

A -U command line option was added which causes the Python compiler to interpret all string literals as Unicode string literals. This is intended to be used in testing and future-proofing your Python code, since some future version of Python may drop support for 8-bit strings and provide only Unicode strings.


### ===🗝 List Comprehensions

Lists are a workhorse data type in Python, and many programs manipulate a list at some point. Two common operations on lists are to loop over them, and either pick out the elements that meet a certain criterion, or apply some function to each element. For example, given a list of strings, you might want to pull out all the strings containing a given substring, or strip off trailing whitespace from each line.

The existing map() and filter() functions can be used for this purpose, but they require a function as one of their arguments. This is fine if there’s an existing built-in function that can be passed directly, but if there isn’t, you have to create a little function to do the required work, and Python’s scoping rules make the result ugly if the little function needs additional information. Take the first example in the previous paragraph, finding all the strings in the list containing a given substring. You could write the following to do it:


```py
# Given the list L, make a list of all strings
# containing the substring S.
sublist = filter( lambda s, substring=S:
                     string.find(s, substring) != -1,
                  L)

```

Because of Python’s scoping rules, a default argument is used so that the anonymous function created by the lambda expression knows what substring is being searched for. List comprehensions make this cleaner:


```py
sublist = [ s for s in L if string.find(s, S) != -1 ]
```


List comprehensions have the form:


    [ expression for expr in sequence1
                 for expr2 in sequence2 ...
                 for exprN in sequenceN
                 if condition ]


The for…in clauses contain the sequences to be iterated over. The sequences do not have to be the same length, because they are not iterated over in parallel, but from left to right; this is explained more clearly in the following paragraphs. The elements of the generated list will be the successive values of expression. The final if clause is optional; if present, expression is only evaluated and added to the result if condition is true.

To make the semantics very clear, a list comprehension is equivalent to the following Python code:


```py
for expr1 in sequence1:
    for expr2 in sequence2:
    ...
        for exprN in sequenceN:
             if (condition):
                  # Append the value of
                  # the expression to the
                  # resulting list.
```


This means that when there are multiple for…in clauses, the resulting list will be equal to the product of the lengths of all the sequences. If you have two lists of length 3, the output list is 9 elements long:


>>>seq1 = 'abc'
seq2 = (1,2,3)
>>> [ (x,y) for x in seq1 for y in seq2]
[('a', 1), ('a', 2), ('a', 3), ('b', 1), ('b', 2), ('b', 3), ('c', 1),
('c', 2), ('c', 3)]


To avoid introducing an ambiguity into Python’s grammar, if expression is creating a tuple, it must be surrounded with parentheses. The first list comprehension below is a syntax error, while the second one is correct:


```py
# Syntax error
[ x,y for x in seq1 for y in seq2]
# Correct
[ (x,y) for x in seq1 for y in seq2]
```


The idea of list comprehensions originally comes from the functional programming language Haskell (https://www.haskell.org). Greg Ewing argued most effectively for adding them to Python and wrote the initial list comprehension patch, which was then discussed for a seemingly endless time on the python-dev mailing list and kept up-to-date by Skip Montanaro.


### ===🗝 Augmented Assignment

Augmented assignment operators, another long-requested feature, have been added to Python 2.0. Augmented assignment operators include `+=, -=, *=`, and so forth. For example, the statement a += 2 increments the value of the variable a by 2, equivalent to the slightly lengthier a = a + 2.

The full list of supported assignment operators is `+=, -=, *=, /=, %=, **=, &=, |=, ^=, >>=, and <<=`. Python classes can override the augmented assignment operators by defining methods named __iadd__(), __isub__(), etc. For example, the following Number class stores a number and supports using += to create a new instance with an incremented value.


```py
class Number:
    def __init__(self, value):
        self.value = value
    def __iadd__(self, increment):
        return Number( self.value + increment)

n = Number(5)
n += 3
print n.value
```


The __iadd__() special method is called with the value of the increment, and should return a new instance with an appropriately modified value; this return value is bound as the new value of the variable on the left-hand side.

Augmented assignment operators were first introduced in the C programming language, and most C-derived languages, such as awk, C++, Java, Perl, and PHP also support them. The augmented assignment patch was implemented by Thomas Wouters.


### ===🗝 String Methods

Until now string-manipulation functionality was in the string module, which was usually a front-end for the strop module written in C. The addition of Unicode posed a difficulty for the strop module, because the functions would all need to be rewritten in order to accept either 8-bit or Unicode strings. For functions such as string.replace(), which takes 3 string arguments, that means eight possible permutations, and correspondingly complicated code.

Instead, Python 2.0 pushes the problem onto the string type, making string manipulation functionality available through methods on both 8-bit strings and Unicode strings.


>>> 'andrew'.capitalize()
'Andrew'
>>> 'hostname'.replace('os', 'linux')
'hlinuxtname'
>>> 'moshe'.find('sh')
2


One thing that hasn’t changed, a noteworthy April Fools’ joke notwithstanding, is that Python strings are immutable. Thus, the string methods return new strings, and do not modify the string on which they operate.

The old string module is still around for backwards compatibility, but it mostly acts as a front-end to the new string methods.

Two methods which have no parallel in pre-2.0 versions, although they did exist in JPython for quite some time, are startswith() and endswith(). s.startswith(t) is equivalent to s[:len(t)] == t, while s.endswith(t) is equivalent to s[-len(t):] == t.

One other method which deserves special mention is join(). The join() method of a string receives one parameter, a sequence of strings, and is equivalent to the string.join() function from the old string module, with the arguments reversed. In other words, s.join(seq) is equivalent to the old string.join(seq, s).


### ===🗝 Garbage Collection of Cycles

The C implementation of Python uses reference counting to implement garbage collection. Every Python object maintains a count of the number of references pointing to itself, and adjusts the count as references are created or destroyed. Once the reference count reaches zero, the object is no longer accessible, since you need to have a reference to an object to access it, and if the count is zero, no references exist any longer.

Reference counting has some pleasant properties: it’s easy to understand and implement, and the resulting implementation is portable, fairly fast, and reacts well with other libraries that implement their own memory handling schemes. The major problem with reference counting is that it sometimes doesn’t realise that objects are no longer accessible, resulting in a memory leak. This happens when there are cycles of references.

Consider the simplest possible cycle, a class instance which has a reference to itself:


```py
instance = SomeClass()
instance.myself = instance
```

After the above two lines of code have been executed, the reference count of `instance` is 2; one reference is from the variable named 'instance', and the other is from the myself attribute of the instance.

If the next line of code is del instance, what happens? The reference count of instance is decreased by 1, so it has a reference count of 1; the reference in the `myself` attribute still exists. Yet the instance is no longer accessible through Python code, and it could be deleted. Several objects can participate in a cycle if they have references to each other, causing all of the objects to be leaked.

Python 2.0 fixes this problem by periodically executing a cycle detection algorithm which looks for inaccessible cycles and deletes the objects involved. A new gc module provides functions to perform a garbage collection, obtain debugging statistics, and tuning the collector’s parameters.

Running the cycle detection algorithm takes some time, and therefore will result in some additional overhead. It is hoped that after we’ve gotten experience with the cycle collection from using 2.0, Python 2.1 will be able to minimize the overhead with careful tuning. It’s not yet obvious how much performance is lost, because benchmarking this is tricky and depends crucially on how often the program creates and destroys objects. The detection of cycles can be disabled when Python is compiled, if you can’t afford even a tiny speed penalty or suspect that the cycle collection is buggy, by specifying the --without-cycle-gc switch when running the configure script.

Several people tackled this problem and contributed to a solution. An early implementation of the cycle detection approach was written by Toby Kelsey. The current algorithm was suggested by Eric Tiedemann during a visit to CNRI, and Guido van Rossum and Neil Schemenauer wrote two different implementations, which were later integrated by Neil. Lots of other people offered suggestions along the way; the March 2000 archives of the python-dev mailing list contain most of the relevant discussion, especially in the threads titled “Reference cycle collection for Python” and “Finalization again”.


### ===🗝 Other Core Changes

Various minor changes have been made to Python’s syntax and built-in functions. None of the changes are very far-reaching, but they’re handy conveniences.


#### Minor Language Changes

A new syntax makes it more convenient to call a given function with a tuple of arguments and/or a dictionary of keyword arguments. In Python 1.5 and earlier, you’d use the apply() built-in function: apply(f, args, kw) calls the function f() with the argument tuple args and the keyword arguments in the dictionary kw. apply() is the same in 2.0, but thanks to a patch from Greg Ewing, `f(*args, **kw)` is a shorter and clearer way to achieve the same effect. This syntax is symmetrical with the syntax for defining functions:


```py
def f(*args, **kw):
    # args is a tuple of positional args,
    # kw is a dictionary of keyword args
    ...
```


The print statement can now have its output directed to a file-like object by following the print with >> file, similar to the redirection operator in Unix shells. Previously you’d either have to use the write() method of the file-like object, which lacks the convenience and simplicity of print, or you could assign a new value to sys.stdout and then restore the old value. For sending output to standard error, it’s much easier to write this:


```py
print >> sys.stderr, "Warning: action field not supplied"
```


Modules can now be renamed on importing them, using the syntax import module as name or from module import name as othername. The patch was submitted by Thomas Wouters.

A new format style is available when using the % operator; ‘%r’ will insert the repr() of its argument. This was also added from symmetry considerations, this time for symmetry with the existing ‘%s’ format style, which inserts the str() of its argument. For example, '%r %s' % ('abc', 'abc') returns a string containing 'abc' abc.

Previously there was no way to implement a class that overrode Python’s built-in in operator and implemented a custom version. obj in seq returns true if obj is present in the sequence seq; Python computes this by simply trying every index of the sequence until either obj is found or an IndexError is encountered. Moshe Zadka contributed a patch which adds a __contains__() magic method for providing a custom implementation for in. Additionally, new built-in objects written in C can define what in means for them via a new slot in the sequence protocol.

Earlier versions of Python used a recursive algorithm for deleting objects. Deeply nested data structures could cause the interpreter to fill up the C stack and crash; Christian Tismer rewrote the deletion logic to fix this problem. On a related note, comparing recursive objects recursed infinitely and crashed; Jeremy Hylton rewrote the code to no longer crash, producing a useful result instead. 

For example, after this code:


```py
a = []
b = []
a.append(a)
b.append(b)
```


The comparison a==b returns true, because the two recursive data structures are isomorphic. See the thread “trashcan and PR#7” in the April 2000 archives of the python-dev mailing list for the discussion leading up to this implementation, and some useful relevant links. Note that comparisons can now also raise exceptions. In earlier versions of Python, a comparison operation such as cmp(a,b) would always produce an answer, even if a user-defined __cmp__() method encountered an error, since the resulting exception would simply be silently swallowed.

Work has been done on porting Python to 64-bit Windows on the Itanium processor, mostly by Trent Mick of ActiveState. (Confusingly, sys.platform is still 'win32' on Win64 because it seems that for ease of porting, MS Visual C++ treats code as 32 bit on Itanium.) PythonWin also supports Windows CE; see the Python CE page at http://pythonce.sourceforge.net/ for more information.

Another new platform is Darwin/MacOS X; initial support for it is in Python 2.0. Dynamic loading works, if you specify “configure –with-dyld –with-suffix=.x”. Consult the README in the Python source distribution for more instructions.

An attempt has been made to alleviate one of Python’s warts, the often-confusing NameError exception when code refers to a local variable before the variable has been assigned a value. For example, the following code raises an exception on the print statement in both 1.5.2 and 2.0; in 1.5.2 a NameError exception is raised, while 2.0 raises a new UnboundLocalError exception. UnboundLocalError is a subclass of NameError, so any existing code that expects NameError to be raised should still work.


```py
def f():
    print "i=",i
    i = i + 1
f()
```


Two new exceptions, TabError and IndentationError, have been introduced. They’re both subclasses of SyntaxError, and are raised when Python code is found to be improperly indented.


#### Changes to Built-in Functions

A new built-in, zip(seq1, seq2, ...), has been added. zip() returns a list of tuples where each tuple contains the i-th element from each of the argument sequences. The difference between zip() and map(None, seq1, seq2) is that map() pads the sequences with None if the sequences aren’t all of the same length, while zip() truncates the returned list to the length of the shortest argument sequence.

The int() and long() functions now accept an optional “base” parameter when the first argument is a string. int('123', 10) returns 123, while int('123', 16) returns 291. int(123, 16) raises a TypeError exception with the message “can’t convert non-string with explicit base”.

A new variable holding more detailed version information has been added to the sys module. sys.version_info is a tuple (major, minor, micro, level, serial) For example, in a hypothetical 2.0.1beta1, sys.version_info would be (2, 0, 1, 'beta', 1). level is a string such as "alpha", "beta", or "final" for a final release.

Dictionaries have an odd new method, setdefault(key, default), which behaves similarly to the existing get() method. However, if the key is missing, setdefault() both returns the value of default as get() would do, and also inserts it into the dictionary as the value for key. Thus, the following lines of code:


```py
if dict.has_key( key ): return dict[key]
else:
    dict[key] = []
    return dict[key]
```

can be reduced to a single return dict.setdefault(key, []) statement.

The interpreter sets a maximum recursion depth in order to catch runaway recursion before filling the C stack and causing a core dump or GPF.. Previously this limit was fixed when you compiled Python, but in 2.0 the maximum recursion depth can be read and modified using sys.getrecursionlimit() and sys.setrecursionlimit(). The default value is 1000, and a rough maximum value for a given platform can be found by running a new script, Misc/find_recursionlimit.py.


### ===🗝 Porting to 2.0

New Python releases try hard to be compatible with previous releases, and the record has been pretty good. However, some changes are considered useful enough, usually because they fix initial design decisions that turned out to be actively mistaken, that breaking backward compatibility can’t always be avoided. This section lists the changes in Python 2.0 that may cause old Python code to break.

The change which will probably break the most code is tightening up the arguments accepted by some methods. Some methods would take multiple arguments and treat them as a tuple, particularly various list methods such as append() and insert(). In earlier versions of Python, if L is a list, L.append( 1,2 ) appends the tuple (1,2) to the list. In Python 2.0 this causes a TypeError exception to be raised, with the message: ‘append requires exactly 1 argument; 2 given’. The fix is to simply add an extra set of parentheses to pass both values as a tuple: L.append( (1,2) ).

The earlier versions of these methods were more forgiving because they used an old function in Python’s C interface to parse their arguments; 2.0 modernizes them to use PyArg_ParseTuple(), the current argument parsing function, which provides more helpful error messages and treats multi-argument calls as errors. If you absolutely must use 2.0 but can’t fix your code, you can edit Objects/listobject.c and define the preprocessor symbol NO_STRICT_LIST_APPEND to preserve the old behaviour; this isn’t recommended.

Some of the functions in the socket module are still forgiving in this way. For example, socket.connect( ('hostname', 25) )() is the correct form, passing a tuple representing an IP address, but socket.connect( 'hostname', 25 )() also works. socket.connect_ex() and socket.bind() are similarly easy-going. 2.0alpha1 tightened these functions up, but because the documentation actually used the erroneous multiple argument form, many people wrote code which would break with the stricter checking. GvR backed out the changes in the face of public reaction, so for the socket module, the documentation was fixed and the multiple argument form is simply marked as deprecated; it will be tightened up again in a future Python version.

The \x escape in string literals now takes exactly 2 hex digits. Previously it would consume all the hex digits following the ‘x’ and take the lowest 8 bits of the result, so \x123456 was equivalent to \x56.

The AttributeError and NameError exceptions have a more friendly error message, whose text will be something like 'Spam' instance has no attribute 'eggs' or name 'eggs' is not defined. Previously the error message was just the missing attribute name eggs, and code written to take advantage of this fact will break in 2.0.

Some work has been done to make integers and long integers a bit more interchangeable. In 1.5.2, large-file support was added for Solaris, to allow reading files larger than 2 GiB; this made the tell() method of file objects return a long integer instead of a regular integer. Some code would subtract two file offsets and attempt to use the result to multiply a sequence or slice a string, but this raised a TypeError. In 2.0, long integers can be used to multiply or slice a sequence, and it’ll behave as you’d intuitively expect it to; 3L * 'abc' produces ‘abcabcabc’, and (0,1,2,3)[2L:4L] produces (2,3). Long integers can also be used in various contexts where previously only integers were accepted, such as in the seek() method of file objects, and in the formats supported by the % operator (%d, %i, %x, etc.). For example, `"%d" % 2L**64` will produce the string 18446744073709551616.

The subtlest long integer change of all is that the str() of a long integer no longer has a trailing ‘L’ character, though repr() still includes it. The ‘L’ annoyed many people who wanted to print long integers that looked just like regular integers, since they had to go out of their way to chop off the character. This is no longer a problem in 2.0, but code which does str(longval)[:-1] and assumes the ‘L’ is there, will now lose the final digit.

Taking the repr() of a float now uses a different formatting precision than str(). repr() uses %.17g format string for C’s sprintf(), while str() uses %.12g as before. The effect is that repr() may occasionally show more decimal places than str(), for certain numbers. For example, the number 8.1 can’t be represented exactly in binary, so repr(8.1) is '8.0999999999999996', while str(8.1) is '8.1'.

The -X command-line option, which turned all standard exceptions into strings instead of classes, has been removed; the standard exceptions will now always be classes. The exceptions module containing the standard exceptions was translated from Python to a built-in C module, written by Barry Warsaw and Fredrik Lundh.


### ===🗝 Extending/Embedding Changes

Some of the changes are under the covers, and will only be apparent to people writing C extension modules or embedding a Python interpreter in a larger application. If you aren’t dealing with Python’s C API, you can safely skip this section.

The version number of the Python C API was incremented, so C extensions compiled for 1.5.2 must be recompiled in order to work with 2.0. On Windows, it’s not possible for Python 2.0 to import a third party extension built for Python 1.5.x due to how Windows DLLs work, so Python will raise an exception and the import will fail.

Users of Jim Fulton’s ExtensionClass module will be pleased to find out that hooks have been added so that ExtensionClasses are now supported by isinstance() and issubclass(). This means you no longer have to remember to write code such as if type(obj) == myExtensionClass, but can use the more natural if isinstance(obj, myExtensionClass).

The Python/importdl.c file, which was a mass of #ifdefs to support dynamic loading on many different platforms, was cleaned up and reorganised by Greg Stein. importdl.c is now quite small, and platform-specific code has been moved into a bunch of Python/dynload_*.c files. Another cleanup: there were also a number of `my*.h` files in the Include/ directory that held various portability hacks; they’ve been merged into a single file, Include/pyport.h.

Vladimir Marangozov’s long-awaited malloc restructuring was completed, to make it easy to have the Python interpreter use a custom allocator instead of C’s standard malloc(). For documentation, read the comments in Include/pymem.h and Include/objimpl.h. For the lengthy discussions during which the interface was hammered out, see the web archives of the ‘patches’ and ‘python-dev’ lists at python.org.

Recent versions of the GUSI development environment for MacOS support POSIX threads. Therefore, Python’s POSIX threading support now works on the Macintosh. Threading support using the user-space GNU pth library was also contributed.

Threading support on Windows was enhanced, too. Windows supports thread locks that use kernel objects only in case of contention; in the common case when there’s no contention, they use simpler functions which are an order of magnitude faster. A threaded version of Python 1.5.2 on NT is twice as slow as an unthreaded version; with the 2.0 changes, the difference is only 10%. These improvements were contributed by Yakov Markovitch.

Python 2.0’s source now uses only ANSI C prototypes, so compiling Python now requires an ANSI C compiler, and can no longer be done using a compiler that only supports K&R C.

Previously the Python virtual machine used 16-bit numbers in its bytecode, limiting the size of source files. In particular, this affected the maximum size of literal lists and dictionaries in Python source; occasionally people who are generating Python code would run into this limit. A patch by Charles G. Waldman raises the limit from 2**16 to 2**32.

Three new convenience functions intended for adding constants to a module’s dictionary at module initialization time were added: PyModule_AddObject(), PyModule_AddIntConstant(), and PyModule_AddStringConstant(). Each of these functions takes a module object, a null-terminated C string containing the name to be added, and a third argument for the value to be assigned to the name. This third argument is, respectively, a Python object, a C long, or a C string.

A wrapper API was added for Unix-style signal handlers. PyOS_getsig() gets a signal handler and PyOS_setsig() will set a new handler.


### ===🗝 Distutils: Making Modules Easy to Install
- PEP 632, Deprecate distutils module https://www.python.org/dev/peps/pep-0632
- Setup Tools https://setuptools.readthedocs.io/en/latest/
- Python Packaging User Guide https://packaging.python.org/

The distutils package is deprecated and slated for removal in Python 3.12. Use setup tools or check PEP 632 for potential alternatives

Before Python 2.0, installing modules was a tedious affair – there was no way to figure out automatically where Python is installed, or what compiler options to use for extension modules. Software authors had to go through an arduous ritual of editing Makefiles and configuration files, which only really work on Unix and leave Windows and MacOS unsupported. Python users faced wildly differing installation instructions which varied between different extension packages, which made administering a Python installation something of a chore.

The SIG for distribution utilities, shepherded by Greg Ward, has created the Distutils, a system to make package installation much easier. They form the distutils package, a new part of Python’s standard library. In the best case, installing a Python module from source will require the same steps: first you simply mean unpack the tarball or zip archive, and the run “python setup.py install”. The platform will be automatically detected, the compiler will be recognized, C extension modules will be compiled, and the distribution installed into the proper directory. Optional command-line arguments provide more control over the installation process, the distutils package offers many places to override defaults – separating the build from the install, building or installing in non-default directories, and more.

In order to use the Distutils, you need to write a setup.py script. For the simple case, when the software contains only .py files, a minimal setup.py can be just a few lines long:


```py
from distutils.core import setup
setup (name = "foo", version = "1.0",
       py_modules = ["module1", "module2"])
```


The setup.py file isn’t much more complicated if the software consists of a few packages:


```py
from distutils.core import setup
setup (name = "foo", version = "1.0",
       packages = ["package", "package.subpackage"])
```


A C extension can be the most complicated case; here’s an example taken from the PyXML package:


```py
from distutils.core import setup, Extension

expat_extension = Extension('xml.parsers.pyexpat',
     define_macros = [('XML_NS', None)],
     include_dirs = [ 'extensions/expat/xmltok',
                      'extensions/expat/xmlparse' ],
     sources = [ 'extensions/pyexpat.c',
                 'extensions/expat/xmltok/xmltok.c',
                 'extensions/expat/xmltok/xmlrole.c', ]
       )
setup (name = "PyXML", version = "0.5.4",
       ext_modules =[ expat_extension ] )
```


The Distutils can also take care of creating source and binary distributions. The “sdist” command, run by “python setup.py sdist’, builds a source distribution such as foo-1.0.tar.gz. Adding new commands isn’t difficult, “bdist_rpm” and “bdist_wininst” commands have already been contributed to create an RPM distribution and a Windows installer for the software, respectively. Commands to create other distribution formats such as Debian packages and Solaris .pkg files are in various stages of development.

All this is documented in a new manual, Distributing Python Modules, that joins the basic set of Python documentation.


### ===🗝 XML Modules

Python 1.5.2 included a simple XML parser in the form of the xmllib module, contributed by Sjoerd Mullender. Since 1.5.2’s release, two different interfaces for processing XML have become common: SAX2 (version 2 of the Simple API for XML) provides an event-driven interface with some similarities to xmllib, and the DOM (Document Object Model) provides a tree-based interface, transforming an XML document into a tree of nodes that can be traversed and modified. Python 2.0 includes a SAX2 interface and a stripped-down DOM interface as part of the xml package. Here we will give a brief overview of these new interfaces; consult the Python documentation or the source code for complete details. The Python XML SIG is also working on improved documentation.


#### SAX2 Support

SAX defines an event-driven interface for parsing XML. To use SAX, you must write a SAX handler class. Handler classes inherit from various classes provided by SAX, and override various methods that will then be called by the XML parser. For example, the startElement() and endElement() methods are called for every starting and end tag encountered by the parser, the characters() method is called for every chunk of character data, and so forth.

The advantage of the event-driven approach is that the whole document doesn’t have to be resident in memory at any one time, which matters if you are processing really huge documents. However, writing the SAX handler class can get very complicated if you’re trying to modify the document structure in some elaborate way.

For example, this little example program defines a handler that prints a message for every starting and ending tag, and then parses the file hamlet.xml using it:


```py
from xml import sax

class SimpleHandler(sax.ContentHandler):
    def startElement(self, name, attrs):
        print 'Start of element:', name, attrs.keys()

    def endElement(self, name):
        print 'End of element:', name

# Create a parser object
parser = sax.make_parser()

# Tell it what handler to use
handler = SimpleHandler()
parser.setContentHandler( handler )

# Parse a file!
parser.parse( 'hamlet.xml' )
```


For more information, consult the Python documentation, or the XML HOWTO at http://pyxml.sourceforge.net/topics/howto/xml-howto.html.


#### DOM Support

The Document Object Model is a tree-based representation for an XML document. A top-level Document instance is the root of the tree, and has a single child which is the top-level Element instance. This Element has children nodes representing character data and any sub-elements, which may have further children of their own, and so forth. Using the DOM you can traverse the resulting tree any way you like, access element and attribute values, insert and delete nodes, and convert the tree back into XML.

The DOM is useful for modifying XML documents, because you can create a DOM tree, modify it by adding new nodes or rearranging subtrees, and then produce a new XML document as output. You can also construct a DOM tree manually and convert it to XML, which can be a more flexible way of producing XML output than simply writing <tag1>…</tag1> to a file.

The DOM implementation included with Python lives in the xml.dom.minidom module. It’s a lightweight implementation of the Level 1 DOM with support for XML namespaces. The parse() and parseString() convenience functions are provided for generating a DOM tree:


```py
from xml.dom import minidom
doc = minidom.parse('hamlet.xml')
```


doc is a Document instance. Document, like all the other DOM classes such as Element and Text, is a subclass of the Node base class. All the nodes in a DOM tree therefore support certain common methods, such as toxml() which returns a string containing the XML representation of the node and its children. Each class also has special methods of its own; for example, Element and Document instances have a method to find all child elements with a given tag name. Continuing from the previous 2-line example:


```py
perslist = doc.getElementsByTagName( 'PERSONA' )
print perslist[0].toxml()
print perslist[1].toxml()
```

For the Hamlet XML file, the above few lines output:


```xml
<PERSONA>CLAUDIUS, king of Denmark. </PERSONA>
<PERSONA>HAMLET, son to the late, and nephew to the present king.</PERSONA>
```


The root element of the document is available as doc.documentElement, and its children can be easily modified by deleting, adding, or removing nodes:


```py
root = doc.documentElement

# Remove the first child
root.removeChild( root.childNodes[0] )

# Move the new first child to the end
root.appendChild( root.childNodes[0] )

# Insert the new first child (originally,
# the third child) before the 20th child.
root.insertBefore( root.childNodes[0], root.childNodes[20] )
```


Again, I will refer you to the Python documentation for a complete listing of the different Node classes and their various methods.


#### Relationship to PyXML

The XML Special Interest Group has been working on XML-related Python code for a while. Its code distribution, called PyXML, is available from the SIG’s web pages at https://www.python.org/community/sigs/current/xml-sig. The PyXML distribution also used the package name xml. If you’ve written programs that used PyXML, you’re probably wondering about its compatibility with the 2.0 xml package.

The answer is that Python 2.0’s xml package isn’t compatible with PyXML, but can be made compatible by installing a recent version PyXML. Many applications can get by with the XML support that is included with Python 2.0, but more complicated applications will require that the full PyXML package will be installed. When installed, PyXML versions 0.6.0 or greater will replace the xml package shipped with Python, and will be a strict superset of the standard package, adding a bunch of additional features. Some of the additional features in PyXML include:

• 4DOM, a full DOM implementation from FourThought, Inc.

• The xmlproc validating parser, written by Lars Marius Garshol.

• The sgmlop parser accelerator module, written by Fredrik Lundh.


### ===🗝 Module changes

Lots of improvements and bugfixes were made to Python’s extensive standard library; some of the affected modules include readline, ConfigParser, cgi, calendar, posix, readline, xmllib, aifc, chunk, wave, random, shelve, and nntplib. Consult the CVS logs for the exact patch-by-patch details.

Brian Gallew contributed OpenSSL support for the socket module. OpenSSL is an implementation of the Secure Socket Layer, which encrypts the data being sent over a socket. When compiling Python, you can edit Modules/Setup to include SSL support, which adds an additional function to the socket module: socket.ssl(socket, keyfile, certfile), which takes a socket object and returns an SSL socket. The httplib and urllib modules were also changed to support https:// URLs, though no one has implemented FTP or SMTP over SSL.

The httplib module has been rewritten by Greg Stein to support HTTP/1.1. Backward compatibility with the 1.5 version of httplib is provided, though using HTTP/1.1 features such as pipelining will require rewriting code to use a different set of interfaces.

The Tkinter module now supports Tcl/Tk version 8.1, 8.2, or 8.3, and support for the older 7.x versions has been dropped. The Tkinter module now supports displaying Unicode strings in Tk widgets. Also, Fredrik Lundh contributed an optimization which makes operations like create_line and create_polygon much faster, especially when using lots of coordinates.

The curses module has been greatly extended, starting from Oliver Andrich’s enhanced version, to provide many additional functions from ncurses and SYSV curses, such as colour, alternative character set support, pads, and mouse support. This means the module is no longer compatible with operating systems that only have BSD curses, but there don’t seem to be any currently maintained OSes that fall into this category.

As mentioned in the earlier discussion of 2.0’s Unicode support, the underlying implementation of the regular expressions provided by the re module has been changed. SRE, a new regular expression engine written by Fredrik Lundh and partially funded by Hewlett Packard, supports matching against both 8-bit strings and Unicode strings.


### ===🗝 New modules

A number of new modules were added. We’ll simply list them with brief descriptions; consult the 2.0 documentation for the details of a particular module.

• `atexit`: For registering functions to be called before the Python interpreter exits. Code that currently sets sys.exitfunc directly should be changed to use the atexit module instead, importing atexit and calling atexit.register() with the function to be called on exit. (Contributed by Skip Montanaro.)

• `codecs`, encodings, unicodedata: Added as part of the new Unicode support.

• `filecmp`: Supersedes the old cmp, cmpcache and dircmp modules, which have now become deprecated. (Contributed by Gordon MacMillan and Moshe Zadka.)

• `gettext`: This module provides internationalization (I18N) and localization (L10N) support for Python programs by providing an interface to the GNU gettext message catalog library. (Integrated by Barry Warsaw, from separate contributions by Martin von Löwis, Peter Funk, and James Henstridge.)

• `linuxaudiodev`: Support for the /dev/audio device on Linux, a twin to the existing sunaudiodev module. (Contributed by Peter Bosch, with fixes by Jeremy Hylton.)

• `mmap`: An interface to memory-mapped files on both Windows and Unix. A file’s contents can be mapped directly into memory, at which point it behaves like a mutable string, so its contents can be read and modified. They can even be passed to functions that expect ordinary strings, such as the re module. (Contributed by Sam Rushing, with some extensions by A.M. Kuchling.)

• `pyexpat`: An interface to the Expat XML parser. (Contributed by Paul Prescod.)

• `robotparser`: Parse a robots.txt file, which is used for writing web spiders that politely avoid certain areas of a web site. The parser accepts the contents of a robots.txt file, builds a set of rules from it, and can then answer questions about the fetchability of a given URL. (Contributed by Skip Montanaro.)

• `tabnanny`: A module/script to check Python source code for ambiguous indentation. (Contributed by Tim Peters.)

• `UserString`: A base class useful for deriving objects that behave like strings.

• `webbrowser`: A module that provides a platform independent way to launch a web browser on a specific URL. For each platform, various browsers are tried in a specific order. The user can alter which browser is launched by setting the BROWSER environment variable. (Originally inspired by Eric S. Raymond’s patch to urllib which added similar functionality, but the final module comes from code originally implemented by Fred Drake as Tools/idle/BrowserControl.py, and adapted for the standard library by Fred.)

• `_winreg`: An interface to the Windows registry. `_winreg` is an adaptation of functions that have been part of PythonWin since 1995, but has now been added to the core distribution, and enhanced to support Unicode. `_winreg` was written by Bill Tutt and Mark Hammond.

• `zipfile`: A module for reading and writing ZIP-format archives. These are archives produced by PKZIP on DOS/Windows or zip on Unix, not to be confused with gzip-format files (which are supported by the gzip module) (Contributed by James C. Ahlstrom.)

• `imputil`: A module that provides a simpler way for writing customized import hooks, in comparison to the existing ihooks module. (Implemented by Greg Stein, with much discussion on python-dev along the way.)


### ===🗝 IDLE Improvements

IDLE is the official Python cross-platform IDE, written using Tkinter. Python 2.0 includes IDLE 0.6, which adds a number of new features and improvements. A partial list:

• UI improvements and optimizations, especially in the area of syntax highlighting and auto-indentation.

• The class browser now shows more information, such as the top level functions in a module.

• Tab width is now a user settable option. When opening an existing Python file, IDLE automatically detects the indentation conventions, and adapts.

• There is now support for calling browsers on various platforms, used to open the Python documentation in a browser.

• IDLE now has a command line, which is largely similar to the vanilla Python interpreter.

• Call tips were added in many places.

• IDLE can now be installed as a package.

• In the editor window, there is now a line/column bar at the bottom.

• Three new keystroke commands: Check module (Alt-F5), Import module (F5) and Run script (Ctrl-F5).


### ===🗝 Deleted and Deprecated Modules

A few modules have been dropped because they’re obsolete, or because there are now better ways to do the same thing. The stdwin module is gone; it was for a platform-independent windowing toolkit that’s no longer developed.

A number of modules have been moved to the lib-old subdirectory: cmp, cmpcache, dircmp, dump, find, grep, packmail, poly, util, whatsound, zmod. If you have code which relies on a module that’s been moved to lib-old, you can simply add that directory to sys.path to get them back, but you’re encouraged to update any code that uses these modules.


### ===🗝 Acknowledgements

The authors would like to thank the following people for offering suggestions on various drafts of this article: David Bolen, Mark Hammond, Gregg Hauser, Jeremy Hylton, Fredrik Lundh, Detlef Lannert, Aahz Maruch, Skip Montanaro, Vladimir Marangozov, Tobias Polzin, Guido van Rossum, Neil Schemenauer, and Russ Schmidt.



# =🚩 Extending and Embedding the Python Interpreter

This document describes how to write modules in C or C++ to extend the Python interpreter with new modules. Those modules can not only define new functions but also new object types and their methods. The document also describes how to embed the Python interpreter in another application, for use as an extension language. Finally, it shows how to compile and link extension modules so that they can be loaded dynamically (at run time) into the interpreter, if the underlying operating system supports this feature.

This document assumes basic knowledge about Python. For an informal introduction to the language, see The Python Tutorial. The Python Language Reference gives a more formal definition of the language. The Python Standard Library documents the existing object types, functions and modules (both built-in and written in Python) that give the language its wide application range.

For a detailed description of the whole Python/C API, see the separate Python/C API Reference Manual.


Recommended third party tools

This guide only covers the basic tools for creating extensions provided as part of this version of CPython. Third party tools like Cython, cffi, SWIG and Numba offer both simpler and more sophisticated approaches to creating C and C++ extensions for Python.

See also:

 Python Packaging User Guide: Binary Extensions
 https://packaging.python.org/guides/packaging-binary-extensions/
 The Python Packaging User Guide not only covers several available tools that simplify the creation of binary extensions, but also discusses the various reasons why creating an extension module may be desirable in the first place.

Creating extensions without third party tools

This section of the guide covers creating C and C++ extensions without assistance from third party tools. It is intended primarily for creators of those tools, rather than being a recommended way to create your own C extensions.

• 1. Extending Python with C or C++
◦ 1.1. A Simple Example
◦ 1.2. Intermezzo: Errors and Exceptions
◦ 1.3. Back to the Example
◦ 1.4. The Module’s Method Table and Initialization Function
◦ 1.5. Compilation and Linkage
◦ 1.6. Calling Python Functions from C
◦ 1.7. Extracting Parameters in Extension Functions
◦ 1.8. Keyword Parameters for Extension Functions
◦ 1.9. Building Arbitrary Values
◦ 1.10. Reference Counts
◦ 1.11. Writing Extensions in C++
◦ 1.12. Providing a C API for an Extension Module

• 2. Defining Extension Types: Tutorial
◦ 2.1. The Basics
◦ 2.2. Adding data and methods to the Basic example
◦ 2.3. Providing finer control over data attributes
◦ 2.4. Supporting cyclic garbage collection
◦ 2.5. Subclassing other types

• 3. Defining Extension Types: Assorted Topics
◦ 3.1. Finalization and De-allocation
◦ 3.2. Object Presentation
◦ 3.3. Attribute Management
◦ 3.4. Object Comparison
◦ 3.5. Abstract Protocol Support
◦ 3.6. Weak Reference Support
◦ 3.7. More Suggestions

• 4. Building C and C++ Extensions
◦ 4.1. Building C and C++ Extensions with distutils
◦ 4.2. Distributing your extension modules

• 5. Building C and C++ Extensions on Windows
◦ 5.1. A Cookbook Approach
◦ 5.2. Differences Between Unix and Windows
◦ 5.3. Using DLLs in Practice


• Embedding the CPython runtime in a larger application
◦ 1. Embedding Python in Another Application
◦ 1.1. Very High Level Embedding
◦ 1.2. Beyond Very High Level Embedding: An overview
◦ 1.3. Pure Embedding
◦ 1.4. Extending Embedded Python
◦ 1.5. Embedding Python in C++
◦ 1.6. Compiling and Linking under Unix-like systems


## ==⚡ • 1. Extending Python with C or C++
- Writing cpython extension modules using C++ https://thomasnyberg.com/cpp_extension_modules.html

It is quite easy to add new built-in modules to Python, if you know how to program in C. Such extension modules can do two things that can’t be done directly in Python: they can implement new built-in object types, and they can call C library functions and system calls.

To support extensions, the Python API (Application Programmers Interface) defines a set of functions, macros and variables that provide access to most aspects of the Python run-time system. The Python API is incorporated in a C source file by including the header "Python.h".

The compilation of an extension module depends on its intended use as well as on your system setup; details are given in later chapters.

Note:
 The C extension interface is specific to CPython, and extension modules do not work on other Python implementations. In many cases, it is possible to avoid writing C extensions and preserve portability to other implementations. For example, if your use case is calling C library functions or system calls, you should consider using the ctypes module or the cffi library rather than writing custom C code. These modules let you write Python code to interface with C code and are more portable between implementations of Python than writing and compiling a C extension module.
 


### ===🗝 ◦ 1.1. A Simple Example

Let’s create an extension module called `spam` (the favorite food of Monty Python fans…) and let’s say we want to create a Python interface to the C library function `system()` [1]. This function takes a null-terminated character string as argument and returns an integer. We want this function to be callable from Python as follows:


>>> import spam
>>> status = spam.system("ls -l")


Begin by creating a file `spammodule.c`. (Historically, if a module is called spam, the C file containing its implementation is called `spammodule.c`; if the module name is very long, like spammify, the module name can be just `spammify.c`.)

The first two lines of our file can be:


```c
#define PY_SSIZE_T_CLEAN
#include <Python.h>
```


which pulls in the Python API (you can add a comment describing the purpose of the module and a copyright notice if you like).

Note:
 Since Python may define some pre-processor definitions which affect the standard headers on some systems, you must include Python.h before any standard headers are included.
 
It is recommended to always define `PY_SSIZE_T_CLEAN` before including `Python.h`. See Extracting Parameters in Extension Functions below for a description of this macro.

All user-visible symbols defined by `Python.h` have a prefix of Py or PY, except those defined in standard header files. For convenience, and since they are used extensively by the Python interpreter, "Python.h" includes a few standard header files: <stdio.h>, <string.h>, <errno.h>, and <stdlib.h>. If the latter header file does not exist on your system, it declares the functions malloc(), free() and realloc() directly.

The next thing we add to our module file is the C function that will be called when the Python expression `spam.system(string)` is evaluated (we’ll see shortly how it ends up being called):


```c
static PyObject *
spam_system(PyObject *self, PyObject *args)
{
    const char *command;
    int sts;

    if (!PyArg_ParseTuple(args, "s", &command))
        return NULL;
    sts = system(command);
    return PyLong_FromLong(sts);
}
```


There is a straightforward translation from the argument list in Python (for example, the single expression "ls -l") to the arguments passed to the C function. The C function `spam_system` always has two arguments, conventionally named self and args.

The `self` argument points to the module object for module-level functions; for a method it would point to the object instance.

The `args` argument will be a pointer to a Python tuple object containing the arguments. Each item of the tuple corresponds to an argument in the call’s argument list. The arguments are Python objects — in order to do anything with them in our C function we have to convert them to C values. 

The function `PyArg_ParseTuple()` in the Python API checks the argument types and converts them to C values. It uses a template string to determine the required types of the arguments as well as the types of the C variables into which to store the converted values. More about this later.

`PyArg_ParseTuple()` returns true (nonzero) if all arguments have the right type and its components have been stored in the variables whose addresses are passed. It returns false (zero) if an invalid argument list was passed. In the latter case it also raises an appropriate exception so the calling function can return NULL immediately (as we saw in the example).

### ===🗝 ◦ 1.2. Intermezzo: Errors and Exceptions

An important convention throughout the Python interpreter is the following: when a function fails, it should set an exception condition and return an error value (usually -1 or a NULL pointer). 

Exception information is stored in three members of the interpreter’s thread state. These are NULL if there is no exception. Otherwise they are the C equivalents of the members of the Python tuple returned by `sys.exc_info()`. These are the exception type, exception instance, and a traceback object. It is important to know about them to understand how errors are passed around.

The Python API defines a number of functions to set various types of exceptions.

The most common one is `PyErr_SetString()`. Its arguments are an exception object and a C string. The exception object is usually a predefined object like `PyExc_ZeroDivisionError`. The C string indicates the cause of the error and is converted to a Python string object and stored as the “associated value” of the exception.

Another useful function is `PyErr_SetFromErrno()`, which only takes an exception argument and constructs the associated value by inspection of the global variable errno. 

The most general function is `PyErr_SetObject()`, which takes two object arguments, the exception and its associated value. You don’t need to **Py_INCREF()** the objects passed to any of these functions.

You can test non-destructively whether an exception has been set with `PyErr_Occurred()`. This returns the current exception object, or NULL if no exception has occurred. You normally don’t need to call `PyErr_Occurred()` to see whether an error occurred in a function call, since you should be able to tell from the return value.

When a function f that calls another function g detects that the latter fails, f should itself return an error value (usually NULL or -1). It should not call one of the `PyErr_*()` functions — one has already been called by g. f’s caller is then supposed to also return an error indication to its caller, again without calling `PyErr_*()`, and so on — the most detailed cause of the error was already reported by the function that first detected it. Once the error reaches the Python interpreter’s main loop, this aborts the currently executing Python code and tries to find an exception handler specified by the Python programmer.

(There are situations where a module can actually give a more detailed error message by calling another `PyErr_*()` function, and in such cases it is fine to do so. As a general rule, however, this is not necessary, and can cause information about the cause of the error to be lost: most operations can fail for a variety of reasons.)

To ignore an exception set by a function call that failed, the exception condition must be cleared explicitly by calling `PyErr_Clear()`. The only time C code should call `PyErr_Clear()` is if it doesn’t want to pass the error on to the interpreter but wants to handle it completely by itself (possibly by trying something else, or pretending nothing went wrong).

Every failing `malloc()` call must be turned into an exception — the direct caller of `malloc()` (or `realloc()`) must call `PyErr_NoMemory()` and return a failure indicator itself. All the object-creating functions (for example, `PyLong_FromLong()`) already do this, so this note is only relevant to those who call `malloc()` directly.

Also note that, with the important exception of PyArg_ParseTuple() and friends, functions that return an integer status usually return a positive value or zero for success and -1 for failure, like Unix system calls.

Finally, be careful to clean up garbage (by making **Py_XDECREF()** or **Py_DECREF()** calls for objects you have already created) when you return an error indicator!

The choice of which exception to raise is entirely yours. There are predeclared C objects corresponding to all built-in Python exceptions, such as `PyExc_ZeroDivisionError`, which you can use directly. Of course, you should choose exceptions wisely — don’t use `PyExc_TypeError` to mean that a file couldn’t be opened (that should probably be `PyExc_IOError`). If something’s wrong with the argument list, the `PyArg_ParseTuple()` function usually raises `PyExc_TypeError`. If you have an argument whose value must be in a particular range or must satisfy other conditions, `PyExc_ValueError` is appropriate.

You can also define a new exception that is unique to your module. For this, you usually declare a static object variable at the beginning of your file:


    static PyObject *SpamError;


and initialize it in your module’s initialization function (`PyInit_spam()`) with an exception object:


```c
PyMODINIT_FUNC
PyInit_spam(void)
{
    PyObject *m;

    m = PyModule_Create(&spammodule);
    if (m == NULL)
        return NULL;

    SpamError = PyErr_NewException("spam.error", NULL, NULL);
    Py_XINCREF(SpamError);
    if (PyModule_AddObject(m, "error", SpamError) < 0) {
        Py_XDECREF(SpamError);
        Py_CLEAR(SpamError);
        Py_DECREF(m);
        return NULL;
    }

    return m;
}
```


Note that the Python name for the exception object is spam.error. The `PyErr_NewException()` function may create a class with the base class being Exception (unless another class is passed in instead of NULL), described in Built-in Exceptions.

Note also that the `SpamError` variable retains a reference to the newly created exception class; this is intentional! Since the exception could be removed from the module by external code, an owned reference to the class is needed to ensure that it will not be discarded, causing `SpamError` to become a dangling pointer. Should it become a dangling pointer, C code which raises the exception could cause a core dump or other unintended side effects.

We discuss the use of *PyMODINIT_FUNC* as a function return type later in this sample.

The spam.error exception can be raised in your extension module using a call to `PyErr_SetString()` as shown below:


```c
static PyObject *
spam_system(PyObject *self, PyObject *args)
{
    const char *command;
    int sts;

    if (!PyArg_ParseTuple(args, "s", &command))
        return NULL;
    sts = system(command);
    if (sts < 0) {
        PyErr_SetString(SpamError, "System command failed");
        return NULL;
    }
    return PyLong_FromLong(sts);
}
```


### ===🗝 ◦ 1.3. Back to the Example


Going back to our example function, you should now be able to understand this statement:


```c
if (!PyArg_ParseTuple(args, "s", &command))
    return NULL;
```


It returns NULL (the error indicator for functions returning object pointers) if an error is detected in the argument list, relying on the exception set by PyArg_ParseTuple(). Otherwise the string value of the argument has been copied to the local variable command. This is a pointer assignment and you are not supposed to modify the string to which it points (so in Standard C, the variable command should properly be declared as const `char *command`).

The next statement is a call to the Unix function `system()`, passing it the string we just got from PyArg_ParseTuple():


```c
sts = system(command);
```


Our `spam.system()` function must return the value of sts as a Python object. This is done using the function `PyLong_FromLong()`.


```c
return PyLong_FromLong(sts);
```


In this case, it will return an integer object. (Yes, even integers are objects on the heap in Python!)

If you have a C function that returns no useful argument (a function returning void), the corresponding Python function must return None. You need this idiom to do so (which is implemented by the `Py_RETURN_NONE` macro):


```c
Py_INCREF(Py_None);
return Py_None;
```


`Py_None` is the C name for the special Python object `None`. It is a genuine Python object rather than a NULL pointer, which means “error” in most contexts, as we have seen.


### ===🗝 ◦ 1.4. The Module’s Method Table and Initialization Function

I promised to show how `spam_system()` is called from Python programs. First, we need to list its name and address in a “method table”:


```c
static PyMethodDef SpamMethods[] = {
    ...
    {"system",  spam_system, METH_VARARGS,
     "Execute a shell command."},
    ...
    {NULL, NULL, 0, NULL}        /* Sentinel */
};
```


Note the third entry (`METH_VARARGS`). This is a flag telling the interpreter the calling convention to be used for the C function. It should normally always be `METH_VARARGS` or `METH_VARARGS | METH_KEYWORDS`; a value of 0 means that an obsolete variant of `PyArg_ParseTuple()` is used.

When using only `METH_VARARGS`, the function should expect the Python-level parameters to be passed in as a tuple acceptable for parsing via `PyArg_ParseTuple()`; more information on this function is provided below.

The `METH_KEYWORDS` bit may be set in the third field if keyword arguments should be passed to the function. In this case, the C function should accept a third `PyObject *` parameter which will be a dictionary of keywords. Use `PyArg_ParseTupleAndKeywords()` to parse the arguments to such a function.

The method table must be referenced in the module definition structure:


```c
static struct PyModuleDef spammodule = {
    PyModuleDef_HEAD_INIT,
    "spam",   /* name of module */
    spam_doc, /* module documentation, may be NULL */
    -1,       /* size of per-interpreter state of the module,
                 or -1 if the module keeps state in global variables. */
    SpamMethods
};
```


This structure, in turn, must be passed to the interpreter in the module’s initialization function. The initialization function must be named `PyInit_name()`, where name is the name of the module, and should be the only non-static item defined in the module file:


```c
PyMODINIT_FUNC
PyInit_spam(void)
{
    return PyModule_Create(&spammodule);
}
```


Note that *PyMODINIT_FUNC* declares the function as `PyObject *` return type, declares any special linkage declarations required by the platform, and for C++ declares the function as extern "C".

When the Python program imports module spam for the first time, `PyInit_spam()` is called. (See below for comments about embedding Python.) It calls *PyModule_Create()*, which returns a module object, and inserts built-in function objects into the newly created module based upon the table (an array of **PyMethodDef** structures) found in the module definition. *PyModule_Create()* returns a pointer to the module object that it creates. It may abort with a fatal error for certain errors, or return NULL if the module could not be initialized satisfactorily. The init function must return the module object to its caller, so that it then gets inserted into sys.modules.

When embedding Python, the `PyInit_spam()` function is not called automatically unless there’s an entry in the *PyImport_Inittab* table. To add the module to the initialization table, use *PyImport_AppendInittab()*, optionally followed by an import of the module:


```c
int
main(int argc, char *argv[])
{
    wchar_t *program = Py_DecodeLocale(argv[0], NULL);
    if (program == NULL) {
        fprintf(stderr, "Fatal error: cannot decode argv[0]\n");
        exit(1);
    }

    /* Add a built-in module, before Py_Initialize */
    if (PyImport_AppendInittab("spam", PyInit_spam) == -1) {
        fprintf(stderr, "Error: could not extend in-built modules table\n");
        exit(1);
    }

    /* Pass argv[0] to the Python interpreter */
    Py_SetProgramName(program);

    /* Initialize the Python interpreter.  Required.
       If this step fails, it will be a fatal error. */
    Py_Initialize();

    /* Optionally import the module; alternatively,
       import can be deferred until the embedded script
       imports it. */
    PyObject *pmodule = PyImport_ImportModule("spam");
    if (!pmodule) {
        PyErr_Print();
        fprintf(stderr, "Error: could not import module 'spam'\n");
    }

    ...

    PyMem_RawFree(program);
    return 0;
}
```


Note:
 Removing entries from sys.modules or importing compiled modules into multiple interpreters within a process (or following a fork() without an intervening exec()) can create problems for some extension modules. Extension module authors should exercise caution when initializing internal data structures.
 

A more substantial example module is included in the Python source distribution as Modules/xxmodule.c. This file may be used as a template or simply read as an example.

Note:
 Unlike our spam example, xxmodule uses multi-phase initialization (new in Python 3.5), where a PyModuleDef structure is returned from PyInit_spam, and creation of the module is left to the import machinery. For details on multi-phase initialization, see PEP 489.
 

### ===🗝 ◦ 1.5. Compilation and Linkage

There are two more things to do before you can use your new extension: compiling and linking it with the Python system. 

If you use dynamic loading, the details may depend on the style of dynamic loading your system uses; see the chapters about building extension modules (chapter 4 Building C and C++ Extensions) and additional information that pertains only to building on Windows (chapter 5 Building C and C++ Extensions on Windows) for more information about this.

If you can’t use dynamic loading, or if you want to make your module a permanent part of the Python interpreter, you will have to change the configuration setup and rebuild the interpreter. Luckily, this is very simple on Unix: just place your file (spammodule.c for example) in the Modules/ directory of an unpacked source distribution, add a line to the file `Modules/Setup.local` describing your file:


    spam spammodule.o


and rebuild the interpreter by running make in the toplevel directory. You can also run make in the Modules/ subdirectory, but then you must first rebuild Makefile there by running ‘make Makefile’. (This is necessary each time you change the Setup file.)

If your module requires additional libraries to link with, these can be listed on the line in the configuration file as well, for instance:


    spam spammodule.o -lX11



### ===🗝 ◦ 1.6. Calling Python Functions from C

So far we have concentrated on making C functions callable from Python. The reverse is also useful: calling Python functions from C. This is especially the case for libraries that support so-called “callback” functions. If a C interface makes use of callbacks, the equivalent Python often needs to provide a callback mechanism to the Python programmer; the implementation will require calling the Python callback functions from a C callback. Other uses are also imaginable.

Fortunately, the Python interpreter is easily called recursively, and there is a standard interface to call a Python function. (I won’t dwell on how to call the Python parser with a particular string as input — if you’re interested, have a look at the implementation of the `-c command` line option in `Modules/main.c` from the Python source code.)

Calling a Python function is easy. First, the Python program must somehow pass you the Python function object. You should provide a function (or some other interface) to do this. When this function is called, save a pointer to the Python function object (be careful to **Py_INCREF()** it!) in a global variable — or wherever you see fit. For example, the following function might be part of a module definition:


```c
static PyObject *my_callback = NULL;

static PyObject *
my_set_callback(PyObject *dummy, PyObject *args)
{
    PyObject *result = NULL;
    PyObject *temp;

    if (PyArg_ParseTuple(args, "O:set_callback", &temp)) {
        if (!PyCallable_Check(temp)) {
            PyErr_SetString(PyExc_TypeError, "parameter must be callable");
            return NULL;
        }
        Py_XINCREF(temp);         /* Add a reference to new callback */
        Py_XDECREF(my_callback);  /* Dispose of previous callback */
        my_callback = temp;       /* Remember new callback */
        /* Boilerplate to return "None" */
        Py_INCREF(Py_None);
        result = Py_None;
    }
    return result;
}
```


This function must be registered with the interpreter using the `METH_VARARGS` flag; this is described in section The Module’s Method Table and Initialization Function. The `PyArg_ParseTuple()` function and its arguments are documented in section Extracting Parameters in Extension Functions.

The macros **Py_XINCREF()** and **Py_XDECREF()** increment/decrement the reference count of an object and are safe in the presence of NULL pointers (but note that temp will not be NULL in this context). More info on them in section Reference Counts.

Later, when it is time to call the function, you call the C function **PyObject_CallObject()**. This function has two arguments, both pointers to arbitrary Python objects: the Python function, and the argument list. The argument list must always be a tuple object, whose length is the number of arguments. To call the Python function with no arguments, pass in NULL, or an empty tuple; to call it with one argument, pass a singleton tuple. **Py_BuildValue()** returns a tuple when its format string consists of zero or more format codes between parentheses. For example:


```c
int arg;
PyObject *arglist;
PyObject *result;
...
arg = 123;
...
/* Time to call the callback */
arglist = Py_BuildValue("(i)", arg);
result = PyObject_CallObject(my_callback, arglist);
Py_DECREF(arglist);
```


**PyObject_CallObject()** returns a Python object pointer: this is the return value of the Python function. **PyObject_CallObject()** is “reference-count-neutral” with respect to its arguments. In the example a new tuple was created to serve as the argument list, which is Py_DECREF()-ed immediately after the **PyObject_CallObject()** call.

The return value of **PyObject_CallObject()** is “new”: either it is a brand new object, or it is an existing object whose reference count has been incremented. So, unless you want to save it in a global variable, you should somehow Py_DECREF() the result, even (especially!) if you are not interested in its value.

Before you do this, however, it is important to check that the return value isn’t NULL. If it is, the Python function terminated by raising an exception. If the C code that called **PyObject_CallObject()** is called from Python, it should now return an error indication to its Python caller, so the interpreter can print a stack trace, or the calling Python code can handle the exception. If this is not possible or desirable, the exception should be cleared by calling `PyErr_Clear()`. For example:


```c
if (result == NULL)
    return NULL; /* Pass error back */
...use result...
Py_DECREF(result);
```


Depending on the desired interface to the Python callback function, you may also have to provide an argument list to **PyObject_CallObject()**. In some cases the argument list is also provided by the Python program, through the same interface that specified the callback function. It can then be saved and used in the same manner as the function object. In other cases, you may have to construct a new tuple to pass as the argument list. The simplest way to do this is to call **Py_BuildValue()**. For example, if you want to pass an integral event code, you might use the following code:


```c
PyObject *arglist;
...
arglist = Py_BuildValue("(l)", eventcode);
result = PyObject_CallObject(my_callback, arglist);
Py_DECREF(arglist);
if (result == NULL)
    return NULL; /* Pass error back */
/* Here maybe use the result */
Py_DECREF(result);
```


Note the placement of `Py_DECREF(arglist)` immediately after the call, before the error check! Also note that strictly speaking this code is not complete: **Py_BuildValue()** may run out of memory, and this should be checked.

You may also call a function with keyword arguments by using **PyObject_Call()**, which supports arguments and keyword arguments. As in the above example, we use **Py_BuildValue()** to construct the dictionary.


```c
PyObject *dict;
...
dict = Py_BuildValue("{s:i}", "name", val);
result = PyObject_Call(my_callback, NULL, dict);
Py_DECREF(dict);
if (result == NULL)
    return NULL; /* Pass error back */
/* Here maybe use the result */
Py_DECREF(result);
```


### ===🗝 ◦ 1.7. Extracting Parameters in Extension Functions


The `PyArg_ParseTuple()` function is declared as follows:


➡ `int PyArg_ParseTuple(PyObject *arg, const char *format, ...);`


The arg argument must be a tuple object containing an argument list passed from Python to a C function. The format argument must be a format string, whose syntax is explained in Parsing arguments and building values in the Python/C API Reference Manual. The remaining arguments must be addresses of variables whose type is determined by the format string.

Note that while `PyArg_ParseTuple()` checks that the Python arguments have the required types, it cannot check the validity of the addresses of C variables passed to the call: if you make mistakes there, your code will probably crash or at least overwrite random bits in memory. So be careful!

Note that any Python object references which are provided to the caller are borrowed references; do not decrement their reference count!

Some example calls:


```c
#define PY_SSIZE_T_CLEAN  /* Make "s#" use Py_ssize_t rather than int. */
#include <Python.h>



int ok;
int i, j;
long k, l;
const char *s;
Py_ssize_t size;

ok = PyArg_ParseTuple(args, ""); /* No arguments */
    /* Python call: f() */

ok = PyArg_ParseTuple(args, "s", &s); /* A string */
    /* Possible Python call: f('whoops!') */

ok = PyArg_ParseTuple(args, "lls", &k, &l, &s); /* 2 longs and 1 string */
    /* Possible Python call: f(1, 2, 'three') */

ok = PyArg_ParseTuple(args, "(ii)s#", &i, &j, &s, &size);
    /* A pair of ints and a string, whose size is also returned */
    /* Possible Python call: f((1, 2), 'three') */

{
    const char *file;
    const char *mode = "r";
    int bufsize = 0;
    ok = PyArg_ParseTuple(args, "s|si", &file, &mode, &bufsize);
    /* A string, and optionally another string and an integer */
    /* Possible Python calls:
       f('spam')
       f('spam', 'w')
       f('spam', 'wb', 100000) */
}



{
    int left, top, right, bottom, h, v;
    ok = PyArg_ParseTuple(args, "((ii)(ii))(ii)",
             &left, &top, &right, &bottom, &h, &v);
    /* A rectangle and a point */
    /* Possible Python call:
       f(((0, 0), (400, 300)), (10, 10)) */
}



{
    Py_complex c;
    ok = PyArg_ParseTuple(args, "D:myfunction", &c);
    /* a complex, also providing a function name for errors */
    /* Possible Python call: myfunction(1+2j) */
}
```

### ===🗝 ◦ 1.8. Keyword Parameters for Extension Functions

The `PyArg_ParseTupleAndKeywords()` function is declared as follows:


➡ `int PyArg_ParseTupleAndKeywords(PyObject *arg, PyObject *kwdict, const char *format, char *kwlist[], ...);`


The arg and format parameters are identical to those of the `PyArg_ParseTuple()` function. The `kwdict` parameter is the dictionary of keywords received as the third parameter from the Python runtime. The `kwlist` parameter is a NULL-terminated list of strings which identify the parameters; the names are matched with the type information from format from left to right. On success, `PyArg_ParseTupleAndKeywords()` returns true, otherwise it returns false and raises an appropriate exception.

Note:
 Nested tuples cannot be parsed when using keyword arguments! Keyword parameters passed in which are not present in the kwlist will cause TypeError to be raised.
 

Here is an example module which uses keywords, based on an example by Geoff Philbrick (philbrick@hks.com):


```c
#define PY_SSIZE_T_CLEAN  /* Make "s#" use Py_ssize_t rather than int. */
#include <Python.h>

static PyObject *
keywdarg_parrot(PyObject *self, PyObject *args, PyObject *keywds)
{
    int voltage;
    const char *state = "a stiff";
    const char *action = "voom";
    const char *type = "Norwegian Blue";

    static char *kwlist[] = {"voltage", "state", "action", "type", NULL};

    if (!PyArg_ParseTupleAndKeywords(args, keywds, "i|sss", kwlist,
                                     &voltage, &state, &action, &type))
        return NULL;

    printf("-- This parrot wouldn't %s if you put %i Volts through it.\n",
           action, voltage);
    printf("-- Lovely plumage, the %s -- It's %s!\n", type, state);

    Py_RETURN_NONE;
}

static PyMethodDef keywdarg_methods[] = {
    /* The cast of the function is necessary since PyCFunction values
     * only take two PyObject* parameters, and keywdarg_parrot() takes
     * three.
     */
    {"parrot", (PyCFunction)(void(*)(void))keywdarg_parrot, METH_VARARGS | METH_KEYWORDS,
     "Print a lovely skit to standard output."},
    {NULL, NULL, 0, NULL}   /* sentinel */
};

static struct PyModuleDef keywdargmodule = {
    PyModuleDef_HEAD_INIT,
    "keywdarg",
    NULL,
    -1,
    keywdarg_methods
};

PyMODINIT_FUNC
PyInit_keywdarg(void)
{
    return PyModule_Create(&keywdargmodule);
}
```


### ===🗝 ◦ 1.9. Building Arbitrary Values - format 


This function is the counterpart to `PyArg_ParseTuple()`. It is declared as follows:


➡ `PyObject *Py_BuildValue(const char *format, ...);`


It recognizes a set of format units similar to the ones recognized by `PyArg_ParseTuple()`, but the arguments (which are input to the function, not output) must not be pointers, just values. It returns a new Python object, suitable for returning from a C function called from Python.

One difference with `PyArg_ParseTuple()`: while the latter requires its first argument to be a tuple (since Python argument lists are always represented as tuples internally), **Py_BuildValue()** does not always build a tuple. It builds a tuple only if its format string contains two or more format units. If the format string is empty, it returns None; if it contains exactly one format unit, it returns whatever object is described by that format unit. To force it to return a tuple of size 0 or one, parenthesize the format string.

Examples (to the left the call, to the right the resulting Python value):


```c
Py_BuildValue("")                        =>  None
Py_BuildValue("i", 123)                  =>  123
Py_BuildValue("iii", 123, 456, 789)      =>  (123, 456, 789)
Py_BuildValue("s", "hello")              =>  'hello'
Py_BuildValue("y", "hello")              =>  b'hello'
Py_BuildValue("ss", "hello", "world")    =>  ('hello', 'world')
Py_BuildValue("s#", "hello", 4)          =>  'hell'
Py_BuildValue("y#", "hello", 4)          =>  b'hell'
Py_BuildValue("()")                      =>  ()
Py_BuildValue("(i)", 123)                =>  (123,)
Py_BuildValue("(ii)", 123, 456)          =>  (123, 456)
Py_BuildValue("(i,i)", 123, 456)         =>  (123, 456)
Py_BuildValue("[i,i]", 123, 456)         =>  [123, 456]
Py_BuildValue("{s:i,s:i}",
              "abc", 123, "def", 456)    =>  {'abc': 123, 'def': 456}
Py_BuildValue("((ii)(ii)) (ii)",
              1, 2, 3, 4, 5, 6)          =>  (((1, 2), (3, 4)), (5, 6))
```

### ===🗝 ◦ 1.10. Reference Counts

In languages like C or C++, the programmer is responsible for dynamic allocation and deallocation of memory on the heap. In C, this is done using the functions `malloc()` and `free()`. In C++, the operators new and delete are used with essentially the same meaning and we’ll restrict the following discussion to the C case.

Every block of memory allocated with `malloc()` should eventually be returned to the pool of available memory by exactly one call to `free()`. It is important to call `free()` at the right time. If a block’s address is forgotten but `free()` is not called for it, the memory it occupies cannot be reused until the program terminates. This is called a memory leak. On the other hand, if a program calls `free()` for a block and then continues to use the block, it creates a conflict with re-use of the block through another `malloc()` call. This is called using freed memory. It has the same bad consequences as referencing uninitialized data — core dumps, wrong results, mysterious crashes.

Common causes of memory leaks are unusual paths through the code. For instance, a function may allocate a block of memory, do some calculation, and then free the block again. Now a change in the requirements for the function may add a test to the calculation that detects an error condition and can return prematurely from the function. It’s easy to forget to free the allocated memory block when taking this premature exit, especially when it is added later to the code. Such leaks, once introduced, often go undetected for a long time: the error exit is taken only in a small fraction of all calls, and most modern machines have plenty of virtual memory, so the leak only becomes apparent in a long-running process that uses the leaking function frequently. Therefore, it’s important to prevent leaks from happening by having a coding convention or strategy that minimizes this kind of errors.

Since Python makes heavy use of `malloc()` and `free()`, it needs a strategy to avoid memory leaks as well as the use of freed memory. The chosen method is called reference counting. The principle is simple: every object contains a counter, which is incremented when a reference to the object is stored somewhere, and which is decremented when a reference to it is deleted. When the counter reaches zero, the last reference to the object has been deleted and the object is freed.

An alternative strategy is called automatic garbage collection. (Sometimes, reference counting is also referred to as a garbage collection strategy, hence my use of “automatic” to distinguish the two.) The big advantage of automatic garbage collection is that the user doesn’t need to call `free()` explicitly. (Another claimed advantage is an improvement in speed or memory usage — this is no hard fact however.) The disadvantage is that for C, there is no truly portable automatic garbage collector, while reference counting can be implemented portably (as long as the functions `malloc()` and `free()` are available — which the C Standard guarantees). Maybe some day a sufficiently portable automatic garbage collector will be available for C. Until then, we’ll have to live with reference counts.

While Python uses the traditional reference counting implementation, it also offers a cycle detector that works to detect reference cycles. This allows applications to not worry about creating direct or indirect circular references; these are the weakness of garbage collection implemented using only reference counting. Reference cycles consist of objects which contain (possibly indirect) references to themselves, so that each object in the cycle has a reference count which is non-zero. Typical reference counting implementations are not able to reclaim the memory belonging to any objects in a reference cycle, or referenced from the objects in the cycle, even though there are no further references to the cycle itself.

The cycle detector is able to detect garbage cycles and can reclaim them. The gc module exposes a way to run the detector (the `collect()` function), as well as configuration interfaces and the ability to disable the detector at runtime.

#### ◦ 1.10.1. Reference Counting in Python

There are two macros, `Py_INCREF(x)` and `Py_DECREF(x)`, which handle the incrementing and decrementing of the reference count. **Py_DECREF()** also frees the object when the count reaches zero. For flexibility, it doesn’t call `free()` directly — rather, it makes a call through a function pointer in the object’s type object. For this purpose (and others), every object also contains a pointer to its type object.

The big question now remains: when to use `Py_INCREF(x)` and `Py_DECREF(x)`? Let’s first introduce some terms. Nobody “owns” an object; however, you can own a reference to an object. An object’s reference count is now defined as the number of owned references to it. The owner of a reference is responsible for calling **Py_DECREF()** when the reference is no longer needed. Ownership of a reference can be transferred. There are three ways to dispose of an owned reference: pass it on, store it, or call **Py_DECREF()**. Forgetting to dispose of an owned reference creates a memory leak.

It is also possible to borrow [2] a reference to an object. The borrower of a reference should not call **Py_DECREF()**. The borrower must not hold on to the object longer than the owner from which it was borrowed. Using a borrowed reference after the owner has disposed of it risks using freed memory and should be avoided completely [3].

The advantage of borrowing over owning a reference is that you don’t need to take care of disposing of the reference on all possible paths through the code — in other words, with a borrowed reference you don’t run the risk of leaking when a premature exit is taken. The disadvantage of borrowing over owning is that there are some subtle situations where in seemingly correct code a borrowed reference can be used after the owner from which it was borrowed has in fact disposed of it.

A borrowed reference can be changed into an owned reference by calling **Py_INCREF()**. This does not affect the status of the owner from which the reference was borrowed — it creates a new owned reference, and gives full owner responsibilities (the new owner must dispose of the reference properly, as well as the previous owner).


#### ◦ 1.10.2. Ownership Rules

Whenever an object reference is passed into or out of a function, it is part of the function’s interface specification whether ownership is transferred with the reference or not.

Most functions that return a reference to an object pass on ownership with the reference. In particular, all functions whose function it is to create a new object, such as **PyLong_FromLong()** and **Py_BuildValue()**, pass ownership to the receiver. Even if the object is not actually new, you still receive ownership of a new reference to that object. For instance, **PyLong_FromLong()** maintains a cache of popular values and can return a reference to a cached item.

Many functions that extract objects from other objects also transfer ownership with the reference, for instance **PyObject_GetAttrString()**. The picture is less clear, here, however, since a few common routines are exceptions: **PyTuple_GetItem()**, **PyList_GetItem()**, **PyDict_GetItem()**, and **PyDict_GetItemString()** all return references that you borrow from the tuple, list or dictionary.

The function **PyImport_AddModule()** also returns a borrowed reference, even though it may actually create the object it returns: this is possible because an owned reference to the object is stored in sys.modules.

When you pass an object reference into another function, in general, the function borrows the reference from you — if it needs to store it, it will use **Py_INCREF()** to become an independent owner. There are exactly two important exceptions to this rule: **PyTuple_SetItem()** and **PyList_SetItem()**. These functions take over ownership of the item passed to them — even if they fail! (Note that **PyDict_SetItem()** and friends don’t take over ownership — they are “normal.”)

When a C function is called from Python, it borrows references to its arguments from the caller. The caller owns a reference to the object, so the borrowed reference’s lifetime is guaranteed until the function returns. Only when such a borrowed reference must be stored or passed on, it must be turned into an owned reference by calling **Py_INCREF()**.

The object reference returned from a C function that is called from Python must be an owned reference — ownership is transferred from the function to its caller.


#### ◦ 1.10.3. Thin Ice

There are a few situations where seemingly harmless use of a borrowed reference can lead to problems. These all have to do with implicit invocations of the interpreter, which can cause the owner of a reference to dispose of it.

The first and most important case to know about is using **Py_DECREF()** on an unrelated object while borrowing a reference to a list item. For instance:


```c
void
bug(PyObject *list)
{
    PyObject *item = PyList_GetItem(list, 0);

    PyList_SetItem(list, 1, PyLong_FromLong(0L));
    PyObject_Print(item, stdout, 0); /* BUG! */
}
```


This function first borrows a reference to list[0], then replaces list[1] with the value 0, and finally prints the borrowed reference. Looks harmless, right? But it’s not!

Let’s follow the control flow into **PyList_SetItem()**. The list owns references to all its items, so when item 1 is replaced, it has to dispose of the original item 1. Now let’s suppose the original item 1 was an instance of a user-defined class, and let’s further suppose that the class defined a __del__() method. If this class instance has a reference count of 1, disposing of it will call its __del__() method.

Since it is written in Python, the __del__() method can execute arbitrary Python code. Could it perhaps do something to invalidate the reference to item in `bug()`? You bet! Assuming that the list passed into `bug()` is accessible to the __del__() method, it could execute a statement to the effect of del list[0], and assuming this was the last reference to that object, it would free the memory associated with it, thereby invalidating item.

The solution, once you know the source of the problem, is easy: temporarily increment the reference count. The correct version of the function reads:


```c
void
no_bug(PyObject *list)
{
    PyObject *item = PyList_GetItem(list, 0);

    Py_INCREF(item);
    PyList_SetItem(list, 1, PyLong_FromLong(0L));
    PyObject_Print(item, stdout, 0);
    Py_DECREF(item);
}
```


This is a true story. An older version of Python contained variants of this bug and someone spent a considerable amount of time in a C debugger to figure out why his __del__() methods would fail…

The second case of problems with a borrowed reference is a variant involving threads. Normally, multiple threads in the Python interpreter can’t get in each other’s way, because there is a global lock protecting Python’s entire object space. However, it is possible to temporarily release this lock using the macro `Py_BEGIN_ALLOW_THREADS`, and to re-acquire it using `Py_END_ALLOW_THREADS`. This is common around blocking I/O calls, to let other threads use the processor while waiting for the I/O to complete. Obviously, the following function has the same problem as the previous one:


```c
void
bug(PyObject *list)
{
    PyObject *item = PyList_GetItem(list, 0);
    Py_BEGIN_ALLOW_THREADS
    ...some blocking I/O call...
    Py_END_ALLOW_THREADS
    PyObject_Print(item, stdout, 0); /* BUG! */
}
```



#### ◦ 1.10.4. NULL Pointers

In general, functions that take object references as arguments do not expect you to pass them NULL pointers, and will dump core (or cause later core dumps) if you do so. Functions that return object references generally return NULL only to indicate that an exception occurred. The reason for not testing for NULL arguments is that functions often pass the objects they receive on to other function — if each function were to test for NULL, there would be a lot of redundant tests and the code would run more slowly.

It is better to test for NULL only at the “source:” when a pointer that may be NULL is received, for example, from `malloc()` or from a function that may raise an exception.

The macros **Py_INCREF()** and **Py_DECREF()** do not check for NULL pointers — however, their variants **Py_XINCREF()** and **Py_XDECREF()** do.

The macros for checking for a particular object type (`Pytype_Check()`) don’t check for NULL pointers — again, there is much code that calls several of these in a row to test an object against various different expected types, and this would generate redundant tests. There are no variants with NULL checking.

The C function calling mechanism guarantees that the argument list passed to C functions (args in the examples) is never NULL — in fact it guarantees that it is always a tuple [4].

It is a severe error to ever let a NULL pointer “escape” to the Python user.



### ===🗝 ◦ 1.11. Writing Extensions in C++

It is possible to write extension modules in C++. Some restrictions apply. If the main program (the Python interpreter) is compiled and linked by the C compiler, global or static objects with constructors cannot be used. This is not a problem if the main program is linked by the C++ compiler. Functions that will be called by the Python interpreter (in particular, module initialization functions) have to be declared using extern "C". 

It is unnecessary to enclose the Python header files in `extern "C" {...}` — they use this form already if the symbol `__cplusplus` is defined (all recent C++ compilers define this symbol).

### ===🗝 ◦ 1.12. Providing a C API for an Extension Module

Many extension modules just provide new functions and types to be used from Python, but sometimes the code in an extension module can be useful for other extension modules. 

For example, an extension module could implement a type “collection” which works like lists without order. Just like the standard Python list type has a C API which permits extension modules to create and manipulate lists, this new collection type should have a set of C functions for direct manipulation from other extension modules.

At first sight this seems easy: just write the functions (without declaring them static, of course), provide an appropriate header file, and document the C API. And in fact this would work if all extension modules were always linked statically with the Python interpreter. When modules are used as shared libraries, however, the symbols defined in one module may not be visible to another module. The details of visibility depend on the operating system; some systems use one global namespace for the Python interpreter and all extension modules (Windows, for example), whereas others require an explicit list of imported symbols at module link time (AIX is one example), or offer a choice of different strategies (most Unices). And even if symbols are globally visible, the module whose functions one wishes to call might not have been loaded yet!

Portability therefore requires not to make any assumptions about symbol visibility. This means that all symbols in extension modules should be declared static, except for the module’s initialization function, in order to avoid name clashes with other extension modules (as discussed in section The Module’s Method Table and Initialization Function). And it means that symbols that should be accessible from other extension modules must be exported in a different way.

Python provides a special mechanism to pass C-level information (pointers) from one extension module to another one: Capsules. A Capsule is a Python data type which stores a pointer (`void *`). Capsules can only be created and accessed via their C API, but they can be passed around like any other Python object. In particular, they can be assigned to a name in an extension module’s namespace. Other extension modules can then import this module, retrieve the value of this name, and then retrieve the pointer from the Capsule.

There are many ways in which Capsules can be used to export the C API of an extension module. Each function could get its own Capsule, or all C API pointers could be stored in an array whose address is published in a Capsule. And the various tasks of storing and retrieving the pointers can be distributed in different ways between the module providing the code and the client modules.

Whichever method you choose, it’s important to name your Capsules properly. The function `PyCapsule_New()` takes a name parameter (`const char *`); you’re permitted to pass in a NULL name, but we strongly encourage you to specify a name. Properly named Capsules provide a degree of runtime type-safety; there is no feasible way to tell one unnamed Capsule from another.

In particular, Capsules used to expose C APIs should be given a name following this convention:


    modulename.attributename


The convenience function `PyCapsule_Import()` makes it easy to load a C API provided via a Capsule, but only if the Capsule’s name matches this convention. This behavior gives C API users a high degree of certainty that the Capsule they load contains the correct C API.

The following example demonstrates an approach that puts most of the burden on the writer of the exporting module, which is appropriate for commonly used library modules. It stores all C API pointers (just one in the example!) in an array of void pointers which becomes the value of a Capsule. The header file corresponding to the module provides a macro that takes care of importing the module and retrieving its C API pointers; client modules only have to call this macro before accessing the C API.

The exporting module is a modification of the spam module from section A Simple Example. The function `spam.system()` does not call the C library function system() directly, but a function `PySpam_System()`, which would of course do something more complicated in reality (such as adding “spam” to every command). This function `PySpam_System()` is also exported to other extension modules.

The function `PySpam_System()` is a plain C function, declared static like everything else:


```c
static int
PySpam_System(const char *command)
{
    return system(command);
}
```


The function spam_system() is modified in a trivial way:


```c
static PyObject *
spam_system(PyObject *self, PyObject *args)
{
    const char *command;
    int sts;

    if (!PyArg_ParseTuple(args, "s", &command))
        return NULL;
    sts = PySpam_System(command);
    return PyLong_FromLong(sts);
}
```


In the beginning of the module, right after the line


    #include <Python.h>


two more lines must be added:


```c
#define SPAM_MODULE
#include "spammodule.h"
```

The `#define` is used to tell the header file that it is being included in the exporting module, not a client module. Finally, the module’s initialization function must take care of initializing the C API pointer array:


```c
PyMODINIT_FUNC
PyInit_spam(void)
{
    PyObject *m;
    static void *PySpam_API[PySpam_API_pointers];
    PyObject *c_api_object;

    m = PyModule_Create(&spammodule);
    if (m == NULL)
        return NULL;

    /* Initialize the C API pointer array */
    PySpam_API[PySpam_System_NUM] = (void *)PySpam_System;

    /* Create a Capsule containing the API pointer array's address */
    c_api_object = PyCapsule_New((void *)PySpam_API, "spam._C_API", NULL);

    if (PyModule_AddObject(m, "_C_API", c_api_object) < 0) {
        Py_XDECREF(c_api_object);
        Py_DECREF(m);
        return NULL;
    }

    return m;
}
```


Note that `PySpam_API` is declared static; otherwise the pointer array would disappear when `PyInit_spam()` terminates!

The bulk of the work is in the header file spammodule.h, which looks like this:


```c
#ifndef Py_SPAMMODULE_H
#define Py_SPAMMODULE_H
#ifdef __cplusplus
extern "C" {
#endif

/* Header file for spammodule */

/* C API functions */
#define PySpam_System_NUM 0
#define PySpam_System_RETURN int
#define PySpam_System_PROTO (const char *command)

/* Total number of C API pointers */
#define PySpam_API_pointers 1


#ifdef SPAM_MODULE
/* This section is used when compiling spammodule.c */

static PySpam_System_RETURN PySpam_System PySpam_System_PROTO;

#else
/* This section is used in modules that use spammodule's API */

static void **PySpam_API;

#define PySpam_System \
 (*(PySpam_System_RETURN (*)PySpam_System_PROTO) PySpam_API[PySpam_System_NUM])

/* Return -1 on error, 0 on success.
 * PyCapsule_Import will set an exception if there's an error.
 */
static int
import_spam(void)
{
    PySpam_API = (void **)PyCapsule_Import("spam._C_API", 0);
    return (PySpam_API != NULL) ? 0 : -1;
}

#endif

#ifdef __cplusplus
}
#endif

#endif /* !defined(Py_SPAMMODULE_H) */
```


All that a client module must do in order to have access to the function `PySpam_System()` is to call the function (or rather macro) `import_spam()` in its initialization function:


```c
PyMODINIT_FUNC
PyInit_client(void)
{
    PyObject *m;

    m = PyModule_Create(&clientmodule);
    if (m == NULL)
        return NULL;
    if (import_spam() < 0)
        return NULL;
    /* additional initialization can happen here */
    return m;
}
```


The main disadvantage of this approach is that the file `spammodule.h` is rather complicated. However, the basic structure is the same for each function that is exported, so it has to be learned only once.

Finally it should be mentioned that Capsules offer additional functionality, which is especially useful for memory allocation and deallocation of the pointer stored in a Capsule. The details are described in the Python/C API Reference Manual in the section Capsules and in the implementation of Capsules (files `Include/pycapsule.h` and `Objects/pycapsule.c` in the Python source code distribution).

Footnotes

[1] An interface for this function already exists in the standard module os — it was chosen as a simple and straightforward example.

[2] The metaphor of “borrowing” a reference is not completely correct: the owner still has a copy of the reference.

[3] Checking that the reference count is at least 1 does not work — the reference count itself could be in freed memory and may thus be reused for another object!

[4] These guarantees don’t hold when you use the “old” style calling convention — this is still found in much existing code. 


## ==⚡ • 2. Defining Extension Types: Tutorial

Python allows the writer of a C extension module to define new types that can be manipulated from Python code, much like the built-in str and list types. The code for all extension types follows a pattern, but there are some details that you need to understand before you can get started. This document is a gentle introduction to the topic.


### ===🗝 ◦ 2.1. The Basics

The CPython runtime sees all Python objects as variables of type `PyObject*`, which serves as a “base type” for all Python objects. The PyObject structure itself only contains the object’s reference count and a pointer to the object’s “type object”. This is where the action is; the type object determines which (C) functions get called by the interpreter when, for instance, an attribute gets looked up on an object, a method called, or it is multiplied by another object. These C functions are called “`type methods`”.

So, if you want to define a new extension type, you need to create a new type object.

This sort of thing can only be explained by example, so here’s a minimal, but complete, module that defines a new type named Custom inside a C extension module custom:

Note:
 What we’re showing here is the traditional way of defining static extension types. It should be adequate for most uses. The C API also allows defining heap-allocated extension types using the `PyType_FromSpec()` function, which isn’t covered in this tutorial.


```c
#define PY_SSIZE_T_CLEAN
#include <Python.h>

typedef struct {
    PyObject_HEAD // PyObject ob_base;
    /* Type-specific fields go here. */
} CustomObject;

static PyTypeObject CustomType = {
    PyVarObject_HEAD_INIT(NULL, 0)
    .tp_name = "custom.Custom",
    .tp_doc = "Custom objects",
    .tp_basicsize = sizeof(CustomObject),
    .tp_itemsize = 0,
    .tp_flags = Py_TPFLAGS_DEFAULT,
    .tp_new = PyType_GenericNew,
};

static PyModuleDef custommodule = {
    PyModuleDef_HEAD_INIT,
    .m_name = "custom",
    .m_doc = "Example module that creates an extension type.",
    .m_size = -1,
};

PyMODINIT_FUNC
PyInit_custom(void)
{
    PyObject *m;
    if (PyType_Ready(&CustomType) < 0)
        return NULL;

    m = PyModule_Create(&custommodule);
    if (m == NULL)
        return NULL;

    Py_INCREF(&CustomType);
    if (PyModule_AddObject(m, "Custom", (PyObject *) &CustomType) < 0) {
        Py_DECREF(&CustomType);
        Py_DECREF(m);
        return NULL;
    }

    return m;
}
```


Now that’s quite a bit to take in at once, but hopefully bits will seem familiar from the previous chapter. This file defines three things:

(1). What a Custom object contains: this is the `CustomObject` struct, which is allocated once for each Custom instance.

(2). How the Custom type behaves: this is the `CustomType` struct, which defines a set of flags and function pointers that the interpreter inspects when specific operations are requested.

(3). How to initialize the custom module: this is the `PyInit_custom` function and the associated custommodule struct.


The first bit is:


```c
typedef struct {
    PyObject_HEAD // PyObject ob_base;
} CustomObject;
```


This is what a Custom object will contain. `PyObject_HEAD` is mandatory at the start of each object struct and defines a field called `ob_base` of type `PyObject`, containing a pointer to a type object and a reference count (these can be accessed using the macros `Py_REFCNT` and `Py_TYPE` respectively). The reason for the macro is to abstract away the layout and to enable additional fields in debug builds.

Note:
 There is no semicolon above after the `PyObject_HEAD` macro. Be wary of adding one by accident: some compilers will complain.
 

Of course, objects generally store additional data besides the standard `PyObject_HEAD` boilerplate; for example, here is the definition for standard Python floats:


```c
typedef struct {
    PyObject_HEAD
    double ob_fval;
} PyFloatObject;
```


The second bit is the definition of the type object.


```c
static PyTypeObject CustomType = {
    PyVarObject_HEAD_INIT(NULL, 0)
    .tp_name = "custom.Custom",
    .tp_doc = "Custom objects",
    .tp_basicsize = sizeof(CustomObject),
    .tp_itemsize = 0,
    .tp_flags = Py_TPFLAGS_DEFAULT,
    .tp_new = PyType_GenericNew,
};
```


Note:
 We recommend using C99-style designated initializers as above, to avoid listing all the `PyTypeObject` fields that you don’t care about and also to avoid caring about the fields’ declaration order.
 

The actual definition of `PyTypeObject` in `object.h` has many more fields than the definition above. The remaining fields will be filled with zeros by the C compiler, and it’s common practice to not specify them explicitly unless you need them.

We’re going to pick it apart, one field at a time:


    PyVarObject_HEAD_INIT(NULL, 0) // PyVarObject initializer


This line is mandatory boilerplate to initialize the `ob_base` field mentioned above.


    .tp_name = "custom.Custom",


The name of our type. This will appear in the default textual representation of our objects and in some error messages, for example:


>>> "" + custom.Custom()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: can only concatenate str (not "custom.Custom") to str


Note that the name is a `dotted name` that includes both the module name and the name of the type within the module. The module in this case is custom and the type is Custom, so we set the type name to `custom.Custom`. Using the real dotted import path is important to make your type compatible with the `pydoc` and `pickle` modules.


    .tp_basicsize = sizeof(CustomObject),
    .tp_itemsize = 0,


This is so that Python knows how much memory to allocate when creating new Custom instances. `tp_itemsize` is only used for variable-sized objects and should otherwise be zero.

Note:
 If you want your type to be subclassable from Python, and your type has the same `tp_basicsize` as its base type, you may have problems with multiple inheritance. A Python subclass of your type will have to list your type first in its __bases__, or else it will not be able to call your type’s __new__() method without getting an error. You can avoid this problem by ensuring that your type has a larger value for `tp_basicsize` than its base type does. Most of the time, this will be true anyway, because either your base type will be object, or else you will be adding data members to your base type, and therefore increasing its size.
 

We set the class flags to `Py_TPFLAGS_DEFAULT`.


    .tp_flags = Py_TPFLAGS_DEFAULT,


All types should include this constant in their flags. It enables all of the members defined until at least Python 3.3. If you need further members, you will need to OR the corresponding flags.

We provide a doc string for the type in `tp_doc`.


    .tp_doc = "Custom objects",


To enable object creation, we have to provide a tp_new handler. This is the equivalent of the Python method __new__(), but has to be specified explicitly. In this case, we can just use the default implementation provided by the API function `PyType_GenericNew()`.


    .tp_new = PyType_GenericNew,


Everything else in the file should be familiar, except for some code in PyInit_custom():


    if (PyType_Ready(&CustomType) < 0)
        return;


This initializes the Custom type, filling in a number of members to the appropriate default values, including ob_type that we initially set to NULL.


```c
Py_INCREF(&CustomType);
if (PyModule_AddObject(m, "Custom", (PyObject *) &CustomType) < 0) {
    Py_DECREF(&CustomType);
    Py_DECREF(m);
    return NULL;
}
```


This adds the type to the module dictionary. This allows us to create Custom instances by calling the Custom class:


>>> import custom
>>> mycustom = custom.Custom()


That’s it! All that remains is to build it; put the above code in a file called custom.c and:


```py
from distutils.core import setup, Extension
setup(name="custom", version="1.0",
      ext_modules=[Extension("custom", ["custom.c"])])
```


in a file called setup.py; then typing


    $ python setup.py build


at a shell should produce a file custom.so in a subdirectory; move to that directory and fire up Python — you should be able to import custom and play around with Custom objects.

That wasn’t so hard, was it?

Of course, the current Custom type is pretty uninteresting. It has no data and doesn’t do anything. It can’t even be subclassed.

Note:
 While this documentation showcases the standard distutils module for building C extensions, it is recommended in real-world use cases to use the newer and better-maintained setuptools library. Documentation on how to do this is out of scope for this document and can be found in the Python Packaging User’s Guide.
 
### ===🗝 ◦ 2.2. Adding data and methods to the Basic example


Let's extend the basic example to add some data and methods.  Let's also make
the type usable as a base class. We'll create a new module, :mod:`custom2` that
adds these capabilities:

.. literalinclude:: ../includes/custom2.c


This version of the module has a number of changes.

We've added an extra include::

   #include <structmember.h>

This include provides declarations that we use to handle attributes, as
described a bit later.

The  :class:`Custom` type now has three data attributes in its C struct,
*first*, *last*, and *number*.  The *first* and *last* variables are Python
strings containing first and last names.  The *number* attribute is a C integer.

The object structure is updated accordingly::

```c
   typedef struct {
       PyObject_HEAD
       PyObject *first; /* first name */
       PyObject *last;  /* last name */
       int number;
   } CustomObject;
```

Because we now have data to manage, we have to be more careful about object
allocation and deallocation.  At a minimum, we need a deallocation method::

```c
   static void
   Custom_dealloc(CustomObject *self)
   {
       Py_XDECREF(self->first);
       Py_XDECREF(self->last);
       Py_TYPE(self)->tp_free((PyObject *) self);
   }
```

which is assigned to the :c:member:`~PyTypeObject.tp_dealloc` member::

   .tp_dealloc = (destructor) Custom_dealloc,

This method first clears the reference counts of the two Python attributes.
:c:func:`Py_XDECREF` correctly handles the case where its argument is
``NULL`` (which might happen here if ``tp_new`` failed midway).  It then
calls the :c:member:`~PyTypeObject.tp_free` member of the object's type
(computed by ``Py_TYPE(self)``) to free the object's memory.  Note that
the object's type might not be :class:`CustomType`, because the object may
be an instance of a subclass.

.. note::
   The explicit cast to ``destructor`` above is needed because we defined
   ``Custom_dealloc`` to take a ``CustomObject *`` argument, but the ``tp_dealloc``
   function pointer expects to receive a ``PyObject *`` argument.  Otherwise,
   the compiler will emit a warning.  This is object-oriented polymorphism,
   in C!

We want to make sure that the first and last names are initialized to empty
strings, so we provide a ``tp_new`` implementation::

```c
   static PyObject *
   Custom_new(PyTypeObject *type, PyObject *args, PyObject *kwds)
   {
       CustomObject *self;
       self = (CustomObject *) type->tp_alloc(type, 0);
       if (self != NULL) {
           self->first = PyUnicode_FromString("");
           if (self->first == NULL) {
               Py_DECREF(self);
               return NULL;
           }
           self->last = PyUnicode_FromString("");
           if (self->last == NULL) {
               Py_DECREF(self);
               return NULL;
           }
           self->number = 0;
       }
       return (PyObject *) self;
   }
```

and install it in the :c:member:`~PyTypeObject.tp_new` member::

   .tp_new = Custom_new,

The ``tp_new`` handler is responsible for creating (as opposed to initializing)
objects of the type.  It is exposed in Python as the :meth:`__new__` method.
It is not required to define a ``tp_new`` member, and indeed many extension
types will simply reuse :c:func:`PyType_GenericNew` as done in the first
version of the ``Custom`` type above.  In this case, we use the ``tp_new``
handler to initialize the ``first`` and ``last`` attributes to non-``NULL``
default values.

``tp_new`` is passed the type being instantiated (not necessarily ``CustomType``,
if a subclass is instantiated) and any arguments passed when the type was
called, and is expected to return the instance created.  ``tp_new`` handlers
always accept positional and keyword arguments, but they often ignore the
arguments, leaving the argument handling to initializer (a.k.a. ``tp_init``
in C or ``__init__`` in Python) methods.

.. note::
   ``tp_new`` shouldn't call ``tp_init`` explicitly, as the interpreter
   will do it itself.

The ``tp_new`` implementation calls the :c:member:`~PyTypeObject.tp_alloc`
slot to allocate memory::

   self = (CustomObject *) type->tp_alloc(type, 0);

Since memory allocation may fail, we must check the :c:member:`~PyTypeObject.tp_alloc`
result against ``NULL`` before proceeding.

.. note::
   We didn't fill the :c:member:`~PyTypeObject.tp_alloc` slot ourselves. Rather
   :c:func:`PyType_Ready` fills it for us by inheriting it from our base class,
   which is :class:`object` by default.  Most types use the default allocation
   strategy.

.. note::
   If you are creating a co-operative :c:member:`~PyTypeObject.tp_new` (one
   that calls a base type's :c:member:`~PyTypeObject.tp_new` or :meth:`__new__`),
   you must *not* try to determine what method to call using method resolution
   order at runtime.  Always statically determine what type you are going to
   call, and call its :c:member:`~PyTypeObject.tp_new` directly, or via
   ``type->tp_base->tp_new``.  If you do not do this, Python subclasses of your
   type that also inherit from other Python-defined classes may not work correctly.
   (Specifically, you may not be able to create instances of such subclasses
   without getting a :exc:`TypeError`.)

We also define an initialization function which accepts arguments to provide
initial values for our instance::

```c
   static int
   Custom_init(CustomObject *self, PyObject *args, PyObject *kwds)
   {
       static char *kwlist[] = {"first", "last", "number", NULL};
       PyObject *first = NULL, *last = NULL, *tmp;

       if (!PyArg_ParseTupleAndKeywords(args, kwds, "|OOi", kwlist,
                                        &first, &last,
                                        &self->number))
           return -1;

       if (first) {
           tmp = self->first;
           Py_INCREF(first);
           self->first = first;
           Py_XDECREF(tmp);
       }
       if (last) {
           tmp = self->last;
           Py_INCREF(last);
           self->last = last;
           Py_XDECREF(tmp);
       }
       return 0;
   }
```

by filling the :c:member:`~PyTypeObject.tp_init` slot. ::

   .tp_init = (initproc) Custom_init,

The :c:member:`~PyTypeObject.tp_init` slot is exposed in Python as the
:meth:`__init__` method.  It is used to initialize an object after it's
created.  Initializers always accept positional and keyword arguments,
and they should return either ``0`` on success or ``-1`` on error.

Unlike the ``tp_new`` handler, there is no guarantee that ``tp_init``
is called at all (for example, the :mod:`pickle` module by default
doesn't call :meth:`__init__` on unpickled instances).  It can also be
called multiple times.  Anyone can call the :meth:`__init__` method on
our objects.  For this reason, we have to be extra careful when assigning
the new attribute values.  We might be tempted, for example to assign the
``first`` member like this::

```c
   if (first) {
       Py_XDECREF(self->first);
       Py_INCREF(first);
       self->first = first;
   }
```

But this would be risky.  Our type doesn't restrict the type of the
``first`` member, so it could be any kind of object.  It could have a
destructor that causes code to be executed that tries to access the
``first`` member; or that destructor could release the
:term:`Global interpreter Lock <GIL>` and let arbitrary code run in other
threads that accesses and modifies our object.

To be paranoid and protect ourselves against this possibility, we almost
always reassign members before decrementing their reference counts.  When
don't we have to do this?

* when we absolutely know that the reference count is greater than 1;

* when we know that deallocation of the object [#]_ will neither release
  the :term:`GIL` nor cause any calls back into our type's code;

* when decrementing a reference count in a :c:member:`~PyTypeObject.tp_dealloc`
  handler on a type which doesn't support cyclic garbage collection [#]_.

We want to expose our instance variables as attributes. There are a
number of ways to do that. The simplest way is to define member definitions::

```c
   static PyMemberDef Custom_members[] = {
       {"first", T_OBJECT_EX, offsetof(CustomObject, first), 0,
        "first name"},
       {"last", T_OBJECT_EX, offsetof(CustomObject, last), 0,
        "last name"},
       {"number", T_INT, offsetof(CustomObject, number), 0,
        "custom number"},
       {NULL}  /* Sentinel */
   };
```

and put the definitions in the :c:member:`~PyTypeObject.tp_members` slot::

   .tp_members = Custom_members,

Each member definition has a member name, type, offset, access flags and
documentation string.  See the :ref:`Generic-Attribute-Management` section
below for details.

A disadvantage of this approach is that it doesn't provide a way to restrict the
types of objects that can be assigned to the Python attributes.  We expect the
first and last names to be strings, but any Python objects can be assigned.
Further, the attributes can be deleted, setting the C pointers to ``NULL``.  Even
though we can make sure the members are initialized to non-``NULL`` values, the
members can be set to ``NULL`` if the attributes are deleted.

We define a single method, :meth:`Custom.name()`, that outputs the objects name as the
concatenation of the first and last names. ::

```c
   static PyObject *
   Custom_name(CustomObject *self, PyObject *Py_UNUSED(ignored))
   {
       if (self->first == NULL) {
           PyErr_SetString(PyExc_AttributeError, "first");
           return NULL;
       }
       if (self->last == NULL) {
           PyErr_SetString(PyExc_AttributeError, "last");
           return NULL;
       }
       return PyUnicode_FromFormat("%S %S", self->first, self->last);
   }
```

The method is implemented as a C function that takes a :class:`Custom` (or
:class:`Custom` subclass) instance as the first argument.  Methods always take an
instance as the first argument. Methods often take positional and keyword
arguments as well, but in this case we don't take any and don't need to accept
a positional argument tuple or keyword argument dictionary. This method is
equivalent to the Python method:

.. code-block:: python

   def name(self):
       return "%s %s" % (self.first, self.last)

Note that we have to check for the possibility that our :attr:`first` and
:attr:`last` members are ``NULL``.  This is because they can be deleted, in which
case they are set to ``NULL``.  It would be better to prevent deletion of these
attributes and to restrict the attribute values to be strings.  We'll see how to
do that in the next section.

Now that we've defined the method, we need to create an array of method
definitions::

```c
   static PyMethodDef Custom_methods[] = {
       {"name", (PyCFunction) Custom_name, METH_NOARGS,
        "Return the name, combining the first and last name"
       },
       {NULL}  /* Sentinel */
   };
```

(note that we used the :const:`METH_NOARGS` flag to indicate that the method
is expecting no arguments other than *self*)

and assign it to the :c:member:`~PyTypeObject.tp_methods` slot::

   .tp_methods = Custom_methods,

Finally, we'll make our type usable as a base class for subclassing.  We've
written our methods carefully so far so that they don't make any assumptions
about the type of the object being created or used, so all we need to do is
to add the :const:`Py_TPFLAGS_BASETYPE` to our class flag definition::

   .tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE,

We rename :c:func:`PyInit_custom` to :c:func:`PyInit_custom2`, update the
module name in the :c:type:`PyModuleDef` struct, and update the full class
name in the :c:type:`PyTypeObject` struct.

Finally, we update our :file:`setup.py` file to build the new module:

.. code-block:: python

```py
   from distutils.core import setup, Extension
   setup(name="custom", version="1.0",
         ext_modules=[
            Extension("custom", ["custom.c"]),
            Extension("custom2", ["custom2.c"]),
            ])
```


### ===🗝 ◦ 2.3. Providing finer control over data attributes

In this section, we'll provide finer control over how the :attr:`first` and
:attr:`last` attributes are set in the :class:`Custom` example. In the previous
version of our module, the instance variables :attr:`first` and :attr:`last`
could be set to non-string values or even deleted. We want to make sure that
these attributes always contain strings.

.. literalinclude:: ../includes/custom3.c


To provide greater control, over the :attr:`first` and :attr:`last` attributes,
we'll use custom getter and setter functions.  Here are the functions for
getting and setting the :attr:`first` attribute::

```c
   static PyObject *
   Custom_getfirst(CustomObject *self, void *closure)
   {
       Py_INCREF(self->first);
       return self->first;
   }

   static int
   Custom_setfirst(CustomObject *self, PyObject *value, void *closure)
   {
       PyObject *tmp;
       if (value == NULL) {
           PyErr_SetString(PyExc_TypeError, "Cannot delete the first attribute");
           return -1;
       }
       if (!PyUnicode_Check(value)) {
           PyErr_SetString(PyExc_TypeError,
                           "The first attribute value must be a string");
           return -1;
       }
       tmp = self->first;
       Py_INCREF(value);
       self->first = value;
       Py_DECREF(tmp);
       return 0;
   }
```

The getter function is passed a :class:`Custom` object and a "closure", which is
a void pointer.  In this case, the closure is ignored.  (The closure supports an
advanced usage in which definition data is passed to the getter and setter. This
could, for example, be used to allow a single set of getter and setter functions
that decide the attribute to get or set based on data in the closure.)

The setter function is passed the :class:`Custom` object, the new value, and the
closure.  The new value may be ``NULL``, in which case the attribute is being
deleted.  In our setter, we raise an error if the attribute is deleted or if its
new value is not a string.

We create an array of :c:type:`PyGetSetDef` structures::

```c
   static PyGetSetDef Custom_getsetters[] = {
       {"first", (getter) Custom_getfirst, (setter) Custom_setfirst,
        "first name", NULL},
       {"last", (getter) Custom_getlast, (setter) Custom_setlast,
        "last name", NULL},
       {NULL}  /* Sentinel */
   };
```

and register it in the :c:member:`~PyTypeObject.tp_getset` slot::

   .tp_getset = Custom_getsetters,

The last item in a :c:type:`PyGetSetDef` structure is the "closure" mentioned
above.  In this case, we aren't using a closure, so we just pass ``NULL``.

We also remove the member definitions for these attributes::

```c
   static PyMemberDef Custom_members[] = {
       {"number", T_INT, offsetof(CustomObject, number), 0,
        "custom number"},
       {NULL}  /* Sentinel */
   };
```

We also need to update the :c:member:`~PyTypeObject.tp_init` handler to only
allow strings [#]_ to be passed::

```c
   static int
   Custom_init(CustomObject *self, PyObject *args, PyObject *kwds)
   {
       static char *kwlist[] = {"first", "last", "number", NULL};
       PyObject *first = NULL, *last = NULL, *tmp;

       if (!PyArg_ParseTupleAndKeywords(args, kwds, "|UUi", kwlist,
                                        &first, &last,
                                        &self->number))
           return -1;

       if (first) {
           tmp = self->first;
           Py_INCREF(first);
           self->first = first;
           Py_DECREF(tmp);
       }
       if (last) {
           tmp = self->last;
           Py_INCREF(last);
           self->last = last;
           Py_DECREF(tmp);
       }
       return 0;
   }
```

With these changes, we can assure that the ``first`` and ``last`` members are
never ``NULL`` so we can remove checks for ``NULL`` values in almost all cases.
This means that most of the :c:func:`Py_XDECREF` calls can be converted to
:c:func:`Py_DECREF` calls.  The only place we can't change these calls is in
the ``tp_dealloc`` implementation, where there is the possibility that the
initialization of these members failed in ``tp_new``.

We also rename the module initialization function and module name in the
initialization function, as we did before, and we add an extra definition to the
:file:`setup.py` file.

### ===🗝 ◦ 2.4. Supporting cyclic garbage collection

Python has a :term:`cyclic garbage collector (GC) <garbage collection>` that
can identify unneeded objects even when their reference counts are not zero.
This can happen when objects are involved in cycles.  For example, consider:

.. code-block:: pycon

   >>> l = []
   >>> l.append(l)
   >>> del l

In this example, we create a list that contains itself. When we delete it, it
still has a reference from itself. Its reference count doesn't drop to zero.
Fortunately, Python's cyclic garbage collector will eventually figure out that
the list is garbage and free it.

In the second version of the `Custom` example, we allowed any kind of
object to be stored in the :attr:`first` or :attr:`last` attributes [#]_.
Besides, in the second and third versions, we allowed subclassing
`Custom`, and subclasses may add arbitrary attributes.  For any of
those two reasons, `Custom` objects can participate in cycles:

.. code-block:: pycon

   >>> import custom3
   >>> class Derived(custom3.Custom): pass
   ...
   >>> n = Derived()
   >>> n.some_attribute = n

To allow a `Custom` instance participating in a reference cycle to
be properly detected and collected by the cyclic GC, our `Custom` type
needs to fill two additional slots and to enable a flag that enables these slots:

.. literalinclude:: ../includes/custom4.c


First, the traversal method lets the cyclic GC know about subobjects that could
participate in cycles::

```c
   static int
   Custom_traverse(CustomObject *self, visitproc visit, void *arg)
   {
       int vret;
       if (self->first) {
           vret = visit(self->first, arg);
           if (vret != 0)
               return vret;
       }
       if (self->last) {
           vret = visit(self->last, arg);
           if (vret != 0)
               return vret;
       }
       return 0;
   }
```

For each subobject that can participate in cycles, we need to call the
`visit` function, which is passed to the traversal method. The
`visit` function takes as arguments the subobject and the extra argument
*arg* passed to the traversal method.  It returns an integer value that must be
returned if it is non-zero.

Python provides a `Py_VISIT` macro that automates calling visit
functions.  With `Py_VISIT`, we can minimize the amount of boilerplate
in ``Custom_traverse``::

```c
   static int
   Custom_traverse(CustomObject *self, visitproc visit, void *arg)
   {
       Py_VISIT(self->first);
       Py_VISIT(self->last);
       return 0;
   }
```

.. note::
   The `~PyTypeObject.tp_traverse` implementation must name its
   arguments exactly *visit* and *arg* in order to use `Py_VISIT`.

Second, we need to provide a method for clearing any subobjects that can
participate in cycles::

```c
   static int
   Custom_clear(CustomObject *self)
   {
       Py_CLEAR(self->first);
       Py_CLEAR(self->last);
       return 0;
   }
```

Notice the use of the `Py_CLEAR` macro.  It is the recommended and safe
way to clear data attributes of arbitrary types while decrementing
their reference counts.  If you were to call `Py_XDECREF` instead
on the attribute before setting it to ``NULL``, there is a possibility
that the attribute's destructor would call back into code that reads the
attribute again (*especially* if there is a reference cycle).

.. note::
   You could emulate `Py_CLEAR` by writing::

      PyObject *tmp;
      tmp = self->first;
      self->first = NULL;
      Py_XDECREF(tmp);

   Nevertheless, it is much easier and less error-prone to always
   use `Py_CLEAR` when deleting an attribute.  Don't
   try to micro-optimize at the expense of robustness!

The deallocator ``Custom_dealloc`` may call arbitrary code when clearing
attributes.  It means the circular GC can be triggered inside the function.
Since the GC assumes reference count is not zero, we need to untrack the object
from the GC by calling `PyObject_GC_UnTrack` before clearing members.
Here is our reimplemented deallocator using `PyObject_GC_UnTrack`
and ``Custom_clear``::

```c
   static void
   Custom_dealloc(CustomObject *self)
   {
       PyObject_GC_UnTrack(self);
       Custom_clear(self);
       Py_TYPE(self)->tp_free((PyObject *) self);
   }
```

Finally, we add the :const:`Py_TPFLAGS_HAVE_GC` flag to the class flags::

   .tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_HAVE_GC,

That's pretty much it.  If we had written custom `~PyTypeObject.tp_alloc` or
`~PyTypeObject.tp_free` handlers, we'd need to modify them for cyclic
garbage collection.  Most extensions will use the versions automatically provided.

### ===🗝 ◦ 2.5. Subclassing other types

It is possible to create new extension types that are derived from existing
types. It is easiest to inherit from the built in types, since an extension can
easily use the `PyTypeObject` it needs. It can be difficult to share
these `PyTypeObject` structures between extension modules.

In this example we will create a `SubList` type that inherits from the
built-in `list` type. The new type will be completely compatible with
regular lists, but will have an additional :meth:`increment` method that
increases an internal counter:

.. code-block:: pycon

   >>> import sublist
   >>> s = sublist.SubList(range(3))
   >>> s.extend(s)
   >>> print(len(s))
   6
   >>> print(s.increment())
   1
   >>> print(s.increment())
   2

.. literalinclude:: ../includes/sublist.c


As you can see, the source code closely resembles the `Custom` examples in
previous sections. We will break down the main differences between them. ::

```c
   typedef struct {
       PyListObject list;
       int state;
   } SubListObject;
```

The primary difference for derived type objects is that the base type's
object structure must be the first value.  The base type will already include
the `PyObject_HEAD` at the beginning of its structure.

When a Python object is a `SubList` instance, its ``PyObject *`` pointer
can be safely cast to both ``PyListObject *`` and ``SubListObject *``::

```c
   static int
   SubList_init(SubListObject *self, PyObject *args, PyObject *kwds)
   {
       if (PyList_Type.tp_init((PyObject *) self, args, kwds) < 0)
           return -1;
       self->state = 0;
       return 0;
   }
```

We see above how to call through to the :attr:`__init__` method of the base
type.

This pattern is important when writing a type with custom
`~PyTypeObject.tp_new` and `~PyTypeObject.tp_dealloc`
members.  The `~PyTypeObject.tp_new` handler should not actually
create the memory for the object with its `~PyTypeObject.tp_alloc`,
but let the base class handle it by calling its own `~PyTypeObject.tp_new`.

The `PyTypeObject` struct supports a `~PyTypeObject.tp_base`
specifying the type's concrete base class.  Due to cross-platform compiler
issues, you can't fill that field directly with a reference to
`PyList_Type`; it should be done later in the module initialization
function::

```c
   PyMODINIT_FUNC
   PyInit_sublist(void)
   {
       PyObject* m;
       SubListType.tp_base = &PyList_Type;
       if (PyType_Ready(&SubListType) < 0)
           return NULL;

       m = PyModule_Create(&sublistmodule);
       if (m == NULL)
           return NULL;

       Py_INCREF(&SubListType);
       if (PyModule_AddObject(m, "SubList", (PyObject *) &SubListType) < 0) {
           Py_DECREF(&SubListType);
           Py_DECREF(m);
           return NULL;
       }

       return m;
   }
```

Before calling `PyType_Ready`, the type structure must have the
`~PyTypeObject.tp_base` slot filled in.  When we are deriving an
existing type, it is not necessary to fill out the `~PyTypeObject.tp_alloc`
slot with `PyType_GenericNew` -- the allocation function from the base
type will be inherited.

After that, calling `PyType_Ready` and adding the type object to the
module is the same as with the basic `Custom` examples.


.. rubric:: Footnotes

.. [#] This is true when we know that the object is a basic type, like a string or a
   float.

.. [#] We relied on this in the `~PyTypeObject.tp_dealloc` handler
   in this example, because our type doesn't support garbage collection.

.. [#] We now know that the first and last members are strings, so perhaps we
   could be less careful about decrementing their reference counts, however,
   we accept instances of string subclasses.  Even though deallocating normal
   strings won't call back into our objects, we can't guarantee that deallocating
   an instance of a string subclass won't call back into our objects.

.. [#] Also, even with our attributes restricted to strings instances, the user
   could pass arbitrary `str` subclasses and therefore still create
   reference cycles.
## ==⚡ • 3. Defining Extension Types: Assorted Topics

This section aims to give a quick fly-by on the various type methods you can implement and what they do.

Here is the definition of `PyTypeObject`, with some fields only used in debug builds omitted:


```c
typedef struct _typeobject {
    PyObject_VAR_HEAD
    const char *tp_name; /* For printing, in format "<module>.<name>" */
    Py_ssize_t tp_basicsize, tp_itemsize; /* For allocation */

    /* Methods to implement standard operations */

    destructor      tp_dealloc;
    Py_ssize_t      tp_vectorcall_offset;
    getattrfunc     tp_getattr;
    setattrfunc     tp_setattr;
    PyAsyncMethods *tp_as_async; /* formerly known as tp_compare (Python 2)
                                    or tp_reserved (Python 3) */
    reprfunc tp_repr;

    /* Method suites for standard classes */

    PyNumberMethods   *tp_as_number;
    PySequenceMethods *tp_as_sequence;
    PyMappingMethods  *tp_as_mapping;

    /* More standard operations (here for binary compatibility) */

    hashfunc     tp_hash;
    ternaryfunc  tp_call;
    reprfunc     tp_str;
    getattrofunc tp_getattro;
    setattrofunc tp_setattro;

    /* Functions to access object as input/output buffer */
    PyBufferProcs *tp_as_buffer;

    /* Flags to define presence of optional/expanded features */
    unsigned long tp_flags;

    const char  *tp_doc; /* Documentation string */

    /* Assigned meaning in release 2.0 */
    /* call function for all accessible objects */
    traverseproc tp_traverse;

    /* delete references to contained objects */
    inquiry      tp_clear;

    /* Assigned meaning in release 2.1 */
    /* rich comparisons */
    richcmpfunc  tp_richcompare;

    /* weak reference enabler */
    Py_ssize_t   tp_weaklistoffset;

    /* Iterators */
    getiterfunc  tp_iter;
    iternextfunc tp_iternext;

    /* Attribute descriptor and subclassing stuff */
    struct PyMethodDef *tp_methods;
    struct PyMemberDef *tp_members;
    struct PyGetSetDef *tp_getset;
    // Strong reference on a heap type, borrowed reference on a static type
    struct _typeobject *tp_base;
    PyObject    *tp_dict;
    descrgetfunc tp_descr_get;
    descrsetfunc tp_descr_set;
    Py_ssize_t   tp_dictoffset;
    initproc     tp_init;
    allocfunc    tp_alloc;
    newfunc      tp_new;
    freefunc     tp_free; /* Low-level free-memory routine */
    inquiry      tp_is_gc; /* For PyObject_IS_GC */
    PyObject    *tp_bases;
    PyObject    *tp_mro; /* method resolution order */
    PyObject    *tp_cache;
    PyObject    *tp_subclasses;
    PyObject    *tp_weaklist;
    destructor   tp_del;

    /* Type attribute cache version tag. Added in version 2.6 */
    unsigned int tp_version_tag;

    destructor   tp_finalize;
    vectorcallfunc tp_vectorcall;
} PyTypeObject;
```


Now that’s a lot of methods. Don’t worry too much though – if you have a type you want to define, the chances are very good that you will only implement a handful of these.

As you probably expect by now, we’re going to go over this and give more information about the various handlers. We won’t go in the order they are defined in the structure, because there is a lot of historical baggage that impacts the ordering of the fields. It’s often easiest to find an example that includes the fields you need and then change the values to suit your new type.


    const char *tp_name; /* For printing */


The name of the type – as mentioned in the previous chapter, this will appear in various places, almost entirely for diagnostic purposes. Try to choose something that will be helpful in such a situation!


    Py_ssize_t tp_basicsize, tp_itemsize; /* For allocation */


These fields tell the runtime how much memory to allocate when new objects of this type are created. Python has some built-in support for variable length structures (think: strings, tuples) which is where the tp_itemsize field comes in. This will be dealt with later.


    const char *tp_doc;


Here you can put a string (or its address) that you want returned when the Python script references obj.__doc__ to retrieve the doc string.

Now we come to the basic type methods – the ones most extension types will implement.


### ===🗝 ◦ 3.1. Finalization and De-allocation

    destructor tp_dealloc;
    destructor tp_del;
    destructor tp_finalize;

This `tp_dealloc` function is called when the reference count of the instance of your type is reduced to zero and the Python interpreter wants to reclaim it. If your type has memory to free or other clean-up to perform, you can put it here. The object itself needs to be freed here as well. Here is an example of this function:


```c
static void
newdatatype_dealloc(newdatatypeobject *obj)
{
    free(obj->obj_UnderlyingDatatypePtr);
    Py_TYPE(obj)->tp_free((PyObject *)obj);
}
```


If your type supports garbage collection, the destructor should call `PyObject_GC_UnTrack()` before clearing any member fields:


```c
static void
newdatatype_dealloc(newdatatypeobject *obj)
{
    PyObject_GC_UnTrack(obj);
    Py_CLEAR(obj->other_obj);
    ...
    Py_TYPE(obj)->tp_free((PyObject *)obj);
}
```


One important requirement of the deallocator function is that it leaves any pending exceptions alone. This is important since deallocators are frequently called as the interpreter unwinds the Python stack; when the stack is unwound due to an exception (rather than normal returns), nothing is done to protect the deallocators from seeing that an exception has already been set. Any actions which a deallocator performs which may cause additional Python code to be executed may detect that an exception has been set. This can lead to misleading errors from the interpreter. The proper way to protect against this is to save a pending exception before performing the unsafe action, and restoring it when done. This can be done using the `PyErr_Fetch()` and `PyErr_Restore()` functions:


```c
static void
my_dealloc(PyObject *obj)
{
    MyObject *self = (MyObject *) obj;
    PyObject *cbresult;

    if (self->my_callback != NULL) {
        PyObject *err_type, *err_value, *err_traceback;

        /* This saves the current exception state */
        PyErr_Fetch(&err_type, &err_value, &err_traceback);

        cbresult = PyObject_CallNoArgs(self->my_callback);
        if (cbresult == NULL)
            PyErr_WriteUnraisable(self->my_callback);
        else
            Py_DECREF(cbresult);

        /* This restores the saved exception state */
        PyErr_Restore(err_type, err_value, err_traceback);

        Py_DECREF(self->my_callback);
    }
    Py_TYPE(obj)->tp_free((PyObject*)self);
}
```


Note:
 There are limitations to what you can safely do in a deallocator function. First, if your type supports garbage collection (using `tp_traverse` and/or `tp_clear`), some of the object’s members can have been cleared or finalized by the time `tp_dealloc` is called. Second, in `tp_dealloc`, your object is in an unstable state: its reference count is equal to zero. Any call to a non-trivial object or API (as in the example above) might end up calling `tp_dealloc` again, causing a double free and a crash.
 
Starting with Python 3.4, it is recommended not to put any complex finalization code in `tp_dealloc`, and instead use the new `tp_finalize` type method.

See also:
 PEP 442 explains the new finalization scheme. https://www.python.org/dev/peps/pep-0442
 

### ===🗝 ◦ 3.2. Object Presentation

In Python, there are two ways to generate a textual representation of an object: the `repr()` function, and the `str()` function. (The `print()` function just calls `str()`.) These handlers are both optional.


    reprfunc tp_repr;
    reprfunc tp_str;


The tp_repr handler should return a string object containing a representation of the instance for which it is called. Here is a simple example:


```c
static PyObject *
newdatatype_repr(newdatatypeobject * obj)
{
    return PyUnicode_FromFormat("Repr-ified_newdatatype{{size:%d}}",
                                obj->obj_UnderlyingDatatypePtr->size);
}
```


If no tp_repr handler is specified, the interpreter will supply a representation that uses the type’s tp_name and a uniquely-identifying value for the object.

The `tp_str` handler is to `str()` what the tp_repr handler described above is to `repr()`; that is, it is called when Python code calls `str()` on an instance of your object. Its implementation is very similar to the tp_repr function, but the resulting string is intended for human consumption. If `tp_str` is not specified, the tp_repr handler is used instead.

Here is a simple example:


```c
static PyObject *
newdatatype_str(newdatatypeobject * obj)
{
    return PyUnicode_FromFormat("Stringified_newdatatype{{size:%d}}",
                                obj->obj_UnderlyingDatatypePtr->size);
}
```

### ===🗝 ◦ 3.3. Attribute Management

For every object which can support attributes, the corresponding type must provide the functions that control how the attributes are resolved. There needs to be a function which can retrieve attributes (if any are defined), and another to set attributes (if setting attributes is allowed). Removing an attribute is a special case, for which the new value passed to the handler is NULL.

Python supports two pairs of attribute handlers; a type that supports attributes only needs to implement the functions for one pair. The difference is that one pair takes the name of the attribute as a `char*`, while the other accepts a `PyObject*`. Each type can use whichever pair makes more sense for the implementation’s convenience.


```c
getattrfunc  tp_getattr;        /* char * version */
setattrfunc  tp_setattr;
/* ... */
getattrofunc tp_getattro;       /* PyObject * version */
setattrofunc tp_setattro;
```

If accessing attributes of an object is always a simple operation (this will be explained shortly), there are generic implementations which can be used to provide the `PyObject*` version of the attribute management functions. The actual need for type-specific attribute handlers almost completely disappeared starting with Python 2.2, though there are many examples which have not been updated to use some of the new generic mechanism that is available.


3.3.1. Generic Attribute Management

Most extension types only use simple attributes. So, what makes the attributes simple? There are only a couple of conditions that must be met:
1.The name of the attributes must be known when PyType_Ready() is called.
2.No special processing is needed to record that an attribute was looked up or set, nor do actions need to be taken based on the value.

Note that this list does not place any restrictions on the values of the attributes, when the values are computed, or how relevant data is stored.

When PyType_Ready() is called, it uses three tables referenced by the type object to create descriptors which are placed in the dictionary of the type object. Each descriptor controls access to one attribute of the instance object. Each of the tables is optional; if all three are NULL, instances of the type will only have attributes that are inherited from their base type, and should leave the tp_getattro and tp_setattro fields NULL as well, allowing the base type to handle attributes.

The tables are declared as three fields of the type object:


    struct PyMethodDef *tp_methods;
    struct PyMemberDef *tp_members;
    struct PyGetSetDef *tp_getset;


If tp_methods is not NULL, it must refer to an array of PyMethodDef structures. Each entry in the table is an instance of this structure:


```c
typedef struct PyMethodDef {
    const char  *ml_name;       /* method name */
    PyCFunction  ml_meth;       /* implementation function */
    int          ml_flags;      /* flags */
    const char  *ml_doc;        /* docstring */
} PyMethodDef;
```


One entry should be defined for each method provided by the type; no entries are needed for methods inherited from a base type. One additional entry is needed at the end; it is a sentinel that marks the end of the array. The ml_name field of the sentinel must be NULL.

The second table is used to define attributes which map directly to data stored in the instance. A variety of primitive C types are supported, and access may be read-only or read-write. The structures in the table are defined as:


```c
typedef struct PyMemberDef {
    const char *name;
    int         type;
    int         offset;
    int         flags;
    const char *doc;
} PyMemberDef;

```

For each entry in the table, a descriptor will be constructed and added to the type which will be able to extract a value from the instance structure. The type field should contain one of the type codes defined in the structmember.h header; the value will be used to determine how to convert Python values to and from C values. The flags field is used to store flags which control how the attribute can be accessed.

The following flag constants are defined in structmember.h; they may be combined using bitwise-OR.


|    Constant   |                         Meaning                         |
|---------------|---------------------------------------------------------|
| READONLY      | Never writable.                                         |
| PY_AUDIT_READ | Emit an object.__getattr__ audit events before reading. |


Changed in version 3.10: RESTRICTED, READ_RESTRICTED and WRITE_RESTRICTED are deprecated. However, READ_RESTRICTED is an alias for PY_AUDIT_READ, so fields that specify either RESTRICTED or READ_RESTRICTED will also raise an audit event.

An interesting advantage of using the tp_members table to build descriptors that are used at runtime is that any attribute defined this way can have an associated doc string simply by providing the text in the table. An application can use the introspection API to retrieve the descriptor from the class object, and get the doc string using its __doc__ attribute.

As with the tp_methods table, a sentinel entry with a name value of NULL is required.


3.3.2. Type-specific Attribute Management

For simplicity, only the char* version will be demonstrated here; the type of the name parameter is the only difference between the char* and `PyObject*` flavors of the interface. This example effectively does the same thing as the generic example above, but does not use the generic support added in Python 2.2. It explains how the handler functions are called, so that if you do need to extend their functionality, you’ll understand what needs to be done.

The tp_getattr handler is called when the object requires an attribute look-up. It is called in the same situations where the __getattr__() method of a class would be called.

Here is an example:


```c
static PyObject *
newdatatype_getattr(newdatatypeobject *obj, char *name)
{
    if (strcmp(name, "data") == 0)
    {
        return PyLong_FromLong(obj->data);
    }

    PyErr_Format(PyExc_AttributeError,
                 "'%.50s' object has no attribute '%.400s'",
                 tp->tp_name, name);
    return NULL;
}
```


The tp_setattr handler is called when the __setattr__() or __delattr__() method of a class instance would be called. When an attribute should be deleted, the third parameter will be NULL. Here is an example that simply raises an exception; if this were really all you wanted, the tp_setattr handler should be set to NULL.


```c
static int
newdatatype_setattr(newdatatypeobject *obj, char *name, PyObject *v)
{
    PyErr_Format(PyExc_RuntimeError, "Read-only attribute: %s", name);
    return -1;
}
```


### ===🗝 ◦ 3.4. Object Comparison

    richcmpfunc tp_richcompare;


The `tp_richcompare` handler is called when comparisons are needed. It is analogous to the rich comparison methods, like __lt__(), and also called by `PyObject_RichCompare()` and `PyObject_RichCompareBool()`.

This function is called with two Python objects and the operator as arguments, where the operator is one of `Py_EQ, Py_NE, Py_LE, Py_GE, Py_LT or Py_GT`. It should compare the two objects with respect to the specified operator and return `Py_True` or `Py_False` if the comparison is successful, Py_NotImplemented to indicate that comparison is not implemented and the other object’s comparison method should be tried, or NULL if an exception was set.

Here is a sample implementation, for a datatype that is considered equal if the size of an internal pointer is equal:


```c
static PyObject *
newdatatype_richcmp(PyObject *obj1, PyObject *obj2, int op)
{
    PyObject *result;
    int c, size1, size2;

    /* code to make sure that both arguments are of type
       newdatatype omitted */

    size1 = obj1->obj_UnderlyingDatatypePtr->size;
    size2 = obj2->obj_UnderlyingDatatypePtr->size;

    switch (op) {
    case Py_LT: c = size1 <  size2; break;
    case Py_LE: c = size1 <= size2; break;
    case Py_EQ: c = size1 == size2; break;
    case Py_NE: c = size1 != size2; break;
    case Py_GT: c = size1 >  size2; break;
    case Py_GE: c = size1 >= size2; break;
    }
    result = c ? Py_True : Py_False;
    Py_INCREF(result);
    return result;
 }
```



### ===🗝 ◦ 3.5. Abstract Protocol Support


Python supports a variety of abstract ‘protocols;’ the specific interfaces provided to use these interfaces are documented in Abstract Objects Layer.

A number of these abstract interfaces were defined early in the development of the Python implementation. In particular, the number, mapping, and sequence protocols have been part of Python since the beginning. Other protocols have been added over time. For protocols which depend on several handler routines from the type implementation, the older protocols have been defined as optional blocks of handlers referenced by the type object. For newer protocols there are additional slots in the main type object, with a flag bit being set to indicate that the slots are present and should be checked by the interpreter. (The flag bit does not indicate that the slot values are non-NULL. The flag may be set to indicate the presence of a slot, but a slot may still be unfilled.)


    PyNumberMethods   *tp_as_number;
    PySequenceMethods *tp_as_sequence;
    PyMappingMethods  *tp_as_mapping;


If you wish your object to be able to act like a number, a sequence, or a mapping object, then you place the address of a structure that implements the C type PyNumberMethods, PySequenceMethods, or PyMappingMethods, respectively. It is up to you to fill in this structure with appropriate values. You can find examples of the use of each of these in the Objects directory of the Python source distribution.


    hashfunc tp_hash;


This function, if you choose to provide it, should return a hash number for an instance of your data type. Here is a simple example:

```c

static Py_hash_t
newdatatype_hash(newdatatypeobject *obj)
{
    Py_hash_t result;
    result = obj->some_size + 32767 * obj->some_number;
    if (result == -1)
       result = -2;
    return result;
}
```


Py_hash_t is a signed integer type with a platform-varying width. Returning -1 from tp_hash indicates an error, which is why you should be careful to avoid returning it when hash computation is successful, as seen above.


    ternaryfunc tp_call;


This function is called when an instance of your data type is “called”, for example, if obj1 is an instance of your data type and the Python script contains `obj1('hello')`, the tp_call handler is invoked.

This function takes three arguments:

(1). `self` is the instance of the data type which is the subject of the call. If the call is obj1('hello'), then self is obj1.

(2). `args` is a tuple containing the arguments to the call. You can use PyArg_ParseTuple() to extract the arguments.

(3). `kwds` is a dictionary of keyword arguments that were passed. If this is non-NULL and you support keyword arguments, use PyArg_ParseTupleAndKeywords() to extract the arguments. If you do not want to support keyword arguments and this is non-NULL, raise a TypeError with a message saying that keyword arguments are not supported.


Here is a toy tp_call implementation:


```c
static PyObject *
newdatatype_call(newdatatypeobject *self, PyObject *args, PyObject *kwds)
{
    PyObject *result;
    const char *arg1;
    const char *arg2;
    const char *arg3;

    if (!PyArg_ParseTuple(args, "sss:call", &arg1, &arg2, &arg3)) {
        return NULL;
    }
    result = PyUnicode_FromFormat(
        "Returning -- value: [%d] arg1: [%s] arg2: [%s] arg3: [%s]\n",
        obj->obj_UnderlyingDatatypePtr->size,
        arg1, arg2, arg3);
    return result;
}


/* Iterators */
getiterfunc tp_iter;
iternextfunc tp_iternext;
```


These functions provide support for the iterator protocol. Both handlers take exactly one parameter, the instance for which they are being called, and return a new reference. In the case of an error, they should set an exception and return NULL. tp_iter corresponds to the Python __iter__() method, while tp_iternext corresponds to the Python __next__() method.

Any iterable object must implement the tp_iter handler, which must return an iterator object. Here the same guidelines apply as for Python classes:
•For collections (such as lists and tuples) which can support multiple independent iterators, a new iterator should be created and returned by each call to tp_iter.
•Objects which can only be iterated over once (usually due to side effects of iteration, such as file objects) can implement tp_iter by returning a new reference to themselves – and should also therefore implement the tp_iternext handler.

Any iterator object should implement both tp_iter and tp_iternext. An iterator’s tp_iter handler should return a new reference to the iterator. Its tp_iternext handler should return a new reference to the next object in the iteration, if there is one. If the iteration has reached the end, tp_iternext may return NULL without setting an exception, or it may set StopIteration in addition to returning NULL; avoiding the exception can yield slightly better performance. If an actual error occurs, tp_iternext should always set an exception and return NULL.


### ===🗝 ◦ 3.6. Weak Reference Support

One of the goals of Python’s weak reference implementation is to allow any type to participate in the weak reference mechanism without incurring the overhead on performance-critical objects (such as numbers).

See also:
 Documentation for the weakref module.
 

For an object to be weakly referencable, the extension type must do two things:

(1). Include a `PyObject*` field in the C object structure dedicated to the weak reference mechanism. The object’s constructor should leave it NULL (which is automatic when using the default tp_alloc).

(2). Set the tp_weaklistoffset type member to the offset of the aforementioned field in the C object structure, so that the interpreter knows how to access and modify that field.


Concretely, here is how a trivial object structure would be augmented with the required field:


```c
typedef struct {
    PyObject_HEAD
    PyObject *weakreflist;  /* List of weak references */
} TrivialObject;
```


And the corresponding member in the statically-declared type object:


```c
static PyTypeObject TrivialType = {
    PyVarObject_HEAD_INIT(NULL, 0)
    /* ... other members omitted for brevity ... */
    .tp_weaklistoffset = offsetof(TrivialObject, weakreflist),
};
```


The only further addition is that tp_dealloc needs to clear any weak references (by calling `PyObject_ClearWeakRefs()`) if the field is non-NULL:


```c
static void
Trivial_dealloc(TrivialObject *self)
{
    /* Clear weakrefs first before calling any destructors */
    if (self->weakreflist != NULL)
        PyObject_ClearWeakRefs((PyObject *) self);
    /* ... remainder of destruction code omitted for brevity ... */
    Py_TYPE(self)->tp_free((PyObject *) self);
}
```


### ===🗝 ◦ 3.7. More Suggestions

In order to learn how to implement any specific method for your new data type, get the CPython source code. Go to the Objects directory, then search the C source files for tp_ plus the function you want (for example, tp_richcompare). You will find examples of the function you want to implement.

When you need to verify that an object is a concrete instance of the type you are implementing, use the PyObject_TypeCheck() function. A sample of its use might be something like the following:


```c
if (!PyObject_TypeCheck(some_object, &MyType)) {
    PyErr_SetString(PyExc_TypeError, "arg #1 not a mything");
    return NULL;
}
```

See also:
 Download CPython source releases. https://www.python.org/downloads/source/
 The CPython project on GitHub, where the CPython source code is developed. https://github.com/python/cpython

 
## ==⚡ • 4. Building C and C++ Extensions

A C extension for CPython is a shared library (e.g. a .so file on Linux, .pyd on Windows), which exports an initialization function.

To be importable, the shared library must be available on `PYTHONPATH`, and must be named after the module name, with an appropriate extension. When using distutils, the correct filename is generated automatically.

The initialization function has the signature:

➡ `PyObject* PyInit_modulename(void)`
It returns either a fully-initialized module, or a PyModuleDef instance. See Initializing C modules for details.

For modules with ASCII-only names, the function must be named PyInit_<modulename>, with <modulename> replaced by the name of the module. When using Multi-phase initialization, non-ASCII module names are allowed. In this case, the initialization function name is PyInitU_<modulename>, with <modulename> encoded using Python’s punycode encoding with hyphens replaced by underscores. In Python:


```py
def initfunc_name(name):
    try:
        suffix = b'_' + name.encode('ascii')
    except UnicodeEncodeError:
        suffix = b'U_' + name.encode('punycode').replace(b'-', b'_')
    return b'PyInit' + suffix
```


It is possible to export multiple modules from a single shared library by defining multiple initialization functions. However, importing them requires using symbolic links or a custom importer, because by default only the function corresponding to the filename is found. See the “Multiple modules in one library” section in PEP 489 for details.
https://www.python.org/dev/peps/pep-0489


### ===🗝 ◦ 4.1. Building C and C++ Extensions with distutils
- PEP 632, Deprecate distutils module https://www.python.org/dev/peps/pep-0632
- Python Packaging User Guide https://packaging.python.org/
- Setup Tools https://setuptools.pypa.io/en/latest/
- Distributing Python Modules (Legacy version) https://setuptools.pypa.io/en/latest/deprecated/distutils/index.html

The distutils package is deprecated and slated for removal in Python 3.12. Use setup tools or check PEP 632 for potential alternatives


Extension modules can be built using distutils, which is included in Python. Since distutils also supports creation of binary packages, users don’t necessarily need a compiler and distutils to install the extension.

A distutils package contains a driver script, setup.py. This is a plain Python file, which, in the most simple case, could look like this:


```py
from distutils.core import setup, Extension

module1 = Extension('demo',
                    sources = ['demo.c'])

setup (name = 'PackageName',
       version = '1.0',
       description = 'This is a demo package',
       ext_modules = [module1])

```

With this setup.py, and a file demo.c, running


    python setup.py build


will compile demo.c, and produce an extension module named demo in the build directory. Depending on the system, the module file will end up in a subdirectory build/lib.system, and may have a name like demo.so or demo.pyd.

In the setup.py, all execution is performed by calling the setup function. This takes a variable number of keyword arguments, of which the example above uses only a subset. Specifically, the example specifies meta-information to build packages, and it specifies the contents of the package. Normally, a package will contain additional modules, like Python source modules, documentation, subpackages, etc. Please refer to the distutils documentation in Distributing Python Modules (Legacy version) to learn more about the features of distutils; this section explains building extension modules only.

It is common to pre-compute arguments to setup(), to better structure the driver script. In the example above, the ext_modules argument to setup() is a list of extension modules, each of which is an instance of the Extension. In the example, the instance defines an extension named demo which is build by compiling a single source file, demo.c.

In many cases, building an extension is more complex, since additional preprocessor defines and libraries may be needed. This is demonstrated in the example below.


```py
from distutils.core import setup, Extension

module1 = Extension('demo',
                    define_macros = [('MAJOR_VERSION', '1'),
                                     ('MINOR_VERSION', '0')],
                    include_dirs = ['/usr/local/include'],
                    libraries = ['tcl83'],
                    library_dirs = ['/usr/local/lib'],
                    sources = ['demo.c'])

setup (name = 'PackageName',
       version = '1.0',
       description = 'This is a demo package',
       author = 'Martin v. Loewis',
       author_email = 'martin@v.loewis.de',
       url = 'https://docs.python.org/extending/building',
       long_description = '''
This is really just a demo package.
''',
       ext_modules = [module1])
```


In this example, setup() is called with additional meta-information, which is recommended when distribution packages have to be built. For the extension itself, it specifies preprocessor defines, include directories, library directories, and libraries. Depending on the compiler, distutils passes this information in different ways to the compiler. For example, on Unix, this may result in the compilation commands


gcc -DNDEBUG -g -O3 -Wall -Wstrict-prototypes -fPIC -DMAJOR_VERSION=1 -DMINOR_VERSION=0 -I/usr/local/include -I/usr/local/include/python2.2 -c demo.c -o build/temp.linux-i686-2.2/demo.o

gcc -shared build/temp.linux-i686-2.2/demo.o -L/usr/local/lib -ltcl83 -o build/lib.linux-i686-2.2/demo.so


These lines are for demonstration purposes only; distutils users should trust that distutils gets the invocations right.



### ===🗝 ◦ 4.2. Distributing your extension modules

When an extension has been successfully built, there are three ways to use it.

End-users will typically want to install the module, they do so by running


    python setup.py install


Module maintainers should produce source packages; to do so, they run


    python setup.py sdist


In some cases, additional files need to be included in a source distribution; this is done through a MANIFEST.in file; see Specifying the files to distribute for details.

If the source distribution has been built successfully, maintainers can also create binary distributions. Depending on the platform, one of the following commands can be used to do so.

    python setup.py bdist_rpm
    python setup.py bdist_dumb



## ==⚡ • 5. Building C and C++ Extensions on Windows

This chapter briefly explains how to create a Windows extension module for Python using Microsoft Visual C++, and follows with more detailed background information on how it works. The explanatory material is useful for both the Windows programmer learning to build Python extensions and the Unix programmer interested in producing software which can be successfully built on both Unix and Windows.

Module authors are encouraged to use the distutils approach for building extension modules, instead of the one described in this section. You will still need the C compiler that was used to build Python; typically Microsoft Visual C++.

Note:
 This chapter mentions a number of filenames that include an encoded Python version number. These filenames are represented with the version number shown as XY; in practice, 'X' will be the major version number and 'Y' will be the minor version number of the Python release you’re working with. For example, if you are using Python 2.2.1, XY will actually be 22.
 

### ===🗝 ◦ 5.1. A Cookbook Approach

There are two approaches to building extension modules on Windows, just as there are on Unix: use the distutils package to control the build process, or do things manually. The distutils approach works well for most extensions; documentation on using distutils to build and package extension modules is available in Distributing Python Modules (Legacy version). If you find you really need to do things manually, it may be instructive to study the project file for the winsound standard library module.


### ===🗝 ◦ 5.2. Differences Between Unix and Windows

Unix and Windows use completely different paradigms for run-time loading of code. Before you try to build a module that can be dynamically loaded, be aware of how your system works.

In Unix, a shared object (.so) file contains code to be used by the program, and also the names of functions and data that it expects to find in the program. When the file is joined to the program, all references to those functions and data in the file’s code are changed to point to the actual locations in the program where the functions and data are placed in memory. This is basically a link operation.

In Windows, a dynamic-link library (.dll) file has no dangling references. Instead, an access to functions or data goes through a lookup table. So the DLL code does not have to be fixed up at runtime to refer to the program’s memory; instead, the code already uses the DLL’s lookup table, and the lookup table is modified at runtime to point to the functions and data.

In Unix, there is only one type of library file (.a) which contains code from several object files (.o). During the link step to create a shared object file (.so), the linker may find that it doesn’t know where an identifier is defined. The linker will look for it in the object files in the libraries; if it finds it, it will include all the code from that object file.

In Windows, there are two types of library, a static library and an import library (both called .lib). A static library is like a Unix .a file; it contains code to be included as necessary. An import library is basically used only to reassure the linker that a certain identifier is legal, and will be present in the program when the DLL is loaded. So the linker uses the information from the import library to build the lookup table for using identifiers that are not included in the DLL. When an application or a DLL is linked, an import library may be generated, which will need to be used for all future DLLs that depend on the symbols in the application or DLL.

Suppose you are building two dynamic-load modules, B and C, which should share another block of code A. On Unix, you would not pass A.a to the linker for B.so and C.so; that would cause it to be included twice, so that B and C would each have their own copy. In Windows, building A.dll will also build A.lib. You do pass A.lib to the linker for B and C. A.lib does not contain code; it just contains information which will be used at runtime to access A’s code.

In Windows, using an import library is sort of like using import spam; it gives you access to spam’s names, but does not create a separate copy. On Unix, linking with a library is more like `from spam import *`; it does create a separate copy.


### ===🗝 ◦ 5.3. Using DLLs in Practice

Windows Python is built in Microsoft Visual C++; using other compilers may or may not work (though Borland seems to). The rest of this section is MSVC++ specific.

When creating DLLs in Windows, you must pass pythonXY.lib to the linker. To build two DLLs, spam and ni (which uses C functions found in spam), you could use these commands:


      cl /LD /I/python/include spam.c ../libs/pythonXY.lib
      cl /LD /I/python/include ni.c spam.lib ../libs/pythonXY.lib


The first command created three files: spam.obj, spam.dll and spam.lib. Spam.dll does not contain any Python functions (such as PyArg_ParseTuple()), but it does know how to find the Python code thanks to pythonXY.lib.

The second command created ni.dll (and .obj and .lib), which knows how to find the necessary functions from spam, and also from the Python executable.

Not every identifier is exported to the lookup table. If you want any other modules (including Python) to be able to see your identifiers, you have to say `_declspec(dllexport)`, as in `void _declspec(dllexport) initspam(void)` or `PyObject _declspec(dllexport) *NiGetSpamData(void)`.

Developer Studio will throw in a lot of import libraries that you do not really need, adding about 100K to your executable. To get rid of them, use the Project Settings dialog, Link tab, to specify ignore default libraries. Add the correct msvcrtxx.lib to the list of libraries.



## ==⚡ • Embedding the CPython runtime in a larger application

Sometimes, rather than creating an extension that runs inside the Python interpreter as the main application, it is desirable to instead embed the CPython runtime inside a larger application. This section covers some of the details involved in doing that successfully.

### ===🗝 • 1. Embedding Python in Another Application

The previous chapters discussed how to extend Python, that is, how to extend the functionality of Python by attaching a library of C functions to it. It is also possible to do it the other way around: enrich your C/C++ application by embedding Python in it. Embedding provides your application with the ability to implement some of the functionality of your application in Python rather than C or C++. This can be used for many purposes; one example would be to allow users to tailor the application to their needs by writing some scripts in Python. You can also use it yourself if some of the functionality can be written in Python more easily.

Embedding Python is similar to extending it, but not quite. The difference is that when you extend Python, the main program of the application is still the Python interpreter, while if you embed Python, the main program may have nothing to do with Python — instead, some parts of the application occasionally call the Python interpreter to run some Python code.

So if you are embedding Python, you are providing your own main program. One of the things this main program has to do is initialize the Python interpreter. At the very least, you have to call the function `Py_Initialize()`. There are optional calls to pass command line arguments to Python. Then later you can call the interpreter from any part of the application.

There are several different ways to call the interpreter: you can pass a string containing Python statements to `PyRun_SimpleString()`, or you can pass a stdio file pointer and a file name (for identification in error messages only) to `PyRun_SimpleFile()`. You can also call the lower-level operations described in the previous chapters to construct and use Python objects.

See also:
 Python/C API Reference ManualThe details of Python’s C interface are given in this manual. A great deal of necessary information can be found here.

### ===🗝 ◦ 1.1. Very High Level Embedding

The simplest form of embedding Python is the use of the very high level interface. This interface is intended to execute a Python script without needing to interact with the application directly. This can for example be used to perform some operation on a file.


```c
#define PY_SSIZE_T_CLEAN
#include <Python.h>

int
main(int argc, char *argv[])
{
    wchar_t *program = Py_DecodeLocale(argv[0], NULL);
    if (program == NULL) {
        fprintf(stderr, "Fatal error: cannot decode argv[0]\n");
        exit(1);
    }
    Py_SetProgramName(program);  /* optional but recommended */
    Py_Initialize();
    PyRun_SimpleString("from time import time,ctime\n"
                       "print('Today is', ctime(time()))\n");
    if (Py_FinalizeEx() < 0) {
        exit(120);
    }
    PyMem_RawFree(program);
    return 0;
}
```


The `Py_SetProgramName()` function should be called before `Py_Initialize()` to inform the interpreter about paths to Python run-time libraries. 

Next, the Python interpreter is initialized with `Py_Initialize()`, followed by the execution of a hard-coded Python script that prints the date and time. 

Afterwards, the `Py_FinalizeEx()` call shuts the interpreter down, followed by the end of the program. 

In a real program, you may want to get the Python script from another source, perhaps a text-editor routine, a file, or a database. Getting the Python code from a file can better be done by using the `PyRun_SimpleFile()` function, which saves you the trouble of allocating memory space and loading the file contents.

### ===🗝 ◦ 1.2. Beyond Very High Level Embedding: An overview

The high level interface gives you the ability to execute arbitrary pieces of Python code from your application, but exchanging data values is quite cumbersome to say the least. If you want that, you should use lower level calls. At the cost of having to write more C code, you can achieve almost anything.

It should be noted that extending Python and embedding Python is quite the same activity, despite the different intent. Most topics discussed in the previous chapters are still valid. 

To show this, consider what the extension code from Python to C really does:

1. Convert data values from Python to C,
2. Perform a function call to a C routine using the converted values, and
3. Convert the data values from the call from C to Python.

When embedding Python, the interface code does:

1. Convert data values from C to Python,
2. Perform a function call to a Python interface routine using the converted values, and
3. Convert the data values from the call from Python to C.

As you can see, the data conversion steps are simply swapped to accommodate the different direction of the cross-language transfer. The only difference is the routine that you call between both data conversions. When extending, you call a C routine, when embedding, you call a Python routine.

This chapter will not discuss how to convert data from Python to C and vice versa. Also, proper use of references and dealing with errors is assumed to be understood. Since these aspects do not differ from extending the interpreter, you can refer to earlier chapters for the required information.

### ===🗝 ◦ 1.3. Pure Embedding

The first program aims to execute a function in a Python script. Like in the section about the very high level interface, the Python interpreter does not directly interact with the application (but that will change in the next section).

The code to run a function defined in a Python script is:


```c
#define PY_SSIZE_T_CLEAN
#include <Python.h>

int
main(int argc, char *argv[])
{
    PyObject *pName, *pModule, *pFunc;
    PyObject *pArgs, *pValue;
    int i;

    if (argc < 3) {
        fprintf(stderr,"Usage: call pythonfile funcname [args]\n");
        return 1;
    }

    Py_Initialize();
    pName = PyUnicode_DecodeFSDefault(argv[1]);
    /* Error checking of pName left out */

    pModule = PyImport_Import(pName);
    Py_DECREF(pName);

    if (pModule != NULL) {
        pFunc = PyObject_GetAttrString(pModule, argv[2]);
        /* pFunc is a new reference */

        if (pFunc && PyCallable_Check(pFunc)) {
            pArgs = PyTuple_New(argc - 3);
            for (i = 0; i < argc - 3; ++i) {
                pValue = PyLong_FromLong(atoi(argv[i + 3]));
                if (!pValue) {
                    Py_DECREF(pArgs);
                    Py_DECREF(pModule);
                    fprintf(stderr, "Cannot convert argument\n");
                    return 1;
                }
                /* pValue reference stolen here: */
                PyTuple_SetItem(pArgs, i, pValue);
            }
            pValue = PyObject_CallObject(pFunc, pArgs);
            Py_DECREF(pArgs);
            if (pValue != NULL) {
                printf("Result of call: %ld\n", PyLong_AsLong(pValue));
                Py_DECREF(pValue);
            }
            else {
                Py_DECREF(pFunc);
                Py_DECREF(pModule);
                PyErr_Print();
                fprintf(stderr,"Call failed\n");
                return 1;
            }
        }
        else {
            if (PyErr_Occurred())
                PyErr_Print();
            fprintf(stderr, "Cannot find function \"%s\"\n", argv[2]);
        }
        Py_XDECREF(pFunc);
        Py_DECREF(pModule);
    }
    else {
        PyErr_Print();
        fprintf(stderr, "Failed to load \"%s\"\n", argv[1]);
        return 1;
    }
    if (Py_FinalizeEx() < 0) {
        return 120;
    }
    return 0;
}
```


This code loads a Python script using `argv[1]`, and calls the function named in `argv[2]`. Its integer arguments are the other values of the argv array. If you compile and link this program (let’s call the finished executable call), and use it to execute a Python script, such as:


```py
def multiply(a,b):
    print("Will compute", a, "times", b)
    c = 0
    for i in range(0, a):
        c = c + b
    return c
```


then the result should be:


>$ call multiply multiply 3 2
Will compute 3 times 2
Result of call: 6


Although the program is quite large for its functionality, most of the code is for data conversion between Python and C, and for error reporting. The interesting part with respect to embedding Python starts with


```c
Py_Initialize();
pName = PyUnicode_DecodeFSDefault(argv[1]);
/* Error checking of pName left out */
pModule = PyImport_Import(pName);
```


After initializing the interpreter, the script is loaded using `PyImport_Import()`. This routine needs a Python string as its argument, which is constructed using the `PyUnicode_FromString()` data conversion routine.


```c
pFunc = PyObject_GetAttrString(pModule, argv[2]);
/* pFunc is a new reference */

if (pFunc && PyCallable_Check(pFunc)) {
    ...
}
Py_XDECREF(pFunc);
```


Once the script is loaded, the name we’re looking for is retrieved using `PyObject_GetAttrString()`. If the name exists, and the object returned is callable, you can safely assume that it is a function. The program then proceeds by constructing a tuple of arguments as normal. The call to the Python function is then made with:


>pValue = PyObject_CallObject(pFunc, pArgs);


Upon return of the function, pValue is either NULL or it contains a reference to the return value of the function. Be sure to release the reference after examining the value.

### ===🗝 ◦ 1.4. Extending Embedded Python

通过 PyModuleDef 和 PyMethodDef 等结构直接在 C 代码中嵌入模块及其方法的实现，通过 PyModule_Create 创建、注册模块。

Until now, the embedded Python interpreter had no access to functionality from the application itself. The Python API allows this by extending the embedded interpreter. That is, the embedded interpreter gets extended with routines provided by the application. While it sounds complex, it is not so bad. 

Simply forget for a while that the application starts the Python interpreter. Instead, consider the application to be a set of subroutines, and write some glue code that gives Python access to those routines, just like you would write a normal Python extension. For example:


```c
static int numargs=0;

/* Return the number of arguments of the application command line */
static PyObject*
emb_numargs(PyObject *self, PyObject *args)
{
    if(!PyArg_ParseTuple(args, ":numargs"))
        return NULL;
    return PyLong_FromLong(numargs);
}

static PyMethodDef EmbMethods[] = {
    {"numargs", emb_numargs, METH_VARARGS,
     "Return the number of arguments received by the process."},
    {NULL, NULL, 0, NULL}
};

static PyModuleDef EmbModule = {
    PyModuleDef_HEAD_INIT, "emb", NULL, -1, EmbMethods,
    NULL, NULL, NULL, NULL
};

static PyObject*
PyInit_emb(void)
{
    return PyModule_Create(&EmbModule);
}
```


Insert the above code just above the main() function. Also, insert the following two statements before the call to Py_Initialize():


```c
numargs = argc;
PyImport_AppendInittab("emb", &PyInit_emb);
```


These two lines initialize the numargs variable, and make the emb.numargs() function accessible to the embedded Python interpreter. With these extensions, the Python script can do things like


```py
import emb
print("Number of arguments", emb.numargs())
```


In a real application, the methods will expose an API of the application to Python.


### ===🗝 ◦ 1.5. Embedding Python in C++

It is also possible to embed Python in a C++ program; precisely how this is done will depend on the details of the C++ system used; in general you will need to write the main program in C++, and use the C++ compiler to compile and link your program. There is no need to recompile Python itself using C++.

### ===🗝 ◦ 1.6. Compiling and Linking under Unix-like systems

It is not necessarily trivial to find the right flags to pass to your compiler (and linker) in order to embed the Python interpreter into your application, particularly because Python needs to load library modules implemented as C dynamic extensions (.so files) linked against it.

To find out the required compiler and linker flags, you can execute the pythonX.Y-config script which is generated as part of the installation process (a python3-config script may also be available). This script has several options, of which the following will be directly useful to you:

• pythonX.Y-config --cflags will give you the recommended flags when compiling:


```sh
$ /opt/bin/python3.4-config --cflags
-I/opt/include/python3.4m -I/opt/include/python3.4m -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes
```


• pythonX.Y-config --ldflags will give you the recommended flags when linking:


```sh
$ /opt/bin/python3.4-config --ldflags
-L/opt/lib/python3.4/config-3.4m -lpthread -ldl -lutil -lm -lpython3.4m -Xlinker -export-dynamic
```



Note:
 To avoid confusion between several Python installations (and especially between the system Python and your own compiled Python), it is recommended that you use the absolute path to pythonX.Y-config, as in the above example.
 

If this procedure doesn’t work for you (it is not guaranteed to work for all Unix-like platforms; however, we welcome bug reports) you will have to read your system’s documentation about dynamic linking and/or examine Python’s Makefile (use sysconfig.get_makefile_filename() to find its location) and compilation options. In this case, the sysconfig module is a useful tool to programmatically extract the configuration values that you will want to combine together. For example:


>>> import sysconfig
>>> sysconfig.get_config_var('LIBS')
'-lpthread -ldl  -lutil'
>>> sysconfig.get_config_var('LINKFORSHARED')
'-Xlinker -export-dynamic'



# =🚩 Python/C API Reference Manual
- PEP 7 – Style Guide for C Code https://peps.python.org/pep-0007/

This manual documents the API used by C and C++ programmers who want to write extension modules or embed Python. It is a companion to Extending and Embedding the Python Interpreter, which describes the general principles of extension writing but does not document the API functions in detail.


• Introduction
◦ Coding standards
◦ Include Files
◦ Useful macros
◦ Objects, Types and Reference Counts
◦ Exceptions
◦ Embedding Python
◦ Debugging Builds

• C API Stability
◦ Stable Application Binary Interface
◦ Platform Considerations
◦ Contents of Limited API

• The Very High Level Layer
• Reference Counting
• Exception Handling
◦ Printing and clearing
◦ Raising exceptions
◦ Issuing warnings
◦ Querying the error indicator
◦ Signal Handling
◦ Exception Classes
◦ Exception Objects
◦ Unicode Exception Objects
◦ Recursion Control
◦ Standard Exceptions
◦ Standard Warning Categories

• Utilities
◦ Operating System Utilities
◦ System Functions
◦ Process Control
◦ Importing Modules
◦ Data marshalling support
◦ Parsing arguments and building values
◦ String conversion and formatting
◦ Reflection
◦ Codec registry and support functions

• Abstract Objects Layer
◦ Object Protocol
◦ Call Protocol
◦ Number Protocol
◦ Sequence Protocol
◦ Mapping Protocol
◦ Iterator Protocol
◦ Buffer Protocol
◦ Old Buffer Protocol

• Concrete Objects Layer
◦ Fundamental Objects
◦ Numeric Objects
◦ Sequence Objects
◦ Container Objects
◦ Function Objects
◦ Other Objects

• Initialization, Finalization, and Threads
◦ Before Python Initialization
◦ Global configuration variables
◦ Initializing and finalizing the interpreter
◦ Process-wide parameters
◦ Thread State and the Global Interpreter Lock
◦ Sub-interpreter support
◦ Asynchronous Notifications
◦ Profiling and Tracing
◦ Advanced Debugger Support
◦ Thread Local Storage Support

• Python Initialization Configuration
◦ Example
◦ PyWideStringList
◦ PyStatus
◦ PyPreConfig
◦ Preinitialize Python with PyPreConfig
◦ PyConfig
◦ Initialization with PyConfig
◦ Isolated Configuration
◦ Python Configuration
◦ Python Path Configuration
◦ Py_RunMain()
◦ Py_GetArgcArgv()
◦ Multi-Phase Initialization Private Provisional API

• Memory Management
◦ Overview
◦ Allocator Domains
◦ Raw Memory Interface
◦ Memory Interface
◦ Object allocators
◦ Default Memory Allocators
◦ Customize Memory Allocators
◦ Debug hooks on the Python memory allocators
◦ The pymalloc allocator
◦ tracemalloc C API
◦ Examples

• Object Implementation Support
◦ Allocating Objects on the Heap
◦ Common Object Structures
◦ Type Objects
◦ Number Object Structures
◦ Mapping Object Structures
◦ Sequence Object Structures
◦ Buffer Object Structures
◦ Async Object Structures
◦ Slot Type typedefs
◦ Examples
◦ Supporting Cyclic Garbage Collection

• API and ABI Versioning


## ==⚡ • Introduction

### ===🗝 ◦ Coding standards

### ===🗝 ◦ Include Files

### ===🗝 ◦ Useful macros

### ===🗝 ◦ Objects, Types and Reference Counts

### ===🗝 ◦ Exceptions

### ===🗝 ◦ Embedding Python

### ===🗝 ◦ Debugging Builds


## ==⚡ • C API Stability

### ===🗝 ◦ Stable Application Binary Interface

### ===🗝 ◦ Platform Considerations

### ===🗝 ◦ Contents of Limited API


## ==⚡ • The Very High Level Layer

## ==⚡ • Reference Counting

## ==⚡ • Exception Handling

### ===🗝 ◦ Printing and clearing

### ===🗝 ◦ Raising exceptions

### ===🗝 ◦ Issuing warnings

### ===🗝 ◦ Querying the error indicator

### ===🗝 ◦ Signal Handling

### ===🗝 ◦ Exception Classes

### ===🗝 ◦ Exception Objects

### ===🗝 ◦ Unicode Exception Objects

### ===🗝 ◦ Recursion Control

### ===🗝 ◦ Standard Exceptions

### ===🗝 ◦ Standard Warning Categories


## ==⚡ • Utilities

### ===🗝 ◦ Operating System Utilities

### ===🗝 ◦ System Functions

### ===🗝 ◦ Process Control

### ===🗝 ◦ Importing Modules

### ===🗝 ◦ Data marshalling support

### ===🗝 ◦ Parsing arguments and building values

### ===🗝 ◦ String conversion and formatting

### ===🗝 ◦ Reflection

### ===🗝 ◦ Codec registry and support functions


## ==⚡ • Abstract Objects Layer

### ===🗝 ◦ Object Protocol

### ===🗝 ◦ Call Protocol

### ===🗝 ◦ Number Protocol

### ===🗝 ◦ Sequence Protocol

### ===🗝 ◦ Mapping Protocol

### ===🗝 ◦ Iterator Protocol

### ===🗝 ◦ Buffer Protocol

### ===🗝 ◦ Old Buffer Protocol


## ==⚡ • Concrete Objects Layer

### ===🗝 ◦ Fundamental Objects

### ===🗝 ◦ Numeric Objects

### ===🗝 ◦ Sequence Objects

### ===🗝 ◦ Container Objects

### ===🗝 ◦ Function Objects

### ===🗝 ◦ Other Objects


## ==⚡ • Initialization, Finalization, and Threads

### ===🗝 ◦ Before Python Initialization

### ===🗝 ◦ Global configuration variables

### ===🗝 ◦ Initializing and finalizing the interpreter

### ===🗝 ◦ Process-wide parameters

### ===🗝 ◦ Thread State and the Global Interpreter Lock

### ===🗝 ◦ Sub-interpreter support

### ===🗝 ◦ Asynchronous Notifications

### ===🗝 ◦ Profiling and Tracing

### ===🗝 ◦ Advanced Debugger Support

### ===🗝 ◦ Thread Local Storage Support


## ==⚡ • Python Initialization Configuration

### ===🗝 ◦ Example

### ===🗝 ◦ PyWideStringList

### ===🗝 ◦ PyStatus

### ===🗝 ◦ PyPreConfig

### ===🗝 ◦ Preinitialize Python with PyPreConfig

### ===🗝 ◦ PyConfig

### ===🗝 ◦ Initialization with PyConfig

### ===🗝 ◦ Isolated Configuration

### ===🗝 ◦ Python Configuration

### ===🗝 ◦ Python Path Configuration

### ===🗝 ◦ Py_RunMain()

### ===🗝 ◦ Py_GetArgcArgv()

### ===🗝 ◦ Multi-Phase Initialization Private Provisional API


## ==⚡ • Memory Management

### ===🗝 ◦ Overview

### ===🗝 ◦ Allocator Domains

### ===🗝 ◦ Raw Memory Interface

### ===🗝 ◦ Memory Interface

### ===🗝 ◦ Object allocators

### ===🗝 ◦ Default Memory Allocators

### ===🗝 ◦ Customize Memory Allocators

### ===🗝 ◦ Debug hooks on the Python memory allocators

### ===🗝 ◦ The pymalloc allocator

### ===🗝 ◦ tracemalloc C API

### ===🗝 ◦ Examples


## ==⚡ • Object Implementation Support

### ===🗝 ◦ Allocating Objects on the Heap

### ===🗝 ◦ Common Object Structures

### ===🗝 ◦ Type Objects

### ===🗝 ◦ Number Object Structures

### ===🗝 ◦ Mapping Object Structures

### ===🗝 ◦ Sequence Object Structures

### ===🗝 ◦ Buffer Object Structures

### ===🗝 ◦ Async Object Structures

### ===🗝 ◦ Slot Type typedefs

### ===🗝 ◦ Examples

### ===🗝 ◦ Supporting Cyclic Garbage Collection


## ==⚡ • API and ABI Versioning



# =🚩The Python Standard Library
Python 3.10 Sources https://github.com/python/cpython/blob/3.10/Lib/

使用命令查询 API 参考文档：

    c:\python310\python.exe -m pydoc audioop > m.md; start m.md


While The Python Language Reference describes the exact syntax and semantics of the Python language, this library reference manual describes the standard library that is distributed with Python. It also describes some of the optional components that are commonly included in Python distributions.

Python’s standard library is very extensive, offering a wide range of facilities as indicated by the long table of contents listed below. The library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.

The Python installers for the Windows platform usually include the entire standard library and often also include many additional components. For Unix-like operating systems Python is normally provided as a collection of packages, so it may be necessary to use the packaging tools provided with the operating system to obtain some or all of the optional components.

In addition to the standard library, there is a growing collection of several thousand components (from individual programs and modules to packages and entire application development frameworks), available from the Python Package Index.

Contents 

• **Introduction**
◦ Notes on availability

• **Built-in Functions**

• **Built-in Constants**
◦ Constants added by the site module

• **Built-in Types**
◦ Truth Value Testing
◦ Boolean Operations — and, or, not
◦ Comparisons
◦ Numeric Types — int, float, complex
◦ Iterator Types
◦ Sequence Types — list, tuple, range
◦ Text Sequence Type — str
◦ Binary Sequence Types — bytes, bytearray, memoryview
◦ Set Types — set, frozenset
◦ Mapping Types — dict
◦ Context Manager Types
◦ Generic Alias Type
◦ Other Built-in Types
◦ Special Attributes

• **Built-in Exceptions**
◦ Base classes
◦ Concrete exceptions
◦ Warnings
◦ Exception hierarchy

• **Text Processing Services**
◦ string — Common string operations
◦ re — Regular expression operations
◦ difflib — Helpers for computing deltas
◦ textwrap — Text wrapping and filling
◦ unicodedata — Unicode Database
◦ stringprep — Internet String Preparation
◦ readline — GNU readline interface
◦ rlcompleter — Completion function for GNU readline

• **Binary Data Services**
◦ struct — Interpret bytes as packed binary data
◦ codecs — Codec registry and base classes

• **Data Types**
◦ datetime — Basic date and time types
◦ zoneinfo — IANA time zone support
◦ calendar — General calendar-related functions
◦ collections — Container datatypes
◦ collections.abc — Abstract Base Classes for Containers
◦ heapq — Heap queue algorithm
◦ bisect — Array bisection algorithm
◦ array — Efficient arrays of numeric values
◦ weakref — Weak references
◦ types — Dynamic type creation and names for built-in types
◦ copy — Shallow and deep copy operations
◦ pprint — Data pretty printer
◦ reprlib — Alternate repr() implementation
◦ enum — Support for enumerations
◦ graphlib — Functionality to operate with graph-like structures

• **Numeric and Mathematical Modules**
◦ numbers — Numeric abstract base classes
◦ math — Mathematical functions
◦ cmath — Mathematical functions for complex numbers
◦ decimal — Decimal fixed point and floating point arithmetic
◦ fractions — Rational numbers
◦ random — Generate pseudo-random numbers
◦ statistics — Mathematical statistics functions

• **Functional Programming Modules**
◦ itertools — Functions creating iterators for efficient looping
◦ functools — Higher-order functions and operations on callable objects
◦ operator — Standard operators as functions

• **File and Directory Access**
◦ pathlib — Object-oriented filesystem paths
◦ os.path — Common pathname manipulations
◦ fileinput — Iterate over lines from multiple input streams
◦ stat — Interpreting stat() results
◦ filecmp — File and Directory Comparisons
◦ tempfile — Generate temporary files and directories
◦ glob — Unix style pathname pattern expansion
◦ fnmatch — Unix filename pattern matching
◦ linecache — Random access to text lines
◦ shutil — High-level file operations

• **Data Persistence**
◦ pickle — Python object serialization
◦ copyreg — Register pickle support functions
◦ shelve — Python object persistence
◦ marshal — Internal Python object serialization
◦ dbm — Interfaces to Unix “databases”
◦ sqlite3 — DB-API 2.0 interface for SQLite databases

• **Data Compression and Archiving**
◦ zlib — Compression compatible with gzip
◦ gzip — Support for gzip files
◦ bz2 — Support for bzip2 compression
◦ lzma — Compression using the LZMA algorithm
◦ zipfile — Work with ZIP archives
◦ tarfile — Read and write tar archive files

• **File Formats**
◦ csv — CSV File Reading and Writing
◦ configparser — Configuration file parser
◦ netrc — netrc file processing
◦ xdrlib — Encode and decode XDR data
◦ plistlib — Generate and parse Apple .plist files

• **Cryptographic Services**
◦ hashlib — Secure hashes and message digests
◦ hmac — Keyed-Hashing for Message Authentication
◦ secrets — Generate secure random numbers for managing secrets

• **Generic Operating System Services**
◦ os — Miscellaneous operating system interfaces
◦ io — Core tools for working with streams
◦ time — Time access and conversions
◦ argparse — Parser for command-line options, arguments and sub-commands
◦ getopt — C-style parser for command line options
◦ logging — Logging facility for Python
◦ logging.config — Logging configuration
◦ logging.handlers — Logging handlers
◦ getpass — Portable password input
◦ curses — Terminal handling for character-cell displays
◦ curses.textpad — Text input widget for curses programs
◦ curses.ascii — Utilities for ASCII characters
◦ curses.panel — A panel stack extension for curses
◦ platform — Access to underlying platform’s identifying data
◦ errno — Standard errno system symbols
◦ ctypes — A foreign function library for Python

• **Concurrent Execution**
◦ threading — Thread-based parallelism
◦ multiprocessing — Process-based parallelism
◦ multiprocessing.shared_memory — Provides shared memory for direct access across processes
◦ The concurrent package
◦ concurrent.futures — Launching parallel tasks
◦ subprocess — Subprocess management
◦ sched — Event scheduler
◦ queue — A synchronized queue class
◦ contextvars — Context Variables
◦ `_thread` — Low-level threading API

• **Networking and Interprocess Communication**
◦ asyncio — Asynchronous I/O
◦ socket — Low-level networking interface
◦ ssl — TLS/SSL wrapper for socket objects
◦ select — Waiting for I/O completion
◦ selectors — High-level I/O multiplexing
◦ asyncore — Asynchronous socket handler
◦ asynchat — Asynchronous socket command/response handler
◦ signal — Set handlers for asynchronous events
◦ mmap — Memory-mapped file support

• **Internet Data Handling**
◦ email — An email and MIME handling package
◦ json — JSON encoder and decoder
◦ mailcap — Mailcap file handling
◦ mailbox — Manipulate mailboxes in various formats
◦ mimetypes — Map filenames to MIME types
◦ base64 — Base16, Base32, Base64, Base85 Data Encodings
◦ binhex — Encode and decode binhex4 files
◦ binascii — Convert between binary and ASCII
◦ quopri — Encode and decode MIME quoted-printable data
◦ uu — Encode and decode uuencode files

• **Structured Markup Processing Tools**
◦ html — HyperText Markup Language support
◦ html.parser — Simple HTML and XHTML parser
◦ html.entities — Definitions of HTML general entities
◦ XML Processing Modules
◦ xml.etree.ElementTree — The ElementTree XML API
◦ xml.dom — The Document Object Model API
◦ xml.dom.minidom — Minimal DOM implementation
◦ xml.dom.pulldom — Support for building partial DOM trees
◦ xml.sax — Support for SAX2 parsers
◦ xml.sax.handler — Base classes for SAX handlers
◦ xml.sax.saxutils — SAX Utilities
◦ xml.sax.xmlreader — Interface for XML parsers
◦ xml.parsers.expat — Fast XML parsing using Expat

• **Internet Protocols and Support**
◦ webbrowser — Convenient Web-browser controller
◦ cgi — Common Gateway Interface support
◦ cgitb — Traceback manager for CGI scripts
◦ wsgiref — WSGI Utilities and Reference Implementation
◦ urllib — URL handling modules
◦ urllib.request — Extensible library for opening URLs
◦ urllib.response — Response classes used by urllib
◦ urllib.parse — Parse URLs into components
◦ urllib.error — Exception classes raised by urllib.request
◦ urllib.robotparser — Parser for robots.txt
◦ http — HTTP modules
◦ http.client — HTTP protocol client
◦ ftplib — FTP protocol client
◦ poplib — POP3 protocol client
◦ imaplib — IMAP4 protocol client
◦ nntplib — NNTP protocol client
◦ smtplib — SMTP protocol client
◦ smtpd — SMTP Server
◦ telnetlib — Telnet client
◦ uuid — UUID objects according to RFC 4122
◦ socketserver — A framework for network servers
◦ http.server — HTTP servers
◦ http.cookies — HTTP state management
◦ http.cookiejar — Cookie handling for HTTP clients
◦ xmlrpc — XMLRPC server and client modules
◦ xmlrpc.client — XML-RPC client access
◦ xmlrpc.server — Basic XML-RPC servers
◦ ipaddress — IPv4/IPv6 manipulation library

• **Multimedia Services**
◦ audioop — Manipulate raw audio data
◦ aifc — Read and write AIFF and AIFC files
◦ sunau — Read and write Sun AU files
◦ wave — Read and write WAV files
◦ chunk — Read IFF chunked data
◦ colorsys — Conversions between color systems
◦ imghdr — Determine the type of an image
◦ sndhdr — Determine type of sound file
◦ ossaudiodev — Access to OSS-compatible audio devices

• **Internationalization**
◦ gettext — Multilingual internationalization services
◦ locale — Internationalization services

• **Program Frameworks**
◦ turtle — Turtle graphics
◦ cmd — Support for line-oriented command interpreters
◦ shlex — Simple lexical analysis

• **Graphical User Interfaces with Tk**
◦ tkinter — Python interface to Tcl/Tk
◦ tkinter.colorchooser — Color choosing dialog
◦ tkinter.font — Tkinter font wrapper
◦ Tkinter Dialogs
◦ tkinter.messagebox — Tkinter message prompts
◦ tkinter.scrolledtext — Scrolled Text Widget
◦ tkinter.dnd — Drag and drop support
◦ tkinter.ttk — Tk themed widgets
◦ tkinter.tix — Extension widgets for Tk
◦ IDLE
◦ Other Graphical User Interface Packages

• **Development Tools**
◦ typing — Support for type hints
◦ pydoc — Documentation generator and online help system
◦ Python Development Mode
◦ Effects of the Python Development Mode
◦ ResourceWarning Example
◦ Bad file descriptor error example
◦ doctest — Test interactive Python examples
◦ unittest — Unit testing framework
◦ unittest.mock — mock object library
◦ unittest.mock — getting started
◦ 2to3 - Automated Python 2 to 3 code translation
◦ test — Regression tests package for Python
◦ test.support — Utilities for the Python test suite
◦ test.support.socket_helper — Utilities for socket tests
◦ test.support.script_helper — Utilities for the Python execution tests
◦ test.support.bytecode_helper — Support tools for testing correct bytecode generation

• **Debugging and Profiling**
◦ Audit events table
◦ bdb — Debugger framework
◦ faulthandler — Dump the Python traceback
◦ pdb — The Python Debugger
◦ The Python Profilers
◦ timeit — Measure execution time of small code snippets
◦ trace — Trace or track Python statement execution
◦ tracemalloc — Trace memory allocations

• **Software Packaging and Distribution**
◦ distutils — Building and installing Python modules
◦ ensurepip — Bootstrapping the pip installer
◦ venv — Creation of virtual environments
◦ zipapp — Manage executable Python zip archives

• **Python Runtime Services**
◦ sys — System-specific parameters and functions
◦ sysconfig — Provide access to Python’s configuration information
◦ builtins — Built-in objects
◦ __main__ — Top-level script environment
◦ warnings — Warning control
◦ dataclasses — Data Classes
◦ contextlib — Utilities for with-statement contexts
◦ abc — Abstract Base Classes
◦ atexit — Exit handlers
◦ traceback — Print or retrieve a stack traceback
◦ __future__ — Future statement definitions
◦ gc — Garbage Collector interface
◦ inspect — Inspect live objects
◦ site — Site-specific configuration hook

• **Custom Python Interpreters**
◦ code — Interpreter base classes
◦ codeop — Compile Python code

• **Importing Modules**
◦ zipimport — Import modules from Zip archives
◦ pkgutil — Package extension utility
◦ modulefinder — Find modules used by a script
◦ runpy — Locating and executing Python modules
◦ importlib — The implementation of import
◦ Using importlib.metadata

• **Python Language Services**
◦ parser — Access Python parse trees
◦ ast — Abstract Syntax Trees
◦ symtable — Access to the compiler’s symbol tables
◦ symbol — Constants used with Python parse trees
◦ token — Constants used with Python parse trees
◦ keyword — Testing for Python keywords
◦ tokenize — Tokenizer for Python source
◦ tabnanny — Detection of ambiguous indentation
◦ pyclbr — Python module browser support
◦ py_compile — Compile Python source files
◦ compileall — Byte-compile Python libraries
◦ dis — Disassembler for Python bytecode
◦ pickletools — Tools for pickle developers

• **Miscellaneous Services**
◦ formatter — Generic output formatting

• **MS Windows Specific Services**
◦ msilib — Read and write Microsoft Installer files
◦ msvcrt — Useful routines from the MS VC++ runtime
◦ winreg — Windows registry access
◦ winsound — Sound-playing interface for Windows

• **Unix Specific Services**
◦ posix — The most common POSIX system calls
◦ pwd — The password database
◦ spwd — The shadow password database
◦ grp — The group database
◦ crypt — Function to check Unix passwords
◦ termios — POSIX style tty control
◦ tty — Terminal control functions
◦ pty — Pseudo-terminal utilities
◦ fcntl — The fcntl and ioctl system calls
◦ pipes — Interface to shell pipelines
◦ resource — Resource usage information
◦ nis — Interface to Sun’s NIS (Yellow Pages)
◦ syslog — Unix syslog library routines

• **Superseded Modules**
◦ optparse — Parser for command line options
◦ imp — Access the import internals

• **Undocumented Modules**
◦ Platform specific modules

# =🚩 Introduction

Introduction

The “Python library” contains several different kinds of components.

It contains data types that would normally be considered part of the “core” of a language, such as numbers and lists. For these types, the Python language core defines the form of literals and places some constraints on their semantics, but does not fully define the semantics. (On the other hand, the language core does define syntactic properties like the spelling and priorities of operators.)

The library also contains built-in functions and exceptions — objects that can be used by all Python code without the need of an import statement. Some of these are defined by the core language, but many are not essential for the core semantics and are only described here.

The bulk of the library, however, consists of a collection of modules. There are many ways to dissect this collection. Some modules are written in C and built in to the Python interpreter; others are written in Python and imported in source form. Some modules provide interfaces that are highly specific to Python, like printing a stack trace; some provide interfaces that are specific to particular operating systems, such as access to specific hardware; others provide interfaces that are specific to a particular application domain, like the World Wide Web. Some modules are available in all versions and ports of Python; others are only available when the underlying system supports or requires them; yet others are available only when a particular configuration option was chosen at the time when Python was compiled and installed.

This manual is organized “from the inside out:” it first describes the built-in functions, data types and exceptions, and finally the modules, grouped in chapters of related modules.

This means that if you start reading this manual from the start, and skip to the next chapter when you get bored, you will get a reasonable overview of the available modules and application areas that are supported by the Python library. Of course, you don’t have to read it like a novel — you can also browse the table of contents (in front of the manual), or look for a specific function, module or term in the index (in the back). And finally, if you enjoy learning about random subjects, you choose a random page number (see module random) and read a section or two. Regardless of the order in which you read the sections of this manual, it helps to start with chapter Built-in Functions, as the remainder of the manual assumes familiarity with this material.

Let the show begin!

Notes on availability

• An “Availability: Unix” note means that this function is commonly found on Unix systems. It does not make any claims about its existence on a specific operating system.
• If not separately noted, all functions that claim “Availability: Unix” are supported on Mac OS X, which builds on a Unix core.


# =🚩 Built-in Functions
- https://docs.python.org/3.9/library/functions.html

71 Built-in Functions

|---------------|--------------|--------------|----------------|
|       A       |      E       |      L       |       R        |
|---------------|--------------|--------------|----------------|
| abs()         | enumerate()  | len()        | range()        |
| aiter()       | eval()       | list()       | repr()         |
| all()         | exec()       | locals()     | reversed()     |
| any()         |--------------|--------------| round()        |
| anext()       | F            | M            |----------------|
| ascii()       |              |              | S              |
|---------------| filter()     | map()        |                |
| B             | float()      | max()        | set()          |
|               | format()     | memoryview() | setattr()      |
| bin()         | frozenset()  | min()        | slice()        |
| bool()        |--------------|--------------| sorted()       |
| breakpoint()  | G            | N            | staticmethod() |
| bytearray()   |              |              | str()          |
| bytes()       | getattr()    | next()       | sum()          |
|---------------| globals()    |--------------| super()        |
| C             |--------------| O            |----------------|
|               | H            |              | T              |
| callable()    |              | object()     |                |
| chr()         | hasattr()    | oct()        | tuple()        |
| classmethod() | hash()       | open()       | type()         |
| compile()     | help()       | ord()        |----------------|
| complex()     | hex()        |--------------| V              |
|---------------|--------------| P            |                |
| D             | I            |              | vars()         |
|               |              | pow()        |----------------|
| delattr()     | id()         | print()      | Z              |
| dict()        | input()      | property()   |                |
| dir()         | int()        |              | zip()          |
| divmod()      | isinstance() |              |----------------|
|               | issubclass() |              | -              |
|               | iter()       |              |                |
|               |              |              | __import__()   |
|---------------|--------------|--------------|----------------|

Python 3.10 增加了两个枚举相关函数：

### ➡ `aiter(async_iterable)`
Return an asynchronous iterator for an asynchronous iterable. Equivalent to calling x.__aiter__().

Note: Unlike iter(), aiter() has no 2-argument variant.
New in version 3.10.


### ➡ `awaitable anext(async_iterator[, default])`
When awaited, return the next item from the given asynchronous iterator, or default if given and the iterator is exhausted.

This is the async variant of the next() builtin, and behaves similarly.

This calls the __anext__() method of async_iterator, returning an awaitable. Awaiting this returns the next value of the iterator. If default is given, it is returned if the iterator is exhausted, otherwise StopAsyncIteration is raised.


New in version 3.10.



### ➡ `abs(x)`
Return the absolute value of a number. The argument may be an integer, a floating point number, or an object implementing __abs__(). If the argument is a complex number, its magnitude is returned.
### ➡ `all(iterable)`
Return True if all elements of the iterable are true (or if the iterable is empty). Equivalent to:


```py
def all(iterable):
    for element in iterable:
        if not element:
            return False
    return True
```

### ➡ `any(iterable)`
Return True if any element of the iterable is true. If the iterable is empty, return False. Equivalent to:


```py
def any(iterable):
    for element in iterable:
        if element:
            return True
    return False
```

### ➡ `ascii(object)`
As repr(), return a string containing a printable representation of an object, but escape the non-ASCII characters in the string returned by repr() using \x, \u or \U escapes. This generates a string similar to that returned by repr() in Python 2.

### ➡ `bin(x)`
Convert an integer number to a binary string prefixed with “0b”. The result is a valid Python expression. If x is not a Python int object, it has to define an __index__() method that returns an integer. Some examples:


>>> bin(3)
'0b11'
>>> bin(-10)
'-0b1010'


If prefix “0b” is desired or not, you can use either of the following ways.


>>> format(14, '#b'), format(14, 'b')
('0b1110', '1110')
>>> f'{14:#b}', f'{14:b}'
('0b1110', '1110')

See also format() for more information.

### ✅ `class bool([x])`
Return a Boolean value, i.e. one of True or False. x is converted using the standard truth testing procedure. If x is false or omitted, this returns False; otherwise it returns True. The bool class is a subclass of int (see Numeric Types — int, float, complex). It cannot be subclassed further. Its only instances are False and True (see Boolean Values).

Changed in version 3.7: x is now a positional-only parameter.

### ➡ `breakpoint(*args, **kws)`
This function drops you into the debugger at the call site. Specifically, it calls sys.breakpointhook(), passing args and kws straight through. By default, sys.breakpointhook() calls pdb.set_trace() expecting no arguments. In this case, it is purely a convenience function so you don’t have to explicitly import pdb or type as much code to enter the debugger. However, sys.breakpointhook() can be set to some other function and breakpoint() will automatically call that, allowing you to drop into the debugger of choice.

Raises an auditing event builtins.breakpoint with argument breakpointhook.

New in version 3.7.

### ✅ `class bytearray([source[, encoding[, errors]]])`
Return a new array of bytes. The bytearray class is a mutable sequence of integers in the range 0 <= x < 256. It has most of the usual methods of mutable sequences, described in Mutable Sequence Types, as well as most methods that the bytes type has, see Bytes and Bytearray Operations.

The optional source parameter can be used to initialize the array in a few different ways:

• If it is a string, you must also give the encoding (and optionally, errors) parameters; bytearray() then converts the string to bytes using str.encode().

• If it is an integer, the array will have that size and will be initialized with null bytes.

• If it is an object conforming to the buffer interface, a read-only buffer of the object will be used to initialize the bytes array.

• If it is an iterable, it must be an iterable of integers in the range 0 <= x < 256, which are used as the initial contents of the array.

Without an argument, an array of size 0 is created.

See also Binary Sequence Types — bytes, bytearray, memoryview and Bytearray Objects.

### ✅ `class bytes([source[, encoding[, errors]]])`
Return a new “bytes” object, which is an immutable sequence of integers in the range 0 <= x < 256. bytes is an immutable version of bytearray – it has the same non-mutating methods and the same indexing and slicing behavior.

Accordingly, constructor arguments are interpreted as for bytearray().

Bytes objects can also be created with literals, see String and Bytes literals.

See also Binary Sequence Types — bytes, bytearray, memoryview, Bytes Objects, and Bytes and Bytearray Operations.

### ➡ `callable(object)`
Return True if the object argument appears callable, False if not. If this returns True, it is still possible that a call fails, but if it is False, calling object will never succeed. Note that classes are callable (calling a class returns a new instance); instances are callable if their class has a __call__() method.

New in version 3.2: This function was first removed in Python 3.0 and then brought back in Python 3.2.

### ➡ `chr(i)`
Return the string representing a character whose Unicode code point is the integer i. For example, chr(97) returns the string 'a', while chr(8364) returns the string '€'. This is the inverse of ord().

The valid range for the argument is from 0 through 1,114,111 (0x10FFFF in base 16). ValueError will be raised if i is outside that range.
### ➡ `@classmethod`
Transform a method into a class method.

A class method receives the class as implicit first argument, just like an instance method receives the instance. To declare a class method, use this idiom:


    class C:
        @classmethod
        def f(cls, arg1, arg2, ...): ...


The @classmethod form is a function decorator – see Function definitions for details.

A class method can be called either on the class (such as C.f()) or on an instance (such as C().f()). The instance is ignored except for its class. If a class method is called for a derived class, the derived class object is passed as the implied first argument.

Class methods are different than C++ or Java static methods. If you want those, see staticmethod() in this section. For more information on class methods, see The standard type hierarchy.

Changed in version 3.9: Class methods can now wrap other descriptors such as property().

### ➡ `compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1)`
Compile the source into a code or AST object. Code objects can be executed by exec() or eval(). source can either be a normal string, a byte string, or an AST object. Refer to the ast module documentation for information on how to work with AST objects.

The filename argument should give the file from which the code was read; pass some recognizable value if it wasn’t read from a file ('<string>' is commonly used).

The mode argument specifies what kind of code must be compiled; it can be 'exec' if source consists of a sequence of statements, 'eval' if it consists of a single expression, or 'single' if it consists of a single interactive statement (in the latter case, expression statements that evaluate to something other than None will be printed).

The optional arguments flags and dont_inherit control which compiler options should be activated and which future features should be allowed. If neither is present (or both are zero) the code is compiled with the same flags that affect the code that is calling compile(). If the flags argument is given and dont_inherit is not (or is zero) then the compiler options and the future statements specified by the flags argument are used in addition to those that would be used anyway. If dont_inherit is a non-zero integer then the flags argument is it – the flags (future features and compiler options) in the surrounding code are ignored.

Compiler options and future statements are specified by bits which can be bitwise ORed together to specify multiple options. The bitfield required to specify a given future feature can be found as the compiler_flag attribute on the _Feature instance in the __future__ module. Compiler flags can be found in ast module, with PyCF_ prefix.

The argument optimize specifies the optimization level of the compiler; the default value of -1 selects the optimization level of the interpreter as given by -O options. Explicit levels are 0 (no optimization; __debug__ is true), 1 (asserts are removed, __debug__ is false) or 2 (docstrings are removed too).

This function raises SyntaxError if the compiled source is invalid, and ValueError if the source contains null bytes.

If you want to parse Python code into its AST representation, see ast.parse().

Raises an auditing event compile with arguments source and filename. This event may also be raised by implicit compilation.

Note:
 When compiling a string with multi-line code in 'single' or 'eval' mode, input must be terminated by at least one newline character. This is to facilitate detection of incomplete and complete statements in the code module.
 

Warning:
 It is possible to crash the Python interpreter with a sufficiently large/complex string when compiling to an AST object due to stack depth limitations in Python’s AST compiler.
 

Changed in version 3.2: Allowed use of Windows and Mac newlines. Also input in 'exec' mode does not have to end in a newline anymore. Added the optimize parameter.

Changed in version 3.5: Previously, TypeError was raised when null bytes were encountered in source.

New in version 3.8: ast.PyCF_ALLOW_TOP_LEVEL_AWAIT can now be passed in flags to enable support for top-level await, async for, and async with.


### ✅ `class complex([real[, imag]])`
Return a complex number with the value `real + imag*1j` or convert a string or number to a complex number. If the first parameter is a string, it will be interpreted as a complex number and the function must be called without a second parameter. The second parameter can never be a string. Each argument may be any numeric type (including complex). If imag is omitted, it defaults to zero and the constructor serves as a numeric conversion like int and float. If both arguments are omitted, returns 0j.

For a general Python object x, complex(x) delegates to x.__complex__(). If __complex__() is not defined then it falls back to __float__(). If __float__() is not defined then it falls back to __index__().

Note:
 When converting from a string, the string must not contain whitespace around the central + or - operator. For example, complex('1+2j') is fine, but complex('1 + 2j') raises ValueError.
 

The complex type is described in Numeric Types — int, float, complex.

Changed in version 3.6: Grouping digits with underscores as in code literals is allowed.

Changed in version 3.8: Falls back to __index__() if __complex__() and __float__() are not defined.


### ➡ `delattr(object, name)`
This is a relative of setattr(). The arguments are an object and a string. The string must be the name of one of the object’s attributes. The function deletes the named attribute, provided the object allows it. For example, delattr(x, 'foobar') is equivalent to del x.foobar.

### ✅ `class dict(**kwarg)`
### ✅ `class dict(mapping, **kwarg)`
### ✅ `class dict(iterable, **kwarg)`
Create a new dictionary. The dict object is the dictionary class. See dict and Mapping Types — dict for documentation about this class.

For other containers see the built-in list, set, and tuple classes, as well as the collections module.
### ➡ `dir([object])`
Without arguments, return the list of names in the current local scope. With an argument, attempt to return a list of valid attributes for that object.

If the object has a method named __dir__(), this method will be called and must return the list of attributes. This allows objects that implement a custom __getattr__() or __getattribute__() function to customize the way dir() reports their attributes.

If the object does not provide __dir__(), the function tries its best to gather information from the object’s __dict__ attribute, if defined, and from its type object. The resulting list is not necessarily complete, and may be inaccurate when the object has a custom __getattr__().

The default dir() mechanism behaves differently with different types of objects, as it attempts to produce the most relevant, rather than complete, information:
•If the object is a module object, the list contains the names of the module’s attributes.
•If the object is a type or class object, the list contains the names of its attributes, and recursively of the attributes of its bases.
•Otherwise, the list contains the object’s attributes’ names, the names of its class’s attributes, and recursively of the attributes of its class’s base classes.

The resulting list is sorted alphabetically. For example:


>>> import struct
>>> dir()   # show the names in the module namespace  
['__builtins__', '__name__', 'struct']
>>> dir(struct)   # show the names in the struct module 
['Struct', '__all__', '__builtins__', '__cached__', '__doc__', '__file__',
 '__initializing__', '__loader__', '__name__', '__package__',
 '_clearcache', 'calcsize', 'error', 'pack', 'pack_into',
 'unpack', 'unpack_from']
>>> class Shape:
...     def __dir__(self):
...         return ['area', 'perimeter', 'location']
>>> s = Shape()
>>> dir(s)
['area', 'location', 'perimeter']


Note:
 Because dir() is supplied primarily as a convenience for use at an interactive prompt, it tries to supply an interesting set of names more than it tries to supply a rigorously or consistently defined set of names, and its detailed behavior may change across releases. For example, metaclass attributes are not in the result list when the argument is a class.
 
### ➡ `divmod(a, b)`
Take two (non complex) numbers as arguments and return a pair of numbers consisting of their quotient and remainder when using integer division. With mixed operand types, the rules for binary arithmetic operators apply. For integers, the result is the same as (a // b, a % b). For floating point numbers the result is (q, a % b), where q is usually math.floor(a / b) but may be 1 less than that. In any case q * b + a % b is very close to a, if a % b is non-zero it has the same sign as b, and 0 <= abs(a % b) < abs(b).

### ➡ `enumerate(iterable, start=0)`
Return an enumerate object. iterable must be a sequence, an iterator, or some other object which supports iteration. The __next__() method of the iterator returned by enumerate() returns a tuple containing a count (from start which defaults to 0) and the values obtained from iterating over iterable.


>>> seasons = ['Spring', 'Summer', 'Fall', 'Winter']
>>> list(enumerate(seasons))
[(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]
>>> list(enumerate(seasons, start=1))
[(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')]


Equivalent to:


    def enumerate(sequence, start=0):
        n = start
        for elem in sequence:
            yield n, elem
            n += 1

### ➡ `eval(expression[, globals[, locals]])`
The arguments are a string and optional globals and locals. If provided, globals must be a dictionary. If provided, locals can be any mapping object.

The expression argument is parsed and evaluated as a Python expression (technically speaking, a condition list) using the globals and locals dictionaries as global and local namespace. If the globals dictionary is present and does not contain a value for the key __builtins__, a reference to the dictionary of the built-in module builtins is inserted under that key before expression is parsed. This means that expression normally has full access to the standard builtins module and restricted environments are propagated. If the locals dictionary is omitted it defaults to the globals dictionary. If both dictionaries are omitted, the expression is executed with the globals and locals in the environment where eval() is called. Note, eval() does not have access to the nested scopes (non-locals) in the enclosing environment.

The return value is the result of the evaluated expression. Syntax errors are reported as exceptions. Example:


>>> x = 1
>>> eval('x+1')
2

This function can also be used to execute arbitrary code objects (such as those created by compile()). In this case pass a code object instead of a string. If the code object has been compiled with 'exec' as the mode argument, eval()’s return value will be None.

Hints: dynamic execution of statements is supported by the exec() function. The globals() and locals() functions returns the current global and local dictionary, respectively, which may be useful to pass around for use by eval() or exec().

See ast.literal_eval() for a function that can safely evaluate strings with expressions containing only literals.

Raises an auditing event exec with the code object as the argument. Code compilation events may also be raised.

### ➡ `exec(object[, globals[, locals]])`
This function supports dynamic execution of Python code. object must be either a string or a code object. If it is a string, the string is parsed as a suite of Python statements which is then executed (unless a syntax error occurs). [1] If it is a code object, it is simply executed. In all cases, the code that’s executed is expected to be valid as file input (see the section “File input” in the Reference Manual). Be aware that the nonlocal, yield, and return statements may not be used outside of function definitions even within the context of code passed to the exec() function. The return value is None.

In all cases, if the optional parts are omitted, the code is executed in the current scope. If only globals is provided, it must be a dictionary (and not a subclass of dictionary), which will be used for both the global and the local variables. If globals and locals are given, they are used for the global and local variables, respectively. If provided, locals can be any mapping object. Remember that at module level, globals and locals are the same dictionary. If exec gets two separate objects as globals and locals, the code will be executed as if it were embedded in a class definition.

If the globals dictionary does not contain a value for the key __builtins__, a reference to the dictionary of the built-in module builtins is inserted under that key. That way you can control what builtins are available to the executed code by inserting your own __builtins__ dictionary into globals before passing it to exec().

Raises an auditing event exec with the code object as the argument. Code compilation events may also be raised.

Note:
 The built-in functions globals() and locals() return the current global and local dictionary, respectively, which may be useful to pass around for use as the second and third argument to exec().
 

Note:
 The default locals act as described for function locals() below: modifications to the default locals dictionary should not be attempted. Pass an explicit locals dictionary if you need to see effects of the code on locals after function exec() returns.
 
### ➡ `filter(function, iterable)`
Construct an iterator from those elements of iterable for which function returns true. iterable may be either a sequence, a container which supports iteration, or an iterator. If function is None, the identity function is assumed, that is, all elements of iterable that are false are removed.

Note that filter(function, iterable) is equivalent to the generator expression (item for item in iterable if function(item)) if function is not None and (item for item in iterable if item) if function is None.

See itertools.filterfalse() for the complementary function that returns elements of iterable for which function returns false.

### ✅ `class float([x])`
Return a floating point number constructed from a number or string x.

If the argument is a string, it should contain a decimal number, optionally preceded by a sign, and optionally embedded in whitespace. The optional sign may be '+' or '-'; a '+' sign has no effect on the value produced. The argument may also be a string representing a NaN (not-a-number), or a positive or negative infinity. More precisely, the input must conform to the following grammar after leading and trailing whitespace characters are removed:

sign           ::=  "+" | "-"
infinity       ::=  "Infinity" | "inf"
nan            ::=  "nan"
numeric_value  ::=  floatnumber | infinity | nan
numeric_string ::=  [sign] numeric_value


Here floatnumber is the form of a Python floating-point literal, described in Floating point literals. Case is not significant, so, for example, “inf”, “Inf”, “INFINITY” and “iNfINity” are all acceptable spellings for positive infinity.

Otherwise, if the argument is an integer or a floating point number, a floating point number with the same value (within Python’s floating point precision) is returned. If the argument is outside the range of a Python float, an OverflowError will be raised.

For a general Python object x, float(x) delegates to x.__float__(). If __float__() is not defined then it falls back to __index__().

If no argument is given, 0.0 is returned.

Examples:


>>> float('+1.23')
1.23
>>> float('   -12345\n')
-12345.0
>>> float('1e-003')
0.001
>>> float('+1E6')
1000000.0
>>> float('-Infinity')
-inf


The float type is described in Numeric Types — int, float, complex.

Changed in version 3.6: Grouping digits with underscores as in code literals is allowed.

Changed in version 3.7: x is now a positional-only parameter.

Changed in version 3.8: Falls back to __index__() if __float__() is not defined.


### ➡ `format(value[, format_spec])`
Convert a value to a “formatted” representation, as controlled by format_spec. The interpretation of format_spec will depend on the type of the value argument, however there is a standard formatting syntax that is used by most built-in types: Format Specification Mini-Language.

The default format_spec is an empty string which usually gives the same effect as calling str(value).

A call to format(value, format_spec) is translated to type(value).__format__(value, format_spec) which bypasses the instance dictionary when searching for the value’s __format__() method. A TypeError exception is raised if the method search reaches object and the format_spec is non-empty, or if either the format_spec or the return value are not strings.

Changed in version 3.4: object().__format__(format_spec) raises TypeError if format_spec is not an empty string.

### ✅ `class frozenset([iterable])`
Return a new frozenset object, optionally with elements taken from iterable. frozenset is a built-in class. See frozenset and Set Types — set, frozenset for documentation about this class.

For other containers see the built-in set, list, tuple, and dict classes, as well as the collections module.

### ➡ `getattr(object, name[, default])`
Return the value of the named attribute of object. name must be a string. If the string is the name of one of the object’s attributes, the result is the value of that attribute. For example, getattr(x, 'foobar') is equivalent to x.foobar. If the named attribute does not exist, default is returned if provided, otherwise AttributeError is raised.

### ➡ `globals()`
Return a dictionary representing the current global symbol table. This is always the dictionary of the current module (inside a function or method, this is the module where it is defined, not the module from which it is called).

### ➡ `hasattr(object, name)`
The arguments are an object and a string. The result is True if the string is the name of one of the object’s attributes, False if The arguments are an object and a string. The result is True if the string is the name of one of the object’s attributes, False if not. (This is implemented by calling getattr(object, name) and seeing whether it raises an AttributeError or not.)

### ➡ `hash(object)`
Return the hash value of the object (if it has one). Hash values are integers. They are used to quickly compare dictionary keys during a dictionary lookup. Numeric values that compare equal have the same hash value (even if they are of different types, as is the case for 1 and 1.0).

Note:
 For objects with custom __hash__() methods, note that hash() truncates the return value based on the bit width of the host machine. See __hash__() for details.
 
### ➡ `help([object])`
Invoke the built-in help system. (This function is intended for interactive use.) If no argument is given, the interactive help system starts on the interpreter console. If the argument is a string, then the string is looked up as the name of a module, function, class, method, keyword, or documentation topic, and a help page is printed on the console. If the argument is any other kind of object, a help page on the object is generated.

Note that if a slash(/) appears in the parameter list of a function, when invoking help(), it means that the parameters prior to the slash are positional-only. For more info, see the FAQ entry on positional-only parameters.

This function is added to the built-in namespace by the site module.

Changed in version 3.4: Changes to pydoc and inspect mean that the reported signatures for callables are now more comprehensive and consistent.

### ➡ `hex(x)`
Convert an integer number to a lowercase hexadecimal string prefixed with “0x”. If x is not a Python int object, it has to define an __index__() method that returns an integer. Some examples:


>>> hex(255)
'0xff'
>>> hex(-42)
'-0x2a'


If you want to convert an integer number to an uppercase or lower hexadecimal string with prefix or not, you can use either of the following ways:


>>> '%#x' % 255, '%x' % 255, '%X' % 255
('0xff', 'ff', 'FF')
>>> format(255, '#x'), format(255, 'x'), format(255, 'X')
('0xff', 'ff', 'FF')
>>> f'{255:#x}', f'{255:x}', f'{255:X}'
('0xff', 'ff', 'FF')


See also format() for more information.

See also int() for converting a hexadecimal string to an integer using a base of 16.

Note:
 To obtain a hexadecimal string representation for a float, use the float.hex() method.
 
### ➡ `id(object)`
Return the “identity” of an object. This is an integer which is guaranteed to be unique and constant for this object during its lifetime. Two objects with non-overlapping lifetimes may have the same id() value.


CPython implementation detail: This is the address of the object in memory.

Raises an auditing event builtins.id with argument id.

### ➡ `input([prompt])`
If the prompt argument is present, it is written to standard output without a trailing newline. The function then reads a line from input, converts it to a string (stripping a trailing newline), and returns that. When EOF is read, EOFError is raised. Example:


>>> s = input('--> ')  
--> Monty Python's Flying Circus
>>> s  
"Monty Python's Flying Circus"


If the readline module was loaded, then input() will use it to provide elaborate line editing and history features.

Raises an auditing event builtins.input with argument prompt before reading input

Raises an auditing event builtins.input/result with the result after successfully reading input.

### ✅ `class int([x])`
### ✅ `class int(x, base=10)`
Return an integer object constructed from a number or string x, or return 0 if no arguments are given. If x defines __int__(), int(x) returns x.__int__(). If x defines __index__(), it returns x.__index__(). If x defines __trunc__(), it returns x.__trunc__(). For floating point numbers, this truncates towards zero.

If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in radix base. Optionally, the literal can be preceded by + or - (with no space in between) and surrounded by whitespace. A base-n literal consists of the digits 0 to n-1, with a to z (or A to Z) having values 10 to 35. The default base is 10. The allowed values are 0 and 2–36. Base-2, -8, and -16 literals can be optionally prefixed with 0b/0B, 0o/0O, or 0x/0X, as with integer literals in code. Base 0 means to interpret exactly as a code literal, so that the actual base is 2, 8, 10, or 16, and so that int('010', 0) is not legal, while int('010') is, as well as int('010', 8).

The integer type is described in Numeric Types — int, float, complex.

Changed in version 3.4: If base is not an instance of int and the base object has a base.__index__ method, that method is called to obtain an integer for the base. Previous versions used base.__int__ instead of base.__index__.
Changed in version 3.6: Grouping digits with underscores as in code literals is allowed.
Changed in version 3.7: x is now a positional-only parameter.
Changed in version 3.8: Falls back to __index__() if __int__() is not defined.



### ➡ `isinstance(object, classinfo)`
Return True if the object argument is an instance of the classinfo argument, or of a (direct, indirect or virtual) subclass thereof. If object is not an object of the given type, the function always returns False. If classinfo is a tuple of type objects (or recursively, other such tuples), return True if object is an instance of any of the types. If classinfo is not a type or tuple of types and such tuples, a TypeError exception is raised.

### ➡ `issubclass(class, classinfo)`
Return True if class is a subclass (direct, indirect or virtual) of classinfo. A class is considered a subclass of itself. classinfo may be a tuple of class objects, in which case every entry in classinfo will be checked. In any other case, a TypeError exception is raised.

### ➡ `iter(object[, sentinel])`
Return an iterator object. The first argument is interpreted very differently depending on the presence of the second argument. Without a second argument, object must be a collection object which supports the iteration protocol (the __iter__() method), or it must support the sequence protocol (the __getitem__() method with integer arguments starting at 0). If it does not support either of those protocols, TypeError is raised. If the second argument, sentinel, is given, then object must be a callable object. The iterator created in this case will call object with no arguments for each call to its __next__() method; if the value returned is equal to sentinel, StopIteration will be raised, otherwise the value will be returned.

See also Iterator Types.

One useful application of the second form of iter() is to build a block-reader. For example, reading fixed-width blocks from a binary database file until the end of file is reached:


    from functools import partial
    with open('mydata.db', 'rb') as f:
        for block in iter(partial(f.read, 64), b''):
            process_block(block)

### ➡ `len(s)`
Return the length (the number of items) of an object. The argument may be a sequence (such as a string, bytes, tuple, list, or range) or a collection (such as a dictionary, set, or frozen set).


CPython implementation detail: len raises OverflowError on lengths larger than sys.maxsize, such as range(2 ** 100).

### ✅ `class list([iterable])`
Rather than being a function, list is actually a mutable sequence type, as documented in Lists and Sequence Types — list, tuple, range.

### ➡ `locals()`
Update and return a dictionary representing the current local symbol table. Free variables are returned by locals() when it is called in function blocks, but not in class blocks. Note that at the module level, locals() and globals() are the same dictionary.

Note:
 The contents of this dictionary should not be modified; changes may not affect the values of local and free variables used by the interpreter.
 
### ➡ `map(function, iterable, ...)`
Return an iterator that applies function to every item of iterable, yielding the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel. With multiple iterables, the iterator stops when the shortest iterable is exhausted. For cases where the function inputs are already arranged into argument tuples, see itertools.starmap().

### ➡ `max(iterable, *[, key, default])`
### ➡ `max(arg1, arg2, *args[, key])`
Return the largest item in an iterable or the largest of two or more arguments.

If one positional argument is provided, it should be an iterable. The largest item in the iterable is returned. If two or more positional arguments are provided, the largest of the positional arguments is returned.

There are two optional keyword-only arguments. The key argument specifies a one-argument ordering function like that used for list.sort(). The default argument specifies an object to return if the provided iterable is empty. If the iterable is empty and default is not provided, a ValueError is raised.

If multiple items are maximal, the function returns the first one encountered. This is consistent with other sort-stability preserving tools such as sorted(iterable, key=keyfunc, reverse=True)[0] and heapq.nlargest(1, iterable, key=keyfunc).

New in version 3.4: The default keyword-only argument.

Changed in version 3.8: The key can be None.


### ✅ `class memoryview(obj)`
Return a “memory view” object created from the given argument. See Memory Views for more information.

### ➡ `min(iterable, *[, key, default])`
### ➡ `min(arg1, arg2, *args[, key])`
Return the smallest item in an iterable or the smallest of two or more arguments.

If one positional argument is provided, it should be an iterable. The smallest item in the iterable is returned. If two or more positional arguments are provided, the smallest of the positional arguments is returned.

There are two optional keyword-only arguments. The key argument specifies a one-argument ordering function like that used for list.sort(). The default argument specifies an object to return if the provided iterable is empty. If the iterable is empty and default is not provided, a ValueError is raised.

If multiple items are minimal, the function returns the first one encountered. This is consistent with other sort-stability preserving tools such as sorted(iterable, key=keyfunc)[0] and heapq.nsmallest(1, iterable, key=keyfunc).

New in version 3.4: The default keyword-only argument.

Changed in version 3.8: The key can be None.


### ➡ `next(iterator[, default])`
Retrieve the next item from the iterator by calling its __next__() method. If default is given, it is returned if the iterator is exhausted, otherwise StopIteration is raised.
class object
Return a new featureless object. object is a base for all classes. It has the methods that are common to all instances of Python classes. This function does not accept any arguments.

Note:
 object does not have a __dict__, so you can’t assign arbitrary attributes to an instance of the object class.
 
### ➡ `oct(x)`
Convert an integer number to an octal string prefixed with “0o”. The result is a valid Python expression. If x is not a Python int object, it has to define an __index__() method that returns an integer. For example:


>>> oct(8)
'0o10'
>>> oct(-56)
'-0o70'


If you want to convert an integer number to octal string either with prefix “0o” or not, you can use either of the following ways.


>>> '%#o' % 10, '%o' % 10
('0o12', '12')
>>> format(10, '#o'), format(10, 'o')
('0o12', '12')
>>> f'{10:#o}', f'{10:o}'
('0o12', '12')


See also format() for more information.



### ➡ `open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)`
Open file and return a corresponding file object. If the file cannot be opened, an OSError is raised. See Reading and Writing Files for more examples of how to use this function.

↪ `file` is a path-like object giving the pathname (absolute or relative to the current working directory) of the file to be opened or an integer file descriptor of the file to be wrapped. (If a file descriptor is given, it is closed when the returned I/O object is file is a path-like object giving the pathname (absolute or relative to the current working directory) of the file to be opened or an integer file descriptor of the file to be wrapped. (If a file descriptor is given, it is closed when the returned I/O object is closed, unless closefd is set to False.)

↪ `mode` is an optional string that specifies the mode in which the file is opened. It defaults to 'r' which means open for reading in text mode. Other common values are 'w' for writing (truncating the file if it already exists), 'x' for exclusive creation and 'a' for appending (which on some Unix systems, means that all writes append to the end of the file regardless of the current seek position). In text mode, if encoding is not specified the encoding used is platform dependent: locale.getpreferredencoding(False) is called to get the current locale encoding. (For reading and writing raw bytes use binary mode and leave encoding unspecified.) The available modes are:
 
| Character |                             Meaning                             |
|-----------|-----------------------------------------------------------------|
| 'r'       | open for reading (default)                                      |
| 'w'       | open for writing, truncating the file first                     |
| 'x'       | open for exclusive creation, failing if the file already exists |
| 'a'       | open for writing, appending to the end of the file if it exists |
| 'b'       | binary mode                                                     |
| 't'       | text mode (default)                                             |
| '+'       | open for updating (reading and writing)                         |

The default mode is 'r' (open for reading text, synonym of 'rt'). Modes 'w+' and 'w+b' open and truncate the file. Modes 'r+' and 'r+b' open the file with no truncation.

As mentioned in the Overview, Python distinguishes between binary and text I/O. Files opened in binary mode (including 'b' in the mode argument) return contents as bytes objects without any decoding. In text mode (the default, or when 't' is included in the mode argument), the contents of the file are returned as str, the bytes having been first decoded using a platform-dependent encoding or using the specified encoding if given.

There is an additional mode character permitted, 'U', which no longer has any effect, and is considered deprecated. It previously enabled universal newlines in text mode, which became the default behaviour in Python 3.0. Refer to the documentation of the newline parameter for further details.

Note:
 Python doesn’t depend on the underlying operating system’s notion of text files; all the processing is done by Python itself, and is therefore platform-independent.
 

↪ `buffering` is an optional integer used to set the buffering policy. Pass 0 to switch buffering off (only allowed in binary mode), 1 to select line buffering (only usable in text mode), and an integer > 1 to indicate the size in bytes of a fixed-size chunk buffer. When no buffering argument is given, the default buffering policy works as follows:
•Binary files are buffered in fixed-size chunks; the size of the buffer is chosen using a heuristic trying to determine the underlying device’s “block size” and falling back on io.DEFAULT_BUFFER_SIZE. On many systems, the buffer will typically be 4096 or 8192 bytes long.
•“Interactive” text files (files for which isatty() returns True) use line buffering. Other text files use the policy described above for binary files.

↪ `encoding` is the name of the encoding used to decode or encode the file. This should only be used in text mode. The default encoding is platform dependent (whatever locale.getpreferredencoding() returns), but any text encoding supported by Python can be used. See the codecs module for the list of supported encodings.

↪ `errors` is an optional string that specifies how encoding and decoding errors are to be handled—this cannot be used in binary mode. A variety of standard error handlers are available (listed under Error Handlers), though any error handling name that has been registered with codecs.register_error() is also valid. The standard names include:
•'strict' to raise a ValueError exception if there is an encoding error. The default value of None has the same effect.
•'ignore' ignores errors. Note that ignoring encoding errors can lead to data loss.
•'replace' causes a replacement marker (such as '?') to be inserted where there is malformed data.
•'surrogateescape' will represent any incorrect bytes as code points in the Unicode Private Use Area ranging from U+DC80 to U+DCFF. These private code points will then be turned back into the same bytes when the surrogateescape error handler is used when writing data. This is useful for processing files in an unknown encoding.
•'xmlcharrefreplace' is only supported when writing to a file. Characters not supported by the encoding are replaced with the appropriate XML character reference &#nnn;.
•'backslashreplace' replaces malformed data by Python’s backslashed escape sequences.
•'namereplace' (also only supported when writing) replaces unsupported characters with \N{...} escape sequences.

↪ `newline` controls how universal newlines mode works (it only applies to text mode). It can be None, '', '\n', '\r', and '\r\n'. It works as follows:
•When reading input from the stream, if newline is None, universal newlines mode is enabled. Lines in the input can end in '\n', '\r', or '\r\n', and these are translated into '\n' before being returned to the caller. If it is '', universal newlines mode is enabled, but line endings are returned to the caller untranslated. If it has any of the other legal values, input lines are only terminated by the given string, and the line ending is returned to the caller untranslated.
•When writing output to the stream, if newline is None, any '\n' characters written are translated to the system default line separator, os.linesep. If newline is '' or '\n', no translation takes place. If newline is any of the other legal values, any '\n' characters written are translated to the given string.

↪ `closefd` If it is False and a file descriptor rather than a filename was given, the underlying file descriptor will be kept open when the file is closed. If a filename is given closefd must be True (the default) otherwise an error will be raised.

↪ A custom `opener` can be used by passing a callable as opener. The underlying file descriptor for the file object is then obtained by calling opener with (file, flags). opener must return an open file descriptor (passing os.open as opener results in functionality similar to passing None).

The newly created file is non-inheritable.

The following example uses the `dir_fd` parameter of the `os.open()` function to open a file relative to a given directory:


>>> import os
>>> dir_fd = os.open('somedir', os.O_RDONLY)
>>> def opener(path, flags):
...     return os.open(path, flags, dir_fd=dir_fd)
...
>>> with open('spamspam.txt', 'w', opener=opener) as f:
...     print('This will be written to somedir/spamspam.txt', file=f)
...
>>> os.close(dir_fd)  # don't leak a file descriptor


The type of file object returned by the open() function depends on the mode. When open() is used to open a file in a text mode ('w', 'r', 'wt', 'rt', etc.), it returns a subclass of io.TextIOBase (specifically io.TextIOWrapper). When used to open a file in a binary mode with buffering, the returned class is a subclass of io.BufferedIOBase. The exact class varies: in read binary mode, it returns an io.BufferedReader; in write binary and append binary modes, it returns an io.BufferedWriter, and in read/write mode, it returns an io.BufferedRandom. When buffering is disabled, the raw stream, a subclass of io.RawIOBase, io.FileIO, is returned.

See also the file handling modules, such as, fileinput, io (where open() is declared), os, os.path, tempfile, and shutil.

Raises an auditing event open with arguments file, mode, flags.

The mode and flags arguments may have been modified or inferred from the original call.


Changed in version 3.3:  

•The opener parameter was added.
•The 'x' mode was added.
•IOError used to be raised, it is now an alias of OSError.
•FileExistsError is now raised if the file opened in exclusive creation mode ('x') already exists.


Changed in version 3.4:  
•The file is now non-inheritable.
Deprecated since version 3.4, will be removed in version 3.10: The 'U' mode.
Changed in version 3.5:  
•If the system call is interrupted and the signal handler does not raise an exception, the function now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the rationale).

•The 'namereplace' error handler was added.


Changed in version 3.6:  •Support added to accept objects implementing os.PathLike.

•On Windows, opening a console buffer may return a subclass of io.RawIOBase other than io.FileIO.

### ➡ `ord(c)`
Given a string representing one Unicode character, return an integer representing the Unicode code point of that character. For example, ord('a') returns the integer 97 and ord('€') (Euro sign) returns 8364. This is the inverse of chr().

### ➡ `pow(base, exp[, mod])`
Return base to the power exp; if mod is present, return base to the power exp, modulo mod (computed more efficiently than pow(base, exp) % mod). The two-argument form pow(base, exp) is equivalent to using the power operator: `base**exp`.

The arguments must have numeric types. With mixed operand types, the coercion rules for binary arithmetic operators apply. For int operands, the result has the same type as the operands (after coercion) unless the second argument is negative; in that case, all arguments are converted to float and a float result is delivered. For example, 10**2 returns 100, but 10**-2 returns 0.01.

For int operands base and exp, if mod is present, mod must also be of integer type and mod must be nonzero. If mod is present and exp is negative, base must be relatively prime to mod. In that case, pow(inv_base, -exp, mod) is returned, where inv_base is an inverse to base modulo mod.

Here’s an example of computing an inverse for 38 modulo 97:


>>> pow(38, -1, mod=97)
23
>>> 23 * 38 % 97 == 1
True


Changed in version 3.8: For int operands, the three-argument form of pow now allows the second argument to be negative, permitting computation of modular inverses.


Changed in version 3.8: Allow keyword arguments. Formerly, only positional arguments were supported.

### ➡ `print(*objects, sep=' ', end='\n', file=sys.stdout, flush=False)`
Print objects to the text stream file, separated by sep and followed by end. sep, end, file and flush, if present, must be given as keyword arguments.

All non-keyword arguments are converted to strings like str() does and written to the stream, separated by sep and followed by end. Both sep and end must be strings; they can also be None, which means to use the default values. If no objects are given, print() will just write end.

The file argument must be an object with a write(string) method; if it is not present or None, sys.stdout will be used. Since printed arguments are converted to text strings, print() cannot be used with binary mode file objects. For these, use file.write(...) instead.

Whether output is buffered is usually determined by file, but if the flush keyword argument is true, the stream is forcibly flushed.

Changed in version 3.3: Added the flush keyword argument.

### ✅ `class property(fget=None, fset=None, fdel=None, doc=None)`
Return a property attribute.

fget is a function for getting an attribute value. fset is a function for setting an attribute value. fdel is a function for deleting an attribute value. And doc creates a docstring for the attribute.

A typical use is to define a managed attribute x:


```py
class C:
    def __init__(self):
        self._x = None

    def getx(self):
        return self._x

    def setx(self, value):
        self._x = value

    def delx(self):
        del self._x

    x = property(getx, setx, delx, "I'm the 'x' property.")
```


If c is an instance of C, c.x will invoke the getter, c.x = value will invoke the setter and del c.x the deleter.

If given, doc will be the docstring of the property attribute. Otherwise, the property will copy fget’s docstring (if it exists). This makes it possible to create read-only properties easily using property() as a decorator:


```py
class Parrot:
    def __init__(self):
        self._voltage = 100000

    @property
    def voltage(self):
        """Get the current voltage."""
        return self._voltage
```


The `@property` decorator turns the voltage() method into a “getter” for a read-only attribute with the same name, and it sets the docstring for voltage to “Get the current voltage.”

A property object has getter, setter, and deleter methods usable as decorators that create a copy of the property with the corresponding accessor function set to the decorated function. This is best explained with an example:


```py
class C:
    def __init__(self):
        self._x = None

    @property
    def x(self):
        """I'm the 'x' property."""
        return self._x

    @x.setter
    def x(self, value):
        self._x = value

    @x.deleter
    def x(self):
        del self._x
```

This code is exactly equivalent to the first example. Be sure to give the additional functions the same name as the original property (x in this case.)

The returned property object also has the attributes fget, fset, and fdel corresponding to the constructor arguments.

Changed in version 3.5: The docstrings of property objects are now writeable.

### ✅ `class range(stop)`
### ✅ `class range(start, stop[, step])`
Rather than being a function, range is actually an immutable sequence type, as documented in Ranges and Sequence Types — list, tuple, range.

### ➡ `repr(object)`
Return a string containing a printable representation of an object. For many types, this function makes an attempt to return a string that would yield an object with the same value when passed to eval(), otherwise the representation is a string enclosed in angle brackets that contains the name of the type of the object together with additional information often including the name and address of the object. A class can control what this function returns for its instances by defining a __repr__() method.

### ➡ `reversed(seq)`
Return a reverse iterator. seq must be an object which has a __reversed__() method or supports the sequence protocol (the __len__() method and the __getitem__() method with integer arguments starting at 0).

### ➡ `round(number[, ndigits])`
Return number rounded to ndigits precision after the decimal point. If ndigits is omitted or is None, it returns the nearest integer to its input.

For the built-in types supporting round(), values are rounded to the closest multiple of 10 to the power minus ndigits; if two multiples are equally close, rounding is done toward the even choice (so, for example, both round(0.5) and round(-0.5) are 0, and round(1.5) is 2). Any integer value is valid for ndigits (positive, zero, or negative). The return value is an integer if ndigits is omitted or None. Otherwise the return value has the same type as number.

For a general Python object number, round delegates to number.__round__.

Note:
 The behavior of round() for floats can be surprising: for example, round(2.675, 2) gives 2.67 instead of the expected 2.68. This is not a bug: it’s a result of the fact that most decimal fractions can’t be represented exactly as a float. See Floating Point Arithmetic: Issues and Limitations for more information.
 
### ✅ `class set([iterable])`
Return a new set object, optionally with elements taken from iterable. set is a built-in class. See set and Set Types — set, frozenset for documentation about this class.

For other containers see the built-in frozenset, list, tuple, and dict classes, as well as the collections module.

### ➡ `setattr(object, name, value)`
This is the counterpart of getattr(). The arguments are an object, a string and an arbitrary value. The string may name an existing attribute or a new attribute. The function assigns the value to the attribute, provided the object allows it. For example, setattr(x, 'foobar', 123) is equivalent to x.foobar = 123.

### ✅ `class slice(stop)`
### ✅ `class slice(start, stop[, step])`
Return a slice object representing the set of indices specified by range(start, stop, step). The start and step arguments default to None. Slice objects have read-only data attributes start, stop and step which merely return the argument values (or their default). They have no other explicit functionality; however they are used by Numerical Python and other third party extensions. Slice objects are also generated when extended indexing syntax is used. For example: a[start:stop:step] or a[start:stop, i]. See itertools.islice() for an alternate version that returns an iterator.

### ➡ `sorted(iterable, *, key=None, reverse=False)`
Return a new sorted list from the items in iterable.

Has two optional arguments which must be specified as keyword arguments.

↪ *key* specifies a function of one argument that is used to extract a comparison key from each element in iterable (for example, key=str.lower). The default value is None (compare the elements directly).

↪ *reverse* is a boolean value. If set to True, then the list elements are sorted as if each comparison were reversed.

Use functools.cmp_to_key() to convert an old-style cmp function to a key function.

The built-in sorted() function is guaranteed to be stable. A sort is stable if it guarantees not to change the relative order of elements that compare equal — this is helpful for sorting in multiple passes (for example, sort by department, then by salary grade).

For sorting examples and a brief sorting tutorial, see Sorting HOW TO.

### ➡ `@staticmethod`
Transform a method into a static method.

A static method does not receive an implicit first argument. To declare a static method, use this idiom:


    class C:
        @staticmethod
        def f(arg1, arg2, ...): ...


The @staticmethod form is a function decorator – see Function definitions for details.

A static method can be called either on the class (such as C.f()) or on an instance (such as C().f()).

Static methods in Python are similar to those found in Java or C++. Also see classmethod() for a variant that is useful for creating alternate class constructors.

Like all decorators, it is also possible to call staticmethod as a regular function and do something with its result. This is needed in some cases where you need a reference to a function from a class body and you want to avoid the automatic transformation to instance method. For these cases, use this idiom:


    class C:
        builtin_open = staticmethod(open)

For more information on static methods, see The standard type hierarchy.

### ✅ `class str(object='')`
### ✅ `class str(object=b'', encoding='utf-8', errors='strict')`
Return a str version of object. See str() for details.

str is the built-in string class. For general information about strings, see Text Sequence Type — str.

### ➡ `sum(iterable, /, start=0)`
Sums start and the items of an iterable from left to right and returns the total. The iterable’s items are normally numbers, and the start value is not allowed to be a string.

For some use cases, there are good alternatives to sum(). The preferred, fast way to concatenate a sequence of strings is by calling ''.join(sequence). To add floating point values with extended precision, see math.fsum(). To concatenate a series of iterables, consider using itertools.chain().

Changed in version 3.8: The start parameter can be specified as a keyword argument.

### ➡ `super([type[, object-or-type]])`
Return a proxy object that delegates method calls to a parent or sibling class of type. This is useful for accessing inherited methods that have been overridden in a class.

The object-or-type determines the method resolution order to be searched. The search starts from the class right after the type.

For example, if __mro__ of object-or-type is D -> B -> C -> A -> object and the value of type is B, then super() searches C -> A -> object.

The __mro__ attribute of the object-or-type lists the method resolution search order used by both getattr() and super(). The attribute is dynamic and can change whenever the inheritance hierarchy is updated.

If the second argument is omitted, the super object returned is unbound. If the second argument is an object, isinstance(obj, type) must be true. If the second argument is a type, issubclass(type2, type) must be true (this is useful for classmethods).

There are two typical use cases for super. In a class hierarchy with single inheritance, super can be used to refer to parent classes without naming them explicitly, thus making the code more maintainable. This use closely parallels the use of super in other programming languages.

The second use case is to support cooperative multiple inheritance in a dynamic execution environment. This use case is unique to Python and is not found in statically compiled languages or languages that only support single inheritance. This makes it possible to implement “diamond diagrams” where multiple base classes implement the same method. Good design dictates that such implementations have the same calling signature in every case (because the order of calls is determined at runtime, because that order adapts to changes in the class hierarchy, and because that order can include sibling classes that are unknown prior to runtime).

For both use cases, a typical superclass call looks like this:


```py
class C(B):
    def method(self, arg):
        super().method(arg)    # This does the same thing as:
                               # super(C, self).method(arg)
```


In addition to method lookups, super() also works for attribute lookups. One possible use case for this is calling descriptors in a parent or sibling class.

Note that super() is implemented as part of the binding process for explicit dotted attribute lookups such as super().__getitem__(name). It does so by implementing its own __getattribute__() method for searching classes in a predictable order that supports cooperative multiple inheritance. Accordingly, super() is undefined for implicit lookups using statements or operators such as super()[name].

Also note that, aside from the zero argument form, super() is not limited to use inside methods. The two argument form specifies the arguments exactly and makes the appropriate references. The zero argument form only works inside a class definition, as the compiler fills in the necessary details to correctly retrieve the class being defined, as well as accessing the current instance for ordinary methods.

For practical suggestions on how to design cooperative classes using super(), see guide to using super().

### ✅ `class tuple([iterable])`
Rather than being a function, tuple is actually an immutable sequence type, as documented in Tuples and Sequence Types — list, tuple, range.

### ✅ `class type(object)`
### ✅ `class type(name, bases, dict, **kwds)`
With one argument, return the type of an object. The return value is a type object and generally the same object as returned by object.__class__.

The isinstance() built-in function is recommended for testing the type of an object, because it takes subclasses into account.

With three arguments, return a new type object. This is essentially a dynamic form of the class statement. The name string is the class name and becomes the __name__ attribute. The bases tuple contains the base classes and becomes the __bases__ attribute; if empty, object, the ultimate base of all classes, is added. The dict dictionary contains attribute and method definitions for the class body; it may be copied or wrapped before becoming the __dict__ attribute. The following two statements create identical type objects:


>>> class X:
...     a = 1
...
>>> X = type('X', (), dict(a=1))


See also Type Objects.

Keyword arguments provided to the three argument form are passed to the appropriate metaclass machinery (usually __init_subclass__()) in the same way that keywords in a class definition (besides metaclass) would.

See also Customizing class creation.

Changed in version 3.6: Subclasses of type which don’t override type.__new__ may no longer use the one-argument form to get the type of an object.

### ➡ `vars([object])`
Return the __dict__ attribute for a module, class, instance, or any other object with a __dict__ attribute.

Objects such as modules and instances have an updateable __dict__ attribute; however, other objects may have write restrictions on their __dict__ attributes (for example, classes use a types.MappingProxyType to prevent direct dictionary updates).

Without an argument, vars() acts like locals(). Note, the locals dictionary is only useful for reads since updates to the locals dictionary are ignored.

A TypeError exception is raised if an object is specified but it doesn’t have a __dict__ attribute (for example, if its class defines the __slots__ attribute).

### ➡ `zip(*iterables)`
Make an iterator that aggregates elements from each of the iterables.

Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The iterator stops when the shortest input iterable is exhausted. With a single iterable argument, it returns an iterator of 1-tuples. With no arguments, it returns an empty iterator. Equivalent to:


    def zip(*iterables):
        # zip('ABCD', 'xy') --> Ax By
        sentinel = object()
        iterators = [iter(it) for it in iterables]
        while iterators:
            result = []
            for it in iterators:
                elem = next(it, sentinel)
                if elem is sentinel:
                    return
                result.append(elem)
            yield tuple(result)


The left-to-right evaluation order of the iterables is guaranteed. This makes possible an idiom for clustering a data series into n-length groups using zip(*[iter(s)]*n). This repeats the same iterator n times so that each output tuple has the result of n calls to the iterator. This has the effect of dividing the input into n-length chunks.

zip() should only be used with unequal length inputs when you don’t care about trailing, unmatched values from the longer iterables. If those values are important, use itertools.zip_longest() instead.

zip() in conjunction with the * operator can be used to unzip a list:


>>> x = [1, 2, 3]
>>> y = [4, 5, 6]
>>> zipped = zip(x, y)
>>> list(zipped)
[(1, 4), (2, 5), (3, 6)]
>>> x2, y2 = zip(*zip(x, y))
>>> x == list(x2) and y == list(y2)
True

### ➡ `__import__(name, globals=None, locals=None, fromlist=(), level=0)`
Note:
 This is an advanced function that is not needed in everyday Python programming, unlike importlib.import_module().
 

This function is invoked by the import statement. It can be replaced (by importing the builtins module and assigning to builtins.__import__) in order to change semantics of the import statement, but doing so is strongly discouraged as it is usually simpler to use import hooks (see PEP 302) to attain the same goals and does not cause issues with code which assumes the default import implementation is in use. Direct use of __import__() is also discouraged in favor of importlib.import_module().

The function imports the module name, potentially using the given globals and locals to determine how to interpret the name in a package context. The fromlist gives the names of objects or submodules that should be imported from the module given by name. The standard implementation does not use its locals argument at all, and uses its globals only to determine the package context of the import statement.

level specifies whether to use absolute or relative imports. 0 (the default) means only perform absolute imports. Positive values for level indicate the number of parent directories to search relative to the directory of the module calling __import__() (see PEP 328 for the details).

When the name variable is of the form package.module, normally, the top-level package (the name up till the first dot) is returned, not the module named by name. However, when a non-empty fromlist argument is given, the module named by name is returned.

For example, the statement import spam results in bytecode resembling the following code:


    spam = __import__('spam', globals(), locals(), [], 0)


The statement import spam.ham results in this call:


    spam = __import__('spam.ham', globals(), locals(), [], 0)


Note how __import__() returns the toplevel module here because this is the object that is bound to a name by the import statement.

On the other hand, the statement from spam.ham import eggs, sausage as saus results in


    _temp = __import__('spam.ham', globals(), locals(), ['eggs', 'sausage'], 0)
    eggs = _temp.eggs
    saus = _temp.sausage


Here, the spam.ham module is returned from __import__(). From this object, the names to import are retrieved and assigned to their respective names.

If you simply want to import a module (potentially within a package) by name, use importlib.import_module().

Changed in version 3.3: Negative values for level are no longer supported (which also changes the default value to 0).

Changed in version 3.9: When the command line options -E or -I are being used, the environment variable PYTHONCASEOK is now ignored.




# =🚩 Built-in Constants
The Python Standard Library » Built-in Constants

A small number of constants live in the built-in namespace. They are:

### ➡ `False`
The false value of the bool type. Assignments to False are illegal and raise a SyntaxError.

### ➡ `True`
The true value of the bool type. Assignments to True are illegal and raise a SyntaxError.

### ➡ `None`
The sole value of the type NoneType. None is frequently used to represent the absence of a value, as when default arguments are not passed to a function. Assignments to None are illegal and raise a SyntaxError.

### ➡ `NotImplemented`
Special value which should be returned by the binary special methods (e.g. __eq__(), __lt__(), __add__(), __rsub__(), etc.) to indicate that the operation is not implemented with respect to the other type; may be returned by the in-place binary special methods (e.g. __imul__(), __iand__(), etc.) for the same purpose. It should not be evaluated in a boolean context.

Note:
 When a binary (or in-place) method returns NotImplemented the interpreter will try the reflected operation on the other type (or some other fallback, depending on the operator). If all attempts return NotImplemented, the interpreter will raise an appropriate exception. Incorrectly returning NotImplemented will result in a misleading error message or the NotImplemented value being returned to Python code.
 
See Implementing the arithmetic operations for examples.

Note:
 NotImplementedError and NotImplemented are not interchangeable, even though they have similar names and purposes. See NotImplementedError for details on when to use it.
 


Changed in version 3.9: Evaluating NotImplemented in a boolean context is deprecated. While it currently evaluates as true, it will emit a DeprecationWarning. It will raise a TypeError in a future version of Python.

### ➡ `Ellipsis`
The same as the ellipsis literal “...”. Special value used mostly in conjunction with extended slicing syntax for user-defined container data types.

### ➡ `__debug__`
This constant is true if Python was not started with an -O option. See also the assert statement.

Note:
 The names None, False, True and __debug__ cannot be reassigned (assignments to them, even as an attribute name, raise SyntaxError), so they can be considered “true” constants.

## ==⚡ Constants added by the site module

The site module (which is imported automatically during startup, except if the -S command-line option is given) adds several constants to the built-in namespace. They are useful for the interactive interpreter shell and should not be used in programs.

### ➡ `quit(code=None)`
### ➡ `exit(code=None)`
Objects that when printed, print a message like “Use quit() or Ctrl-D (i.e. EOF) to exit”, and when called, raise SystemExit with the specified exit code.

### ➡ `copyright`

### ➡ `credits`
Objects that when printed or called, print the text of copyright or credits, respectively.

### ➡ `license`
Object that when printed, prints the message “Type license() to see the full license text”, and when called, displays the full license text in a pager-like fashion (one screen at a time).



# =🚩 Built-in Types
- https://docs.python.org/3.9/library/stdtypes.html

## ==⚡ 4.1 Truth Value Testing

Any object can be tested for truth value, for use in an if or while condition or as operand of the Boolean operations below.

By default, an object is considered true unless its class defines either a __bool__() method that returns `False` or a __len__() method that returns zero, when called with the object. Here are most of the built-in objects considered false:

1. constants defined to be false: `None` and `False`.
2. zero of any numeric type: 0, 0.0, 0j, Decimal(0), Fraction(0, 1)
3. empty sequences and collections: '', (), [], {}, set(), range(0)

Operations and built-in functions that have a Boolean result always return 0 or `False` for false and 1 or True for true, unless otherwise stated. (Important exception: the Boolean operations or and and always return one of their operands.)


## ==⚡ 4.2. Boolean Operations — and, or, not

These are the Boolean operations, ordered by ascending priority:

    | Operation |                Result                | Notes |
    |-----------|--------------------------------------|-------|
    | x or y    | if x is false, then y, else x        | (1)   |
    | x and y   | if x is false, then x, else y        | (2)   |
    | not x     | if x is false, then True, else False | (3)   |

Notes:

1. This is a short-circuit operator, so it only evaluates the second argument if the first one is false.
2. This is a short-circuit operator, so it only evaluates the second argument if the first one is true.
3. not has a lower priority than non-Boolean operators, so not a == b is interpreted as not (a == b), and a == not b is a syntax error.

## ==⚡ 4.3. Comparisons

There are eight comparison operations in Python. They all have the same priority (which is higher than that of the Boolean operations). Comparisons can be chained arbitrarily; for example, x < y <= z is equivalent to x < y and y <= z, except that y is evaluated only once (but in both cases z is not evaluated at all when x < y is found to be false).

This table summarizes the comparison operations:

    | Operation |         Meaning         |
    |-----------|-------------------------|
    | <         | strictly less than      |
    | <=        | less than or equal      |
    | >         | strictly greater than   |
    | >=        | greater than or equal   |
    | ==        | equal                   |
    | !=        | not equal               |
    | is        | object identity         |
    | is not    | negated object identity |

You can use != operator to check for inequality. Moreover in python 2 there was <> operator which used to do the same thing but it has been deprecated in python 3.


## ==⚡ 4.4. Numeric Types — int, float, complex


|    Operation    |                Result               | Notes  |
|-----------------|-------------------------------------|--------|
| x + y           | sum of x and y                      |        |
| x - y           | difference of x and y               |        |
| x * y           | product of x and y                  |        |
| x / y           | quotient of x and y                 |        |
| x // y          | floored quotient of x and y         | (1)    |
| x % y           | remainder of x / y                  | (2)    |
| -x              | x negated                           |        |
| +x              | x unchanged                         |        |
| abs(x)          | absolute value or magnitude of x    | abs()  |
| int(x)          | x converted to integer (3)(6) int() |        |
| float(x)        | x converted to floating point       | (4)(6) |
| complex(re, im) | real part re, imaginary part im.    | (6)    |
| c.conjugate()   | conjugate of the complex number c   |        |
| divmod(x, y)    | the pair (x // y, x % y)            | (2)    |
| pow(x, y)       | x to the power y                    | (5)    |
| x ** y          | x to the power y                    | (5)    |


Notes:

1. Also referred to as integer division. The resultant value is a whole integer, though the result’s type is not necessarily int. The result is always rounded towards minus infinity: 1//2 is 0, (-1)//2 is -1, 1//(-2) is -1, and (-1)//(-2) is 0.
2. Not for complex numbers. Instead convert to floats using abs() if appropriate.
3. Conversion from floating point to integer may round or truncate as in C; see functions math.floor() and math.ceil() for well-defined conversions.
4. float also accepts the strings “nan” and “inf” with an optional prefix “+” or “-” for Not a Number (NaN) and positive or negative infinity.
5. Python defines pow(0, 0) and 0 ** 0 to be 1, as is common for programming languages.
6. The numeric literals accepted include the digits 0 to 9 or any Unicode equivalent (code points with the Nd property).

Python 没有 ++ 运算符：

    | Operator |      Type      | Example   Equivalent |
    |----------|----------------|--------------------|
    | =        | Equals         | a = 2 a = 2        |
    | +=       | Addition       | a += 2    a = a + 2   |
    | -=       | Subtraction    | a -= 2    a = a – 2   |
    | *=       | Multiplication | a *= 2    a = a * 2   |
    | /=       | Division       | a /= 2    a = a / 2   |
    | **       | Power          | a ** 2    a = a ** 2  |

参考 The Python Language Reference » 7.2.1. Augmented assignment statements


4.4.1. Bitwise Operations on Integer Types

Bitwise operations only make sense for integers. Negative numbers are treated as their 2’s complement value (this assumes that there are enough bits so that no overflow occurs during the operation).

The priorities of the binary bitwise operations are all lower than the numeric operations and higher than the comparisons; the unary operation ~ has the same priority as the other unary numeric operations (+ and -).

This table lists the bitwise operations sorted in ascending priority:

    | Operation |              Result             | Notes  |
    |-----------|---------------------------------|--------|
    | x | y     | bitwise or of x and y           |        |
    | x ^ y     | bitwise exclusive or of x and y |        |
    | x & y     | bitwise and of x and y          |        |
    | x << n x  | shifted left by n bits          | (1)(2) |
    | x >> n x  | shifted right by n bits         | (1)(3) |
    | ~x        | the bits of x inverted          |        |

Notes:

1. Negative shift counts are illegal and cause a ValueError to be raised.
2. A left shift by n bits is equivalent to multiplication by pow(2, n) without overflow check.
3. A right shift by n bits is equivalent to division by pow(2, n) without overflow check.


4.4.2. Additional Methods on Integer Types

The int type implements the numbers.Integral abstract base class. In addition, it provides a few more methods:

    int.bit_length()
    int.to_bytes(length, byteorder, *, signed=False)
    int.from_bytes(bytes, byteorder, *, signed=False)

4.4.3. Additional Methods on Float

The float type implements the numbers.Real abstract base class. float also has the following additional methods.

    float.as_integer_ratio() # 0.5 => (1,2)
    float.is_integer()
    float.hex()
    float.fromhex(s)

4.4.4. Hashing of numeric types

## ==⚡ 4.5. Iterator Types

4.5.1. Generator Types

Python supports a concept of iteration over containers. This is implemented using two distinct methods; these are used to allow user-defined classes to support iteration. Sequences, described below in more detail, always support the iteration methods.

One method needs to be defined for container objects to provide iteration support:
➡ **container.__iter__()**
Return an iterator object. The object is required to support the iterator protocol described below. If a container supports different types of iteration, additional methods can be provided to specifically request iterators for those iteration types. (An example of an object supporting multiple forms of iteration would be a tree structure which supports both breadth-first and depth-first traversal.) This method corresponds to the tp_iter slot of the type structure for Python objects in the Python/C API.

The iterator objects themselves are required to support the following two methods, which together form the iterator protocol:
➡ **iterator.__iter__()**
Return the iterator object itself. This is required to allow both containers and iterators to be used with the for and in statements. This method corresponds to the tp_iter slot of the type structure for Python objects in the Python/C API.
➡ **iterator.__next__()**
Return the next item from the container. If there are no further items, raise the StopIteration exception. This method corresponds to the tp_iternext slot of the type structure for Python objects in the Python/C API.

Python defines several iterator objects to support iteration over general and specific sequence types, dictionaries, and other more specialized forms. The specific types are not important beyond their implementation of the iterator protocol.

Once an iterator’s __next__() method raises StopIteration, it must continue to do so on subsequent calls. Implementations that do not obey this property are deemed broken.


Generator Types

Python’s generators provide a convenient way to implement the iterator protocol. If a container object’s __iter__() method is implemented as a generator, it will automatically return an iterator object (technically, a generator object) supplying the __iter__() and __next__() methods. More information about generators can be found in the documentation for the yield expression.


## ==⚡ 4.6. Sequence Types — list, tuple, range

4.6.1. Common Sequence Operations


    |      Operation       |                                Result                                |
    |----------------------|----------------------------------------------------------------------|
    | x in s               | True if an item of s is equal to x, else False                       |
    | x not in s           | False if an item of s is equal to x, else True                       |
    | s + t                | the concatenation of s and t                                         |
    | s * n or n * s       | equivalent to adding s to itself n times                             |
    | s[i]                 | ith item of s, origin 0                                              |
    | s[i:j]               | slice of s from i to j                                               |
    | s[i:j:k]             | slice of s from i to j with step k                                   |
    | len(s)               | length of s                                                          |
    | min(s)               | smallest item of s                                                   |
    | max(s)               | largest item of s                                                    |
    | s.index(x[, i[, j]]) | index of the first occurrence of x in s (at or after i and before j) |
    | s.count(x)           | total number of occurrences of x in s                                |

4.6.2. Immutable Sequence Types

The only operation that immutable sequence types generally implement that is not also implemented by mutable sequence types is support for the hash() built-in.

This support allows immutable sequences, such as tuple instances, to be used as dict keys and stored in set and frozenset instances.

Attempting to hash an immutable sequence that contains unhashable values will result in TypeError.


4.6.3. Mutable Sequence Types

    |     Operation      |                                 Result                                |
    |--------------------|-----------------------------------------------------------------------|
    | s[i] = x           | item i of s is replaced by x                                          |
    | s[i:j] = t         | slice of s from i to j is replaced by the contents of the iterable t  |
    | del s[i:j]         | same as s[i:j] = []                                                   |
    | s[i:j:k] = t       | the elements of s[i:j:k] are replaced by those of t (1)               |
    | del s[i:j:k]       | removes the elements of s[i:j:k] from the list                        |
    | s.append(x)        | appends x to the end of the sequence (same as s[len(s):len(s)] = [x]) |
    | s.clear()          | removes all items from s (same as del s[:])                           |
    | s.copy()           | creates a shallow copy of s (same as s[:])                            |
    | s.extend(t) s += t | extends s with the contents of t                                      |
    | s *= n             | updates s with its contents repeated n times                          |
    | s.insert(i, x)     | inserts x into s at the index given by i (same as s[i:i] = [x])       |
    | s.pop([i])         | retrieves the item at i and also removes it from s                    |
    | s.remove(x)        | remove the first item from s where s[i] is equal to x                 |
    | s.reverse()        | reverses the items of s in place                                      |

4.6.4. Lists

➡ `class list([iterable])`

Lists may be constructed in several ways:

- Using a pair of square brackets to denote the empty list: `[]`
- Using square brackets, separating items with commas: `[a], [a, b, c]`
- Using a list comprehension: `[x for x in iterable]`
- Using the type constructor: `list()` or `list(iterable)`

Lists implement all of the common and mutable sequence operations. Lists also provide the following additional method:

    sort(*, key=None, reverse=False)

4.6.5. Tuples

Tuples are immutable sequences, typically used to store collections of heterogeneous data (such as the 2-tuples produced by the enumerate() built-in). Tuples are also used for cases where an immutable sequence of homogeneous data is needed (such as allowing storage in a set or dict instance).

➡ `class tuple([iterable])`

Tuples may be constructed in a number of ways:

- Using a pair of parentheses to denote the empty tuple: `()`
- Using a trailing comma for a singleton tuple: `a, or (a,)`
- Separating items with commas: `a, b, c or (a, b, c)`
- Using the tuple() built-in: `tuple()` or `tuple(iterable)`

4.6.6. Ranges

The range type represents an immutable sequence of numbers and is commonly used for looping a specific number of times in for loops.

➡ `class range(stop)`
➡ `class range(start, stop[, step])`

The arguments to the range constructor must be integers (either built-in int or any object that implements the `__index__` special method). If the step argument is omitted, it defaults to 1. If the start argument is omitted, it defaults to 0. If step is zero, ValueError is raised.

For a positive step, the contents of a range r are determined by the formula `r[i] = start + step*i` where `i >= 0` and `r[i] < stop`.

For a negative step, the contents of the range are still determined by the formula `r[i] = start + step*i`, but the constraints are `i >= 0` and `r[i] > stop`.

A range object will be empty if `r[0]` does not meet the value constraint. Ranges do support negative indices, but these are interpreted as indexing from the end of the sequence determined by the positive indices.

Ranges containing absolute values larger than sys.maxsize are permitted but some features (such as `len()`) may raise `OverflowError`.

Range examples:


```py
>>> list(range(10))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> list(range(1, 11))
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
>>> list(range(0, 30, 5))
[0, 5, 10, 15, 20, 25]
>>> list(range(0, 10, 3))
[0, 3, 6, 9]
>>> list(range(0, -10, -1))
[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]
>>> list(range(0))
[]
>>> list(range(1, 0))
[]
```

## ==⚡ 4.7. Text Sequence Type — str

Textual data in Python is handled with str objects, or strings. Strings are immutable sequences of Unicode code points. String literals are written in a variety of ways:

➡ `class str(object='')`
➡ `class str(object=b'', encoding='utf-8', errors='strict')`

- Single quotes: 'allows embedded "double" quotes'
- Double quotes: "allows embedded 'single' quotes".
- Triple quoted: '''Three single quotes''', """Three double quotes"""


4.7.1. String Methods

```py
str.capitalize() # capitalized --> Capitalized
str.casefold()   # Casefolded strings
str.center(width[, fillchar]) # "C".center(3, '_') --> "_C_"
str.count(sub[, start[, end]]) # "ccc".count('cc') --> 1
str.encode(encoding="utf-8", errors="strict") # str --> bytes
str.endswith(suffix[, start[, end]]) # "py or php".endswith('or', 0, 5) --> true
str.expandtabs(tabsize=8) # '01\t012\t0123\t01234'.expandtabs() --> '01      012     0123    01234'
str.find(sub[, start[, end]]) # 'abcd'.find('cd', 2) --> 3   # 'Py' in 'Python' --> True
str.format(*args, **kwargs) # "1 + 2 is {0}".format(1+2) --> '1 + 2 is 3'
str.index(sub[, start[, end]])
str.isalnum()
str.isalpha()
str.isascii()
str.isdecimal()
str.isdigit()
str.isidentifier()
str.islower()
str.isnumeric()
str.isprintable()
str.isspace()
str.istitle() # "Abc Is A Song".istitle() --> true
str.isupper()
str.join(iterable)
str.ljust(width[, fillchar])
str.rjust(width[, fillchar]) 'app'.rjust(10) --> "       app"
str.lower()
str.lstrip([chars]) # 'app'.lstrip("a") --> "pp"
static str.maketrans(x[, y[, z]]) # x is dict or x & y combine to be a dict
str.translate(table) # got a table from maketrans
str.replace(old, new[, count])
str.rfind(sub[, start[, end]])  # return -1 if not found
str.rindex(sub[, start[, end]]) # return ValueError if not found
str.rpartition(sep) # print("abc".rpartition("a") --> ('', 'a', 'bc')
str.rsplit(sep=None, maxsplit=-1)
str.rstrip([chars]) # 'mississippi'.rstrip('pi') --> 'mississ'
str.split(sep=None, maxsplit=-1)
str.splitlines([keepends])
str.startswith(prefix[, start[, end]])
str.strip([chars]) # ' mississippi'.strip(' pi') --> misssiss
str.swapcase()
str.title()
str.upper()
str.zfill(width) # "-42".zfill(5) --> '-0042'
```

Casefolding is similar to lowercasing but more aggressive because it is intended to remove all case distinctions in a string. For example, the German lowercase letter 'ß' is equivalent to "ss". Since it is already lowercase, lower() would do nothing to 'ß'; casefold() converts it to "ss".

The casefolding algorithm is described in section 3.13 of the Unicode Standard.


Similar to `str.format(**mapping)`, except that mapping is used directly and not copied to a dict. `str.format_map(mapping)` is useful if for example mapping is a dict subclass:


```sh
>>> class Default(dict):
...     def __missing__(self, key):
...         return key
...
>>> '{name} was born in {country}'.format_map(Default(name='Guido'))
'Guido was born in country'
```

```py
table = str.maketrans({"a":"b"})
print("abc".translate(table))
# bbc
```

The `splitlines` method splits on the following line boundaries. In particular, the boundaries are a superset of universal newlines.

    | Representation |         Description         |
    |----------------|-----------------------------|
    | \n             | Line Feed                   |
    | \r             | Carriage Return             |
    | \r\n           | Carriage Return + Line Feed |
    | \v or \x0b     | Line Tabulation             |
    | \f or \x0c     | Form Feed                   |
    | \x1c           | File Separator              |
    | \x1d           | Group Separator             |
    | \x1e           | Record Separator            |
    | \x85           | Next Line (C1 Control Code) |
    | \u2028         | Line Separator              |
    | \u2029         | Paragraph Separator         |

A workaround for apostrophes in title string can be constructed using regular expressions:

```py
>>> import re
>>> def titlecase(s):
...     return re.sub(r"[A-Za-z]+('[A-Za-z]+)?",
...                   lambda mo: mo.group(0)[0].upper() +
...                              mo.group(0)[1:].lower(),
...                   s)
...
>>> titlecase("they're bill's friends.")
"They're Bill's Friends."
```

4.7.2. printf-style String Formatting

Note
 The formatting operations described here exhibit a variety of quirks that lead to a number of common errors (such as failing to display tuples and dictionaries correctly). Using the newer formatted string literals, the `str.format()` interface, or template strings may help avoid these errors. Each of these alternatives provides their own trade-offs and benefits of simplicity, flexibility, and/or extensibility.
 
String objects have one unique built-in operation: the % operator (modulo). This is also known as the string formatting or interpolation operator. Given *format % values* (where format is a string), % conversion specifications in format are replaced with zero or more elements of values. The effect is similar to using the *sprintf()* in the C language.

If format requires a single argument, values may be a single non-tuple object. [5] Otherwise, values must be a tuple with exactly the number of items specified by the format string, or a single mapping object (for example, a dictionary).

A conversion specifier contains two or more characters and has the following components, which must occur in this order:

1. ↪ The '%' character, which marks the start of the specifier.
2. ↪ Mapping key (optional), consisting of a parenthesised sequence of characters (for example, (somename)).
3. ↪ Conversion flags (optional), which affect the result of some conversion types.
4. ↪ Minimum field width (optional). If specified as an * (asterisk), the actual width is read from the next element of the tuple in values, and the object to convert comes after the minimum field width and optional precision.
5. ↪ Precision (optional), given as a '.' (dot) followed by the precision. If specified as * (an asterisk), the actual precision is read from the next element of the tuple in values, and the value to convert comes after the precision.
6. ↪ Length modifier (optional).
7. ↪ Conversion type.

When the right argument is a dictionary (or other mapping type), then the formats in the string must include a parenthesised mapping key into that dictionary inserted immediately after the '%' character. The mapping key selects the value to be formatted from the mapping. For example:

>>> print('%(language)s has %(number)03d quote types.' %
...       {'language': "Python", "number": 2})
Python has 002 quote types.


In this case no * specifiers may occur in a format (since they require a sequential parameter list).

The conversion flag characters are:

    | Flag |                                              Meaning                                               |
    |------|----------------------------------------------------------------------------------------------------|
    | '#'  | The value conversion will use the “alternate form” (where defined below).                          |
    | '0'  | The conversion will be zero padded for numeric values.                                             |
    | '-'  | The converted value is left adjusted (overrides the '0' conversion if both are given).             |
    | ' '  | A blank should be left before a positive number (or empty string) produced by a signed conversion. |
    | '+'  | A sign character ('+' or '-') will precede the conversion (overrides a “space” flag).              |

A length modifier (h, l, or L) may be present, but is ignored as it is not necessary for Python – so e.g. %ld is identical to %d.

The conversion types are:

    | Conversion |                               Meaning                               | Notes |
    |------------|---------------------------------------------------------------------|-------|
    | 'd'        | Signed integer decimal.                                             |       |
    | 'i'        | Signed integer decimal.                                             |       |
    | 'o'        | Signed octal value.                                                 | (1)   |
    | 'u'        | Obsolete type – it is identical to 'd'.                             | (6)   |
    | 'x'        | Signed hexadecimal (lowercase).                                     | (2)   |
    | 'X'        | Signed hexadecimal (uppercase).                                     | (2)   |
    | 'e'        | Floating point exponential format (lowercase).                      | (3)   |
    | 'E'        | Floating point exponential format (uppercase).                      | (3)   |
    | 'f'        | Floating point decimal format.                                      | (3)   |
    | 'F'        | Floating point decimal format.                                      | (3)   |
    | 'g'        | Floating point format.                                              | (4)   |
    | 'G'        | Floating point format.                                              | (4)   |
    | 'c'        | Single character (accepts integer or single character string).      |       |
    | 'r'        | String (converts any Python object using repr()).                   | (5)   |
    | 's'        | String (converts any Python object using str()).                    | (5)   |
    | 'a'        | String (converts any Python object using ascii()).                  | (5)   |
    | '%'        | No argument is converted, results in a '%' character in the result. |       |

Uses uppercase(G)/lowercase(g) exponential format if exponent is less than -4 or not less than precision, decimal format otherwise.

Notes:

1.The alternate form causes a leading octal specifier ('0o') to be inserted before the first digit.
2.The alternate form causes a leading '0x' or '0X' (depending on whether the 'x' or 'X' format was used) to be inserted before the first digit.
3.The alternate form causes the result to always contain a decimal point, even if no digits follow it.

The precision determines the number of digits after the decimal point and defaults to 6.
4.The alternate form causes the result to always contain a decimal point, and trailing zeroes are not removed as they would otherwise be.

The precision determines the number of significant digits before and after the decimal point and defaults to 6.
5.If precision is N, the output is truncated to N characters.
6.See PEP 237.


Since Python strings have an explicit length, %s conversions do not assume that '\0' is the end of the string.


Changed in version 3.1: %f conversions for numbers whose absolute value is over 1e50 are no longer replaced by %g conversions.



## ==⚡ 4.8. Binary Sequence Types — bytes, bytearray, memoryview

The core built-in types for manipulating binary data are bytes and bytearray. They are supported by memoryview which uses the buffer protocol to access the memory of other binary objects without needing to make a copy.

The array module supports efficient storage of basic data types like 32-bit integers and IEEE754 double-precision floating values.

4.8.1. Bytes Objects

Bytes objects are immutable sequences of single bytes. Since many major binary protocols are based on the ASCII text encoding, bytes objects offer several methods that are only valid when working with ASCII compatible data and are closely related to string objects in a variety of other ways.

➡ `class bytes([source[, encoding[, errors]]])`

Firstly, the syntax for bytes literals is largely the same as that for string literals, except that a b prefix is added:

- Single quotes: b'still allows embedded "double" quotes'
- Double quotes: b"still allows embedded 'single' quotes".
- Triple quoted: b'''3 single quotes''', b"""3 double quotes"""

4.8.2. Bytearray Objects

bytearray objects are a mutable counterpart to bytes objects.

➡ `class bytearray([source[, encoding[, errors]]])`

There is no dedicated literal syntax for bytearray objects, instead they are always created by calling the constructor:

- Creating an empty instance: `bytearray()`
- Creating a zero-filled instance with a given length: `bytearray(10)`
- From an iterable of integers: `bytearray(range(20))`
- Copying existing binary data via the buffer protocol: `bytearray(b'Hi!')`

4.8.3. Bytes and Bytearray Operations

```py
bytes.count(sub[, start[, end]])
bytearray.count(sub[, start[, end]])

bytes.decode(encoding="utf-8", errors="strict")
bytearray.decode(encoding="utf-8", errors="strict")

bytes.endswith(suffix[, start[, end]])
bytearray.endswith(suffix[, start[, end]])

bytes.find(sub[, start[, end]])
bytearray.find(sub[, start[, end]])

bytes.index(sub[, start[, end]])
bytearray.index(sub[, start[, end]])

bytes.join(iterable)
bytearray.join(iterable)

static bytes.maketrans(from, to)
static bytearray.maketrans(from, to)

bytes.partition(sep)
bytearray.partition(sep)

bytes.replace(old, new[, count])
bytearray.replace(old, new[, count])

bytes.rfind(sub[, start[, end]])
bytearray.rfind(sub[, start[, end]])

bytes.rindex(sub[, start[, end]])
bytearray.rindex(sub[, start[, end]])

bytes.rpartition(sep)
bytearray.rpartition(sep)

bytes.startswith(prefix[, start[, end]])
bytearray.startswith(prefix[, start[, end]])

bytes.translate(table, delete=b'')  bytearray.translate(table, delete=b'')
bytes.center(width[, fillbyte])     bytearray.center(width[, fillbyte])
bytes.ljust(width[, fillbyte])      bytearray.ljust(width[, fillbyte])
bytes.lstrip([chars])               bytearray.lstrip([chars])
bytes.rjust(width[, fillbyte])      bytearray.rjust(width[, fillbyte])
bytes.rsplit(sep=None, maxsplit=-1) bytearray.rsplit(sep=None, maxsplit=-1)
bytes.rstrip([chars])               bytearray.rstrip([chars])
bytes.split(sep=None, maxsplit=-1)  bytearray.split(sep=None, maxsplit=-1)
bytes.strip([chars])                bytearray.strip([chars])
bytes.capitalize()                  bytearray.capitalize()
bytes.expandtabs(tabsize=8)         bytearray.expandtabs(tabsize=8)

bytes.isalnum()     bytearray.isalnum()
bytes.isalpha()     bytearray.isalpha
bytes.isascii()     bytearray.isascii()
bytes.isdigit()     bytearray.isdigit()
bytes.islower()     bytearray.islower()
bytes.isspace()     bytearray.isspace
bytes.istitle()     bytearray.istitle()
bytes.isupper()     bytearray.isupper()

bytes.lower()                       bytearray.lower()
bytes.splitlines(keepends=False)    bytearray.splitlines(keepends=False)
bytes.swapcase()                    bytearray.swapcase()
bytes.title()                       bytearray.title()
bytes.upper()                       bytearray.upper()
bytes.zfill(width)                  bytearray.zfill(width)
```

4.8.4. printf-style Bytes Formatting

4.8.5. Memory Views

memoryview objects allow Python code to access the internal data of an object that supports the buffer protocol without copying.

➡ `class memoryview(obj)`

a subview:

```sh
>>> v = memoryview(b'abcefg')
>>> v[1]
98
>>> v[-1]
103
>>> v[1:4]
<memory at 0x7f3ddc9f4350>
>>> bytes(v[1:4])
b'bce'
```

## ==⚡ 4.9. Set Types — set, frozenset

A set object is an unordered collection of distinct hashable objects. Common uses include membership testing, removing duplicates from a sequence, and computing mathematical operations such as intersection, union, difference, and symmetric difference. (For other containers see the built-in dict, list, and tuple classes, and the collections module.)

Like other collections, sets support x in set, len(set), and for x in set. Being an unordered collection, sets do not record element position or order of insertion. Accordingly, sets do not support indexing, slicing, or other sequence-like behavior.

There are currently two built-in set types, set and frozenset. The set type is mutable — the contents can be changed using methods like add() and remove(). Since it is mutable, it has no hash value and cannot be used as either a dictionary key or as an element of another set. The frozenset type is immutable and hashable — its contents cannot be altered after it is created; it can therefore be used as a dictionary key or as an element of another set.

Non-empty sets (not frozensets) can be created by placing a comma-separated list of elements within braces, for example: {'jack', 'sjoerd'}, in addition to the set constructor.

The constructors for both classes work the same:

➡ `class set([iterable])class frozenset([iterable])`

Instances of set and frozenset provide the following operations:

```py
len(s)
# Return the number of elements in set s (cardinality of s).
x in s
# Test x for membership in s.
x not in s
# Test x for non-membership in s.
isdisjoint(other)
# Return True if the set has no elements in common with other. Sets are disjoint if and only if their intersection is the empty set.
issubset(other)
set <= other
# Test whether every element in the set is in other.
set < other
# Test whether the set is a proper subset of other, that is, set <= other and set != other.
issuperset(other)
set >= other
# Test whether every element in other is in the set.
set > other
# Test whether the set is a proper superset of other, that is, set >= other and set != other.
union(*others)set | other | ...
# Return a new set with elements from the set and all others.
intersection(*others)set & other & ...
# Return a new set with elements common to the set and all others.
difference(*others)set - other - ...
# Return a new set with elements in the set that are not in the others.
symmetric_difference(other)set ^ other
# Return a new set with elements in either the set or other but not both.
copy()
# Return a new set with a shallow copy of s.
```

The following table lists operations available for set that do not apply to immutable instances of frozenset:

```py
update(*others)
set |= other | ...
# Update the set, adding elements from all others.
intersection_update(*others)
set &= other & ...
# Update the set, keeping only elements found in it and all others.
difference_update(*others)
set -= other | ...
# Update the set, removing elements found in others.
symmetric_difference_update(other)
set ^= other
# Update the set, keeping only elements found in either set, but not in both.
add(elem)
# Add element elem to the set.
remove(elem)
# Remove element elem from the set. Raises KeyError if elem is not contained in the set.
discard(elem)
# Remove element elem from the set if it is present.
pop()
# Remove and return an arbitrary element from the set. Raises KeyError if the set is empty.
clear()
# Remove all elements from the set.
```


## ==⚡ 4.10. Mapping Types — dict

A mapping object maps hashable values to arbitrary objects. Mappings are mutable objects. There is currently only one standard mapping type, the dictionary. (For other containers see the built-in list, set, and tuple classes, and the collections module.)

A dictionary’s keys are almost arbitrary values. Values that are not hashable, that is, values containing lists, dictionaries or other mutable types (that are compared by value rather than by object identity) may not be used as keys. Numeric types used for keys obey the normal rules for numeric comparison: if two numbers compare equal (such as 1 and 1.0) then they can be used interchangeably to index the same dictionary entry. (Note however, that since computers store floating-point numbers as approximations it is usually unwise to use them as dictionary keys.)

Dictionaries can be created by placing a comma-separated list of key: value pairs within braces, for example: `{'jack': 4098, 'sjoerd': 4127}` or `{4098: 'jack', 4127: 'sjoerd'}`, or by the dict constructor.

Dictionaries can be created by several means:

• Use a comma-separated list of key: value pairs within braces: 
    - *{'jack': 4098, 'sjoerd': 4127}* or 
    - *{4098: 'jack', 4127: 'sjoerd'}*
• Use a dict comprehension: 
    - *{}*, 
    - *{x: x ** 2 for x in range(10)}*
• Use the type constructor: 
    - *dict()*, 
    - *dict([('foo', 100), ('bar', 200)])*, 
    - *dict(foo=100, bar=200)*

If no positional argument is given, an empty dictionary is created. If a positional argument is given and it is a mapping object, a dictionary is created with the same key-value pairs as the mapping object. Otherwise, the positional argument must be an iterable object. Each item in the iterable must itself be an iterable with exactly two objects. The first object of each item becomes a key in the new dictionary, and the second object the corresponding value. If a key occurs more than once, the last value for that key becomes the corresponding value in the new dictionary.

If keyword arguments are given, the keyword arguments and their values are added to the dictionary created from the positional argument. If a key being added is already present, the value from the keyword argument replaces the value from the positional argument.

To illustrate, the following examples all return a dictionary equal to {"one": 1, "two": 2, "three": 3}:

```py
class dict(**kwarg)
class dict(mapping, **kwarg)
class dict(iterable, **kwarg)

>>> a = dict(one=1, two=2, three=3)
>>> b = {'one': 1, 'two': 2, 'three': 3}
>>> c = dict(zip(['one', 'two', 'three'], [1, 2, 3]))
>>> d = dict([('two', 2), ('one', 1), ('three', 3)])
>>> e = dict({'three': 3, 'one': 1, 'two': 2})
>>> a == b == c == d == e
True
```

These are the operations that dictionaries support (and therefore, custom mapping types should support too):

```py
len(d)
# Return the number of items in the dictionary d.
d[key]
# Return the item of d with key key. Raises a KeyError if key is not in the map.
d[key] = value
# Set d[key] to value.
del d[key]
# Remove d[key] from d. Raises a KeyError if key is not in the map.
key in d
# Return True if d has a key key, else False.
key not in d
# Equivalent to not key in d.
iter(d)
# Return an iterator over the keys of the dictionary. This is a shortcut for iter(d.keys()).
clear()
# Remove all items from the dictionary.
copy()
# Return a shallow copy of the dictionary.
classmethod fromkeys(seq[, value])
# Create a new dictionary with keys from seq and values set to value.
get(key[, default])
# Return the value for key if key is in the dictionary, else default. If default is not given, it defaults to None, so that this method never raises a KeyError.
items()
# Return a new view of the dictionary’s items ((key, value) pairs). See the documentation of view objects.
keys()
# Return a new view of the dictionary’s keys. See the documentation of view objects.
pop(key[, default])
# If key is in the dictionary, remove it and return its value, else return default. If default is not given and key is not in the dictionary, a KeyError is raised.
popitem()
# Remove and return an arbitrary (key, value) pair from the dictionary.
# popitem() is useful to destructively iterate over a dictionary, as often used in set algorithms. If the dictionary is empty, calling popitem() raises a KeyError.
setdefault(key[, default])
# If key is in the dictionary, return its value. If not, insert key with a value of default and return default. default defaults to None.
update([other])
# Update the dictionary with the key/value pairs from other, overwriting existing keys. Return None.
values()
# Return a new view of the dictionary’s values. See the documentation of view objects.
```

```py
class Counter(dict):
    def __missing__(self, key):
        return 0
c = Counter()
c['red'] # 0
c['red'] += 1
c['red'] # 1
```

The example above shows part of the implementation of collections.Counter. A different `__missing__` method is used by collections.defaultdict.

4.10.1. Dictionary view objects

The objects returned by dict.keys(), dict.values() and dict.items() are view objects. They provide a dynamic view on the dictionary’s entries, which means that when the dictionary changes, the view reflects these changes.


## ==⚡ 4.11. Context Manager Types

Python’s with statement supports the concept of a runtime context defined by a context manager. This is implemented using a pair of methods that allow user-defined classes to define a runtime context that is entered before the statement body is executed and exited when the statement ends:

    contextmanager.__enter__()
    contextmanager.__exit__(exc_type, exc_val, exc_tb)

一个上下文管理器的类，最起码要定义 `__enter__` 和 `__exit__` 两个魔术方法。 以下构造一个上下文管理器类：

    class File(object):
        def __init__(self, file_name, method):
            self.file_obj = open(file_name, method)
        def __enter__(self):
            return self.file_obj
        def __exit__(self, type, value, traceback):
            self.file_obj.close()
            return True

可以在 with 语句里使用它们，试试：

    with File('demo.txt', 'w') as opened_file:
        opened_file.write('Hola!')

退出函数接受三个参数，这些参数对于每个上下文管理器类都是必须的。

注意，传递给 `__exit__` 方法的 traceback 参数，它包含异常信息，可以方法内处理异常。如果退出方法返回 True，那么就表示这个异常就被优雅地处理了。否则，这个异常将被 with 语句抛出。

可以用装饰器和生成器来实现上下文管理器，Python 有个 contextlib 模块专门用于这个目的。

来看一个没用的例子：

    from contextlib import contextmanager
    
    @contextmanager
    def open_file(name):
        f = open(name, 'w')
        yield f
        f.close()

这个实现方式看起来更加直观和简单

。然而，这个方法需要关于生成器、yield和装饰器的一些知识。在这个例子中我们还没有捕捉可能产生的任何异常。它的工作方式和之前的方法大致相同。

Python 解释器遇到了 yield 关键字。因为这个缘故它创建了一个生成器而不是一个普通的函数。contextmanager 这个装饰器会被调用并传入函数名 open_file 作为参数，并返回一个 GeneratorContextManager 对象封装过的生成器赋值给 open_file 函数。执行 open_file 实际上是使用 GeneratorContextManager 对象：

    with open_file('some_file') as f:
        f.write('hola!')

## ==⚡ 4.12 Generic Alias Type


## ==⚡ 4.13 Other Built-in Types

The interpreter supports several other kinds of objects. Most of these support only one or two operations.

### ===🗝 4.12.1. Modules

The only special operation on a module is attribute access: m.name, where m is a module and name accesses a name defined in m’s symbol table. Module attributes can be assigned to. (Note that the import statement is not, strictly speaking, an operation on a module object; import foo does not require a module object named foo to exist, rather it requires an (external) definition for a module named foo somewhere.)

A special attribute of every module is `__dict__`. This is the dictionary containing the module’s symbol table. Modifying this dictionary will actually change the module’s symbol table, but direct assignment to the `__dict__` attribute is not possible (you can write `m.__dict__['a'] = 1`, which defines m.a to be 1, but you can’t write m.`__dict__` = {}). Modifying `__dict__` directly is not recommended.

Modules built into the interpreter are written like this: `<module 'sys' (built-in)>.` If loaded from a file, they are written as `<module 'os' from '/usr/local/lib/pythonX.Y/os.pyc'>`.


### ===🗝 4.12.2. Classes and Class Instances

See Objects, values and types and Class definitions for these.


### ===🗝 4.12.3. Functions

Function objects are created by function definitions. The only operation on a function object is to call it: func(argument-list).

There are really two flavors of function objects: built-in functions and user-defined functions. Both support the same operation (to call the function), but the implementation is different, hence the different object types.

See Function definitions for more information.


### ===🗝 4.12.4. Methods

Methods are functions that are called using the attribute notation. There are two flavors: built-in methods (such as append() on lists) and class instance methods. Built-in methods are described with the types that support them.

If you access a method (a function defined in a class namespace) through an instance, you get a special object: a bound method (also called instance method) object. When called, it will add the self argument to the argument list. Bound methods have two special read-only attributes: m.__self__ is the object on which the method operates, and `m.__func__` is the function implementing the method. Calling m(arg-1, arg-2, ..., arg-n) is completely equivalent to calling `m.__func__(m.__self__, arg-1, arg-2, ..., arg-n)`.

Like function objects, bound method objects support getting arbitrary attributes. However, since method attributes are actually stored on the underlying function object (`meth.__func__`), setting method attributes on bound methods is disallowed. Attempting to set an attribute on a method results in an AttributeError being raised. In order to set a method attribute, you need to explicitly set it on the underlying function object:


>>> class C:
...     def method(self):
...         pass
...
>>> c = C()
>>> c.method.whoami = 'my name is method'  # can't set on the method
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'method' object has no attribute 'whoami'
>>> c.method.__func__.whoami = 'my name is method'
>>> c.method.whoami
'my name is method'


See The standard type hierarchy for more information.


### ===🗝 4.12.5. Code Objects

Code objects are used by the implementation to represent “pseudo-compiled” executable Python code such as a function body. They differ from function objects because they don’t contain a reference to their global execution environment. Code objects are returned by the built-in `compile()` function and can be extracted from function objects through their `__code__` attribute. See also the code module.

A code object can be executed or evaluated by passing it (instead of a source string) to the `exec()` or `eval()` built-in functions.

See The standard type hierarchy for more information.


### ===🗝 4.12.6. Type Objects

Type objects represent the various object types. An object’s type is accessed by the built-in function type(). There are no special operations on types. The standard module types defines names for all standard built-in types.

Types are written like this: `<class 'int'>`.


### ===🗝 4.12.7. The Null Object

This object is returned by functions that don’t explicitly return a value. It supports no special operations. There is exactly one null object, named `None` (a built-in name). `type(None)()` produces the same singleton.

It is written as `None`.


### ===🗝 4.12.8. The Ellipsis Object

This object is commonly used by slicing (see Slicings). It supports no special operations. There is exactly one ellipsis object, named Ellipsis (a built-in name). `type(Ellipsis)()` produces the Ellipsis singleton.

It is written as `Ellipsis` or `...`.


### ===🗝 4.12.9. The NotImplemented Object

This object is returned from comparisons and binary operations when they are asked to operate on types they don’t support. See Comparisons for more information. There is exactly one NotImplemented object. type(NotImplemented)() produces the singleton instance.

It is written as NotImplemented.


### ===🗝 4.12.10. Boolean Values

Boolean values are the two constant objects `False` and `True`. They are used to represent truth values (although other values can also be considered false or true). In numeric contexts (for example when used as the argument to an arithmetic operator), they behave like the integers 0 and 1, respectively. The built-in function bool() can be used to convert any value to a Boolean, if the value can be interpreted as a truth value (see section Truth Value Testing above).

They are written as `False` and `True`, respectively.


### ===🗝 4.12.11. Internal Objects

See The standard type hierarchy for this information. It describes stack `frame objects`, traceback objects, and slice objects.


## ==⚡ 4.14 Special Attributes


The implementation adds a few special read-only attributes to several object types, where they are relevant. Some of these are not reported by the dir() built-in function.

- `object.__dict__` A dictionary or other mapping object used to store an object’s (writable) attributes.
- `instance.__class__` The class to which a class instance belongs.
- `class.__bases__` The tuple of base classes of a class object.
- `definition.__name__` The name of the class, function, method, descriptor, or generator instance.
- `definition.__qualname__` The qualified name of the class, function, method, descriptor, or generator instance.


New in version 3.3.

- `class.__mro__` This attribute is a tuple of classes that are considered when looking for base classes during method resolution.
- `class.mro()` This method can be overridden by a metaclass to customize the method resolution order for its instances. It is called at class instantiation, and its result is stored in `__mro__`.
- `class.__subclasses__()` Each class keeps a list of weak references to its immediate subclasses. This method returns a list of all those references still alive. 

方法解析顺序 Method Resolution Order - MRO 是一种在多重继承中用于确定方法搜索顺序的算法，又称 C3 超类线性化(superclass linearization)。Python 会计算出每一个类的 MRO 列表。一个类的 MRO 列表是一个包含了其继承链上所有基类的线性顺序列，并且列表中的每一项均保持唯一。当需要在继承链中寻找某个属性时，Python 会在 MRO 列表中从左到右开始查找各个基类，直到找到第一个匹配这个属性的类为止。



# =🚩 Built-in Exceptions
The Python Standard Library » Built-in Exceptions

## ==⚡ Exception hierarchy

The class hierarchy for built-in exceptions is:

    BaseException
     +-- SystemExit
     +-- KeyboardInterrupt
     +-- GeneratorExit
     +-- Exception
          +-- StopIteration
          +-- StopAsyncIteration
          +-- ArithmeticError
          |    +-- FloatingPointError
          |    +-- OverflowError
          |    +-- ZeroDivisionError
          +-- AssertionError
          +-- AttributeError
          +-- BufferError
          +-- EOFError
          +-- ImportError
          |    +-- ModuleNotFoundError
          +-- LookupError
          |    +-- IndexError
          |    +-- KeyError
          +-- MemoryError
          +-- NameError
          |    +-- UnboundLocalError
          +-- OSError
          |    +-- BlockingIOError
          |    +-- ChildProcessError
          |    +-- ConnectionError
          |    |    +-- BrokenPipeError
          |    |    +-- ConnectionAbortedError
          |    |    +-- ConnectionRefusedError
          |    |    +-- ConnectionResetError
          |    +-- FileExistsError
          |    +-- FileNotFoundError
          |    +-- InterruptedError
          |    +-- IsADirectoryError
          |    +-- NotADirectoryError
          |    +-- PermissionError
          |    +-- ProcessLookupError
          |    +-- TimeoutError
          +-- ReferenceError
          +-- RuntimeError
          |    +-- NotImplementedError
          |    +-- RecursionError
          +-- SyntaxError
          |    +-- IndentationError
          |         +-- TabError
          +-- SystemError
          +-- TypeError
          +-- ValueError
          |    +-- UnicodeError
          |         +-- UnicodeDecodeError
          |         +-- UnicodeEncodeError
          |         +-- UnicodeTranslateError
          +-- Warning
               +-- DeprecationWarning


## ==⚡ Built-in Exceptions

In Python, all exceptions must be instances of a class that derives from *BaseException*. In a *try* statement with an *except* clause that mentions a particular class, that clause also handles any exception classes derived from that class (but not exception classes from which it is derived). Two exception classes that are not related via subclassing are never equivalent, even if they have the same name.

The built-in exceptions listed below can be generated by the interpreter or built-in functions. Except where mentioned, they have an “associated value” indicating the detailed cause of the error. This may be a string or a tuple of several items of information (e.g., an error code and a string explaining the code). The associated value is usually passed as arguments to the exception class’s constructor.

User code can raise built-in exceptions. This can be used to test an exception handler or to report an error condition “just like” the situation in which the interpreter raises the same exception; but beware that there is nothing to prevent user code from raising an inappropriate error.

The built-in exception classes can be subclassed to define new exceptions; programmers are encouraged to derive new exceptions from the Exception class or one of its subclasses, and not from *BaseException*. More information on defining exceptions is available in the Python Tutorial under User-defined Exceptions. The Python Tutorial » 8. Errors and Exceptions

When raising (or re-raising) an exception in an *except* or *finally* clause __context__ is automatically set to the last exception caught; if the new exception is not handled the traceback that is eventually displayed will include the originating exception(s) and the final exception.

When raising a new exception (rather than using a bare *raise* to re-raise the exception currently being handled), the implicit exception context can be supplemented with an explicit cause by using *from* with *raise*:


    raise new_exc from original_exc


The expression following from must be an exception or None. It will be set as __cause__ on the raised exception. Setting __cause__ also implicitly sets the __suppress_context__ attribute to True, so that using raise new_exc from None effectively replaces the old exception with the new one for display purposes (e.g. converting KeyError to AttributeError), while leaving the old exception available in __context__ for introspection when debugging.

The default traceback display code shows these chained exceptions in addition to the traceback for the exception itself. An explicitly chained exception in __cause__ is always shown when present. An implicitly chained exception in __context__ is shown only if __cause__ is None and __suppress_context__ is false.

In either case, the exception itself is always shown after any chained exceptions so that the final line of the traceback always shows the last exception that was raised.


## ==⚡ Base classes

The following exceptions are used mostly as base classes for other exceptions.
➡ **BaseException**
The base class for all built-in exceptions. It is not meant to be directly inherited by user-defined classes (for that, use Exception). If str() is called on an instance of this class, the representation of the argument(s) to the instance are returned, or the empty string when there were no arguments.
↪`args`
The tuple of arguments given to the exception constructor. Some built-in exceptions (like OSError) expect a certain number of arguments and assign a special meaning to the elements of this tuple, while others are usually called only with a single string giving an error message.
↪`with_traceback(tb)`
This method sets tb as the new traceback for the exception and returns the exception object. It is usually used in exception handling code like this:

    try:
        ...
    except SomeException:
        tb = sys.exc_info()[2]
        raise OtherException(...).with_traceback(tb)

➡ **Exception**
All built-in, non-system-exiting exceptions are derived from this class. All user-defined exceptions should also be derived from this class.
exception ArithmeticError
The base class for those built-in exceptions that are raised for various arithmetic errors: OverflowError, ZeroDivisionError, FloatingPointError.
➡ **BufferError**
Raised when a buffer related operation cannot be performed.
➡ **LookupError**
The base class for the exceptions that are raised when a key or index used on a mapping or sequence is invalid: IndexError, KeyError. This can be raised directly by codecs.lookup().


## ==⚡ Concrete exceptions

The following exceptions are the exceptions that are usually raised.
➡ **AssertionError**
Raised when an assert statement fails.
➡ **AttributeError**
Raised when an attribute reference (see Attribute references) or assignment fails. (When an object does not support attribute references or attribute assignments at all, TypeError is raised.)
➡ **EOFError**
Raised when the input() function hits an end-of-file condition (EOF) without reading any data. (N.B.: the io.IOBase.read() and io.IOBase.readline() methods return an empty string when they hit EOF.)
➡ **FloatingPointError**
Not currently used.
➡ **GeneratorExit**
Raised when a generator or coroutine is closed; see generator.close() and coroutine.close(). It directly inherits from BaseException instead of Exception since it is technically not an error.
➡ **ImportError**
Raised when the import statement has troubles trying to load a module. Also raised when the “from list” in from ... import has a name that cannot be found.

The name and path attributes can be set using keyword-only arguments to the constructor. When set they represent the name of the module that was attempted to be imported and the path to any file which triggered the exception, respectively.


Changed in version 3.3: Added the name and path attributes.
➡ **ModuleNotFoundError**
A subclass of ImportError which is raised by import when a module could not be located. It is also raised when None is found in sys.modules.


New in version 3.6.
➡ **IndexError**
Raised when a sequence subscript is out of range. (Slice indices are silently truncated to fall in the allowed range; if an index is not an integer, TypeError is raised.)
➡ **KeyError**
Raised when a mapping (dictionary) key is not found in the set of existing keys.
➡ **KeyboardInterrupt**
Raised when the user hits the interrupt key (normally Control-C or Delete). During execution, a check for interrupts is made regularly. The exception inherits from BaseException so as to not be accidentally caught by code that catches Exception and thus prevent the interpreter from exiting.
➡ **MemoryError**
Raised when an operation runs out of memory but the situation may still be rescued (by deleting some objects). The associated value is a string indicating what kind of (internal) operation ran out of memory. Note that because of the underlying memory management architecture (C’s malloc() function), the interpreter may not always be able to completely recover from this situation; it nevertheless raises an exception so that a stack traceback can be printed, in case a run-away program was the cause.
➡ **NameError**
Raised when a local or global name is not found. This applies only to unqualified names. The associated value is an error message that includes the name that could not be found.
➡ **NotImplementedError**
This exception is derived from RuntimeError. In user defined base classes, abstract methods should raise this exception when they require derived classes to override the method, or while the class is being developed to indicate that the real implementation still needs to be added.

Note:
 It should not be used to indicate that an operator or method is not meant to be supported at all – in that case either leave the operator / method undefined or, if a subclass, set it to None.
 

Note:
 NotImplementedError and NotImplemented are not interchangeable, even though they have similar names and purposes. See NotImplemented for details on when to use it.
 
➡ **OSError([arg])exception OSError(errno, strerror[, filename[, winerror[, filename2]]])**
This exception is raised when a system function returns a system-related error, including I/O failures such as “file not found” or “disk full” (not for illegal argument types or other incidental errors).

The second form of the constructor sets the corresponding attributes, described below. The attributes default to None if not specified. For backwards compatibility, if three arguments are passed, the args attribute contains only a 2-tuple of the first two constructor arguments.

The constructor often actually returns a subclass of OSError, as described in OS exceptions below. The particular subclass depends on the final errno value. This behaviour only occurs when constructing OSError directly or via an alias, and is not inherited when subclassing.
↪ `errno` A numeric error code from the C variable errno.
↪ `winerror` Under Windows, this gives you the native Windows error code. The errno attribute is then an approximate translation, in POSIX terms, of that native error code.

Under Windows, if the winerror constructor argument is an integer, the errno attribute is determined from the Windows error code, and the errno argument is ignored. On other platforms, the winerror argument is ignored, and the winerror attribute does not exist.
↪ `strerror` The corresponding error message, as provided by the operating system. It is formatted by the C functions perror() under POSIX, and FormatMessage() under Windows.
↪ `filenamefilename2` For exceptions that involve a file system path (such as open() or os.unlink()), filename is the file name passed to the function. For functions that involve two file system paths (such as os.rename()), filename2 corresponds to the second file name passed to the function.


Changed in version 3.3: EnvironmentError, IOError, WindowsError, socket.error, select.error and mmap.error have been merged into OSError, and the constructor may return a subclass.


Changed in version 3.4: The filename attribute is now the original file name passed to the function, instead of the name encoded to or decoded from the filesystem encoding. Also, the filename2 constructor argument and attribute was added.
➡ **OverflowError**
Raised when the result of an arithmetic operation is too large to be represented. This cannot occur for integers (which would rather raise MemoryError than give up). However, for historical reasons, OverflowError is sometimes raised for integers that are outside a required range. Because of the lack of standardization of floating point exception handling in C, most floating point operations are not checked.
➡ **RecursionError**
This exception is derived from RuntimeError. It is raised when the interpreter detects that the maximum recursion depth (see sys.getrecursionlimit()) is exceeded.


New in version 3.5: Previously, a plain RuntimeError was raised.
➡ **ReferenceError**
This exception is raised when a weak reference proxy, created by the weakref.proxy() function, is used to access an attribute of the referent after it has been garbage collected. For more information on weak references, see the weakref module.
➡ **RuntimeError**
Raised when an error is detected that doesn’t fall in any of the other categories. The associated value is a string indicating what precisely went wrong.
➡ **StopIteration**
Raised by built-in function next() and an iterator’s __next__() method to signal that there are no further items produced by the iterator.

The exception object has a single attribute value, which is given as an argument when constructing the exception, and defaults to None.

When a generator or coroutine function returns, a new StopIteration instance is raised, and the value returned by the function is used as the value parameter to the constructor of the exception.

If a generator code directly or indirectly raises StopIteration, it is converted into a RuntimeError (retaining the StopIteration as the new exception’s cause).


Changed in version 3.3: Added value attribute and the ability for generator functions to use it to return a value.


Changed in version 3.5: Introduced the RuntimeError transformation via from __future__ import generator_stop, see PEP 479.


Changed in version 3.7: Enable PEP 479 for all code by default: a StopIteration error raised in a generator is transformed into a RuntimeError.
➡ **StopAsyncIteration**
Must be raised by __anext__() method of an asynchronous iterator object to stop the iteration.


New in version 3.5.
➡ **SyntaxError**
Raised when the parser encounters a syntax error. This may occur in an import statement, in a call to the built-in functions exec() or eval(), or when reading the initial script or standard input (also interactively).

The str() of the exception instance returns only the error message.
↪ `filename` The name of the file the syntax error occurred in.
↪ `lineno` Which line number in the file the error occurred in. This is 1-indexed: the first line in the file has a lineno of 1.
↪ `offset` The column in the line where the error occurred. This is 1-indexed: the first character in the line has an offset of 1.
↪ `text` The source code text involved in the error.

➡ **IndentationError**
Base class for syntax errors related to incorrect indentation. This is a subclass of SyntaxError.
➡ **TabError**
Raised when indentation contains an inconsistent use of tabs and spaces. This is a subclass of IndentationError.
➡ **SystemError**
Raised when the interpreter finds an internal error, but the situation does not look so serious to cause it to abandon all hope. The associated value is a string indicating what went wrong (in low-level terms).

You should report this to the author or maintainer of your Python interpreter. Be sure to report the version of the Python interpreter (sys.version; it is also printed at the start of an interactive Python session), the exact error message (the exception’s associated value) and if possible the source of the program that triggered the error.
➡ **SystemExit**
This exception is raised by the sys.exit() function. It inherits from BaseException instead of Exception so that it is not accidentally caught by code that catches Exception. This allows the exception to properly propagate up and cause the interpreter to exit. When it is not handled, the Python interpreter exits; no stack traceback is printed. The constructor accepts the same optional argument passed to sys.exit(). If the value is an integer, it specifies the system exit status (passed to C’s exit() function); if it is None, the exit status is zero; if it has another type (such as a string), the object’s value is printed and the exit status is one.

A call to sys.exit() is translated into an exception so that clean-up handlers (finally clauses of try statements) can be executed, and so that a debugger can execute a script without running the risk of losing control. The `os._exit()` function can be used if it is absolutely positively necessary to exit immediately (for example, in the child process after a call to `os.fork()`).
code
The exit status or error message that is passed to the constructor. (Defaults to None.)
➡ **TypeError**
Raised when an operation or function is applied to an object of inappropriate type. The associated value is a string giving details about the type mismatch.

This exception may be raised by user code to indicate that an attempted operation on an object is not supported, and is not meant to be. If an object is meant to support a given operation but has not yet provided an implementation, NotImplementedError is the proper exception to raise.

Passing arguments of the wrong type (e.g. passing a list when an int is expected) should result in a TypeError, but passing arguments with the wrong value (e.g. a number outside expected boundaries) should result in a ValueError.
➡ **UnboundLocalError**
Raised when a reference is made to a local variable in a function or method, but no value has been bound to that variable. This is a subclass of NameError.
➡ **UnicodeError**
Raised when a Unicode-related encoding or decoding error occurs. It is a subclass of ValueError.

UnicodeError has attributes that describe the encoding or decoding error. For example, err.object[err.start:err.end] gives the particular invalid input that the codec failed on.
↪ `encoding` The name of the encoding that raised the error.
↪ `reason` A string describing the specific codec error.
↪ `object` The object the codec was attempting to encode or decode.
↪ `start` The first index of invalid data in object.
↪ `end` The index after the last invalid data in object.

➡ **UnicodeEncodeError**
Raised when a Unicode-related error occurs during encoding. It is a subclass of UnicodeError.
➡ **UnicodeDecodeError**
Raised when a Unicode-related error occurs during decoding. It is a subclass of UnicodeError.
➡ **UnicodeTranslateError**
Raised when a Unicode-related error occurs during translating. It is a subclass of UnicodeError.
➡ **ValueError**
Raised when an operation or function receives an argument that has the right type but an inappropriate value, and the situation is not described by a more precise exception such as IndexError.
➡ **ZeroDivisionError**
Raised when the second argument of a division or modulo operation is zero. The associated value is a string indicating the type of the operands and the operation.

The following exceptions are kept for compatibility with previous versions; starting from Python 3.3, they are aliases of OSError.
➡ **EnvironmentErrorexception IOErrorexception WindowsError**
Only available on Windows.


## ==⚡ OS exceptions

The following exceptions are subclasses of OSError, they get raised depending on the system error code.
➡ **BlockingIOError**
Raised when an operation would block on an object (e.g. socket) set for non-blocking operation. Corresponds to errno EAGAIN, EALREADY, EWOULDBLOCK and EINPROGRESS.

In addition to those of OSError, BlockingIOError can have one more attribute:
characters_written
An integer containing the number of characters written to the stream before it blocked. This attribute is available when using the buffered I/O classes from the io module.
➡ **ChildProcessError**
Raised when an operation on a child process failed. Corresponds to errno ECHILD.
➡ **ConnectionError**
A base class for connection-related issues.

Subclasses are BrokenPipeError, ConnectionAbortedError, ConnectionRefusedError and ConnectionResetError.
➡ **BrokenPipeError**
A subclass of ConnectionError, raised when trying to write on a pipe while the other end has been closed, or trying to write on a socket which has been shutdown for writing. Corresponds to errno EPIPE and ESHUTDOWN.
➡ **ConnectionAbortedError**
A subclass of ConnectionError, raised when a connection attempt is aborted by the peer. Corresponds to errno ECONNABORTED.
➡ **ConnectionRefusedError**
A subclass of ConnectionError, raised when a connection attempt is refused by the peer. Corresponds to errno ECONNREFUSED.
➡ **ConnectionResetError**
A subclass of ConnectionError, raised when a connection is reset by the peer. Corresponds to errno ECONNRESET.
➡ **FileExistsError**
Raised when trying to create a file or directory which already exists. Corresponds to errno EEXIST.
➡ **FileNotFoundError**
Raised when a file or directory is requested but doesn’t exist. Corresponds to errno ENOENT.
➡ **InterruptedError**
Raised when a system call is interrupted by an incoming signal. Corresponds to errno EINTR.


Changed in version 3.5: Python now retries system calls when a syscall is interrupted by a signal, except if the signal handler raises an exception (see PEP 475 for the rationale), instead of raising InterruptedError.
➡ **IsADirectoryError**
Raised when a file operation (such as os.remove()) is requested on a directory. Corresponds to errno EISDIR.
➡ **NotADirectoryError**
Raised when a directory operation (such as os.listdir()) is requested on something which is not a directory. Corresponds to errno ENOTDIR.
➡ **PermissionError**
Raised when trying to run an operation without the adequate access rights - for example filesystem permissions. Corresponds to errno EACCES and EPERM.
➡ **ProcessLookupError**
Raised when a given process doesn’t exist. Corresponds to errno ESRCH.
➡ **TimeoutError**
Raised when a system function timed out at the system level. Corresponds to errno ETIMEDOUT.


New in version 3.3: All the above OSError subclasses were added.

See also:
 PEP 3151 - Reworking the OS and IO exception hierarchy
 


## ==⚡ Warnings

The following exceptions are used as warning categories; see the Warning Categories documentation for more details.
➡ **Warning**
Base class for warning categories.
➡ **UserWarning**
Base class for warnings generated by user code.
➡ **DeprecationWarning**
Base class for warnings about deprecated features when those warnings are intended for other Python developers.

Ignored by the default warning filters, except in the __main__ module (PEP 565). Enabling the Python Development Mode shows this warning.
➡ **PendingDeprecationWarning**
Base class for warnings about features which are obsolete and expected to be deprecated in the future, but are not deprecated at the moment.

This class is rarely used as emitting a warning about a possible upcoming deprecation is unusual, and DeprecationWarning is preferred for already active deprecations.

Ignored by the default warning filters. Enabling the Python Development Mode shows this warning.
➡ **SyntaxWarning**
Base class for warnings about dubious syntax.
➡ **RuntimeWarning**
Base class for warnings about dubious runtime behavior.
➡ **FutureWarning**
Base class for warnings about deprecated features when those warnings are intended for end users of applications that are written in Python.
➡ **ImportWarning**
Base class for warnings about probable mistakes in module imports.

Ignored by the default warning filters. Enabling the Python Development Mode shows this warning.
➡ **UnicodeWarning**
Base class for warnings related to Unicode.
➡ **BytesWarning**
Base class for warnings related to bytes and bytearray.
➡ **ResourceWarning**
Base class for warnings related to resource usage.

Ignored by the default warning filters. Enabling the Python Development Mode shows this warning.


New in version 3.2




# =🚩 Text Processing Services
- https://docs.python.org/3.9/library/text.html

The modules described in this chapter provide a wide range of string manipulation operations and other text processing services.

The codecs module described under Binary Data Services is also highly relevant to text processing. In addition, see the documentation for Python’s built-in string type in Text Sequence Type — str.

• string — Common string operations
◦ String constants
◦ Custom String Formatting
◦ Format String Syntax
◾ Format Specification Mini-Language
◾ Format examples

◦ Template strings
◦ Helper functions

• re — Regular expression operations
◦ Regular Expression Syntax
◦ Module Contents
◦ Regular Expression Objects
◦ Match Objects
◦ Regular Expression Examples
◾ Checking for a Pair
◾ Simulating scanf()
◾ search() vs. match()
◾ Making a Phonebook
◾ Text Munging
◾ Finding all Adverbs
◾ Finding all Adverbs and their Positions
◾ Raw String Notation
◾ Writing a Tokenizer


• difflib — Helpers for computing deltas
◦ SequenceMatcher Objects
◦ SequenceMatcher Examples
◦ Differ Objects
◦ Differ Example
◦ A command-line interface to difflib

• textwrap — Text wrapping and filling
• unicodedata — Unicode Database
• stringprep — Internet String Preparation
• readline — GNU readline interface
◦ Init file
◦ Line buffer
◦ History file
◦ History list
◦ Startup hooks
◦ Completion
◦ Example

• rlcompleter — Completion function for GNU readline
◦ Completer Objects


## ==⚡ • string — Common string operations


## ==⚡ • re — Regular expression operations
- https://docs.python.org/3/library/re.html

利用正则表达式提取符合规则的字符串：

```py
import requests
import time
import re

def download_page(url):
    with requests.Session() as session:
        return session.get(url)

def main():
    site= "http://olympus.realpython.org/dice"
    start_time = time.time()
    res = download_page(site)
    duration = time.time() - start_time
    dice = r".*result..(\d)"
    title = r".*<h1>(.*)</h1>"
    print(f"""
        Downloaded {len([site])} in {duration} seconds
        {re.search(title, res.text)[1]}
        {re.search(dice, res.text)[1]}
        """)

if __name__ == '__main__':
    main()
```


Python 中使用正则表达式，首先执行编译方法生成正则表达式对象 Regular Expression ，由于该对象自己包含了正则表达式，所以调用对应的方法时不用给出正则字符串。

然后用编译后的正则表达式去匹配字符串。如果一个正则表达式要重复使用，出于效率的考虑，可以预编译该正则表达式，直接匹配：

```py
import re

# 编译:
re_telephone = re.compile(r'^(\d{3})-(\d{3,8})$')

# 使用：
re_telephone.match('010-12345').groups()
# ('010', '12345')
re_telephone.match('010-8086').groups()
# ('010', '8086')
```

也可以将正则表达式规则直接在 re 包提供的其它功能函数执行：

```py
re.match(r'^\d{3}\-\d{3,8}$', '010-12345')
re.split(r'\s+', 'a b   c')
# ['a', 'b', 'c']
```

常见的判断方法就是 match() 匹配内容，它只从头开始匹配内容，多行模式也一样，返回值就是一个 Match 对象，否则返回 None：

```py
test = '用户输入的字符串'
if re.match(r'正则表达式', test):
    print('ok')
else:
    print('failed')

# 正则实例
reg_jpg = r'(?:src=)"(.+\.jpg)"'
reg_pdf = r'(?:href|HREF)="?((?:http://)?.+?\.pdf)'

re.match(r'.*result..(\d)','<p id="result">3</p>')[1] # 3 
re.search(r'.*result..(\d)','<p \r\nid="result">3</p>')[1]

re.match("dave", "Dave", flags=re.I)
re.match("dave", "Dave", flags=re.IGNORECASE)

# To use an inline flag name globally, 
# write it like "(?i)" and include it at the beginning of the regex.
# To use an inline flag name locally, 
# write it like "(?i:...)" instead of "(?i)...". E.g.
re.match("(?i)dave", "Dave")
re.match("hello (?i:dave)", "HELLO Dave")
```


方法参考：

```py
re.compile(pattern, flags=0)
re.search(pattern, string, flags=0)
re.fullmatch(pattern, string, flags=0)
re.match(pattern, string, flags=0)
re.split(pattern, string, maxsplit=0, flags=0)¶
re.finditer(pattern, string, flags=0)
re.sub(pattern, repl, string, count=0, flags=0)
re.subn(pattern, repl, string, count=0, flags=0)
re.escape(pattern)
re.purge()
exception re.error(msg, pattern=None, pos=None)
```

Source code: Lib/re.py


This module provides regular expression matching operations similar to those found in Perl.

Both patterns and strings to be searched can be Unicode strings (str) as well as 8-bit strings (bytes). However, Unicode strings and 8-bit strings cannot be mixed: that is, you cannot match a Unicode string with a byte pattern or vice-versa; similarly, when asking for a substitution, the replacement string must be of the same type as both the pattern and the search string.

Regular expressions use the backslash character ('\') to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Python’s usage of the same character for the same purpose in string literals; for example, to match a literal backslash, one might have to write '\\\\' as the pattern string, because the regular expression must be \\, and each backslash must be expressed as \\ inside a regular Python string literal. Also, please note that any invalid escape sequences in Python’s usage of the backslash in string literals now generate a DeprecationWarning and in the future this will become a SyntaxError. This behaviour will happen even if it is a valid escape sequence for a regular expression.

The solution is to use Python’s raw string notation for regular expression patterns; backslashes are not handled in any special way in a string literal prefixed with 'r'. So r"\n" is a two-character string containing '\' and 'n', while "\n" is a one-character string containing a newline. Usually patterns will be expressed in Python code using this raw string notation.

It is important to note that most regular expression operations are available as module-level functions and methods on compiled regular expressions. The functions are shortcuts that don’t require you to compile a regex object first, but miss some fine-tuning parameters.

See also:
 The third-party regex module, which has an API compatible with the standard library re module, but offers additional functionality and a more thorough Unicode support.
 


### ===🗝 Regular Expression Syntax

A regular expression (or RE) specifies a set of strings that matches it; the functions in this module let you check if a particular string matches a given regular expression (or if a given regular expression matches a particular string, which comes down to the same thing).

Regular expressions can be concatenated to form new regular expressions; if A and B are both regular expressions, then AB is also a regular expression. In general, if a string p matches A and another string q matches B, the string pq will match AB. This holds unless A or B contain low precedence operations; boundary conditions between A and B; or have numbered group references. Thus, complex expressions can easily be constructed from simpler primitive expressions like the ones described here. For details of the theory and implementation of regular expressions, consult the Friedl book [Frie09], or almost any textbook about compiler construction.

A brief explanation of the format of regular expressions follows. For further information and a gentler presentation, consult the Regular Expression HOWTO.

Regular expressions can contain both special and ordinary characters. Most ordinary characters, like 'A', 'a', or '0', are the simplest regular expressions; they simply match themselves. You can concatenate ordinary characters, so last matches the string 'last'. (In the rest of this section, we’ll write RE’s in this special style, usually without quotes, and strings to be matched 'in single quotes'.)

Some characters, like `'|'` or `'('`, are special. Special characters either stand for classes of ordinary characters, or affect how the regular expressions around them are interpreted.

Repetition qualifiers (`*, +, ?, {m,n}`, etc) cannot be directly nested. This avoids ambiguity with the non-greedy modifier suffix ?, and with other modifiers in other implementations. To apply a second repetition to an inner repetition, parentheses may be used. For example, the expression `(?:a{6})*` matches any multiple of six 'a' characters.

The special characters are:

➡ .(Dot.) In the default mode, this matches any character except a newline. If the DOTALL flag has been specified, this matches any character including a newline.

➡ ^(Caret.) Matches the start of the string, and in `MULTILINE` mode also matches immediately after each newline.

➡ $ Matches the end of the string or just before the newline at the end of the string, and in `MULTILINE` mode also matches before a newline. foo matches both ‘foo’ and ‘foobar’, while the regular expression foo$ matches only ‘foo’. More interestingly, searching for foo.$ in 'foo1\nfoo2\n' matches ‘foo2’ normally, but ‘foo1’ in `MULTILINE` mode; searching for a single $ in 'foo\n' will find two (empty) matches: one just before the newline, and one at the end of the string.

➡ * Causes the resulting RE to match 0 or more repetitions of the preceding RE, as many repetitions as are possible. ab* will match ‘a’, ‘ab’, or ‘a’ followed by any number of ‘b’s.

➡ + Causes the resulting RE to match 1 or more repetitions of the preceding RE. ab+ will match ‘a’ followed by any non-zero number of ‘b’s; it will not match just ‘a’.

➡ ? Causes the resulting RE to match 0 or 1 repetitions of the preceding RE. ab? will match either ‘a’ or ‘ab’.

➡ `*?, +?, ??` The `'*', '+', and '?'` qualifiers are all greedy; they match as much text as possible. Sometimes this behaviour isn’t desired; if the RE `<.*>` is matched against `'<a> b <c>'`, it will match the entire string, and not just `'<a>'`. Adding ? after the qualifier makes it perform the match in non-greedy or minimal fashion; as few characters as possible will be matched. Using the RE `<.*?>` will match only `'<a>'`.

➡ {m} Specifies that exactly m copies of the previous RE should be matched; fewer matches cause the entire RE not to match. For example, `a{6}` will match exactly six 'a' characters, but not five.

➡ {m,n} Causes the resulting RE to match from m to n repetitions of the preceding RE, attempting to match as many repetitions as possible. For example, a{3,5} will match from 3 to 5 'a' characters. Omitting m specifies a lower bound of zero, and omitting n specifies an infinite upper bound. As an example, a{4,}b will match 'aaaab' or a thousand 'a' characters followed by a 'b', but not 'aaab'. The comma may not be omitted or the modifier would be confused with the previously described form.

➡ {m,n}? Causes the resulting RE to match from m to n repetitions of the preceding RE, attempting to match as few repetitions as possible. This is the non-greedy version of the previous qualifier. For example, on the 6-character string 'aaaaaa', `a{3,5}` will match 5 'a' characters, while `a{3,5}?` will only match 3 characters.

➡ \ Either escapes special characters (permitting you to match characters like `'*', '?'`, and so forth), or signals a special sequence; special sequences are discussed below.

If you’re not using a raw string to express the pattern, remember that Python also uses the backslash as an escape sequence in string literals; if the escape sequence isn’t recognized by Python’s parser, the backslash and subsequent character are included in the resulting string. However, if Python would recognize the resulting sequence, the backslash should be repeated twice. This is complicated and hard to understand, so it’s highly recommended that you use raw strings for all but the simplest expressions.

➡ [] Used to indicate a set of characters. In a set:

• Characters can be listed individually, e.g. [amk] will match 'a', 'm', or 'k'.

• Ranges of characters can be indicated by giving two characters and separating them by a '-', for example [a-z] will match any lowercase ASCII letter, [0-5][0-9] will match all the two-digits numbers from 00 to 59, and [0-9A-Fa-f] will match any hexadecimal digit. If - is escaped (e.g. [a\-z]) or if it’s placed as the first or last character (e.g. [-a] or [a-]), it will match a literal '-'.

• Special characters lose their special meaning inside sets. For example, `[(+*)]` will match any of the literal characters `'(', '+', '*', or ')'`.

• Character classes such as \w or \S (defined below) are also accepted inside a set, although the characters they match depends on whether ASCII or LOCALE mode is in force.

• Characters that are not within a range can be matched by complementing the set. If the first character of the set is '^', all the characters that are not in the set will be matched. For example, `[^5]` will match any character except '5', and `[^^]` will match any character except '^'. ^ has no special meaning if it’s not the first character in the set.

• To match a literal ']' inside a set, precede it with a backslash, or place it at the beginning of the set. For example, both `[()[\]{}]` and `[]()[{}]` will both match a parenthesis.

• Support of nested sets and set operations as in Unicode Technical Standard #18 might be added in the future. This would change the syntax, so to facilitate this change a FutureWarning will be raised in ambiguous cases for the time being. That includes sets starting with a literal '[' or containing literal character sequences `'--', '&&', '~~', and '||'`. To avoid a warning escape them with a backslash.


Changed in version 3.7: FutureWarning is raised if a character set contains constructs that will change semantically in the future.

➡ `|` Such `A|B`, where A and B can be arbitrary REs, creates a regular expression that will match either A or B. An arbitrary number of REs can be separated by the `'|'` in this way. This can be used inside groups (see below) as well. As the target string is scanned, REs separated by `'|'` are tried from left to right. When one pattern completely matches, that branch is accepted. This means that once A matches, B will not be tested further, even if it would produce a longer overall match. In other words, the `'|'` operator is never greedy. To match a literal `'|'`, use `\|`, or enclose it inside a character class, as in `[|]`.

➡ (...) Matches whatever regular expression is inside the parentheses, and indicates the start and end of a group; the contents of a group can be retrieved after a match has been performed, and can be matched later in the string with the \number special sequence, described below. To match the literals '(' or ')', use \( or \), or enclose them inside a character class: [(], [)].

➡ (?...) This is an extension notation (a `'?'` following a `'('` is not meaningful otherwise). The first character after the '?' determines what the meaning and further syntax of the construct is. Extensions usually do not create a new group; `(?P<name>...)` is the only exception to this rule. Following are the currently supported extensions.

➡ (?aiLmsux) (One or more letters from the set 'a', 'i', 'L', 'm', 's', 'u', 'x'.) The group matches the empty string; the letters set the corresponding flags: 

- `re.A` (ASCII-only matching),
- `re.I` (ignore case),
- `re.L` (locale dependent),
- `re.M` (multi-line),
- `re.S` (dot matches all),
- `re.U` (Unicode matching), and
- `re.X` (verbose), for the entire regular expression. 

The flags are described in Module Contents.

This is useful if you wish to include the flags as part of the regular expression, instead of passing a flag argument to the re.compile() function. Flags should be used first in the expression string.

➡ (?:...) A non-capturing version of regular parentheses. Matches whatever regular expression is inside the parentheses, but the substring matched by the group cannot be retrieved after performing a match or referenced later in the pattern.

➡ (?aiLmsux-imsx:...) Zero or more letters from the set 'a', 'i', 'L', 'm', 's', 'u', 'x', optionally followed by '-' followed by one or more letters from the 'i', 'm', 's', 'x'. 

The letters set or remove the corresponding flags: 

- `re.A` (ASCII-only matching),
- `re.I` (ignore case),
- `re.L` (locale dependent),
- `re.M` (multi-line),
- `re.S` (dot matches all),
- `re.U` (Unicode matching), and
- `re.X` (verbose), for the part of the expression.

The flags are described in Module Contents.

The letters 'a', 'L' and 'u' are mutually exclusive when used as inline flags, so they can’t be combined or follow '-'. Instead, when one of them appears in an inline group, it overrides the matching mode in the enclosing group. In Unicode patterns `(?a:...)` switches to ASCII-only matching, and `(?u:...) `switches to Unicode matching (default). In byte pattern `(?L:...)` switches to locale depending matching, and `(?a:...)` switches to ASCII-only matching (default). This override is only in effect for the narrow inline group, and the original matching mode is restored outside of the group.

New in version 3.6.

Changed in version 3.7: The letters 'a', 'L' and 'u' also can be used in a group.

➡ (?P<name>...) Similar to regular parentheses, but the substring matched by the group is accessible via the symbolic group name name. Group names must be valid Python identifiers, and each group name must be defined only once within a regular expression. A symbolic group is also a numbered group, just as if the group were not named.

Named groups can be referenced in three contexts. If the pattern is `(?P<quote>['"]).*?(?P=quote)` (i.e. matching a string quoted with either single or double quotes):


|        Context of reference to group “quote”        |   Ways to reference it  |
|-----------------------------------------------------|-------------------------|
| in the same pattern itself                          | • (?P=quote) (as shown) |
|                                                     | • \1                    |
|-----------------------------------------------------|-------------------------|
| when processing match object m                      | • m.group('quote')      |
|                                                     | • m.end('quote') (etc.) |
|-----------------------------------------------------|-------------------------|
| in a string passed to the repl argument of re.sub() | • \g<quote>             |
|                                                     | • \g<1>                 |
|                                                     | • \1                    |
 
➡ (?P=name) 
A backreference to a named group; it matches whatever text was matched by the earlier group named name.

➡ (?#...) 
A comment; the contents of the parentheses are simply ignored.

➡ (?=...) 
Matches if ... matches next, but doesn’t consume any of the string. This is called a lookahead assertion. For example, Isaac `(?=Asimov)` will match 'Isaac ' only if it’s followed by 'Asimov'.

➡ (?!...) 
Matches if ... doesn’t match next. This is a negative lookahead assertion. For example, Isaac `(?!Asimov)` will match 'Isaac ' only if it’s not followed by 'Asimov'.

➡ (?<=...)
Matches if the current position in the string is preceded by a match for ... that ends at the current position. This is called a positive lookbehind assertion. `(?<=abc)` def will find a match in 'abcdef', since the lookbehind will back up 3 characters and check if the contained pattern matches. The contained pattern must only match strings of some fixed length, meaning that `abc` or `a|b` are allowed, but `a*` and `a{3,4}` are not. Note that patterns which start with positive lookbehind assertions will not match at the beginning of the string being searched; you will most likely want to use the search() function rather than the match() function:


>>> import re
>>> m = re.search('(?<=abc)def', 'abcdef')
>>> m.group(0)
'def'


This example looks for a word following a hyphen:


>>> m = re.search(r'(?<=-)\w+', 'spam-egg')
>>> m.group(0)
'egg'


Changed in version 3.5: Added support for group references of fixed length.


➡ (?<!...)
Matches if the current position in the string is not preceded by a match for .... This is called a negative lookbehind assertion. Similar to positive lookbehind assertions, the contained pattern must only match strings of some fixed length. Patterns which start with negative lookbehind assertions may match at the beginning of the string being searched.

➡ `(?(id/name)yes-pattern|no-pattern)` Will try to match with yes-pattern if the group with given id or name exists, and with no-pattern if it doesn’t. no-pattern is optional and can be omitted. For example, `(<)?(\w+@\w+(?:\.\w+)+)(?(1)>|$)` is a poor email matching pattern, which will match with '<user@host.com>' as well as 'user@host.com', but not with `'<user@host.com'` nor `'user@host.com>'`.

The special sequences consist of '\' and a character from the list below. If the ordinary character is not an ASCII digit or an ASCII letter, then the resulting RE will match the second character. For example, \$ matches the character '$'.

➡ \number 
Matches the contents of the group of the same number. Groups are numbered starting from 1. For example, (.+) \1 matches 'the the' or '55 55', but not 'thethe' (note the space after the group). This special sequence can only be used to match one of the first 99 groups. If the first digit of number is 0, or number is 3 octal digits long, it will not be interpreted as a group match, but as the character with octal value number. Inside the '[' and ']' of a character class, all numeric escapes are treated as characters.\AMatches only at the start of the string.

➡ \b
Matches the empty string, but only at the beginning or end of a word. A word is defined as a sequence of word characters. Note that formally, \b is defined as the boundary between a \w and a \W character (or vice versa), or between \w and the beginning/end of the string. This means that r'\bfoo\b' matches 'foo', 'foo.', '(foo)', 'bar foo baz' but not 'foobar' or 'foo3'.

By default Unicode alphanumerics are the ones used in Unicode patterns, but this can be changed by using the ASCII flag. Word boundaries are determined by the current locale if the LOCALE flag is used. Inside a character range, \b represents the backspace character, for compatibility with Python’s string literals.

➡ \B
Matches the empty string, but only when it is not at the beginning or end of a word. This means that r'py\B' matches 'python', 'py3', 'py2', but not 'py', 'py.', or 'py!'. \B is just the opposite of \b, so word characters in Unicode patterns are Unicode alphanumerics or the underscore, although this can be changed by using the ASCII flag. Word boundaries are determined by the current locale if the LOCALE flag is used.

➡ \d
For Unicode (str) patterns:Matches any Unicode decimal digit (that is, any character in Unicode character category [Nd]). This includes [0-9], and also many other digit characters. If the ASCII flag is used only [0-9] is matched.For 8-bit (bytes) patterns:Matches any decimal digit; this is equivalent to [0-9].

➡ \D
Matches any character which is not a decimal digit. This is the opposite of \d. If the ASCII flag is used this becomes the equivalent of [^0-9].

➡ \s
For Unicode (str) patterns:Matches Unicode whitespace characters (which includes [ \t\n\r\f\v], and also many other characters, for example the non-breaking spaces mandated by typography rules in many languages). If the ASCII flag is used, only [ \t\n\r\f\v] is matched.For 8-bit (bytes) patterns:Matches characters considered whitespace in the ASCII character set; this is equivalent to [ \t\n\r\f\v].

➡ \S
Matches any character which is not a whitespace character. This is the opposite of \s. If the ASCII flag is used this becomes the equivalent of [^ \t\n\r\f\v].

➡ \w
For Unicode (str) patterns:

Matches Unicode word characters; this includes most characters that can be part of a word in any language, as well as numbers and the underscore. If the ASCII flag is used, only [a-zA-Z0-9_] is matched.

For 8-bit (bytes) patterns:

Matches characters considered alphanumeric in the ASCII character set; this is equivalent to [a-zA-Z0-9_]. If the LOCALE flag is used, matches characters considered alphanumeric in the current locale and the underscore.

➡ \W
Matches any character which is not a word character. This is the opposite of \w. If the ASCII flag is used this becomes the equivalent of [^a-zA-Z0-9_]. If the LOCALE flag is used, matches characters which are neither alphanumeric in the current locale nor the underscore.

➡ \Z
Matches only at the end of the string.

Most of the standard escapes supported by Python string literals are also accepted by the regular expression parser:


    \a      \b      \f      \n
    \N      \r      \t      \u
    \U      \v      \x      \\


(Note that \b is used to represent word boundaries, and means “backspace” only inside character classes.)

'\u', '\U', and '\N' escape sequences are only recognized in Unicode patterns. In bytes patterns they are errors. Unknown escapes of ASCII letters are reserved for future use and treated as errors.

Octal escapes are included in a limited form. If the first digit is a 0, or if there are three octal digits, it is considered an octal escape. Otherwise, it is a group reference. As for string literals, octal escapes are always at most three digits in length.


Changed in version 3.3: The '\u' and '\U' escape sequences have been added.


Changed in version 3.6: Unknown escapes consisting of '\' and an ASCII letter now are errors.


Changed in version 3.8: The '\N{name}' escape sequence has been added. As in string literals, it expands to the named Unicode character (e.g. '\N{EM DASH}').


### ===🗝 Module Contents

The module defines several functions, constants, and an exception. Some of the functions are simplified versions of the full featured methods for compiled regular expressions. Most non-trivial applications always use the compiled form.

Changed in version 3.6: Flag constants are now instances of RegexFlag, which is a subclass of enum.IntFlag.


➡ `re.compile(pattern, flags=0)`
Compile a regular expression pattern into a regular expression object, which can be used for matching using its match(), search() and other methods, described below.

The expression’s behaviour can be modified by specifying a flags value. Values can be any of the following variables, combined using bitwise OR (the `|` operator).

The sequence


    prog = re.compile(pattern)
    result = prog.match(string)


is equivalent to


    result = re.match(pattern, string)


but using re.compile() and saving the resulting regular expression object for reuse is more efficient when the expression will be used several times in a single program.

Note:
 The compiled versions of the most recent patterns passed to re.compile() and the module-level matching functions are cached, so programs that use only a few regular expressions at a time needn’t worry about compiling regular expressions.
 

➡ `re.Are.ASCII`
Make \w, \W, \b, \B, \d, \D, \s and \S perform ASCII-only matching instead of full Unicode matching. This is only meaningful for Unicode patterns, and is ignored for byte patterns. Corresponds to the inline flag (?a).

Note that for backward compatibility, the re.U flag still exists (as well as its synonym re.UNICODE and its embedded counterpart (?u)), but these are redundant in Python 3 since matches are Unicode by default for strings (and Unicode matching isn’t allowed for bytes).

➡ `re.DEBUG`
Display debug information about compiled expression. No corresponding inline flag.

➡ `re.Ire.IGNORECASE`
Perform case-insensitive matching; expressions like [A-Z] will also match lowercase letters. Full Unicode matching (such as Ü matching ü) also works unless the re.ASCII flag is used to disable non-ASCII matches. The current locale does not change the effect of this flag unless the re.LOCALE flag is also used. Corresponds to the inline flag (?i).

Note that when the Unicode patterns [a-z] or [A-Z] are used in combination with the IGNORECASE flag, they will match the 52 ASCII letters and 4 additional non-ASCII letters: ‘İ’ (U+0130, Latin capital letter I with dot above), ‘ı’ (U+0131, Latin small letter dotless i), ‘ſ’ (U+017F, Latin small letter long s) and ‘K’ (U+212A, Kelvin sign). If the ASCII flag is used, only letters ‘a’ to ‘z’ and ‘A’ to ‘Z’ are matched.

➡ `re.Lre.LOCALE`
Make \w, \W, \b, \B and case-insensitive matching dependent on the current locale. This flag can be used only with bytes patterns. The use of this flag is discouraged as the locale mechanism is very unreliable, it only handles one “culture” at a time, and it only works with 8-bit locales. Unicode matching is already enabled by default in Python 3 for Unicode (str) patterns, and it is able to handle different locales/languages. Corresponds to the inline flag (?L).


Changed in version 3.6: re.LOCALE can be used only with bytes patterns and is not compatible with re.ASCII.

Changed in version 3.7: Compiled regular expression objects with the re.LOCALE flag no longer depend on the locale at compile time. Only the locale at matching time affects the result of matching.


➡ `re.Mre.MULTILINE`
When specified, the pattern character '^' matches at the beginning of the string and at the beginning of each line (immediately following each newline); and the pattern character '$' matches at the end of the string and at the end of each line (immediately preceding each newline). By default, '^' matches only at the beginning of the string, and '$' only at the end of the string and immediately before the newline (if any) at the end of the string. Corresponds to the inline flag (?m).

➡ `re.Sre.DOTALL`
Make the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).

➡ `re.Xre.VERBOSE`
This flag allows you to write regular expressions that look nicer and are more readable by allowing you to visually separate logical sections of the pattern and add comments. Whitespace within the pattern is ignored, except when in a character class, or when preceded by an unescaped backslash, or within tokens like `*?, (?: or (?P<...>`. When a line contains a # that is not in a character class and is not preceded by an unescaped backslash, all characters from the leftmost such # through the end of the line are ignored.

This means that the two following regular expression objects that match a decimal number are functionally equal:


```py
a = re.compile(r"""\d +  # the integral part
                   \.    # the decimal point
                   \d *  # some fractional digits""", re.X)
b = re.compile(r"\d+\.\d*")
```

Corresponds to the inline flag (?x).

➡ `re.search(pattern, string, flags=0)`
Scan through string looking for the first location where the regular expression pattern produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string.

➡ `re.match(pattern, string, flags=0)`
If zero or more characters at the beginning of string match the regular expression pattern, return a corresponding match object. Return `None` if the string does not match the pattern; note that this is different from a zero-length match.

Note that even in MULTILINE mode, re.match() will only match at the beginning of the string and not at the beginning of each line.

If you want to locate a match anywhere in string, use search() instead (see also search() vs. match()).

➡ `re.fullmatch(pattern, string, flags=0)`
If the whole string matches the regular expression pattern, return a corresponding match object. Return None if the string does not match the pattern; note that this is different from a zero-length match.


New in version 3.4.

➡ `re.split(pattern, string, maxsplit=0, flags=0)`
Split string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list.


>>> re.split(r'\W+', 'Words, words, words.')
['Words', 'words', 'words', '']
>>> re.split(r'(\W+)', 'Words, words, words.')
['Words', ', ', 'words', ', ', 'words', '.', '']
>>> re.split(r'\W+', 'Words, words, words.', 1)
['Words', 'words, words.']
>>> re.split('[a-f]+', '0a3B9', flags=re.IGNORECASE)
['0', '3', '9']


If there are capturing groups in the separator and it matches at the start of the string, the result will start with an empty string. The same holds for the end of the string:


>>> re.split(r'(\W+)', '...words, words...')
['', '...', 'words', ', ', 'words', '...', '']


That way, separator components are always found at the same relative indices within the result list.

Empty matches for the pattern split the string only when not adjacent to a previous empty match.


>>> re.split(r'\b', 'Words, words, words.')
['', 'Words', ', ', 'words', ', ', 'words', '.']
>>> re.split(r'\W*', '...words...')
['', '', 'w', 'o', 'r', 'd', 's', '', '']
>>> re.split(r'(\W*)', '...words...')
['', '...', '', '', 'w', '', 'o', '', 'r', '', 'd', '', 's', '...', '', '', '']


Changed in version 3.1: Added the optional flags argument.


Changed in version 3.7: Added support of splitting on a pattern that could match an empty string.


➡ `re.findall(pattern, string, flags=0)`
Return all non-overlapping matches of pattern in string, as a list of strings or tuples. The string is scanned left-to-right, and matches are returned in the order found. Empty matches are included in the result.

The result depends on the number of capturing groups in the pattern. If there are no groups, return a list of strings matching the whole pattern. If there is exactly one group, return a list of strings matching that group. If multiple groups are present, return a list of tuples of strings matching the groups. Non-capturing groups do not affect the form of the result.


>>> re.findall(r'\bf[a-z]*', 'which foot or hand fell fastest')
['foot', 'fell', 'fastest']
>>> re.findall(r'(\w+)=(\d+)', 'set width=20 and height=10')
[('width', '20'), ('height', '10')]


Changed in version 3.7: Non-empty matches can now start just after a previous empty match.


➡ `re.finditer(pattern, string, flags=0)`
Return an iterator yielding match objects over all non-overlapping matches for the RE pattern in string. The string is scanned left-to-right, and matches are returned in the order found. Empty matches are included in the result.

Changed in version 3.7: Non-empty matches can now start just after a previous empty match.


➡ `re.sub(pattern, repl, string, count=0, flags=0)`
Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl. If the pattern isn’t found, string is returned unchanged. repl can be a string or a function; if it is a string, any backslash escapes in it are processed. That is, \n is converted to a single newline character, \r is converted to a carriage return, and so forth. Unknown escapes of ASCII letters are reserved for future use and treated as errors. Other unknown escapes such as \& are left alone. Backreferences, such as \6, are replaced with the substring matched by group 6 in the pattern. For example:


>>> re.sub(r'def\s+([a-zA-Z_][a-zA-Z_0-9]*)\s*\(\s*\):',
...        r'static PyObject*\npy_\1(void)\n{',
...        'def myfunc():')
'static PyObject*\npy_myfunc(void)\n{'


If repl is a function, it is called for every non-overlapping occurrence of pattern. The function takes a single match object argument, and returns the replacement string. For example:


>>> def dashrepl(matchobj):
...     if matchobj.group(0) == '-': return ' '
...     else: return '-'
>>> re.sub('-{1,2}', dashrepl, 'pro----gram-files')
'pro--gram files'
>>> re.sub(r'\sAND\s', ' & ', 'Baked Beans And Spam', flags=re.IGNORECASE)
'Baked Beans & Spam'


The pattern may be a string or a pattern object.

The optional argument count is the maximum number of pattern occurrences to be replaced; count must be a non-negative integer. If omitted or zero, all occurrences will be replaced. Empty matches for the pattern are replaced only when not adjacent to a previous empty match, so `sub('x*', '-', 'abxd')` returns '-a-b--d-'.

In string-type repl arguments, in addition to the character escapes and backreferences described above, \g<name> will use the substring matched by the group named name, as defined by the (?P<name>...) syntax. \g<number> uses the corresponding group number; \g<2> is therefore equivalent to \2, but isn’t ambiguous in a replacement such as \g<2>0. \20 would be interpreted as a reference to group 20, not a reference to group 2 followed by the literal character '0'. The backreference \g<0> substitutes in the entire substring matched by the RE.


Changed in version 3.1: Added the optional flags argument.


Changed in version 3.5: Unmatched groups are replaced with an empty string.


Changed in version 3.6: Unknown escapes in pattern consisting of '\' and an ASCII letter now are errors.


Changed in version 3.7: Unknown escapes in repl consisting of '\' and an ASCII letter now are errors.


Changed in version 3.7: Empty matches for the pattern are replaced when adjacent to a previous non-empty match.

➡ `re.subn(pattern, repl, string, count=0, flags=0)`
Perform the same operation as sub(), but return a tuple (new_string, number_of_subs_made).


Changed in version 3.1: Added the optional flags argument.


Changed in version 3.5: Unmatched groups are replaced with an empty string.

➡ `re.escape(pattern)`
Escape special characters in pattern. This is useful if you want to match an arbitrary literal string that may have regular expression metacharacters in it. For example:


```py
>>> print(re.escape('https://www.python.org'))
https://www\.python\.org

>>> legal_chars = string.ascii_lowercase + string.digits + "!#$%&'*+-.^_`|~:"
>>> print('[%s]+' % re.escape(legal_chars))
[abcdefghijklmnopqrstuvwxyz0123456789!\#\$%\&'\*\+\-\.\^_`\|\~:]+

>>> operators = ['+', '-', '*', '/', '**']
>>> print('|'.join(map(re.escape, sorted(operators, reverse=True))))
/|\-|\+|\*\*|\*
```


This function must not be used for the replacement string in sub() and subn(), only backslashes should be escaped. For example:


>>> digits_re = r'\d+'
>>> sample = '/usr/sbin/sendmail - 0 errors, 12 warnings'
>>> print(re.sub(digits_re, digits_re.replace('\\', r'\\'), sample))
/usr/sbin/sendmail - \d+ errors, \d+ warnings



Changed in version 3.3: The '`_`' character is no longer escaped.


Changed in version 3.7: Only characters that can have special meaning in a regular expression are escaped. As a result, '!', '"', '%', "'", ',', '/', ':', ';', '<', '=', '>', '@', and "`" are no longer escaped.

➡ `re.purge()`
Clear the regular expression cache.

✅ `exception re.error(msg, pattern=None, pos=None)`
Exception raised when a string passed to one of the functions here is not a valid regular expression (for example, it might contain unmatched parentheses) or when some other error occurs during compilation or matching. It is never an error if a string contains no match for a pattern. 

The error instance has the following additional attributes:

➡ `msg` The unformatted error message.
➡ `pattern` The regular expression pattern.
➡ `pos` The index in pattern where compilation failed (may be None).
➡ `lineno` The line corresponding to pos (may be None).
➡ `colno` The column corresponding to pos (may be None).


Changed in version 3.5: Added additional attributes.


### ===👉 Regular Expression Objects


Compiled regular expression objects support the following methods and attributes:

➡ `Pattern.search(string[, pos[, endpos]])`
Scan through string looking for the first location where this regular expression produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string.

The optional second parameter pos gives an index in the string where the search is to start; it defaults to 0. This is not completely equivalent to slicing the string; the '^' pattern character matches at the real beginning of the string and at positions just after a newline, but not necessarily at the index where the search is to start.

The optional parameter endpos limits how far the string will be searched; it will be as if the string is endpos characters long, so only the characters from pos to endpos - 1 will be searched for a match. If endpos is less than pos, no match will be found; otherwise, if rx is a compiled regular expression object, rx.search(string, 0, 50) is equivalent to rx.search(string[:50], 0).


>>> pattern = re.compile("d")
>>> pattern.search("dog")     # Match at index 0
<re.Match object; span=(0, 1), match='d'>
>>> pattern.search("dog", 1)  # No match; search doesn't include the "d"


➡ `Pattern.match(string[, pos[, endpos]])`
If zero or more characters at the beginning of string match this regular expression, return a corresponding match object. Return None if the string does not match the pattern; note that this is different from a zero-length match.

The optional pos and endpos parameters have the same meaning as for the search() method.


>>> pattern = re.compile("o")
>>> pattern.match("dog")      # No match as "o" is not at the start of "dog".
>>> pattern.match("dog", 1)   # Match as "o" is the 2nd character of "dog".
<re.Match object; span=(1, 2), match='o'>


If you want to locate a match anywhere in string, use search() instead (see also search() vs. match()).

➡ `Pattern.fullmatch(string[, pos[, endpos]])`
If the whole string matches this regular expression, return a corresponding match object. Return None if the string does not match the pattern; note that this is different from a zero-length match.

The optional pos and endpos parameters have the same meaning as for the search() method.


>>> pattern = re.compile("o[gh]")
>>> pattern.fullmatch("dog")      # No match as "o" is not at the start of "dog".
>>> pattern.fullmatch("ogre")     # No match as not the full string matches.
>>> pattern.fullmatch("doggie", 1, 3)   # Matches within given limits.
<re.Match object; span=(1, 3), match='og'>

New in version 3.4.


➡ `Pattern.split(string, maxsplit=0)`
Identical to the split() function, using the compiled pattern.

➡ `Pattern.findall(string[, pos[, endpos]])`
Similar to the findall() function, using the compiled pattern, but also accepts optional pos and endpos parameters that limit the search region like for search().

➡ `Pattern.finditer(string[, pos[, endpos]])`
Similar to the finditer() function, using the compiled pattern, but also accepts optional pos and endpos parameters that limit the search region like for search().

➡ `Pattern.sub(repl, string, count=0)`
Identical to the sub() function, using the compiled pattern.

➡ `Pattern.subn(repl, string, count=0)`
Identical to the subn() function, using the compiled pattern.

➡ `Pattern.flags`
The regex matching flags. This is a combination of the flags given to compile(), any (?...) inline flags in the pattern, and implicit flags such as UNICODE if the pattern is a Unicode string.

➡ `Pattern.groups`
The number of capturing groups in the pattern.

➡ `Pattern.groupindex`
A dictionary mapping any symbolic group names defined by (?P<id>) to group numbers. The dictionary is empty if no symbolic groups were used in the pattern.

➡ `Pattern.pattern`
The pattern string from which the pattern object was compiled.


### ===👉 Match Objects
Match objects always have a boolean value of True. Since match() and search() return None when there is no match, you can test whether there was a match with a simple if statement:

```py
match = re.search(pattern, string)
if match:
    process(match)
```

Match objects support the following methods and attributes:

➡ `Match.expand(template)` Return the string obtained by doing backslash substitution on the template string template, as done by the sub() method. Escapes such as \n are converted to the appropriate characters, and numeric backreferences (\1, \2) and named backreferences (\g<1>, \g<name>) are replaced by the contents of the corresponding group.

Changed in version 3.5: Unmatched groups are replaced with an empty string.

➡ `Match.group([group1, ...])`
Returns one or more subgroups of the match. If there is a single argument, the result is a single string; if there are multiple arguments, the result is a tuple with one item per argument. Without arguments, group1 defaults to zero (the whole match is returned). If a groupN argument is zero, the corresponding return value is the entire matching string; if it is in the inclusive range [1..99], it is the string matching the corresponding parenthesized group. If a group number is negative or larger than the number of groups defined in the pattern, an IndexError exception is raised. If a group is contained in a part of the pattern that did not match, the corresponding result is None. If a group is contained in a part of the pattern that matched multiple times, the last match is returned.

>>> m = re.match(r"(\w+) (\w+)", "Isaac Newton, physicist")
>>> m.group(0)       # The entire match
'Isaac Newton'
>>> m.group(1)       # The first parenthesized subgroup.
'Isaac'
>>> m.group(2)       # The second parenthesized subgroup.
'Newton'
>>> m.group(1, 2)    # Multiple arguments give us a tuple.
('Isaac', 'Newton')

If the regular expression uses the (?P<name>...) syntax, the groupN arguments may also be strings identifying groups by their group name. If a string argument is not used as a group name in the pattern, an IndexError exception is raised.

A moderately complicated example:

>>>
>>> m = re.match(r"(?P<first_name>\w+) (?P<last_name>\w+)", "Malcolm Reynolds")
>>> m.group('first_name')
'Malcolm'
>>> m.group('last_name')
'Reynolds'

Named groups can also be referred to by their index:

>>>
>>> m.group(1)
'Malcolm'
>>> m.group(2)
'Reynolds'

If a group matches multiple times, only the last match is accessible:

>>>
>>> m = re.match(r"(..)+", "a1b2c3")  # Matches 3 times.
>>> m.group(1)                        # Returns only the last match.
'c3'

➡ `Match.__getitem__(g)` This is identical to m.group(g). This allows easier access to an individual group from a match:

>>>
>>> m = re.match(r"(\w+) (\w+)", "Isaac Newton, physicist")
>>> m[0]       # The entire match
'Isaac Newton'
>>> m[1]       # The first parenthesized subgroup.
'Isaac'
>>> m[2]       # The second parenthesized subgroup.
'Newton'
New in version 3.6.


➡ `Match.groups(default=None)` Return a tuple containing all the subgroups of the match, from 1 up to however many groups are in the pattern. The default argument is used for groups that did not participate in the match; it defaults to None. For example:

>>>
>>> m = re.match(r"(\d+)\.(\d+)", "24.1632")
>>> m.groups()
('24', '1632')
If we make the decimal place and everything after it optional, not all groups might participate in the match. These groups will default to None unless the default argument is given:

>>>
>>> m = re.match(r"(\d+)\.?(\d+)?", "24")
>>> m.groups()      # Second group defaults to None.
('24', None)
>>> m.groups('0')   # Now, the second group defaults to '0'.
('24', '0')

➡ `Match.groupdict(default=None)` Return a dictionary containing all the named subgroups of the match, keyed by the subgroup name. The default argument is used for groups that did not participate in the match; it defaults to None. For example: 

    >>> m = re.match(r"(?P<first_name>\w+) (?P<last_name>\w+)", "Malcolm Reynolds")
    >>> m.groupdict()
    {'first_name': 'Malcolm', 'last_name': 'Reynolds'}

➡ `Match.start([group])` - `Match.end([group])` Return the indices of the start and end of the substring matched by group; group defaults to zero (meaning the whole matched substring). Return -1 if group exists but did not contribute to the match. For a match object m, and a group g that did contribute to the match, the substring matched by group g (equivalent to m.group(g)) is m.string[m.start(g):m.end(g)]

Note that m.start(group) will equal m.end(group) if group matched a null string. For example, after m = re.search('b(c?)', 'cba'), m.start(0) is 1, m.end(0) is 2, m.start(1) and m.end(1) are both 2, and m.start(2) raises an IndexError exception.

An example that will remove remove_this from email addresses:

    >>> email = "tony@tiremove_thisger.net"
    >>> m = re.search("remove_this", email)
    >>> email[:m.start()] + email[m.end():]
    'tony@tiger.net'

➡ `Match.span([group])` For a match m, return the 2-tuple (m.start(group), m.end(group)). Note that if group did not contribute to the match, this is (-1, -1). group defaults to zero, the entire match.

➡ `Match.pos` The value of pos which was passed to the search() or match() method of a regex object. This is the index into the string at which the RE engine started looking for a match.

➡ `Match.endpos` The value of endpos which was passed to the search() or match() method of a regex object. This is the index into the string beyond which the RE engine will not go.

➡ `Match.lastindex` The integer index of the last matched capturing group, or None if no group was matched at all. For example, the expressions (a)b, ((a)(b)), and ((ab)) will have lastindex == 1 if applied to the string 'ab', while the expression (a)(b) will have lastindex == 2, if applied to the same string.

➡ `Match.lastgroup` The name of the last matched capturing group, or None if the group didn’t have a name, or if no group was matched at all.

➡ `Match.re` The regular expression object whose match() or search() method produced this match instance.

➡ `Match.string` The string passed to match() or search().


Changed in version 3.7: Added support of copy.copy() and copy.deepcopy(). Match objects are considered atomic.


### ===👉 Regular Expression Examples


#### Checking for a Pair

In this example, we’ll use the following helper function to display match objects a little more gracefully:

```py
def displaymatch(match):
    if match is None:
        return None
    return '<Match: %r, groups=%r>' % (match.group(), match.groups())
```


Suppose you are writing a poker program where a player’s hand is represented as a 5-character string with each character representing a card, “a” for ace, “k” for king, “q” for queen, “j” for jack, “t” for 10, and “2” through “9” representing the card with that value.

To see if a given string is a valid hand, one could do the following:


>>> valid = re.compile(r"^[a2-9tjqk]{5}$")
>>> displaymatch(valid.match("akt5q"))  # Valid.
"<Match: 'akt5q', groups=()>"
>>> displaymatch(valid.match("akt5e"))  # Invalid.
>>> displaymatch(valid.match("akt"))    # Invalid.
>>> displaymatch(valid.match("727ak"))  # Valid.
"<Match: '727ak', groups=()>"


That last hand, "727ak", contained a pair, or two of the same valued cards. To match this with a regular expression, one could use backreferences as such:


>>> pair = re.compile(r".*(.).*\1")
>>> displaymatch(pair.match("717ak"))     # Pair of 7s.
"<Match: '717', groups=('7',)>"
>>> displaymatch(pair.match("718ak"))     # No pairs.
>>> displaymatch(pair.match("354aa"))     # Pair of aces.
"<Match: '354aa', groups=('a',)>"


To find out what card the pair consists of, one could use the group() method of the match object in the following manner:


```sh
>>> pair = re.compile(r".*(.).*\1")
>>> pair.match("717ak").group(1)
'7'
# Error because re.match() returns None, which doesn't have a group() method:
>>> pair.match("718ak").group(1)
Traceback (most recent call last):
  File "<pyshell#23>", line 1, in <module>
    re.match(r".*(.).*\1", "718ak").group(1)
AttributeError: 'NoneType' object has no attribute 'group'

>>> pair.match("354aa").group(1)
'a'
```



#### Simulating scanf()

Python does not currently have an equivalent to scanf(). Regular expressions are generally more powerful, though also more verbose, than scanf() format strings. The table below offers some more-or-less equivalent mappings between scanf() format tokens and regular expressions.

| scanf() Token  |            Regular Expression           |
|----------------|-----------------------------------------|
| %c             | .                                       |
| %5c            | .{5}                                    |
| %d             | [-+]?\d+                                |
| %e, %E, %f, %g | [-+]?(\d+(\.\d*)?|\.\d+)([eE][-+]?\d+)? |
| %i             | [-+]?(0[xX][\dA-Fa-f]+|0[0-7]*|\d+)     |
| %o             | [-+]?[0-7]+                             |
| %s             | \S+                                     |
| %u             | \d+                                     |
| %x, %X         | [-+]?(0[xX])?[\dA-Fa-f]+                |

To extract the filename and numbers from a string like


    /usr/sbin/sendmail - 0 errors, 4 warnings


you would use a scanf() format like


    %s - %d errors, %d warnings


The equivalent regular expression would be


    (\S+) - (\d+) errors, (\d+) warnings


#### search() vs. match()

Python offers two different primitive operations based on regular expressions: re.match() checks for a match only at the beginning of the string, while re.search() checks for a match anywhere in the string (this is what Perl does by default).

For example:


>>> re.match("c", "abcdef")    # No match
>>> re.search("c", "abcdef")   # Match
<re.Match object; span=(2, 3), match='c'>


Regular expressions beginning with '^' can be used with search() to restrict the match at the beginning of the string:


>>> re.match("c", "abcdef")    # No match
>>> re.search("^c", "abcdef")  # No match
>>> re.search("^a", "abcdef")  # Match
<re.Match object; span=(0, 1), match='a'>


Note however that in MULTILINE mode match() only matches at the beginning of the string, whereas using search() with a regular expression beginning with '^' will match at the beginning of each line.


>>> re.match('X', 'A\nB\nX', re.MULTILINE)  # No match
>>> re.search('^X', 'A\nB\nX', re.MULTILINE)  # Match
<re.Match object; span=(4, 5), match='X'>



#### Making a Phonebook

split() splits a string into a list delimited by the passed pattern. The method is invaluable for converting textual data into data structures that can be easily read and modified by Python as demonstrated in the following example that creates a phonebook.

First, here is the input. Normally it may come from a file, here we are using triple-quoted string syntax


>>> text = """Ross McFluff: 834.345.1254 155 Elm Street
...
... Ronald Heathmore: 892.345.3428 436 Finley Avenue
... Frank Burger: 925.541.7625 662 South Dogwood Way
...
...
... Heather Albrecht: 548.326.4584 919 Park Place"""


The entries are separated by one or more newlines. Now we convert the string into a list with each nonempty line having its own entry:


>>> entries = re.split("\n+", text)
>>> entries
['Ross McFluff: 834.345.1254 155 Elm Street',
'Ronald Heathmore: 892.345.3428 436 Finley Avenue',
'Frank Burger: 925.541.7625 662 South Dogwood Way',
'Heather Albrecht: 548.326.4584 919 Park Place']


Finally, split each entry into a list with first name, last name, telephone number, and address. We use the maxsplit parameter of split() because the address has spaces, our splitting pattern, in it:


>>> [re.split(":? ", entry, 3) for entry in entries]
[['Ross', 'McFluff', '834.345.1254', '155 Elm Street'],
['Ronald', 'Heathmore', '892.345.3428', '436 Finley Avenue'],
['Frank', 'Burger', '925.541.7625', '662 South Dogwood Way'],
['Heather', 'Albrecht', '548.326.4584', '919 Park Place']]


The :? pattern matches the colon after the last name, so that it does not occur in the result list. With a maxsplit of 4, we could separate the house number from the street name:


>>> [re.split(":? ", entry, 4) for entry in entries]
[['Ross', 'McFluff', '834.345.1254', '155', 'Elm Street'],
['Ronald', 'Heathmore', '892.345.3428', '436', 'Finley Avenue'],
['Frank', 'Burger', '925.541.7625', '662', 'South Dogwood Way'],
['Heather', 'Albrecht', '548.326.4584', '919', 'Park Place']]



#### Text Munging

sub() replaces every occurrence of a pattern with a string or the result of a function. This example demonstrates using sub() with a function to “munge” text, or randomize the order of all the characters in each word of a sentence except for the first and last characters:


>>> def repl(m):
...     inner_word = list(m.group(2))
...     random.shuffle(inner_word)
...     return m.group(1) + "".join(inner_word) + m.group(3)
>>> text = "Professor Abdolmalek, please report your absences promptly."
>>> re.sub(r"(\w)(\w+)(\w)", repl, text)
'Poefsrosr Aealmlobdk, pslaee reorpt your abnseces plmrptoy.'
>>> re.sub(r"(\w)(\w+)(\w)", repl, text)
'Pofsroser Aodlambelk, plasee reoprt yuor asnebces potlmrpy.'



#### Finding all Adverbs

findall() matches all occurrences of a pattern, not just the first one as search() does. For example, if a writer wanted to find all of the adverbs in some text, they might use findall() in the following manner:


>>> text = "He was carefully disguised but captured quickly by police."
>>> re.findall(r"\w+ly\b", text)
['carefully', 'quickly']



#### Finding all Adverbs and their Positions

If one wants more information about all matches of a pattern than the matched text, finditer() is useful as it provides match objects instead of strings. Continuing with the previous example, if a writer wanted to find all of the adverbs and their positions in some text, they would use finditer() in the following manner:


>>> text = "He was carefully disguised but captured quickly by police."
>>> for m in re.finditer(r"\w+ly\b", text):
...     print('%02d-%02d: %s' % (m.start(), m.end(), m.group(0)))
07-16: carefully
40-47: quickly



#### Raw String Notation

Raw string notation (r"text") keeps regular expressions sane. Without it, every backslash ('\') in a regular expression would have to be prefixed with another one to escape it. For example, the two following lines of code are functionally identical:


>>> re.match(r"\W(.)\1\W", " ff ")
<re.Match object; span=(0, 4), match=' ff '>
>>> re.match("\\W(.)\\1\\W", " ff ")
<re.Match object; span=(0, 4), match=' ff '>


When one wants to match a literal backslash, it must be escaped in the regular expression. With raw string notation, this means r"\\". Without raw string notation, one must use "\\\\", making the following lines of code functionally identical:


>>> re.match(r"\\", r"\\")
<re.Match object; span=(0, 1), match='\\'>
>>> re.match("\\\\", r"\\")
<re.Match object; span=(0, 1), match='\\'>



#### Writing a Tokenizer

A tokenizer or scanner analyzes a string to categorize groups of characters. This is a useful first step in writing a compiler or interpreter.

The text categories are specified with regular expressions. The technique is to combine those into a single master regular expression and to loop over successive matches:


```py
from typing import NamedTuple
import re

class Token(NamedTuple):
    type: str
    value: str
    line: int
    column: int

def tokenize(code):
    keywords = {'IF', 'THEN', 'ENDIF', 'FOR', 'NEXT', 'GOSUB', 'RETURN'}
    token_specification = [
        ('NUMBER',   r'\d+(\.\d*)?'),  # Integer or decimal number
        ('ASSIGN',   r':='),           # Assignment operator
        ('END',      r';'),            # Statement terminator
        ('ID',       r'[A-Za-z]+'),    # Identifiers
        ('OP',       r'[+\-*/]'),      # Arithmetic operators
        ('NEWLINE',  r'\n'),           # Line endings
        ('SKIP',     r'[ \t]+'),       # Skip over spaces and tabs
        ('MISMATCH', r'.'),            # Any other character
    ]
    tok_regex = '|'.join('(?P<%s>%s)' % pair for pair in token_specification)
    line_num = 1
    line_start = 0
    for mo in re.finditer(tok_regex, code):
        kind = mo.lastgroup
        value = mo.group()
        column = mo.start() - line_start
        if kind == 'NUMBER':
            value = float(value) if '.' in value else int(value)
        elif kind == 'ID' and value in keywords:
            kind = value
        elif kind == 'NEWLINE':
            line_start = mo.end()
            line_num += 1
            continue
        elif kind == 'SKIP':
            continue
        elif kind == 'MISMATCH':
            raise RuntimeError(f'{value!r} unexpected on line {line_num}')
        yield Token(kind, value, line_num, column)

statements = '''
    IF quantity THEN
        total := total + price * quantity;
        tax := price * 0.05;
    ENDIF;
'''

for token in tokenize(statements):
    print(token)

```

The tokenizer produces the following output:

>>>
Token(type='IF', value='IF', line=2, column=4)
Token(type='ID', value='quantity', line=2, column=7)
Token(type='THEN', value='THEN', line=2, column=16)
Token(type='ID', value='total', line=3, column=8)
Token(type='ASSIGN', value=':=', line=3, column=14)
Token(type='ID', value='total', line=3, column=17)
Token(type='OP', value='+', line=3, column=23)
Token(type='ID', value='price', line=3, column=25)
Token(type='OP', value='*', line=3, column=31)
Token(type='ID', value='quantity', line=3, column=33)
Token(type='END', value=';', line=3, column=41)
Token(type='ID', value='tax', line=4, column=8)
Token(type='ASSIGN', value=':=', line=4, column=12)
Token(type='ID', value='price', line=4, column=15)
Token(type='OP', value='*', line=4, column=21)
Token(type='NUMBER', value=0.05, line=4, column=23)
Token(type='END', value=';', line=4, column=27)
Token(type='ENDIF', value='ENDIF', line=5, column=4)
Token(type='END', value=';', line=5, column=9)

[Frie09] Friedl, Jeffrey. Mastering Regular Expressions. 3rd ed., O’Reilly Media, 2009. The third edition of the book no longer covers Python at all, but the first edition covered writing good regular expression patterns in great detail. 


## ==⚡ • difflib — Helpers for computing deltas

## ==⚡ • textwrap — Text wrapping and filling

## ==⚡ • unicodedata — Unicode Database

## ==⚡ • stringprep — Internet String Preparation

## ==⚡ • readline — GNU readline interface

## ==⚡ • rlcompleter — Completion function for GNU readline



# =🚩 Binary Data Services

The modules described in this chapter provide some basic services operations for manipulation of binary data. Other operations on binary data, specifically in relation to file formats and network protocols, are described in the relevant sections.

Some libraries described under Text Processing Services also work with either ASCII-compatible binary formats (for example, re) or all binary data (for example, difflib).

In addition, see the documentation for Python’s built-in binary data types in Binary Sequence Types — bytes, bytearray, memoryview.

• struct — Interpret bytes as packed binary data
◦ Functions and Exceptions
◦ Format Strings
◾ Byte Order, Size, and Alignment
◾ Format Characters
◾ Examples

◦ Classes

• codecs — Codec registry and base classes
◦ Codec Base Classes
◾ Error Handlers
◾ Stateless Encoding and Decoding
◾ Incremental Encoding and Decoding
◾ IncrementalEncoder Objects
◾ IncrementalDecoder Objects

◾ Stream Encoding and Decoding
◾ StreamWriter Objects
◾ StreamReader Objects
◾ StreamReaderWriter Objects
◾ StreamRecoder Objects


◦ Encodings and Unicode
◦ Standard Encodings
◦ Python Specific Encodings
◾ Text Encodings
◾ Binary Transforms
◾ Text Transforms

◦ encodings.idna — Internationalized Domain Names in Applications
◦ encodings.mbcs — Windows ANSI codepage
◦ encodings.utf_8_sig — UTF-8 codec with BOM signature

## ==⚡ • struct — Interpret bytes as packed binary data


Source code: Lib/struct.py


This module performs conversions between Python values and C structs represented as Python bytes objects. This can be used in handling binary data stored in files or from network connections, among other sources. It uses Format Strings as compact descriptions of the layout of the C structs and the intended conversion to/from Python values.

Note:
 By default, the result of packing a given C struct includes pad bytes in order to maintain proper alignment for the C types involved; similarly, alignment is taken into account when unpacking. This behavior is chosen so that the bytes of a packed struct correspond exactly to the layout in memory of the corresponding C struct. To handle platform-independent data formats or omit implicit pad bytes, use standard size and alignment instead of native size and alignment: see Byte Order, Size, and Alignment for details.
 

Several `struct` functions (and methods of Struct) take a buffer argument. This refers to objects that implement the Buffer Protocol and provide either a readable or read-writable buffer. The most common types used for that purpose are bytes and bytearray, but many other types that can be viewed as an array of bytes implement the buffer protocol, so that they can be read/filled without additional copying from a bytes object.


### ===🗝 Functions and Exceptions

The module defines the following exception and functions:

✅ `exception struct.error`
Exception raised on various occasions; argument is a string describing what is wrong.

➡ `struct.pack(format, v1, v2, ...)`
Return a bytes object containing the values v1, v2, … packed according to the format string format. The arguments must match the values required by the format exactly.

➡ `struct.pack_into(format, buffer, offset, v1, v2, ...)`
Pack the values v1, v2, … according to the format string format and write the packed bytes into the writable buffer buffer starting at position offset. Note that offset is a required argument.

➡ `struct.unpack(format, buffer)`
Unpack from the buffer buffer (presumably packed by pack(format, ...)) according to the format string format. The result is a tuple even if it contains exactly one item. The buffer’s size in bytes must match the size required by the format, as reflected by calcsize().

➡ `struct.unpack_from(format, /, buffer, offset=0)`
Unpack from buffer starting at position offset, according to the format string format. The result is a tuple even if it contains exactly one item. The buffer’s size in bytes, starting at position offset, must be at least the size required by the format, as reflected by calcsize().

➡ `struct.iter_unpack(format, buffer)`
Iteratively unpack from the buffer buffer according to the format string format. This function returns an iterator which will read equally-sized chunks from the buffer until all its contents have been consumed. The buffer’s size in bytes must be a multiple of the size required by the format, as reflected by calcsize().

Each iteration yields a tuple as specified by the format string.

New in version 3.4.


➡ `struct.calcsize(format)`
Return the size of the struct (and hence of the bytes object produced by pack(format, ...)) corresponding to the format string format.


### ===🗝 Format Strings

Format strings are the mechanism used to specify the expected layout when packing and unpacking data. They are built up from Format Characters, which specify the type of data being packed/unpacked. In addition, there are special characters for controlling the Byte Order, Size, and Alignment.


#### Byte Order, Size, and Alignment

By default, C types are represented in the machine’s native format and byte order, and properly aligned by skipping pad bytes if necessary (according to the rules used by the C compiler).

Alternatively, the first character of the format string can be used to indicate the byte order, size and alignment of the packed data, according to the following table:


| Character |       Byte order       |   Size   | Alignment |
|-----------|------------------------|----------|-----------|
| @         | native                 | native   | native    |
| =         | native                 | standard | none      |
| <         | little-endian          | standard | none      |
| >         | big-endian             | standard | none      |
| !         | network (= big-endian) | standard | none      |

If the first character is not one of these, '@' is assumed.

Native byte order is big-endian or little-endian, depending on the host system. For example, Intel x86 and AMD64 (x86-64) are little-endian; Motorola 68000 and PowerPC G5 are big-endian; ARM and Intel Itanium feature switchable endianness (bi-endian). Use sys.byteorder to check the endianness of your system.

Native size and alignment are determined using the C compiler’s sizeof expression. This is always combined with native byte order.

Standard size depends only on the format character; see the table in the Format Characters section.

Note the difference between '@' and '=': both use native byte order, but the size and alignment of the latter is standardized.

The form '!' represents the network byte order which is always big-endian as defined in IETF RFC 1700.

There is no way to indicate non-native byte order (force byte-swapping); use the appropriate choice of '<' or '>'.

Notes:

1. Padding is only automatically added between successive structure members. No padding is added at the beginning or the end of the encoded struct.

2. No padding is added when using non-native size and alignment, e.g. with ‘<’, ‘>’, ‘=’, and ‘!’.

3. To align the end of a structure to the alignment requirement of a particular type, end the format with the code for that type with a repeat count of zero. See Examples.


#### Format Characters

Format characters have the following meaning; the conversion between C and Python values should be obvious given their types. The ‘size’ column refers to the size of the packed value in bytes when using standard size; that is, when the format string starts with one of '<', '>', '!' or '='. When using native size, the size of the packed value is platform-dependent.

| Format |       C Type       |    Python type    | size |  Notes   |
|--------|--------------------|-------------------|------|----------|
| x      | pad byte           | no value          |      |          |
| c      | char               | bytes of length 1 |    1 |          |
| b      | signed char        | integer           |    1 | (1),(2) |
| B      | unsigned char      | integer           |    1 | (2)      |
| ?      | _Bool              | bool              |    1 | (1)      |
| h      | short              | integer           |    2 | (2)      |
| H      | unsigned short     | integer           |    2 | (2)      |
| i      | int                | integer           |    4 | (2)      |
| I      | unsigned int       | integer           |    4 | (2)      |
| l      | long               | integer           |    4 | (2)      |
| L      | unsigned long      | integer           |    4 | (2)      |
| q      | long long          | integer           |    8 | (2)      |
| Q      | unsigned long long | integer           |    8 | (2)      |
| n      | ssize_t            | integer           |      | (3)      |
| N      | size_t             | integer           |      | (3)      |
| e      | (6)                | float             |    2 | (4)      |
| f      | float              | float             |    4 | (4)      |
| d      | double             | float             |    8 | (4)      |
| s      | char[]             | bytes             |      |          |
| p      | char[]             | bytes             |      |          |
| P      | void *             | integer           |      | (5)      |

Changed in version 3.3: Added support for the 'n' and 'N' formats.

Changed in version 3.6: Added support for the 'e' format.


Notes:

1. The '?' conversion code corresponds to the `_Bool` type defined by C99. If this type is not available, it is simulated using a char. In standard mode, it is always represented by one byte.


2. When attempting to pack a non-integer using any of the integer conversion codes, if the non-integer has a __index__() method then that method is called to convert the argument to an integer before packing.

Changed in version 3.2: Added use of the __index__() method for non-integers.


3. The 'n' and 'N' conversion codes are only available for the native size (selected as the default or with the '@' byte order character). For the standard size, you can use whichever of the other integer formats fits your application.


4. For the 'f', 'd' and 'e' conversion codes, the packed representation uses the IEEE 754 binary32, binary64 or binary16 format (for 'f', 'd' or 'e' respectively), regardless of the floating-point format used by the platform.


5. The 'P' format character is only available for the native byte ordering (selected as the default or with the '@' byte order character). The byte order character '=' chooses to use little- or big-endian ordering based on the host system. The struct module does not interpret this as native ordering, so the 'P' format is not available.


6.The IEEE 754 binary16 “half precision” type was introduced in the 2008 revision of the IEEE 754 standard. It has a sign bit, a 5-bit exponent and 11-bit precision (with 10 bits explicitly stored), and can represent numbers between approximately 6.1e-05 and 6.5e+04 at full precision. This type is not widely supported by C compilers: on a typical machine, an unsigned short can be used for storage, but not for math operations. See the Wikipedia page on the half-precision floating-point format for more information.


A format character may be preceded by an integral repeat count. For example, the format string '4h' means exactly the same as 'hhhh'.

Whitespace characters between formats are ignored; a count and its format must not contain whitespace though.

For the 's' format character, the count is interpreted as the length of the bytes, not a repeat count like for the other format characters; for example, '10s' means a single 10-byte string, while '10c' means 10 characters. If a count is not given, it defaults to 1. For packing, the string is truncated or padded with null bytes as appropriate to make it fit. For unpacking, the resulting bytes object always has exactly the specified number of bytes. As a special case, '0s' means a single, empty string (while '0c' means 0 characters).

When packing a value x using one of the integer formats ('b', 'B', 'h', 'H', 'i', 'I', 'l', 'L', 'q', 'Q'), if x is outside the valid range for that format then struct.error is raised.

Changed in version 3.1: Previously, some of the integer formats wrapped out-of-range values and raised DeprecationWarning instead of struct.error.


The 'p' format character encodes a “Pascal string”, meaning a short variable-length string stored in a fixed number of bytes, given by the count. The first byte stored is the length of the string, or 255, whichever is smaller. The bytes of the string follow. If the string passed in to pack() is too long (longer than the count minus 1), only the leading count-1 bytes of the string are stored. If the string is shorter than count-1, it is padded with null bytes so that exactly count bytes in all are used. Note that for unpack(), the 'p' format character consumes count bytes, but that the string returned can never contain more than 255 bytes.

For the '?' format character, the return value is either True or False. When packing, the truth value of the argument object is used. Either 0 or 1 in the native or standard bool representation will be packed, and any non-zero value will be True when unpacking.


Examples

Note:
 All examples assume a native byte order, size, and alignment with a big-endian machine.
 

A basic example of packing/unpacking three integers:


```sh
>>> from struct import *
>>> pack('hhl', 1, 2, 3)
b'\x00\x01\x00\x02\x00\x00\x00\x03'
>>> unpack('hhl', b'\x00\x01\x00\x02\x00\x00\x00\x03')
(1, 2, 3)
>>> calcsize('hhl')
8
```


Unpacked fields can be named by assigning them to variables or by wrapping the result in a named tuple:


```sh
>>> record = b'raymond   \x32\x12\x08\x01\x08'
>>> name, serialnum, school, gradelevel = unpack('<10sHHb', record)

>>> from collections import namedtuple
>>> Student = namedtuple('Student', 'name serialnum school gradelevel')
>>> Student._make(unpack('<10sHHb', record))
Student(name=b'raymond   ', serialnum=4658, school=264, gradelevel=8)
```


The ordering of format characters may have an impact on size since the padding needed to satisfy alignment requirements is different:


```py
>>> pack('ci', b'*', 0x12131415)
b'*\x00\x00\x00\x12\x13\x14\x15'
>>> pack('ic', 0x12131415, b'*')
b'\x12\x13\x14\x15*'
>>> calcsize('ci')
8
>>> calcsize('ic')
5
```

The following format 'llh0l' specifies two pad bytes at the end, assuming longs are aligned on 4-byte boundaries:


>>> pack('llh0l', 1, 2, 3)
b'\x00\x00\x00\x01\x00\x00\x00\x02\x00\x03\x00\x00'


This only works when native size and alignment are in effect; standard size and alignment does not enforce any alignment.

See also:
 Module arrayPacked binary storage of homogeneous data.Module xdrlibPacking and unpacking of XDR data.

### ===🗝 Classes struct.Struct

The struct module also defines the following type:

✅ `class struct.Struct(format)`
Return a new Struct object which writes and reads binary data according to the format string format. Creating a Struct object once and calling its methods is more efficient than calling the struct functions with the same format since the format string only needs to be compiled once.

Note:
 The compiled versions of the most recent format strings passed to Struct and the module-level functions are cached, so programs that use only a few format strings needn’t worry about reusing a single Struct instance.
 

Compiled Struct objects support the following methods and attributes:

➡ `pack(v1, v2, ...)`

Identical to the pack() function, using the compiled format. (len(result) will equal ➡ `Identical to the pack() function, using the compiled format. (len(result) will equal size.)`

➡ `pack_into(buffer, offset, v1, v2, ...)`
Identical to the pack_into() function, using the compiled format.

➡ `unpack(buffer)`
Identical to the unpack() function, using the compiled format. The buffer’s size in bytes must equal size.

➡ `unpack_from(buffer, offset=0)`
Identical to the unpack_from() function, using the compiled format. The buffer’s size in bytes, starting at position offset, must be at least size.

➡ `iter_unpack(buffer)`
Identical to the iter_unpack() function, using the compiled format. The buffer’s size in bytes must be a multiple of size.

New in version 3.4.


➡ `format`
The format string used to construct this Struct object.

Changed in version 3.7: The format string type is now str instead of bytes.


➡ `size`
The calculated size of the struct (and hence of the bytes object produced by the pack() method) corresponding to format.



## ==⚡ • codecs — Codec registry and base classes

Source code: Lib/codecs.py


This module defines base classes for standard Python `codecs` (encoders and decoders) and provides access to the internal Python codec registry, which manages the codec and error handling lookup process. Most standard codecs are text encodings, which encode text to bytes, but there are also codecs provided that encode text to text, and bytes to bytes. Custom codecs may encode and decode between arbitrary types, but some module features are restricted to use specifically with text encodings, or with codecs that encode to bytes.

The module defines the following functions for encoding and decoding with any codec:

➡ `codecs.encode(obj, encoding='utf-8', errors='strict')`
Encodes obj using the codec registered for encoding.

Errors may be given to set the desired error handling scheme. The default error handler is 'strict' meaning that encoding errors raise ValueError (or a more codec specific subclass, such as UnicodeEncodeError). Refer to Codec Base Classes for more information on codec error handling.

➡ `codecs.decode(obj, encoding='utf-8', errors='strict')`
Decodes obj using the codec registered for encoding.

Errors may be given to set the desired error handling scheme. The default error handler is 'strict' meaning that decoding errors raise ValueError (or a more codec specific subclass, such as UnicodeDecodeError). Refer to Codec Base Classes for more information on codec error handling.

The full details for each codec can also be looked up directly:

➡ `codecs.lookup(encoding)`
Looks up the codec info in the Python codec registry and returns a CodecInfo object as defined below.

Encodings are first looked up in the registry’s cache. If not found, the list of registered search functions is scanned. If no CodecInfo object is found, a LookupError is raised. Otherwise, the CodecInfo object is stored in the cache and returned to the caller.

✅ `class codecs.CodecInfo(encode, decode, streamreader=None, streamwriter=None, incrementalencoder=None, incrementaldecoder=None, name=None)`
Codec details when looking up the codec registry. The constructor arguments are stored in attributes of the same name:

➡ `name`
The name of the encoding.
➡ `encodedecode`
The stateless encoding and decoding functions. These must be functions or methods which have the same interface as the encode() and decode() methods of Codec instances (see Codec Interface). The functions or methods are expected to work in a stateless mode.
➡ `incrementalencoder`
➡ `incrementaldecoder`
Incremental encoder and decoder classes or factory functions. These have to provide the interface defined by the base classes IncrementalEncoder and IncrementalDecoder, respectively. Incremental codecs can maintain state.
➡ `streamwriter`
➡ `streamreader`
Stream writer and reader classes or factory functions. These have to provide the interface defined by the base classes StreamWriter and StreamReader, respectively. Stream codecs can maintain state.

To simplify access to the various codec components, the module provides these additional functions which use lookup() for the codec lookup:

➡ `codecs.getencoder(encoding)`
Look up the codec for the given encoding and return its encoder function.

Raises a LookupError in case the encoding cannot be found.

➡ `codecs.getdecoder(encoding)`
Look up the codec for the given encoding and return its decoder function.

Raises a LookupError in case the encoding cannot be found.

➡ `codecs.getincrementalencoder(encoding)`
Look up the codec for the given encoding and return its incremental encoder class or factory function.

Raises a LookupError in case the encoding cannot be found or the codec doesn’t support an incremental encoder.

➡ `codecs.getincrementaldecoder(encoding)`
Look up the codec for the given encoding and return its incremental decoder class or factory function.

Raises a LookupError in case the encoding cannot be found or the codec doesn’t support an incremental decoder.

➡ `codecs.getreader(encoding)`
Look up the codec for the given encoding and return its StreamReader class or factory function.

Raises a LookupError in case the encoding cannot be found.

➡ `codecs.getwriter(encoding)`
Look up the codec for the given encoding and return its StreamWriter class or factory function.

Raises a LookupError in case the encoding cannot be found.

Custom codecs are made available by registering a suitable codec search function:

➡ `codecs.register(search_function)`
Register a codec search function. Search functions are expected to take one argument, being the encoding name in all lower case letters with hyphens and spaces converted to underscores, and return a CodecInfo object. In case a search function cannot find a given encoding, it should return None.

Changed in version 3.9: Hyphens and spaces are converted to underscore.


➡ `codecs.unregister(search_function)`
Unregister a codec search function and clear the registry’s cache. If the search function is not registered, do nothing.

New in version 3.10.


While the builtin open() and the associated io module are the recommended approach for working with encoded text files, this module provides additional utility functions and classes that allow the use of a wider range of codecs when working with binary files:

➡ `codecs.open(filename, mode='r', encoding=None, errors='strict', buffering=-1)`
Open an encoded file using the given mode and return an instance of StreamReaderWriter, providing transparent encoding/decoding. The default file `mode` is 'r', meaning to open the file in read mode.

Note:
 Underlying encoded files are always opened in binary mode. No automatic conversion of '\n' is done on reading and writing. The mode argument may be any binary mode acceptable to the built-in open() function; the 'b' is automatically added.
 
`encoding` specifies the encoding which is to be used for the file. Any encoding that encodes to and decodes from bytes is allowed, and the data types supported by the file methods depend on the codec used.

`errors` may be given to define the error handling. It defaults to 'strict' which causes a ValueError to be raised in case an encoding error occurs.

`buffering` has the same meaning as for the built-in open() function. It defaults to -1 which means that the default buffer size will be used.


➡ `codecs.EncodedFile(file, data_encoding, file_encoding=None, errors='strict')`
Return a StreamRecoder instance, a wrapped version of file which provides transparent transcoding. The original file is closed when the wrapped version is closed.

Data written to the wrapped file is decoded according to the given data_encoding and then written to the original file as bytes using file_encoding. Bytes read from the original file are decoded according to file_encoding, and the result is encoded using data_encoding.

If file_encoding is not given, it defaults to data_encoding.

errors may be given to define the error handling. It defaults to 'strict', which causes ValueError to be raised in case an encoding error occurs.

➡ `codecs.iterencode(iterator, encoding, errors='strict', **kwargs)`
Uses an incremental encoder to iteratively encode the input provided by iterator. This function is a generator. The errors argument (as well as any other keyword argument) is passed through to the incremental encoder.

This function requires that the codec accept text str objects to encode. Therefore it does not support bytes-to-bytes encoders such as base64_codec.

➡ `codecs.iterdecode(iterator, encoding, errors='strict', **kwargs)`
Uses an incremental decoder to iteratively decode the input provided by iterator. This function is a generator. The errors argument (as well as any other keyword argument) is passed through to the incremental decoder.

This function requires that the codec accept bytes objects to decode. Therefore it does not support text-to-text encoders such as rot_13, although rot_13 may be used equivalently with iterencode().

The module also provides the following constants which are useful for reading and writing to platform dependent files:

➡ `codecs.BOM`
➡ `codecs.BOM_BE`
➡ `codecs.BOM_LE`
➡ `codecs.BOM_UTF8`
➡ `codecs.BOM_UTF16`
➡ `codecs.BOM_UTF16_BE`
➡ `codecs.BOM_UTF16_LE`
➡ `codecs.BOM_UTF32`
➡ `codecs.BOM_UTF32_BE`
➡ `codecs.BOM_UTF32_LE`
These constants define various byte sequences, being Unicode byte order marks (BOMs) for several encodings. They are used in UTF-16 and UTF-32 data streams to indicate the byte order used, and in UTF-8 as a Unicode signature. BOM_UTF16 is either BOM_UTF16_BE or BOM_UTF16_LE depending on the platform’s native byte order, BOM is an alias for BOM_UTF16, BOM_LE for BOM_UTF16_LE and BOM_BE for BOM_UTF16_BE. The others represent the BOM in UTF-8 and UTF-32 encodings.


### ===🗝 Codec Base Classes

The codecs module defines a set of base classes which define the interfaces for working with codec objects, and can also be used as the basis for custom codec implementations.

Each codec has to define four interfaces to make it usable as codec in Python: stateless encoder, stateless decoder, stream reader and stream writer. The stream reader and writers typically reuse the stateless encoder/decoder to implement the file protocols. Codec authors also need to define how the codec will handle encoding and decoding errors.


#### Error Handlers

To simplify and standardize error handling, codecs may implement different error handling schemes by accepting the errors string argument. The following string values are defined and implemented by all standard Python codecs:

|Value     | Meaning
| 'strict' | Raise UnicodeError (or a subclass); this is the default. Implemented in strict_errors(). 
| 'ignore' | Ignore the malformed data and continue without further notice. Implemented in ignore_errors(). 

The following error handlers are only applicable to text encodings:

➡ `'replace'` Replace with a suitable replacement marker; Python will use the official U+FFFD REPLACEMENT CHARACTER for the built-in codecs on decoding, and ‘?’ on encoding. Implemented in replace_errors(). 
➡ `'xmlcharrefreplace'` Replace with the appropriate XML character reference (only for encoding). Implemented in xmlcharrefreplace_errors(). 
➡ `'backslashreplace'` Replace with backslashed escape sequences. Implemented in backslashreplace_errors(). 
➡ `'namereplace'` Replace with \N{...} escape sequences (only for encoding). Implemented in namereplace_errors(). 
➡ `'surrogateescape'` On decoding, replace byte with individual surrogate code ranging from U+DC80 to U+DCFF. This code will then be turned back into the same byte when the 'surrogateescape' error handler is used when encoding the data. (See PEP 383 for more.) 

In addition, the following error handler is specific to the given codecs:


|Value  |Codecs  |Meaning 
'surrogatepass' utf-8, utf-16, utf-32, utf-16-be, utf-16-le, utf-32-be, utf-32-le Allow encoding and decoding of surrogate codes. These codecs normally treat the presence of surrogates as an error. 

New in version 3.1: The 'surrogateescape' and 'surrogatepass' error handlers.


Changed in version 3.4: The 'surrogatepass' error handlers now works with utf-16* and utf-32* codecs.


New in version 3.5: The 'namereplace' error handler.


Changed in version 3.5: The 'backslashreplace' error handlers now works with decoding and translating.


The set of allowed values can be extended by registering a new named error handler:

➡ `codecs.register_error(name, error_handler)`
Register the error handling function error_handler under the name name. The error_handler argument will be called during encoding and decoding in case of an error, when name is specified as the errors parameter.

For encoding, error_handler will be called with a UnicodeEncodeError instance, which contains information about the location of the error. The error handler must either raise this or a different exception, or return a tuple with a replacement for the unencodable part of the input and a position where encoding should continue. The replacement may be either str or bytes. If the replacement is bytes, the encoder will simply copy them into the output buffer. If the replacement is a string, the encoder will encode the replacement. Encoding continues on original input at the specified position. Negative position values will be treated as being relative to the end of the input string. If the resulting position is out of bound an IndexError will be raised.

Decoding and translating works similarly, except UnicodeDecodeError or UnicodeTranslateError will be passed to the handler and that the replacement from the error handler will be put into the output directly.

Previously registered error handlers (including the standard error handlers) can be looked up by name:

➡ `codecs.lookup_error(name)`
Return the error handler previously registered under the name name.

Raises a LookupError in case the handler cannot be found.

The following standard error handlers are also made available as module level functions:

➡ `codecs.strict_errors(exception)`
Implements the 'strict' error handling: each encoding or decoding error raises a UnicodeError.

➡ `codecs.replace_errors(exception)`
Implements the 'replace' error handling (for text encodings only): substitutes '?' for encoding errors (to be encoded by the codec), and '\ufffd' (the Unicode replacement character) for decoding errors.

➡ `codecs.ignore_errors(exception)`
Implements the 'ignore' error handling: malformed data is ignored and encoding or decoding is continued without further notice.

➡ `codecs.xmlcharrefreplace_errors(exception)`
Implements the 'xmlcharrefreplace' error handling (for encoding with text encodings only): the unencodable character is replaced by an appropriate XML character reference.

➡ `codecs.backslashreplace_errors(exception)`
Implements the 'backslashreplace' error handling (for text encodings only): malformed data is replaced by a backslashed escape sequence.

➡ `codecs.namereplace_errors(exception)`
Implements the 'namereplace' error handling (for encoding with text encodings only): the unencodable character is replaced by a \N{...} escape sequence.

New in version 3.5.



#### Stateless Encoding and Decoding

The base Codec class defines these methods which also define the function interfaces of the stateless encoder and decoder:

➡ `Codec.encode(input[, errors])`
Encodes the object input and returns a tuple (output object, length consumed). For instance, text encoding converts a string object to a bytes object using a particular character set encoding (e.g., cp1252 or iso-8859-1).

The errors argument defines the error handling to apply. It defaults to 'strict' handling.

The method may not store state in the Codec instance. Use StreamWriter for codecs which have to keep state in order to make encoding efficient.

The encoder must be able to handle zero length input and return an empty object of the output object type in this situation.

➡ `Codec.decode(input[, errors])`
Decodes the object input and returns a tuple (output object, length consumed). For instance, for a text encoding, decoding converts a bytes object encoded using a particular character set encoding to a string object.

For text encodings and bytes-to-bytes codecs, input must be a bytes object or one which provides the read-only buffer interface – for example, buffer objects and memory mapped files.

The errors argument defines the error handling to apply. It defaults to 'strict' handling.

The method may not store state in the Codec instance. Use StreamReader for codecs which have to keep state in order to make decoding efficient.

The decoder must be able to handle zero length input and return an empty object of the output object type in this situation.


#### Incremental Encoding and Decoding

The IncrementalEncoder and IncrementalDecoder classes provide the basic interface for incremental encoding and decoding. Encoding/decoding the input isn’t done with one call to the stateless encoder/decoder function, but with multiple calls to the encode()/decode() method of the incremental encoder/decoder. The incremental encoder/decoder keeps track of the encoding/decoding process during method calls.

The joined output of calls to the encode()/decode() method is the same as if all the single inputs were joined into one, and this input was encoded/decoded with the stateless encoder/decoder.


#### IncrementalEncoder Objects

The IncrementalEncoder class is used for encoding an input in multiple steps. It defines the following methods which every incremental encoder must define in order to be compatible with the Python codec registry.

✅ `class codecs.IncrementalEncoder(errors='strict')`
Constructor for an IncrementalEncoder instance.

All incremental encoders must provide this constructor interface. They are free to add additional keyword arguments, but only the ones defined here are used by the Python codec registry.

The IncrementalEncoder may implement different error handling schemes by providing the errors keyword argument. See Error Handlers for possible values.

The errors argument will be assigned to an attribute of the same name. Assigning to this attribute makes it possible to switch between different error handling strategies during the lifetime of the IncrementalEncoder object.

➡ `encode(object[, final])`
Encodes object (taking the current state of the encoder into account) and returns the resulting encoded object. If this is the last call to encode() final must be true (the default is false).

➡ `reset()`
Reset the encoder to the initial state. The output is discarded: call .encode(object, final=True), passing an empty byte or text string if necessary, to reset the encoder and to get the output.

➡ `getstate()`

Return the current state of the encoder which must be an integer. The implementation should make sure that 0 is the most common state. (States that are more complicated than integers can be converted into an integer by marshaling/pickling the state and encoding the bytes of the resulting string into an integer.)

➡ `setstate(state)`
Set the state of the encoder to state. state must be an encoder state returned by getstate().


#### IncrementalDecoder Objects

The IncrementalDecoder class is used for decoding an input in multiple steps. It defines the following methods which every incremental decoder must define in order to be compatible with the Python codec registry.

✅ `class codecs.IncrementalDecoder(errors='strict')`
Constructor for an IncrementalDecoder instance.

All incremental decoders must provide this constructor interface. They are free to add additional keyword arguments, but only the ones defined here are used by the Python codec registry.

The IncrementalDecoder may implement different error handling schemes by providing the errors keyword argument. See Error Handlers for possible values.

The errors argument will be assigned to an attribute of the same name. Assigning to this attribute makes it possible to switch between different error handling strategies during the lifetime of the IncrementalDecoder object.

➡ `decode(object[, final])`
Decodes object (taking the current state of the decoder into account) and returns the resulting decoded object. If this is the last call to decode() final must be true (the default is false). If final is true the decoder must decode the input completely and must flush all buffers. If this isn’t possible (e.g. because of incomplete byte sequences at the end of the input) it must initiate error handling just like in the stateless case (which might raise an exception).

➡ `reset()`
Reset the decoder to the initial state.

➡ `getstate()`

Return the current state of the decoder. This must be a tuple with two items, the first must be the buffer containing the still undecoded input. The second must be an integer and can be additional state info. (The implementation should make sure that 0 is the most common additional state info.) If this additional state info is 0 it must be possible to set the decoder to the state which has no input buffered and 0 as the additional state info, so that feeding the previously buffered input to the decoder returns it to the previous state without producing any output. (Additional state info that is more complicated than integers can be converted into an integer by marshaling/pickling the info and encoding the bytes of the resulting string into an integer.)


➡ `setstate(state)`
Set the state of the decoder to state. state must be a decoder state returned by getstate().


#### Stream Encoding and Decoding

The StreamWriter and StreamReader classes provide generic working interfaces which can be used to implement new encoding submodules very easily. See encodings.utf_8 for an example of how this is done.


#### StreamWriter Objects

The StreamWriter class is a subclass of Codec and defines the following methods which every stream writer must define in order to be compatible with the Python codec registry.

✅ `class codecs.StreamWriter(stream, errors='strict')`
Constructor for a StreamWriter instance.

All stream writers must provide this constructor interface. They are free to add additional keyword arguments, but only the ones defined here are used by the Python codec registry.

The stream argument must be a file-like object open for writing text or binary data, as appropriate for the specific codec.

The StreamWriter may implement different error handling schemes by providing the errors keyword argument. See Error Handlers for the standard error handlers the underlying stream codec may support.

The errors argument will be assigned to an attribute of the same name. Assigning to this attribute makes it possible to switch between different error handling strategies during the lifetime of the StreamWriter object.

➡ `write(object)`
Writes the object’s contents encoded to the stream.

➡ `writelines(list)`
Writes the concatenated list of strings to the stream (possibly by reusing the write() method). The standard bytes-to-bytes codecs do not support this method.

➡ `reset()`
Resets the codec buffers used for keeping internal state.

Calling this method should ensure that the data on the output is put into a clean state that allows appending of new fresh data without having to rescan the whole stream to recover state.

In addition to the above methods, the StreamWriter must also inherit all other methods and attributes from the underlying stream.


#### StreamReader Objects

The StreamReader class is a subclass of Codec and defines the following methods which every stream reader must define in order to be compatible with the Python codec registry.

✅ `class codecs.StreamReader(stream, errors='strict')`
Constructor for a StreamReader instance.

All stream readers must provide this constructor interface. They are free to add additional keyword arguments, but only the ones defined here are used by the Python codec registry.

The stream argument must be a file-like object open for reading text or binary data, as appropriate for the specific codec.

The StreamReader may implement different error handling schemes by providing the errors keyword argument. See Error Handlers for the standard error handlers the underlying stream codec may support.

The errors argument will be assigned to an attribute of the same name. Assigning to this attribute makes it possible to switch between different error handling strategies during the lifetime of the StreamReader object.

The set of allowed values for the errors argument can be extended with register_error().

➡ `read([size[, chars[, firstline]]])`
Decodes data from the stream and returns the resulting object.

The chars argument indicates the number of decoded code points or bytes to return. The read() method will never return more data than requested, but it might return less, if there is not enough available.

The size argument indicates the approximate maximum number of encoded bytes or code points to read for decoding. The decoder can modify this setting as appropriate. The default value -1 indicates to read and decode as much as possible. This parameter is intended to prevent having to decode huge files in one step.

The firstline flag indicates that it would be sufficient to only return the first line, if there are decoding errors on later lines.

The method should use a greedy read strategy meaning that it should read as much data as is allowed within the definition of the encoding and the given size, e.g. if optional encoding endings or state markers are available on the stream, these should be read too.

➡ `readline([size[, keepends]])`
Read one line from the input stream and return the decoded data.

size, if given, is passed as size argument to the stream’s read() method.

If keepends is false line-endings will be stripped from the lines returned.

➡ `readlines([sizehint[, keepends]])`
Read all lines available on the input stream and return them as a list of lines.

Line-endings are implemented using the codec’s decode() method and are included in the list entries if keepends is true.

sizehint, if given, is passed as the size argument to the stream’s read() method.

➡ `reset()`
Resets the codec buffers used for keeping internal state.

Note that no stream repositioning should take place. This method is primarily intended to be able to recover from decoding errors.

In addition to the above methods, the StreamReader must also inherit all other methods and attributes from the underlying stream.


#### StreamReaderWriter Objects

The StreamReaderWriter is a convenience class that allows wrapping streams which work in both read and write modes.

The design is such that one can use the factory functions returned by the lookup() function to construct the instance.

✅ `class codecs.StreamReaderWriter(stream, Reader, Writer, errors='strict')`
Creates a StreamReaderWriter instance. stream must be a file-like object. Reader and Writer must be factory functions or classes providing the StreamReader and StreamWriter interface resp. Error handling is done in the same way as defined for the stream readers and writers.

StreamReaderWriter instances define the combined interfaces of StreamReader and StreamWriter classes. They inherit all other methods and attributes from the underlying stream.


#### StreamRecoder Objects

The StreamRecoder translates data from one encoding to another, which is sometimes useful when dealing with different encoding environments.

The design is such that one can use the factory functions returned by the lookup() function to construct the instance.

✅ `class codecs.StreamRecoder(stream, encode, decode, Reader, Writer, errors='strict')`
Creates a StreamRecoder instance which implements a two-way conversion: encode and decode work on the frontend — the data visible to code calling read() and write(), while Reader and Writer work on the backend — the data in stream.

You can use these objects to do transparent transcodings, e.g., from Latin-1 to UTF-8 and back.

The stream argument must be a file-like object.

The encode and decode arguments must adhere to the Codec interface. Reader and Writer must be factory functions or classes providing objects of the StreamReader and StreamWriter interface respectively.

Error handling is done in the same way as defined for the stream readers and writers.

StreamRecoder instances define the combined interfaces of StreamReader and StreamWriter classes. They inherit all other methods and attributes from the underlying stream.


### ===🗝 Encodings and Unicode

Strings are stored internally as sequences of code points in range `0x0–0x10FFFF`. (See PEP 393 for more details about the implementation.) Once a string object is used outside of CPU and memory, endianness and how these arrays are stored as bytes become an issue. As with other codecs, serialising a string into a sequence of bytes is known as encoding, and recreating the string from the sequence of bytes is known as decoding. There are a variety of different text serialisation codecs, which are collectivity referred to as text encodings.

The simplest text encoding (called 'latin-1' or 'iso-8859-1') maps the code points 0–255 to the bytes `0x0–0xff`, which means that a string object that contains code points above U+00FF can’t be encoded with this codec. Doing so will raise a UnicodeEncodeError that looks like the following (although the details of the error message may differ): UnicodeEncodeError: 'latin-1' codec can't encode character '\u1234' in position 3: ordinal not in range(256).

There’s another group of encodings (the so called charmap encodings) that choose a different subset of all Unicode code points and how these code points are mapped to the bytes `0x0–0xff`. To see how this is done simply open e.g. encodings/cp1252.py (which is an encoding that is used primarily on Windows). There’s a string constant with 256 characters that shows you which character is mapped to which byte value.

All of these encodings can only encode 256 of the 1114112 code points defined in Unicode. A simple and straightforward way that can store each Unicode code point, is to store each code point as four consecutive bytes. There are two possibilities: store the bytes in big endian or in little endian order. These two encodings are called UTF-32-BE and UTF-32-LE respectively. Their disadvantage is that if e.g. you use UTF-32-BE on a little endian machine you will always have to swap bytes on encoding and decoding. UTF-32 avoids this problem: bytes will always be in natural endianness. When these bytes are read by a CPU with a different endianness, then bytes have to be swapped though. To be able to detect the endianness of a UTF-16 or UTF-32 byte sequence, there’s the so called BOM (“Byte Order Mark”). This is the Unicode character `U+FEFF`. This character can be prepended to every UTF-16 or UTF-32 byte sequence. The byte swapped version of this character (`0xFFFE`) is an illegal character that may not appear in a Unicode text. So when the first character in an UTF-16 or UTF-32 byte sequence appears to be a `U+FFFE` the bytes have to be swapped on decoding. Unfortunately the character `U+FEFF` had a second purpose as a ZERO WIDTH NO-BREAK SPACE: a character that has no width and doesn’t allow a word to be split. It can e.g. be used to give hints to a ligature algorithm. With Unicode 4.0 using `U+FEFF` as a ZERO WIDTH NO-BREAK SPACE has been deprecated (with `U+2060` (WORD JOINER) assuming this role). Nevertheless Unicode software still must be able to handle `U+FEFF` in both roles: as a BOM it’s a device to determine the storage layout of the encoded bytes, and vanishes once the byte sequence has been decoded into a string; as a ZERO WIDTH NO-BREAK SPACE it’s a normal character that will be decoded like any other.

There’s another encoding that is able to encode the full range of Unicode characters: UTF-8. UTF-8 is an 8-bit encoding, which means there are no issues with byte order in UTF-8. Each byte in a UTF-8 byte sequence consists of two parts: marker bits (the most significant bits) and payload bits. The marker bits are a sequence of zero to four 1 bits followed by a 0 bit. Unicode characters are encoded like this (with x being payload bits, which when concatenated give the Unicode character):



|          Range          |               Encoding              |
|-------------------------|-------------------------------------|
| U-00000000 … U-0000007F | 0xxxxxxx                            |
| U-00000080 … U-000007FF | 110xxxxx 10xxxxxx                   |
| U-00000800 … U-0000FFFF | 1110xxxx 10xxxxxx 10xxxxxx          |
| U-00010000 … U-0010FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx |

The least significant bit of the Unicode character is the rightmost x bit.

As UTF-8 is an 8-bit encoding no BOM is required and any `U+FEFF` character in the decoded string (even if it’s the first character) is treated as a ZERO WIDTH NO-BREAK SPACE.

Without external information it’s impossible to reliably determine which encoding was used for encoding a string. Each charmap encoding can decode any random byte sequence. However that’s not possible with UTF-8, as UTF-8 byte sequences have a structure that doesn’t allow arbitrary byte sequences. To increase the reliability with which a UTF-8 encoding can be detected, Microsoft invented a variant of UTF-8 (that Python 2.5 calls "utf-8-sig") for its Notepad program: Before any of the Unicode characters is written to the file, a UTF-8 encoded BOM (which looks like this as a byte sequence: 0xef, 0xbb, 0xbf) is written. As it’s rather improbable that any charmap encoded file starts with these byte values (which would e.g. map to


    LATIN SMALL LETTER I WITH DIAERESIS

    RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK

    INVERTED QUESTION MARK

in iso-8859-1), this increases the probability that a utf-8-sig encoding can be correctly guessed from the byte sequence. So here the BOM is not used to be able to determine the byte order used for generating the byte sequence, but as a signature that helps in guessing the encoding. On encoding the utf-8-sig codec will write 0xef, 0xbb, 0xbf as the first three bytes to the file. On decoding utf-8-sig will skip those three bytes if they appear as the first three bytes in the file. In UTF-8, the use of the BOM is discouraged and should generally be avoided.


### ===🗝 Standard Encodings

Python comes with a number of codecs built-in, either implemented as C functions or with dictionaries as mapping tables. The following table lists the codecs by name, together with a few common aliases, and the languages for which the encoding is likely used. Neither the list of aliases nor the list of languages is meant to be exhaustive. Notice that spelling alternatives that only differ in case or use a hyphen instead of an underscore are also valid aliases; therefore, e.g. 'utf-8' is a valid alias for the 'utf_8' codec.


CPython implementation detail: Some common encodings can bypass the codecs lookup machinery to improve performance. These optimization opportunities are only recognized by CPython for a limited set of (case insensitive) aliases: utf-8, utf8, latin-1, latin1, iso-8859-1, iso8859-1, mbcs (Windows only), ascii, us-ascii, utf-16, utf16, utf-32, utf32, and the same using underscores instead of dashes. Using alternative aliases for these encodings may result in slower execution.

Changed in version 3.6: Optimization opportunity recognized for us-ascii.


Many of the character sets support the same languages. They vary in individual characters (e.g. whether the EURO SIGN is supported or not), and in the assignment of characters to code positions. For the European languages in particular, the following variants typically exist:

• an ISO 8859 codeset
• a Microsoft Windows code page, which is typically derived from an 8859 codeset, but replaces control characters with additional graphic characters
• an IBM EBCDIC code page
• an IBM PC code page, which is ASCII compatible

|   Codec   |        Aliases        |         Languages          |
|-----------|-----------------------|----------------------------|
| ascii     | 646, us-ascii         | English                    |
| big5      | big5-tw, csbig5       | Traditional Chinese        |
| big5hkscs | big5-hkscs, hkscs     | Traditional Chinese        |
| cp037     | IBM037, IBM039        | English                    |
| cp273     | 273, IBM273, csIBM273 | German New in version 3.4. |

cp424 EBCDIC-CP-HE, IBM424 Hebrew 
cp437 437, IBM437 English 
cp500 EBCDIC-CP-BE, EBCDIC-CP-CH, IBM500 Western Europe 
cp720   Arabic 
cp737   Greek 
cp775 IBM775 Baltic languages 
cp850 850, IBM850 Western Europe 
cp852 852, IBM852 Central and Eastern Europe 
cp855 855, IBM855 Bulgarian, Byelorussian, Macedonian, Russian, Serbian 
cp856   Hebrew 
cp857 857, IBM857 Turkish 
cp858 858, IBM858 Western Europe 
cp860 860, IBM860 Portuguese 
cp861 861, CP-IS, IBM861 Icelandic 
cp862 862, IBM862 Hebrew 
cp863 863, IBM863 Canadian 
cp864 IBM864 Arabic 
cp865 865, IBM865 Danish, Norwegian 
cp866 866, IBM866 Russian 
cp869 869, CP-GR, IBM869 Greek 
cp874   Thai 
cp875   Greek 
cp932 932, ms932, mskanji, ms-kanji Japanese 
cp949 949, ms949, uhc Korean 
cp950 950, ms950 Traditional Chinese 
cp1006   Urdu 
cp1026 ibm1026 Turkish 
cp1125 1125, ibm1125, cp866u, ruscii Ukrainian New in version 3.4.
cp1140 ibm1140 Western Europe 
cp1250 windows-1250 Central and Eastern Europe 
cp1251 windows-1251 Bulgarian, Byelorussian, Macedonian, Russian, Serbian 
cp1252 windows-1252 Western Europe 
cp1253 windows-1253 Greek 
cp1254 windows-1254 Turkish 
cp1255 windows-1255 Hebrew 
cp1256 windows-1256 Arabic 
cp1257 windows-1257 Baltic languages 
cp1258 windows-1258 Vietnamese 
euc_jp eucjp, ujis, u-jis Japanese 
euc_jis_2004 jisx0213, eucjis2004 Japanese 
euc_jisx0213 eucjisx0213 Japanese 
euc_kr euckr, korean, ksc5601, ks_c-5601, ks_c-5601-1987, ksx1001, ks_x-1001 Korean 
gb2312 chinese, csiso58gb231280, euc-cn, euccn, eucgb2312-cn, gb2312-1980, gb2312-80, iso-ir-58 Simplified Chinese 
gbk 936, cp936, ms936 Unified Chinese 
gb18030 gb18030-2000 Unified Chinese 
hz hzgb, hz-gb, hz-gb-2312 Simplified Chinese 
iso2022_jp csiso2022jp, iso2022jp, iso-2022-jp Japanese 
iso2022_jp_1 iso2022jp-1, iso-2022-jp-1 Japanese 
iso2022_jp_2 iso2022jp-2, iso-2022-jp-2 Japanese, Korean, Simplified Chinese, Western Europe, Greek 
iso2022_jp_2004 iso2022jp-2004, iso-2022-jp-2004 Japanese 
iso2022_jp_3 iso2022jp-3, iso-2022-jp-3 Japanese 
iso2022_jp_ext iso2022jp-ext, iso-2022-jp-ext Japanese 
iso2022_kr csiso2022kr, iso2022kr, iso-2022-kr Korean 
latin_1 iso-8859-1, iso8859-1, 8859, cp819, latin, latin1, L1 Western Europe 
iso8859_2 iso-8859-2, latin2, L2 Central and Eastern Europe 
iso8859_3 iso-8859-3, latin3, L3 Esperanto, Maltese 
iso8859_4 iso-8859-4, latin4, L4 Baltic languages 
iso8859_5 iso-8859-5, cyrillic Bulgarian, Byelorussian, Macedonian, Russian, Serbian 
iso8859_6 iso-8859-6, arabic Arabic 
iso8859_7 iso-8859-7, greek, greek8 Greek 
iso8859_8 iso-8859-8, hebrew Hebrew 
iso8859_9 iso-8859-9, latin5, L5 Turkish 
iso8859_10 iso-8859-10, latin6, L6 Nordic languages 
iso8859_11 iso-8859-11, thai Thai languages 
iso8859_13 iso-8859-13, latin7, L7 Baltic languages 
iso8859_14 iso-8859-14, latin8, L8 Celtic languages 
iso8859_15 iso-8859-15, latin9, L9 Western Europe 
iso8859_16 iso-8859-16, latin10, L10 South-Eastern Europe 
johab cp1361, ms1361 Korean 
koi8_r   Russian 
koi8_t   Tajik New in version 3.5.
koi8_u   Ukrainian 
kz1048 kz_1048, strk1048_2002, rk1048  Kazakh New in version 3.5.
mac_cyrillic maccyrillic Bulgarian, Byelorussian, Macedonian, Russian, Serbian 
mac_greek macgreek Greek 
mac_iceland maciceland Icelandic 
mac_latin2 maclatin2, maccentraleurope, mac_centeuro Central and Eastern Europe 
mac_roman macroman, macintosh Western Europe 
mac_turkish macturkish Turkish 
ptcp154 csptcp154, pt154, cp154, cyrillic-asian Kazakh 
shift_jis csshiftjis, shiftjis, sjis, s_jis Japanese 
shift_jis_2004 shiftjis2004, sjis_2004, sjis2004 Japanese 
shift_jisx0213 shiftjisx0213, sjisx0213, s_jisx0213 Japanese 
utf_32 U32, utf32 all languages 
utf_32_be UTF-32BE all languages 
utf_32_le UTF-32LE all languages 
utf_16 U16, utf16 all languages 
utf_16_be UTF-16BE all languages 
utf_16_le UTF-16LE all languages 
utf_7 U7, unicode-1-1-utf-7 all languages 
utf_8 U8, UTF, utf8, cp65001 all languages 
utf_8_sig   all languages 

Changed in version 3.4: The utf-16* and utf-32* encoders no longer allow surrogate code points (U+D800–U+DFFF) to be encoded. The utf-32* decoders no longer decode byte sequences that correspond to surrogate code points.


Changed in version 3.8: cp65001 is now an alias to utf_8.



### ===🗝 Python Specific Encodings

A number of predefined codecs are specific to Python, so their codec names have no meaning outside Python. These are listed in the tables below based on the expected input and output types (note that while text encodings are the most common use case for codecs, the underlying codec infrastructure supports arbitrary data transforms rather than just text encodings). For asymmetric codecs, the stated meaning describes the encoding direction.


#### Text Encodings

The following codecs provide str to bytes encoding and bytes-like object to str decoding, similar to the Unicode text encodings.


|Codec |Aliases |Meaning
idna   Implement RFC 3490, see also encodings.idna. Only errors='strict' is supported. 
mbcs ansi, dbcs Windows only: Encode the operand according to the ANSI codepage (CP_ACP). 
oem   Windows only: Encode the operand according to the OEM codepage (CP_OEMCP). New in version 3.6.
palmos   Encoding of PalmOS 3.5. 
punycode   Implement RFC 3492. Stateful codecs are not supported. 
raw_unicode_escape   Latin-1 encoding with \uXXXX and \UXXXXXXXX for other code points. Existing backslashes are not escaped in any way. It is used in the Python pickle protocol. 
undefined   Raise an exception for all conversions, even empty strings. The error handler is ignored. 
unicode_escape   Encoding suitable as the contents of a Unicode literal in ASCII-encoded Python source code, except that quotes are not escaped. Decode from Latin-1 source code. Beware that Python source code actually uses UTF-8 by default. 

Changed in version 3.8: “unicode_internal” codec is removed.


#### Binary Transforms

The following codecs provide binary transforms: bytes-like object to bytes mappings. They are not supported by bytes.decode() (which only produces str output).

|Codec |Aliases |Meaning |Encoder / decoder
base64_codec [1] base64, base_64 
Convert the operand to multiline MIME base64 (the result always includes a trailing '\n'). Changed in version 3.4: accepts any bytes-like object as input for encoding and decoding
base64.encodebytes() / base64.decodebytes() 
bz2_codec bz2 Compress the operand using bz2. bz2.compress() / bz2.decompress() 
hex_codec hex Convert the operand to hexadecimal representation, with two digits per byte. binascii.b2a_hex() / binascii.a2b_hex() 
quopri_codec quopri, quotedprintable, quoted_printable Convert the operand to MIME quoted printable. quopri.encode() with quotetabs=True / quopri.decode() 
uu_codec uu Convert the operand using uuencode. uu.encode() / uu.decode() 
zlib_codec zip, zlib Compress the operand using gzip. zlib.compress() / zlib.decompress() 

[1] In addition to bytes-like objects, 'base64_codec' also accepts ASCII-only instances of str for decoding 

New in version 3.2: Restoration of the binary transforms.

Changed in version 3.4: Restoration of the aliases for the binary transforms.


#### Text Transforms

The following codec provides a text transform: a str to str mapping. It is not supported by str.encode() (which only produces bytes output).


|Codec |Aliases |Meaning
rot_13 rot13 Return the Caesar-cypher encryption of the operand. 

New in version 3.2: Restoration of the rot_13 text transform.

Changed in version 3.4: Restoration of the rot13 alias.


### ===🗝 encodings.idna — Internationalized Domain Names in Applications

This module implements RFC 3490 (Internationalized Domain Names in Applications) and RFC 3492 (Nameprep: A Stringprep Profile for Internationalized Domain Names (IDN)). It builds upon the punycode encoding and stringprep.

If you need the IDNA 2008 standard from RFC 5891 and RFC 5895, use the third-party idna module <https://pypi.org/project/idna/>.

These RFCs together define a protocol to support non-ASCII characters in domain names. A domain name containing non-ASCII characters (such as www.Alliancefrançaise.nu) is converted into an ASCII-compatible encoding (ACE, such as www.xn--alliancefranaise-npb.nu). The ACE form of the domain name is then used in all places where arbitrary characters are not allowed by the protocol, such as DNS queries, HTTP Host fields, and so on. This conversion is carried out in the application; if possible invisible to the user: The application should transparently convert Unicode domain labels to IDNA on the wire, and convert back ACE labels to Unicode before presenting them to the user.

Python supports this conversion in several ways: the idna codec performs conversion between Unicode and ACE, separating an input string into labels based on the separator characters defined in section 3.1 of RFC 3490 and converting each label to ACE as required, and conversely separating an input byte string into labels based on the . separator and converting any ACE labels found into unicode. Furthermore, the socket module transparently converts Unicode host names to ACE, so that applications need not be concerned about converting host names themselves when they pass them to the socket module. On top of that, modules that have host names as function parameters, such as http.client and ftplib, accept Unicode host names (http.client then also transparently sends an IDNA hostname in the Host field if it sends that field at all).

When receiving host names from the wire (such as in reverse name lookup), no automatic conversion to Unicode is performed: applications wishing to present such host names to the user should decode them to Unicode.

The module encodings.idna also implements the nameprep procedure, which performs certain normalizations on host names, to achieve case-insensitivity of international domain names, and to unify similar characters. The nameprep functions can be used directly if desired.

➡ `encodings.idna.nameprep(label)`
Return the nameprepped version of label. The implementation currently assumes query strings, so AllowUnassigned is true.

➡ `encodings.idna.ToASCII(label)`
Convert a label to ASCII, as specified in RFC 3490. UseSTD3ASCIIRules is assumed to be false.

➡ `encodings.idna.ToUnicode(label)`
Convert a label to Unicode, as specified in RFC 3490.


#### encodings.mbcs — Windows ANSI codepage

This module implements the ANSI codepage (CP_ACP).

Availability: Windows only.

Changed in version 3.3: Support any error handler.


Changed in version 3.2: Before 3.2, the errors argument was ignored; 'replace' was always used to encode, and 'ignore' to decode.



#### encodings.utf_8_sig — UTF-8 codec with BOM signature

This module implements a variant of the UTF-8 codec. On encoding, a UTF-8 encoded BOM will be prepended to the UTF-8 encoded bytes. For the stateful encoder this is only done once (on the first write to the byte stream). On decoding, an optional UTF-8 encoded BOM at the start of the data will be skipped.


# =🚩 Data Types
- The Python Standard Library » Data Types

- `datetime` — Basic date and time types
- `zoneinfo` — IANA time zone support
- `calendar` — General calendar-related functions
- `collections` — Container datatypes
- `collections`.abc — Abstract Base Classes for Containers
- `heapq` — Heap queue algorithm
- `bisect` — Array bisection algorithm
- `array` — Efficient arrays of numeric values
- `weakref` — Weak references
- `types` — Dynamic type creation and names for built-in types
- `copy` — Shallow and deep copy operations
- `pprint` — Data pretty printer
- `reprlib` — Alternate repr() implementation
- `enum` — Support for enumerations
- `graphlib` — Functionality to operate with graph-like structures

The modules described in this chapter provide a variety of specialized data types such as dates and times, fixed-type arrays, heap queues, double-ended queues, and enumerations.

Python also provides some built-in data types, in particular, dict, list, set and frozenset, and tuple. The str class is used to hold Unicode strings, and the bytes and bytearray classes are used to hold binary data.

The following modules are documented in this chapter:

## ==⚡ datetime — Basic date and time types
◦ Aware and Naive Objects
◦ Constants
◦ Available Types
◾Common Properties
◾Determining if an Object is Aware or Naive

◦ timedelta Objects
◾Examples of usage: timedelta

◦ date Objects
◾Examples of Usage: date

◦ datetime Objects
◾Examples of Usage: datetime

◦ time Objects
◾Examples of Usage: time

◦ tzinfo Objects
◦ timezone Objects
◦ strftime() and strptime() Behavior
◾strftime() and strptime() Format Codes
◾Technical Detail

### ===🗝 time Objects

16. Generic Operating System Services »
 
16.3. time — Time access and conversions

• The epoch is the point where the time starts, and is platform dependent. For Unix, the epoch is January 1, 1970, 00:00:00 (UTC). To find out what the epoch is on a given platform, look at time.gmtime(0).

• The term seconds since the epoch refers to the total number of elapsed seconds since the epoch, typically excluding leap seconds. Leap seconds are excluded from this total on all POSIX-compliant platforms.

• The functions in this module may not handle dates and times before the epoch or far in the future. The cut-off point in the future is determined by the C library; for 32-bit systems, it is typically in 2038.

• Year 2000 (Y2K) issues: Python depends on the platform’s C library, which generally doesn’t have year 2000 issues, since all dates and times are represented internally as seconds since the epoch. Function strptime() can parse 2-digit years when given %y format code. When 2-digit years are parsed, they are converted according to the POSIX and ISO C standards: values 69–99 are mapped to 1969–1999, and values 0–68 are mapped to 2000–2068.

• UTC is Coordinated Universal Time (formerly known as Greenwich Mean Time, or GMT). The acronym UTC is not a mistake but a compromise between English and French.

• DST is Daylight Saving Time, an adjustment of the timezone by (usually) one hour during part of the year. DST rules are magic (determined by local law) and can change from year to year. The C library has a table containing the local rules (often it is read from a system file for flexibility) and is the only source of True Wisdom in this respect.


• The precision of the various real-time functions may be less than suggested by the units in which their value or argument is expressed. E.g. on most Unix systems, the clock “ticks” only 50 or 100 times a second.


• On the other hand, the precision of time() and sleep() is better than their Unix equivalents: times are expressed as floating point numbers, time() returns the most accurate time available (using Unix gettimeofday() where available), and sleep() will accept a time with a nonzero fraction (Unix select() is used to implement this, where available).


• The time value as returned by gmtime(), localtime(), and strptime(), and accepted by asctime(), mktime() and strftime(), is a sequence of 9 integers. The return values of gmtime(), localtime(), and strptime() also offer attribute names for individual fields.


• Use the following functions to convert between time representations:


    From                       To                         Use
    seconds since the epoch    struct_time in UTC         gmtime() 
    seconds since the epoch    struct_time in local time  localtime() 
    struct_time in UTC seconds since the epoch            calendar.timegm() 
    struct_time in local time  seconds since the epoch    mktime() 

16.3.1. Functions

    time.asctime([t])   # ex 'Sun Jun 20 23:21:05 1993'
    time.clock()
    time.pthread_getcpuclockid(thread_id)
    time.clock_getres(clk_id)
    time.clock_gettime(clk_id) → float
    time.clock_gettime_ns(clk_id) → int
    time.clock_settime(clk_id, time: float)
    time.clock_settime_ns(clk_id, time: int)
    time.ctime([secs])

    time.get_clock_info(name)

Get information on the specified clock as a namespace object. Supported clock names and the corresponding functions to read their value are:

•'clock': time.clock()
•'monotonic': time.monotonic()
•'perf_counter': time.perf_counter()
•'process_time': time.process_time()
•'thread_time': time.thread_time()
•'time': time.time()

The result has the following attributes:
•adjustable: True if the clock can be changed automatically (e.g. by a NTP daemon) or manually by the system administrator, False otherwise
•implementation: The name of the underlying C function used to get the clock value. Refer to Clock ID Constants for possible values.
•monotonic: True if the clock cannot go backward, False otherwise
•resolution: The resolution of the clock in seconds (float)


    time.gmtime([secs])
    time.localtime([secs])
    time.mktime(t)
    time.monotonic() → float
    time.monotonic_ns() → int
    time.perf_counter() → float
    time.perf_counter_ns() → int
    time.process_time() → float
    time.process_time_ns() → int
    time.sleep(secs)
    time.strftime(format[, t])


|Directive    |Meaning      |
|  --:--      |  --:--      |
|%a |Locale’s abbreviated weekday name.|
|%A |Locale’s full weekday name.|
|%b |Locale’s abbreviated month name.|
|%B |Locale’s full month name.|
|%c |Locale’s appropriate date and time representation.|
|%d |Day of the month as a decimal number [01,31].|
|%H |Hour (24-hour clock) as a decimal number [00,23].|
|%I |Hour (12-hour clock) as a decimal number [01,12].|
|%j |Day of the year as a decimal number [001,366].|
|%m |Month as a decimal number [01,12].|
|%M |Minute as a decimal number [00,59].|
|%p |Locale’s equivalent of either AM or PM. |
|%S |Second as a decimal number [00,61]. |
|%U |Week number of the year (Sunday as the first day of the week) as a decimal number [00,53]. All days in a new year preceding the first Sunday are considered to be in week 0. |
|%w |Weekday as a decimal number [0(Sunday),6].|
|%W |Week number of the year (Monday as the first day of the week) as a decimal number [00,53]. All days in a new year preceding the first Monday are considered to be in week 0. |
|%x |Locale’s appropriate date representation.|
|%X |Locale’s appropriate time representation.|
|%y |Year without century as a decimal number [00,99].|
|%Y |Year with century as a decimal number.|
|%z |Time zone offset indicating a positive or negative time difference from UTC/GMT of the form +HHMM or -HHMM, where H represents decimal hour digits and M represents decimal minute digits [-23:59, +23:59].|
|%Z |Time zone name (no characters if no time zone exists).|
|%% |A literal '%' character.|

Notes:
1.When used with the strptime() function, the %p directive only affects the output hour field if the %I directive is used to parse the hour.
2.The range really is 0 to 61; value 60 is valid in timestamps representing leap seconds and value 61 is supported for historical reasons.
3.When used with the strptime() function, %U and %W are only used in calculations when the day of the week and the year are specified.

time.strptime(string[, format])

class time.struct_time

| Index   | Attribute   | Values |
| --:--   | --:--       | --:--  |
|0        |tm_year      |for example, 1993 |
|1        |tm_mon       |range [1, 12] |
|2        |tm_mday      |range [1, 31] |
|3        |tm_hour      |range [0, 23] |
|4        |tm_min       |range [0, 59] |
|5        |tm_sec       |range [0, 61]; see (2) in strftime() description |
|6        |tm_wday      |range [0, 6], Monday is 0 |
|7        |tm_yday      |range [1, 366] |
|8        |tm_isdst     |0, 1 or -1; see below |
|N/A      |tm_zone      |abbreviation of timezone name |
|N/A      |tm_gmtoff    |offset east of UTC in seconds |

    time.time() → float
    time.thread_time() → float
    time.thread_time_ns() → int
    time.time_ns() → int
    time.tzset()


16.3.2. Clock ID Constants

    time.CLOCK_BOOTTIME
    time.CLOCK_HIGHRES
    time.CLOCK_MONOTONIC
    time.CLOCK_MONOTONIC_RAW
    time.CLOCK_PROCESS_CPUTIME_ID
    time.CLOCK_PROF
    time.CLOCK_THREAD_CPUTIME_ID
    time.CLOCK_UPTIME
    time.CLOCK_REALTIME


16.3.3. Timezone Constants

    time.altzone
    time.daylight
    time.timezone
    time.tzname


## ==⚡ zoneinfo — IANA time zone support
◦ Using ZoneInfo
◦ Data sources
◾Configuring the data sources
◾Compile-time configuration
◾Environment configuration
◾Runtime configuration


◦ The ZoneInfo class
◾String representations
◾Pickle serialization

◦ Functions
◦ Globals
◦ Exceptions and warnings

## ==⚡ calendar — General calendar-related functions
## ==⚡ collections — Container datatypes
◦ ChainMap objects
◾ChainMap Examples and Recipes

◦ Counter objects
◦ deque objects
◾deque Recipes

◦ defaultdict objects
◾defaultdict Examples

◦ namedtuple() Factory Function for Tuples with Named Fields
◦ OrderedDict objects
◾OrderedDict Examples and Recipes

◦ UserDict objects
◦ UserList objects
◦ UserString objects

## ==⚡ collections.abc — Abstract Base Classes for Containers
◦ Collections Abstract Base Classes

## ==⚡ heapq — Heap queue algorithm
◦ Basic Examples
◦ Priority Queue Implementation Notes
◦ Theory

## ==⚡ bisect — Array bisection algorithm
◦ Searching Sorted Lists
◦ Other Examples

## ==⚡ array — Efficient arrays of numeric values
## ==⚡ weakref — Weak references
◦ Weak Reference Objects
◦ Example
◦ Finalizer Objects
◦ Comparing finalizers with __del__() methods

## ==⚡ types — Dynamic type creation and names for built-in types
◦ Dynamic Type Creation
◦ Standard Interpreter Types
◦ Additional Utility Classes and Functions
◦ Coroutine Utility Functions

Source code: Lib/types.py


This module defines utility functions to assist in dynamic creation of new types.

It also defines names for some object types that are used by the standard Python interpreter, but not exposed as builtins like int or str are.

Finally, it provides some additional type-related utility classes and functions that are not fundamental enough to be builtins.


### ===🗝 Dynamic Type Creation
➡ `types.new_class(name, bases=(), kwds=None, exec_body=None)`
Creates a class object dynamically using the appropriate metaclass.

The first three arguments are the components that make up a class definition header: the class name, the base classes (in order), the keyword arguments (such as metaclass).

The exec_body argument is a callback that is used to populate the freshly created class namespace. It should accept the class namespace as its sole argument and update the namespace directly with the class contents. If no callback is provided, it has the same effect as passing in lambda ns: None.

New in version 3.3.

➡ `types.prepare_class(name, bases=(), kwds=None)`
Calculates the appropriate metaclass and creates the class namespace.

The arguments are the components that make up a class definition header: the class name, the base classes (in order) and the keyword arguments (such as metaclass).

The return value is a 3-tuple: metaclass, namespace, kwds

metaclass is the appropriate metaclass, namespace is the prepared class namespace and kwds is an updated copy of the passed in kwds argument with any 'metaclass' entry removed. If no kwds argument is passed in, this will be an empty dict.

New in version 3.3.



Changed in version 3.6: The default value for the namespace element of the returned tuple has changed. Now an insertion-order-preserving mapping is used when the metaclass does not have a __prepare__ method.

See also:
 MetaclassesFull details of the class creation process supported by these functionsPEP 3115 - Metaclasses in Python 3000Introduced the __prepare__ namespace hooktypes.resolve_bases(bases)
Resolve MRO entries dynamically as specified by PEP 560.

This function looks for items in bases that are not instances of type, and returns a tuple where each such object that has an __mro_entries__ method is replaced with an unpacked result of calling this method. If a bases item is an instance of type, or it doesn’t have an __mro_entries__ method, then it is included in the return tuple unchanged.

New in version 3.7.


See also:
 PEP 560 - Core support for typing module and generic types
 

### ===🗝 Standard Interpreter Types

This module provides names for many of the types that are required to implement a Python interpreter. It deliberately avoids including some of the types that arise only incidentally during processing such as the listiterator type.

Typical use of these names is for isinstance() or issubclass() checks.

If you instantiate any of these types, note that signatures may vary between Python versions.

Standard names are defined for the following types:
➡ `types.FunctionTypetypes.LambdaType`
The type of user-defined functions and functions created by lambda expressions.

Raises an auditing event function.__new__ with argument code.

The audit event only occurs for direct instantiation of function objects, and is not raised for normal compilation.
➡ `types.GeneratorType`
The type of generator-iterator objects, created by generator functions.
➡ `types.CoroutineType`
The type of coroutine objects, created by async def functions.

New in version 3.5.

➡ `types.AsyncGeneratorType`
The type of asynchronous generator-iterator objects, created by asynchronous generator functions.

New in version 3.6.

✅ `class types.CodeType(**kwargs)`
The type for code objects such as returned by compile().

Raises an auditing event code.__new__ with arguments code, filename, name, argcount, posonlyargcount, kwonlyargcount, nlocals, stacksize, flags.

Note that the audited arguments may not match the names or positions required by the initializer. The audit event only occurs for direct instantiation of code objects, and is not raised for normal compilation.
↪ `replace(**kwargs)`
Return a copy of the code object with new values for the specified fields.

New in version 3.8.

➡ `types.CellType`
The type for cell objects: such objects are used as containers for a function’s free variables.

New in version 3.8.

➡ `types.MethodType`
The type of methods of user-defined class instances.
➡ `types.BuiltinFunctionTypetypes.BuiltinMethodType`
The type of built-in functions like len() or sys.exit(), and methods of built-in classes. (Here, the term “built-in” means “written in C”.)
➡ `types.WrapperDescriptorType`
The type of methods of some built-in data types and base classes such as object.__init__() or object.__lt__().

New in version 3.7.

➡ `types.MethodWrapperType`
The type of bound methods of some built-in data types and base classes. For example it is the type of object().__str__.

New in version 3.7.

➡ `types.MethodDescriptorType`
The type of methods of some built-in data types such as str.join().

New in version 3.7.

➡ `types.ClassMethodDescriptorType`
The type of unbound class methods of some built-in data types such as dict.__dict__['fromkeys'].

New in version 3.7.

✅ `class types.ModuleType(name, doc=None)`
The type of modules. The constructor takes the name of the module to be created and optionally its docstring.

Note:
 Use importlib.util.module_from_spec() to create a new module if you wish to set the various import-controlled attributes.
 
↪ `__doc__`
The docstring of the module. Defaults to None.
↪ `__loader__`
The loader which loaded the module. Defaults to None.

This attribute is to match importlib.machinery.ModuleSpec.loader as stored in the attr:__spec__ object.

Note:
 A future version of Python may stop setting this attribute by default. To guard against this potential change, preferrably read from the __spec__ attribute instead or use getattr(module, "__loader__", None) if you explicitly need to use this attribute.
 
Changed in version 3.4: Defaults to None. Previously the attribute was optional.


↪ `__name__`
The name of the module. Expected to match importlib.machinery.ModuleSpec.name.
↪ `__package__`
Which package a module belongs to. If the module is top-level (i.e. not a part of any specific package) then the attribute should be set to '', else it should be set to the name of the package (which can be __name__ if the module is a package itself). Defaults to None.

This attribute is to match importlib.machinery.ModuleSpec.parent as stored in the attr:__spec__ object.

Note:
 A future version of Python may stop setting this attribute by default. To guard against this potential change, preferrably read from the __spec__ attribute instead or use getattr(module, "__package__", None) if you explicitly need to use this attribute.
 
Changed in version 3.4: Defaults to None. Previously the attribute was optional.


↪ `__spec__`
A record of the the module’s import-system-related state. Expected to be an instance of importlib.machinery.ModuleSpec.

New in version 3.4.

✅ `class types.GenericAlias(t_origin, t_args)`
The type of parameterized generics such as list[int].

t_origin should be a non-parameterized generic class, such as list, tuple or dict. t_args should be a tuple (possibly of length 1) of types which parameterize t_origin:


>>> from types import GenericAlias

>>> list[int] == GenericAlias(list, (int,))
True
>>> dict[str, int] == GenericAlias(dict, (str, int))
True

New in version 3.9.

Changed in version 3.9.2: This type can now be subclassed.


✅ `class types.TracebackType(tb_next, tb_frame, tb_lasti, tb_lineno)`
The type of traceback objects such as found in sys.exc_info()[2].

See the language reference for details of the available attributes and operations, and guidance on creating tracebacks dynamically.
➡ `types.FrameType`
The type of `frame objects` such as found in tb.tb_frame if tb is a traceback object.

See the language reference for details of the available attributes and operations.
➡ `types.GetSetDescriptorType`
The type of objects defined in extension modules with PyGetSetDef, such as FrameType.f_locals or array.array.typecode. This type is used as descriptor for object attributes; it has the same purpose as the property type, but for classes defined in extension modules.
➡ `types.MemberDescriptorType`
The type of objects defined in extension modules with PyMemberDef, such as datetime.timedelta.days. This type is used as descriptor for simple C data members which use standard conversion functions; it has the same purpose as the property type, but for classes defined in extension modules.


CPython implementation detail: In other implementations of Python, this type may be identical to GetSetDescriptorType.
✅ `class types.MappingProxyType(mapping)`
Read-only proxy of a mapping. It provides a dynamic view on the mapping’s entries, which means that when the mapping changes, the view reflects these changes.

New in version 3.3.
Changed in version 3.9: Updated to support the new union (|) operator from PEP 584, which simply delegates to the underlying mapping.

↪ `key in proxy`
Return True if the underlying mapping has a key key, else False.
↪ `proxy[key]`
Return the item of the underlying mapping with key key. Raises a KeyError if key is not in the underlying mapping.
↪ `iter(proxy)`
Return an iterator over the keys of the underlying mapping. This is a shortcut for iter(proxy.keys()).
↪ `len(proxy)`
Return the number of items in the underlying mapping.
↪ `copy()`
Return a shallow copy of the underlying mapping.
↪ `get(key[, default])`
Return the value for key if key is in the underlying mapping, else default. If default is not given, it defaults to None, so that this method never raises a KeyError.
↪ `items()`
Return a new view of the underlying mapping’s items ((key, value) pairs).
↪ `keys()`
Return a new view of the underlying mapping’s keys.
↪ `values()`
Return a new view of the underlying mapping’s values.
↪ `reversed(proxy)`
Return a reverse iterator over the keys of the underlying mapping.

New in version 3.9.


Additional Utility Classes and Functions

✅ `class types.SimpleNamespace`
A simple object subclass that provides attribute access to its namespace, as well as a meaningful repr.

Unlike object, with SimpleNamespace you can add and remove attributes. If a SimpleNamespace object is initialized with keyword arguments, those are directly added to the underlying namespace.

The type is roughly equivalent to the following code:


```py
class SimpleNamespace:
    def __init__(self, /, **kwargs):
        self.__dict__.update(kwargs)

    def __repr__(self):
        items = (f"{k}={v!r}" for k, v in self.__dict__.items())
        return "{}({})".format(type(self).__name__, ", ".join(items))

    def __eq__(self, other):
        if isinstance(self, SimpleNamespace) and isinstance(other, SimpleNamespace):
           return self.__dict__ == other.__dict__
        return NotImplemented
```

SimpleNamespace may be useful as a replacement for class NS: pass. However, for a structured record type use namedtuple() instead.

New in version 3.3.
Changed in version 3.9: Attribute order in the repr changed from alphabetical to insertion (like dict).


➡ `types.DynamicClassAttribute(fget=None, fset=None, fdel=None, doc=None)`
Route attribute access on a class to __getattr__.

This is a descriptor, used to define attributes that act differently when accessed through an instance and through a class. Instance access remains normal, but access to an attribute through a class will be routed to the class’s __getattr__ method; this is done by raising AttributeError.

This allows one to have properties active on an instance, and have virtual attributes on the class with the same name (see enum.Enum for an example).

New in version 3.4.



### ===🗝 Coroutine Utility Functions
➡ `types.coroutine(gen_func)`
This function transforms a generator function into a coroutine function which returns a generator-based coroutine. The generator-based coroutine is still a generator iterator, but is also considered to be a coroutine object and is awaitable. However, it may not necessarily implement the __await__() method.

If gen_func is a generator function, it will be modified in-place.

If gen_func is not a generator function, it will be wrapped. If it returns an instance of collections.abc.Generator, the instance will be wrapped in an awaitable proxy object. All other types of objects will be returned as is.

New in version 3.5.


## ==⚡ copy — Shallow and deep copy operations
## ==⚡ pprint — Data pretty printer
◦ PrettyPrinter Objects
◦ Example

## ==⚡ reprlib — Alternate repr() implementation
◦ Repr Objects
◦ Subclassing Repr Objects

## ==⚡ enum — Support for enumerations
◦ Module Contents
◦ Creating an Enum
◦ Programmatic access to enumeration members and their attributes
◦ Duplicating enum members and values
◦ Ensuring unique enumeration values
◦ Using automatic values
◦ Iteration
◦ Comparisons
◦ Allowed members and attributes of enumerations
◦ Restricted Enum subclassing
◦ Pickling
◦ Functional API
◦ Derived Enumerations
◾IntEnum
◾IntFlag
◾Flag
◾Others

◦ When to use __new__() vs. __init__()
◦ Interesting examples
◾Omitting values
◾Using auto
◾Using object
◾Using a descriptive string
◾Using a custom __new__()

◾OrderedEnum
◾DuplicateFreeEnum
◾Planet
◾TimePeriod

◦ How are Enums different?
◾Enum Classes
◾Enum Members (aka instances)
◾Finer Points
◾Supported __dunder__ names
◾Supported _sunder_ names
◾_Private__names
◾Enum member type
◾Boolean value of Enum classes and members
◾Enum classes with methods
◾Combining members of Flag


## ==⚡ graphlib — Functionality to operate with graph-like structures
◦ Exceptions



# =🚩 Numeric and Mathematical Modules

- https://docs.python.org/3.9/library/numeric.html

The modules described in this chapter provide numeric and math-related functions and data types. The numbers module defines an abstract hierarchy of numeric types. The math and cmath modules contain various mathematical functions for floating-point and complex numbers. The decimal module supports exact representations of decimal numbers, using arbitrary precision arithmetic.

The following modules are documented in this chapter:

➡ `• numbers — Numeric abstract base classes`
◦ The numeric tower
◦ Notes for type implementors
◾ Adding More Numeric ABCs
◾ Implementing the arithmetic operations


➡ `• math — Mathematical functions`
◦ Number-theoretic and representation functions
◦ Power and logarithmic functions
◦ Trigonometric functions
◦ Angular conversion
◦ Hyperbolic functions
◦ Special functions
◦ Constants

➡ `• cmath — Mathematical functions for complex numbers`
◦ Conversions to and from polar coordinates
◦ Power and logarithmic functions
◦ Trigonometric functions
◦ Hyperbolic functions
◦ Classification functions
◦ Constants

➡ `• decimal — Decimal fixed point and floating point arithmetic`
◦ Quick-start Tutorial
◦ Decimal objects
◾ Logical operands

◦ Context objects
◦ Constants
◦ Rounding modes
◦ Signals
◦ Floating Point Notes
◾ Mitigating round-off error with increased precision
◾ Special values

◦ Working with threads
◦ Recipes
◦ Decimal FAQ

➡ `• fractions — Rational numbers`
➡ `• random — Generate pseudo-random numbers`
◦ Bookkeeping functions
◦ Functions for bytes
◦ Functions for integers
◦ Functions for sequences
◦ Real-valued distributions
◦ Alternative Generator
◦ Notes on Reproducibility
◦ Examples
◦ Recipes

➡ `• statistics — Mathematical statistics functions`
◦ Averages and measures of central location
◦ Measures of spread
◦ Statistics for relations between two inputs
◦ Function details
◦ Exceptions
◦ NormalDist objects
◾ NormalDist Examples and Recipes



## ==⚡ • numbers — Numeric abstract base classes

## ==⚡ • math — Mathematical functions

## ==⚡ • cmath — Mathematical functions for complex numbers

## ==⚡ • decimal — Decimal fixed point and floating point arithmetic

## ==⚡ • fractions — Rational numbers

## ==⚡ • random — Generate pseudo-random numbers

    import random

    print( random.randint(1,10) )        # 产生 1 到 10 的一个整数型随机数  
    print( random.random() )             # 产生 0 到 1 之间的随机浮点数
    print( random.uniform(1.1,5.4) )     # 产生  1.1 到 5.4 之间的随机浮点数，区间可以不是整数
    print( random.choice('tomorrow') )   # 从序列中随机选取一个元素
    print( random.randrange(1,100,2) )   # 生成从1到100的间隔为2的随机整数

    a=[1,3,5,6,7]                # 将序列a中的元素顺序打乱
    random.shuffle(a)

## ==⚡ • statistics — Mathematical statistics functions


# =🚩 Functional Programming Modules
- Python 3.10.2 Documentation » The Python Standard Library » Functional Programming Modules »

Functional Programming Modules

The modules described in this chapter provide functions and classes that support a functional programming style, and general operations on callables.

The following modules are documented in this chapter:

• itertools — Functions creating iterators for efficient looping
◦Itertool functions
◦ Itertools Recipes

• functools — Higher-order functions and operations on callable objects
◦partial Objects

• operator — Standard operators as functions
◦Mapping Operators to Functions
◦ In-place Operators

## ==⚡ itertools — Functions creating iterators for efficient looping


## ==⚡ functools — Higher-order functions and operations on callable objects

Source code: Lib/functools.py


The functools module is for higher-order functions: functions that act on or return other functions. In general, any callable object can be treated as a function for the purposes of this module.

The functools module defines the following functions:

#### @functools.cache(user_function)
Simple lightweight unbounded function cache. Sometimes called “memoize”.

Returns the same as lru_cache(maxsize=None), creating a thin wrapper around a dictionary lookup for the function arguments. Because it never needs to evict old values, this is smaller and faster than lru_cache() with a size limit.

For example:


```py
@cache
def factorial(n):
    return n * factorial(n-1) if n else 1

>>> factorial(10)      # no previously cached result, makes 11 recursive calls
3628800
>>> factorial(5)       # just looks up cached value result
120
>>> factorial(12)      # makes two new recursive calls, the other 10 are cached
479001600
```


New in version 3.9.


#### @functools.cached_property(func)
Transform a method of a class into a property whose value is computed once and then cached as a normal attribute for the life of the instance. Similar to property(), with the addition of caching. Useful for expensive computed properties of instances that are otherwise effectively immutable.

Example:


```py
class DataSet:

    def __init__(self, sequence_of_numbers):
        self._data = tuple(sequence_of_numbers)

    @cached_property
    def stdev(self):
        return statistics.stdev(self._data)
```


The mechanics of cached_property() are somewhat different from property(). A regular property blocks attribute writes unless a setter is defined. In contrast, a cached_property allows writes.

The cached_property decorator only runs on lookups and only when an attribute of the same name doesn’t exist. When it does run, the cached_property writes to the attribute with the same name. Subsequent attribute reads and writes take precedence over the cached_property method and it works like a normal attribute.

The cached value can be cleared by deleting the attribute. This allows the cached_property method to run again.

Note, this decorator interferes with the operation of PEP 412 key-sharing dictionaries. This means that instance dictionaries can take more space than usual.

Also, this decorator requires that the __dict__ attribute on each instance be a mutable mapping. This means it will not work with some types, such as metaclasses (since the __dict__ attributes on type instances are read-only proxies for the class namespace), and those that specify __slots__ without including __dict__ as one of the defined slots (as such classes don’t provide a __dict__ attribute at all).

If a mutable mapping is not available or if space-efficient key sharing is desired, an effect similar to cached_property() can be achieved by a stacking property() on top of cache():


```py
class DataSet:
    def __init__(self, sequence_of_numbers):
        self._data = sequence_of_numbers

    @property
    @cache
    def stdev(self):
        return statistics.stdev(self._data)
```


New in version 3.8.


#### functools.cmp_to_key(func)
Transform an old-style comparison function to a key function. Used with tools that accept key functions (such as sorted(), min(), max(), heapq.nlargest(), heapq.nsmallest(), itertools.groupby()). This function is primarily used as a transition tool for programs being converted from Python 2 which supported the use of comparison functions.

A comparison function is any callable that accept two arguments, compares them, and returns a negative number for less-than, zero for equality, or a positive number for greater-than. A key function is a callable that accepts one argument and returns another value to be used as the sort key.

Example:


```py
sorted(iterable, key=cmp_to_key(locale.strcoll))  # locale-aware sort order
```


For sorting examples and a brief sorting tutorial, see Sorting HOW TO.

New in version 3.2.


#### @functools.lru_cache(user_function)@functools.lru_cache(maxsize=128, typed=False)
Decorator to wrap a function with a memoizing callable that saves up to the maxsize most recent calls. It can save time when an expensive or I/O bound function is periodically called with the same arguments.

Since a dictionary is used to cache results, the positional and keyword arguments to the function must be hashable.

Distinct argument patterns may be considered to be distinct calls with separate cache entries. For example, f(a=1, b=2) and f(b=2, a=1) differ in their keyword argument order and may have two separate cache entries.

If user_function is specified, it must be a callable. This allows the lru_cache decorator to be applied directly to a user function, leaving the maxsize at its default value of 128:


```py
@lru_cache
def count_vowels(sentence):
    return sum(sentence.count(vowel) for vowel in 'AEIOUaeiou')

```

If maxsize is set to None, the LRU feature is disabled and the cache can grow without bound.

If typed is set to true, function arguments of different types will be cached separately. If typed is false, the implementation will usually regard them as equivalent calls and only cache a single result. (Some types such as str and int may be cached separately even when typed is false.)

Note, type specificity applies only to the function’s immediate arguments rather than their contents. The scalar arguments, Decimal(42) and Fraction(42) are be treated as distinct calls with distinct results. In contrast, the tuple arguments ('answer', Decimal(42)) and ('answer', Fraction(42)) are treated as equivalent.

The wrapped function is instrumented with a cache_parameters() function that returns a new dict showing the values for maxsize and typed. This is for information purposes only. Mutating the values has no effect.

To help measure the effectiveness of the cache and tune the maxsize parameter, the wrapped function is instrumented with a cache_info() function that returns a named tuple showing hits, misses, maxsize and currsize.

The decorator also provides a cache_clear() function for clearing or invalidating the cache.

The original underlying function is accessible through the __wrapped__ attribute. This is useful for introspection, for bypassing the cache, or for rewrapping the function with a different cache.

The cache keeps references to the arguments and return values until they age out of the cache or until the cache is cleared.

An LRU (least recently used) cache works best when the most recent calls are the best predictors of upcoming calls (for example, the most popular articles on a news server tend to change each day). The cache’s size limit assures that the cache does not grow without bound on long-running processes such as web servers.

In general, the LRU cache should only be used when you want to reuse previously computed values. Accordingly, it doesn’t make sense to cache functions with side-effects, functions that need to create distinct mutable objects on each call, or impure functions such as time() or random().

Example of an LRU cache for static web content:


```py
@lru_cache(maxsize=32)
def get_pep(num):
    'Retrieve text of a Python Enhancement Proposal'
    resource = 'https://www.python.org/dev/peps/pep-%04d/' % num
    try:
        with urllib.request.urlopen(resource) as s:
            return s.read()
    except urllib.error.HTTPError:
        return 'Not Found'
```

>>> for n in 8, 290, 308, 320, 8, 218, 320, 279, 289, 320, 9991:
...     pep = get_pep(n)
...     print(n, len(pep))

>>> get_pep.cache_info()
CacheInfo(hits=3, misses=8, maxsize=32, currsize=8)


Example of efficiently computing Fibonacci numbers using a cache to implement a dynamic programming technique:


```py
@lru_cache(maxsize=None)
def fib(n):
    if n < 2:
        return n
    return fib(n-1) + fib(n-2)
```

>>> [fib(n) for n in range(16)]
[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610]

>>> fib.cache_info()
CacheInfo(hits=28, misses=16, maxsize=None, currsize=16)


New in version 3.2.

Changed in version 3.3: Added the typed option.

Changed in version 3.8: Added the user_function option.

New in version 3.9: Added the function cache_parameters()


#### @functools.total_ordering
Given a class defining one or more rich comparison ordering methods, this class decorator supplies the rest. This simplifies the effort involved in specifying all of the possible rich comparison operations:

The class must define one of __lt__(), __le__(), __gt__(), or __ge__(). In addition, the class should supply an __eq__() method.

For example:


```py
@total_ordering
class Student:
    def _is_valid_operand(self, other):
        return (hasattr(other, "lastname") and
                hasattr(other, "firstname"))
    def __eq__(self, other):
        if not self._is_valid_operand(other):
            return NotImplemented
        return ((self.lastname.lower(), self.firstname.lower()) ==
                (other.lastname.lower(), other.firstname.lower()))
    def __lt__(self, other):
        if not self._is_valid_operand(other):
            return NotImplemented
        return ((self.lastname.lower(), self.firstname.lower()) <
                (other.lastname.lower(), other.firstname.lower()))

```

Note:
 While this decorator makes it easy to create well behaved totally ordered types, it does come at the cost of slower execution and more complex stack traces for the derived comparison methods. If performance benchmarking indicates this is a bottleneck for a given application, implementing all six rich comparison methods instead is likely to provide an easy speed boost.
 

Note:
 This decorator makes no attempt to override methods that have been declared in the class or its superclasses. Meaning that if a superclass defines a comparison operator, total_ordering will not implement it again, even if the original method is abstract.
 

New in version 3.2.



Changed in version 3.4: Returning NotImplemented from the underlying comparison function for unrecognised types is now supported.

#### `functools.partial(func, /, *args, **keywords)`
Return a new partial object which when called will behave like func called with the positional arguments args and keyword arguments keywords. If more arguments are supplied to the call, they are appended to args. If additional keyword arguments are supplied, they extend and override keywords. Roughly equivalent to:


```py
def partial(func, /, *args, **keywords):
    def newfunc(*fargs, **fkeywords):
        newkeywords = {**keywords, **fkeywords}
        return func(*args, *fargs, **newkeywords)
    newfunc.func = func
    newfunc.args = args
    newfunc.keywords = keywords
    return newfunc
```


The partial() is used for partial function application which “freezes” some portion of a function’s arguments and/or keywords resulting in a new object with a simplified signature. For example, partial() can be used to create a callable that behaves like the int() function where the base argument defaults to two:


>>> from functools import partial
>>> basetwo = partial(int, base=2)
>>> basetwo.__doc__ = 'Convert base 2 string to an int.'
>>> basetwo('10010')
18


#### ✅ `class functools.partialmethod(func, /, *args, **keywords)`
Return a new partialmethod descriptor which behaves like partial except that it is designed to be used as a method definition rather than being directly callable.

func must be a descriptor or a callable (objects which are both, like normal functions, are handled as descriptors).

When func is a descriptor (such as a normal Python function, classmethod(), staticmethod(), abstractmethod() or another instance of partialmethod), calls to __get__ are delegated to the underlying descriptor, and an appropriate partial object returned as the result.

When func is a non-descriptor callable, an appropriate bound method is created dynamically. This behaves like a normal Python function when used as a method: the self argument will be inserted as the first positional argument, even before the args and keywords supplied to the partialmethod constructor.

Example:


>>> class Cell:
...     def __init__(self):
...         self._alive = False
...     @property
...     def alive(self):
...         return self._alive
...     def set_state(self, state):
...         self._alive = bool(state)
...     set_alive = partialmethod(set_state, True)
...     set_dead = partialmethod(set_state, False)
...
>>> c = Cell()
>>> c.alive
False
>>> c.set_alive()
>>> c.alive
True


New in version 3.4.


#### functools.reduce(function, iterable[, initializer])
Apply function of two arguments cumulatively to the items of iterable, from left to right, so as to reduce the iterable to a single value. For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates ((((1+2)+3)+4)+5). The left argument, x, is the accumulated value and the right argument, y, is the update value from the iterable. If the optional initializer is present, it is placed before the items of the iterable in the calculation, and serves as a default when the iterable is empty. If initializer is not given and iterable contains only one item, the first item is returned.

Roughly equivalent to:


```py
def reduce(function, iterable, initializer=None):
    it = iter(iterable)
    if initializer is None:
        value = next(it)
    else:
        value = initializer
    for element in it:
        value = function(value, element)
    return value
```


See itertools.accumulate() for an iterator that yields all intermediate values.

#### @functools.singledispatch
Transform a function into a single-dispatch generic function.

To define a generic function, decorate it with the @singledispatch decorator. When defining a function using @singledispatch, note that the dispatch happens on the type of the first argument:


>>> from functools import singledispatch
>>> @singledispatch
... def fun(arg, verbose=False):
...     if verbose:
...         print("Let me just say,", end=" ")
...     print(arg)


To add overloaded implementations to the function, use the register() attribute of the generic function, which can be used as a decorator. For functions annotated with types, the decorator will infer the type of the first argument automatically:


>>> @fun.register
... def _(arg: int, verbose=False):
...     if verbose:
...         print("Strength in numbers, eh?", end=" ")
...     print(arg)
...
>>> @fun.register
... def _(arg: list, verbose=False):
...     if verbose:
...         print("Enumerate this:")
...     for i, elem in enumerate(arg):
...         print(i, elem)


For code which doesn’t use type annotations, the appropriate type argument can be passed explicitly to the decorator itself:


>>> @fun.register(complex)
... def _(arg, verbose=False):
...     if verbose:
...         print("Better than complicated.", end=" ")
...     print(arg.real, arg.imag)
...


To enable registering lambdas and pre-existing functions, the register() attribute can also be used in a functional form:


>>> def nothing(arg, verbose=False):
...     print("Nothing.")
...
>>> fun.register(type(None), nothing)


The register() attribute returns the undecorated function. This enables decorator stacking, pickling, and the creation of unit tests for each variant independently:


>>> @fun.register(float)
... @fun.register(Decimal)
... def fun_num(arg, verbose=False):
...     if verbose:
...         print("Half of your number:", end=" ")
...     print(arg / 2)
...
>>> fun_num is fun
False


When called, the generic function dispatches on the type of the first argument:


>>> fun("Hello, world.")
Hello, world.
>>> fun("test.", verbose=True)
Let me just say, test.
>>> fun(42, verbose=True)
Strength in numbers, eh? 42
>>> fun(['spam', 'spam', 'eggs', 'spam'], verbose=True)
Enumerate this:
0 spam
1 spam
2 eggs
3 spam
>>> fun(None)
Nothing.
>>> fun(1.23)
0.615


Where there is no registered implementation for a specific type, its method resolution order is used to find a more generic implementation. The original function decorated with @singledispatch is registered for the base object type, which means it is used if no better implementation is found.

If an implementation is registered to an abstract base class, virtual subclasses of the base class will be dispatched to that implementation:


>>> from collections.abc import Mapping
>>> @fun.register
... def _(arg: Mapping, verbose=False):
...     if verbose:
...         print("Keys & Values")
...     for key, value in arg.items():
...         print(key, "=>", value)
...
>>> fun({"a": "b"})
a => b


To check which implementation the generic function will choose for a given type, use the dispatch() attribute:


>>> fun.dispatch(float)
<function fun_num at 0x1035a2840>
>>> fun.dispatch(dict)    # note: default implementation
<function fun at 0x103fe0000>


To access all registered implementations, use the read-only registry attribute:


>>> fun.registry.keys()
dict_keys([<class 'NoneType'>, <class 'int'>, <class 'object'>,
          <class 'decimal.Decimal'>, <class 'list'>,
          <class 'float'>])
>>> fun.registry[float]
<function fun_num at 0x1035a2840>
>>> fun.registry[object]
<function fun at 0x103fe0000>

New in version 3.4.

Changed in version 3.7: The register() attribute now supports using type annotations.


#### ✅ `class functools.singledispatchmethod(func)`
Transform a method into a single-dispatch generic function.

To define a generic method, decorate it with the @singledispatchmethod decorator. When defining a function using @singledispatchmethod, note that the dispatch happens on the type of the first non-self or non-cls argument:


```py
class Negator:
    @singledispatchmethod
    def neg(self, arg):
        raise NotImplementedError("Cannot negate a")

    @neg.register
    def _(self, arg: int):
        return -arg

    @neg.register
    def _(self, arg: bool):
        return not arg
```


@singledispatchmethod supports nesting with other decorators such as @classmethod. Note that to allow for dispatcher.register, singledispatchmethod must be the outer most decorator. Here is the Negator class with the neg methods bound to the class, rather than an instance of the class:


```py
class Negator:
    @singledispatchmethod
    @classmethod
    def neg(cls, arg):
        raise NotImplementedError("Cannot negate a")

    @neg.register
    @classmethod
    def _(cls, arg: int):
        return -arg

    @neg.register
    @classmethod
    def _(cls, arg: bool):
        return not arg
```


The same pattern can be used for other similar decorators: @staticmethod, @abstractmethod, and others.

New in version 3.8.


#### functools.update_wrapper(wrapper, wrapped, assigned=WRAPPER_ASSIGNMENTS, updated=WRAPPER_UPDATES)
Update a wrapper function to look like the wrapped function. The optional arguments are tuples to specify which attributes of the original function are assigned directly to the matching attributes on the wrapper function and which attributes of the wrapper function are updated with the corresponding attributes from the original function. The default values for these arguments are the module level constants WRAPPER_ASSIGNMENTS (which assigns to the wrapper function’s __module__, __name__, __qualname__, __annotations__ and __doc__, the documentation string) and WRAPPER_UPDATES (which updates the wrapper function’s __dict__, i.e. the instance dictionary).

To allow access to the original function for introspection and other purposes (e.g. bypassing a caching decorator such as lru_cache()), this function automatically adds a __wrapped__ attribute to the wrapper that refers to the function being wrapped.

The main intended use for this function is in decorator functions which wrap the decorated function and return the wrapper. If the wrapper function is not updated, the metadata of the returned function will reflect the wrapper definition rather than the original function definition, which is typically less than helpful.

update_wrapper() may be used with callables other than functions. Any attributes named in assigned or updated that are missing from the object being wrapped are ignored (i.e. this function will not attempt to set them on the wrapper function). AttributeError is still raised if the wrapper function itself is missing any attributes named in updated.

New in version 3.2: Automatic addition of the __wrapped__ attribute.


New in version 3.2: Copying of the __annotations__ attribute by default.

Changed in version 3.2: Missing attributes no longer trigger an AttributeError.

Changed in version 3.4: The __wrapped__ attribute now always refers to the wrapped function, even if that function defined a __wrapped__ attribute. (see bpo-17482)

#### @functools.wraps(wrapped, assigned=WRAPPER_ASSIGNMENTS, updated=WRAPPER_UPDATES)
This is a convenience function for invoking update_wrapper() as a function decorator when defining a wrapper function. It is equivalent to partial(update_wrapper, wrapped=wrapped, assigned=assigned, updated=updated). For example:


>>> from functools import wraps
>>> def my_decorator(f):
...     @wraps(f)
...     def wrapper(*args, **kwds):
...         print('Calling decorated function')
...         return f(*args, **kwds)
...     return wrapper
...
>>> @my_decorator
... def example():
...     """Docstring"""
...     print('Called example function')
...
>>> example()
Calling decorated function
Called example function
>>> example.__name__
'example'
>>> example.__doc__
'Docstring'


Without the use of this decorator factory, the name of the example function would have been 'wrapper', and the docstring of the original example() would have been lost.


### ===🗝 partial Objects

partial objects are callable objects created by partial(). They have three read-only attributes:


➡ `partial.func`
A callable object or function. Calls to the partial object will be forwarded to func with new arguments and keywords.

➡ `partial.args`
The leftmost positional arguments that will be prepended to the positional arguments provided to a partial object call.

➡ `partial.keywords`
The keyword arguments that will be supplied when the partial object is called.

partial objects are like function objects in that they are callable, weak referencable, and can have attributes. There are some important differences. For instance, the __name__ and __doc__ attributes are not created automatically. Also, partial objects defined in classes behave like static methods and do not transform into bound methods during instance attribute look-up.

## ==⚡ operator — Standard operators as functions



# =🚩 File and Directory Access
- https://docs.python.org/3.9/library/filesys.html

- `pathlib` — Object-oriented filesystem paths
- `os.path` — Common pathname manipulations
- `fileinput` — Iterate over lines from multiple input streams
- `stat` — Interpreting stat() results
- `filecmp` — File and Directory Comparisons
- `tempfile` — Generate temporary files and directories
- `glob` — Unix style pathname pattern expansion
- `fnmatch` — Unix filename pattern matching
- `linecache` — Random access to text lines
- `shutil` — High-level file operations

File and Directory Access

The modules described in this chapter deal with disk files and directories. For example, there are modules for reading the properties of files, manipulating paths in a portable way, and creating temporary files. The full list of modules in this chapter is:

### • pathlib — Object-oriented filesystem paths
◦Basic use
◦Pure paths
◾General properties
◾Operators
◾Accessing individual parts
◾Methods and properties

◦Concrete paths
◾Methods

◦Correspondence to tools in the os module

### • os.path — Common pathname manipulations
### • fileinput — Iterate over lines from multiple input streams
### • stat — Interpreting stat() results
### • filecmp — File and Directory Comparisons
◦The dircmp class

### • tempfile — Generate temporary files and directories
◦Examples
◦Deprecated functions and variables

### • glob — Unix style pathname pattern expansion
### • fnmatch — Unix filename pattern matching
### • linecache — Random access to text lines
### • shutil — High-level file operations
◦Directory and files operations
◾Platform-dependent efficient copy operations
◾copytree example
◾rmtree example

◦Archiving operations
◾Archiving example
◾Archiving example with base_dir

◦Querying the size of the output terminal


See also:
 ➡ ` Module os`
 Operating system interfaces, including functions to work with files at a lower level than Python file objects.
 ➡ ` Module io`
 Python’s built-in I/O library, including both abstract classes and some concrete classes such as file I/O.
 ➡ ` Built-in function open()`
 The standard way to open files for reading and writing with Python.


## ==⚡ pathlib — Object-oriented filesystem paths

## ==⚡ os.path — Common pathname manipulations

↪ `os.path.abspath(path)`   返回绝对路径
↪ `os.path.basename(path)`  返回文件名
↪ `os.path.commonprefix(list)`  返回list(多个路径)中，所有path共有的最长的路径
↪ `os.path.dirname(path)`   返回文件路径
↪ `os.path.exists(path)`    如果路径 path 存在，返回 True；如果路径 path 不存在，返回 False。
↪ `os.path.lexists(path)` 路径存在则返回 True，路径损坏也返回 True
↪ `os.path.expanduser(path)`    把path中包含的"~"和"~user"转换成用户目录
↪ `os.path.expandvars(path)`    根据环境变量的值替换path中包含的"$name"和"${name}"
↪ `os.path.getatime(path)`  返回最近访问时间（浮点型秒数）
↪ `os.path.getmtime(path)`  返回最近文件修改时间
↪ `os.path.getctime(path)`  返回文件 path 创建时间
↪ `os.path.getsize(path)`   返回文件大小，如果文件不存在就返回错误
↪ `os.path.isabs(path)` 判断是否为绝对路径
↪ `os.path.isfile(path)`    判断路径是否为文件
↪ `os.path.isdir(path)` 判断路径是否为目录
↪ `os.path.islink(path)`    判断路径是否为链接
↪ `os.path.ismount(path)`   判断路径是否为挂载点
↪ `os.path.join(path1[, path2[, ...]])` 把目录和文件名合成一个路径
↪ `os.path.normcase(path)`  转换path的大小写和斜杠
↪ `os.path.normpath(path)`  规范path字符串形式
↪ `os.path.realpath(path)`  返回path的真实路径
↪ `os.path.relpath(path[, start])`  从start开始计算相对路径
↪ `os.path.samefile(path1, path2)`  判断目录或文件是否相同
↪ `os.path.sameopenfile(fp1, fp2)`  判断fp1和fp2是否指向同一文件
↪ `os.path.samestat(stat1, stat2)`  判断stat tuple stat1和stat2是否指向同一个文件
↪ `os.path.split(path)` 把路径分割成 dirname 和 basename，返回一个元组
↪ `os.path.splitdrive(path)`    一般用在 windows 下，返回驱动器名和路径组成的元组
↪ `os.path.splitext(path)`  分割路径，返回路径名和文件扩展名的元组
↪ `os.path.splitunc(path)`  把路径分割为加载点与文件
↪ `os.path.walk(path, visit, arg)`  遍历 path，进入每个目录都调用 visit 函数，visit函数必须有3个参数(arg, dirname, names)，dirname 表示当前目录的目录名，names 代表当前目录下的所有文件名，args 则为 walk 的第三个参数
↪ `os.path.supports_unicode_filenames`  设置是否支持 unicode 路径名

Source code: Lib/posixpath.py (for POSIX) and Lib/ntpath.py (for Windows NT).


This module implements some useful functions on pathnames. To read or write files see `open()`, and for accessing the filesystem see the os module. The path parameters can be passed as either strings, or bytes. Applications are encouraged to represent file names as (Unicode) character strings. Unfortunately, some file names may not be representable as strings on Unix, so applications that need to support arbitrary file names on Unix should use bytes objects to represent path names. Vice versa, using bytes objects cannot represent all file names on Windows (in the standard mbcs encoding), hence Windows applications should use string objects to access all files.

Unlike a unix shell, Python does not do any automatic path expansions. Functions such as `expanduser()` and `expandvars()` can be invoked explicitly when an application desires shell-like path expansion. (See also the glob module.)

See also:
 The `pathlib` module offers high-level path objects.
 

Note:
 All of these functions accept either only bytes or only string objects as their parameters. The result is an object of the same type, if a path or file name is returned.
 

Note:
 Since different operating systems have different path name conventions, there are several versions of this module in the standard library. The `os.path` module is always the path module suitable for the operating system Python is running on, and therefore usable for local paths. However, you can also import and use the individual modules if you want to manipulate a path that is always in one of the different formats. 

They all have the same interface:
• `posixpath` for UNIX-style paths
• `ntpath` for Windows paths

Changed in version 3.8: exists(), lexists(), isdir(), isfile(), islink(), and ismount() now return False instead of raising an exception for paths that contain characters or bytes unrepresentable at the OS level.


➡ `os.path.abspath(path)`
Return a normalized absolutized version of the pathname path. On most platforms, this is equivalent to calling the function normpath() as follows: normpath(join(os.getcwd(), path)).

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.basename(path)`
Return the base name of pathname path. This is the second element of the pair returned by passing path to the function split(). Note that the result of this function is different from the Unix basename program; where basename for '/foo/bar/' returns 'bar', the basename() function returns an empty string ('').

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.commonpath(paths)`
Return the longest common sub-path of each pathname in the sequence paths. Raise ValueError if paths contain both absolute and relative pathnames, the paths are on the different drives or if paths is empty. Unlike commonprefix(), this returns a valid path.

Availability: Unix, Windows.


New in version 3.5.

Changed in version 3.6: Accepts a sequence of path-like objects.


➡ `os.path.commonprefix(list)`
Return the longest path prefix (taken character-by-character) that is a prefix of all paths in list. If list is empty, return the empty string ('').

Note:
 This function may return invalid paths because it works a character at a time. To obtain a valid path, see commonpath().
 

>>> os.path.commonprefix(['/usr/lib', '/usr/local/lib'])
'/usr/l'
>>> os.path.commonpath(['/usr/lib', '/usr/local/lib'])
'/usr'


Changed in version 3.6: Accepts a path-like object.


➡ `os.path.dirname(path)`
Return the directory name of pathname path. This is the first element of the pair returned by passing path to the function split().

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.exists(path)`
Return True if path refers to an existing path or an open file descriptor. Returns False for broken symbolic links. On some platforms, this function may return False if permission is not granted to execute `os.stat()` on the requested file, even if the path physically exists.

Changed in version 3.3: path can now be an integer: True is returned if it is anopen file descriptor, False otherwise.


Changed in version 3.6: Accepts a path-like object.


➡ `os.path.lexists(path)`
Return True if path refers to an existing path. Returns True for broken symbolic links. Equivalent to exists() on platforms lacking os.lstat().

Changed in version 3.6: Accepts a path-like object.

➡ `os.path.expanduser(path)`
On Unix and Windows, return the argument with an initial component of ~ or `~user` replaced by that user’s home directory.

On Unix, an initial ~ is replaced by the environment variable HOME if it is set; otherwise the current user’s home directory is looked up in the password directory through the built-in module pwd. An initial `~user` is looked up directly in the password directory.

On Windows, USERPROFILE will be used if set, otherwise a combination of HOMEPATH and HOMEDRIVE will be used. An initial `~user` is handled by stripping the last directory component from the created user path derived above.

If the expansion fails or if the path does not begin with a tilde, the path is returned unchanged.

Changed in version 3.6: Accepts a path-like object.


Changed in version 3.8: No longer uses HOME on Windows.

➡ `os.path.expandvars(path)`
Return the argument with environment variables expanded. Substrings of the form $name or ${name} are replaced by the value of environment variable name. Malformed variable names and references to non-existing variables are left unchanged.

On Windows, %name% expansions are supported in addition to $name and ${name}.

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.getatime(path)`
Return the time of last access of path. The return value is a floating point number giving the number of seconds since the epoch (see the time module). Raise OSError if the file does not exist or is inaccessible.

➡ `os.path.getmtime(path)`
Return the time of last modification of path. The return value is a floating point number giving the number of seconds since the epoch (see the time module). Raise OSError if the file does not exist or is inaccessible.

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.getctime(path)`
Return the system’s ctime which, on some systems (like Unix) is the time of the last metadata change, and, on others (like Windows), is the creation time for path. The return value is a number giving the number of seconds since the epoch (see the time module). Raise OSError if the file does not exist or is inaccessible.

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.getsize(path)`
Return the size, in bytes, of path. Raise OSError if the file does not exist or is inaccessible.

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.isabs(path)`
Return True if path is an absolute pathname. On Unix, that means it begins with a slash, on Windows that it begins with a (back)slash after chopping off a potential drive letter.

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.isfile(path)`
Return True if path is an existing regular file. This follows symbolic links, so both islink() and isfile() can be true for the same path.

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.isdir(path)`
Return True if path is an existing directory. This follows symbolic links, so both islink() and isdir() can be true for the same path.

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.islink(path)`
Return True if path refers to an existing directory entry that is a symbolic link. Always False if symbolic links are not supported by the Python runtime.

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.ismount(path)`
Return True if pathname path is a mount point: a point in a file system where a different file system has been mounted. On POSIX, the function checks whether path’s parent, path/.., is on a different device than path, or whether path/.. and path point to the same i-node on the same device — this should detect mount points for all Unix and POSIX variants. It is not able to reliably detect bind mounts on the same filesystem. On Windows, a drive letter root and a share UNC are always mount points, and for any other path GetVolumePathName is called to see if it is different from the input path.


New in version 3.4: Support for detecting non-root mount points on Windows.

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.join(path, *paths)`
Join one or more path components intelligently. The return value is the concatenation of path and any members of `*paths` with exactly one directory separator following each non-empty part except the last, meaning that the result will only end in a separator if the last part is empty. If a component is an absolute path, all previous components are thrown away and joining continues from the absolute path component.

On Windows, the drive letter is not reset when an absolute path component (e.g., r'\foo') is encountered. If a component contains a drive letter, all previous components are thrown away and the drive letter is reset. Note that since there is a current directory for each drive, os.path.join("c:", "foo") represents a path relative to the current directory on drive C: (c:foo), not c:\foo.

Changed in version 3.6: Accepts a path-like object for path and paths.


➡ `os.path.normcase(path)`
Normalize the case of a pathname. On Windows, convert all characters in the pathname to lowercase, and also convert forward slashes to backward slashes. On other operating systems, return the path unchanged.

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.normpath(path)`
Normalize a pathname by collapsing redundant separators and up-level references so that A//B, A/B/, A/./B and A/foo/../B all become A/B. This string manipulation may change the meaning of a path that contains symbolic links. On Windows, it converts forward slashes to backward slashes. To normalize case, use normcase().

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.realpath(path)`
Return the canonical path of the specified filename, eliminating any symbolic links encountered in the path (if they are supported by the operating system).

Note:
 When symbolic link cycles occur, the returned path will be one member of the cycle, but no guarantee is made about which member that will be.
 

Changed in version 3.6: Accepts a path-like object.


Changed in version 3.8: Symbolic links and junctions are now resolved on Windows.


➡ `os.path.relpath(path, start=os.curdir)`
Return a relative filepath to path either from the current directory or from an optional start directory. This is a path computation: the filesystem is not accessed to confirm the existence or nature of path or start. On Windows, ValueError is raised when path and start are on different drives.

start defaults to os.curdir.

Availability: Unix, Windows.

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.samefile(path1, path2)`
Return True if both pathname arguments refer to the same file or directory. This is determined by the device number and i-node number and raises an exception if an os.stat() call on either pathname fails.

Availability: Unix, Windows.

Changed in version 3.2: Added Windows support.


Changed in version 3.4: Windows now uses the same implementation as all other platforms.


Changed in version 3.6: Accepts a path-like object.


➡ `os.path.sameopenfile(fp1, fp2)`
Return True if the file descriptors fp1 and fp2 refer to the same file.

Availability: Unix, Windows.

Changed in version 3.2: Added Windows support.


Changed in version 3.6: Accepts a path-like object.


➡ `os.path.samestat(stat1, stat2)`
Return True if the stat tuples stat1 and stat2 refer to the same file. These structures may have been returned by os.fstat(), os.lstat(), or os.stat(). This function implements the underlying comparison used by samefile() and sameopenfile().

Availability: Unix, Windows.

Changed in version 3.4: Added Windows support.


Changed in version 3.6: Accepts a path-like object.


➡ `os.path.split(path)`
Split the pathname path into a pair, (head, tail) where tail is the last pathname component and head is everything leading up to that. The tail part will never contain a slash; if path ends in a slash, tail will be empty. If there is no slash in path, head will be empty. If path is empty, both head and tail are empty. Trailing slashes are stripped from head unless it is the root (one or more slashes only). In all cases, join(head, tail) returns a path to the same location as path (but the strings may differ). Also see the functions dirname() and basename().

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.splitdrive(path)`
Split the pathname path into a pair (drive, tail) where drive is either a mount point or the empty string. On systems which do not use drive specifications, drive will always be the empty string. In all cases, drive + tail will be the same as path.

On Windows, splits a pathname into drive/UNC sharepoint and relative path.

If the path contains a drive letter, drive will contain everything up to and including the colon. e.g. splitdrive("c:/dir") returns ("c:", "/dir")

If the path contains a UNC path, drive will contain the host name and share, up to but not including the fourth separator. e.g. splitdrive("//host/computer/dir") returns ("//host/computer", "/dir")

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.splitext(path)`
Split the pathname path into a pair (root, ext) such that root + ext == path, and ext is empty or begins with a period and contains at most one period. Leading periods on the basename are ignored; splitext('.cshrc') returns ('.cshrc', '').

Changed in version 3.6: Accepts a path-like object.


➡ `os.path.supports_unicode_filenames`
True if arbitrary Unicode strings can be used as file names (within limitations imposed by the file system).


## ==⚡ fileinput — Iterate over lines from multiple input streams

## ==⚡ stat — Interpreting stat() results

## ==⚡ filecmp — File and Directory Comparisons

## ==⚡ tempfile — Generate temporary files and directories

## ==⚡ glob — Unix style pathname pattern expansion

## ==⚡ fnmatch — Unix filename pattern matching

## ==⚡ linecache — Random access to text lines

## ==⚡ shutil — High-level file operations



# =🚩 Data Persistenc

The modules described in this chapter support storing Python data in a persistent form on disk. The pickle and marshal modules can turn many Python data types into a stream of bytes and then recreate the objects from the bytes. The various DBM-related modules support a family of hash-based file formats that store a mapping of strings to other strings.

The list of modules described in this chapter is:

➡ `• pickle — Python object serialization`
◦ Relationship to other Python modules
◾ Comparison with marshal
◾ Comparison with json

◦ Data stream format
◦ Module Interface
◦ What can be pickled and unpickled?
◦ Pickling Class Instances
◾ Persistence of External Objects
◾ Dispatch Tables
◾ Handling Stateful Objects

◦ Custom Reduction for Types, Functions, and Other Objects
◦ Out-of-band Buffers
◾ Provider API
◾ Consumer API
◾ Example

◦ Restricting Globals
◦ Performance
◦ Examples

➡ `• copyreg — Register pickle support functions`
◦ Example

➡ `• shelve — Python object persistence`
◦ Restrictions
◦ Example

➡ `• marshal — Internal Python object serialization`
➡ `• dbm — Interfaces to Unix “databases”`
◦ dbm.gnu — GNU’s reinterpretation of dbm
◦ dbm.ndbm — Interface based on ndbm
◦ dbm.dumb — Portable DBM implementation

➡ `• sqlite3 — DB-API 2.0 interface for SQLite databases`
◦ Module functions and constants
◦ Connection Objects
◦ Cursor Objects
◦ Row Objects
◦ Exceptions
◦ SQLite and Python types
◾ Introduction
◾ Using adapters to store additional Python types in SQLite databases
◾ Letting your object adapt itself
◾ Registering an adapter callable

◾ Converting SQLite values to custom Python types
◾ Default adapters and converters

◦ Controlling Transactions
◦ Using sqlite3 efficiently
◾ Using shortcut methods
◾ Accessing columns by name instead of by index
◾ Using the connection as a context manager


## ==⚡ • pickle — Python object serialization

## ==⚡ • copyreg — Register pickle support functions

## ==⚡ • shelve — Python object persistence

## ==⚡ • marshal — Internal Python object serialization

## ==⚡ • dbm — Interfaces to Unix “databases”

## ==⚡ • sqlite3 — DB-API 2.0 interface for SQLite databases


# =🚩 Data Compression and Archiving


The modules described in this chapter support data compression with the zlib, gzip, bzip2 and lzma algorithms, and the creation of ZIP- and tar-format archives. See also Archiving operations provided by the shutil module.

➡ `• zlib — Compression compatible with gzip`
➡ `• gzip — Support for gzip files`
◦ Examples of usage
◦ Command Line Interface
◾ Command line options


➡ `• bz2 — Support for bzip2 compression`
◦ (De)compression of files
◦ Incremental (de)compression
◦ One-shot (de)compression
◦ Examples of usage

➡ `• lzma — Compression using the LZMA algorithm`
◦ Reading and writing compressed files
◦ Compressing and decompressing data in memory
◦ Miscellaneous
◦ Specifying custom filter chains
◦ Examples

➡ `• zipfile — Work with ZIP archives`
◦ ZipFile Objects
◦ Path Objects
◦ PyZipFile Objects
◦ ZipInfo Objects
◦ Command-Line Interface
◾ Command-line options

◦ Decompression pitfalls
◾ From file itself
◾ File System limitations
◾ Resources limitations
◾ Interruption
◾ Default behaviors of extraction


➡ `• tarfile — Read and write tar archive files`
◦ TarFile Objects
◦ TarInfo Objects
◦ Command-Line Interface
◾ Command-line options

◦ Examples
◦ Supported tar formats
◦ Unicode issues


## ==⚡ • zlib — Compression compatible with gzip

## ==⚡ • gzip — Support for gzip files

## ==⚡ • bz2 — Support for bzip2 compression

## ==⚡ • lzma — Compression using the LZMA algorithm

## ==⚡ • zipfile — Work with ZIP archives

## ==⚡ • tarfile — Read and write tar archive files


# =🚩 File Formats

The modules described in this chapter parse various miscellaneous file formats that aren’t markup languages and are not related to e-mail.

➡ `• csv — CSV File Reading and Writing`
◦ Module Contents
◦ Dialects and Formatting Parameters
◦ Reader Objects
◦ Writer Objects
◦ Examples

➡ `• configparser — Configuration file parser`
◦ Quick Start
◦ Supported Datatypes
◦ Fallback Values
◦ Supported INI File Structure
◦ Interpolation of values
◦ Mapping Protocol Access
◦ Customizing Parser Behaviour
◦ Legacy API Examples
◦ ConfigParser Objects
◦ RawConfigParser Objects
◦ Exceptions

➡ `• netrc — netrc file processing`
◦ netrc Objects

➡ `• xdrlib — Encode and decode XDR data`
◦ Packer Objects
◦ Unpacker Objects
◦ Exceptions

➡ `• plistlib — Generate and parse Apple .plist files`
◦ Examples


## ==⚡ • csv — CSV File Reading and Writing

## ==⚡ • configparser — Configuration file parser

## ==⚡ • netrc — netrc file processing

## ==⚡ • xdrlib — Encode and decode XDR data

## ==⚡ • plistlib — Generate and parse Apple .plist files


# =🚩 Cryptographic Services

The modules described in this chapter implement various algorithms of a cryptographic nature. They are available at the discretion of the installation. On Unix systems, the crypt module may also be available. Here’s an overview:

➡ `• hashlib — Secure hashes and message digests`
◦ Hash algorithms
◦ SHAKE variable length digests
◦ Key derivation
◦ BLAKE2
◾ Creating hash objects
◾ Constants
◾ Examples
◾ Simple hashing
◾ Using different digest sizes
◾ Keyed hashing
◾ Randomized hashing
◾ Personalization
◾ Tree mode

◾ Credits


➡ `• hmac — Keyed-Hashing for Message Authentication`
➡ `• secrets — Generate secure random numbers for managing secrets`
◦ Random numbers
◦ Generating tokens
◾ How many bytes should tokens use?

◦ Other functions
◦ Recipes and best practices

## ==⚡ • hashlib — Secure hashes and message digests

## ==⚡ • hmac — Keyed-Hashing for Message Authentication

## ==⚡ • secrets — Generate secure random numbers for managing secrets


# =🚩 Generic Operating System Services
- https://docs.python.org/3.9/library/allos.html

The Python Standard Library » Generic Operating System Services


The modules described in this chapter provide interfaces to operating system features that are available on (almost) all operating systems, such as files and a clock. The interfaces are generally modeled after the Unix or C interfaces, but they are available on most other systems as well. Here’s an overview:

➡ `• os — Miscellaneous operating system interfaces`
◦ File Names, Command Line Arguments, and Environment Variables
◦ Python UTF-8 Mode
◦ Process Parameters
◦ File Object Creation
◦ File Descriptor Operations
◾ Querying the size of a terminal
◾ Inheritance of File Descriptors

◦ Files and Directories
◾ Linux extended attributes

◦ Process Management
◦ Interface to the scheduler
◦ Miscellaneous System Information
◦ Random numbers

➡ `• io — Core tools for working with streams`
◦ Overview
◾ Text I/O
◾ Binary I/O
◾ Raw I/O

◦ Text Encoding
◾ Opt-in EncodingWarning

◦ High-level Module Interface
◦ Class hierarchy
◾ I/O Base Classes
◾ Raw File I/O
◾ Buffered Streams
◾ Text I/O

◦ Performance
◾ Binary I/O
◾ Text I/O
◾ Multi-threading
◾ Reentrancy


➡ `• time — Time access and conversions`
◦ Functions
◦ Clock ID Constants
◦ Timezone Constants

➡ `• argparse — Parser for command-line options, arguments and sub-commands`
◦ Example
◾ Creating a parser
◾ Adding arguments
◾ Parsing arguments

◦ ArgumentParser objects
◾ prog
◾ usage
◾ description
◾ epilog
◾ parents
◾ formatter_class
◾ prefix_chars
◾ fromfile_prefix_chars
◾ argument_default
◾ allow_abbrev
◾ conflict_handler
◾ add_help
◾ exit_on_error

◦ The add_argument() method
◾ name or flags
◾ action
◾ nargs
◾ const
◾ default
◾ type
◾ choices
◾ required
◾ help
◾ metavar
◾ dest
◾ Action classes

◦ The parse_args() method
◾ Option value syntax
◾ Invalid arguments
◾ Arguments containing -
◾ Argument abbreviations (prefix matching)
◾ Beyond sys.argv
◾ The Namespace object

◦ Other utilities
◾ Sub-commands
◾ FileType objects
◾ Argument groups
◾ Mutual exclusion
◾ Parser defaults
◾ Printing help
◾ Partial parsing
◾ Customizing file parsing
◾ Exiting methods
◾ Intermixed parsing

◦ Upgrading optparse code

➡ `• getopt — C-style parser for command line options`
➡ `• logging — Logging facility for Python`
◦ Logger Objects
◦ Logging Levels
◦ Handler Objects
◦ Formatter Objects
◦ Filter Objects
◦ LogRecord Objects
◦ LogRecord attributes
◦ LoggerAdapter Objects
◦ Thread Safety
◦ Module-Level Functions
◦ Module-Level Attributes
◦ Integration with the warnings module

➡ `• logging.config — Logging configuration`
◦ Configuration functions
◦ Security considerations
◦ Configuration dictionary schema
◾ Dictionary Schema Details
◾ Incremental Configuration
◾ Object connections
◾ User-defined objects
◾ Access to external objects
◾ Access to internal objects
◾ Import resolution and custom importers

◦ Configuration file format

➡ `• logging.handlers — Logging handlers`
◦ StreamHandler
◦ FileHandler
◦ NullHandler
◦ WatchedFileHandler
◦ BaseRotatingHandler
◦ RotatingFileHandler
◦ TimedRotatingFileHandler
◦ SocketHandler
◦ DatagramHandler
◦ SysLogHandler
◦ NTEventLogHandler
◦ SMTPHandler
◦ MemoryHandler
◦ HTTPHandler
◦ QueueHandler
◦ QueueListener

➡ `• getpass — Portable password input`
➡ `• curses — Terminal handling for character-cell displays`
◦ Functions
◦ Window Objects
◦ Constants

➡ `• curses.textpad — Text input widget for curses programs`
◦ Textbox objects

➡ `• curses.ascii — Utilities for ASCII characters`
➡ `• curses.panel — A panel stack extension for curses`
◦ Functions
◦ Panel Objects

➡ `• platform — Access to underlying platform’s identifying data`
◦ Cross Platform
◦ Java Platform
◦ Windows Platform
◦ macOS Platform
◦ Unix Platforms
◦ Linux Platforms

➡ `• errno — Standard errno system symbols`
➡ `• ctypes — A foreign function library for Python`
◦ ctypes tutorial
◾ Loading dynamic link libraries
◾ Accessing functions from loaded dlls
◾ Calling functions
◾ Fundamental data types
◾ Calling functions, continued
◾ Calling functions with your own custom data types
◾ Specifying the required argument types (function prototypes)
◾ Return types
◾ Passing pointers (or: passing parameters by reference)
◾ Structures and unions
◾ Structure/union alignment and byte order
◾ Bit fields in structures and unions
◾ Arrays
◾ Pointers
◾ Type conversions
◾ Incomplete Types
◾ Callback functions
◾ Accessing values exported from dlls
◾ Surprises
◾ Variable-sized data types

◦ ctypes reference
◾ Finding shared libraries
◾ Loading shared libraries
◾ Foreign functions
◾ Function prototypes
◾ Utility functions
◾ Data types
◾ Fundamental data types
◾ Structured data types
◾ Arrays and pointers



## ==⚡ • os — Miscellaneous operating system interfaces

Source code: Lib/os.py

◦ File Names, Command Line Arguments, and Environment Variables
◦ Process Parameters
◦ File Object Creation
◦ File Descriptor Operations
◾Querying the size of a terminal
◾Inheritance of File Descriptors

◦ Files and Directories
◾Linux extended attributes

◦ Process Management
◦ Interface to the scheduler
◦ Miscellaneous System Information
◦ Random numbers




This module provides a portable way of using operating system dependent functionality. If you just want to read or write a file see open(), if you want to manipulate paths, see the os.path module, and if you want to read all the lines in all the files on the command line see the fileinput module. For creating temporary files and directories see the tempfile module, and for high-level file and directory handling see the shutil module.

Notes on the availability of these functions:

• The design of all built-in operating system dependent modules of Python is such that as long as the same functionality is available, it uses the same interface; for example, the function os.stat(path) returns stat information about path in the same format (which happens to have originated with the POSIX interface).

• Extensions peculiar to a particular operating system are also available through the os module, but using them is of course a threat to portability.

• All functions accepting path or file names accept both bytes and string objects, and result in an object of the same type, if a path or file name is returned.

• On VxWorks, os.popen, os.fork, os.execv and os.spawn*p* are not supported.

Note:
 All functions in this module raise OSError (or subclasses thereof) in the case of invalid or inaccessible file names and paths, or other arguments that have the correct type, but are not accepted by the operating system.
 
✅ `exception os.error`
An alias for the built-in OSError exception.

➡ `os.name`
The name of the operating system dependent module imported. The following names have currently been registered: 'posix', 'nt', 'java'.

See also:
 sys.platform has a finer granularity. os.uname() gives system-dependent version information.
 
The platform module provides detailed checks for the system’s identity.

### ===🗝 ◦ File Names, Command Line Arguments, and Environment Variables

In Python, file names, command line arguments, and environment variables are represented using the string type. On some systems, decoding these strings to and from bytes is necessary before passing them to the operating system. Python uses the filesystem encoding and error handler to perform this conversion (see sys.getfilesystemencoding()).

The filesystem encoding and error handler are configured at Python startup by the PyConfig_Read() function: see filesystem_encoding and filesystem_errors members of PyConfig.


Changed in version 3.1: On some systems, conversion using the file system encoding may fail. In this case, Python uses the surrogateescape encoding error handler, which means that undecodable bytes are replaced by a Unicode character U+DCxx on decoding, and these are again translated to the original byte on encoding.

The file system encoding must guarantee to successfully decode all bytes below 128. If the file system encoding fails to provide this guarantee, API functions can raise UnicodeError.

See also the locale encoding.

### ===🗝 ◦ Python UTF-8 Mode

New in version 3.7: See PEP 540 for more details.

The Python UTF-8 Mode ignores the locale encoding and forces the usage of the UTF-8 encoding:

• Use UTF-8 as the filesystem encoding.

• sys.getfilesystemencoding() returns 'UTF-8'.

• locale.getpreferredencoding() returns 'UTF-8' (the do_setlocale argument has no effect).

• sys.stdin, sys.stdout, and sys.stderr all use UTF-8 as their text encoding, with the surrogateescape error handler being enabled for sys.stdin and sys.stdout (sys.stderr continues to use backslashreplace as it does in the default locale-aware mode)

• On Unix, os.device_encoding() returns 'UTF-8'. rather than the device encoding.


Note that the standard stream settings in UTF-8 mode can be overridden by PYTHONIOENCODING (just as they can be in the default locale-aware mode).

As a consequence of the changes in those lower level APIs, other higher level APIs also exhibit different default behaviours:

• Command line arguments, environment variables and filenames are decoded to text using the UTF-8 encoding.

• os.fsdecode() and os.fsencode() use the UTF-8 encoding.

• open(), io.open(), and codecs.open() use the UTF-8 encoding by default. However, they still use the strict error handler by default so that attempting to open a binary file in text mode is likely to raise an exception rather than producing nonsense data.

The Python UTF-8 Mode is enabled if the LC_CTYPE locale is C or POSIX at Python startup (see the PyConfig_Read() function).

It can be enabled or disabled using the -X utf8 command line option and the PYTHONUTF8 environment variable.

If the `PYTHONUTF8` environment variable is not set at all, then the interpreter defaults to using the current locale settings, unless the current locale is identified as a legacy ASCII-based locale (as described for PYTHONCOERCECLOCALE), and locale coercion is either disabled or fails. In such legacy locales, the interpreter will default to enabling UTF-8 mode unless explicitly instructed not to do so.

The Python UTF-8 Mode can only be enabled at the Python startup. Its value can be read from sys.flags.utf8_mode.

See also the UTF-8 mode on Windows and the filesystem encoding and error handler.



### ===🗝 ◦ Process Parameters

These functions and data items provide information and operate on the current process and user.

➡ `os.ctermid()`
Return the filename corresponding to the controlling terminal of the process.

Availability: Unix.

➡ `os.environ`
A mapping object where keys and values are strings that represent the process environment. For example, environ['HOME'] is the pathname of your home directory (on some platforms), and is equivalent to getenv("HOME") in C.

This mapping is captured the first time the os module is imported, typically during Python startup as part of processing site.py. Changes to the environment made after this time are not reflected in os.environ, except for changes made by modifying os.environ directly.

This mapping may be used to modify the environment as well as query the environment. putenv() will be called automatically when the mapping is modified.

On Unix, keys and values use sys.getfilesystemencoding() and 'surrogateescape' error handler. Use environb if you would like to use a different encoding.

Note:
 Calling putenv() directly does not change os.environ, so it’s better to modify os.environ.
 

Note:
 On some platforms, including FreeBSD and macOS, setting environ may cause memory leaks. Refer to the system documentation for putenv().
 

You can delete items in this mapping to unset environment variables. unsetenv() will be called automatically when an item is deleted from os.environ, and when one of the pop() or clear() methods is called.

Changed in version 3.9: Updated to support PEP 584’s merge `(|)` and update `(|=)` operators.


➡ `os.environb`
Bytes version of environ: a mapping object where both keys and values are bytes objects representing the process environment. environ and environb are synchronized (modifying environb updates environ, and vice versa).

`environb` is only available if `supports_bytes_environ` is True.

New in version 3.2.


Changed in version 3.9: Updated to support PEP 584’s merge `(|)` and update `(|=)` operators.


➡ `os.chdir(path)os.fchdir(fd)os.getcwd()`
These functions are described in Files and Directories.

➡ `os.fsencode(filename)`
Encode path-like filename to the filesystem encoding and error handler; return bytes unchanged.

fsdecode() is the reverse function.

New in version 3.2.


Changed in version 3.6: Support added to accept objects implementing the os.PathLike interface.


➡ `os.fsdecode(filename)`
Decode the path-like filename from the filesystem encoding and error handler; return str unchanged.

fsencode() is the reverse function.

New in version 3.2.


Changed in version 3.6: Support added to accept objects implementing the os.PathLike interface.


➡ `os.fspath(path)`
Return the file system representation of the path.

If str or bytes is passed in, it is returned unchanged. Otherwise __fspath__() is called and its value is returned as long as it is a str or bytes object. In all other cases, TypeError is raised.

New in version 3.6.


✅ `class os.PathLike`
An abstract base class for objects representing a file system path, e.g. pathlib.PurePath.

New in version 3.6.

➡ `abstractmethod __fspath__()`
Return the file system path representation of the object.

The method should only return a str or bytes object, with the preference being for str.

➡ `os.getenv(key, default=None)`
Return the value of the environment variable key if it exists, or default if it doesn’t. key, default and the result are str.

On Unix, keys and values are decoded with sys.getfilesystemencoding() and 'surrogateescape' error handler. Use os.getenvb() if you would like to use a different encoding.

Availability: most flavors of Unix, Windows.

➡ `os.getenvb(key, default=None)`
Return the value of the environment variable key if it exists, or default if it doesn’t. key, default and the result are bytes.

getenvb() is only available if supports_bytes_environ is True.

Availability: most flavors of Unix.

New in version 3.2.


➡ `os.get_exec_path(env=None)`
Returns the list of directories that will be searched for a named executable, similar to a shell, when launching a process. env, when specified, should be an environment variable dictionary to lookup the PATH in. By default, when env is None, environ is used.

New in version 3.2.


➡ `os.getegid()`
Return the effective group id of the current process. This corresponds to the “set id” bit on the file being executed in the current process.

Availability: Unix.

➡ `os.geteuid()`
Return the current process’s effective user id.

Availability: Unix.

➡ `os.getgid()`
Return the real group id of the current process.

Availability: Unix.

➡ `os.getgrouplist(user, group)`
Return list of group ids that user belongs to. If group is not in the list, it is included; typically, group is specified as the group ID field from the password record for user.

Availability: Unix.

New in version 3.3.


➡ `os.getgroups()`
Return list of supplemental group ids associated with the current process.

Availability: Unix.

Note:
 On macOS, getgroups() behavior differs somewhat from other Unix platforms. If the Python interpreter was built with a deployment target of 10.5 or earlier, getgroups() returns the list of effective group ids associated with the current user process; this list is limited to a system-defined number of entries, typically 16, and may be modified by calls to setgroups() if suitably privileged. If built with a deployment target greater than 10.5, getgroups() returns the current group access list for the user associated with the effective user id of the process; the group access list may change over the lifetime of the process, it is not affected by calls to setgroups(), and its length is not limited to 16. The deployment target value, MACOSX_DEPLOYMENT_TARGET, can be obtained with sysconfig.get_config_var().
 

➡ `os.getlogin()`
Return the name of the user logged in on the controlling terminal of the process. For most purposes, it is more useful to use getpass.getuser() since the latter checks the environment variables LOGNAME or USERNAME to find out who the user is, and falls back to pwd.getpwuid(os.getuid())[0] to get the login name of the current real user id.

Availability: Unix, Windows.

➡ `os.getpgid(pid)`
Return the process group id of the process with process id pid. If pid is 0, the process group id of the current process is returned.

Availability: Unix.

➡ `os.getpgrp()`
Return the id of the current process group.

Availability: Unix.

➡ `os.getpid()`
Return the current process id.

➡ `os.getppid()`
Return the parent’s process id. When the parent process has exited, on Unix the id returned is the one of the init process (1), on Windows it is still the same id, which may be already reused by another process.

Availability: Unix, Windows.

Changed in version 3.2: Added support for Windows.


➡ `os.getpriority(which, who)`
Get program scheduling priority. The value which is one of PRIO_PROCESS, PRIO_PGRP, or PRIO_USER, and who is interpreted relative to which (a process identifier for PRIO_PROCESS, process group identifier for PRIO_PGRP, and a user ID for PRIO_USER). A zero value for who denotes (respectively) the calling process, the process group of the calling process, or the real user ID of the calling process.

Availability: Unix.

New in version 3.3.


➡ `os.PRIO_PROCESSos.PRIO_PGRPos.PRIO_USER`
Parameters for the getpriority() and setpriority() functions.

Availability: Unix.

New in version 3.3.


➡ `os.getresuid()`
Return a tuple (ruid, euid, suid) denoting the current process’s real, effective, and saved user ids.

Availability: Unix.

New in version 3.2.


➡ `os.getresgid()`
Return a tuple (rgid, egid, sgid) denoting the current process’s real, effective, and saved group ids.

Availability: Unix.

New in version 3.2.


➡ `os.getuid()`
Return the current process’s real user id.

Availability: Unix.

➡ `os.initgroups(username, gid)`
Call the system initgroups() to initialize the group access list with all of the groups of which the specified username is a member, plus the specified group id.

Availability: Unix.

New in version 3.2.


➡ `os.putenv(key, value)`
Set the environment variable named key to the string value. Such changes to the environment affect subprocesses started with os.system(), popen() or fork() and execv().

Assignments to items in os.environ are automatically translated into corresponding calls to putenv(); however, calls to putenv() don’t update os.environ, so it is actually preferable to assign to items of os.environ.

Note:
 On some platforms, including FreeBSD and macOS, setting environ may cause memory leaks. Refer to the system documentation for putenv().
 

Raises an auditing event os.putenv with arguments key, value.

Changed in version 3.9: The function is now always available.


➡ `os.setegid(egid)`
Set the current process’s effective group id.

Availability: Unix.

➡ `os.seteuid(euid)`
Set the current process’s effective user id.

Availability: Unix.

➡ `os.setgid(gid)`
Set the current process’ group id.

Availability: Unix.

➡ `os.setgroups(groups)`
Set the list of supplemental group ids associated with the current process to groups. groups must be a sequence, and each element must be an integer identifying a group. This operation is typically available only to the superuser.

Availability: Unix.

Note:
 On macOS, the length of groups may not exceed the system-defined maximum number of effective group ids, typically 16. See the documentation for getgroups() for cases where it may not return the same group list set by calling setgroups().
 

➡ `os.setpgrp()`
Call the system call setpgrp() or setpgrp(0, 0) depending on which version is implemented (if any). See the Unix manual for the semantics.

Availability: Unix.

➡ `os.setpgid(pid, pgrp)`
Call the system call setpgid() to set the process group id of the process with id pid to the process group with id pgrp. See the Unix manual for the semantics.

Availability: Unix.

➡ `os.setpriority(which, who, priority)`
Set program scheduling priority. The value which is one of PRIO_PROCESS, PRIO_PGRP, or PRIO_USER, and who is interpreted relative to which (a process identifier for PRIO_PROCESS, process group identifier for PRIO_PGRP, and a user ID for PRIO_USER). A zero value for who denotes (respectively) the calling process, the process group of the calling process, or the real user ID of the calling process. priority is a value in the range -20 to 19. The default priority is 0; lower priorities cause more favorable scheduling.

Availability: Unix.

New in version 3.3.


➡ `os.setregid(rgid, egid)`
Set the current process’s real and effective group ids.

Availability: Unix.

➡ `os.setresgid(rgid, egid, sgid)`
Set the current process’s real, effective, and saved group ids.

Availability: Unix.

New in version 3.2.


➡ `os.setresuid(ruid, euid, suid)`
Set the current process’s real, effective, and saved user ids.

Availability: Unix.

New in version 3.2.


➡ `os.setreuid(ruid, euid)`
Set the current process’s real and effective user ids.

Availability: Unix.

➡ `os.getsid(pid)`
Call the system call getsid(). See the Unix manual for the semantics.

Availability: Unix.

➡ `os.setsid()`
Call the system call setsid(). See the Unix manual for the semantics.

Availability: Unix.

➡ `os.setuid(uid)`
Set the current process’s user id.

Availability: Unix.

➡ `os.strerror(code)`
Return the error message corresponding to the error code in code. On platforms where strerror() returns NULL when given an unknown error number, ValueError is raised.

➡ `os.supports_bytes_environ`
True if the native OS type of the environment is bytes (eg. False on Windows).

New in version 3.2.


➡ `os.umask(mask)`
Set the current numeric umask and return the previous umask.

➡ `os.uname()`
Returns information identifying the current operating system. The return value is an object with five attributes:

• sysname - operating system name
• nodename - name of machine on network (implementation-defined)
• release - operating system release
• version - operating system version
• machine - hardware identifier

For backwards compatibility, this object is also iterable, behaving like a five-tuple containing sysname, nodename, release, version, and machine in that order.

Some systems truncate nodename to 8 characters or to the leading component; a better way to get the hostname is socket.gethostname() or even socket.gethostbyaddr(socket.gethostname()).

Availability: recent flavors of Unix.

Changed in version 3.3: Return type changed from a tuple to a tuple-like object with named attributes.


➡ `os.unsetenv(key)`
Unset (delete) the environment variable named key. Such changes to the environment affect subprocesses started with os.system(), popen() or fork() and execv().

Deletion of items in os.environ is automatically translated into a corresponding call to unsetenv(); however, calls to unsetenv() don’t update os.environ, so it is actually preferable to delete items of os.environ.

Raises an auditing event os.unsetenv with argument key.

Changed in version 3.9: The function is now always available and is also available on Windows.



### ===🗝 ◦ File Object Creation

These functions create new file objects. (See also open() for opening file descriptors.)

➡ `os.fdopen(fd, *args, **kwargs)`
Return an open file object connected to the file descriptor fd. This is an alias of the open() built-in function and accepts the same arguments. The only difference is that the first argument of fdopen() must always be an integer.



### ===🗝 ◦ File Descriptor Operations
◾Querying the size of a terminal
◾Inheritance of File Descriptors

These functions operate on I/O streams referenced using file descriptors.

File descriptors are small integers corresponding to a file that has been opened by the current process. For example, standard input is usually file descriptor 0, standard output is 1, and standard error is 2. Further files opened by a process will then be assigned 3, 4, 5, and so forth. The name “file descriptor” is slightly deceptive; on Unix platforms, sockets and pipes are also referenced by file descriptors.

The fileno() method can be used to obtain the file descriptor associated with a file object when required. Note that using the file descriptor directly will bypass the file object methods, ignoring aspects such as internal buffering of data.

➡ `os.close(fd)`
Close file descriptor fd.

Note:
 This function is intended for low-level I/O and must be applied to a file descriptor as returned by os.open() or pipe(). To close a “file object” returned by the built-in function open() or by popen() or fdopen(), use its close() method.
 

➡ `os.closerange(fd_low, fd_high)`
Close all file descriptors from fd_low (inclusive) to fd_high (exclusive), ignoring errors. Equivalent to (but much faster than):


```py
for fd in range(fd_low, fd_high):
    try:
        os.close(fd)
    except OSError:
        pass
```


➡ `os.copy_file_range(src, dst, count, offset_src=None, offset_dst=None)`
Copy count bytes from file descriptor src, starting from offset offset_src, to file descriptor dst, starting from offset offset_dst. If offset_src is None, then src is read from the current position; respectively for offset_dst. The files pointed by src and dst must reside in the same filesystem, otherwise an OSError is raised with errno set to errno.EXDEV.

This copy is done without the additional cost of transferring data from the kernel to user space and then back into the kernel. Additionally, some filesystems could implement extra optimizations. The copy is done as if both files are opened as binary.

The return value is the amount of bytes copied. This could be less than the amount requested.

Availability: Linux kernel >= 4.5 or glibc >= 2.27.

New in version 3.8.


➡ `os.device_encoding(fd)`
Return a string describing the encoding of the device associated with fd if it is connected to a terminal; else return None.

On Unix, if the Python UTF-8 Mode is enabled, return 'UTF-8' rather than the device encoding.

Changed in version 3.10: On Unix, the function now implements the Python UTF-8 Mode.


➡ `os.dup(fd)`
Return a duplicate of file descriptor fd. The new file descriptor is non-inheritable.

On Windows, when duplicating a standard stream (0: stdin, 1: stdout, 2: stderr), the new file descriptor is inheritable.

Changed in version 3.4: The new file descriptor is now non-inheritable.


➡ `os.dup2(fd, fd2, inheritable=True)`
Duplicate file descriptor fd to fd2, closing the latter first if necessary. Return fd2. The new file descriptor is inheritable by default or non-inheritable if inheritable is False.

Changed in version 3.4: Add the optional inheritable parameter.


Changed in version 3.7: Return fd2 on success. Previously, None was always returned.


➡ `os.fchmod(fd, mode)`
Change the mode of the file given by fd to the numeric mode. See the docs for chmod() for possible values of mode. As of Python 3.3, this is equivalent to os.chmod(fd, mode).

Raises an auditing event os.chmod with arguments path, mode, dir_fd.

Availability: Unix.

➡ `os.fchown(fd, uid, gid)`
Change the owner and group id of the file given by fd to the numeric uid and gid. To leave one of the ids unchanged, set it to -1. See chown(). As of Python 3.3, this is equivalent to os.chown(fd, uid, gid).

Raises an auditing event os.chown with arguments path, uid, gid, dir_fd.

Availability: Unix.

➡ `os.fdatasync(fd)`
Force write of file with filedescriptor fd to disk. Does not force update of metadata.

Availability: Unix.

Note:
 This function is not available on MacOS.
 

➡ `os.fpathconf(fd, name)`
Return system configuration information relevant to an open file. name specifies the configuration value to retrieve; it may be a string which is the name of a defined system value; these names are specified in a number of standards (POSIX.1, Unix 95, Unix 98, and others). Some platforms define additional names as well. The names known to the host operating system are given in the pathconf_names dictionary. For configuration variables not included in that mapping, passing an integer for name is also accepted.

If name is a string and is not known, ValueError is raised. If a specific value for name is not supported by the host system, even if it is included in pathconf_names, an OSError is raised with errno.EINVAL for the error number.

As of Python 3.3, this is equivalent to os.pathconf(fd, name).

Availability: Unix.

➡ `os.fstat(fd)`
Get the status of the file descriptor fd. Return a stat_result object.

As of Python 3.3, this is equivalent to os.stat(fd).

See also:
 The stat() function.
 

➡ `os.fstatvfs(fd)`
Return information about the filesystem containing the file associated with file descriptor fd, like statvfs(). As of Python 3.3, this is equivalent to os.statvfs(fd).

Availability: Unix.

➡ `os.fsync(fd)`
Force write of file with filedescriptor fd to disk. On Unix, this calls the native fsync() function; on Windows, the MS `_commit()` function.

If you’re starting with a buffered Python file object f, first do f.flush(), and then do os.fsync(f.fileno()), to ensure that all internal buffers associated with f are written to disk.

Availability: Unix, Windows.

➡ `os.ftruncate(fd, length)`
Truncate the file corresponding to file descriptor fd, so that it is at most length bytes in size. As of Python 3.3, this is equivalent to os.truncate(fd, length).

Raises an auditing event os.truncate with arguments fd, length.

Availability: Unix, Windows.

Changed in version 3.5: Added support for Windows


➡ `os.get_blocking(fd)`
Get the blocking mode of the file descriptor: False if the O_NONBLOCK flag is set, True if the flag is cleared.

See also set_blocking() and socket.socket.setblocking().

Availability: Unix.

New in version 3.5.


➡ `os.isatty(fd)`
Return True if the file descriptor fd is open and connected to a tty(-like) device, else False.

➡ `os.lockf(fd, cmd, len)`
Apply, test or remove a POSIX lock on an open file descriptor. fd is an open file descriptor. cmd specifies the command to use - one of F_LOCK, F_TLOCK, F_ULOCK or F_TEST. len specifies the section of the file to lock.

Raises an auditing event os.lockf with arguments fd, cmd, len.

Availability: Unix.

New in version 3.3.


➡ `os.F_LOCK`
➡ `os.F_TLOCK`
➡ `os.F_ULOCK`
➡ `os.F_TEST`
Flags that specify what action lockf() will take.

Availability: Unix.

New in version 3.3.


➡ `os.lseek(fd, pos, how)`
Set the current position of file descriptor fd to position pos, modified by how: SEEK_SET or 0 to set the position relative to the beginning of the file; SEEK_CUR or 1 to set it relative to the current position; SEEK_END or 2 to set it relative to the end of the file. Return the new cursor position in bytes, starting from the beginning.

➡ `os.SEEK_SET`
➡ `os.SEEK_CUR`
➡ `os.SEEK_END`
Parameters to the lseek() function. Their values are 0, 1, and 2, respectively.

New in version 3.3: Some operating systems could support additional values, like os.SEEK_HOLE or os.SEEK_DATA.


➡ `os.open(path, flags, mode=0o777, *, dir_fd=None)`
Open the file path and set various flags according to flags and possibly its mode according to mode. When computing mode, the current umask value is first masked out. Return the file descriptor for the newly opened file. The new file descriptor is non-inheritable.

For a description of the flag and mode values, see the C run-time documentation; flag constants (like O_RDONLY and O_WRONLY) are defined in the os module. In particular, on Windows adding O_BINARY is needed to open files in binary mode.

This function can support paths relative to directory descriptors with the dir_fd parameter.

Raises an auditing event open with arguments path, mode, flags.

Changed in version 3.4: The new file descriptor is now non-inheritable.


Note:
 This function is intended for low-level I/O. For normal usage, use the built-in function open(), which returns a file object with read() and write() methods (and many more). To wrap a file descriptor in a file object, use fdopen().
 

New in version 3.3: The dir_fd argument.


Changed in version 3.5: If the system call is interrupted and the signal handler does not raise an exception, the function now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the rationale).


Changed in version 3.6: Accepts a path-like object.


The following constants are options for the flags parameter to the open() function. They can be combined using the bitwise OR operator `|`. Some of them are not available on all platforms. For descriptions of their availability and use, consult the open(2) manual page on Unix or the MSDN on Windows.

➡ `os.O_RDONLY`
➡ `os.O_WRONLY`
➡ `os.O_RDWR`
➡ `os.O_APPEND`
➡ `os.O_CREAT`
➡ `os.O_EXCL`
➡ `os.O_TRUNC`
The above constants are available on Unix and Windows.

➡ `os.O_DSYNC`
➡ `os.O_RSYNC`
➡ `os.O_SYNC`
➡ `os.O_NDELAY`
➡ `os.O_NONBLOCK`
➡ `os.O_NOCTTY`
➡ `os.O_CLOEXEC`
The above constants are only available on Unix.

Changed in version 3.3: Add O_CLOEXEC constant.


➡ `os.O_BINARY`
➡ `os.O_NOINHERIT`
➡ `os.O_SHORT_LIVED`
➡ `os.O_TEMPORARY`
➡ `os.O_RANDOM`
➡ `os.O_SEQUENTIALos.O_TEXT`
The above constants are only available on Windows.

➡ `os.O_EVTONLY`
➡ `os.O_FSYNC`
➡ `os.O_SYMLINK`
➡ `os.O_NOFOLLOW_ANY`
The above constants are only available on macOS.

Changed in version 3.10: Add O_EVTONLY, O_FSYNC, O_SYMLINK and O_NOFOLLOW_ANY constants.


➡ `os.O_ASYNC`
➡ `os.O_DIRECT`
➡ `os.O_DIRECTORY`
➡ `os.O_NOFOLLOW`
➡ `os.O_NOATIME`
➡ `os.O_PATH`
➡ `os.O_TMPFILE`
➡ `os.O_SHLOCK`
➡ `os.O_EXLOCK`
The above constants are extensions and not present if they are not defined by the C library.

Changed in version 3.4: Add O_PATH on systems that support it. Add O_TMPFILE, only available on Linux Kernel 3.11or newer.


➡ `os.openpty()`
Open a new pseudo-terminal pair. Return a pair of file descriptors (master, slave) for the pty and the tty, respectively. The new file descriptors are non-inheritable. For a (slightly) more portable approach, use the pty module.

Availability: some flavors of Unix.

Changed in version 3.4: The new file descriptors are now non-inheritable.


➡ `os.pipe()`
Create a pipe. Return a pair of file descriptors (r, w) usable for reading and writing, respectively. The new file descriptor is non-inheritable.

Availability: Unix, Windows.

Changed in version 3.4: The new file descriptors are now non-inheritable.


➡ `os.pipe2(flags)`
Create a pipe with flags set atomically. flags can be constructed by ORing together one or more of these values: O_NONBLOCK, O_CLOEXEC. Return a pair of file descriptors (r, w) usable for reading and writing, respectively.

Availability: some flavors of Unix.

New in version 3.3.


➡ `os.posix_fallocate(fd, offset, len)`
Ensures that enough disk space is allocated for the file specified by fd starting from offset and continuing for len bytes.

Availability: Unix.

New in version 3.3.


➡ `os.posix_fadvise(fd, offset, len, advice)`
Announces an intention to access data in a specific pattern thus allowing the kernel to make optimizations. The advice applies to the region of the file specified by fd starting at offset and continuing for len bytes. advice is one of POSIX_FADV_NORMAL, POSIX_FADV_SEQUENTIAL, POSIX_FADV_RANDOM, POSIX_FADV_NOREUSE, POSIX_FADV_WILLNEED or POSIX_FADV_DONTNEED.

Availability: Unix.

New in version 3.3.


➡ `os.POSIX_FADV_NORMALos.POSIX_FADV_SEQUENTIALos.POSIX_FADV_RANDOMos.POSIX_FADV_NOREUSEos.POSIX_FADV_WILLNEEDos.POSIX_FADV_DONTNEED`
Flags that can be used in advice in posix_fadvise() that specify the access pattern that is likely to be used.

Availability: Unix.

New in version 3.3.


➡ `os.pread(fd, n, offset)`
Read at most n bytes from file descriptor fd at a position of offset, leaving the file offset unchanged.

Return a bytestring containing the bytes read. If the end of the file referred to by fd has been reached, an empty bytes object is returned.

Availability: Unix.

New in version 3.3.


➡ `os.preadv(fd, buffers, offset, flags=0)`
Read from a file descriptor fd at a position of offset into mutable bytes-like objects buffers, leaving the file offset unchanged. Transfer data into each buffer until it is full and then move on to the next buffer in the sequence to hold the rest of the data.

The flags argument contains a bitwise OR of zero or more of the following flags:

• RWF_HIPRI
• RWF_NOWAIT

Return the total number of bytes actually read which can be less than the total capacity of all the objects.

The operating system may set a limit (sysconf() value 'SC_IOV_MAX') on the number of buffers that can be used.

Combine the functionality of os.readv() and os.pread().

Availability: Linux 2.6.30 and newer, FreeBSD 6.0 and newer, OpenBSD 2.7 and newer, AIX 7.1 and newer. Using flags requires Linux 4.6 or newer.

New in version 3.7.


➡ `os.RWF_NOWAIT`
Do not wait for data which is not immediately available. If this flag is specified, the system call will return instantly if it would have to read data from the backing storage or wait for a lock.

If some data was successfully read, it will return the number of bytes read. If no bytes were read, it will return -1 and set errno to errno.EAGAIN.

Availability: Linux 4.14 and newer.

New in version 3.7.


➡ `os.RWF_HIPRI`
High priority read/write. Allows block-based filesystems to use polling of the device, which provides lower latency, but may use additional resources.

Currently, on Linux, this feature is usable only on a file descriptor opened using the O_DIRECT flag.

Availability: Linux 4.6 and newer.

New in version 3.7.


➡ `os.pwrite(fd, str, offset)`
Write the bytestring in str to file descriptor fd at position of offset, leaving the file offset unchanged.

Return the number of bytes actually written.

Availability: Unix.

New in version 3.3.


➡ `os.pwritev(fd, buffers, offset, flags=0)`
Write the buffers contents to file descriptor fd at a offset offset, leaving the file offset unchanged. buffers must be a sequence of bytes-like objects. Buffers are processed in array order. Entire contents of the first buffer is written before proceeding to the second, and so on.

The flags argument contains a bitwise OR of zero or more of the following flags:

• RWF_DSYNC
• RWF_SYNC
• RWF_APPEND

Return the total number of bytes actually written.

The operating system may set a limit (sysconf() value 'SC_IOV_MAX') on the number of buffers that can be used.

Combine the functionality of os.writev() and os.pwrite().

Availability: Linux 2.6.30 and newer, FreeBSD 6.0 and newer, OpenBSD 2.7 and newer, AIX 7.1 and newer. Using flags requires Linux 4.7 or newer.

New in version 3.7.


➡ `os.RWF_DSYNC`
Provide a per-write equivalent of the O_DSYNC os.open() flag. This flag effect applies only to the data range written by the system call.

Availability: Linux 4.7 and newer.

New in version 3.7.


➡ `os.RWF_SYNC`
Provide a per-write equivalent of the O_SYNC os.open() flag. This flag effect applies only to the data range written by the system call.

Availability: Linux 4.7 and newer.

New in version 3.7.


➡ `os.RWF_APPEND`
Provide a per-write equivalent of the O_APPEND os.open() flag. This flag is meaningful only for os.pwritev(), and its effect applies only to the data range written by the system call. The offset argument does not affect the write operation; the data is always appended to the end of the file. However, if the offset argument is -1, the current file offset is updated.

Availability: Linux 4.16 and newer.

New in version 3.10.


➡ `os.read(fd, n)`
Read at most n bytes from file descriptor fd.

Return a bytestring containing the bytes read. If the end of the file referred to by fd has been reached, an empty bytes object is returned.

Note:
 This function is intended for low-level I/O and must be applied to a file descriptor as returned by os.open() or pipe(). To read a “file object” returned by the built-in function open() or by popen() or fdopen(), or sys.stdin, use its read() or readline() methods.
 

Changed in version 3.5: If the system call is interrupted and the signal handler does not raise an exception, the function now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the rationale).


➡ `os.sendfile(out_fd, in_fd, offset, count)`
➡ `os.sendfile(out_fd, in_fd, offset, count, headers=(), trailers=(), flags=0)`
Copy count bytes from file descriptor `in_fd` to file descriptor `out_fd` starting at offset. Return the number of bytes sent. When EOF is reached return 0.

The first function notation is supported by all platforms that define sendfile().

On Linux, if offset is given as None, the bytes are read from the current position of in_fd and the position of in_fd is updated.

The second case may be used on macOS and FreeBSD where headers and trailers are arbitrary sequences of buffers that are written before and after the data from in_fd is written. It returns the same as the first case.

On macOS and FreeBSD, a value of 0 for count specifies to send until the end of in_fd is reached.

All platforms support sockets as out_fd file descriptor, and some platforms allow other types (e.g. regular file, pipe) as well.

Cross-platform applications should not use headers, trailers and flags arguments.

Availability: Unix.

Note:
 For a higher-level wrapper of sendfile(), see socket.socket.sendfile().
 

New in version 3.3.


Changed in version 3.9: Parameters out and in was renamed to out_fd and in_fd.


➡ `os.set_blocking(fd, blocking)`
Set the blocking mode of the specified file descriptor. Set the O_NONBLOCK flag if blocking is False, clear the flag otherwise.

See also get_blocking() and socket.socket.setblocking().

Availability: Unix.

New in version 3.5.


➡ `os.SF_NODISKIO`
➡ `os.SF_MNOWAIT`
➡ `os.SF_SYNC`
Parameters to the sendfile() function, if the implementation supports them.

Availability: Unix.

New in version 3.3.


➡ `os.splice(src, dst, count, offset_src=None, offset_dst=None)`
Transfer count bytes from file descriptor src, starting from offset offset_src, to file descriptor dst, starting from offset offset_dst. At least one of the file descriptors must refer to a pipe. If offset_src is None, then src is read from the current position; respectively for offset_dst. The offset associated to the file descriptor that refers to a pipe must be None. The files pointed by src and dst must reside in the same filesystem, otherwise an OSError is raised with errno set to errno.EXDEV.

This copy is done without the additional cost of transferring data from the kernel to user space and then back into the kernel. Additionally, some filesystems could implement extra optimizations. The copy is done as if both files are opened as binary.

Upon successful completion, returns the number of bytes spliced to or from the pipe. A return value of 0 means end of input. If src refers to a pipe, then this means that there was no data to transfer, and it would not make sense to block because there are no writers connected to the write end of the pipe.

Availability: Linux kernel >= 2.6.17 and glibc >= 2.5

New in version 3.10.


➡ `os.SPLICE_F_MOVE`
➡ `os.SPLICE_F_NONBLOCK`
➡ `os.SPLICE_F_MORE`
New in version 3.10.


➡ `os.readv(fd, buffers)`
Read from a file descriptor fd into a number of mutable bytes-like objects buffers. Transfer data into each buffer until it is full and then move on to the next buffer in the sequence to hold the rest of the data.

Return the total number of bytes actually read which can be less than the total capacity of all the objects.

The operating system may set a limit (sysconf() value 'SC_IOV_MAX') on the number of buffers that can be used.

Availability: Unix.

New in version 3.3.


➡ `os.tcgetpgrp(fd)`
Return the process group associated with the terminal given by fd (an open file descriptor as returned by os.open()).

Availability: Unix.

➡ `os.tcsetpgrp(fd, pg)`
Set the process group associated with the terminal given by fd (an open file descriptor as returned by os.open()) to pg.

Availability: Unix.

➡ `os.ttyname(fd)`
Return a string which specifies the terminal device associated with file descriptor fd. If fd is not associated with a terminal device, an exception is raised.

Availability: Unix.

➡ `os.write(fd, str)`
Write the bytestring in str to file descriptor fd.

Return the number of bytes actually written.

Note:
 This function is intended for low-level I/O and must be applied to a file descriptor as returned by os.open() or pipe(). To write a “file object” returned by the built-in function open() or by popen() or fdopen(), or sys.stdout or sys.stderr, use its write() method.
 

Changed in version 3.5: If the system call is interrupted and the signal handler does not raise an exception, the function now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the rationale).


➡ `os.writev(fd, buffers)`
Write the contents of buffers to file descriptor fd. buffers must be a sequence of bytes-like objects. Buffers are processed in array order. Entire contents of the first buffer is written before proceeding to the second, and so on.

Returns the total number of bytes actually written.

The operating system may set a limit (sysconf() value 'SC_IOV_MAX') on the number of buffers that can be used.

Availability: Unix.

New in version 3.3.



#### ◾ Querying the size of a terminal

New in version 3.3.


➡ `os.get_terminal_size(fd=STDOUT_FILENO)`
Return the size of the terminal window as (columns, lines), tuple of type terminal_size.

The optional argument fd (default STDOUT_FILENO, or standard output) specifies which file descriptor should be queried.

If the file descriptor is not connected to a terminal, an OSError is raised.

shutil.get_terminal_size() is the high-level function which should normally be used, os.get_terminal_size is the low-level implementation.

Availability: Unix, Windows.

✅ `class os.terminal_size`
A subclass of tuple, holding (columns, lines) of the terminal window size.

➡ `columns` Width of the terminal window in characters.
➡ `lines` Height of the terminal window in characters.


#### ◾ Inheritance of File Descriptors

New in version 3.4.


A file descriptor has an “inheritable” flag which indicates if the file descriptor can be inherited by child processes. Since Python 3.4, file descriptors created by Python are non-inheritable by default.

On UNIX, non-inheritable file descriptors are closed in child processes at the execution of a new program, other file descriptors are inherited.

On Windows, non-inheritable handles and file descriptors are closed in child processes, except for standard streams (file descriptors 0, 1 and 2: stdin, stdout and stderr), which are always inherited. Using spawn* functions, all inheritable handles and all inheritable file descriptors are inherited. Using the subprocess module, all file descriptors except standard streams are closed, and inheritable handles are only inherited if the close_fds parameter is False.

➡ `os.get_inheritable(fd)`
Get the “inheritable” flag of the specified file descriptor (a boolean).

➡ `os.set_inheritable(fd, inheritable)`
Set the “inheritable” flag of the specified file descriptor.

➡ `os.get_handle_inheritable(handle)`
Get the “inheritable” flag of the specified handle (a boolean).

Availability: Windows.

➡ `os.set_handle_inheritable(handle, inheritable)`
Set the “inheritable” flag of the specified handle.

Availability: Windows.



### ===🗝 ◦ Files and Directories
◾ Linux extended attributes

16. Generic Operating System Services -  Miscellaneous operating system interfaces

- `os.access(path, mode)`              检验权限模式
- `os.chdir(path)`                     改变当前工作目录
- `os.chflags(path, flags)`            设置路径的标记为数字标记。
- `os.chmod(path, mode)`               更改权限
- `os.chown(path, uid, gid)`           更改文件所有者
- `os.chroot(path)`                    改变当前进程的根目录
- `os.close(fd)`                       关闭文件描述符 fd
- `os.closerange(fd_low, fd_high)`     关闭所有文件描述符，从 fd_low (包含) 到 fd_high (不包含), 错误会忽略
- `os.dup(fd)`                         复制文件描述符 fd
- `os.dup2(fd, fd2)`                   将一个文件描述符 fd 复制到另一个 fd2
- `os.fchdir(fd)`                      通过文件描述符改变当前工作目录
- `os.fchmod(fd, mode)`                改变一个文件的访问权限，该文件由参数fd指定，参数mode是Unix下的文件访问权限。
- `os.fchown(fd, uid, gid)`            修改一个文件的所有权，这个函数修改一个文件的用户ID和用户组ID，该文件由文件描述符fd指定。
- `os.fdatasync(fd)`                   强制将文件写入磁盘，该文件由文件描述符fd指定，但是不强制更新文件的状态信息。
- `os.fdopen(fd[, mode[, bufsize]])`   通过文件描述符 fd 创建一个文件对象，并返回这个文件对象
- `os.fpathconf(fd, name)`             返回一个打开的文件的系统配置信息。name为检索的系统配置的值，它也许是一个定义系统值的字符串，这些名字在很多标准中指定（POSIX.1, Unix 95, Unix 98, 和其它）。
- `os.fstat(fd)`                       返回文件描述符fd的状态，像stat()。
- `os.fstatvfs(fd)`                    返回包含文件描述符fd的文件的文件系统的信息，像 statvfs()
- `os.fsync(fd)`                       强制将文件描述符为fd的文件写入硬盘。
- `os.ftruncate(fd, length)`           裁剪文件描述符fd对应的文件, 所以它最大不能超过文件大小。
- `os.getcwd()`                        返回当前工作目录
- `os.getcwdu()`                       返回一个当前工作目录的Unicode对象
- `os.isatty(fd)`                      如果文件描述符fd是打开的，同时与tty(-like)设备相连，则返回true, 否则False。
- `os.lchflags(path, flags)`           设置路径的标记为数字标记，类似 chflags()，但是没有软链接
- `os.lchmod(path, mode)`              修改连接文件权限
- `os.lchown(path, uid, gid)`          更改文件所有者，类似 chown，但是不追踪链接。
- `os.link(src, dst)`                  创建硬链接，名为参数 dst，指向参数 src
- `os.listdir(path)`                   返回path指定的文件夹包含的文件或文件夹的名字的列表。
- `os.lseek(fd, pos, how)`             设置文件描述符 fd当前位置为pos, how方式修改: SEEK_SET 或者 0 设置从文件开始的计算的pos; SEEK_CUR或者 1 则从当前位置计算; os.SEEK_END 或者 2 则从文件尾部开始. 在 Unix，Windows 中有效
- `os.lstat(path)`                     像stat(),但是没有软链接
- `os.major(device)`                   从原始的设备号中提取设备major号码 (使用stat中的st_dev或者st_rdev field)。
- `os.makedev(major, minor)`           以major和minor设备号组成一个原始设备号
- `os.makedirs(path[, mode])`          递归文件夹创建函数。像mkdir(), 但创建的所有intermediate-level文件夹需要包含子文件夹。
- `os.minor(device)`                   从原始的设备号中提取设备minor号码 (使用stat中的st_dev或者st_rdev field )。
- `os.mkdir(path[, mode])`             以数字 mode 指定的文件模式创建一个名为 path 的文件夹.默认的 mode 是 0777 (八进制)。
- `os.mkfifo(path[, mode])`            创建命名管道，mode 为数字，默认为 0666 (八进制)
- `os.mknod(filename[, mode=0600, device])` 创建一个名为filename文件系统节点（文件，设备特别文件或者命名pipe）。
- `os.open(file, flags[, mode])`        打开一个文件，并且设置需要的打开选项，mode参数是可选的
- `os.openpty()`                        打开一个新的伪终端对。返回 pty 和 tty的文件描述符。
- `os.pathconf(path, name)`             返回相关文件的系统配置信息。
- `os.pipe()`                           创建一个管道. 返回一对文件描述符(r, w) 分别为读和写
- `os.popen(command[, mode[, bufsize]])` 从一个 command 打开一个管道
- `os.read(fd, n)`                      从文件描述符 fd 中读取最多 n 个字节，返回包含读取字节的字符串，文件描述符 fd对应文件已达到结尾, 返回一个空字符串。
- `os.readlink(path)`                   返回软链接所指向的文件
- `os.remove(path)`                     删除路径为path的文件。如果path 是一个文件夹，将抛出OSError; 查看下面的rmdir()删除一个 directory。
- `os.removedirs(path)`                 递归删除目录。
- `os.rename(src, dst)`                 重命名文件或目录，从 src 到 dst
- `os.renames(old, new)`                递归地对目录进行更名，也可以对文件进行更名。
- `os.rmdir(path)`                      删除path指定的空目录，如果目录非空，则抛出一个OSError异常。
- `os.stat(path)`                       获取path指定的路径的信息，功能等同于C API中的stat()系统调用。
- `os.stat_float_times([newvalue])`     决定stat_result是否以float对象显示时间戳
- `os.statvfs(path)`                    获取指定路径的文件系统统计信息
- `os.symlink(src, dst)`                创建一个软链接
- `os.tcgetpgrp(fd)`                    返回与终端fd（一个由os.open()返回的打开的文件描述符）关联的进程组
- `os.tcsetpgrp(fd, pg)`                设置与终端fd（一个由os.open()返回的打开的文件描述符）关联的进程组为pg。
- `os.tempnam([dir[, prefix]])`         返回唯一的路径名用于创建临时文件。
- `os.tmpfile()`                        返回一个打开的模式为(w+b)的文件对象 .这文件对象没有文件夹入口，没有文件描述符，将会自动删除。
- `os.tmpnam()`                         为创建一个临时文件返回一个唯一的路径
- `os.ttyname(fd)`                      返回一个字符串，它表示与文件描述符fd 关联的终端设备。如果fd 没有与终端设备关联，则引发一个异常。
- `os.unlink(path)`                     删除文件路径
- `os.utime(path, times)`               返回指定的path文件的访问和修改的时间。
- `os.walk(top[, topdown=True[, onerror=None[, followlinks=False]]])` 输出在文件夹中的文件名通过在树中游走，向上或者向下。
- `os.write(fd, str)`                   写入字符串到文件描述符 fd中. 返回实际写入的字符串长度

On some Unix platforms, many of these functions support one or more of these features:

• specifying a file descriptor: Normally the path argument provided to functions in the os module must be a string specifying a file path. However, some functions now alternatively accept an open file descriptor for their path argument. The function will then operate on the file referred to by the descriptor. (For POSIX systems, Python will call the variant of the function prefixed with f (e.g. call fchdir instead of chdir).)

You can check whether or not path can be specified as a file descriptor for a particular function on your platform using os.supports_fd. If this functionality is unavailable, using it will raise a NotImplementedError.

If the function also supports dir_fd or follow_symlinks arguments, it’s an error to specify one of those when supplying path as a file descriptor.


• paths relative to directory descriptors: If dir_fd is not None, it should be a file descriptor referring to a directory, and the path to operate on should be relative; path will then be relative to that directory. If the path is absolute, dir_fd is ignored. (For POSIX systems, Python will call the variant of the function with an at suffix and possibly prefixed with f (e.g. call faccessat instead of access).

You can check whether or not dir_fd is supported for a particular function on your platform using os.supports_dir_fd. If it’s unavailable, using it will raise a NotImplementedError.


• not following symlinks: If follow_symlinks is False, and the last element of the path to operate on is a symbolic link, the function will operate on the symbolic link itself rather than the file pointed to by the link. (For POSIX systems, Python will call the l... variant of the function.)

You can check whether or not follow_symlinks is supported for a particular function on your platform using os.supports_follow_symlinks. If it’s unavailable, using it will raise a NotImplementedError.


➡ `os.access(path, mode, *, dir_fd=None, effective_ids=False, follow_symlinks=True)`
Use the real uid/gid to test for access to path. Note that most operations will use the effective uid/gid, therefore this routine can be used in a suid/sgid environment to test if the invoking user has the specified access to path. mode should be F_OK to test the existence of path, or it can be the inclusive OR of one or more of R_OK, W_OK, and X_OK to test permissions. Return True if access is allowed, False if not. See the Unix man page access(2) for more information.

This function can support specifying paths relative to directory descriptors and not following symlinks.

If effective_ids is True, access() will perform its access checks using the effective uid/gid instead of the real uid/gid. effective_ids may not be supported on your platform; you can check whether or not it is available using os.supports_effective_ids. If it is unavailable, using it will raise a NotImplementedError.

Note:
 Using access() to check if a user is authorized to e.g. open a file before actually doing so using open() creates a security hole, because the user might exploit the short time interval between checking and opening the file to manipulate it. It’s preferable to use EAFP techniques. For example:
 

```py
if os.access("myfile", os.R_OK):
    with open("myfile") as fp:
        return fp.read()
return "some default data"
```


is better written as:


```py
try:
    fp = open("myfile")
except PermissionError:
    return "some default data"
else:
    with fp:
        return fp.read()
```


Note:
 I/O operations may fail even when access() indicates that they would succeed, particularly for operations on network filesystems which may have permissions semantics beyond the usual POSIX permission-bit model.
 

Changed in version 3.3: Added the dir_fd, effective_ids, and follow_symlinks parameters.


Changed in version 3.6: Accepts a path-like object.


➡ `os.F_OKos.R_OKos.W_OKos.X_OK`
Values to pass as the mode parameter of access() to test the existence, readability, writability and executability of path, respectively.

➡ `os.chdir(path)`
Change the current working directory to path.

This function can support specifying a file descriptor. The descriptor must refer to an opened directory, not an open file.

This function can raise OSError and subclasses such as FileNotFoundError, PermissionError, and NotADirectoryError.

Raises an auditing event os.chdir with argument path.

New in version 3.3: Added support for specifying path as a file descriptor on some platforms.


Changed in version 3.6: Accepts a path-like object.


➡ `os.chflags(path, flags, *, follow_symlinks=True)`
Set the flags of path to the numeric flags. flags may take a combination (bitwise OR) of the following values (as defined in the stat module):

• stat.UF_NODUMP
• stat.UF_IMMUTABLE
• stat.UF_APPEND
• stat.UF_OPAQUE
• stat.UF_NOUNLINK
• stat.UF_COMPRESSED
• stat.UF_HIDDEN
• stat.SF_ARCHIVED
• stat.SF_IMMUTABLE
• stat.SF_APPEND
• stat.SF_NOUNLINK
• stat.SF_SNAPSHOT

This function can support not following symlinks.

Raises an auditing event os.chflags with arguments path, flags.

Availability: Unix.

New in version 3.3: The follow_symlinks argument.


Changed in version 3.6: Accepts a path-like object.


➡ `os.chmod(path, mode, *, dir_fd=None, follow_symlinks=True)`
Change the mode of path to the numeric mode. mode may take one of the following values (as defined in the stat module) or bitwise ORed combinations of them:

• stat.S_ISUID
• stat.S_ISGID
• stat.S_ENFMT
• stat.S_ISVTX
• stat.S_IREAD
• stat.S_IWRITE
• stat.S_IEXEC
• stat.S_IRWXU
• stat.S_IRUSR
• stat.S_IWUSR
• stat.S_IXUSR
• stat.S_IRWXG
• stat.S_IRGRP
• stat.S_IWGRP
• stat.S_IXGRP
• stat.S_IRWXO
• stat.S_IROTH
• stat.S_IWOTH
• stat.S_IXOTH

This function can support specifying a file descriptor, paths relative to directory descriptors and not following symlinks.

Note:
 Although Windows supports chmod(), you can only set the file’s read-only flag with it (via the stat.S_IWRITE and stat.S_IREAD constants or a corresponding integer value). All other bits are ignored.
 

Raises an auditing event os.chmod with arguments path, mode, dir_fd.

New in version 3.3: Added support for specifying path as an open file descriptor, and the dir_fd and follow_symlinks arguments.


Changed in version 3.6: Accepts a path-like object.


➡ `os.chown(path, uid, gid, *, dir_fd=None, follow_symlinks=True)`
Change the owner and group id of path to the numeric uid and gid. To leave one of the ids unchanged, set it to -1.

This function can support specifying a file descriptor, paths relative to directory descriptors and not following symlinks.

See shutil.chown() for a higher-level function that accepts names in addition to numeric ids.

Raises an auditing event os.chown with arguments path, uid, gid, dir_fd.

Availability: Unix.

New in version 3.3: Added support for specifying path as an open file descriptor, and the dir_fd and follow_symlinks arguments.


Changed in version 3.6: Supports a path-like object.


➡ `os.chroot(path)`
Change the root directory of the current process to path.

Availability: Unix.

Changed in version 3.6: Accepts a path-like object.


➡ `os.fchdir(fd)`
Change the current working directory to the directory represented by the file descriptor fd. The descriptor must refer to an opened directory, not an open file. As of Python 3.3, this is equivalent to os.chdir(fd).

Raises an auditing event os.chdir with argument path.

Availability: Unix.

➡ `os.getcwd()`
Return a string representing the current working directory.

➡ `os.getcwdb()`
Return a bytestring representing the current working directory.

Changed in version 3.8: The function now uses the UTF-8 encoding on Windows, rather than the ANSI code page: see PEP 529 for the rationale. The function is no longer deprecated on Windows.


➡ `os.lchflags(path, flags)`
Set the flags of path to the numeric flags, like chflags(), but do not follow symbolic links. As of Python 3.3, this is equivalent to os.chflags(path, flags, follow_symlinks=False).

Raises an auditing event os.chflags with arguments path, flags.

Availability: Unix.

Changed in version 3.6: Accepts a path-like object.


➡ `os.lchmod(path, mode)`
Change the mode of path to the numeric mode. If path is a symlink, this affects the symlink rather than the target. See the docs for chmod() for possible values of mode. As of Python 3.3, this is equivalent to os.chmod(path, mode, follow_symlinks=False).

Raises an auditing event os.chmod with arguments path, mode, dir_fd.

Availability: Unix.

Changed in version 3.6: Accepts a path-like object.


➡ `os.lchown(path, uid, gid)`
Change the owner and group id of path to the numeric uid and gid. This function will not follow symbolic links. As of Python 3.3, this is equivalent to os.chown(path, uid, gid, follow_symlinks=False).

Raises an auditing event os.chown with arguments path, uid, gid, dir_fd.

Availability: Unix.

Changed in version 3.6: Accepts a path-like object.


➡ `os.link(src, dst, *, src_dir_fd=None, dst_dir_fd=None, follow_symlinks=True)`
Create a hard link pointing to src named dst.

This function can support specifying src_dir_fd and/or dst_dir_fd to supply paths relative to directory descriptors, and not following symlinks.

Raises an auditing event os.link with arguments src, dst, src_dir_fd, dst_dir_fd.

Availability: Unix, Windows.

Changed in version 3.2: Added Windows support.


New in version 3.3: Added the src_dir_fd, dst_dir_fd, and follow_symlinks arguments.


Changed in version 3.6: Accepts a path-like object for src and dst.


➡ `os.listdir(path='.')`
Return a list containing the names of the entries in the directory given by path. The list is in arbitrary order, and does not include the special entries '.' and '..' even if they are present in the directory. If a file is removed from or added to the directory during the call of this function, whether a name for that file be included is unspecified.

path may be a path-like object. If path is of type bytes (directly or indirectly through the PathLike interface), the filenames returned will also be of type bytes; in all other circumstances, they will be of type str.

This function can also support specifying a file descriptor; the file descriptor must refer to a directory.

Raises an auditing event os.listdir with argument path.

Note:
 To encode str filenames to bytes, use fsencode().
 

See also:
 The scandir() function returns directory entries along with file attribute information, giving better performance for many common use cases.
 

Changed in version 3.2: The path parameter became optional.


New in version 3.3: Added support for specifying path as an open file descriptor.


Changed in version 3.6: Accepts a path-like object.


➡ `os.lstat(path, *, dir_fd=None)`
Perform the equivalent of an lstat() system call on the given path. Similar to stat(), but does not follow symbolic links. Return a stat_result object.

On platforms that do not support symbolic links, this is an alias for stat().

As of Python 3.3, this is equivalent to os.stat(path, dir_fd=dir_fd, follow_symlinks=False).

This function can also support paths relative to directory descriptors.

See also:
 The stat() function.
 

Changed in version 3.2: Added support for Windows 6.0 (Vista) symbolic links.

Changed in version 3.3: Added the dir_fd parameter.

Changed in version 3.6: Accepts a path-like object.

Changed in version 3.8: On Windows, now opens reparse points that represent another path (name surrogates), including symbolic links and directory junctions. Other kinds of reparse points are resolved by the operating system as for stat().


➡ `os.mkdir(path, mode=0o777, *, dir_fd=None)`
Create a directory named path with numeric mode mode.

If the directory already exists, FileExistsError is raised.

On some systems, mode is ignored. Where it is used, the current umask value is first masked out. If bits other than the last 9 (i.e. the last 3 digits of the octal representation of the mode) are set, their meaning is platform-dependent. On some platforms, they are ignored and you should call chmod() explicitly to set them.

This function can also support paths relative to directory descriptors.

It is also possible to create temporary directories; see the tempfile module’s tempfile.mkdtemp() function.

Raises an auditing event os.mkdir with arguments path, mode, dir_fd.

New in version 3.3: The dir_fd argument.


Changed in version 3.6: Accepts a path-like object.


➡ `os.makedirs(name, mode=0o777, exist_ok=False)`
Recursive directory creation function. Like mkdir(), but makes all intermediate-level directories needed to contain the leaf directory.

The mode parameter is passed to mkdir() for creating the leaf directory; see the mkdir() description for how it is interpreted. To set the file permission bits of any newly-created parent directories you can set the umask before invoking makedirs(). The file permission bits of existing parent directories are not changed.

If exist_ok is False (the default), an FileExistsError is raised if the target directory already exists.

Note:
 makedirs() will become confused if the path elements to create include pardir (eg. “..” on UNIX systems).
 

This function handles UNC paths correctly.

Raises an auditing event os.mkdir with arguments path, mode, dir_fd.

New in version 3.2: The exist_ok parameter.


Changed in version 3.4.1: Before Python 3.4.1, if exist_ok was True and the directory existed, makedirs() would still raise an error if mode did not match the mode of the existing directory. Since this behavior was impossible to implement safely, it was removed in Python 3.4.1. See bpo-21082.


Changed in version 3.6: Accepts a path-like object.


Changed in version 3.7: The mode argument no longer affects the file permission bits of newly-created intermediate-level directories.


➡ `os.mkfifo(path, mode=0o666, *, dir_fd=None)`
Create a FIFO (a named pipe) named path with numeric mode mode. The current umask value is first masked out from the mode.

This function can also support paths relative to directory descriptors.

FIFOs are pipes that can be accessed like regular files. FIFOs exist until they are deleted (for example with os.unlink()). Generally, FIFOs are used as rendezvous between “client” and “server” type processes: the server opens the FIFO for reading, and the client opens it for writing. Note that mkfifo() doesn’t open the FIFO — it just creates the rendezvous point.

Availability: Unix.

New in version 3.3: The dir_fd argument.


Changed in version 3.6: Accepts a path-like object.


➡ `os.mknod(path, mode=0o600, device=0, *, dir_fd=None)`
Create a filesystem node (file, device special file or named pipe) named path. mode specifies both the permissions to use and the type of node to be created, being combined (bitwise OR) with one of stat.S_IFREG, stat.S_IFCHR, stat.S_IFBLK, and stat.S_IFIFO (those constants are available in stat). For stat.S_IFCHR and stat.S_IFBLK, device defines the newly created device special file (probably using os.makedev()), otherwise it is ignored.

This function can also support paths relative to directory descriptors.

Availability: Unix.

New in version 3.3: The dir_fd argument.


Changed in version 3.6: Accepts a path-like object.


➡ `os.major(device)`
Extract the device major number from a raw device number (usually the st_dev or st_rdev field from stat).

➡ `os.minor(device)`
Extract the device minor number from a raw device number (usually the st_dev or st_rdev field from stat).

➡ `os.makedev(major, minor)`
Compose a raw device number from the major and minor device numbers.

➡ `os.pathconf(path, name)`
Return system configuration information relevant to a named file. name specifies the configuration value to retrieve; it may be a string which is the name of a defined system value; these names are specified in a number of standards (POSIX.1, Unix 95, Unix 98, and others). Some platforms define additional names as well. The names known to the host operating system are given in the pathconf_names dictionary. For configuration variables not included in that mapping, passing an integer for name is also accepted.

If name is a string and is not known, ValueError is raised. If a specific value for name is not supported by the host system, even if it is included in pathconf_names, an OSError is raised with errno.EINVAL for the error number.

This function can support specifying a file descriptor.

Availability: Unix.

Changed in version 3.6: Accepts a path-like object.


➡ `os.pathconf_names`
Dictionary mapping names accepted by pathconf() and fpathconf() to the integer values defined for those names by the host operating system. This can be used to determine the set of names known to the system.

Availability: Unix.

➡ `os.readlink(path, *, dir_fd=None)`
Return a string representing the path to which the symbolic link points. The result may be either an absolute or relative pathname; if it is relative, it may be converted to an absolute pathname using os.path.join(os.path.dirname(path), result).

If the path is a string object (directly or indirectly through a PathLike interface), the result will also be a string object, and the call may raise a UnicodeDecodeError. If the path is a bytes object (direct or indirectly), the result will be a bytes object.

This function can also support paths relative to directory descriptors.

When trying to resolve a path that may contain links, use realpath() to properly handle recursion and platform differences.

Availability: Unix, Windows.

Changed in version 3.2: Added support for Windows 6.0 (Vista) symbolic links.


New in version 3.3: The dir_fd argument.

Changed in version 3.6: Accepts a path-like object on Unix.

Changed in version 3.8: Accepts a path-like object and a bytes object on Windows.


Changed in version 3.8: Added support for directory junctions, and changed to return the substitution path (which typically includes \\?\ prefix) rather than the optional “print name” field that was previously returned.


➡ `os.remove(path, *, dir_fd=None)`
Remove (delete) the file path. If path is a directory, an IsADirectoryError is raised. Use rmdir() to remove directories. If the file does not exist, a FileNotFoundError is raised.

This function can support paths relative to directory descriptors.

On Windows, attempting to remove a file that is in use causes an exception to be raised; on Unix, the directory entry is removed but the storage allocated to the file is not made available until the original file is no longer in use.

This function is semantically identical to unlink().

Raises an auditing event os.remove with arguments path, dir_fd.

New in version 3.3: The dir_fd argument.


Changed in version 3.6: Accepts a path-like object.


➡ `os.removedirs(name)`
Remove directories recursively. Works like rmdir() except that, if the leaf directory is successfully removed, removedirs() tries to successively remove every parent directory mentioned in path until an error is raised (which is ignored, because it generally means that a parent directory is not empty). For example, os.removedirs('foo/bar/baz') will first remove the directory 'foo/bar/baz', and then remove 'foo/bar' and 'foo' if they are empty. Raises OSError if the leaf directory could not be successfully removed.

Raises an auditing event os.remove with arguments path, dir_fd.

Changed in version 3.6: Accepts a path-like object.


➡ `os.rename(src, dst, *, src_dir_fd=None, dst_dir_fd=None)`
Rename the file or directory src to dst. If dst exists, the operation will fail with an OSError subclass in a number of cases:

On Windows, if dst exists a FileExistsError is always raised.

On Unix, if src is a file and dst is a directory or vice-versa, an IsADirectoryError or a NotADirectoryError will be raised respectively. If both are directories and dst is empty, dst will be silently replaced. If dst is a non-empty directory, an OSError is raised. If both are files, dst it will be replaced silently if the user has permission. The operation may fail on some Unix flavors if src and dst are on different filesystems. If successful, the renaming will be an atomic operation (this is a POSIX requirement).

This function can support specifying src_dir_fd and/or dst_dir_fd to supply paths relative to directory descriptors.

If you want cross-platform overwriting of the destination, use replace().

Raises an auditing event os.rename with arguments src, dst, src_dir_fd, dst_dir_fd.

New in version 3.3: The src_dir_fd and dst_dir_fd arguments.


Changed in version 3.6: Accepts a path-like object for src and dst.


➡ `os.renames(old, new)`
Recursive directory or file renaming function. Works like rename(), except creation of any intermediate directories needed to make the new pathname good is attempted first. After the rename, directories corresponding to rightmost path segments of the old name will be pruned away using removedirs().

Note:
 This function can fail with the new directory structure made if you lack permissions needed to remove the leaf directory or file.
 

Raises an auditing event os.rename with arguments src, dst, src_dir_fd, dst_dir_fd.

Changed in version 3.6: Accepts a path-like object for old and new.


➡ `os.replace(src, dst, *, src_dir_fd=None, dst_dir_fd=None)`
Rename the file or directory src to dst. If dst is a directory, OSError will be raised. If dst exists and is a file, it will be replaced silently if the user has permission. The operation may fail if src and dst are on different filesystems. If successful, the renaming will be an atomic operation (this is a POSIX requirement).

This function can support specifying src_dir_fd and/or dst_dir_fd to supply paths relative to directory descriptors.

Raises an auditing event os.rename with arguments src, dst, src_dir_fd, dst_dir_fd.

New in version 3.3.


Changed in version 3.6: Accepts a path-like object for src and dst.


➡ `os.rmdir(path, *, dir_fd=None)`
Remove (delete) the directory path. If the directory does not exist or is not empty, an FileNotFoundError or an OSError is raised respectively. In order to remove whole directory trees, shutil.rmtree() can be used.

This function can support paths relative to directory descriptors.

Raises an auditing event os.rmdir with arguments path, dir_fd.

New in version 3.3: The dir_fd parameter.


Changed in version 3.6: Accepts a path-like object.


➡ `os.scandir(path='.')`
Return an iterator of os.DirEntry objects corresponding to the entries in the directory given by path. The entries are yielded in arbitrary order, and the special entries '.' and '..' are not included. If a file is removed from or added to the directory after creating the iterator, whether an entry for that file be included is unspecified.

Using scandir() instead of listdir() can significantly increase the performance of code that also needs file type or file attribute information, because os.DirEntry objects expose this information if the operating system provides it when scanning a directory. All os.DirEntry methods may perform a system call, but is_dir() and is_file() usually only require a system call for symbolic links; os.DirEntry.stat() always requires a system call on Unix but only requires one for symbolic links on Windows.

path may be a path-like object. If path is of type bytes (directly or indirectly through the PathLike interface), the type of the name and path attributes of each os.DirEntry will be bytes; in all other circumstances, they will be of type str.

This function can also support specifying a file descriptor; the file descriptor must refer to a directory.

Raises an auditing event os.scandir with argument path.

The scandir() iterator supports the context manager protocol and has the following method:

scandir.close()
Close the iterator and free acquired resources.

This is called automatically when the iterator is exhausted or garbage collected, or when an error happens during iterating. However it is advisable to call it explicitly or use the with statement.

New in version 3.6.


The following example shows a simple use of scandir() to display all the files (excluding directories) in the given path that don’t start with '.'. The entry.is_file() call will generally not make an additional system call:


```py
with os.scandir(path) as it:
    for entry in it:
        if not entry.name.startswith('.') and entry.is_file():
            print(entry.name)
```


Note:
 On Unix-based systems, scandir() uses the system’s opendir() and readdir() functions. On Windows, it uses the Win32 FindFirstFileW and FindNextFileW functions.
 

New in version 3.5.


New in version 3.6: Added support for the context manager protocol and the close() method. If a scandir() iterator is neither exhausted nor explicitly closed a ResourceWarning will be emitted in its destructor.


The function accepts a path-like object.

Changed in version 3.7: Added support for file descriptors on Unix.


✅ `class os.DirEntry`
Object yielded by scandir() to expose the file path and other file attributes of a directory entry.

scandir() will provide as much of this information as possible without making additional system calls. When a stat() or lstat() system call is made, the os.DirEntry object will cache the result.


os.DirEntry instances are not intended to be stored in long-lived data structures; if you know the file metadata has changed or if a long time has elapsed since calling scandir(), call os.stat(entry.path) to fetch up-to-date information.

Because the os.DirEntry methods can make operating system calls, they may also raise OSError. If you need very fine-grained control over errors, you can catch OSError when calling one of the os.DirEntry methods and handle as appropriate.

To be directly usable as a path-like object, os.DirEntry implements the PathLike interface.

Attributes and methods on a os.DirEntry instance are as follows:


➡ ↪`name`
The entry’s base filename, relative to the scandir() path argument.

The name attribute will be bytes if the scandir() path argument is of type bytes and str otherwise. Use fsdecode() to decode byte filenames.

➡ ↪`path`
The entry’s full path name: equivalent to os.path.join(scandir_path, entry.name) where scandir_path is the scandir() path argument. The path is only absolute if the scandir() path argument was absolute. If the scandir() path argument was a file descriptor, the path attribute is the same as the name attribute.

The path attribute will be bytes if the scandir() path argument is of type bytes and str otherwise. Use fsdecode() to decode byte filenames.

➡ ↪`inode()`
Return the inode number of the entry.

The result is cached on the os.DirEntry object. Use os.stat(entry.path, follow_symlinks=False).st_ino to fetch up-to-date information.

On the first, uncached call, a system call is required on Windows but not on Unix.

➡ ↪`is_dir(*, follow_symlinks=True)`
Return True if this entry is a directory or a symbolic link pointing to a directory; return False if the entry is or points to any other kind of file, or if it doesn’t exist anymore.

If follow_symlinks is False, return True only if this entry is a directory (without following symlinks); return False if the entry is any other kind of file or if it doesn’t exist anymore.

The result is cached on the os.DirEntry object, with a separate cache for follow_symlinks True and False. Call os.stat() along with stat.S_ISDIR() to fetch up-to-date information.

On the first, uncached call, no system call is required in most cases. Specifically, for non-symlinks, neither Windows or Unix require a system call, except on certain Unix file systems, such as network file systems, that return dirent.d_type == DT_UNKNOWN. If the entry is a symlink, a system call will be required to follow the symlink unless follow_symlinks is False.

This method can raise OSError, such as PermissionError, but FileNotFoundError is caught and not raised.

➡ ↪`is_file(*, follow_symlinks=True)`
Return True if this entry is a file or a symbolic link pointing to a file; return False if the entry is or points to a directory or other non-file entry, or if it doesn’t exist anymore.

If follow_symlinks is False, return True only if this entry is a file (without following symlinks); return False if the entry is a directory or other non-file entry, or if it doesn’t exist anymore.

The result is cached on the os.DirEntry object. Caching, system calls made, and exceptions raised are as per is_dir().

➡ ↪`is_symlink()`
Return True if this entry is a symbolic link (even if broken); return False if the entry points to a directory or any kind of file, or if it doesn’t exist anymore.

The result is cached on the os.DirEntry object. Call os.path.islink() to fetch up-to-date information.

On the first, uncached call, no system call is required in most cases. Specifically, neither Windows or Unix require a system call, except on certain Unix file systems, such as network file systems, that return dirent.d_type == DT_UNKNOWN.

This method can raise OSError, such as PermissionError, but FileNotFoundError is caught and not raised.

➡ ↪`stat(*, follow_symlinks=True)`
Return a stat_result object for this entry. This method follows symbolic links by default; to stat a symbolic link add the follow_symlinks=False argument.

On Unix, this method always requires a system call. On Windows, it only requires a system call if follow_symlinks is True and the entry is a reparse point (for example, a symbolic link or directory junction).

On Windows, the st_ino, st_dev and st_nlink attributes of the stat_result are always set to zero. Call os.stat() to get these attributes.

The result is cached on the os.DirEntry object, with a separate cache for follow_symlinks True and False. Call os.stat() to fetch up-to-date information.

Note that there is a nice correspondence between several attributes and methods of os.DirEntry and of pathlib.Path. In particular, the name attribute has the same meaning, as do the is_dir(), is_file(), is_symlink() and stat() methods.

New in version 3.5.


Changed in version 3.6: Added support for the PathLike interface. Added support for bytes paths on Windows.


➡ `os.stat(path, *, dir_fd=None, follow_symlinks=True)`
Get the status of a file or a file descriptor. Perform the equivalent of a stat() system call on the given path. path may be specified as either a string or bytes – directly or indirectly through the PathLike interface – or as an open file descriptor. Return a stat_result object.

This function normally follows symlinks; to stat a symlink add the argument follow_symlinks=False, or use lstat().

This function can support specifying a file descriptor and not following symlinks.

On Windows, passing follow_symlinks=False will disable following all name-surrogate reparse points, which includes symlinks and directory junctions. Other types of reparse points that do not resemble links or that the operating system is unable to follow will be opened directly. When following a chain of multiple links, this may result in the original link being returned instead of the non-link that prevented full traversal. To obtain stat results for the final path in this case, use the os.path.realpath() function to resolve the path name as far as possible and call lstat() on the result. This does not apply to dangling symlinks or junction points, which will raise the usual exceptions.

Example:


>>> import os
>>> statinfo = os.stat('somefile.txt')
>>> statinfo
os.stat_result(st_mode=33188, st_ino=7876932, st_dev=234881026,
st_nlink=1, st_uid=501, st_gid=501, st_size=264, st_atime=1297230295,
st_mtime=1297230027, st_ctime=1297230027)
>>> statinfo.st_size
264


See also:
 fstat() and lstat() functions.
 

New in version 3.3: Added the dir_fd and follow_symlinks arguments, specifying a file descriptor instead of a path.


Changed in version 3.6: Accepts a path-like object.


Changed in version 3.8: On Windows, all reparse points that can be resolved by the operating system are now followed, and passing follow_symlinks=False disables following all name surrogate reparse points. If the operating system reaches a reparse point that it is not able to follow, stat now returns the information for the original path as if follow_symlinks=False had been specified instead of raising an error.


✅ `class os.stat_result`
Object whose attributes correspond roughly to the members of the stat structure. It is used for the result of os.stat(), os.fstat() and os.lstat().

Attributes:


➡ ↪`st_mode`
File mode: file type and file mode bits (permissions).

➡ ↪`st_ino`
Platform dependent, but if non-zero, uniquely identifies the file for a given value of st_dev. Typically:
• the inode number on Unix,
• the file index on Windows

➡ ↪`st_dev`
Identifier of the device on which this file resides.

➡ ↪`st_nlink`
Number of hard links.

➡ ↪`st_uid`
User identifier of the file owner.

➡ ↪`st_gid`
Group identifier of the file owner.

➡ ↪`st_size`
Size of the file in bytes, if it is a regular file or a symbolic link. The size of a symbolic link is the length of the pathname it contains, without a terminating null byte.

Timestamps:

➡ ↪`st_atime`
Time of most recent access expressed in seconds.

➡ ↪`st_mtime`
Time of most recent content modification expressed in seconds.

➡ ↪`st_ctime`
Platform dependent:
• the time of most recent metadata change on Unix,
• the time of creation on Windows, expressed in seconds.

➡ ↪`st_atime_ns`
Time of most recent access expressed in nanoseconds as an integer.

➡ ↪`st_mtime_ns`
Time of most recent content modification expressed in nanoseconds as an integer.

➡ ↪`st_ctime_ns`
Platform dependent:
• the time of most recent metadata change on Unix,
• the time of creation on Windows, expressed in nanoseconds as an integer.

Note:
 The exact meaning and resolution of the st_atime, st_mtime, and st_ctime attributes depend on the operating system and the file system. For example, on Windows systems using the FAT or FAT32 file systems, st_mtime has 2-second resolution, and st_atime has only 1-day resolution. See your operating system documentation for details.
 
Similarly, although st_atime_ns, st_mtime_ns, and st_ctime_ns are always expressed in nanoseconds, many systems do not provide nanosecond precision. On systems that do provide nanosecond precision, the floating-point object used to store st_atime, st_mtime, and st_ctime cannot preserve all of it, and as such will be slightly inexact. If you need the exact timestamps you should always use st_atime_ns, st_mtime_ns, and st_ctime_ns.

On some Unix systems (such as Linux), the following attributes may also be available:

➡ ↪`st_blocks`
Number of 512-byte blocks allocated for file. This may be smaller than st_size/512 when the file has holes.

➡ ↪`st_blksize`
“Preferred” blocksize for efficient file system I/O. Writing to a file in smaller chunks may cause an inefficient read-modify-rewrite.

➡ ↪`st_rdev`
Type of device if an inode device.

➡ ↪`st_flags`
User defined flags for file.

On other Unix systems (such as FreeBSD), the following attributes may be available (but may be only filled out if root tries to use them):

➡ ↪`st_gen`
File generation number.

➡ ↪`st_birthtime`
Time of file creation.

On Solaris and derivatives, the following attributes may also be available:

➡ ↪`st_fstype`
String that uniquely identifies the type of the filesystem that contains the file.

On macOS systems, the following attributes may also be available:

➡ ↪`st_rsize`
Real size of the file.

➡ ↪`st_creator`
Creator of the file.

➡ ↪`st_type`
File type.

On Windows systems, the following attributes are also available:

➡ ↪`st_file_attributes`
Windows file attributes: dwFileAttributes member of the BY_HANDLE_FILE_INFORMATION structure returned by GetFileInformationByHandle(). See the FILE_ATTRIBUTE_* constants in the stat module.

➡ ↪`st_reparse_tag`
When st_file_attributes has the FILE_ATTRIBUTE_REPARSE_POINT set, this field contains the tag identifying the type of reparse point. See the IO_REPARSE_TAG_* constants in the stat module.

The standard module stat defines functions and constants that are useful for extracting information from a stat structure. (On Windows, some items are filled with dummy values.)

For backward compatibility, a stat_result instance is also accessible as a tuple of at least 10 integers giving the most important (and portable) members of the stat structure, in the order st_mode, st_ino, st_dev, st_nlink, st_uid, st_gid, st_size, st_atime, st_mtime, st_ctime. More items may be added at the end by some implementations. For compatibility with older Python versions, accessing stat_result as a tuple always returns integers.

New in version 3.3: Added the st_atime_ns, st_mtime_ns, and st_ctime_ns members.


New in version 3.5: Added the st_file_attributes member on Windows.


Changed in version 3.5: Windows now returns the file index as st_ino when available.


New in version 3.7: Added the st_fstype member to Solaris/derivatives.


New in version 3.8: Added the st_reparse_tag member on Windows.


Changed in version 3.8: On Windows, the st_mode member now identifies special files as S_IFCHR, S_IFIFO or S_IFBLK as appropriate.


➡ `os.statvfs(path)`
Perform a statvfs() system call on the given path. The return value is an object whose attributes describe the filesystem on the given path, and correspond to the members of the statvfs structure, namely: f_bsize, f_frsize, f_blocks, f_bfree, f_bavail, f_files, f_ffree, f_favail, f_flag, f_namemax, f_fsid.

Two module-level constants are defined for the f_flag attribute’s bit-flags: if ST_RDONLY is set, the filesystem is mounted read-only, and if ST_NOSUID is set, the semantics of setuid/setgid bits are disabled or not supported.

Additional module-level constants are defined for GNU/glibc based systems. These are ST_NODEV (disallow access to device special files), ST_NOEXEC (disallow program execution), ST_SYNCHRONOUS (writes are synced at once), ST_MANDLOCK (allow mandatory locks on an FS), ST_WRITE (write on file/directory/symlink), ST_APPEND (append-only file), ST_IMMUTABLE (immutable file), ST_NOATIME (do not update access times), ST_NODIRATIME (do not update directory access times), ST_RELATIME (update atime relative to mtime/ctime).

This function can support specifying a file descriptor.

Availability: Unix.

Changed in version 3.2: The ST_RDONLY and ST_NOSUID constants were added.


New in version 3.3: Added support for specifying path as an open file descriptor.


Changed in version 3.4: The ST_NODEV, ST_NOEXEC, ST_SYNCHRONOUS, ST_MANDLOCK, ST_WRITE, ST_APPEND, ST_IMMUTABLE, ST_NOATIME, ST_NODIRATIME, and ST_RELATIME constants were added.


Changed in version 3.6: Accepts a path-like object.


New in version 3.7: Added f_fsid.


➡ `os.supports_dir_fd`
A set object indicating which functions in the os module accept an open file descriptor for their dir_fd parameter. Different platforms provide different features, and the underlying functionality Python uses to implement the dir_fd parameter is not available on all platforms Python supports. For consistency’s sake, functions that may support dir_fd always allow specifying the parameter, but will throw an exception if the functionality is used when it’s not locally available. (Specifying None for dir_fd is always supported on all platforms.)

To check whether a particular function accepts an open file descriptor for its dir_fd parameter, use the in operator on supports_dir_fd. As an example, this expression evaluates to True if os.stat() accepts open file descriptors for dir_fd on the local platform:



➡ `os.stat in os.supports_dir_fd`


Currently dir_fd parameters only work on Unix platforms; none of them work on Windows.

New in version 3.3.


➡ `os.supports_effective_ids`
A set object indicating whether os.access() permits specifying True for its effective_ids parameter on the local platform. (Specifying False for effective_ids is always supported on all platforms.) If the local platform supports it, the collection will contain os.access(); otherwise it will be empty.

This expression evaluates to True if os.access() supports effective_ids=True on the local platform:



➡ `os.access in os.supports_effective_ids`


Currently effective_ids is only supported on Unix platforms; it does not work on Windows.

New in version 3.3.


➡ `os.supports_fd`
A set object indicating which functions in the os module permit specifying their path parameter as an open file descriptor on the local platform. Different platforms provide different features, and the underlying functionality Python uses to accept open file descriptors as path arguments is not available on all platforms Python supports.

To determine whether a particular function permits specifying an open file descriptor for its path parameter, use the in operator on supports_fd. As an example, this expression evaluates to True if os.chdir() accepts open file descriptors for path on your local platform:



➡ `os.chdir in os.supports_fd`


New in version 3.3.


➡ `os.supports_follow_symlinks`
A set object indicating which functions in the os module accept False for their follow_symlinks parameter on the local platform. Different platforms provide different features, and the underlying functionality Python uses to implement follow_symlinks is not available on all platforms Python supports. For consistency’s sake, functions that may support follow_symlinks always allow specifying the parameter, but will throw an exception if the functionality is used when it’s not locally available. (Specifying True for follow_symlinks is always supported on all platforms.)

To check whether a particular function accepts False for its follow_symlinks parameter, use the in operator on supports_follow_symlinks. As an example, this expression evaluates to True if you may specify follow_symlinks=False when calling os.stat() on the local platform:



➡ `os.stat in os.supports_follow_symlinks`


New in version 3.3.


➡ `os.symlink(src, dst, target_is_directory=False, *, dir_fd=None)`
Create a symbolic link pointing to src named dst.

On Windows, a symlink represents either a file or a directory, and does not morph to the target dynamically. If the target is present, the type of the symlink will be created to match. Otherwise, the symlink will be created as a directory if target_is_directory is True or a file symlink (the default) otherwise. On non-Windows platforms, target_is_directory is ignored.

This function can support paths relative to directory descriptors.

Note:
 On newer versions of Windows 10, unprivileged accounts can create symlinks if Developer Mode is enabled. When Developer Mode is not available/enabled, the SeCreateSymbolicLinkPrivilege privilege is required, or the process must be run as an administrator.
 
OSError is raised when the function is called by an unprivileged user.

Raises an auditing event os.symlink with arguments src, dst, dir_fd.

Availability: Unix, Windows.

Changed in version 3.2: Added support for Windows 6.0 (Vista) symbolic links.


New in version 3.3: Added the dir_fd argument, and now allow target_is_directory on non-Windows platforms.


Changed in version 3.6: Accepts a path-like object for src and dst.


Changed in version 3.8: Added support for unelevated symlinks on Windows with Developer Mode.


➡ `os.sync()`
Force write of everything to disk.

Availability: Unix.

New in version 3.3.


➡ `os.truncate(path, length)`
Truncate the file corresponding to path, so that it is at most length bytes in size.

This function can support specifying a file descriptor.

Raises an auditing event os.truncate with arguments path, length.

Availability: Unix, Windows.

New in version 3.3.


Changed in version 3.5: Added support for Windows


Changed in version 3.6: Accepts a path-like object.


➡ `os.unlink(path, *, dir_fd=None)`
Remove (delete) the file path. This function is semantically identical to remove(); the unlink name is its traditional Unix name. Please see the documentation for remove() for further information.

Raises an auditing event os.remove with arguments path, dir_fd.

New in version 3.3: The dir_fd parameter.


Changed in version 3.6: Accepts a path-like object.


➡ `os.utime(path, times=None, *, [ns, ]dir_fd=None, follow_symlinks=True)`
Set the access and modified times of the file specified by path.

utime() takes two optional parameters, times and ns. These specify the times set on path and are used as follows:

• If `ns` is specified, it must be a 2-tuple of the form (atime_ns, mtime_ns) where each member is an int expressing nanoseconds.

• If `times` is not None, it must be a 2-tuple of the form (atime, mtime) where each member is an int or float expressing seconds.

• If `times` is None and `ns` is unspecified, this is equivalent to specifying ns=(atime_ns, mtime_ns) where both times are the current time.

It is an error to specify tuples for both times and ns.

Note that the exact times you set here may not be returned by a subsequent stat() call, depending on the resolution with which your operating system records access and modification times; see stat(). The best way to preserve exact times is to use the st_atime_ns and st_mtime_ns fields from the os.stat() result object with the ns parameter to utime.

This function can support specifying a file descriptor, paths relative to directory descriptors and not following symlinks.

Raises an auditing event os.utime with arguments path, times, ns, dir_fd.

New in version 3.3: Added support for specifying path as an open file descriptor, and the dir_fd, follow_symlinks, and ns parameters.


Changed in version 3.6: Accepts a path-like object.


➡ `os.walk(top, topdown=True, onerror=None, followlinks=False)`
Generate the file names in a directory tree by walking the tree either top-down or bottom-up. For each directory in the tree rooted at directory top (including top itself), it yields a 3-tuple (dirpath, dirnames, filenames).

dirpath is a string, the path to the directory. dirnames is a list of the names of the subdirectories in dirpath (excluding '.' and '..'). filenames is a list of the names of the non-directory files in dirpath. Note that the names in the lists contain no path components. To get a full path (which begins with top) to a file or directory in dirpath, do os.path.join(dirpath, name). Whether or not the lists are sorted depends on the file system. If a file is removed from or added to the dirpath directory during generating the lists, whether a name for that file be included is unspecified.

If optional argument topdown is True or not specified, the triple for a directory is generated before the triples for any of its subdirectories (directories are generated top-down). If topdown is False, the triple for a directory is generated after the triples for all of its subdirectories (directories are generated bottom-up). No matter the value of topdown, the list of subdirectories is retrieved before the tuples for the directory and its subdirectories are generated.

When topdown is True, the caller can modify the dirnames list in-place (perhaps using del or slice assignment), and walk() will only recurse into the subdirectories whose names remain in dirnames; this can be used to prune the search, impose a specific order of visiting, or even to inform walk() about directories the caller creates or renames before it resumes walk() again. Modifying dirnames when topdown is False has no effect on the behavior of the walk, because in bottom-up mode the directories in dirnames are generated before dirpath itself is generated.

By default, errors from the scandir() call are ignored. If optional argument onerror is specified, it should be a function; it will be called with one argument, an OSError instance. It can report the error to continue with the walk, or raise the exception to abort the walk. Note that the filename is available as the filename attribute of the exception object.

By default, walk() will not walk down into symbolic links that resolve to directories. Set followlinks to True to visit directories pointed to by symlinks, on systems that support them.

Note:
 Be aware that setting followlinks to True can lead to infinite recursion if a link points to a parent directory of itself. walk() does not keep track of the directories it visited already.
 

Note:
 If you pass a relative pathname, don’t change the current working directory between resumptions of walk(). walk() never changes the current directory, and assumes that its caller doesn’t either.
 

This example displays the number of bytes taken by non-directory files in each directory under the starting directory, except that it doesn’t look under any CVS subdirectory:


```py
import os
from os.path import join, getsize
for root, dirs, files in os.walk('python/Lib/email'):
    print(root, "consumes", end=" ")
    print(sum(getsize(join(root, name)) for name in files), end=" ")
    print("bytes in", len(files), "non-directory files")
    if 'CVS' in dirs:
        dirs.remove('CVS')  # don't visit CVS directories
```


In the next example (simple implementation of shutil.rmtree()), walking the tree bottom-up is essential, rmdir() doesn’t allow deleting a directory before the directory is empty:


```py
# Delete everything reachable from the directory named in "top",
# assuming there are no symbolic links.
# CAUTION:  This is dangerous!  For example, if top == '/', it
# could delete all your disk files.
import os
for root, dirs, files in os.walk(top, topdown=False):
    for name in files:
        os.remove(os.path.join(root, name))
    for name in dirs:
        os.rmdir(os.path.join(root, name))
```


Raises an auditing event os.walk with arguments top, topdown, onerror, followlinks.

Changed in version 3.5: This function now calls os.scandir() instead of os.listdir(), making it faster by reducing the number of calls to os.stat().


Changed in version 3.6: Accepts a path-like object.


➡ `os.fwalk(top='.', topdown=True, onerror=None, *, follow_symlinks=False, dir_fd=None)`
This behaves exactly like walk(), except that it yields a 4-tuple (dirpath, dirnames, filenames, dirfd), and it supports dir_fd.

dirpath, dirnames and filenames are identical to walk() output, and dirfd is a file descriptor referring to the directory dirpath.

This function always supports paths relative to directory descriptors and not following symlinks. Note however that, unlike other functions, the fwalk() default value for follow_symlinks is False.

Note:
 Since fwalk() yields file descriptors, those are only valid until the next iteration step, so you should duplicate them (e.g. with dup()) if you want to keep them longer.
 

This example displays the number of bytes taken by non-directory files in each directory under the starting directory, except that it doesn’t look under any CVS subdirectory:


```py
import os
for root, dirs, files, rootfd in os.fwalk('python/Lib/email'):
    print(root, "consumes", end="")
    print(sum([os.stat(name, dir_fd=rootfd).st_size for name in files]),
          end="")
    print("bytes in", len(files), "non-directory files")
    if 'CVS' in dirs:
        dirs.remove('CVS')  # don't visit CVS directories
```


In the next example, walking the tree bottom-up is essential: rmdir() doesn’t allow deleting a directory before the directory is empty:


```py
# Delete everything reachable from the directory named in "top",
# assuming there are no symbolic links.
# CAUTION:  This is dangerous!  For example, if top == '/', it
# could delete all your disk files.
import os
for root, dirs, files, rootfd in os.fwalk(top, topdown=False):
    for name in files:
        os.unlink(name, dir_fd=rootfd)
    for name in dirs:
        os.rmdir(name, dir_fd=rootfd)
```


Raises an auditing event os.fwalk with arguments top, topdown, onerror, follow_symlinks, dir_fd.

Availability: Unix.

New in version 3.3.


Changed in version 3.6: Accepts a path-like object.


Changed in version 3.7: Added support for bytes paths.


➡ `os.memfd_create(name[, flags=os.MFD_CLOEXEC])`
Create an anonymous file and return a file descriptor that refers to it. flags must be one of the os.MFD_* constants available on the system (or a bitwise ORed combination of them). By default, the new file descriptor is non-inheritable.

The name supplied in name is used as a filename and will be displayed as the target of the corresponding symbolic link in the directory /proc/self/fd/. The displayed name is always prefixed with memfd: and serves only for debugging purposes. Names do not affect the behavior of the file descriptor, and as such multiple files can have the same name without any side effects.

Availability: Linux 3.17 or newer with glibc 2.27 or newer.

New in version 3.8.


➡ `os.MFD_CLOEXEC`
➡ `os.MFD_ALLOW_SEALING`
➡ `os.MFD_HUGETLB`
➡ `os.MFD_HUGE_SHIFT`
➡ `os.MFD_HUGE_MASK`
➡ `os.MFD_HUGE_64KB`
➡ `os.MFD_HUGE_512KB`
➡ `os.MFD_HUGE_1MB`
➡ `os.MFD_HUGE_2MB`
➡ `os.MFD_HUGE_8MB`
➡ `os.MFD_HUGE_16MB`
➡ `os.MFD_HUGE_32MB`
➡ `os.MFD_HUGE_256MB`
➡ `os.MFD_HUGE_512MB`
➡ `os.MFD_HUGE_1GB`
➡ `os.MFD_HUGE_2GB`
➡ `os.MFD_HUGE_16GB`
These flags can be passed to memfd_create().

Availability: Linux 3.17 or newer with glibc 2.27 or newer. The MFD_HUGE* flags are only available since Linux 4.14.

New in version 3.8.


➡ `os.eventfd(initval[, flags=os.EFD_CLOEXEC])`
Create and return an event file descriptor. The file descriptors supports raw read() and write() with a buffer size of 8, select(), poll() and similar. See man page eventfd(2) for more information. By default, the new file descriptor is non-inheritable.

initval is the initial value of the event counter. The initial value must be an 32 bit unsigned integer. Please note that the initial value is limited to a 32 bit unsigned int although the event counter is an unsigned 64 bit integer with a maximum value of 264-2.

flags can be constructed from EFD_CLOEXEC, EFD_NONBLOCK, and EFD_SEMAPHORE.

If EFD_SEMAPHORE is specified and the event counter is non-zero, eventfd_read() returns 1 and decrements the counter by one.

If EFD_SEMAPHORE is not specified and the event counter is non-zero, eventfd_read() returns the current event counter value and resets the counter to zero.

If the event counter is zero and EFD_NONBLOCK is not specified, eventfd_read() blocks.

eventfd_write() increments the event counter. Write blocks if the write operation would increment the counter to a value larger than 264-2.

Example:


```py
import os

# semaphore with start value '1'
fd = os.eventfd(1, os.EFD_SEMAPHORE | os.EFC_CLOEXEC)
try:
    # acquire semaphore
    v = os.eventfd_read(fd)
    try:
        do_work()
    finally:
        # release semaphore
        os.eventfd_write(fd, v)
finally:
    os.close(fd)
```


Availability: Linux 2.6.27 or newer with glibc 2.8 or newer.

New in version 3.10.


➡ `os.eventfd_read(fd)`
Read value from an eventfd() file descriptor and return a 64 bit unsigned int. The function does not verify that fd is an eventfd().

Availability: See eventfd()

New in version 3.10.


➡ `os.eventfd_write(fd, value)`
Add value to an eventfd() file descriptor. value must be a 64 bit unsigned int. The function does not verify that fd is an eventfd().

Availability: See eventfd()

New in version 3.10.


➡ `os.EFD_CLOEXEC`
Set close-on-exec flag for new eventfd() file descriptor.

Availability: See eventfd()

New in version 3.10.


➡ `os.EFD_NONBLOCK`
Set O_NONBLOCK status flag for new eventfd() file descriptor.

Availability: See eventfd()

New in version 3.10.


➡ `os.EFD_SEMAPHORE`
Provide semaphore-like semantics for reads from a eventfd() file descriptor. On read the internal counter is decremented by one.

Availability: Linux 2.6.30 or newer with glibc 2.8 or newer.

New in version 3.10.



#### ◾ Linux extended attributes

New in version 3.3.


These functions are all available on Linux only.

➡ `os.getxattr(path, attribute, *, follow_symlinks=True)`
Return the value of the extended filesystem attribute attribute for path. attribute can be bytes or str (directly or indirectly through the PathLike interface). If it is str, it is encoded with the filesystem encoding.

This function can support specifying a file descriptor and not following symlinks.

Raises an auditing event os.getxattr with arguments path, attribute.

Changed in version 3.6: Accepts a path-like object for path and attribute.


➡ `os.listxattr(path=None, *, follow_symlinks=True)`
Return a list of the extended filesystem attributes on path. The attributes in the list are represented as strings decoded with the filesystem encoding. If path is None, listxattr() will examine the current directory.

This function can support specifying a file descriptor and not following symlinks.

Raises an auditing event os.listxattr with argument path.

Changed in version 3.6: Accepts a path-like object.


➡ `os.removexattr(path, attribute, *, follow_symlinks=True)`
Removes the extended filesystem attribute attribute from path. attribute should be bytes or str (directly or indirectly through the PathLike interface). If it is a string, it is encoded with the filesystem encoding and error handler.

This function can support specifying a file descriptor and not following symlinks.

Raises an auditing event os.removexattr with arguments path, attribute.

Changed in version 3.6: Accepts a path-like object for path and attribute.


➡ `os.setxattr(path, attribute, value, flags=0, *, follow_symlinks=True)`
Set the extended filesystem attribute attribute on path to value. attribute must be a bytes or str with no embedded NULs (directly or indirectly through the PathLike interface). If it is a str, it is encoded with the filesystem encoding and error handler. flags may be XATTR_REPLACE or XATTR_CREATE. If XATTR_REPLACE is given and the attribute does not exist, ENODATA will be raised. If XATTR_CREATE is given and the attribute already exists, the attribute will not be created and EEXISTS will be raised.

This function can support specifying a file descriptor and not following symlinks.

Note:
 A bug in Linux kernel versions less than 2.6.39 caused the flags argument to be ignored on some filesystems.
 

Raises an auditing event os.setxattr with arguments path, attribute, value, flags.

Changed in version 3.6: Accepts a path-like object for path and attribute.


➡ `os.XATTR_SIZE_MAX`
The maximum size the value of an extended attribute can be. Currently, this is 64 KiB on Linux.

➡ `os.XATTR_CREATE`
This is a possible value for the flags argument in setxattr(). It indicates the operation must create an attribute.

➡ `os.XATTR_REPLACE`
This is a possible value for the flags argument in setxattr(). It indicates the operation must replace an existing attribute.


### ===🗝 ◦ Process Management


These functions may be used to create and manage processes.

The various exec* functions take a list of arguments for the new program loaded into the process. In each case, the first of these arguments is passed to the new program as its own name rather than as an argument a user may have typed on a command line. For the C programmer, this is the argv[0] passed to a program’s main(). For example, os.execv('/bin/echo', ['foo', 'bar']) will only print bar on standard output; foo will seem to be ignored.

➡ `os.abort()`
Generate a SIGABRT signal to the current process. On Unix, the default behavior is to produce a core dump; on Windows, the process immediately returns an exit code of 3. Be aware that calling this function will not call the Python signal handler registered for SIGABRT with signal.signal().

➡ `os.add_dll_directory(path)`
Add a path to the DLL search path.

This search path is used when resolving dependencies for imported extension modules (the module itself is resolved through sys.path), and also by ctypes.

Remove the directory by calling close() on the returned object or using it in a with statement.

See the Microsoft documentation for more information about how DLLs are loaded.

Raises an auditing event os.add_dll_directory with argument path.

Availability: Windows.

New in version 3.8: Previous versions of CPython would resolve DLLs using the default behavior for the current process. This led to inconsistencies, such as only sometimes searching PATH or the current working directory, and OS functions such as AddDllDirectory having no effect.


In 3.8, the two primary ways DLLs are loaded now explicitly override the process-wide behavior to ensure consistency. See the porting notes for information on updating libraries.

➡ `os.execl(path, arg0, arg1, ...)`
➡ `os.execle(path, arg0, arg1, ..., env)`
➡ `os.execlp(file, arg0, arg1, ...)`
➡ `os.execlpe(file, arg0, arg1, ..., env)`
➡ `os.execv(path, args)`
➡ `os.execve(path, args, env)`
➡ `os.execvp(file, args)`
➡ `os.execvpe(file, args, env)`
These functions all execute a new program, replacing the current process; they do not return. On Unix, the new executable is loaded into the current process, and will have the same process id as the caller. Errors will be reported as OSError exceptions.

The current process is replaced immediately. Open file objects and descriptors are not flushed, so if there may be data buffered on these open files, you should flush them using sys.stdout.flush() or os.fsync() before calling an exec* function.

The “l” and “v” variants of the exec* functions differ in how command-line arguments are passed. The `“l”` variants are perhaps the easiest to work with if the number of parameters is fixed when the code is written; the individual parameters simply become additional parameters to the `execl*()` functions. The `“v”` variants are good when the number of parameters is variable, with the arguments being passed in a list or tuple as the args parameter. In either case, the arguments to the child process should start with the name of the command being run, but this is not enforced.

The variants which include a `“p”` near the end (execlp(), execlpe(), execvp(), and execvpe()) will use the PATH environment variable to locate the program file. When the environment is being replaced (using one of the `exec*e` variants, discussed in the next paragraph), the new environment is used as the source of the PATH variable. The other variants, execl(), execle(), execv(), and execve(), will not use the PATH variable to locate the executable; path must contain an appropriate absolute or relative path.

For execle(), execlpe(), execve(), and execvpe() (note that these all end in `“e”`), the env parameter must be a mapping which is used to define the environment variables for the new process (these are used instead of the current process’ environment); the functions execl(), execlp(), execv(), and execvp() all cause the new process to inherit the environment of the current process.

For execve() on some platforms, path may also be specified as an open file descriptor. This functionality may not be supported on your platform; you can check whether or not it is available using os.supports_fd. If it is unavailable, using it will raise a NotImplementedError.

Raises an auditing event os.exec with arguments path, args, env.

Availability: Unix, Windows.

New in version 3.3: Added support for specifying path as an open file descriptor for execve().


Changed in version 3.6: Accepts a path-like object.


➡ `os._exit(n)`
Exit the process with status n, without calling cleanup handlers, flushing stdio buffers, etc.

Note:
 The standard way to exit is sys.exit(n). `_exit()` should normally only be used in the child process after a fork().
 

The following exit codes are defined and can be used with `_exit()`, although they are not required. These are typically used for system programs written in Python, such as a mail server’s external command delivery program.

Note:
 Some of these may not be available on all Unix platforms, since there is some variation. These constants are defined where they are defined by the underlying platform.
 

➡ `os.EX_OK`
Exit code that means no error occurred.

Availability: Unix.

➡ `os.EX_USAGE`
Exit code that means the command was used incorrectly, such as when the wrong number of arguments are given.

Availability: Unix.

➡ `os.EX_DATAERR`
Exit code that means the input data was incorrect.

Availability: Unix.

➡ `os.EX_NOINPUT`
Exit code that means an input file did not exist or was not readable.

Availability: Unix.

➡ `os.EX_NOUSER`
Exit code that means a specified user did not exist.

Availability: Unix.

➡ `os.EX_NOHOST`
Exit code that means a specified host did not exist.

Availability: Unix.

➡ `os.EX_UNAVAILABLE`
Exit code that means that a required service is unavailable.

Availability: Unix.

➡ `os.EX_SOFTWARE`
Exit code that means an internal software error was detected.

Availability: Unix.

➡ `os.EX_OSERR`
Exit code that means an operating system error was detected, such as the inability to fork or create a pipe.

Availability: Unix.

➡ `os.EX_OSFILE`
Exit code that means some system file did not exist, could not be opened, or had some other kind of error.

Availability: Unix.

➡ `os.EX_CANTCREAT`
Exit code that means a user specified output file could not be created.

Availability: Unix.

➡ `os.EX_IOERR`
Exit code that means that an error occurred while doing I/O on some file.

Availability: Unix.

➡ `os.EX_TEMPFAIL`
Exit code that means a temporary failure occurred. This indicates something that may not really be an error, such as a network connection that couldn’t be made during a retryable operation.

Availability: Unix.

➡ `os.EX_PROTOCOL`
Exit code that means that a protocol exchange was illegal, invalid, or not understood.

Availability: Unix.

➡ `os.EX_NOPERM`
Exit code that means that there were insufficient permissions to perform the operation (but not intended for file system problems).

Availability: Unix.

➡ `os.EX_CONFIG`
Exit code that means that some kind of configuration error occurred.

Availability: Unix.

➡ `os.EX_NOTFOUND`
Exit code that means something like “an entry was not found”.

Availability: Unix.

➡ `os.fork()`
Fork a child process. Return 0 in the child and the child’s process id in the parent. If an error occurs OSError is raised.

Note that some platforms including FreeBSD <= 6.3 and Cygwin have known issues when using fork() from a thread.

Raises an auditing event os.fork with no arguments.

Changed in version 3.8: Calling fork() in a subinterpreter is no longer supported (RuntimeError is raised).


Warning:
 See ssl for applications that use the SSL module with fork().
 

Availability: Unix.

➡ `os.forkpty()`
Fork a child process, using a new pseudo-terminal as the child’s controlling terminal. Return a pair of (pid, fd), where pid is 0 in the child, the new child’s process id in the parent, and fd is the file descriptor of the master end of the pseudo-terminal. For a more portable approach, use the pty module. If an error occurs OSError is raised.

Raises an auditing event os.forkpty with no arguments.

Changed in version 3.8: Calling forkpty() in a subinterpreter is no longer supported (RuntimeError is raised).


Availability: some flavors of Unix.

➡ `os.kill(pid, sig)`
Send signal sig to the process pid. Constants for the specific signals available on the host platform are defined in the signal module.

Windows: The signal.CTRL_C_EVENT and signal.CTRL_BREAK_EVENT signals are special signals which can only be sent to console processes which share a common console window, e.g., some subprocesses. Any other value for sig will cause the process to be unconditionally killed by the TerminateProcess API, and the exit code will be set to sig. The Windows version of kill() additionally takes process handles to be killed.

See also signal.pthread_kill().

Raises an auditing event os.kill with arguments pid, sig.

New in version 3.2: Windows support.


➡ `os.killpg(pgid, sig)`
Send the signal sig to the process group pgid.

Raises an auditing event os.killpg with arguments pgid, sig.

Availability: Unix.

➡ `os.nice(increment)`
Add increment to the process’s “niceness”. Return the new niceness.

Availability: Unix.

➡ `os.pidfd_open(pid, flags=0)`
Return a file descriptor referring to the process pid. This descriptor can be used to perform process management without races and signals. The flags argument is provided for future extensions; no flag values are currently defined.

See the pidfd_open(2) man page for more details.

Availability: Linux 5.3+

New in version 3.9.


➡ `os.plock(op)`
Lock program segments into memory. The value of op (defined in <sys/lock.h>) determines which segments are locked.

Availability: Unix.

➡ `os.popen(cmd, mode='r', buffering=-1)`
Open a pipe to or from command cmd. The return value is an open file object connected to the pipe, which can be read or written depending on whether mode is 'r' (default) or 'w'. The buffering argument has the same meaning as the corresponding argument to the built-in open() function. The returned file object reads or writes text strings rather than bytes.

The close method returns None if the subprocess exited successfully, or the subprocess’s return code if there was an error. On POSIX systems, if the return code is positive it represents the return value of the process left-shifted by one byte. If the return code is negative, the process was terminated by the signal given by the negated value of the return code. (For example, the return value might be - signal.SIGKILL if the subprocess was killed.) On Windows systems, the return value contains the signed integer return code from the child process.

On Unix, waitstatus_to_exitcode() can be used to convert the close method result (exit status) into an exit code if it is not None. On Windows, the close method result is directly the exit code (or None).

This is implemented using subprocess.Popen; see that class’s documentation for more powerful ways to manage and communicate with subprocesses.

➡ `os.posix_spawn(path, argv, env, *, file_actions=None, setpgroup=None, resetids=False, setsid=False, setsigmask=(), setsigdef=(), scheduler=None)`
Wraps the posix_spawn() C library API for use from Python.

Most users should use subprocess.run() instead of posix_spawn().

The positional-only arguments path, args, and env are similar to execve().

The path parameter is the path to the executable file. The path should contain a directory. Use posix_spawnp() to pass an executable file without directory.

The file_actions argument may be a sequence of tuples describing actions to take on specific file descriptors in the child process between the C library implementation’s fork() and exec() steps. The first item in each tuple must be one of the three type indicator listed below describing the remaining tuple elements:

➡ `os.POSIX_SPAWN_OPEN`
(os.POSIX_SPAWN_OPEN, fd, path, flags, mode)

Performs os.dup2(os.open(path, flags, mode), fd).

➡ `os.POSIX_SPAWN_CLOSE`
(os.POSIX_SPAWN_CLOSE, fd)

Performs os.close(fd).

➡ `os.POSIX_SPAWN_DUP2`
(os.POSIX_SPAWN_DUP2, fd, new_fd)

Performs os.dup2(fd, new_fd).

These tuples correspond to the C library posix_spawn_file_actions_addopen(), posix_spawn_file_actions_addclose(), and posix_spawn_file_actions_adddup2() API calls used to prepare for the posix_spawn() call itself.

The setpgroup argument will set the process group of the child to the value specified. If the value specified is 0, the child’s process group ID will be made the same as its process ID. If the value of setpgroup is not set, the child will inherit the parent’s process group ID. This argument corresponds to the C library POSIX_SPAWN_SETPGROUP flag.

If the resetids argument is True it will reset the effective UID and GID of the child to the real UID and GID of the parent process. If the argument is False, then the child retains the effective UID and GID of the parent. In either case, if the set-user-ID and set-group-ID permission bits are enabled on the executable file, their effect will override the setting of the effective UID and GID. This argument corresponds to the C library POSIX_SPAWN_RESETIDS flag.

If the setsid argument is True, it will create a new session ID for posix_spawn. setsid requires POSIX_SPAWN_SETSID or POSIX_SPAWN_SETSID_NP flag. Otherwise, NotImplementedError is raised.

The setsigmask argument will set the signal mask to the signal set specified. If the parameter is not used, then the child inherits the parent’s signal mask. This argument corresponds to the C library POSIX_SPAWN_SETSIGMASK flag.

The sigdef argument will reset the disposition of all signals in the set specified. This argument corresponds to the C library POSIX_SPAWN_SETSIGDEF flag.

The scheduler argument must be a tuple containing the (optional) scheduler policy and an instance of sched_param with the scheduler parameters. A value of None in the place of the scheduler policy indicates that is not being provided. This argument is a combination of the C library POSIX_SPAWN_SETSCHEDPARAM and POSIX_SPAWN_SETSCHEDULER flags.

Raises an auditing event os.posix_spawn with arguments path, argv, env.

New in version 3.8.


Availability: Unix.

➡ `os.posix_spawnp(path, argv, env, *, file_actions=None, setpgroup=None, resetids=False, setsid=False, setsigmask=(), setsigdef=(), scheduler=None)`
Wraps the posix_spawnp() C library API for use from Python.

Similar to posix_spawn() except that the system searches for the executable file in the list of directories specified by the PATH environment variable (in the same way as for execvp(3)).

Raises an auditing event os.posix_spawn with arguments path, argv, env.

New in version 3.8.


Availability: See posix_spawn() documentation.

➡ `os.register_at_fork(*, before=None, after_in_parent=None, after_in_child=None)`
Register callables to be executed when a new child process is forked using os.fork() or similar process cloning APIs. The parameters are optional and keyword-only. Each specifies a different call point.

• before is a function called before forking a child process.
• after_in_parent is a function called from the parent process after forking a child process.
• after_in_child is a function called from the child process.

These calls are only made if control is expected to return to the Python interpreter. A typical subprocess launch will not trigger them as the child is not going to re-enter the interpreter.

Functions registered for execution before forking are called in reverse registration order. Functions registered for execution after forking (either in the parent or in the child) are called in registration order.

Note that fork() calls made by third-party C code may not call those functions, unless it explicitly calls PyOS_BeforeFork(), PyOS_AfterFork_Parent() and PyOS_AfterFork_Child().

There is no way to unregister a function.

Availability: Unix.

New in version 3.7.


➡ `os.spawnl(mode, path, ...)`
➡ `os.spawnle(mode, path, ..., env)`
➡ `os.spawnlp(mode, file, ...)`
➡ `os.spawnlpe(mode, file, ..., env)`
➡ `os.spawnv(mode, path, args)`
➡ `os.spawnve(mode, path, args, env)`
➡ `os.spawnvp(mode, file, args)`
➡ `os.spawnvpe(mode, file, args, env)`
Execute the program path in a new process.

(Note that the subprocess module provides more powerful facilities for spawning new processes and retrieving their results; using that module is preferable to using these functions. Check especially the Replacing Older Functions with the subprocess Module section.)

If mode is P_NOWAIT, this function returns the process id of the new process; if mode is P_WAIT, returns the process’s exit code if it exits normally, or -signal, where signal is the signal that killed the process. On Windows, the process id will actually be the process handle, so can be used with the waitpid() function.

Note on VxWorks, this function doesn’t return -signal when the new process is killed. Instead it raises OSError exception.

The “l” and “v” variants of the spawn* functions differ in how command-line arguments are passed. The `“l”` variants are perhaps the easiest to work with if the number of parameters is fixed when the code is written; the individual parameters simply become additional parameters to the `spawnl*()` functions. The `“v”` variants are good when the number of parameters is variable, with the arguments being passed in a list or tuple as the args parameter. In either case, the arguments to the child process must start with the name of the command being run.

The variants which include a second `“p”` near the end (spawnlp(), spawnlpe(), spawnvp(), and spawnvpe()) will use the PATH environment variable to locate the program file. When the environment is being replaced (using one of the `spawn*e` variants, discussed in the next paragraph), the new environment is used as the source of the PATH variable. The other variants, spawnl(), spawnle(), spawnv(), and spawnve(), will not use the PATH variable to locate the executable; path must contain an appropriate absolute or relative path.

For spawnle(), spawnlpe(), spawnve(), and spawnvpe() (note that these all end in `“e”`), the env parameter must be a mapping which is used to define the environment variables for the new process (they are used instead of the current process’ environment); the functions spawnl(), spawnlp(), spawnv(), and spawnvp() all cause the new process to inherit the environment of the current process. Note that keys and values in the env dictionary must be strings; invalid keys or values will cause the function to fail, with a return value of 127.

As an example, the following calls to spawnlp() and spawnvpe() are equivalent:


import os

➡ `os.spawnlp(os.P_WAIT, 'cp', 'cp', 'index.html', '/dev/null')`

L = ['cp', 'index.html', '/dev/null']

➡ `os.spawnvpe(os.P_WAIT, 'cp', L, os.environ)`


Raises an auditing event os.spawn with arguments mode, path, args, env.

Availability: Unix, Windows. spawnlp(), spawnlpe(), spawnvp() and spawnvpe() are not available on Windows. spawnle() and spawnve() are not thread-safe on Windows; we advise you to use the subprocess module instead.

Changed in version 3.6: Accepts a path-like object.


➡ `os.P_NOWAIT`
➡ `os.P_NOWAITO`
Possible values for the mode parameter to the spawn* family of functions. If either of these values is given, the `spawn*() `functions will return as soon as the new process has been created, with the process id as the return value.

Availability: Unix, Windows.

➡ `os.P_WAIT`
Possible value for the mode parameter to the spawn* family of functions. If this is given as mode, the `spawn*() `functions will not return until the new process has run to completion and will return the exit code of the process the run is successful, or -signal if a signal kills the process.

Availability: Unix, Windows.

➡ `os.P_DETACH`
➡ `os.P_OVERLAY`
Possible values for the mode parameter to the spawn* family of functions. These are less portable than those listed above. P_DETACH is similar to P_NOWAIT, but the new process is detached from the console of the calling process. If P_OVERLAY is used, the current process will be replaced; the spawn* function will not return.

Availability: Windows.

➡ `os.startfile(path[, operation][, arguments][, cwd][, show_cmd])`
Start a file with its associated application.

When operation is not specified or 'open', this acts like double-clicking the file in Windows Explorer, or giving the file name as an argument to the start command from the interactive command shell: the file is opened with whatever application (if any) its extension is associated.

When another operation is given, it must be a “command verb” that specifies what should be done with the file. Common verbs documented by Microsoft are 'print' and 'edit' (to be used on files) as well as 'explore' and 'find' (to be used on directories).

When launching an application, specify arguments to be passed as a single string. This argument may have no effect when using this function to launch a document.

The default working directory is inherited, but may be overridden by the cwd argument. This should be an absolute path. A relative path will be resolved against this argument.

Use show_cmd to override the default window style. Whether this has any effect will depend on the application being launched. Values are integers as supported by the Win32 ShellExecute() function.

startfile() returns as soon as the associated application is launched. There is no option to wait for the application to close, and no way to retrieve the application’s exit status. The path parameter is relative to the current directory or cwd. If you want to use an absolute path, make sure the first character is not a slash ('/') Use pathlib or the os.path.normpath() function to ensure that paths are properly encoded for Win32.

To reduce interpreter startup overhead, the Win32 ShellExecute() function is not resolved until this function is first called. If the function cannot be resolved, NotImplementedError will be raised.

Raises an auditing event os.startfile with arguments path, operation.

Raises an auditing event os.startfile/2 with arguments path, operation, arguments, cwd, show_cmd.

Availability: Windows.

Changed in version 3.10: Added the arguments, cwd and show_cmd arguments, and the os.startfile/2 audit event.


➡ `os.system(command)`
Execute the command (a string) in a subshell. This is implemented by calling the Standard C function system(), and has the same limitations. Changes to sys.stdin, etc. are not reflected in the environment of the executed command. If command generates any output, it will be sent to the interpreter standard output stream. The C standard does not specify the meaning of the return value of the C function, so the return value of the Python function is system-dependent.

On Unix, the return value is the exit status of the process encoded in the format specified for wait().

On Windows, the return value is that returned by the system shell after running command. The shell is given by the Windows environment variable COMSPEC: it is usually cmd.exe, which returns the exit status of the command run; on systems using a non-native shell, consult your shell documentation.

The subprocess module provides more powerful facilities for spawning new processes and retrieving their results; using that module is preferable to using this function. See the Replacing Older Functions with the subprocess Module section in the subprocess documentation for some helpful recipes.

On Unix, waitstatus_to_exitcode() can be used to convert the result (exit status) into an exit code. On Windows, the result is directly the exit code.

Raises an auditing event os.system with argument command.

Availability: Unix, Windows.

➡ `os.times()`
Returns the current global process times. The return value is an object with five attributes:

• user - user time
• system - system time
• children_user - user time of all child processes
• children_system - system time of all child processes
• elapsed - elapsed real time since a fixed point in the past

For backwards compatibility, this object also behaves like a five-tuple containing user, system, children_user, children_system, and elapsed in that order.

See the Unix manual page times(2) and times(3) manual page on Unix or the GetProcessTimes MSDN on Windows. On Windows, only user and system are known; the other attributes are zero.

Availability: Unix, Windows.

Changed in version 3.3: Return type changed from a tuple to a tuple-like object with named attributes.


➡ `os.wait()`
Wait for completion of a child process, and return a tuple containing its pid and exit status indication: a 16-bit number, whose low byte is the signal number that killed the process, and whose high byte is the exit status (if the signal number is zero); the high bit of the low byte is set if a core file was produced.

waitstatus_to_exitcode() can be used to convert the exit status into an exit code.

Availability: Unix.

See also:
 waitpid() can be used to wait for the completion of a specific child process and has more options.
 

➡ `os.waitid(idtype, id, options)`
Wait for the completion of one or more child processes. idtype can be P_PID, P_PGID, P_ALL, or P_PIDFD on Linux. id specifies the pid to wait on. options is constructed from the ORing of one or more of WEXITED, WSTOPPED or WCONTINUED and additionally may be ORed with WNOHANG or WNOWAIT. The return value is an object representing the data contained in the siginfo_t structure, namely: si_pid, si_uid, si_signo, si_status, si_code or None if WNOHANG is specified and there are no children in a waitable state.

Availability: Unix.

New in version 3.3.


➡ `os.P_PID`
➡ `os.P_PGID`
➡ `os.P_ALL`
These are the possible values for idtype in waitid(). They affect how id is interpreted.

Availability: Unix.

New in version 3.3.


➡ `os.P_PIDFD`
This is a Linux-specific idtype that indicates that id is a file descriptor that refers to a process.

Availability: Linux 5.4+

New in version 3.9.


➡ `os.WEXITED`
➡ `os.WSTOPPED`
➡ `os.WNOWAIT`
Flags that can be used in options in waitid() that specify what child signal to wait for.

Availability: Unix.

New in version 3.3.


➡ `os.CLD_EXITED`
➡ `os.CLD_KILLED`
➡ `os.CLD_DUMPED`
➡ `os.CLD_TRAPPED`
➡ `os.CLD_STOPPED`
➡ `os.CLD_CONTINUED`
These are the possible values for si_code in the result returned by waitid().

Availability: Unix.

New in version 3.3.


Changed in version 3.9: Added CLD_KILLED and CLD_STOPPED values.


➡ `os.waitpid(pid, options)`
The details of this function differ on Unix and Windows.

On Unix: Wait for completion of a child process given by process id pid, and return a tuple containing its process id and exit status indication (encoded as for wait()). The semantics of the call are affected by the value of the integer options, which should be 0 for normal operation.

If pid is greater than 0, waitpid() requests status information for that specific process. If pid is 0, the request is for the status of any child in the process group of the current process. If pid is -1, the request pertains to any child of the current process. If pid is less than -1, status is requested for any process in the process group -pid (the absolute value of pid).

An OSError is raised with the value of errno when the syscall returns -1.

On Windows: Wait for completion of a process given by process handle pid, and return a tuple containing pid, and its exit status shifted left by 8 bits (shifting makes cross-platform use of the function easier). A pid less than or equal to 0 has no special meaning on Windows, and raises an exception. The value of integer options has no effect. pid can refer to any process whose id is known, not necessarily a child process. The spawn* functions called with P_NOWAIT return suitable process handles.

waitstatus_to_exitcode() can be used to convert the exit status into an exit code.

Changed in version 3.5: If the system call is interrupted and the signal handler does not raise an exception, the function now retries the system call instead of raising an InterruptedError exception (see PEP 475 for the rationale).


➡ `os.wait3(options)`
Similar to waitpid(), except no process id argument is given and a 3-element tuple containing the child’s process id, exit status indication, and resource usage information is returned. Refer to resource.getrusage() for details on resource usage information. The option argument is the same as that provided to waitpid() and wait4().

waitstatus_to_exitcode() can be used to convert the exit status into an exitcode.

Availability: Unix.

➡ `os.wait4(pid, options)`
Similar to waitpid(), except a 3-element tuple, containing the child’s process id, exit status indication, and resource usage information is returned. Refer to resource.getrusage() for details on resource usage information. The arguments to wait4() are the same as those provided to waitpid().

waitstatus_to_exitcode() can be used to convert the exit status into an exitcode.

Availability: Unix.

➡ `os.waitstatus_to_exitcode(status)`
Convert a wait status to an exit code.

On Unix:

• If the process exited normally (if WIFEXITED(status) is true), return the process exit status (return WEXITSTATUS(status)): result greater than or equal to 0.
• If the process was terminated by a signal (if WIFSIGNALED(status) is true), return -signum where signum is the number of the signal that caused the process to terminate (return -WTERMSIG(status)): result less than 0.
• Otherwise, raise a ValueError.

On Windows, return status shifted right by 8 bits.

On Unix, if the process is being traced or if waitpid() was called with WUNTRACED option, the caller must first check if WIFSTOPPED(status) is true. This function must not be called if WIFSTOPPED(status) is true.

See also:
 WIFEXITED(), WEXITSTATUS(), WIFSIGNALED(), WTERMSIG(), WIFSTOPPED(), WSTOPSIG() functions.
 

New in version 3.9.


➡ `os.WNOHANG`
The option for waitpid() to return immediately if no child process status is available immediately. The function returns (0, 0) in this case.

Availability: Unix.

➡ `os.WCONTINUED`
This option causes child processes to be reported if they have been continued from a job control stop since their status was last reported.

Availability: some Unix systems.

➡ `os.WUNTRACED`
This option causes child processes to be reported if they have been stopped but their current state has not been reported since they were stopped.

Availability: Unix.

The following functions take a process status code as returned by system(), wait(), or waitpid() as a parameter. They may be used to determine the disposition of a process.

➡ `os.WCOREDUMP(status)`
Return True if a core dump was generated for the process, otherwise return False.

This function should be employed only if WIFSIGNALED() is true.

Availability: Unix.

➡ `os.WIFCONTINUED(status)`
Return True if a stopped child has been resumed by delivery of SIGCONT (if the process has been continued from a job control stop), otherwise return False.

See WCONTINUED option.

Availability: Unix.

➡ `os.WIFSTOPPED(status)`
Return True if the process was stopped by delivery of a signal, otherwise return False.

WIFSTOPPED() only returns True if the waitpid() call was done using WUNTRACED option or when the process is being traced (see ptrace(2)).

Availability: Unix.

➡ `os.WIFSIGNALED(status)`
Return True if the process was terminated by a signal, otherwise return False.

Availability: Unix.

➡ `os.WIFEXITED(status)`
Return True if the process exited terminated normally, that is, by calling exit() or `_exit()`, or by returning from main(); otherwise return False.

Availability: Unix.

➡ `os.WEXITSTATUS(status)`
Return the process exit status.

This function should be employed only if WIFEXITED() is true.

Availability: Unix.

➡ `os.WSTOPSIG(status)`
Return the signal which caused the process to stop.

This function should be employed only if WIFSTOPPED() is true.

Availability: Unix.

➡ `os.WTERMSIG(status)`
Return the number of the signal that caused the process to terminate.

This function should be employed only if WIFSIGNALED() is true.

Availability: Unix.



### ===🗝 ◦ Interface to the scheduler

These functions control how a process is allocated CPU time by the operating system. They are only available on some Unix platforms. For more detailed information, consult your Unix manpages.

New in version 3.3.


The following scheduling policies are exposed if they are supported by the operating system.

➡ `os.SCHED_OTHER`
The default scheduling policy.

➡ `os.SCHED_BATCH`
Scheduling policy for CPU-intensive processes that tries to preserve interactivity on the rest of the computer.

➡ `os.SCHED_IDLE`
Scheduling policy for extremely low priority background tasks.

➡ `os.SCHED_SPORADIC`
Scheduling policy for sporadic server programs.

➡ `os.SCHED_FIFO`
A First In First Out scheduling policy.

➡ `os.SCHED_RR`
A round-robin scheduling policy.

➡ `os.SCHED_RESET_ON_FORK`
This flag can be OR’ed with any other scheduling policy. When a process with this flag set forks, its child’s scheduling policy and priority are reset to the default.

✅ `class os.sched_param(sched_priority)`
This class represents tunable scheduling parameters used in sched_setparam(), sched_setscheduler(), and sched_getparam(). It is immutable.

At the moment, there is only one possible parameter:
sched_priority
The scheduling priority for a scheduling policy.

➡ `os.sched_get_priority_min(policy)`
Get the minimum priority value for policy. policy is one of the scheduling policy constants above.

➡ `os.sched_get_priority_max(policy)`
Get the maximum priority value for policy. policy is one of the scheduling policy constants above.

➡ `os.sched_setscheduler(pid, policy, param)`
Set the scheduling policy for the process with PID pid. A pid of 0 means the calling process. policy is one of the scheduling policy constants above. param is a sched_param instance.

➡ `os.sched_getscheduler(pid)`
Return the scheduling policy for the process with PID pid. A pid of 0 means the calling process. The result is one of the scheduling policy constants above.

➡ `os.sched_setparam(pid, param)`
Set the scheduling parameters for the process with PID pid. A pid of 0 means the calling process. param is a sched_param instance.

➡ `os.sched_getparam(pid)`
Return the scheduling parameters as a sched_param instance for the process with PID pid. A pid of 0 means the calling process.

➡ `os.sched_rr_get_interval(pid)`
Return the round-robin quantum in seconds for the process with PID pid. A pid of 0 means the calling process.

➡ `os.sched_yield()`
Voluntarily relinquish the CPU.

➡ `os.sched_setaffinity(pid, mask)`
Restrict the process with PID pid (or the current process if zero) to a set of CPUs. mask is an iterable of integers representing the set of CPUs to which the process should be restricted.

➡ `os.sched_getaffinity(pid)`
Return the set of CPUs the process with PID pid (or the current process if zero) is restricted to.


### ===🗝 ◦ Miscellaneous System Information


➡ `os.confstr(name)`
Return string-valued system configuration values. name specifies the configuration value to retrieve; it may be a string which is the name of a defined system value; these names are specified in a number of standards (POSIX, Unix 95, Unix 98, and others). Some platforms define additional names as well. The names known to the host operating system are given as the keys of the confstr_names dictionary. For configuration variables not included in that mapping, passing an integer for name is also accepted.

If the configuration value specified by name isn’t defined, None is returned.

If name is a string and is not known, ValueError is raised. If a specific value for name is not supported by the host system, even if it is included in confstr_names, an OSError is raised with errno.EINVAL for the error number.

Availability: Unix.

➡ `os.confstr_names`
Dictionary mapping names accepted by confstr() to the integer values defined for those names by the host operating system. This can be used to determine the set of names known to the system.

Availability: Unix.

➡ `os.cpu_count()`
Return the number of CPUs in the system. Returns None if undetermined.

This number is not equivalent to the number of CPUs the current process can use. The number of usable CPUs can be obtained with len(os.sched_getaffinity(0))

New in version 3.4.


➡ `os.getloadavg()`
Return the number of processes in the system run queue averaged over the last 1, 5, and 15 minutes or raises OSError if the load average was unobtainable.

Availability: Unix.

➡ `os.sysconf(name)`
Return integer-valued system configuration values. If the configuration value specified by name isn’t defined, -1 is returned. The comments regarding the name parameter for confstr() apply here as well; the dictionary that provides information on the known names is given by sysconf_names.

Availability: Unix.

➡ `os.sysconf_names`
Dictionary mapping names accepted by sysconf() to the integer values defined for those names by the host operating system. This can be used to determine the set of names known to the system.

Availability: Unix.

The following data values are used to support path manipulation operations. These are defined for all platforms.

Higher-level operations on pathnames are defined in the os.path module.
 os.curdir
The constant string used by the operating system to refer to the current directory. This is '.' for Windows and POSIX. Also available via os.path.
 os.pardir
The constant string used by the operating system to refer to the parent directory. This is '..' for Windows and POSIX. Also available via os.path.
 os.sep
The character used by the operating system to separate pathname components. This is '/' for POSIX and '\\' for Windows. Note that knowing this is not sufficient to be able to parse or concatenate pathnames — use os.path.split() and os.path.join() — but it is occasionally useful. Also available via os.path.
 os.altsep
An alternative character used by the operating system to separate pathname components, or None if only one separator character exists. This is set to '/' on Windows systems where sep is a backslash. Also available via os.path.
 os.extsep
The character which separates the base filename from the extension; for example, the '.' in os.py. Also available via os.path.
 os.pathsep
The character conventionally used by the operating system to separate search path components (as in PATH), such as ':' for POSIX or ';' for Windows. Also available via os.path.

➡ `os.defpath`
The default search path used by exec*p* and spawn*p* if the environment doesn’t have a 'PATH' key. Also available via os.path.

➡ `os.linesep`
The string used to separate (or, rather, terminate) lines on the current platform. This may be a single character, such as '\n' for POSIX, or multiple characters, for example, '\r\n' for Windows. Do not use os.linesep as a line terminator when writing files opened in text mode (the default); use a single '\n' instead, on all platforms.

➡ `os.devnull`
The file path of the null device. For example: '/dev/null' for POSIX, 'nul' for Windows. Also available via os.path.

➡ `os.RTLD_LAZYos.RTLD_NOWos.RTLD_GLOBALos.RTLD_LOCALos.RTLD_NODELETEos.RTLD_NOLOADos.RTLD_DEEPBIND`
Flags for use with the setdlopenflags() and getdlopenflags() functions. See the Unix manual page dlopen(3) for what the different flags mean.

New in version 3.3.


### ===🗝 ◦ Random numbers


Random numbers

➡ `os.getrandom(size, flags=0)`
Get up to size random bytes. The function can return less bytes than requested.

These bytes can be used to seed user-space random number generators or for cryptographic purposes.

getrandom() relies on entropy gathered from device drivers and other sources of environmental noise. Unnecessarily reading large quantities of data will have a negative impact on other users of the /dev/random and /dev/urandom devices.

The flags argument is a bit mask that can contain zero or more of the following values ORed together: os.GRND_RANDOM and GRND_NONBLOCK.

See also the Linux getrandom() manual page.

Availability: Linux 3.17 and newer.

New in version 3.6.


➡ `os.urandom(size)`
Return a string of size random bytes suitable for cryptographic use.

This function returns random bytes from an OS-specific randomness source. The returned data should be unpredictable enough for cryptographic applications, though its exact quality depends on the OS implementation.

On Linux, if the getrandom() syscall is available, it is used in blocking mode: block until the system urandom entropy pool is initialized (128 bits of entropy are collected by the kernel). See the PEP 524 for the rationale. On Linux, the getrandom() function can be used to get random bytes in non-blocking mode (using the GRND_NONBLOCK flag) or to poll until the system urandom entropy pool is initialized.

On a Unix-like system, random bytes are read from the /dev/urandom device. If the /dev/urandom device is not available or not readable, the NotImplementedError exception is raised.

On Windows, it will use CryptGenRandom().

See also:
 The secrets module provides higher level functions. For an easy-to-use interface to the random number generator provided by your platform, please see random.SystemRandom.
 

Changed in version 3.6.0: On Linux, getrandom() is now used in blocking mode to increase the security.


Changed in version 3.5.2: On Linux, if the getrandom() syscall blocks (the urandom entropy pool is not initialized yet), fall back on reading /dev/urandom.


Changed in version 3.5: On Linux 3.17 and newer, the getrandom() syscall is now used when available. On OpenBSD 5.6 and newer, the C getentropy() function is now used. These functions avoid the usage of an internal file descriptor.


➡ `os.GRND_NONBLOCK`
By default, when reading from /dev/random, getrandom() blocks if no random bytes are available, and when reading from /dev/urandom, it blocks if the entropy pool has not yet been initialized.

If the GRND_NONBLOCK flag is set, then getrandom() does not block in these cases, but instead immediately raises BlockingIOError.

New in version 3.6.


➡ `os.GRND_RANDOM`
If this bit is set, then random bytes are drawn from the /dev/random pool instead of the /dev/urandom pool.

New in version 3.6.




## ==⚡ • io — Core tools for working with streams
◦  Overview
◾Text I/O
◾Binary I/O
◾Raw I/O

◦ High-level Module Interface
◦ Class hierarchy
◾I/O Base Classes
◾Raw File I/O
◾Buffered Streams
◾Text I/O

◦ Performance
◾Binary I/O
◾Text I/O
◾Multi-threading
◾Reentrancy


## ==⚡ • time — Time access and conversions

This module provides various time-related functions. For related functionality, see also the datetime and calendar modules.

Although this module is always available, not all functions are available on all platforms. Most of the functions defined in this module call platform C library functions with the same name. It may sometimes be helpful to consult the platform documentation, because the semantics of these functions varies among platforms.

An explanation of some terminology and conventions is in order.

• The epoch is the point where the time starts, and is platform dependent. For Unix, the epoch is January 1, 1970, 00:00:00 (UTC). To find out what the epoch is on a given platform, look at time.gmtime(0).

• The term seconds since the epoch refers to the total number of elapsed seconds since the epoch, typically excluding leap seconds. Leap seconds are excluded from this total on all POSIX-compliant platforms.

• The functions in this module may not handle dates and times before the epoch or far in the future. The cut-off point in the future is determined by the C library; for 32-bit systems, it is typically in 2038.

• Function strptime() can parse 2-digit years when given %y format code. When 2-digit years are parsed, they are converted according to the POSIX and ISO C standards: values 69–99 are mapped to 1969–1999, and values 0–68 are mapped to 2000–2068.

• UTC is Coordinated Universal Time (formerly known as Greenwich Mean Time, or GMT). The acronym UTC is not a mistake but a compromise between English and French.


• DST is Daylight Saving Time, an adjustment of the timezone by (usually) one hour during part of the year. DST rules are magic (determined by local law) and can change from year to year. The C library has a table containing the local rules (often it is read from a system file for flexibility) and is the only source of True Wisdom in this respect.


• The precision of the various real-time functions may be less than suggested by the units in which their value or argument is expressed. E.g. on most Unix systems, the clock “ticks” only 50 or 100 times a second.



• On the other hand, the precision of time() and sleep() is better than their Unix equivalents: times are expressed as floating point numbers, time() returns the most accurate time available (using Unix gettimeofday() where available), and sleep() will accept a time with a nonzero fraction (Unix select() is used to implement this, where available).



• The time value as returned by gmtime(), localtime(), and strptime(), and accepted by asctime(), mktime() and strftime(), is a sequence of 9 integers. The return values of gmtime(), localtime(), and strptime() also offer attribute names for individual fields.

See struct_time for a description of these objects.


Changed in version 3.3: The struct_time type was extended to provide the tm_gmtoff and tm_zone attributes when platform supports corresponding struct tm members.


Changed in version 3.6: The struct_time attributes tm_gmtoff and tm_zone are now available on all platforms.


• Use the following functions to convert between time representations:

|            From           |             To            |        Use        |
|---------------------------|---------------------------|-------------------|
| seconds since the epoch   | struct_time in UTC        | gmtime()          |
| seconds since the epoch   | struct_time in local time | localtime()       |
| struct_time in UTC        | seconds since the epoch   | calendar.timegm() |
| struct_time in local time | seconds since the epoch   | mktime()          |


### ===🗝 ◦ Functions

➡ `time.asctime([t])`
➡ `time.pthread_getcpuclockid(thread_id)`
➡ `time.clock_getres(clk_id)`
➡ `time.clock_gettime(clk_id) → float`
➡ `time.clock_gettime_ns(clk_id) → int`
➡ `time.clock_settime(clk_id, time: float)`
➡ `time.clock_settime_ns(clk_id, time: int)`
➡ `time.ctime([secs])`
➡ `time.get_clock_info(name)`
➡ `time.gmtime([secs])`
➡ `time.localtime([secs])`
➡ `time.mktime(t)`
➡ `time.monotonic() → float`
➡ `time.monotonic_ns() → int`
➡ `time.perf_counter() → float`
➡ `time.perf_counter_ns() → int`
➡ `time.process_time() → float`
➡ `time.process_time_ns() → int`
➡ `time.sleep(secs)`
➡ `time.strftime(format[, t])`
➡ `time.strptime(string[, format])`
➡ `time.time() → float`
➡ `time.time_ns() → int`
➡ `time.thread_time() → float`
➡ `time.thread_time_ns() → int`
➡ `time.tzset()`

➡ `time.asctime([t])`
Convert a tuple or struct_time representing a time as returned by gmtime() or localtime() to a string of the following form: 'Sun Jun 20 23:21:05 1993'. The day field is two characters long and is space padded if the day is a single digit, e.g.: 'Wed Jun  9 04:26:40 1993'.

If t is not provided, the current time as returned by localtime() is used. Locale information is not used by asctime().

Note:
 Unlike the C function of the same name, asctime() does not add a trailing newline.
 
➡ `time.pthread_getcpuclockid(thread_id)`
Return the clk_id of the thread-specific CPU-time clock for the specified thread_id.

Use threading.get_ident() or the ident attribute of threading.Thread objects to get a suitable value for thread_id.

Warning:
 Passing an invalid or expired thread_id may result in undefined behavior, such as segmentation fault.
 

Availability: Unix (see the man page for pthread_getcpuclockid(3) for further information).

New in version 3.7.

➡ `time.clock_getres(clk_id)`
Return the resolution (precision) of the specified clock clk_id. Refer to Clock ID Constants for a list of accepted values for clk_id.

Availability: Unix.

New in version 3.3.


➡ `time.clock_gettime(clk_id) → float`
Return the time of the specified clock clk_id. Refer to Clock ID Constants for a list of accepted values for clk_id.

Use clock_gettime_ns() to avoid the precision loss caused by the float type.

Availability: Unix.

New in version 3.3.


➡ `time.clock_gettime_ns(clk_id) → int`
Similar to clock_gettime() but return time as nanoseconds.

Availability: Unix.

New in version 3.7.

➡ `time.clock_settime(clk_id, time: float)`
Set the time of the specified clock clk_id. Currently, CLOCK_REALTIME is the only accepted value for clk_id.

Use clock_settime_ns() to avoid the precision loss caused by the float type.

Availability: Unix.

New in version 3.3.

➡ `time.clock_settime_ns(clk_id, time: int)`
Similar to clock_settime() but set time with nanoseconds.

Availability: Unix.

New in version 3.7.

➡ `time.ctime([secs])`
Convert a time expressed in seconds since the epoch to a string of a form: 'Sun Jun 20 23:21:05 1993' representing local time. The day field is two characters long and is space padded if the day is a single digit, e.g.: 'Wed Jun  9 04:26:40 1993'.

If secs is not provided or None, the current time as returned by time() is used. ctime(secs) is equivalent to asctime(localtime(secs)). Locale information is not used by ctime().

➡ `time.get_clock_info(name)`
Get information on the specified clock as a namespace object. Supported clock names and the corresponding functions to read their value are:

• 'monotonic': time.monotonic()
• 'perf_counter': time.perf_counter()
• 'process_time': time.process_time()
• 'thread_time': time.thread_time()
• 'time': time.time()

The result has the following attributes:

• adjustable: True if the clock can be changed automatically (e.g. by a NTP daemon) or manually by the system administrator, False otherwise

• implementation: The name of the underlying C function used to get the clock value. Refer to Clock ID Constants for possible values.

• monotonic: True if the clock cannot go backward, False otherwise

• resolution: The resolution of the clock in seconds (float)

New in version 3.3.

➡ `time.gmtime([secs])`
Convert a time expressed in seconds since the epoch to a struct_time in UTC in which the dst flag is always zero. If secs is not provided or None, the current time as returned by time() is used. Fractions of a second are ignored. See above for a description of the struct_time object. See calendar.timegm() for the inverse of this function.

➡ `time.localtime([secs])`
Like gmtime() but converts to local time. If secs is not provided or None, the current time as returned by time() is used. The dst flag is set to 1 when DST applies to the given time.

➡ `time.mktime(t)`
This is the inverse function of localtime(). Its argument is the struct_time or full 9-tuple (since the dst flag is needed; use -1 as the dst flag if it is unknown) which expresses the time in local time, not UTC. It returns a floating point number, for compatibility with time(). If the input value cannot be represented as a valid time, either OverflowError or ValueError will be raised (which depends on whether the invalid value is caught by Python or the underlying C libraries). The earliest date for which it can generate a time is platform-dependent.

➡ `time.monotonic() → float`
Return the value (in fractional seconds) of a monotonic clock, i.e. a clock that cannot go backwards. The clock is not affected by system clock updates. The reference point of the returned value is undefined, so that only the difference between the results of two calls is valid.

Use monotonic_ns() to avoid the precision loss caused by the float type.

New in version 3.3.



Changed in version 3.5: The function is now always available and always system-wide.


Changed in version 3.10: On macOS, the function is now system-wide.

➡ `time.monotonic_ns() → int`
Similar to monotonic(), but return time as nanoseconds.

New in version 3.7.


➡ `time.perf_counter() → float`
Return the value (in fractional seconds) of a performance counter, i.e. a clock with the highest available resolution to measure a short duration. It does include time elapsed during sleep and is system-wide. The reference point of the returned value is undefined, so that only the difference between the results of two calls is valid.

Use perf_counter_ns() to avoid the precision loss caused by the float type.

New in version 3.3.

Changed in version 3.10: On Windows, the function is now system-wide.

➡ `time.perf_counter_ns() → int`
Similar to perf_counter(), but return time as nanoseconds.

New in version 3.7.


➡ `time.process_time() → float`
Return the value (in fractional seconds) of the sum of the system and user CPU time of the current process. It does not include time elapsed during sleep. It is process-wide by definition. The reference point of the returned value is undefined, so that only the difference between the results of two calls is valid.

Use process_time_ns() to avoid the precision loss caused by the float type.

New in version 3.3.


➡ `time.process_time_ns() → int`
Similar to process_time() but return time as nanoseconds.

New in version 3.7.

➡ `time.sleep(secs)`
Suspend execution of the calling thread for the given number of seconds. The argument may be a floating point number to indicate a more precise sleep time. The actual suspension time may be less than that requested because any caught signal will terminate the sleep() following execution of that signal’s catching routine. Also, the suspension time may be longer than requested by an arbitrary amount because of the scheduling of other activity in the system.

Changed in version 3.5: The function now sleeps at least secs even if the sleep is interrupted by a signal, except if the signal handler raises an exception (see PEP 475 for the rationale).

➡ `time.strftime(format[, t])`
Convert a tuple or struct_time representing a time as returned by gmtime() or localtime() to a string as specified by the format argument. If t is not provided, the current time as returned by localtime() is used. format must be a string. ValueError is raised if any field in t is outside of the allowed range.

0 is a legal argument for any position in the time tuple; if it is normally illegal the value is forced to a correct one.

The following directives can be embedded in the format string. They are shown without the optional field width and precision specification, and are replaced by the indicated characters in the strftime() result:


| Directive |              Meaning               |
|-----------|------------------------------------|
| %a  | Locale’s abbreviated weekday name. 
| %A  | Locale’s full weekday name.   
| %b  | Locale’s abbreviated month name.   
| %B  | Locale’s full month name.   
| %c  | Locale’s appropriate date and time representation.   
| %d  | Day of the month as a decimal number [01,31].   
| %H  | Hour (24-hour clock) as a decimal number [00,23].   
| %I  | Hour (12-hour clock) as a decimal number [01,12].   
| %j  | Day of the year as a decimal number [001,366].   
| %m  | Month as a decimal number [01,12].   
| %M  | Minute as a decimal number [00,59].   
| %p  | Locale’s equivalent of either AM or PM. (1) 
| %S  | Second as a decimal number [00,61]. (2) 
| %U  | Week number of the year (Sunday as the first day of the week) as a decimal number [00,53]. All days in a new year preceding the first Sunday are considered to be in week 0. (3) 
| %w  | Weekday as a decimal number [0(Sunday),6].   
| %W  | Week number of the year (Monday as the first day of the week) as a decimal number [00,53]. All days in a new year preceding the first Monday are considered to be in week 0. (3) 
| %x  | Locale’s appropriate date representation.   
| %X  | Locale’s appropriate time representation.   
| %y  | Year without century as a decimal number [00,99].   
| %Y  | Year with century as a decimal number.   
| %z  | Time zone offset indicating a positive or negative time difference from UTC/GMT of the form +HHMM or -HHMM, where H represents decimal hour digits and M represents decimal minute digits [-23:59, +23:59].   
| %Z  | Time zone name (no characters if no time zone exists).   
| %%  | A literal '%' character.   

Notes:

(1). When used with the strptime() function, the %p directive only affects the output hour field if the %I directive is used to parse the hour.
(2). The range really is 0 to 61; value 60 is valid in timestamps representing leap seconds and value 61 is supported for historical reasons.
(3). When used with the strptime() function, %U and %W are only used in calculations when the day of the week and the year are specified.

Here is an example, a format for dates compatible with that specified in the RFC 2822 Internet email standard. [1]


>>> from time import gmtime, strftime
>>> strftime("%a, %d %b %Y %H:%M:%S +0000", gmtime())
'Thu, 28 Jun 2001 14:17:15 +0000'


Additional directives may be supported on certain platforms, but only the ones listed here have a meaning standardized by ANSI C. To see the full set of format codes supported on your platform, consult the strftime(3) documentation.

On some platforms, an optional field width and precision specification can immediately follow the initial '%' of a directive in the following order; this is also not portable. The field width is normally 2 except for %j where it is 3.

➡ `time.strptime(string[, format])`
Parse a string representing a time according to a format. The return value is a struct_time as returned by gmtime() or localtime().

The format parameter uses the same directives as those used by strftime(); it defaults to "%a %b %d %H:%M:%S %Y" which matches the formatting returned by ctime(). If string cannot be parsed according to format, or if it has excess data after parsing, ValueError is raised. The default values used to fill in any missing data when more accurate values cannot be inferred are (1900, 1, 1, 0, 0, 0, 0, 1, -1). Both string and format must be strings.

For example:


```py
import time
time.strptime("30 Nov 00", "%d %b %y")   

time.struct_time(tm_year=2000, tm_mon=11, tm_mday=30, tm_hour=0, tm_min=0,
                 tm_sec=0, tm_wday=3, tm_yday=335, tm_isdst=-1)
```


Support for the %Z directive is based on the values contained in tzname and whether daylight is true. Because of this, it is platform-specific except for recognizing UTC and GMT which are always known (and are considered to be non-daylight savings timezones).

Only the directives specified in the documentation are supported. Because strftime() is implemented per platform it can sometimes offer more directives than those listed. But strptime() is independent of any platform and thus does not necessarily support all directives available that are not documented as supported.
class time.struct_time
The type of the time value sequence returned by gmtime(), localtime(), and strptime(). It is an object with a named tuple interface: values can be accessed by index and by attribute name. The following values are present:


| Index | Attribute |                      Values                      |
|-------|-----------|--------------------------------------------------|
| 0     | tm_year   | (for example, 1993)                              |
| 1     | tm_mon    | range [1, 12]                                    |
| 2     | tm_mday   | range [1, 31]                                    |
| 3     | tm_hour   | range [0, 23]                                    |
| 4     | tm_min    | range [0, 59]                                    |
| 5     | tm_sec    | range [0, 61]; see (2) in strftime() description |
| 6     | tm_wday   | range [0, 6], Monday is 0                        |
| 7     | tm_yday   | range [1, 366]                                   |
| 8     | tm_isdst  | 0, 1 or -1; see below                            |
| N/A   | tm_zone   | abbreviation of timezone name                    |
| N/A   | tm_gmtoff | offset east of UTC in seconds                    |

Note that unlike the C structure, the month value is a range of [1, 12], not [0, 11].

In calls to mktime(), tm_isdst may be set to 1 when daylight savings time is in effect, and 0 when it is not. A value of -1 indicates that this is not known, and will usually result in the correct state being filled in.

When a tuple with an incorrect length is passed to a function expecting a struct_time, or having elements of the wrong type, a TypeError is raised.

➡ `time.time() → float`
Return the time in seconds since the epoch as a floating point number. The specific date of the epoch and the handling of leap seconds is platform dependent. On Windows and most Unix systems, the epoch is January 1, 1970, 00:00:00 (UTC) and leap seconds are not counted towards the time in seconds since the epoch. This is commonly referred to as Unix time. To find out what the epoch is on a given platform, look at gmtime(0).

Note that even though the time is always returned as a floating point number, not all systems provide time with a better precision than 1 second. While this function normally returns non-decreasing values, it can return a lower value than a previous call if the system clock has been set back between the two calls.

The number returned by time() may be converted into a more common time format (i.e. year, month, day, hour, etc…) in UTC by passing it to gmtime() function or in local time by passing it to the localtime() function. In both cases a struct_time object is returned, from which the components of the calendar date may be accessed as attributes.

Use time_ns() to avoid the precision loss caused by the float type.

➡ `time.time_ns() → int`
Similar to time() but returns time as an integer number of nanoseconds since the epoch.

New in version 3.7.


➡ `time.thread_time() → float`
Return the value (in fractional seconds) of the sum of the system and user CPU time of the current thread. It does not include time elapsed during sleep. It is thread-specific by definition. The reference point of the returned value is undefined, so that only the difference between the results of two calls in the same thread is valid.

Use thread_time_ns() to avoid the precision loss caused by the float type.

Availability: Windows, Linux, Unix systems supporting CLOCK_THREAD_CPUTIME_ID.

New in version 3.7.


➡ `time.thread_time_ns() → int`
Similar to thread_time() but return time as nanoseconds.

New in version 3.7.

➡ `time.tzset()`
Reset the time conversion rules used by the library routines. The environment variable TZ specifies how this is done. It will also set the variables tzname (from the TZ environment variable), timezone (non-DST seconds West of UTC), altzone (DST seconds west of UTC) and daylight (to 0 if this timezone does not have any daylight saving time rules, or to nonzero if there is a time, past, present or future when daylight saving time applies).

Availability: Unix.

Note:
 Although in many cases, changing the TZ environment variable may affect the output of functions like localtime() without calling tzset(), this behavior should not be relied on.
 
The TZ environment variable should contain no whitespace.

The standard format of the TZ environment variable is (whitespace added for clarity):


    std offset [dst [offset [,start[/time], end[/time]]]]


Where the components are:


`std` and `dst`Three or more alphanumerics giving the timezone abbreviations. These will be propagated into time.tzname

`offset`The offset has the form: ± hh[:mm[:ss]]. This indicates the value added the local time to arrive at UTC. If preceded by a ‘-’, the timezone is east of the Prime Meridian; otherwise, it is west. If no offset follows dst, summer time is assumed to be one hour ahead of standard time.

`start[/time], end[/time]` Indicates when to change to and back from DST. The format of the start and end dates are one of the following:


`Jn` The Julian day n (1 <= n <= 365). Leap days are not counted, so in all years February 28 is day 59 and March 1 is day 60.

`n` The zero-based Julian day (0 <= n <= 365). Leap days are counted, and it is possible to refer to February 29.

`Mm.n.d` The d’th day (0 <= d <= 6) of week n of month m of the year (1 <= n <= 5, 1 <= m <= 12, where week 5 means “the last d day in month m” which may occur in either the fourth or the fifth week). Week 1 is the first week in which the d’th day occurs. Day zero is a Sunday.

`time` has the same format as offset except that no leading sign (‘-’ or ‘+’) is allowed. The default, if time is not given, is 02:00:00.


>>> os.environ['TZ'] = 'EST+05EDT,M4.1.0,M10.5.0'
>>> time.tzset()
>>> time.strftime('%X %x %Z')
'02:07:36 05/08/03 EDT'
>>> os.environ['TZ'] = 'AEST-10AEDT-11,M10.5.0,M3.5.0'
>>> time.tzset()
>>> time.strftime('%X %x %Z')
'16:08:12 05/08/03 AEST'


On many Unix systems (including * BSD, Linux, Solaris, and Darwin), it is more convenient to use the system’s zoneinfo (tzfile(5)) database to specify the timezone rules. To do this, set the TZ environment variable to the path of the required timezone datafile, relative to the root of the systems ‘zoneinfo’ timezone database, usually located at /usr/share/zoneinfo. For example, 'US/Eastern', 'Australia/Melbourne', 'Egypt' or 'Europe/Amsterdam'.


>>> os.environ['TZ'] = 'US/Eastern'
>>> time.tzset()
>>> time.tzname
('EST', 'EDT')
>>> os.environ['TZ'] = 'Egypt'
>>> time.tzset()
>>> time.tzname
('EET', 'EEST')



### ===🗝 ◦ Clock ID Constants

These constants are used as parameters for clock_getres() and clock_gettime().

➡ `time.CLOCK_BOOTTIME`
Identical to CLOCK_MONOTONIC, except it also includes any time that the system is suspended.

This allows applications to get a suspend-aware monotonic clock without having to deal with the complications of CLOCK_REALTIME, which may have discontinuities if the time is changed using settimeofday() or similar.

Availability: Linux 2.6.39 or later.

New in version 3.7.


➡ `time.CLOCK_HIGHRES`
The Solaris OS has a CLOCK_HIGHRES timer that attempts to use an optimal hardware source, and may give close to nanosecond resolution. CLOCK_HIGHRES is the nonadjustable, high-resolution clock.

Availability: Solaris.

New in version 3.3.


➡ `time.CLOCK_MONOTONIC`
Clock that cannot be set and represents monotonic time since some unspecified starting point.

Availability: Unix.

New in version 3.3.


➡ `time.CLOCK_MONOTONIC_RAW`
Similar to CLOCK_MONOTONIC, but provides access to a raw hardware-based time that is not subject to NTP adjustments.

Availability: Linux 2.6.28 and newer, macOS 10.12 and newer.

New in version 3.3.


➡ `time.CLOCK_PROCESS_CPUTIME_ID`
High-resolution per-process timer from the CPU.

Availability: Unix.

New in version 3.3.


➡ `time.CLOCK_PROF`
High-resolution per-process timer from the CPU.

Availability: FreeBSD, NetBSD 7 or later, OpenBSD.

New in version 3.7.


➡ `time.CLOCK_TAI`
International Atomic Time

The system must have a current leap second table in order for this to give the correct answer. PTP or NTP software can maintain a leap second table.

Availability: Linux.

New in version 3.9.


➡ `time.CLOCK_THREAD_CPUTIME_ID`
Thread-specific CPU-time clock.

Availability: Unix.

New in version 3.3.


➡ `time.CLOCK_UPTIME`
Time whose absolute value is the time the system has been running and not suspended, providing accurate uptime measurement, both absolute and interval.

Availability: FreeBSD, OpenBSD 5.5 or later.

New in version 3.7.


➡ `time.CLOCK_UPTIME_RAW`
Clock that increments monotonically, tracking the time since an arbitrary point, unaffected by frequency or time adjustments and not incremented while the system is asleep.

Availability: macOS 10.12 and newer.

New in version 3.8.


The following constant is the only parameter that can be sent to clock_settime().

➡ `time.CLOCK_REALTIME`
System-wide real-time clock. Setting this clock requires appropriate privileges.

Availability: Unix.

New in version 3.3.




### ===🗝 ◦ Timezone Constants

➡ `time.altzone`
The offset of the local DST timezone, in seconds west of UTC, if one is defined. This is negative if the local DST timezone is east of UTC (as in Western Europe, including the UK). Only use this if daylight is nonzero. See note below.

➡ `time.daylight`
Nonzero if a DST timezone is defined. See note below.

➡ `time.timezone`
The offset of the local (non-DST) timezone, in seconds west of UTC (negative in most of Western Europe, positive in the US, zero in the UK). See note below.

➡ `time.tzname`
A tuple of two strings: the first is the name of the local non-DST timezone, the second is the name of the local DST timezone. If no DST timezone is defined, the second string should not be used. See note below.

Note:
 For the above Timezone constants (altzone, daylight, timezone, and tzname), the value is determined by the timezone rules in effect at module load time or the last time tzset() is called and may be incorrect for times in the past. It is recommended to use the tm_gmtoff and tm_zone results from localtime() to obtain timezone information.
 

See also:
 Module datetimeMore object-oriented interface to dates and times.Module localeInternationalization services. The locale setting affects the interpretation of many format specifiers in strftime() and strptime().Module calendarGeneral calendar-related functions. timegm() is the inverse of gmtime() from this module.
Footnotes



[1] The use of %Z is now deprecated, but the %z escape that expands to the preferred hour/minute offset is not supported by all ANSI C libraries. Also, a strict reading of the original 1982 RFC 822 standard calls for a two-digit year (%y rather than %Y), but practice moved to 4-digit years long before the year 2000. After that, RFC 822 became obsolete and the 4-digit year has been first recommended by RFC 1123 and then mandated by RFC 2822. 



## ==⚡ • argparse — Parser for command-line options, arguments and sub-commands
◦  Example
◾Creating a parser
◾Adding arguments
◾Parsing arguments

◦ ArgumentParser objects
◾prog
◾usage
◾description
◾epilog
◾parents
◾formatter_class
◾prefix_chars
◾fromfile_prefix_chars
◾argument_default
◾allow_abbrev
◾conflict_handler
◾add_help
◾exit_on_error

◦ The add_argument() method
◾name or flags
◾action
◾nargs
◾const
◾default
◾type
◾choices
◾required
◾help
◾metavar
◾dest
◾Action classes

◦ The parse_args() method
◾Option value syntax
◾Invalid arguments
◾Arguments containing -
◾Argument abbreviations (prefix matching)
◾Beyond sys.argv
◾The Namespace object

◦ Other utilities
◾Sub-commands
◾FileType objects
◾Argument groups
◾Mutual exclusion
◾Parser defaults
◾Printing help
◾Partial parsing
◾Customizing file parsing
◾Exiting methods
◾Intermixed parsing

◦ Upgrading optparse code

## ==⚡ • getopt — C-style parser for command line options
## ==⚡ • logging — Logging facility for Python
◦  Logger Objects
◦ Logging Levels
◦ Handler Objects
◦ Formatter Objects
◦ Filter Objects
◦ LogRecord Objects
◦ LogRecord attributes
◦ LoggerAdapter Objects
◦ Thread Safety
◦ Module-Level Functions
◦ Module-Level Attributes
◦ Integration with the warnings module

## ==⚡ • logging.config — Logging configuration
◦  Configuration functions
◦ Configuration dictionary schema
◾Dictionary Schema Details
◾Incremental Configuration
◾Object connections
◾User-defined objects
◾Access to external objects
◾Access to internal objects
◾Import resolution and custom importers

◦ Configuration file format

## ==⚡ • logging.handlers — Logging handlers
◦  StreamHandler
◦ FileHandler
◦ NullHandler
◦ WatchedFileHandler
◦ BaseRotatingHandler
◦ RotatingFileHandler
◦ TimedRotatingFileHandler
◦ SocketHandler
◦ DatagramHandler
◦ SysLogHandler
◦ NTEventLogHandler
◦ SMTPHandler
◦ MemoryHandler
◦ HTTPHandler
◦ QueueHandler
◦ QueueListener

## ==⚡ • getpass — Portable password input
## ==⚡ • curses — Terminal handling for character-cell displays
◦  Functions
◦ Window Objects
◦ Constants

## ==⚡ • curses.textpad — Text input widget for curses programs
◦  Textbox objects

## ==⚡ • curses.ascii — Utilities for ASCII characters
## ==⚡ • curses.panel — A panel stack extension for curses
◦  Functions
◦ Panel Objects

## ==⚡ • platform — Access to underlying platform’s identifying data
◦  Cross Platform
◦ Java Platform
◦ Windows Platform
◦ Mac OS Platform
◦ Unix Platforms

## ==⚡ • errno — Standard errno system symbols
## ==⚡ • ctypes — A foreign function library for Python
◦  ctypes tutorial
◾Loading dynamic link libraries
◾Accessing functions from loaded dlls
◾Calling functions
◾Fundamental data types
◾Calling functions, continued
◾Calling functions with your own custom data types
◾Specifying the required argument types (function prototypes)
◾Return types
◾Passing pointers (or: passing parameters by reference)
◾Structures and unions
◾Structure/union alignment and byte order
◾Bit fields in structures and unions
◾Arrays
◾Pointers
◾Type conversions
◾Incomplete Types
◾Callback functions
◾Accessing values exported from dlls
◾Surprises
◾Variable-sized data types

◦ ctypes reference
◾Finding shared libraries
◾Loading shared libraries
◾Foreign functions
◾Function prototypes
◾Utility functions
◾Data types
◾Fundamental data types
◾Structured data types
◾Arrays and pointers


# =🚩 Concurrent Execution
- https://docs.python.org/3.9/library/concurrency.html

- `threading` — Thread-based parallelism
- `multiprocessing` — Process-based parallelism
- `multiprocessing`.shared_memory — Provides shared memory for direct access across processes
- `The` concurrent package
- `concurrent`.futures — Launching parallel tasks
- `subprocess` — Subprocess management
- `sched` — Event scheduler
- `queue` — A synchronized queue class
- `contextvars` — Context Variables
- `_thread` — Low-level threading API

The modules described in this chapter provide support for concurrent execution of code. The appropriate choice of tool will depend on the task to be executed (CPU bound vs IO bound) and preferred style of development (event driven cooperative multitasking vs preemptive multitasking). Here’s an overview:

➡ `• threading — Thread-based parallelism`
◦ Thread-Local Data
◦ Thread Objects
◦ Lock Objects
◦ RLock Objects
◦ Condition Objects
◦ Semaphore Objects
◾ Semaphore Example

◦ Event Objects
◦ Timer Objects
◦ Barrier Objects
◦ Using locks, conditions, and semaphores in the with statement

➡ `• multiprocessing — Process-based parallelism`
◦ Introduction
◾ The Process class
◾ Contexts and start methods
◾ Exchanging objects between processes
◾ Synchronization between processes
◾ Sharing state between processes
◾ Using a pool of workers

◦ Reference
◾ Process and exceptions
◾ Pipes and Queues
◾ Miscellaneous
◾ Connection Objects
◾ Synchronization primitives
◾ Shared ctypes Objects
◾ The multiprocessing.sharedctypes module

◾ Managers
◾ Customized managers
◾ Using a remote manager

◾ Proxy Objects
◾ Cleanup

◾ Process Pools
◾ Listeners and Clients
◾ Address Formats

◾ Authentication keys
◾ Logging
◾ The multiprocessing.dummy module

◦ Programming guidelines
◾ All start methods
◾ The spawn and forkserver start methods

◦ Examples

➡ `• multiprocessing.shared_memory — Provides shared memory for direct access across processes`
➡ `• The concurrent package`
➡ `• concurrent.futures — Launching parallel tasks`
◦ Executor Objects
◦ ThreadPoolExecutor
◾ ThreadPoolExecutor Example

◦ ProcessPoolExecutor
◾ ProcessPoolExecutor Example

◦ Future Objects
◦ Module Functions
◦ Exception classes

➡ `• subprocess — Subprocess management`
◦ Using the subprocess Module
◾ Frequently Used Arguments
◾ Popen Constructor
◾ Exceptions

◦ Security Considerations
◦ Popen Objects
◦ Windows Popen Helpers
◾ Windows Constants

◦ Older high-level API
◦ Replacing Older Functions with the subprocess Module
◾ Replacing /bin/sh shell command substitution
◾ Replacing shell pipeline
◾ Replacing os.system()
◾ Replacing the os.spawn family
◾ Replacing os.popen(), os.popen2(), os.popen3()
◾ Replacing functions from the popen2 module

◦ Legacy Shell Invocation Functions
◦ Notes
◾ Converting an argument sequence to a string on Windows


➡ `• sched — Event scheduler`
◦ Scheduler Objects

➡ `• queue — A synchronized queue class`
◦ Queue Objects
◦ SimpleQueue Objects

➡ `• contextvars — Context Variables`
◦ Context Variables
◦ Manual Context Management
◦ asyncio support


The following are support modules for some of the above services:

➡ `• _thread — Low-level threading API`

## ==⚡ • threading — Thread-based parallelism

Source code: Lib/threading.py

➡ `threading.active_count()`
➡ `threading.current_thread()`
➡ `threading.excepthook(args, /)`
➡ `threading.__excepthook__`
➡ `threading.get_ident()`
➡ `threading.get_native_id()`
➡ `threading.enumerate()`
➡ `threading.main_thread()`
➡ `threading.settrace(func)`
➡ `threading.gettrace()`
➡ `threading.setprofile(func)`
➡ `threading.getprofile()`
➡ `threading.stack_size([size])`
➡ `threading.TIMEOUT_MAX`

This module constructs higher-level threading interfaces on top of the lower level `_thread` module. See also the queue module.


Changed in version 3.7: This module used to be optional, it is now always available.

Note:
 In the Python 2.x series, this module contained camelCase names for some methods and functions. These are deprecated as of Python 3.10, but they are still supported for compatibility with Python 2.5 and lower.
 

CPython implementation detail: In CPython, due to the Global Interpreter Lock, only one thread can execute Python code at once (even though certain performance-oriented libraries might overcome this limitation). If you want your application to make better use of the computational resources of multi-core machines, you are advised to use multiprocessing or concurrent.futures.ProcessPoolExecutor. However, threading is still an appropriate model if you want to run multiple I/O-bound tasks simultaneously.

This module defines the following functions:

➡ `threading.active_count()`
Return the number of Thread objects currently alive. The returned count is equal to the length of the list returned by enumerate().

The function activeCount is a deprecated alias for this function.

➡ `threading.current_thread()`
Return the current Thread object, corresponding to the caller’s thread of control. If the caller’s thread of control was not created through the threading module, a dummy thread object with limited functionality is returned.

The function currentThread is a deprecated alias for this function.

➡ `threading.excepthook(args, /)`
Handle uncaught exception raised by Thread.run().

The args argument has the following attributes:
•exc_type: Exception type.
•exc_value: Exception value, can be None.
•exc_traceback: Exception traceback, can be None.
•thread: Thread which raised the exception, can be None.

If exc_type is SystemExit, the exception is silently ignored. Otherwise, the exception is printed out on sys.stderr.

If this function raises an exception, sys.excepthook() is called to handle it.

threading.excepthook() can be overridden to control how uncaught exceptions raised by Thread.run() are handled.

Storing exc_value using a custom hook can create a reference cycle. It should be cleared explicitly to break the reference cycle when the exception is no longer needed.

Storing thread using a custom hook can resurrect it if it is set to an object which is being finalized. Avoid storing thread after the custom hook completes to avoid resurrecting objects.

See also:
 sys.excepthook() handles uncaught exceptions.
 

New in version 3.8.


➡ `threading.__excepthook__`
Holds the original value of threading.excepthook(). It is saved so that the original value can be restored in case they happen to get replaced with broken or alternative objects.

New in version 3.10.


➡ `threading.get_ident()`
Return the ‘thread identifier’ of the current thread. This is a nonzero integer. Its value has no direct meaning; it is intended as a magic cookie to be used e.g. to index a dictionary of thread-specific data. Thread identifiers may be recycled when a thread exits and another thread is created.

New in version 3.3.


➡ `threading.get_native_id()`
Return the native integral Thread ID of the current thread assigned by the kernel. This is a non-negative integer. Its value may be used to uniquely identify this particular thread system-wide (until the thread terminates, after which the value may be recycled by the OS).

Availability: Windows, FreeBSD, Linux, macOS, OpenBSD, NetBSD, AIX.

New in version 3.8.


➡ `threading.enumerate()`
Return a list of all Thread objects currently active. The list includes daemonic threads and dummy thread objects created by current_thread(). It excludes terminated threads and threads that have not yet been started. However, the main thread is always part of the result, even when terminated.

➡ `threading.main_thread()`
Return the main Thread object. In normal conditions, the main thread is the thread from which the Python interpreter was started.

New in version 3.4.


➡ `threading.settrace(func)`
Set a trace function for all threads started from the threading module. The func will be passed to sys.settrace() for each thread, before its run() method is called.

➡ `threading.gettrace()`
Get the trace function as set by settrace().

New in version 3.10.


➡ `threading.setprofile(func)`
Set a profile function for all threads started from the threading module. The func will be passed to sys.setprofile() for each thread, before its run() method is called.

➡ `threading.getprofile()`
Get the profiler function as set by setprofile().

New in version 3.10.


➡ `threading.stack_size([size])`
Return the thread stack size used when creating new threads. The optional size argument specifies the stack size to be used for subsequently created threads, and must be 0 (use platform or configured default) or a positive integer value of at least 32,768 (32 KiB). If size is not specified, 0 is used. If changing the thread stack size is unsupported, a RuntimeError is raised. If the specified stack size is invalid, a ValueError is raised and the stack size is unmodified. 32 KiB is currently the minimum supported stack size value to guarantee sufficient stack space for the interpreter itself. Note that some platforms may have particular restrictions on values for the stack size, such as requiring a minimum stack size > 32 KiB or requiring allocation in multiples of the system memory page size - platform documentation should be referred to for more information (4 KiB pages are common; using multiples of 4096 for the stack size is the suggested approach in the absence of more specific information).

Availability: Windows, systems with POSIX threads.

This module also defines the following constant:

➡ `threading.TIMEOUT_MAX`
The maximum value allowed for the timeout parameter of blocking functions (Lock.acquire(), RLock.acquire(), Condition.wait(), etc.). Specifying a timeout greater than this value will raise an OverflowError.

New in version 3.2.


This module defines a number of classes, which are detailed in the sections below.

The design of this module is loosely based on Java’s threading model. However, where Java makes locks and condition variables basic behavior of every object, they are separate objects in Python. Python’s Thread class supports a subset of the behavior of Java’s Thread class; currently, there are no priorities, no thread groups, and threads cannot be destroyed, stopped, suspended, resumed, or interrupted. The static methods of Java’s Thread class, when implemented, are mapped to module-level functions.

All of the methods described below are executed atomically.



### ===🗝 ◦ Thread-Local Data

    class threading.local

Thread-local data is data whose values are thread specific. To manage thread-local data, just create an instance of local (or a subclass) and store attributes on it:


```py
mydata = threading.local()
mydata.x = 1
```

The instance’s values will be different for separate threads.

✅ `class threading.local`
A class that represents thread-local data.

For more details and extensive examples, see the documentation string of the `_threading_local` module.



### ===🗝 ◦ Thread Objects

    class threading.Thread(group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None)

    Thread.start()
    Thread.run()
    Thread.join(timeout=None)
    Thread.name
    Thread.getName()
    Thread.setName()
    Thread.ident
    Thread.is_alive()
    Thread.daemon
    Thread.isDaemon()
    Thread.setDaemon()

The Thread class represents an activity that is run in a separate thread of control. There are two ways to specify the activity: by passing a callable object to the constructor, or by overriding the run() method in a subclass. No other methods (except for the constructor) should be overridden in a subclass. In other words, only override the __init__() and run() methods of this class.

Once a thread object is created, its activity must be started by calling the thread’s start() method. This invokes the run() method in a separate thread of control.

Once the thread’s activity is started, the thread is considered ‘alive’. It stops being alive when its run() method terminates – either normally, or by raising an unhandled exception. The is_alive() method tests whether the thread is alive.

Other threads can call a thread’s join() method. This blocks the calling thread until the thread whose join() method is called is terminated.

A thread has a name. The name can be passed to the constructor, and read or changed through the name attribute.

If the run() method raises an exception, threading.excepthook() is called to handle it. By default, threading.excepthook() ignores silently SystemExit.

A thread can be flagged as a “daemon thread”. The significance of this flag is that the entire Python program exits when only daemon threads are left. The initial value is inherited from the creating thread. The flag can be set through the daemon property or the daemon constructor argument.

Note:
 Daemon threads are abruptly stopped at shutdown. Their resources (such as open files, database transactions, etc.) may not be released properly. If you want your threads to stop gracefully, make them non-daemonic and use a suitable signalling mechanism such as an Event.
 

There is a “main thread” object; this corresponds to the initial thread of control in the Python program. It is not a daemon thread.

There is the possibility that “dummy thread objects” are created. These are thread objects corresponding to “alien threads”, which are threads of control started outside the threading module, such as directly from C code. Dummy thread objects have limited functionality; they are always considered alive and daemonic, and cannot be join()ed. They are never deleted, since it is impossible to detect the termination of alien threads.

✅ `class threading.Thread(group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None)`
This constructor should always be called with keyword arguments. Arguments are:

➡↪`group` should be None; reserved for future extension when a ThreadGroup class is implemented.

➡↪`target` is the callable object to be invoked by the run() method. Defaults to None, meaning nothing is called.

➡↪`name` is the thread name. By default, a unique name is constructed of the form “Thread-N” where N is a small decimal number, or “Thread-N (target)” where “target” is target.__name__ if the target argument is specified.

➡↪`args` is the argument tuple for the target invocation. Defaults to ().

➡↪`kwargs` is a dictionary of keyword arguments for the target invocation. Defaults to {}.

If not None, daemon explicitly sets whether the thread is daemonic. If None (the default), the daemonic property is inherited from the current thread.

If the subclass overrides the constructor, it must make sure to invoke the base class constructor (Thread.__init__()) before doing anything else to the thread.


Changed in version 3.10: Use the target name if name argument is omitted.


Changed in version 3.3: Added the daemon argument.

➡ `start()`
Start the thread’s activity.

It must be called at most once per thread object. It arranges for the object’s run() method to be invoked in a separate thread of control.

This method will raise a RuntimeError if called more than once on the same thread object.

➡ `run()`
Method representing the thread’s activity.

You may override this method in a subclass. The standard run() method invokes the callable object passed to the object’s constructor as the target argument, if any, with positional and keyword arguments taken from the args and kwargs arguments, respectively.

➡ `join(timeout=None)`
Wait until the thread terminates. This blocks the calling thread until the thread whose join() method is called terminates – either normally or through an unhandled exception – or until the optional timeout occurs.

When the timeout argument is present and not None, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). As join() always returns None, you must call is_alive() after join() to decide whether a timeout happened – if the thread is still alive, the join() call timed out.

When the timeout argument is not present or None, the operation will block until the thread terminates.

A thread can be join()ed many times.

join() raises a RuntimeError if an attempt is made to join the current thread as that would cause a deadlock. It is also an error to join() a thread before it has been started and attempts to do so raise the same exception.

➡ `name`
A string used for identification purposes only. It has no semantics. Multiple threads may be given the same name. The initial name is set by the constructor.

➡ `getName()`
➡ `setName()`
Deprecated getter/setter API for name; use it directly as a property instead.


Deprecated since version 3.10.

➡ `ident`
The ‘thread identifier’ of this thread or None if the thread has not been started. This is a nonzero integer. See the get_ident() function. Thread identifiers may be recycled when a thread exits and another thread is created. The identifier is available even after the thread has exited.

➡ `native_id`
The Thread ID (TID) of this thread, as assigned by the OS (kernel). This is a non-negative integer, or None if the thread has not been started. See the get_native_id() function. This value may be used to uniquely identify this particular thread system-wide (until the thread terminates, after which the value may be recycled by the OS).

Note:
 Similar to Process IDs, Thread IDs are only valid (guaranteed unique system-wide) from the time the thread is created until the thread has been terminated.
 

Availability: Requires get_native_id() function.

New in version 3.8.


➡ `is_alive()`
Return whether the thread is alive.

This method returns True just before the run() method starts until just after the run() method terminates. The module function enumerate() returns a list of all alive threads.

➡ `daemon`
A boolean value indicating whether this thread is a daemon thread (True) or not (False). This must be set before start() is called, otherwise RuntimeError is raised. Its initial value is inherited from the creating thread; the main thread is not a daemon thread and therefore all threads created in the main thread default to daemon = False.

The entire Python program exits when no alive non-daemon threads are left.

➡ `isDaemon()`
➡ `setDaemon()`
Deprecated getter/setter API for daemon; use it directly as a property instead.


Deprecated since version 3.10.


### ===🗝 ◦ Lock Objects

    class threading.Lock

    Lock.acquire(blocking=True, timeout=-1)
    Lock.release()

A `primitive lock` is a synchronization primitive that is not owned by a particular thread when locked. In Python, it is currently the lowest level synchronization primitive available, implemented directly by the `_thread` extension module.

A primitive lock is in one of two states, “locked” or “unlocked”. It is created in the unlocked state. It has two basic methods, acquire() and release(). When the state is unlocked, acquire() changes the state to locked and returns immediately. When the state is locked, acquire() blocks until a call to release() in another thread changes it to unlocked, then the acquire() call resets it to locked and returns. The release() method should only be called in the locked state; it changes the state to unlocked and returns immediately. If an attempt is made to release an unlocked lock, a RuntimeError will be raised.

Locks also support the context management protocol.

When more than one thread is blocked in acquire() waiting for the state to turn to unlocked, only one thread proceeds when a release() call resets the state to unlocked; which one of the waiting threads proceeds is not defined, and may vary across implementations.

All methods are executed atomically.

✅ `class threading.Lock`
The class implementing primitive lock objects. Once a thread has acquired a lock, subsequent attempts to acquire it block, until it is released; any thread may release it.

Note that Lock is actually a factory function which returns an instance of the most efficient version of the concrete Lock class that is supported by the platform.

➡ `acquire(blocking=True, timeout=-1)`
Acquire a lock, blocking or non-blocking.

When invoked with the blocking argument set to True (the default), block until the lock is unlocked, then set it to locked and return True.

When invoked with the blocking argument set to False, do not block. If a call with blocking set to True would block, return False immediately; otherwise, set the lock to locked and return True.

When invoked with the floating-point timeout argument set to a positive value, block for at most the number of seconds specified by timeout and as long as the lock cannot be acquired. A timeout argument of -1 specifies an unbounded wait. It is forbidden to specify a timeout when blocking is false.

The return value is True if the lock is acquired successfully, False if not (for example if the timeout expired).


Changed in version 3.2: The timeout parameter is new.


Changed in version 3.2: Lock acquisition can now be interrupted by signals on POSIX if the underlying threading implementation supports it.

➡ `release()`
Release a lock. This can be called from any thread, not only the thread which has acquired the lock.

When the lock is locked, reset it to unlocked, and return. If any other threads are blocked waiting for the lock to become unlocked, allow exactly one of them to proceed.

When invoked on an unlocked lock, a RuntimeError is raised.

There is no return value.

➡ `locked()`
Return true if the lock is acquired.



### ===🗝 ◦ RLock Objects

    class threading.RLock

    RLock.acquire(blocking=True, timeout=-1)
    RLock.release()

A `reentrant lock` is a synchronization primitive that may be acquired multiple times by the same thread. Internally, it uses the concepts of “owning thread” and “recursion level” in addition to the locked/unlocked state used by primitive locks. In the locked state, some thread owns the lock; in the unlocked state, no thread owns it.

To lock the lock, a thread calls its acquire() method; this returns once the thread owns the lock. To unlock the lock, a thread calls its release() method. acquire()/release() call pairs may be nested; only the final release() (the release() of the outermost pair) resets the lock to unlocked and allows another thread blocked in acquire() to proceed.

Reentrant locks also support the context management protocol.

✅ `class threading.RLock`
This class implements reentrant lock objects. A reentrant lock must be released by the thread that acquired it. Once a thread has acquired a reentrant lock, the same thread may acquire it again without blocking; the thread must release it once for each time it has acquired it.

Note that RLock is actually a factory function which returns an instance of the most efficient version of the concrete RLock class that is supported by the platform.

➡ `acquire(blocking=True, timeout=-1)`
Acquire a lock, blocking or non-blocking.

When invoked without arguments: if this thread already owns the lock, increment the recursion level by one, and return immediately. Otherwise, if another thread owns the lock, block until the lock is unlocked. Once the lock is unlocked (not owned by any thread), then grab ownership, set the recursion level to one, and return. If more than one thread is blocked waiting until the lock is unlocked, only one at a time will be able to grab ownership of the lock. There is no return value in this case.

When invoked with the blocking argument set to true, do the same thing as when called without arguments, and return True.

When invoked with the blocking argument set to false, do not block. If a call without an argument would block, return False immediately; otherwise, do the same thing as when called without arguments, and return True.

When invoked with the floating-point timeout argument set to a positive value, block for at most the number of seconds specified by timeout and as long as the lock cannot be acquired. Return True if the lock has been acquired, false if the timeout has elapsed.


Changed in version 3.2: The timeout parameter is new.

➡ `release()`
Release a lock, decrementing the recursion level. If after the decrement it is zero, reset the lock to unlocked (not owned by any thread), and if any other threads are blocked waiting for the lock to become unlocked, allow exactly one of them to proceed. If after the decrement the recursion level is still nonzero, the lock remains locked and owned by the calling thread.

Only call this method when the calling thread owns the lock. A RuntimeError is raised if this method is called when the lock is unlocked.

There is no return value.


### ===🗝 ◦ Condition Objects

    class threading.Condition(lock=None)

    Condition.acquire(*args)
    Condition.release()
    Condition.wait(timeout=None)
    Condition.wait_for(predicate, timeout=None)
    Condition.notify(n=1)
    Condition.notify_all()

A `condition variable` is always associated with some kind of lock; this can be passed in or one will be created by default. Passing one in is useful when several condition variables must share the same lock. The lock is part of the condition object: you don’t have to track it separately.

A condition variable obeys the context management protocol: using the with statement acquires the associated lock for the duration of the enclosed block. The acquire() and release() methods also call the corresponding methods of the associated lock.

Other methods must be called with the associated lock held. The wait() method releases the lock, and then blocks until another thread awakens it by calling notify() or notify_all(). Once awakened, wait() re-acquires the lock and returns. It is also possible to specify a timeout.

The notify() method wakes up one of the threads waiting for the condition variable, if any are waiting. The notify_all() method wakes up all threads waiting for the condition variable.

Note: the notify() and notify_all() methods don’t release the lock; this means that the thread or threads awakened will not return from their wait() call immediately, but only when the thread that called notify() or notify_all() finally relinquishes ownership of the lock.

The typical programming style using condition variables uses the lock to synchronize access to some shared state; threads that are interested in a particular change of state call wait() repeatedly until they see the desired state, while threads that modify the state call notify() or notify_all() when they change the state in such a way that it could possibly be a desired state for one of the waiters. For example, the following code is a generic producer-consumer situation with unlimited buffer capacity:


```py
# Consume one item
with cv:
    while not an_item_is_available():
        cv.wait()
    get_an_available_item()

# Produce one item
with cv:
    make_an_item_available()
    cv.notify()
```


The while loop checking for the application’s condition is necessary because wait() can return after an arbitrary long time, and the condition which prompted the notify() call may no longer hold true. This is inherent to multi-threaded programming. The wait_for() method can be used to automate the condition checking, and eases the computation of timeouts:


```py
# Consume an item
with cv:
    cv.wait_for(an_item_is_available)
    get_an_available_item()
```


To choose between notify() and notify_all(), consider whether one state change can be interesting for only one or several waiting threads. E.g. in a typical producer-consumer situation, adding one item to the buffer only needs to wake up one consumer thread.

✅ `class threading.Condition(lock=None)`
This class implements condition variable objects. A condition variable allows one or more threads to wait until they are notified by another thread.

If the lock argument is given and not None, it must be a Lock or RLock object, and it is used as the underlying lock. Otherwise, a new RLock object is created and used as the underlying lock.


Changed in version 3.3: changed from a factory function to a class.

➡ `acquire(*args)`
Acquire the underlying lock. This method calls the corresponding method on the underlying lock; the return value is whatever that method returns.

➡ `release()`
Release the underlying lock. This method calls the corresponding method on the underlying lock; there is no return value.

➡ `wait(timeout=None)`
Wait until notified or until a timeout occurs. If the calling thread has not acquired the lock when this method is called, a RuntimeError is raised.

This method releases the underlying lock, and then blocks until it is awakened by a notify() or notify_all() call for the same condition variable in another thread, or until the optional timeout occurs. Once awakened or timed out, it re-acquires the lock and returns.

When the timeout argument is present and not None, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof).

When the underlying lock is an RLock, it is not released using its release() method, since this may not actually unlock the lock when it was acquired multiple times recursively. Instead, an internal interface of the RLock class is used, which really unlocks it even when it has been recursively acquired several times. Another internal interface is then used to restore the recursion level when the lock is reacquired.

The return value is True unless a given timeout expired, in which case it is False.


Changed in version 3.2: Previously, the method always returned None.

➡ `wait_for(predicate, timeout=None)`
Wait until a condition evaluates to true. predicate should be a callable which result will be interpreted as a boolean value. A timeout may be provided giving the maximum time to wait.

This utility method may call wait() repeatedly until the predicate is satisfied, or until a timeout occurs. The return value is the last return value of the predicate and will evaluate to False if the method timed out.

Ignoring the timeout feature, calling this method is roughly equivalent to writing:


```py
while not predicate():
    cv.wait()
```


Therefore, the same rules apply as with wait(): The lock must be held when called and is re-acquired on return. The predicate is evaluated with the lock held.

New in version 3.2.


➡ `notify(n=1)`
By default, wake up one thread waiting on this condition, if any. If the calling thread has not acquired the lock when this method is called, a RuntimeError is raised.

This method wakes up at most n of the threads waiting for the condition variable; it is a no-op if no threads are waiting.

The current implementation wakes up exactly n threads, if at least n threads are waiting. However, it’s not safe to rely on this behavior. A future, optimized implementation may occasionally wake up more than n threads.

Note: an awakened thread does not actually return from its wait() call until it can reacquire the lock. Since notify() does not release the lock, its caller should.

➡ `notify_all()`
Wake up all threads waiting on this condition. This method acts like notify(), but wakes up all waiting threads instead of one. If the calling thread has not acquired the lock when this method is called, a RuntimeError is raised.

The method notifyAll is a deprecated alias for this method.


### ===🗝 ◦ Semaphore Objects ◾Semaphore Example

    class threading.Semaphore(value=1)

    Semaphore.acquire(blocking=True, timeout=None)
    Semaphore.release()

    class threading.BoundedSemaphore(value=1)

This is one of the oldest synchronization primitives in the history of computer science, invented by the early Dutch computer scientist Edsger W. Dijkstra (he used the names P() and V() instead of acquire() and release()).

A semaphore manages an internal counter which is decremented by each acquire() call and incremented by each release() call. The counter can never go below zero; when acquire() finds that it is zero, it blocks, waiting until some other thread calls release().

Semaphores also support the context management protocol.

✅ `class threading.Semaphore(value=1)`
This class implements semaphore objects. A semaphore manages an atomic counter representing the number of release() calls minus the number of acquire() calls, plus an initial value. The acquire() method blocks if necessary until it can return without making the counter negative. If not given, value defaults to 1.

The optional argument gives the initial value for the internal counter; it defaults to 1. If the value given is less than 0, ValueError is raised.


Changed in version 3.3: changed from a factory function to a class.

➡ `acquire(blocking=True, timeout=None)`
Acquire a semaphore.

When invoked without arguments:

• If the internal counter is larger than zero on entry, decrement it by one and return True immediately.

• If the internal counter is zero on entry, block until awoken by a call to release(). Once awoken (and the counter is greater than 0), decrement the counter by 1 and return True. Exactly one thread will be awoken by each call to release(). The order in which threads are awoken should not be relied on.


When invoked with blocking set to false, do not block. If a call without an argument would block, return False immediately; otherwise, do the same thing as when called without arguments, and return True.

When invoked with a timeout other than None, it will block for at most timeout seconds. If acquire does not complete successfully in that interval, return False. Return True otherwise.


Changed in version 3.2: The timeout parameter is new.

➡ `release(n=1)`
Release a semaphore, incrementing the internal counter by n. When it was zero on entry and other threads are waiting for it to become larger than zero again, wake up n of those threads.


Changed in version 3.9: Added the n parameter to release multiple waiting threads at once.

✅ `class threading.BoundedSemaphore(value=1)`
Class implementing bounded semaphore objects. A bounded semaphore checks to make sure its current value doesn’t exceed its initial value. If it does, ValueError is raised. In most situations semaphores are used to guard resources with limited capacity. If the semaphore is released too many times it’s a sign of a bug. If not given, value defaults to 1.


Changed in version 3.3: changed from a factory function to a class.


#### ◾ Semaphore Example

Semaphores are often used to guard resources with limited capacity, for example, a database server. In any situation where the size of the resource is fixed, you should use a bounded semaphore. Before spawning any worker threads, your main thread would initialize the semaphore:


```py
maxconnections = 5
# ...
pool_sema = BoundedSemaphore(value=maxconnections)
```


Once spawned, worker threads call the semaphore’s acquire and release methods when they need to connect to the server:


```py
with pool_sema:
    conn = connectdb()
    try:
        # ... use connection ...
    finally:
        conn.close()
```


The use of a bounded semaphore reduces the chance that a programming error which causes the semaphore to be released more than it’s acquired will go undetected.


### ===🗝 ◦ Event Objects

    class threading.Event

    Event.is_set()
    Event.set()
    Event.clear()
    Event.wait(timeout=None)

This is one of the simplest mechanisms for communication between threads: one thread signals an event and other threads wait for it.

An event object manages an internal flag that can be set to true with the set() method and reset to false with the clear() method. The wait() method blocks until the flag is true.

✅ `class threading.Event`
Class implementing event objects. An event manages a flag that can be set to true with the set() method and reset to false with the clear() method. The wait() method blocks until the flag is true. The flag is initially false.


Changed in version 3.3: changed from a factory function to a class.

➡ `is_set()`
Return True if and only if the internal flag is true.

The method isSet is a deprecated alias for this method.

➡ `set()`
Set the internal flag to true. All threads waiting for it to become true are awakened. Threads that call wait() once the flag is true will not block at all.

➡ `clear()`
Reset the internal flag to false. Subsequently, threads calling wait() will block until set() is called to set the internal flag to true again.

➡ `wait(timeout=None)`
Block until the internal flag is true. If the internal flag is true on entry, return immediately. Otherwise, block until another thread calls set() to set the flag to true, or until the optional timeout occurs.

When the timeout argument is present and not None, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof).

This method returns True if and only if the internal flag has been set to true, either before the wait call or after the wait starts, so it will always return True except if a timeout is given and the operation times out.


Changed in version 3.1: Previously, the method always returned None.



### ===🗝 ◦ Timer Objects

    class threading.Timer(interval, function, args=None, kwargs=None)

    Timer.start()
    Timer.cancel()

This class represents an action that should be run only after a certain amount of time has passed — a timer. Timer is a subclass of Thread and as such also functions as an example of creating custom threads.

Timers are started, as with threads, by calling their start() method. The timer can be stopped (before its action has begun) by calling the cancel() method. The interval the timer will wait before executing its action may not be exactly the same as the interval specified by the user.

For example:


```py
def hello():
    print("hello, world")

t = Timer(30.0, hello)
t.start()  # after 30 seconds, "hello, world" will be printed
```


✅ `class threading.Timer(interval, function, args=None, kwargs=None)`
Create a timer that will run function with arguments args and keyword arguments kwargs, after interval seconds have passed. If args is None (the default) then an empty list will be used. If kwargs is None (the default) then an empty dict will be used.


Changed in version 3.3: changed from a factory function to a class.

➡ `cancel()`
Stop the timer, and cancel the execution of the timer’s action. This will only work if the timer is still in its waiting stage.



### ===🗝 ◦ Barrier Objects

    class threading.Barrier(parties, action=None, timeout=None)

    Barrier.wait(timeout=None)
    Barrier.reset()
    Barrier.abort()
    Barrier.parties
    Barrier.n_waiting
    Barrier.broken

    exception threading.BrokenBarrierError

New in version 3.2.


This class provides a simple synchronization primitive for use by a fixed number of threads that need to wait for each other. Each of the threads tries to pass the barrier by calling the wait() method and will block until all of the threads have made their wait() calls. At this point, the threads are released simultaneously.

The barrier can be reused any number of times for the same number of threads.

As an example, here is a simple way to synchronize a client and server thread:


```py
b = Barrier(2, timeout=5)

def server():
    start_server()
    b.wait()
    while True:
        connection = accept_connection()
        process_server_connection(connection)

def client():
    b.wait()
    while True:
        connection = make_connection()
        process_client_connection(connection)
```


✅ `class threading.Barrier(parties, action=None, timeout=None)`
Create a barrier object for parties number of threads. An action, when provided, is a callable to be called by one of the threads when they are released. timeout is the default timeout value if none is specified for the wait() method.

➡ `wait(timeout=None)`
Pass the barrier. When all the threads party to the barrier have called this function, they are all released simultaneously. If a timeout is provided, it is used in preference to any that was supplied to the class constructor.

The return value is an integer in the range 0 to parties – 1, different for each thread. This can be used to select a thread to do some special housekeeping, e.g.:


```py
i = barrier.wait()
if i == 0:
    # Only one thread needs to print this
    print("passed the barrier")
```


If an action was provided to the constructor, one of the threads will have called it prior to being released. Should this call raise an error, the barrier is put into the broken state.

If the call times out, the barrier is put into the broken state.

This method may raise a BrokenBarrierError exception if the barrier is broken or reset while a thread is waiting.

➡ `reset()`
Return the barrier to the default, empty state. Any threads waiting on it will receive the BrokenBarrierError exception.

Note that using this function may require some external synchronization if there are other threads whose state is unknown. If a barrier is broken it may be better to just leave it and create a new one.

➡ `abort()`
Put the barrier into a broken state. This causes any active or future calls to wait() to fail with the BrokenBarrierError. Use this for example if one of the threads needs to abort, to avoid deadlocking the application.

It may be preferable to simply create the barrier with a sensible timeout value to automatically guard against one of the threads going awry.

➡ `parties`
The number of threads required to pass the barrier.

➡ `n_waiting`
The number of threads currently waiting in the barrier.

➡ `broken`
A boolean that is True if the barrier is in the broken state.

✅ `exception threading.BrokenBarrierError`
This exception, a subclass of RuntimeError, is raised when the Barrier object is reset or broken.

### ===🗝 ◦ Using locks, conditions, and semaphores in the with statement

All of the objects provided by this module that have acquire() and release() methods can be used as context managers for a with statement. The acquire() method will be called when the block is entered, and release() will be called when the block is exited. Hence, the following snippet:


```py
with some_lock:
    # do something...
```


is equivalent to:


```py
some_lock.acquire()
try:
    # do something...
finally:
    some_lock.release()
```


Currently, Lock, RLock, Condition, Semaphore, and BoundedSemaphore objects may be used as with statement context managers.



## ==⚡ • multiprocessing — Process-based parallelism

Source code: Lib/multiprocessing/



### ===🗝 ◦ Introduction

multiprocessing is a package that supports spawning processes using an API similar to the threading module. The multiprocessing package offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads. Due to this, the multiprocessing module allows the programmer to fully leverage multiple processors on a given machine. It runs on both Unix and Windows.

The multiprocessing module also introduces APIs which do not have analogs in the threading module. A prime example of this is the Pool object which offers a convenient means of parallelizing the execution of a function across multiple input values, distributing the input data across processes (data parallelism). The following example demonstrates the common practice of defining such functions in a module so that child processes can successfully import that module. This basic example of data parallelism using Pool,


```py
from multiprocessing import Pool

def f(x):
    return x*x

if __name__ == '__main__':
    with Pool(5) as p:
        print(p.map(f, [1, 2, 3]))
```


will print to standard output


[1, 4, 9]


#### ◾ The Process class

In multiprocessing, processes are spawned by creating a Process object and then calling its start() method. Process follows the API of threading.Thread. A trivial example of a multiprocess program is


```py
from multiprocessing import Process

def f(name):
    print('hello', name)

if __name__ == '__main__':
    p = Process(target=f, args=('bob',))
    p.start()
    p.join()
```


To show the individual process IDs involved, here is an expanded example:


```py
from multiprocessing import Process
import os

def info(title):
    print(title)
    print('module name:', __name__)
    print('parent process:', os.getppid())
    print('process id:', os.getpid())

def f(name):
    info('function f')
    print('hello', name)

if __name__ == '__main__':
    info('main line')
    p = Process(target=f, args=('bob',))
    p.start()
    p.join()
```


For an explanation of why the if __name__ == '__main__' part is necessary, see Programming guidelines.


#### ◾ Contexts and start methods

Depending on the platform, multiprocessing supports three ways to start a process. These start methods are

➡ `spawn`
The parent process starts a fresh python interpreter process. The child process will only inherit those resources necessary to run the process object’s run() method. In particular, unnecessary file descriptors and handles from the parent process will not be inherited. Starting a process using this method is rather slow compared to using fork or forkserver.

Available on Unix and Windows. The default on Windows and macOS.

➡ `fork`
The parent process uses os.fork() to fork the Python interpreter. The child process, when it begins, is effectively identical to the parent process. All resources of the parent are inherited by the child process. Note that safely forking a multithreaded process is problematic.

Available on Unix only. The default on Unix.

➡ `forkserver`
When the program starts and selects the forkserver start method, a server process is started. From then on, whenever a new process is needed, the parent process connects to the server and requests that it fork a new process. The fork server process is single threaded so it is safe for it to use os.fork(). No unnecessary resources are inherited.

Available on Unix platforms which support passing file descriptors over Unix pipes.


Changed in version 3.8: On macOS, the spawn start method is now the default. The fork start method should be considered unsafe as it can lead to crashes of the subprocess. See bpo-33725.


Changed in version 3.4: spawn added on all unix platforms, and forkserver added for some unix platforms. Child processes no longer inherit all of the parents inheritable handles on Windows.

On Unix using the spawn or forkserver start methods will also start a resource tracker process which tracks the unlinked named system resources (such as named semaphores or SharedMemory objects) created by processes of the program. When all processes have exited the resource tracker unlinks any remaining tracked object. Usually there should be none, but if a process was killed by a signal there may be some “leaked” resources. (Neither leaked semaphores nor shared memory segments will be automatically unlinked until the next reboot. This is problematic for both objects because the system allows only a limited number of named semaphores, and shared memory segments occupy some space in the main memory.)

To select a start method you use the set_start_method() in the if __name__ == '__main__' clause of the main module. For example:


```py
import multiprocessing as mp

def foo(q):
    q.put('hello')

if __name__ == '__main__':
    mp.set_start_method('spawn')
    q = mp.Queue()
    p = mp.Process(target=foo, args=(q,))
    p.start()
    print(q.get())
    p.join()
```


set_start_method() should not be used more than once in the program.

Alternatively, you can use get_context() to obtain a context object. Context objects have the same API as the multiprocessing module, and allow one to use multiple start methods in the same program.


```py
import multiprocessing as mp

def foo(q):
    q.put('hello')

if __name__ == '__main__':
    ctx = mp.get_context('spawn')
    q = ctx.Queue()
    p = ctx.Process(target=foo, args=(q,))
    p.start()
    print(q.get())
    p.join()
```


Note that objects related to one context may not be compatible with processes for a different context. In particular, locks created using the fork context cannot be passed to processes started using the spawn or forkserver start methods.

A library which wants to use a particular start method should probably use get_context() to avoid interfering with the choice of the library user.

Warning:
 The 'spawn' and 'forkserver' start methods cannot currently be used with “frozen” executables (i.e., binaries produced by packages like PyInstaller and cx_Freeze) on Unix. The 'fork' start method does work.
 

#### ◾ Exchanging objects between processes

multiprocessing supports two types of communication channel between processes:

➡ `Queues`


The Queue class is a near clone of queue.Queue. For example:


```py
from multiprocessing import Process, Queue

def f(q):
    q.put([42, None, 'hello'])

if __name__ == '__main__':
    q = Queue()
    p = Process(target=f, args=(q,))
    p.start()
    print(q.get())    # prints "[42, None, 'hello']"
    p.join()
```


Queues are thread and process safe.

➡ `Pipes`


The Pipe() function returns a pair of connection objects connected by a pipe which by default is duplex (two-way). For example:


```py
from multiprocessing import Process, Pipe

def f(conn):
    conn.send([42, None, 'hello'])
    conn.close()

if __name__ == '__main__':
    parent_conn, child_conn = Pipe()
    p = Process(target=f, args=(child_conn,))
    p.start()
    print(parent_conn.recv())   # prints "[42, None, 'hello']"
    p.join()
```


The two connection objects returned by Pipe() represent the two ends of the pipe. Each connection object has send() and recv() methods (among others). Note that data in a pipe may become corrupted if two processes (or threads) try to read from or write to the same end of the pipe at the same time. Of course there is no risk of corruption from processes using different ends of the pipe at the same time.


#### ◾ Synchronization between processes

multiprocessing contains equivalents of all the synchronization primitives from threading. For instance one can use a lock to ensure that only one process prints to standard output at a time:


```py
from multiprocessing import Process, Lock

def f(l, i):
    l.acquire()
    try:
        print('hello world', i)
    finally:
        l.release()

if __name__ == '__main__':
    lock = Lock()

    for num in range(10):
        Process(target=f, args=(lock, num)).start()
```


Without using the lock output from the different processes is liable to get all mixed up.


#### ◾ Sharing state between processes

As mentioned above, when doing concurrent programming it is usually best to avoid using shared state as far as possible. This is particularly true when using multiple processes.

However, if you really do need to use some shared data then multiprocessing provides a couple of ways of doing so.

➡ `Shared memory`


Data can be stored in a shared memory map using Value or Array. For example, the following code


```py
from multiprocessing import Process, Value, Array

def f(n, a):
    n.value = 3.1415927
    for i in range(len(a)):
        a[i] = -a[i]

if __name__ == '__main__':
    num = Value('d', 0.0)
    arr = Array('i', range(10))

    p = Process(target=f, args=(num, arr))
    p.start()
    p.join()

    print(num.value)
    print(arr[:])
```


will print


3.1415927
[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]


The 'd' and 'i' arguments used when creating num and arr are typecodes of the kind used by the array module: 'd' indicates a double precision float and 'i' indicates a signed integer. These shared objects will be process and thread-safe.

For more flexibility in using shared memory one can use the multiprocessing.sharedctypes module which supports the creation of arbitrary ctypes objects allocated from shared memory.

➡ `Server process`



A manager object returned by Manager() controls a server process which holds Python objects and allows other processes to manipulate them using proxies.

A manager returned by Manager() will support types list, dict, Namespace, Lock, RLock, Semaphore, BoundedSemaphore, Condition, Event, Barrier, Queue, Value and Array. For example,


```py
from multiprocessing import Process, Manager

def f(d, l):
    d[1] = '1'
    d['2'] = 2
    d[0.25] = None
    l.reverse()

if __name__ == '__main__':
    with Manager() as manager:
        d = manager.dict()
        l = manager.list(range(10))

        p = Process(target=f, args=(d, l))
        p.start()
        p.join()

        print(d)
        print(l)
```


will print


{0.25: None, 1: '1', '2': 2}
[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]


Server process managers are more flexible than using shared memory objects because they can be made to support arbitrary object types. Also, a single manager can be shared by processes on different computers over a network. They are, however, slower than using shared memory.

#### ◾ Using a pool of workers

The Pool class represents a pool of worker processes. It has methods which allows tasks to be offloaded to the worker processes in a few different ways.

For example:


```py
from multiprocessing import Pool, TimeoutError
import time
import os

def f(x):
    return x*x

if __name__ == '__main__':
    # start 4 worker processes
    with Pool(processes=4) as pool:

        # print "[0, 1, 4,..., 81]"
        print(pool.map(f, range(10)))

        # print same numbers in arbitrary order
        for i in pool.imap_unordered(f, range(10)):
            print(i)

        # evaluate "f(20)" asynchronously
        res = pool.apply_async(f, (20,))      # runs in *only* one process
        print(res.get(timeout=1))             # prints "400"

        # evaluate "os.getpid()" asynchronously
        res = pool.apply_async(os.getpid, ()) # runs in *only* one process
        print(res.get(timeout=1))             # prints the PID of that process

        # launching multiple evaluations asynchronously *may* use more processes
        multiple_results = [pool.apply_async(os.getpid, ()) for i in range(4)]
        print([res.get(timeout=1) for res in multiple_results])

        # make a single worker sleep for 10 secs
        res = pool.apply_async(time.sleep, (10,))
        try:
            print(res.get(timeout=1))
        except TimeoutError:
            print("We lacked patience and got a multiprocessing.TimeoutError")

        print("For the moment, the pool remains available for more work")

    # exiting the 'with'-block has stopped the pool
    print("Now the pool is closed and no longer available")
```


Note that the methods of a pool should only ever be used by the process which created it.

Note:
 Functionality within this package requires that the __main__ module be importable by the children. This is covered in Programming guidelines however it is worth pointing out here. This means that some examples, such as the multiprocessing.pool.Pool examples will not work in the interactive interpreter. For example:
 

>>> from multiprocessing import Pool
>>> p = Pool(5)
>>> def f(x):
...     return x*x
...
>>> with p:
...   p.map(f, [1,2,3])
Process PoolWorker-1:
Process PoolWorker-2:
Process PoolWorker-3:
Traceback (most recent call last):
AttributeError: 'module' object has no attribute 'f'
AttributeError: 'module' object has no attribute 'f'
AttributeError: 'module' object has no attribute 'f'


(If you try this it will actually output three full tracebacks interleaved in a semi-random fashion, and then you may have to stop the parent process somehow.)

### ===🗝 ◦ Reference

The multiprocessing package mostly replicates the API of the threading module.

#### ◾ Process and exceptions

✅ `class multiprocessing.Process(group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None)`
    ➡ `run()`
    ➡ `start()`
    ➡ `join([timeout])`
    ➡ `name`
    ➡ `is_alive()`
    ➡ `daemon`
    ➡ `pid`
    ➡ `exitcode`
    ➡ `authkey`
    ➡ `sentinel`
    ➡ `terminate()`
    ➡ `kill()`
    ➡ `close()`
✅ `exception multiprocessing.ProcessError`
✅ `exception multiprocessing.BufferTooShort`
✅ `exception multiprocessing.AuthenticationError`
✅ `exception multiprocessing.TimeoutError`


✅ `class multiprocessing.Process(group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None)`
Process objects represent activity that is run in a separate process. The Process class has equivalents of all the methods of threading.Thread.

The constructor should always be called with keyword arguments. group should always be None; it exists solely for compatibility with threading.Thread. target is the callable object to be invoked by the run() method. It defaults to None, meaning nothing is called. name is the process name (see name for more details). args is the argument tuple for the target invocation. kwargs is a dictionary of keyword arguments for the target invocation. If provided, the keyword-only daemon argument sets the process daemon flag to True or False. If None (the default), this flag will be inherited from the creating process.

By default, no arguments are passed to target.

If a subclass overrides the constructor, it must make sure it invokes the base class constructor (Process.__init__()) before doing anything else to the process.

Changed in version 3.3: Added the daemon argument.


➡ `run()`
Method representing the process’s activity.

You may override this method in a subclass. The standard run() method invokes the callable object passed to the object’s constructor as the target argument, if any, with sequential and keyword arguments taken from the args and kwargs arguments, respectively.

➡ `start()`
Start the process’s activity.

This must be called at most once per process object. It arranges for the object’s run() method to be invoked in a separate process.

➡ `join([timeout])`
If the optional argument timeout is None (the default), the method blocks until the process whose join() method is called terminates. If timeout is a positive number, it blocks at most timeout seconds. Note that the method returns None if its process terminates or if the method times out. Check the process’s exitcode to determine if it terminated.

A process can be joined many times.

A process cannot join itself because this would cause a deadlock. It is an error to attempt to join a process before it has been started.

➡ `name`
The process’s name. The name is a string used for identification purposes only. It has no semantics. Multiple processes may be given the same name.

The initial name is set by the constructor. If no explicit name is provided to the constructor, a name of the form ‘Process-N1:N2:…:Nk’ is constructed, where each Nk is the N-th child of its parent.

➡ `is_alive()`
Return whether the process is alive.

Roughly, a process object is alive from the moment the start() method returns until the child process terminates.

➡ `daemon`
The process’s daemon flag, a Boolean value. This must be set before start() is called.

The initial value is inherited from the creating process.

When a process exits, it attempts to terminate all of its daemonic child processes.

Note that a daemonic process is not allowed to create child processes. Otherwise a daemonic process would leave its children orphaned if it gets terminated when its parent process exits. Additionally, these are not Unix daemons or services, they are normal processes that will be terminated (and not joined) if non-daemonic processes have exited.

In addition to the threading.Thread API, Process objects also support the following attributes and methods:

➡ `pid`
Return the process ID. Before the process is spawned, this will be None.

➡ `exitcode`
The child’s exit code. This will be None if the process has not yet terminated. A negative value -N indicates that the child was terminated by signal N.

➡ `authkey`
The process’s authentication key (a byte string).

When multiprocessing is initialized the main process is assigned a random string using os.urandom().

When a Process object is created, it will inherit the authentication key of its parent process, although this may be changed by setting authkey to another byte string.

See Authentication keys.

➡ `sentinel`
A numeric handle of a system object which will become “ready” when the process ends.

You can use this value if you want to wait on several events at once using multiprocessing.connection.wait(). Otherwise calling join() is simpler.

On Windows, this is an OS handle usable with the WaitForSingleObject and WaitForMultipleObjects family of API calls. On Unix, this is a file descriptor usable with primitives from the select module.

New in version 3.3.


➡ `terminate()`
Terminate the process. On Unix this is done using the SIGTERM signal; on Windows TerminateProcess() is used. Note that exit handlers and finally clauses, etc., will not be executed.

Note that descendant processes of the process will not be terminated – they will simply become orphaned.

Warning:
 If this method is used when the associated process is using a pipe or queue then the pipe or queue is liable to become corrupted and may become unusable by other process. Similarly, if the process has acquired a lock or semaphore etc. then terminating it is liable to cause other processes to deadlock.
 

➡ `kill()`
Same as terminate() but using the SIGKILL signal on Unix.

New in version 3.7.


➡ `close()`
Close the Process object, releasing all resources associated with it. ValueError is raised if the underlying process is still running. Once close() returns successfully, most other methods and attributes of the Process object will raise ValueError.

New in version 3.7.


Note that the start(), join(), is_alive(), terminate() and exitcode methods should only be called by the process that created the process object.

Example usage of some of the methods of Process:


>>> import multiprocessing, time, signal
>>> p = multiprocessing.Process(target=time.sleep, args=(1000,))`
>>> print(p, p.is_alive())`
 <Process ... initial> False
>>> p.start()`
>>> print(p, p.is_alive())`
 <Process ... started> True
>>> p.terminate()`
>>> time.sleep(0.1)`
>>> print(p, p.is_alive())`
 <Process ... stopped exitcode=-SIGTERM> False
>>> p.exitcode == -signal.SIGTERM
 True


✅ `exception multiprocessing.ProcessError`
The base class of all multiprocessing exceptions.

✅ `exception multiprocessing.BufferTooShort`
Exception raised by Connection.recv_bytes_into() when the supplied buffer object is too small for the message read.

If e is an instance of BufferTooShort then e.args[0] will give the message as a byte string.

✅ `exception multiprocessing.AuthenticationError`
Raised when there is an authentication error.

✅ `exception multiprocessing.TimeoutError`
Raised by methods with a timeout when the timeout expires.


#### ◾ Pipes and Queues

✅ `multiprocessing.Pipe([duplex])`
✅ `class multiprocessing.Queue([maxsize])`
    ➡ `qsize()`
    ➡ `empty()`
    ➡ `full()`
    ➡ `put(obj[, block[, timeout]])`
    ➡ `put_nowait(obj)`
    ➡ `get([block[, timeout]])`
    ➡ `get_nowait()`
    ➡ `close()`
    ➡ `join_thread()`
    ➡ `cancel_join_thread()`
✅ `class multiprocessing.SimpleQueue`
    ➡ `close()`
    ➡ `empty()`
    ➡ `get()`
    ➡ `put(item)`
✅ `class multiprocessing.JoinableQueue([maxsize])`
    ➡ `task_done()`
    ➡ `join()`

When using multiple processes, one generally uses message passing for communication between processes and avoids having to use any synchronization primitives like locks.

For passing messages one can use Pipe() (for a connection between two processes) or a queue (which allows multiple producers and consumers).

The Queue, SimpleQueue and JoinableQueue types are multi-producer, multi-consumer FIFO queues modelled on the queue.Queue class in the standard library. They differ in that Queue lacks the task_done() and join() methods introduced into Python 2.5’s queue.Queue class.

If you use JoinableQueue then you must call JoinableQueue.task_done() for each task removed from the queue or else the semaphore used to count the number of unfinished tasks may eventually overflow, raising an exception.

Note that one can also create a shared queue by using a manager object – see Managers.

Note:
 multiprocessing uses the usual queue.Empty and queue.Full exceptions to signal a timeout. They are not available in the multiprocessing namespace so you need to import them from queue.
 

Note:
 When an object is put on a queue, the object is pickled and a background thread later flushes the pickled data to an underlying pipe. This has some consequences which are a little surprising, but should not cause any practical difficulties – if they really bother you then you can instead use a queue created with a manager.

1. After putting an object on an empty queue there may be an infinitesimal delay before the queue’s empty() method returns False and get_nowait() can return without raising queue.Empty.

2. If multiple processes are enqueuing objects, it is possible for the objects to be received at the other end out-of-order. However, objects enqueued by the same process will always be in the expected order with respect to each other.

Warning:
 If a process is killed using Process.terminate() or os.kill() while it is trying to use a Queue, then the data in the queue is likely to become corrupted. This may cause any other process to get an exception when it tries to use the queue later on.
 

Warning:
 As mentioned above, if a child process has put items on a queue (and it has not used JoinableQueue.cancel_join_thread), then that process will not terminate until all buffered items have been flushed to the pipe.
 
This means that if you try joining that process you may get a deadlock unless you are sure that all items which have been put on the queue have been consumed. Similarly, if the child process is non-daemonic then the parent process may hang on exit when it tries to join all its non-daemonic children.

Note that a queue created using a manager does not have this issue. See Programming guidelines.

For an example of the usage of queues for interprocess communication see Examples.

➡ `multiprocessing.Pipe([duplex])`
Returns a pair (conn1, conn2) of Connection objects representing the ends of a pipe.

If duplex is True (the default) then the pipe is bidirectional. If duplex is False then the pipe is unidirectional: conn1 can only be used for receiving messages and conn2 can only be used for sending messages.

✅ `class multiprocessing.Queue([maxsize])`
Returns a process shared queue implemented using a pipe and a few locks/semaphores. When a process first puts an item on the queue a feeder thread is started which transfers objects from a buffer into the pipe.

The usual queue.Empty and queue.Full exceptions from the standard library’s queue module are raised to signal timeouts.

Queue implements all the methods of queue.Queue except for task_done() and join().

➡ `qsize()`
Return the approximate size of the queue. Because of multithreading/multiprocessing semantics, this number is not reliable.

Note that this may raise NotImplementedError on Unix platforms like macOS where sem_getvalue() is not implemented.

➡ `empty()`
Return True if the queue is empty, False otherwise. Because of multithreading/multiprocessing semantics, this is not reliable.

➡ `full()`
Return True if the queue is full, False otherwise. Because of multithreading/multiprocessing semantics, this is not reliable.

➡ `put(obj[, block[, timeout]])`
Put obj into the queue. If the optional argument block is True (the default) and timeout is None (the default), block if necessary until a free slot is available. If timeout is a positive number, it blocks at most timeout seconds and raises the queue.Full exception if no free slot was available within that time. Otherwise (block is False), put an item on the queue if a free slot is immediately available, else raise the queue.Full exception (timeout is ignored in that case).

Changed in version 3.8: If the queue is closed, ValueError is raised instead of AssertionError.


➡ `put_nowait(obj)`
Equivalent to put(obj, False).

➡ `get([block[, timeout]])`
Remove and return an item from the queue. If optional args block is True (the default) and timeout is None (the default), block if necessary until an item is available. If timeout is a positive number, it blocks at most timeout seconds and raises the queue.Empty exception if no item was available within that time. Otherwise (block is False), return an item if one is immediately available, else raise the queue.Empty exception (timeout is ignored in that case).

Changed in version 3.8: If the queue is closed, ValueError is raised instead of OSError.


➡ `get_nowait()`
Equivalent to get(False).

multiprocessing.Queue has a few additional methods not found in queue.Queue. These methods are usually unnecessary for most code:

➡ `close()`
Indicate that no more data will be put on this queue by the current process. The background thread will quit once it has flushed all buffered data to the pipe. This is called automatically when the queue is garbage collected.

➡ `join_thread()`
Join the background thread. This can only be used after close() has been called. It blocks until the background thread exits, ensuring that all data in the buffer has been flushed to the pipe.

By default if a process is not the creator of the queue then on exit it will attempt to join the queue’s background thread. The process can call cancel_join_thread() to make join_thread() do nothing.

➡ `cancel_join_thread()`
Prevent join_thread() from blocking. In particular, this prevents the background thread from being joined automatically when the process exits – see join_thread().

A better name for this method might be allow_exit_without_flush(). It is likely to cause enqueued data to be lost, and you almost certainly will not need to use it. It is really only there if you need the current process to exit immediately without waiting to flush enqueued data to the underlying pipe, and you don’t care about lost data.

Note:
 This class’s functionality requires a functioning shared semaphore implementation on the host operating system. Without one, the functionality in this class will be disabled, and attempts to instantiate a Queue will result in an ImportError. See bpo-3770 for additional information. The same holds true for any of the specialized queue types listed below.
 

✅ `class multiprocessing.SimpleQueue`
It is a simplified Queue type, very close to a locked Pipe.

➡ `close()`
Close the queue: release internal resources.

A queue must not be used anymore after it is closed. For example, get(), put() and empty() methods must no longer be called.

New in version 3.9.


➡ `empty()`
Return True if the queue is empty, False otherwise.

➡ `get()`
Remove and return an item from the queue.

➡ `put(item)`
Put item into the queue.

✅ `class multiprocessing.JoinableQueue([maxsize])`
JoinableQueue, a Queue subclass, is a queue which additionally has task_done() and join() methods.

➡ `task_done()`
Indicate that a formerly enqueued task is complete. Used by queue consumers. For each get() used to fetch a task, a subsequent call to task_done() tells the queue that the processing on the task is complete.

If a join() is currently blocking, it will resume when all items have been processed (meaning that a task_done() call was received for every item that had been put() into the queue).

Raises a ValueError if called more times than there were items placed in the queue.

➡ `join()`
Block until all items in the queue have been gotten and processed.

The count of unfinished tasks goes up whenever an item is added to the queue. The count goes down whenever a consumer calls task_done() to indicate that the item was retrieved and all work on it is complete. When the count of unfinished tasks drops to zero, join() unblocks.


#### ◾ Miscellaneous

➡ `multiprocessing.active_children()`
➡ `multiprocessing.cpu_count()`
➡ `multiprocessing.current_process()`
➡ `multiprocessing.parent_process()`
➡ `multiprocessing.freeze_support()`
➡ `multiprocessing.get_all_start_methods()`
➡ `multiprocessing.get_context(method=None)`
➡ `multiprocessing.get_start_method(allow_none=False)`
➡ `multiprocessing.set_executable()`
➡ `multiprocessing.set_start_method(method)`

➡ `multiprocessing.active_children()`
Return list of all live children of the current process.

Calling this has the side effect of “joining” any processes which have already finished.

➡ `multiprocessing.cpu_count()`
Return the number of CPUs in the system.

This number is not equivalent to the number of CPUs the current process can use. The number of usable CPUs can be obtained with len(os.sched_getaffinity(0))

When the number of CPUs cannot be determined a NotImplementedError is raised.

See also:

os.cpu_count()
 

➡ `multiprocessing.current_process()`
Return the Process object corresponding to the current process.

An analogue of threading.current_thread().

➡ `multiprocessing.parent_process()`
Return the Process object corresponding to the parent process of the current_process(). For the main process, parent_process will be None.

New in version 3.8.


➡ `multiprocessing.freeze_support()`
Add support for when a program which uses multiprocessing has been frozen to produce a Windows executable. (Has been tested with py2exe, PyInstaller and cx_Freeze.)

One needs to call this function straight after the if __name__ == '__main__' line of the main module. For example:


```py
from multiprocessing import Process, freeze_support

def f():
    print('hello world!')

if __name__ == '__main__':
    freeze_support()
    Process(target=f).start()
```


If the freeze_support() line is omitted then trying to run the frozen executable will raise RuntimeError.

Calling freeze_support() has no effect when invoked on any operating system other than Windows. In addition, if the module is being run normally by the Python interpreter on Windows (the program has not been frozen), then freeze_support() has no effect.

➡ `multiprocessing.get_all_start_methods()`
Returns a list of the supported start methods, the first of which is the default. The possible start methods are 'fork', 'spawn' and 'forkserver'. On Windows only 'spawn' is available. On Unix 'fork' and 'spawn' are always supported, with 'fork' being the default.

New in version 3.4.


➡ `multiprocessing.get_context(method=None)`
Return a context object which has the same attributes as the multiprocessing module.

If method is None then the default context is returned. Otherwise method should be 'fork', 'spawn', 'forkserver'. ValueError is raised if the specified start method is not available.

New in version 3.4.


➡ `multiprocessing.get_start_method(allow_none=False)`
Return the name of start method used for starting processes.

If the start method has not been fixed and allow_none is false, then the start method is fixed to the default and the name is returned. If the start method has not been fixed and allow_none is true then None is returned.

The return value can be 'fork', 'spawn', 'forkserver' or None. 'fork' is the default on Unix, while 'spawn' is the default on Windows and macOS.

Changed in version 3.8: On macOS, the spawn start method is now the default. The fork start method should be considered unsafe as it can lead to crashes of the subprocess. See bpo-33725.


New in version 3.4.


➡ `multiprocessing.set_executable()`
Sets the path of the Python interpreter to use when starting a child process. (By default sys.executable is used). Embedders will probably need to do some thing like


```py
set_executable(os.path.join(sys.exec_prefix, 'pythonw.exe'))
```


before they can create child processes.

Changed in version 3.4: Now supported on Unix when the 'spawn' start method is used.


➡ `multiprocessing.set_start_method(method)`
Set the method which should be used to start child processes. method can be 'fork', 'spawn' or 'forkserver'.

Note that this should be called at most once, and it should be protected inside the if __name__ == '__main__' clause of the main module.

New in version 3.4.


Note:
 multiprocessing contains no analogues of threading.active_count(), threading.enumerate(), threading.settrace(), threading.setprofile(), threading.Timer, or threading.local.

#### ◾ Connection Objects

✅ `class multiprocessing.connection.Connectionsend(obj)`
    ➡ `recv()`
    ➡ `fileno()`
    ➡ `close()`
    ➡ `poll([timeout])`
    ➡ `send_bytes(buffer[, offset[, size]])`
    ➡ `recv_bytes([maxlength])`
    ➡ `recv_bytes_into(buffer[, offset])`

Connection objects allow the sending and receiving of picklable objects or strings. They can be thought of as message oriented connected sockets.

Connection objects are usually created using Pipe – see also Listeners and Clients.

✅ `class multiprocessing.connection.Connectionsend(obj)`
Send an object to the other end of the connection which should be read using recv().

The object must be picklable. Very large pickles (approximately 32 MiB+, though it depends on the OS) may raise a ValueError exception.

➡ `recv()`
Return an object sent from the other end of the connection using send(). Blocks until there is something to receive. Raises EOFError if there is nothing left to receive and the other end was closed.

➡ `fileno()`
Return the file descriptor or handle used by the connection.

➡ `close()`
Close the connection.

This is called automatically when the connection is garbage collected.

➡ `poll([timeout])`
Return whether there is any data available to be read.

If timeout is not specified then it will return immediately. If timeout is a number then this specifies the maximum time in seconds to block. If timeout is None then an infinite timeout is used.

Note that multiple connection objects may be polled at once by using multiprocessing.connection.wait().

➡ `send_bytes(buffer[, offset[, size]])`
Send byte data from a bytes-like object as a complete message.

If offset is given then data is read from that position in buffer. If size is given then that many bytes will be read from buffer. Very large buffers (approximately 32 MiB+, though it depends on the OS) may raise a ValueError exception

➡ `recv_bytes([maxlength])`
Return a complete message of byte data sent from the other end of the connection as a string. Blocks until there is something to receive. Raises EOFError if there is nothing left to receive and the other end has closed.

If maxlength is specified and the message is longer than maxlength then OSError is raised and the connection will no longer be readable.

Changed in version 3.3: This function used to raise IOError, which is now an alias of OSError.


➡ `recv_bytes_into(buffer[, offset])`
Read into buffer a complete message of byte data sent from the other end of the connection and return the number of bytes in the message. Blocks until there is something to receive. Raises EOFError if there is nothing left to receive and the other end was closed.

buffer must be a writable bytes-like object. If offset is given then the message will be written into the buffer from that position. Offset must be a non-negative integer less than the length of buffer (in bytes).

If the buffer is too short then a BufferTooShort exception is raised and the complete message is available as e.args[0] where e is the exception instance.

Changed in version 3.3: Connection objects themselves can now be transferred between processes using Connection.send() and Connection.recv().


New in version 3.3: Connection objects now support the context management protocol – see Context Manager Types. __enter__() returns the connection object, and __exit__() calls close().


For example:


>>> from multiprocessing import Pipe
>>> a, b = Pipe()
>>> a.send([1, 'hello', None])
>>> b.recv()
[1, 'hello', None]
>>> b.send_bytes(b'thank you')
>>> a.recv_bytes()
b'thank you'
>>> import array
>>> arr1 = array.array('i', range(5))
>>> arr2 = array.array('i', [0] * 10)
>>> a.send_bytes(arr1)
>>> count = b.recv_bytes_into(arr2)
>>> assert count == len(arr1) * arr1.itemsize
>>> arr2
array('i', [0, 1, 2, 3, 4, 0, 0, 0, 0, 0])


Warning:
 The Connection.recv() method automatically unpickles the data it receives, which can be a security risk unless you can trust the process which sent the message.
 
Therefore, unless the connection object was produced using Pipe() you should only use the recv() and send() methods after performing some sort of authentication. See Authentication keys.

Warning:
 If a process is killed while it is trying to read or write to a pipe then the data in the pipe is likely to become corrupted, because it may become impossible to be sure where the message boundaries lie.
 

#### ◾ Synchronization primitives

✅ `class multiprocessing.Barrier(parties[, action[, timeout]])`
✅ `class multiprocessing.BoundedSemaphore([value])`
✅ `class multiprocessing.Condition([lock])`
✅ `class multiprocessing.Event`
✅ `class multiprocessing.Lock`
    ➡ `acquire(block=True, timeout=None)`
    ➡ `release()`
✅ `class multiprocessing.RLock`
    ➡ `acquire(block=True, timeout=None)`
    ➡ `release()`
✅ `class multiprocessing.Semaphore([value])`

Generally synchronization primitives are not as necessary in a multiprocess program as they are in a multithreaded program. See the documentation for threading module.

Note that one can also create synchronization primitives by using a manager object – see Managers.

✅ `class multiprocessing.Barrier(parties[, action[, timeout]])`
A barrier object: a clone of threading.Barrier.

New in version 3.3.


✅ `class multiprocessing.BoundedSemaphore([value])`
A bounded semaphore object: a close analog of threading.BoundedSemaphore.

A solitary difference from its close analog exists: its acquire method’s first argument is named block, as is consistent with Lock.acquire().

Note:
 On macOS, this is indistinguishable from Semaphore because sem_getvalue() is not implemented on that platform.
 

✅ `class multiprocessing.Condition([lock])`
A condition variable: an alias for threading.Condition.

If lock is specified then it should be a Lock or RLock object from multiprocessing.

Changed in version 3.3: The wait_for() method was added.


✅ `class multiprocessing.Event`
A clone of threading.Event.

✅ `class multiprocessing.Lock`
A non-recursive lock object: a close analog of threading.Lock. Once a process or thread has acquired a lock, subsequent attempts to acquire it from any process or thread will block until it is released; any process or thread may release it. The concepts and behaviors of threading.Lock as it applies to threads are replicated here in multiprocessing.Lock as it applies to either processes or threads, except as noted.

Note that Lock is actually a factory function which returns an instance of multiprocessing.synchronize.Lock initialized with a default context.

Lock supports the context manager protocol and thus may be used in with statements.

➡ `acquire(block=True, timeout=None)`
Acquire a lock, blocking or non-blocking.

With the block argument set to True (the default), the method call will block until the lock is in an unlocked state, then set it to locked and return True. Note that the name of this first argument differs from that in threading.Lock.acquire().

With the block argument set to False, the method call does not block. If the lock is currently in a locked state, return False; otherwise set the lock to a locked state and return True.

When invoked with a positive, floating-point value for timeout, block for at most the number of seconds specified by timeout as long as the lock can not be acquired. Invocations with a negative value for timeout are equivalent to a timeout of zero. Invocations with a timeout value of None (the default) set the timeout period to infinite. Note that the treatment of negative or None values for timeout differs from the implemented behavior in threading.Lock.acquire(). The timeout argument has no practical implications if the block argument is set to False and is thus ignored. Returns True if the lock has been acquired or False if the timeout period has elapsed.

➡ `release()`
Release a lock. This can be called from any process or thread, not only the process or thread which originally acquired the lock.

Behavior is the same as in threading.Lock.release() except that when invoked on an unlocked lock, a ValueError is raised.

✅ `class multiprocessing.RLock`
A recursive lock object: a close analog of threading.RLock. A recursive lock must be released by the process or thread that acquired it. Once a process or thread has acquired a recursive lock, the same process or thread may acquire it again without blocking; that process or thread must release it once for each time it has been acquired.

Note that RLock is actually a factory function which returns an instance of multiprocessing.synchronize.RLock initialized with a default context.

RLock supports the context manager protocol and thus may be used in with statements.

➡ `acquire(block=True, timeout=None)`
Acquire a lock, blocking or non-blocking.

When invoked with the block argument set to True, block until the lock is in an unlocked state (not owned by any process or thread) unless the lock is already owned by the current process or thread. The current process or thread then takes ownership of the lock (if it does not already have ownership) and the recursion level inside the lock increments by one, resulting in a return value of True. Note that there are several differences in this first argument’s behavior compared to the implementation of threading.RLock.acquire(), starting with the name of the argument itself.

When invoked with the block argument set to False, do not block. If the lock has already been acquired (and thus is owned) by another process or thread, the current process or thread does not take ownership and the recursion level within the lock is not changed, resulting in a return value of False. If the lock is in an unlocked state, the current process or thread takes ownership and the recursion level is incremented, resulting in a return value of True.

Use and behaviors of the timeout argument are the same as in Lock.acquire(). Note that some of these behaviors of timeout differ from the implemented behaviors in threading.RLock.acquire().

➡ `release()`
Release a lock, decrementing the recursion level. If after the decrement the recursion level is zero, reset the lock to unlocked (not owned by any process or thread) and if any other processes or threads are blocked waiting for the lock to become unlocked, allow exactly one of them to proceed. If after the decrement the recursion level is still nonzero, the lock remains locked and owned by the calling process or thread.

Only call this method when the calling process or thread owns the lock. An AssertionError is raised if this method is called by a process or thread other than the owner or if the lock is in an unlocked (unowned) state. Note that the type of exception raised in this situation differs from the implemented behavior in threading.RLock.release().

✅ `class multiprocessing.Semaphore([value])`
A semaphore object: a close analog of threading.Semaphore.

A solitary difference from its close analog exists: its acquire method’s first argument is named block, as is consistent with Lock.acquire().

Note:
 On macOS, sem_timedwait is unsupported, so calling acquire() with a timeout will emulate that function’s behavior using a sleeping loop.
 

Note:
 If the SIGINT signal generated by Ctrl-C arrives while the main thread is blocked by a call to BoundedSemaphore.acquire(), Lock.acquire(), RLock.acquire(), Semaphore.acquire(), Condition.acquire() or Condition.wait() then the call will be immediately interrupted and KeyboardInterrupt will be raised.
 
This differs from the behaviour of threading where SIGINT will be ignored while the equivalent blocking calls are in progress.

Note:
 Some of this package’s functionality requires a functioning shared semaphore implementation on the host operating system. Without one, the multiprocessing.synchronize module will be disabled, and attempts to import it will result in an ImportError. See bpo-3770 for additional information.
 
#### ◾ Shared ctypes Objects

➡ `multiprocessing.Value(typecode_or_type, *args, lock=True)`
➡ `multiprocessing.Array(typecode_or_type, size_or_initializer, *, lock=True)`


It is possible to create shared objects using shared memory which can be inherited by child processes.

➡ `multiprocessing.Value(typecode_or_type, *args, lock=True)`
Return a ctypes object allocated from shared memory. By default the return value is actually a synchronized wrapper for the object. The object itself can be accessed via the value attribute of a Value.

typecode_or_type determines the type of the returned object: it is either a ctypes type or a one character typecode of the kind used by the array module. `*args` is passed on to the constructor for the type.

If lock is True (the default) then a new recursive lock object is created to synchronize access to the value. If lock is a Lock or RLock object then that will be used to synchronize access to the value. If lock is False then access to the returned object will not be automatically protected by a lock, so it will not necessarily be “process-safe”.

Operations like += which involve a read and write are not atomic. So if, for instance, you want to atomically increment a shared value it is insufficient to just do


```py
counter.value += 1
```


Assuming the associated lock is recursive (which it is by default) you can instead do


```py
with counter.get_lock():
    counter.value += 1
```


Note that lock is a keyword-only argument.

➡ `multiprocessing.Array(typecode_or_type, size_or_initializer, *, lock=True)`
Return a ctypes array allocated from shared memory. By default the return value is actually a synchronized wrapper for the array.

typecode_or_type determines the type of the elements of the returned array: it is either a ctypes type or a one character typecode of the kind used by the array module. If size_or_initializer is an integer, then it determines the length of the array, and the array will be initially zeroed. Otherwise, size_or_initializer is a sequence which is used to initialize the array and whose length determines the length of the array.

If lock is True (the default) then a new lock object is created to synchronize access to the value. If lock is a Lock or RLock object then that will be used to synchronize access to the value. If lock is False then access to the returned object will not be automatically protected by a lock, so it will not necessarily be “process-safe”.

Note that lock is a keyword only argument.

Note that an array of ctypes.c_char has value and raw attributes which allow one to use it to store and retrieve strings.

#### ◾ The multiprocessing.sharedctypes module

➡ `sharedctypes.RawArray(typecode_or_type, size_or_initializer)`
➡ `sharedctypes.RawValue(typecode_or_type, *args)`
➡ `sharedctypes.Array(typecode_or_type, size_or_initializer, *, lock=True)`
➡ `sharedctypes.Value(typecode_or_type, *args, lock=True)`
➡ `sharedctypes.copy(obj)`
➡ `sharedctypes.synchronized(obj[, lock])`

The multiprocessing.sharedctypes module provides functions for allocating ctypes objects from shared memory which can be inherited by child processes.

Note:
 Although it is possible to store a pointer in shared memory remember that this will refer to a location in the address space of a specific process. However, the pointer is quite likely to be invalid in the context of a second process and trying to dereference the pointer from the second process may cause a crash.
 

➡ `multiprocessing.sharedctypes.RawArray(typecode_or_type, size_or_initializer)`
Return a ctypes array allocated from shared memory.

typecode_or_type determines the type of the elements of the returned array: it is either a ctypes type or a one character typecode of the kind used by the array module. If size_or_initializer is an integer then it determines the length of the array, and the array will be initially zeroed. Otherwise size_or_initializer is a sequence which is used to initialize the array and whose length determines the length of the array.

Note that setting and getting an element is potentially non-atomic – use Array() instead to make sure that access is automatically synchronized using a lock.

➡ `multiprocessing.sharedctypes.RawValue(typecode_or_type, *args)`
Return a ctypes object allocated from shared memory.

typecode_or_type determines the type of the returned object: it is either a ctypes type or a one character typecode of the kind used by the array module. `*args` is passed on to the constructor for the type.

Note that setting and getting the value is potentially non-atomic – use Value() instead to make sure that access is automatically synchronized using a lock.

Note that an array of ctypes.c_char has value and raw attributes which allow one to use it to store and retrieve strings – see documentation for ctypes.

➡ `multiprocessing.sharedctypes.Array(typecode_or_type, size_or_initializer, *, lock=True)`
The same as RawArray() except that depending on the value of lock a process-safe synchronization wrapper may be returned instead of a raw ctypes array.

If lock is True (the default) then a new lock object is created to synchronize access to the value. If lock is a Lock or RLock object then that will be used to synchronize access to the value. If lock is False then access to the returned object will not be automatically protected by a lock, so it will not necessarily be “process-safe”.

Note that lock is a keyword-only argument.

➡ `multiprocessing.sharedctypes.Value(typecode_or_type, *args, lock=True)`
The same as RawValue() except that depending on the value of lock a process-safe synchronization wrapper may be returned instead of a raw ctypes object.

If lock is True (the default) then a new lock object is created to synchronize access to the value. If lock is a Lock or RLock object then that will be used to synchronize access to the value. If lock is False then access to the returned object will not be automatically protected by a lock, so it will not necessarily be “process-safe”.

Note that lock is a keyword-only argument.

➡ `multiprocessing.sharedctypes.copy(obj)`
Return a ctypes object allocated from shared memory which is a copy of the ctypes object obj.

➡ `multiprocessing.sharedctypes.synchronized(obj[, lock])`
Return a process-safe wrapper object for a ctypes object which uses lock to synchronize access. If lock is None (the default) then a multiprocessing.RLock object is created automatically.

A synchronized wrapper will have two methods in addition to those of the object it wraps: get_obj() returns the wrapped object and get_lock() returns the lock object used for synchronization.

Note that accessing the ctypes object through the wrapper can be a lot slower than accessing the raw ctypes object.

Changed in version 3.5: Synchronized objects support the context manager protocol.


The table below compares the syntax for creating shared ctypes objects from shared memory with the normal ctypes syntax. (In the table MyStruct is some subclass of ctypes.Structure.)

| ctypes               | sharedctypes using type    | sharedctypes using typecode           |
|----------------------|----------------------------|--------------------------|
| c_double(2.4)        | RawValue(c_double, 2.4)    | RawValue(‘d’, 2.4)       |
| MyStruct(4, 6)       | RawValue(MyStruct, 4, 6)   |                          |
| (c_short * 7)()      | RawArray(c_short, 7)       | RawArray(‘h’, 7)         |
| (c_int * 3)(9, 2, 8) | RawArray(c_int, (9, 2, 8)) | RawArray(‘i’, (9, 2, 8)) |

Below is an example where a number of ctypes objects are modified by a child process:


```py
from multiprocessing import Process, Lock
from multiprocessing.sharedctypes import Value, Array
from ctypes import Structure, c_double

class Point(Structure):
    _fields_ = [('x', c_double), ('y', c_double)]

def modify(n, x, s, A):
    n.value **= 2
    x.value **= 2
    s.value = s.value.upper()
    for a in A:
        a.x **= 2
        a.y **= 2

if __name__ == '__main__':
    lock = Lock()

    n = Value('i', 7)
    x = Value(c_double, 1.0/3.0, lock=False)
    s = Array('c', b'hello world', lock=lock)
    A = Array(Point, [(1.875,-6.25), (-5.75,2.0), (2.375,9.5)], lock=lock)

    p = Process(target=modify, args=(n, x, s, A))
    p.start()
    p.join()

    print(n.value)
    print(x.value)
    print(s.value)
    print([(a.x, a.y) for a in A])
```


The results printed are


49
0.1111111111111111
HELLO WORLD
[(3.515625, 39.0625), (33.0625, 4.0), (5.640625, 90.25)]

#### ◾ Managers

➡ `multiprocessing.Manager()`
✅ `class multiprocessing.managers.BaseManager([address[, authkey]])`
    ➡ `start([initializer[, initargs]])`
    ➡ `get_server()`
    ➡ `connect()`
    ➡ `shutdown()`
    ➡ `register(typeid[, callable[, proxytype[, exposed[, method_to_typeid[, create_method]]]]])`
    ➡↪ `typeid` is a “type identifier” of shared object.
    ➡↪ `callable` is a callable used for creating objects for this type identifier. 
    ➡↪ `proxytype` is a subclass of BaseProxy.
    ➡↪ `exposed` is used to specify a sequence of method names which proxies for this typeid should be allowed to access using `BaseProxy._callmethod()`.
    ➡↪ `method_to_typeid` is a mapping used to specify the return type of those exposed methods which should return a proxy.
    ➡↪ `create_method` determines whether a method should be created with name typeid which can be used to tell the server process to create a new shared object and return a proxy for it. By default it is True.
    ➡↪ `address` The address used by the manager.
✅ `class multiprocessing.managers.SyncManager`
    ➡ `Barrier(parties[, action[, timeout]])`
    ➡ `BoundedSemaphore([value])`
    ➡ `Condition([lock])`
    ➡ `Event()`
    ➡ `Lock()`
    ➡ `Namespace()`
    ➡ `Queue([maxsize])`
    ➡ `RLock()`
    ➡ `Semaphore([value])`
    ➡ `Array(typecode, sequence)`
    ➡ `Value(typecode, value)`
    ➡ `dict()dict(mapping)dict(sequence)`
    ➡ `list()`
    ➡ `list(sequence)`
✅ `class multiprocessing.managers.Namespace`

Managers provide a way to create data which can be shared between different processes, including sharing over a network between processes running on different machines. A manager object controls a server process which manages shared objects. Other processes can access the shared objects by using proxies.

➡ `multiprocessing.Manager()`
Returns a started SyncManager object which can be used for sharing objects between processes. The returned manager object corresponds to a spawned child process and has methods which will create shared objects and return corresponding proxies.

Manager processes will be shutdown as soon as they are garbage collected or their parent process exits. The manager classes are defined in the multiprocessing.managers module:

✅ `class multiprocessing.managers.BaseManager([address[, authkey]])`
Create a BaseManager object.

Once created one should call start() or get_server().serve_forever() to ensure that the manager object refers to a started manager process.

address is the address on which the manager process listens for new connections. If address is None then an arbitrary one is chosen.

authkey is the authentication key which will be used to check the validity of incoming connections to the server process. If authkey is None then current_process().authkey is used. Otherwise authkey is used and it must be a byte string.

➡ `start([initializer[, initargs]])`
Start a subprocess to start the manager. If initializer is not None then the subprocess will call `initializer(*initargs)` when it starts.

➡ `get_server()`
Returns a Server object which represents the actual server under the control of the Manager. The Server object supports the serve_forever() method:


>>> from multiprocessing.managers import BaseManager
>>> manager = BaseManager(address=('', 50000), authkey=b'abc')
>>> server = manager.get_server()
>>> server.serve_forever()


Server additionally has an address attribute.

➡ `connect()`
Connect a local manager object to a remote manager process:


>>> from multiprocessing.managers import BaseManager
>>> m = BaseManager(address=('127.0.0.1', 50000), authkey=b'abc')
>>> m.connect()


➡ `shutdown()`
Stop the process used by the manager. This is only available if start() has been used to start the server process.

This can be called multiple times.

➡ `register(typeid[, callable[, proxytype[, exposed[, method_to_typeid[, create_method]]]]])`
A classmethod which can be used for registering a type or callable with the manager class.

➡↪ `typeid` is a “type identifier” which is used to identify a particular type of shared object. This must be a string.

➡↪ `callable` is a callable used for creating objects for this type identifier. If a manager instance will be connected to the server using the connect() method, or if the create_method argument is False then this can be left as None.

➡↪ `proxytype` is a subclass of BaseProxy which is used to create proxies for shared objects with this typeid. If None then a proxy class is created automatically.

➡↪ `exposed` is used to specify a sequence of method names which proxies for this typeid should be allowed to access using `BaseProxy._callmethod()`. (If exposed is None then proxytype._exposed_ is used instead if it exists.) In the case where no exposed list is specified, all “public methods” of the shared object will be accessible. (Here a “public method” means any attribute which has a __call__() method and whose name does not begin with _ .)

➡↪ `method_to_typeid` is a mapping used to specify the return type of those exposed methods which should return a proxy. It maps method names to typeid strings. (If method_to_typeid is None then proxytype._method_to_typeid_ is used instead if it exists.) If a method’s name is not a key of this mapping or if the mapping is None then the object returned by the method will be copied by value.

➡↪ `create_method` determines whether a method should be created with name typeid which can be used to tell the server process to create a new shared object and return a proxy for it. By default it is True.

BaseManager instances also have one read-only property:

➡↪ `address`
The address used by the manager. The address used by the manager.

Changed in version 3.3: Manager objects support the context management protocol – see Context Manager Types. __enter__() starts the server process (if it has not already started) and then returns the manager object. __exit__() calls shutdown().

In previous versions __enter__() did not start the manager’s server process if it was not already started.


✅ `class multiprocessing.managers.SyncManager`
A subclass of BaseManager which can be used for the synchronization of processes. Objects of this type are returned by multiprocessing.Manager().

Its methods create and return Proxy Objects for a number of commonly used data types to be synchronized across processes. This notably includes shared lists and dictionaries.

➡ `Barrier(parties[, action[, timeout]])`
Create a shared threading.Barrier object and return a proxy for it.

New in version 3.3.


➡ `BoundedSemaphore([value])`
Create a shared threading.BoundedSemaphore object and return a proxy for it.

➡ `Condition([lock])`
Create a shared threading.Condition object and return a proxy for it.

If lock is supplied then it should be a proxy for a threading.Lock or threading.RLock object.

Changed in version 3.3: The wait_for() method was added.


➡ `Event()`
Create a shared threading.Event object and return a proxy for it.

➡ `Lock()`
Create a shared threading.Lock object and return a proxy for it.

➡ `Namespace()`
Create a shared Namespace object and return a proxy for it.

➡ `Queue([maxsize])`
Create a shared queue.Queue object and return a proxy for it.

➡ `RLock()`
Create a shared threading.RLock object and return a proxy for it.

➡ `Semaphore([value])`
Create a shared threading.Semaphore object and return a proxy for it.

➡ `Array(typecode, sequence)`
Create an array and return a proxy for it.

➡ `Value(typecode, value)`
Create an object with a writable value attribute and return a proxy for it.

➡ `dict()dict(mapping)dict(sequence)`
Create a shared dict object and return a proxy for it.

➡ `list()`
➡ `list(sequence)`
Create a shared list object and return a proxy for it.

Changed in version 3.6: Shared objects are capable of being nested. For example, a shared container object such as a shared list can contain other shared objects which will all be managed and synchronized by the SyncManager.


✅ `class multiprocessing.managers.Namespace`
A type that can register with SyncManager.

A namespace object has no public methods, but does have writable attributes. Its representation shows the values of its attributes.

However, when using a proxy for a namespace object, an attribute beginning with _ will be an attribute of the proxy and not an attribute of the referent:


>>> manager = multiprocessing.Manager()
>>> Global = manager.Namespace()
>>> Global.x = 10
>>> Global.y = 'hello'
>>> Global._z = 12.3    # this is an attribute of the proxy
>>> print(Global)
Namespace(x=10, y='hello')


#### ◾ Customized managers

To create one’s own manager, one creates a subclass of BaseManager and uses the register() classmethod to register new types or callables with the manager class. For example:


```py
from multiprocessing.managers import BaseManager

class MathsClass:
    def add(self, x, y):
        return x + y
    def mul(self, x, y):
        return x * y

class MyManager(BaseManager):
    pass

MyManager.register('Maths', MathsClass)

if __name__ == '__main__':
    with MyManager() as manager:
        maths = manager.Maths()
        print(maths.add(4, 3))         # prints 7
        print(maths.mul(7, 8))         # prints 56
```


#### ◾ Using a remote manager

It is possible to run a manager server on one machine and have clients use it from other machines (assuming that the firewalls involved allow it).

Running the following commands creates a server for a single shared queue which remote clients can access:


>>> from multiprocessing.managers import BaseManager
>>> from queue import Queue
>>> queue = Queue()
>>> class QueueManager(BaseManager): pass
>>> QueueManager.register('get_queue', callable=lambda:queue)
>>> m = QueueManager(address=('', 50000), authkey=b'abracadabra')
>>> s = m.get_server()
>>> s.serve_forever()


One client can access the server as follows:


>>> from multiprocessing.managers import BaseManager
>>> class QueueManager(BaseManager): pass
>>> QueueManager.register('get_queue')
>>> m = QueueManager(address=('foo.bar.org', 50000), authkey=b'abracadabra')
>>> m.connect()
>>> queue = m.get_queue()
>>> queue.put('hello')


Another client can also use it:


>>> from multiprocessing.managers import BaseManager
>>> class QueueManager(BaseManager): pass
>>> QueueManager.register('get_queue')
>>> m = QueueManager(address=('foo.bar.org', 50000), authkey=b'abracadabra')
>>> m.connect()
>>> queue = m.get_queue()
>>> queue.get()
'hello'


Local processes can also access that queue, using the code from above on the client to access it remotely:


>>> from multiprocessing import Process, Queue
>>> from multiprocessing.managers import BaseManager
>>> class Worker(Process):
...     def __init__(self, q):
...         self.q = q

➡ `...         super().__init__()`
...     def run(self):

➡ `...         self.q.put('local hello')`
...
>>> queue = Queue()
>>> w = Worker(queue)
>>> w.start()
>>> class QueueManager(BaseManager): pass
...
>>> QueueManager.register('get_queue', callable=lambda: queue)
>>> m = QueueManager(address=('', 50000), authkey=b'abracadabra')
>>> s = m.get_server()
>>> s.serve_forever()


#### ◾ Proxy Objects

✅ `class multiprocessing.managers.BaseProxy`
➡ ↪ `_callmethod(methodname[, args[, kwds]])`
➡ ↪ `_getvalue()`
➡ ↪ `__repr__()`
➡ ↪ `__str__()`

A proxy is an object which refers to a shared object which lives (presumably) in a different process. The shared object is said to be the referent of the proxy. Multiple proxy objects may have the same referent.

A proxy object has methods which invoke corresponding methods of its referent (although not every method of the referent will necessarily be available through the proxy). In this way, a proxy can be used just like its referent can:


>>> from multiprocessing import Manager
>>> manager = Manager()
>>> l = manager.list([i*i for i in range(10)])
>>> print(l)
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
>>> print(repr(l))
<ListProxy object, typeid 'list' at 0x...>
>>> l[4]
16
>>> l[2:5]
[4, 9, 16]


Notice that applying str() to a proxy will return the representation of the referent, whereas applying repr() will return the representation of the proxy.

An important feature of proxy objects is that they are picklable so they can be passed between processes. As such, a referent can contain Proxy Objects. This permits nesting of these managed lists, dicts, and other Proxy Objects:


>>> a = manager.list()
>>> b = manager.list()
>>> a.append(b)         # referent of a now contains referent of b
>>> print(a, b)
[<ListProxy object, typeid 'list' at ...>] []
>>> b.append('hello')
>>> print(a[0], b)
['hello'] ['hello']


Similarly, dict and list proxies may be nested inside one another:


>>> l_outer = manager.list([ manager.dict() for i in range(2) ])
>>> d_first_inner = l_outer[0]
>>> d_first_inner['a'] = 1
>>> d_first_inner['b'] = 2
>>> l_outer[1]['c'] = 3
>>> l_outer[1]['z'] = 26
>>> print(l_outer[0])
{'a': 1, 'b': 2}
>>> print(l_outer[1])
{'c': 3, 'z': 26}


If standard (non-proxy) list or dict objects are contained in a referent, modifications to those mutable values will not be propagated through the manager because the proxy has no way of knowing when the values contained within are modified. However, storing a value in a container proxy (which triggers a __setitem__ on the proxy object) does propagate through the manager and so to effectively modify such an item, one could re-assign the modified value to the container proxy:


```py
# create a list proxy and append a mutable object (a dictionary)
lproxy = manager.list()
lproxy.append({})
# now mutate the dictionary
d = lproxy[0]
d['a'] = 1
d['b'] = 2
# at this point, the changes to d are not yet synced, but by
# updating the dictionary, the proxy is notified of the change
lproxy[0] = d
```


This approach is perhaps less convenient than employing nested Proxy Objects for most use cases but also demonstrates a level of control over the synchronization.

Note:
 The proxy types in multiprocessing do nothing to support comparisons by value. So, for instance, we have:
 

>>> manager.list([1,2,3]) == [1,2,3]
False


One should just use a copy of the referent instead when making comparisons.

✅ `class multiprocessing.managers.BaseProxy`
Proxy objects are instances of subclasses of BaseProxy.

➡ ↪ `_callmethod(methodname[, args[, kwds]])`
Call and return the result of a method of the proxy’s referent.

If proxy is a proxy whose referent is obj then the expression


    proxy._callmethod(methodname, args, kwds)


will evaluate the expression



    getattr(obj, methodname)(*args, **kwds)


in the manager’s process.

The returned value will be a copy of the result of the call or a proxy to a new shared object – see documentation for the method_to_typeid argument of BaseManager.register().

If an exception is raised by the call, then is re-raised by `_callmethod()`. If some other exception is raised in the manager’s process then this is converted into a RemoteError exception and is raised by `_callmethod()`.

Note in particular that an exception will be raised if methodname has not been exposed.

An example of the usage of `_callmethod()`:


>>> l = manager.list(range(10))
>>> l._callmethod('__len__')
10
>>> l._callmethod('__getitem__', (slice(2, 7),)) # equivalent to l[2:7]
[2, 3, 4, 5, 6]
>>> l._callmethod('__getitem__', (20,))          # equivalent to l[20]
Traceback (most recent call last):
...
IndexError: list index out of range


➡ ↪ `_getvalue()`
Return a copy of the referent.

If the referent is unpicklable then this will raise an exception.

➡ ↪ `__repr__()`
Return a representation of the proxy object.

➡ ↪ `__str__()`
Return the representation of the referent.

#### ◾ Proxy Objects Cleanup

A proxy object uses a weakref callback so that when it gets garbage collected it deregisters itself from the manager which owns its referent.

A shared object gets deleted from the manager process when there are no longer any proxies referring to it.



#### ◾ Process Pools

✅ `class multiprocessing.pool.Pool([processes[, initializer[, initargs[, maxtasksperchild[, context]]]]])`
➡ `apply(func[, args[, kwds]])`
➡ `apply_async(func[, args[, kwds[, callback[, error_callback]]]])`
➡ `map(func, iterable[, chunksize])`
➡ `map_async(func, iterable[, chunksize[, callback[, error_callback]]])`
➡ `imap(func, iterable[, chunksize])`
➡ `imap_unordered(func, iterable[, chunksize])`
➡ `starmap(func, iterable[, chunksize])`
➡ `starmap_async(func, iterable[, chunksize[, callback[, error_callback]]])`
➡ `close()`
➡ `terminate()`
➡ `join()`
✅ `class multiprocessing.pool.AsyncResult`
➡ `get([timeout])`
➡ `wait([timeout])`
➡ `ready()`
➡ `successful()`

One can create a pool of processes which will carry out tasks submitted to it with the Pool class.

✅ `class multiprocessing.pool.Pool([processes[, initializer[, initargs[, maxtasksperchild[, context]]]]])`
A process pool object which controls a pool of worker processes to which jobs can be submitted. It supports asynchronous results with timeouts and callbacks and has a parallel map implementation.

`processes` is the number of worker processes to use. If processes is None then the number returned by os.cpu_count() is used.

If `initializer` is not None then each worker process will call `initializer(*initargs)` when it starts.

`maxtasksperchild` is the number of tasks a worker process can complete before it will exit and be replaced with a fresh worker process, to enable unused resources to be freed. The default maxtasksperchild is None, which means worker processes will live as long as the pool.

`context` can be used to specify the context used for starting the worker processes. Usually a pool is created using the function multiprocessing.Pool() or the Pool() method of a context object. In both cases context is set appropriately.

Note that the methods of the pool object should only be called by the process which created the pool.

Warning:
 multiprocessing.pool objects have internal resources that need to be properly managed (like any other resource) by using the pool as a context manager or by calling close() and terminate() manually. Failure to do this can lead to the process hanging on finalization.
 
Note that it is not correct to rely on the garbage collector to destroy the pool as CPython does not assure that the finalizer of the pool will be called (see object.__del__() for more information).

New in version 3.2: maxtasksperchild


New in version 3.4: context


Note:
 Worker processes within a Pool typically live for the complete duration of the Pool’s work queue. A frequent pattern found in other systems (such as Apache, mod_wsgi, etc) to free resources held by workers is to allow a worker within a pool to complete only a set amount of work before being exiting, being cleaned up and a new process spawned to replace the old one. The maxtasksperchild argument to the Pool exposes this ability to the end user.
 

➡ `apply(func[, args[, kwds]])`
Call func with arguments args and keyword arguments kwds. It blocks until the result is ready. Given this blocks, apply_async() is better suited for performing work in parallel. Additionally, func is only executed in one of the workers of the pool.

➡ `apply_async(func[, args[, kwds[, callback[, error_callback]]]])`
A variant of the apply() method which returns a AsyncResult object.

If callback is specified then it should be a callable which accepts a single argument. When the result becomes ready callback is applied to it, that is unless the call failed, in which case the error_callback is applied instead.

If error_callback is specified then it should be a callable which accepts a single argument. If the target function fails, then the error_callback is called with the exception instance.

Callbacks should complete immediately since otherwise the thread which handles the results will get blocked.

➡ `map(func, iterable[, chunksize])`
A parallel equivalent of the map() built-in function (it supports only one iterable argument though, for multiple iterables see starmap()). It blocks until the result is ready.

This method chops the iterable into a number of chunks which it submits to the process pool as separate tasks. The (approximate) size of these chunks can be specified by setting chunksize to a positive integer.

Note that it may cause high memory usage for very long iterables. Consider using imap() or imap_unordered() with explicit chunksize option for better efficiency.

➡ `map_async(func, iterable[, chunksize[, callback[, error_callback]]])`
A variant of the map() method which returns a AsyncResult object.

If callback is specified then it should be a callable which accepts a single argument. When the result becomes ready callback is applied to it, that is unless the call failed, in which case the error_callback is applied instead.

If error_callback is specified then it should be a callable which accepts a single argument. If the target function fails, then the error_callback is called with the exception instance.

Callbacks should complete immediately since otherwise the thread which handles the results will get blocked.

➡ `imap(func, iterable[, chunksize])`
A lazier version of map().

The chunksize argument is the same as the one used by the map() method. For very long iterables using a large value for chunksize can make the job complete much faster than using the default value of 1.

Also if chunksize is 1 then the next() method of the iterator returned by the imap() method has an optional timeout parameter: next(timeout) will raise multiprocessing.TimeoutError if the result cannot be returned within timeout seconds.

➡ `imap_unordered(func, iterable[, chunksize])`
The same as imap() except that the ordering of the results from the returned iterator should be considered arbitrary. (Only when there is only one worker process is the order guaranteed to be “correct”.)

➡ `starmap(func, iterable[, chunksize])`
Like map() except that the elements of the iterable are expected to be iterables that are unpacked as arguments.

Hence an iterable of [(1,2), (3, 4)] results in [func(1,2), func(3,4)].

New in version 3.3.


➡ `starmap_async(func, iterable[, chunksize[, callback[, error_callback]]])`
A combination of starmap() and map_async() that iterates over iterable of iterables and calls func with the iterables unpacked. Returns a result object.

New in version 3.3.


➡ `close()`
Prevents any more tasks from being submitted to the pool. Once all the tasks have been completed the worker processes will exit.

➡ `terminate()`
Stops the worker processes immediately without completing outstanding work. When the pool object is garbage collected terminate() will be called immediately.

➡ `join()`
Wait for the worker processes to exit. One must call close() or terminate() before using join().

New in version 3.3: Pool objects now support the context management protocol – see Context Manager Types. __enter__() returns the pool object, and __exit__() calls terminate().


✅ `class multiprocessing.pool.AsyncResult`
The class of the result returned by Pool.apply_async() and Pool.map_async().

➡ `get([timeout])`
Return the result when it arrives. If timeout is not None and the result does not arrive within timeout seconds then multiprocessing.TimeoutError is raised. If the remote call raised an exception then that exception will be reraised by get().

➡ `wait([timeout])`
Wait until the result is available or until timeout seconds pass.

➡ `ready()`
Return whether the call has completed.

➡ `successful()`
Return whether the call completed without raising an exception. Will raise ValueError if the result is not ready.

Changed in version 3.7: If the result is not ready, ValueError is raised instead of AssertionError.


The following example demonstrates the use of a pool:


```py
from multiprocessing import Pool
import time

def f(x):
    return x*x

if __name__ == '__main__':
    with Pool(processes=4) as pool:         # start 4 worker processes
        result = pool.apply_async(f, (10,)) # evaluate "f(10)" asynchronously in a single process
        print(result.get(timeout=1))        # prints "100" unless your computer is *very* slow

        print(pool.map(f, range(10)))       # prints "[0, 1, 4,..., 81]"

        it = pool.imap(f, range(10))
        print(next(it))                     # prints "0"
        print(next(it))                     # prints "1"
        print(it.next(timeout=1))           # prints "4" unless your computer is *very* slow

        result = pool.apply_async(time.sleep, (10,))
        print(result.get(timeout=1))        # raises multiprocessing.TimeoutError
```



#### ◾ Listeners and Clients

➡ `multiprocessing.connection.deliver_challenge(connection, authkey)`
➡ `multiprocessing.connection.answer_challenge(connection, authkey)`
➡ `multiprocessing.connection.Client(address[, family[, authkey]])`
✅ `class multiprocessing.connection.Listener([address[, family[, backlog[, authkey]]]])`
➡ `accept()`
➡ `close()`
➡ ↪ `address`
➡ ↪ `last_accepted`
➡ `multiprocessing.connection.wait(object_list, timeout=None)`


Usually message passing between processes is done using queues or by using Connection objects returned by Pipe().

However, the multiprocessing.connection module allows some extra flexibility. It basically gives a high level message oriented API for dealing with sockets or Windows named pipes. It also has support for digest authentication using the hmac module, and for polling multiple connections at the same time.

➡ `multiprocessing.connection.deliver_challenge(connection, authkey)`
Send a randomly generated message to the other end of the connection and wait for a reply.

If the reply matches the digest of the message using authkey as the key then a welcome message is sent to the other end of the connection. Otherwise AuthenticationError is raised.

➡ `multiprocessing.connection.answer_challenge(connection, authkey)`
Receive a message, calculate the digest of the message using authkey as the key, and then send the digest back.

If a welcome message is not received, then AuthenticationError is raised.

➡ `multiprocessing.connection.Client(address[, family[, authkey]])`
Attempt to set up a connection to the listener which is using address address, returning a Connection.

The type of the connection is determined by family argument, but this can generally be omitted since it can usually be inferred from the format of address. (See Address Formats)

If authkey is given and not None, it should be a byte string and will be used as the secret key for an HMAC-based authentication challenge. No authentication is done if authkey is None. AuthenticationError is raised if authentication fails. See Authentication keys.

✅ `class multiprocessing.connection.Listener([address[, family[, backlog[, authkey]]]])`
A wrapper for a bound socket or Windows named pipe which is ‘listening’ for connections.

`address` is the address to be used by the bound socket or named pipe of the listener object.

Note:
 If an address of ‘0.0.0.0’ is used, the address will not be a connectable end point on Windows. If you require a connectable end-point, you should use ‘127.0.0.1’.
 

`family` is the type of socket (or named pipe) to use. This can be one of the strings 'AF_INET' (for a TCP socket), 'AF_UNIX' (for a Unix domain socket) or 'AF_PIPE' (for a Windows named pipe). Of these only the first is guaranteed to be available. If family is None then the family is inferred from the format of address. If address is also None then a default is chosen. This default is the family which is assumed to be the fastest available. See Address Formats. Note that if family is 'AF_UNIX' and address is None then the socket will be created in a private temporary directory created using tempfile.mkstemp().

If the listener object uses a socket then `backlog` (1 by default) is passed to the listen() method of the socket once it has been bound.

If `authkey` is given and not None, it should be a byte string and will be used as the secret key for an HMAC-based authentication challenge. No authentication is done if authkey is None. AuthenticationError is raised if authentication fails. See Authentication keys.

➡ `accept()`
Accept a connection on the bound socket or named pipe of the listener object and return a Connection object. If authentication is attempted and fails, then AuthenticationError is raised.

➡ `close()`
Close the bound socket or named pipe of the listener object. This is called automatically when the listener is garbage collected. However it is advisable to call it explicitly.

Listener objects have the following read-only properties:

➡ ↪ `address`
The address which is being used by the Listener object.

➡ ↪ `last_accepted`
The address from which the last accepted connection came. If this is unavailable then it is None.

New in version 3.3: Listener objects now support the context management protocol – see Context Manager Types. __enter__() returns the listener object, and __exit__() calls close().


➡ `multiprocessing.connection.wait(object_list, timeout=None)`
Wait till an object in object_list is ready. Returns the list of those objects in object_list which are ready. If timeout is a float then the call blocks for at most that many seconds. If timeout is None then it will block for an unlimited period. A negative timeout is equivalent to a zero timeout.

For both Unix and Windows, an object can appear in object_list if it is

• a readable Connection object;
• a connected and readable socket.socket object; or
• the sentinel attribute of a Process object.

A connection or socket object is ready when there is data available to be read from it, or the other end has been closed.

Unix: wait(object_list, timeout) almost equivalent select.select(object_list, [], [], timeout). The difference is that, if select.select() is interrupted by a signal, it can raise OSError with an error number of EINTR, whereas wait() will not.

Windows: An item in object_list must either be an integer handle which is waitable (according to the definition used by the documentation of the Win32 function WaitForMultipleObjects()) or it can be an object with a fileno() method which returns a socket handle or pipe handle. (Note that pipe handles and socket handles are not waitable handles.)

New in version 3.3.


Examples

The following server code creates a listener which uses 'secret password' as an authentication key. It then waits for a connection and sends some data to the client:


```py
from multiprocessing.connection import Listener
from array import array

address = ('localhost', 6000)     # family is deduced to be 'AF_INET'

with Listener(address, authkey=b'secret password') as listener:
    with listener.accept() as conn:
        print('connection accepted from', listener.last_accepted)

        conn.send([2.25, None, 'junk', float])

        conn.send_bytes(b'hello')

        conn.send_bytes(array('i', [42, 1729]))
```


The following code connects to the server and receives some data from the server:


```py
from multiprocessing.connection import Client
from array import array


➡ `address = ('localhost', 6000)`

with Client(address, authkey=b'secret password') as conn:
    print(conn.recv())                  # => [2.25, None, 'junk', float]

    print(conn.recv_bytes())            # => 'hello'

    arr = array('i', [0, 0, 0, 0, 0])
    print(conn.recv_bytes_into(arr))    # => 8
    print(arr)                          # => array('i', [42, 1729, 0, 0, 0])
```


The following code uses wait() to wait for messages from multiple processes at once:


```py
import time, random
from multiprocessing import Process, Pipe, current_process
from multiprocessing.connection import wait

def foo(w):
    for i in range(10):
        w.send((i, current_process().name))
    w.close()

if __name__ == '__main__':
    readers = []

    for i in range(4):
        r, w = Pipe(duplex=False)
        readers.append(r)
        p = Process(target=foo, args=(w,))
        p.start()
        # We close the writable end of the pipe now to be sure that
        # p is the only process which owns a handle for it.  This
        # ensures that when p closes its handle for the writable end,
        # wait() will promptly report the readable end as being ready.
        w.close()

    while readers:
        for r in wait(readers):
            try:
                msg = r.recv()
            except EOFError:
                readers.remove(r)
            else:
                print(msg)
```


#### ◾ Address Formats

Address Formats

• An 'AF_INET' address is a tuple of the form (hostname, port) where hostname is a string and port is an integer.

• An 'AF_UNIX' address is a string representing a filename on the filesystem.

• An 'AF_PIPE' address is a string of the form `r'\.\pipe{PipeName}'`. To use Client() to connect to a named pipe on a remote computer called ServerName one should use an address of the form `r'\ServerName\pipe{PipeName}'` instead.

Note that any string beginning with two backslashes is assumed by default to be an 'AF_PIPE' address rather than an 'AF_UNIX' address.


#### ◾ Authentication keys

When one uses Connection.recv, the data received is automatically unpickled. Unfortunately unpickling data from an untrusted source is a security risk. Therefore Listener and Client() use the hmac module to provide digest authentication.

An authentication key is a byte string which can be thought of as a password: once a connection is established both ends will demand proof that the other knows the authentication key. (Demonstrating that both ends are using the same key does not involve sending the key over the connection.)

If authentication is requested but no authentication key is specified then the return value of current_process().authkey is used (see Process). This value will be automatically inherited by any Process object that the current process creates. This means that (by default) all processes of a multi-process program will share a single authentication key which can be used when setting up connections between themselves.

Suitable authentication keys can also be generated by using os.urandom().

#### ◾ Logging

Some support for logging is available. Note, however, that the logging package does not use process shared locks so it is possible (depending on the handler type) for messages from different processes to get mixed up.

➡ `multiprocessing.get_logger()`
Returns the logger used by multiprocessing. If necessary, a new one will be created.

When first created the logger has level `logging.NOTSET` and no default handler. Messages sent to this logger will not by default propagate to the root logger.

Note that on Windows child processes will only inherit the level of the parent process’s logger – any other customization of the logger will not be inherited.

➡ `multiprocessing.log_to_stderr(level=None)`
This function performs a call to get_logger() but in addition to returning the logger created by get_logger, it adds a handler which sends output to sys.stderr using format `'[%(levelname)s/%(processName)s] %(message)s'`. You can modify levelname of the logger by passing a level argument.

Below is an example session with logging turned on:


>>> import multiprocessing, logging
>>> logger = multiprocessing.log_to_stderr()
>>> logger.setLevel(logging.INFO)
>>> logger.warning('doomed')
[WARNING/MainProcess] doomed
>>> m = multiprocessing.Manager()
[INFO/SyncManager-...] child process calling self.run()
[INFO/SyncManager-...] created temp directory /.../pymp-...
[INFO/SyncManager-...] manager serving at '/.../listener-...'
>>> del m
[INFO/MainProcess] sending shutdown message to manager
[INFO/SyncManager-...] manager exiting with exitcode 0

For a full table of logging levels, see the logging module.


#### ◾ The multiprocessing.dummy module

multiprocessing.dummy replicates the API of multiprocessing but is no more than a wrapper around the threading module.

In particular, the Pool function provided by multiprocessing.dummy returns an instance of ThreadPool, which is a subclass of Pool that supports all the same method calls but uses a pool of worker threads rather than worker processes.

✅ `class multiprocessing.pool.ThreadPool([processes[, initializer[, initargs]]])`
A thread pool object which controls a pool of worker threads to which jobs can be submitted. ThreadPool instances are fully interface compatible with Pool instances, and their resources must also be properly managed, either by using the pool as a context manager or by calling close() and terminate() manually.

processes is the number of worker threads to use. If processes is None then the number returned by os.cpu_count() is used.

If initializer is not None then each worker process will call `initializer(*initargs)` when it starts.

Unlike Pool, maxtasksperchild and context cannot be provided.



Note:
 A ThreadPool shares the same interface as Pool, which is designed around a pool of processes and predates the introduction of the concurrent.futures module. As such, it inherits some operations that don’t make sense for a pool backed by threads, and it has its own type for representing the status of asynchronous jobs, AsyncResult, that is not understood by any other libraries.
 
Users should generally prefer to use concurrent.futures.ThreadPoolExecutor, which has a simpler interface that was designed around threads from the start, and which returns concurrent.futures.Future instances that are compatible with many other libraries, including asyncio.


### ===🗝 ◦ Programming guidelines

There are certain guidelines and idioms which should be adhered to when using multiprocessing.

#### ◾ All start methods

The following applies to all start methods.


➡ Avoid shared state

As far as possible one should try to avoid shifting large amounts of data between processes.

It is probably best to stick to using queues or pipes for communication between processes rather than using the lower level synchronization primitives.


➡ Picklability

Ensure that the arguments to the methods of proxies are picklable.


➡ Thread safety of proxies


Do not use a proxy object from more than one thread unless you protect it with a lock.

(There is never a problem with different processes using the same proxy.)


➡ Joining zombie processes

On Unix when a process finishes but has not been joined it becomes a zombie. There should never be very many because each time a new process starts (or active_children() is called) all completed processes which have not yet been joined will be joined. Also calling a finished process’s Process.is_alive will join the process. Even so it is probably good practice to explicitly join all the processes that you start.


➡ Better to inherit than pickle/unpickle

When using the spawn or forkserver start methods many types from multiprocessing need to be picklable so that child processes can use them. However, one should generally avoid sending shared objects to other processes using pipes or queues. Instead you should arrange the program so that a process which needs access to a shared resource created elsewhere can inherit it from an ancestor process.


➡ Avoid terminating processes


Using the Process.terminate method to stop a process is liable to cause any shared resources (such as locks, semaphores, pipes and queues) currently being used by the process to become broken or unavailable to other processes.

Therefore it is probably best to only consider using Process.terminate on processes which never use any shared resources.


➡ Joining processes that use queues


Bear in mind that a process that has put items in a queue will wait before terminating until all the buffered items are fed by the “feeder” thread to the underlying pipe. (The child process can call the Queue.cancel_join_thread method of the queue to avoid this behaviour.)

This means that whenever you use a queue you need to make sure that all items which have been put on the queue will eventually be removed before the process is joined. Otherwise you cannot be sure that processes which have put items on the queue will terminate. Remember also that non-daemonic processes will be joined automatically.

An example which will deadlock is the following:


```py
from multiprocessing import Process, Queue

def f(q):
    q.put('X' * 1000000)

if __name__ == '__main__':
    queue = Queue()
    p = Process(target=f, args=(queue,))
    p.start()
    p.join()                    # this deadlocks
    obj = queue.get()
```


A fix here would be to swap the last two lines (or simply remove the p.join() line).


➡ Explicitly pass resources to child processes


On Unix using the fork start method, a child process can make use of a shared resource created in a parent process using a global resource. However, it is better to pass the object as an argument to the constructor for the child process.

Apart from making the code (potentially) compatible with Windows and the other start methods this also ensures that as long as the child process is still alive the object will not be garbage collected in the parent process. This might be important if some resource is freed when the object is garbage collected in the parent process.

So for instance


```py
from multiprocessing import Process, Lock

def f():
    ... do something using "lock" ...

if __name__ == '__main__':
    lock = Lock()
    for i in range(10):
        Process(target=f).start()


should be rewritten as


from multiprocessing import Process, Lock

def f(l):
    ... do something using "l" ...

if __name__ == '__main__':
    lock = Lock()
    for i in range(10):
        Process(target=f, args=(lock,)).start()
```


Beware of replacing sys.stdin with a “file like object”



multiprocessing originally unconditionally called:


    os.close(sys.stdin.fileno())


in the `multiprocessing.Process._bootstrap()` method — this resulted in issues with processes-in-processes. This has been changed to:


```py
sys.stdin.close()
sys.stdin = open(os.open(os.devnull, os.O_RDONLY), closefd=False)
```

Which solves the fundamental issue of processes colliding with each other resulting in a bad file descriptor error, but introduces a potential danger to applications which replace sys.stdin() with a “file-like object” with output buffering. This danger is that if multiple processes call close() on this file-like object, it could result in the same data being flushed to the object multiple times, resulting in corruption.

If you write a file-like object and implement your own caching, you can make it fork-safe by storing the pid whenever you append to the cache, and discarding the cache when the pid changes. For example:


```py
@property
def cache(self):
    pid = os.getpid()
    if pid != self._pid:
        self._pid = pid
        self._cache = []
    return self._cache
```


For more information, see bpo-5155, bpo-5313 and bpo-5331


#### ◾ The spawn and forkserver start methods

There are a few extra restriction which don’t apply to the fork start method.


➡ More picklability

Ensure that all arguments to Process.__init__() are picklable. Also, if you subclass Process then make sure that instances will be picklable when the Process.start method is called.


➡ Global variables


Bear in mind that if code run in a child process tries to access a global variable, then the value it sees (if any) may not be the same as the value in the parent process at the time that Process.start was called.

However, global variables which are just module level constants cause no problems.


➡ Safe importing of main module


Make sure that the main module can be safely imported by a new Python interpreter without causing unintended side effects (such a starting a new process).

For example, using the spawn or forkserver start method running the following module would fail with a RuntimeError:


```py
from multiprocessing import Process

def foo():
    print('hello')

p = Process(target=foo)
p.start()
```


Instead one should protect the “entry point” of the program by using if __name__ == '__main__': as follows:


```py
from multiprocessing import Process, freeze_support, set_start_method

def foo():
    print('hello')

if __name__ == '__main__':
    freeze_support()
    set_start_method('spawn')
    p = Process(target=foo)
    p.start()
```


(The freeze_support() line can be omitted if the program will be run normally instead of frozen.)

This allows the newly spawned Python interpreter to safely import the module and then run the module’s foo() function.

Similar restrictions apply if a pool or manager is created in the main module.

### ===🗝 ◦ Examples

Demonstration of how to create and use customized managers and proxies:


```py
from multiprocessing import freeze_support
from multiprocessing.managers import BaseManager, BaseProxy
import operator

##

class Foo:
    def f(self):
        print('you called Foo.f()')
    def g(self):
        print('you called Foo.g()')
    def _h(self):
        print('you called Foo._h()')

# A simple generator function
def baz():
    for i in range(10):
        yield i*i

# Proxy type for generator objects
class GeneratorProxy(BaseProxy):
    _exposed_ = ['__next__']
    def __iter__(self):
        return self
    def __next__(self):
        return self._callmethod('__next__')

# Function to return the operator module
def get_operator_module():
    return operator

##

class MyManager(BaseManager):
    pass

# register the Foo class; make `f()` and `g()` accessible via proxy
MyManager.register('Foo1', Foo)

# register the Foo class; make `g()` and `_h()` accessible via proxy
MyManager.register('Foo2', Foo, exposed=('g', '_h'))

# register the generator function baz; use `GeneratorProxy` to make proxies
MyManager.register('baz', baz, proxytype=GeneratorProxy)

# register get_operator_module(); make public functions accessible via proxy
MyManager.register('operator', get_operator_module)

##

def test():
    manager = MyManager()
    manager.start()

    print('-' * 20)

    f1 = manager.Foo1()
    f1.f()
    f1.g()
    assert not hasattr(f1, '_h')
    assert sorted(f1._exposed_) == sorted(['f', 'g'])

    print('-' * 20)

    f2 = manager.Foo2()
    f2.g()
    f2._h()
    assert not hasattr(f2, 'f')
    assert sorted(f2._exposed_) == sorted(['g', '_h'])

    print('-' * 20)

    it = manager.baz()
    for i in it:
        print('<%d>' % i, end=' ')
    print()

    print('-' * 20)

    op = manager.operator()
    print('op.add(23, 45) =', op.add(23, 45))
    print('op.pow(2, 94) =', op.pow(2, 94))
    print('op._exposed_ =', op._exposed_)

##

if __name__ == '__main__':
    freeze_support()
    test()
```


Using Pool:


```py
import multiprocessing
import time
import random
import sys

#
# Functions used by test code
#

def calculate(func, args):
    result = func(*args)
    return '%s says that %s%s = %s' % (
        multiprocessing.current_process().name,
        func.__name__, args, result
        )

def calculatestar(args):
    return calculate(*args)

def mul(a, b):
    time.sleep(0.5 * random.random())
    return a * b

def plus(a, b):
    time.sleep(0.5 * random.random())
    return a + b

def f(x):
    return 1.0 / (x - 5.0)

def pow3(x):
    return x ** 3

def noop(x):
    pass

#
# Test code
#

def test():
    PROCESSES = 4
    print('Creating pool with %d processes\n' % PROCESSES)

    with multiprocessing.Pool(PROCESSES) as pool:
        #
        # Tests
        #

        TASKS = [(mul, (i, 7)) for i in range(10)] + \
                [(plus, (i, 8)) for i in range(10)]

        results = [pool.apply_async(calculate, t) for t in TASKS]
        imap_it = pool.imap(calculatestar, TASKS)
        imap_unordered_it = pool.imap_unordered(calculatestar, TASKS)

        print('Ordered results using pool.apply_async():')
        for r in results:
            print('\t', r.get())
        print()

        print('Ordered results using pool.imap():')
        for x in imap_it:
            print('\t', x)
        print()

        print('Unordered results using pool.imap_unordered():')
        for x in imap_unordered_it:
            print('\t', x)
        print()

        print('Ordered results using pool.map() --- will block till complete:')
        for x in pool.map(calculatestar, TASKS):
            print('\t', x)
        print()

        #
        # Test error handling
        #

        print('Testing error handling:')

        try:
            print(pool.apply(f, (5,)))
        except ZeroDivisionError:
            print('\tGot ZeroDivisionError as expected from pool.apply()')
        else:
            raise AssertionError('expected ZeroDivisionError')

        try:
            print(pool.map(f, list(range(10))))
        except ZeroDivisionError:
            print('\tGot ZeroDivisionError as expected from pool.map()')
        else:
            raise AssertionError('expected ZeroDivisionError')

        try:
            print(list(pool.imap(f, list(range(10)))))
        except ZeroDivisionError:
            print('\tGot ZeroDivisionError as expected from list(pool.imap())')
        else:
            raise AssertionError('expected ZeroDivisionError')

        it = pool.imap(f, list(range(10)))
        for i in range(10):
            try:
                x = next(it)
            except ZeroDivisionError:
                if i == 5:
                    pass
            except StopIteration:
                break
            else:
                if i == 5:
                    raise AssertionError('expected ZeroDivisionError')

        assert i == 9
        print('\tGot ZeroDivisionError as expected from IMapIterator.next()')
        print()

        #
        # Testing timeouts
        #

        print('Testing ApplyResult.get() with timeout:', end=' ')
        res = pool.apply_async(calculate, TASKS[0])
        while 1:
            sys.stdout.flush()
            try:
                sys.stdout.write('\n\t%s' % res.get(0.02))
                break
            except multiprocessing.TimeoutError:
                sys.stdout.write('.')
        print()
        print()

        print('Testing IMapIterator.next() with timeout:', end=' ')
        it = pool.imap(calculatestar, TASKS)
        while 1:
            sys.stdout.flush()
            try:
                sys.stdout.write('\n\t%s' % it.next(0.02))
            except StopIteration:
                break
            except multiprocessing.TimeoutError:
                sys.stdout.write('.')
        print()
        print()


if __name__ == '__main__':
    multiprocessing.freeze_support()
    test()
```


An example showing how to use queues to feed tasks to a collection of worker processes and collect the results:


```py
import time
import random

from multiprocessing import Process, Queue, current_process, freeze_support

#
# Function run by worker processes
#

def worker(input, output):
    for func, args in iter(input.get, 'STOP'):
        result = calculate(func, args)
        output.put(result)

#
# Function used to calculate result
#

def calculate(func, args):
    result = func(*args)
    return '%s says that %s%s = %s' % \
        (current_process().name, func.__name__, args, result)

#
# Functions referenced by tasks
#

def mul(a, b):
    time.sleep(0.5*random.random())
    return a * b

def plus(a, b):
    time.sleep(0.5*random.random())
    return a + b

#
#
#

def test():
    NUMBER_OF_PROCESSES = 4
    TASKS1 = [(mul, (i, 7)) for i in range(20)]
    TASKS2 = [(plus, (i, 8)) for i in range(10)]

    # Create queues
    task_queue = Queue()
    done_queue = Queue()

    # Submit tasks
    for task in TASKS1:
        task_queue.put(task)

    # Start worker processes
    for i in range(NUMBER_OF_PROCESSES):
        Process(target=worker, args=(task_queue, done_queue)).start()

    # Get and print results
    print('Unordered results:')
    for i in range(len(TASKS1)):
        print('\t', done_queue.get())

    # Add more tasks using `put()`
    for task in TASKS2:
        task_queue.put(task)

    # Get and print some more results
    for i in range(len(TASKS2)):
        print('\t', done_queue.get())

    # Tell child processes to stop
    for i in range(NUMBER_OF_PROCESSES):
        task_queue.put('STOP')


if __name__ == '__main__':
    freeze_support()
    test()
```




## ==⚡ • multiprocessing.shared_memory — Provides shared memory for direct access across processes

Source code: Lib/multiprocessing/shared_memory.py

✅ `class multiprocessing.managers.SharedMemoryManager([address[, authkey]])`
    ➡ `SharedMemory(size)`
    ➡ `ShareableList(sequence)`
✅ `class multiprocessing.shared_memory.SharedMemory(name=None, create=False, size=0)`
    ➡ `close()`
    ➡ `unlink()`
    ➡ `buf`
    ➡ `name`
    ➡ `size`
✅ `class multiprocessing.shared_memory.ShareableList(sequence=None, *, name=None)`
    ➡ ↪ `sequence` is used in populating a new ShareableList full of values.
    ➡ ↪ `name` is the unique name for the requested shared memory.
    ➡ `count(value)`
    ➡ `index(value)`
    ➡ `format`
    ➡ `shm`


New in version 3.8.

基于**对称多处理器结构**提供内存共享对象。

This module provides a class, `SharedMemory`, for the allocation and management of shared memory to be accessed by one or more processes on a multicore or symmetric multiprocessor (SMP) machine. To assist with the life-cycle management of shared memory especially across distinct processes, a BaseManager subclass, SharedMemoryManager, is also provided in the multiprocessing.managers module.

In this module, shared memory refers to “System V style” shared memory blocks (though is not necessarily implemented explicitly as such) and does not refer to “distributed shared memory”. This style of shared memory permits distinct processes to potentially read and write to a common (or shared) region of volatile memory. Processes are conventionally limited to only have access to their own process memory space but shared memory permits the sharing of data between processes, avoiding the need to instead send messages between processes containing that data. Sharing data directly via memory can provide significant performance benefits compared to sharing data via disk or socket or other communications requiring the serialization/deserialization and copying of data.

✅ `class multiprocessing.shared_memory.SharedMemory(name=None, create=False, size=0)`

Creates a new shared memory block or attaches to an existing shared memory block. Each shared memory block is assigned a unique name. In this way, one process can create a shared memory block with a particular name and a different process can attach to that same shared memory block using that same name.

As a resource for sharing data across processes, shared memory blocks may outlive the original process that created them. When one process no longer needs access to a shared memory block that might still be needed by other processes, the close() method should be called. When a shared memory block is no longer needed by any process, the unlink() method should be called to ensure proper cleanup.

➡ ↪ `name` is the unique name for the requested shared memory, specified as a string. When creating a new shared memory block, if None (the default) is supplied for the name, a novel name will be generated.

➡ ↪ `create` controls whether a new shared memory block is created (True) or an existing shared memory block is attached (False).

➡ ↪ `size` specifies the requested number of bytes when creating a new shared memory block. Because some platforms choose to allocate chunks of memory based upon that platform’s memory page size, the exact size of the shared memory block may be larger or equal to the size requested. When attaching to an existing shared memory block, the size parameter is ignored.

➡ `close()`
Closes access to the shared memory from this instance. In order to ensure proper cleanup of resources, all instances should call close() once the instance is no longer needed. Note that calling close() does not cause the shared memory block itself to be destroyed.

➡ `unlink()`
Requests that the underlying shared memory block be destroyed. In order to ensure proper cleanup of resources, unlink() should be called once (and only once) across all processes which have need for the shared memory block. After requesting its destruction, a shared memory block may or may not be immediately destroyed and this behavior may differ across platforms. Attempts to access data inside the shared memory block after unlink() has been called may result in memory access errors. Note: the last process relinquishing its hold on a shared memory block may call unlink() and close() in either order.

➡ `buf`
A memoryview of contents of the shared memory block.

➡ `name`
Read-only access to the unique name of the shared memory block.

➡ `size`
Read-only access to size in bytes of the shared memory block.

The following example demonstrates low-level use of SharedMemory instances:


>>> from multiprocessing import shared_memory
>>> shm_a = shared_memory.SharedMemory(create=True, size=10)
>>> type(shm_a.buf)
<class 'memoryview'>
>>> buffer = shm_a.buf
>>> len(buffer)
10
>>> buffer[:4] = bytearray([22, 33, 44, 55])  # Modify multiple at once
>>> buffer[4] = 100                           # Modify single byte at a time
>>> # Attach to an existing shared memory block
>>> shm_b = shared_memory.SharedMemory(shm_a.name)
>>> import array
>>> array.array('b', shm_b.buf[:5])  # Copy the data into a new array.array
array('b', [22, 33, 44, 55, 100])
>>> shm_b.buf[:5] = b'howdy'  # Modify via shm_b using bytes
>>> bytes(shm_a.buf[:5])      # Access via shm_a
b'howdy'
>>> shm_b.close()   # Close each SharedMemory instance
>>> shm_a.close()
>>> shm_a.unlink()  # Call unlink only once to release the shared memory


The following example demonstrates a practical use of the SharedMemory class with NumPy arrays, accessing the same numpy.ndarray from two distinct Python shells:


>>> # In the first Python interactive shell
>>> import numpy as np
>>> a = np.array([1, 1, 2, 3, 5, 8])  # Start with an existing NumPy array
>>> from multiprocessing import shared_memory
>>> shm = shared_memory.SharedMemory(create=True, size=a.nbytes)
>>> # Now create a NumPy array backed by shared memory
>>> b = np.ndarray(a.shape, dtype=a.dtype, buffer=shm.buf)
>>> b[:] = a[:]  # Copy the original data into shared memory
>>> b
array([1, 1, 2, 3, 5, 8])
>>> type(b)
<class 'numpy.ndarray'>
>>> type(a)
<class 'numpy.ndarray'>
>>> shm.name  # We did not specify a name so one was chosen for us
'psm_21467_46075'

>>> # In either the same shell or a new Python shell on the same machine
>>> import numpy as np
>>> from multiprocessing import shared_memory
>>> # Attach to the existing shared memory block
>>> existing_shm = shared_memory.SharedMemory(name='psm_21467_46075')
>>> # Note that a.shape is (6,) and a.dtype is np.int64 in this example
>>> c = np.ndarray((6,), dtype=np.int64, buffer=existing_shm.buf)
>>> c
array([1, 1, 2, 3, 5, 8])
>>> c[-1] = 888
>>> c
array([  1,   1,   2,   3,   5, 888])

>>> # Back in the first Python interactive shell, b reflects this change
>>> b
array([  1,   1,   2,   3,   5, 888])

>>> # Clean up from within the second Python shell
>>> del c  # Unnecessary; merely emphasizing the array is no longer used
>>> existing_shm.close()

>>> # Clean up from within the first Python shell
>>> del b  # Unnecessary; merely emphasizing the array is no longer used
>>> shm.close()
>>> shm.unlink()  # Free and release the shared memory block at the very end


✅ `class multiprocessing.managers.SharedMemoryManager([address[, authkey]])`
A subclass of BaseManager which can be used for the management of shared memory blocks across processes.

A call to start() on a SharedMemoryManager instance causes a new process to be started. This new process’s sole purpose is to manage the life cycle of all shared memory blocks created through it. To trigger the release of all shared memory blocks managed by that process, call shutdown() on the instance. This triggers a SharedMemory.unlink() call on all of the SharedMemory objects managed by that process and then stops the process itself. By creating SharedMemory instances through a SharedMemoryManager, we avoid the need to manually track and trigger the freeing of shared memory resources.

This class provides methods for creating and returning SharedMemory instances and for creating a list-like object (ShareableList) backed by shared memory.

Refer to multiprocessing.managers.BaseManager for a description of the inherited `address` and `authkey` optional input arguments and how they may be used to connect to an existing SharedMemoryManager service from other processes.


➡ `SharedMemory(size)`
Create and return a new SharedMemory object with the specified size in bytes.

➡ `ShareableList(sequence)`
Create and return a new ShareableList object, initialized by the values from the input sequence.

The following example demonstrates the basic mechanisms of a SharedMemoryManager:


>>> from multiprocessing.managers import SharedMemoryManager
>>> smm = SharedMemoryManager()
>>> smm.start()  # Start the process that manages the shared memory blocks
>>> sl = smm.ShareableList(range(4))
>>> sl
ShareableList([0, 1, 2, 3], name='psm_6572_7512')
>>> raw_shm = smm.SharedMemory(size=128)
>>> another_sl = smm.ShareableList('alpha')
>>> another_sl
ShareableList(['a', 'l', 'p', 'h', 'a'], name='psm_6572_12221')
>>> smm.shutdown()  # Calls unlink() on sl, raw_shm, and another_sl


The following example depicts a potentially more convenient pattern for using SharedMemoryManager objects via the with statement to ensure that all shared memory blocks are released after they are no longer needed:


>>> with SharedMemoryManager() as smm:
...     sl = smm.ShareableList(range(2000))
...     # Divide the work among two processes, storing partial results in sl
...     p1 = Process(target=do_work, args=(sl, 0, 1000))
...     p2 = Process(target=do_work, args=(sl, 1000, 2000))
...     p1.start()
...     p2.start()  # A multiprocessing.Pool might be more efficient
...     p1.join()
...     p2.join()   # Wait for all work to complete in both processes
...     total_result = sum(sl)  # Consolidate the partial results now in sl


When using a SharedMemoryManager in a with statement, the shared memory blocks created using that manager are all released when the with statement’s code block finishes execution.

✅ `class multiprocessing.shared_memory.ShareableList(sequence=None, *, name=None)`
Provides a mutable list-like object where all values stored within are stored in a shared memory block. This constrains storable values to only the int, float, bool, str (less than 10M bytes each), bytes (less than 10M bytes each), and None built-in data types. It also notably differs from the built-in list type in that these lists can not change their overall length (i.e. no append, insert, etc.) and do not support the dynamic creation of new ShareableList instances via slicing.

➡ ↪ `sequence` is used in populating a new ShareableList full of values. Set to None to instead attach to an already existing ShareableList by its unique shared memory name.

➡ ↪ `name` is the unique name for the requested shared memory, as described in the definition for SharedMemory. When attaching to an existing ShareableList, specify its shared memory block’s unique name while leaving sequence set to None.

➡ `count(value)`
Returns the number of occurrences of value.

➡ `index(value)`
Returns first index position of value. Raises ValueError if value is not present.

➡ `format`
Read-only attribute containing the struct packing format used by all currently stored values.

➡ `shm`
The SharedMemory instance where the values are stored.

The following example demonstrates basic use of a ShareableList instance:


>>> from multiprocessing import shared_memory
>>> a = shared_memory.ShareableList(['howdy', b'HoWdY', -273.154, 100, None, True, 42])
>>> [ type(entry) for entry in a ]
[<class 'str'>, <class 'bytes'>, <class 'float'>, <class 'int'>, <class 'NoneType'>, <class 'bool'>, <class 'int'>]
>>> a[2]
-273.154
>>> a[2] = -78.5
>>> a[2]
-78.5
>>> a[2] = 'dry ice'  # Changing data types is supported as well
>>> a[2]
'dry ice'
>>> a[2] = 'larger than previously allocated storage space'
Traceback (most recent call last):
  ...
ValueError: exceeds available storage for existing str
>>> a[2]
'dry ice'
>>> len(a)
7
>>> a.index(42)
6
>>> a.count(b'howdy')
0
>>> a.count(b'HoWdY')
1
>>> a.shm.close()
>>> a.shm.unlink()
>>> del a  # Use of a ShareableList after call to unlink() is unsupported


The following example depicts how one, two, or many processes may access the same ShareableList by supplying the name of the shared memory block behind it:


>>> b = shared_memory.ShareableList(range(5))         # In a first process
>>> c = shared_memory.ShareableList(name=b.shm.name)  # In a second process
>>> c
ShareableList([0, 1, 2, 3, 4], name='...')
>>> c[-1] = -999
>>> b[-1]
-999
>>> b.shm.close()
>>> c.shm.close()
>>> c.shm.unlink()


The following examples demonstrates that ShareableList (and underlying SharedMemory) objects can be pickled and unpickled if needed. Note, that it will still be the same shared object. This happens, because the deserialized object has the same unique name and is just attached to an existing object with the same name (if the object is still alive):


>>> import pickle
>>> from multiprocessing import shared_memory
>>> sl = shared_memory.ShareableList(range(10))
>>> list(sl)
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]



>>> deserialized_sl = pickle.loads(pickle.dumps(sl))
>>> list(deserialized_sl)
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]



>>> sl[0] = -1
>>> deserialized_sl[1] = -2
>>> list(sl)
[-1, -2, 2, 3, 4, 5, 6, 7, 8, 9]
>>> list(deserialized_sl)
[-1, -2, 2, 3, 4, 5, 6, 7, 8, 9]



>>> sl.shm.close()
>>> sl.shm.unlink()


## ==⚡ • The concurrent package

Currently, there is only one module in this package:

• concurrent.futures – Launching parallel tasks

### ===🗝 17.4. concurrent.futures — Launching parallel tasks

New in version 3.2.

Source code: Lib/concurrent/futures/thread.py and Lib/concurrent/futures/process.py


The concurrent.futures module provides a high-level interface for asynchronously executing callables.

The asynchronous execution can be performed with threads, using ThreadPoolExecutor, or separate processes, using ProcessPoolExecutor. Both implement the same interface, which is defined by the abstract Executor class.


### ===🗝 17.4.1. Executor Objects

    class concurrent.futures.Executor

    Executor.submit(fn, *args, **kwargs)
    Executor.map(func, *iterables, timeout=None, chunksize=1)
    Executor.shutdown(wait=True)

✅ `class concurrent.futures.Executor`
An abstract class that provides methods to execute calls asynchronously. It should not be used directly, but through its concrete subclasses.


➡ `submit(fn, /, *args, **kwargs)`
Schedules the callable, fn, to be executed as `fn(*args, **kwargs)` and returns a Future object representing the execution of the callable.


```py
with ThreadPoolExecutor(max_workers=1) as executor:
    future = executor.submit(pow, 323, 1235)
    print(future.result())
```


➡ `map(func, *iterables, timeout=None, chunksize=1)`
Similar to `map(func, *iterables)` except:

• the iterables are collected immediately rather than lazily;
• func is executed asynchronously and several calls to func may be made concurrently.

The returned iterator raises a concurrent.futures.TimeoutError if __next__() is called and the result isn’t available after timeout seconds from the original call to Executor.map(). timeout can be an int or a float. If timeout is not specified or None, there is no limit to the wait time.

If a func call raises an exception, then that exception will be raised when its value is retrieved from the iterator.

When using ProcessPoolExecutor, this method chops iterables into a number of chunks which it submits to the pool as separate tasks. The (approximate) size of these chunks can be specified by setting chunksize to a positive integer. For very long iterables, using a large value for chunksize can significantly improve performance compared to the default size of 1. With ThreadPoolExecutor, chunksize has no effect.


Changed in version 3.5: Added the chunksize argument.

➡ `shutdown(wait=True, *, cancel_futures=False)`
Signal the executor that it should free any resources that it is using when the currently pending futures are done executing. Calls to Executor.submit() and Executor.map() made after shutdown will raise RuntimeError.

If wait is True then this method will not return until all the pending futures are done executing and the resources associated with the executor have been freed. If wait is False then this method will return immediately and the resources associated with the executor will be freed when all pending futures are done executing. Regardless of the value of wait, the entire Python program will not exit until all pending futures are done executing.

If cancel_futures is True, this method will cancel all pending futures that the executor has not started running. Any futures that are completed or running won’t be cancelled, regardless of the value of cancel_futures.

If both cancel_futures and wait are True, all futures that the executor has started running will be completed prior to this method returning. The remaining futures are cancelled.

You can avoid having to call this method explicitly if you use the with statement, which will shutdown the Executor (waiting as if Executor.shutdown() were called with wait set to True):


```py
import shutil
with ThreadPoolExecutor(max_workers=4) as e:
    e.submit(shutil.copy, 'src1.txt', 'dest1.txt')
    e.submit(shutil.copy, 'src2.txt', 'dest2.txt')
    e.submit(shutil.copy, 'src3.txt', 'dest3.txt')
    e.submit(shutil.copy, 'src4.txt', 'dest4.txt')
```


Changed in version 3.9: Added cancel_futures.


### ===🗝 17.4.2. ThreadPoolExecutor

ThreadPoolExecutor is an Executor subclass that uses a pool of threads to execute calls asynchronously.

Deadlocks can occur when the callable associated with a Future waits on the results of another Future. For example:


```py
import time
def wait_on_b():
    time.sleep(5)
    print(b.result())  # b will never complete because it is waiting on a.
    return 5

def wait_on_a():
    time.sleep(5)
    print(a.result())  # a will never complete because it is waiting on b.
    return 6


executor = ThreadPoolExecutor(max_workers=2)
a = executor.submit(wait_on_b)
b = executor.submit(wait_on_a)
```


And:


```py
def wait_on_future():
    f = executor.submit(pow, 5, 2)
    # This will never complete because there is only one worker thread and
    # it is executing this function.
    print(f.result())

executor = ThreadPoolExecutor(max_workers=1)
executor.submit(wait_on_future)
```


➡ `class concurrent.futures.ThreadPoolExecutor(max_workers=None, thread_name_prefix='', initializer=None, initargs=())`
An Executor subclass that uses a pool of at most max_workers threads to execute calls asynchronously.

initializer is an optional callable that is called at the start of each worker thread; initargs is a tuple of arguments passed to the initializer. Should initializer raise an exception, all currently pending jobs will raise a BrokenThreadPool, as well as any attempt to submit more jobs to the pool.


Changed in version 3.5: If max_workers is None or not given, it will default to the number of processors on the machine, multiplied by 5, assuming that ThreadPoolExecutor is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for ProcessPoolExecutor.

New in version 3.6: The thread_name_prefix argument was added to allow users to control the threading.Thread names for worker threads created by the pool for easier debugging.



Changed in version 3.7: Added the initializer and initargs arguments.


Changed in version 3.8: Default value of max_workers is changed to min(32, os.cpu_count() + 4). This default value preserves at least 5 workers for I/O bound tasks. It utilizes at most 32 CPU cores for CPU bound tasks which release the GIL. And it avoids using very large resources implicitly on many-core machines.

ThreadPoolExecutor now reuses idle worker threads before starting max_workers worker threads too.




### ===🗝 17.4.2.1. ThreadPoolExecutor Example


```py
import concurrent.futures
import urllib.request

URLS = ['http://www.foxnews.com/',
        'http://www.cnn.com/',
        'http://europe.wsj.com/',
        'http://www.bbc.co.uk/',
        'http://some-made-up-domain.com/']

# Retrieve a single page and report the URL and contents
def load_url(url, timeout):
    with urllib.request.urlopen(url, timeout=timeout) as conn:
        return conn.read()

# We can use a with statement to ensure threads are cleaned up promptly
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    # Start the load operations and mark each future with its URL
    future_to_url = {executor.submit(load_url, url, 60): url for url in URLS}
    for future in concurrent.futures.as_completed(future_to_url):
        url = future_to_url[future]
        try:
            data = future.result()
        except Exception as exc:
            print('%r generated an exception: %s' % (url, exc))
        else:
            print('%r page is %d bytes' % (url, len(data)))
```




### ===🗝 17.4.3. ProcessPoolExecutor

The ProcessPoolExecutor class is an Executor subclass that uses a pool of processes to execute calls asynchronously. ProcessPoolExecutor uses the multiprocessing module, which allows it to side-step the Global Interpreter Lock but also means that only picklable objects can be executed and returned.

The __main__ module must be importable by worker subprocesses. This means that ProcessPoolExecutor will not work in the interactive interpreter.

Calling Executor or Future methods from a callable submitted to a ProcessPoolExecutor will result in deadlock.

➡ `class concurrent.futures.ProcessPoolExecutor(max_workers=None, mp_context=None, initializer=None, initargs=())`
An Executor subclass that executes calls asynchronously using a pool of at most max_workers processes. If max_workers is None or not given, it will default to the number of processors on the machine. If max_workers is less than or equal to 0, then a ValueError will be raised. On Windows, max_workers must be less than or equal to 61. If it is not then ValueError will be raised. If max_workers is None, then the default chosen will be at most 61, even if more processors are available. mp_context can be a multiprocessing context or None. It will be used to launch the workers. If mp_context is None or not given, the default multiprocessing context is used.

initializer is an optional callable that is called at the start of each worker process; initargs is a tuple of arguments passed to the initializer. Should initializer raise an exception, all currently pending jobs will raise a BrokenProcessPool, as well as any attempt to submit more jobs to the pool.


Changed in version 3.3: When one of the worker processes terminates abruptly, a BrokenProcessPool error is now raised. Previously, behaviour was undefined but operations on the executor or its futures would often freeze or deadlock.


Changed in version 3.7: The mp_context argument was added to allow users to control the start_method for worker processes created by the pool.

Added the initializer and initargs arguments.


### ===🗝 17.4.3.1. ProcessPoolExecutor Example


```py
import concurrent.futures
import math

PRIMES = [
    112272535095293,
    112582705942171,
    112272535095293,
    115280095190773,
    115797848077099,
    1099726899285419]

def is_prime(n):
    if n < 2:
        return False
    if n == 2:
        return True
    if n % 2 == 0:
        return False

    sqrt_n = int(math.floor(math.sqrt(n)))
    for i in range(3, sqrt_n + 1, 2):
        if n % i == 0:
            return False
    return True

def main():
    with concurrent.futures.ProcessPoolExecutor() as executor:
        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):
            print('%d is prime: %s' % (number, prime))

if __name__ == '__main__':
    main()
```



### ===🗝 17.4.4. Future Objects

class concurrent.futures.Future

    Future.cancel()
    Future.cancelled()
    Future.running()
    Future.done()
    Future.result(timeout=None)
    Future.exception(timeout=None)
    Future.add_done_callback(fn)
    Future.set_running_or_notify_cancel()
    Future.set_result(result)
    Future.set_exception(exception)

The Future class encapsulates the asynchronous execution of a callable. Future instances are created by Executor.submit().

✅ `class concurrent.futures.Future`
Encapsulates the asynchronous execution of a callable. Future instances are created by Executor.submit() and should not be created directly except for testing.


➡ ↪`cancel()`
Attempt to cancel the call. If the call is currently being executed or finished running and cannot be cancelled then the method will return False, otherwise the call will be cancelled and the method will return True.

➡ ↪`cancelled()`
Return True if the call was successfully cancelled.

➡ ↪`running()`
Return True if the call is currently being executed and cannot be cancelled.

➡ ↪`done()`
Return True if the call was successfully cancelled or finished running.

➡ ↪`result(timeout=None)`
Return the value returned by the call. If the call hasn’t yet completed then this method will wait up to timeout seconds. If the call hasn’t completed in timeout seconds, then a concurrent.futures.TimeoutError will be raised. timeout can be an int or float. If timeout is not specified or None, there is no limit to the wait time.

If the future is cancelled before completing then CancelledError will be raised.

If the call raised an exception, this method will raise the same exception.

➡ ↪`exception(timeout=None)`
Return the exception raised by the call. If the call hasn’t yet completed then this method will wait up to timeout seconds. If the call hasn’t completed in timeout seconds, then a concurrent.futures.TimeoutError will be raised. timeout can be an int or float. If timeout is not specified or None, there is no limit to the wait time.

If the future is cancelled before completing then CancelledError will be raised.

If the call completed without raising, None is returned.

➡ ↪`add_done_callback(fn)`
Attaches the callable fn to the future. fn will be called, with the future as its only argument, when the future is cancelled or finishes running.

Added callables are called in the order that they were added and are always called in a thread belonging to the process that added them. If the callable raises an Exception subclass, it will be logged and ignored. If the callable raises a BaseException subclass, the behavior is undefined.

If the future has already completed or been cancelled, fn will be called immediately.

The following Future methods are meant for use in unit tests and Executor implementations.



➡ ↪`set_running_or_notify_cancel()`
This method should only be called by Executor implementations before executing the work associated with the Future and by unit tests.

If the method returns False then the Future was cancelled, i.e. Future.cancel() was called and returned True. Any threads waiting on the Future completing (i.e. through as_completed() or wait()) will be woken up.

If the method returns True then the Future was not cancelled and has been put in the running state, i.e. calls to Future.running() will return True.

This method can only be called once and cannot be called after Future.set_result() or Future.set_exception() have been called.

➡ ↪`set_result(result)`
Sets the result of the work associated with the Future to result.

This method should only be used by Executor implementations and unit tests.


Changed in version 3.8: This method raises concurrent.futures.InvalidStateError if the Future is already done.

➡ ↪`set_exception(exception)`
Sets the result of the work associated with the Future to the Exception exception.

This method should only be used by Executor implementations and unit tests.


Changed in version 3.8: This method raises concurrent.futures.InvalidStateError if the Future is already done.



### ===🗝 17.4.5. Module Functions

➡ `concurrent.futures.wait(fs, timeout=None, return_when=ALL_COMPLETED)`
Wait for the Future instances (possibly created by different Executor instances) given by fs to complete. Duplicate futures given to fs are removed and will be returned only once. Returns a named 2-tuple of sets. The first set, named done, contains the futures that completed (finished or cancelled futures) before the wait completed. The second set, named not_done, contains the futures that did not complete (pending or running futures).

timeout can be used to control the maximum number of seconds to wait before returning. timeout can be an int or float. If timeout is not specified or None, there is no limit to the wait time.

return_when indicates when this function should return. It must be one of the following constants:


| Constant        | Description |
|-----------------|-------------|
| FIRST_COMPLETED | The function will return when any future finishes or is cancelled. 
| FIRST_EXCEPTION | The function will return when any future finishes by raising an exception. If no future raises an exception then it is equivalent to ALL_COMPLETED. 
| ALL_COMPLETED   | The function will return when all futures finish or are cancelled. 



➡ `concurrent.futures.as_completed(fs, timeout=None)`
Returns an iterator over the Future instances (possibly created by different Executor instances) given by fs that yields futures as they complete (finished or cancelled futures). Any futures given by fs that are duplicated will be returned once. Any futures that completed before as_completed() is called will be yielded first. The returned iterator raises a concurrent.futures.TimeoutError if __next__() is called and the result isn’t available after timeout seconds from the original call to as_completed(). timeout can be an int or float. If timeout is not specified or None, there is no limit to the wait time.

See also:
 PEP 3148 – futures - execute computations asynchronouslyThe proposal which described this feature for inclusion in the Python standard library.


### ===🗝 17.4.6. Exception classes


✅ `exception concurrent.futures.CancelledError`
Raised when a future is cancelled.

✅ `exception concurrent.futures.TimeoutError`
Raised when a future operation exceeds the given timeout.

✅ `exception concurrent.futures.BrokenExecutor`
Derived from RuntimeError, this exception class is raised when an executor is broken for some reason, and cannot be used to submit or execute new tasks.

New in version 3.7.


✅ `exception concurrent.futures.InvalidStateError`
Raised when an operation is performed on a future that is not allowed in the current state.

New in version 3.8.


✅ `exception concurrent.futures.thread.BrokenThreadPool`
Derived from BrokenExecutor, this exception class is raised when one of the workers of a ThreadPoolExecutor has failed initializing.

New in version 3.7.


✅ `exception concurrent.futures.process.BrokenProcessPool`
Derived from BrokenExecutor (formerly RuntimeError), this exception class is raised when one of the workers of a ProcessPoolExecutor has terminated in a non-clean fashion (for example, if it was killed from the outside).

New in version 3.3.




## ==⚡ • subprocess — Subprocess management

Source code: Lib/subprocess.py


The subprocess module allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes. This module intends to replace several older modules and functions:


    os.system
    os.spawn*


Information about how the subprocess module can be used to replace these modules and functions can be found in the following sections.

See also:
 PEP 324 – PEP proposing the subprocess module
 

### ===🗝 ◦ Using the subprocess Module

The recommended approach to invoking subprocesses is to use the run() function for all use cases it can handle. For more advanced use cases, the underlying Popen interface can be used directly.

The run() function was added in Python 3.5; if you need to retain compatibility with older versions, see the Older high-level API section.

➡ `subprocess.run(args, *, stdin=None, input=None, stdout=None, stderr=None, capture_output=False, shell=False, cwd=None, timeout=None, check=False, encoding=None, errors=None, text=None, env=None, universal_newlines=None, **other_popen_kwargs)`
Run the command described by args. Wait for command to complete, then return a CompletedProcess instance.

The arguments shown above are merely the most common ones, described below in Frequently Used Arguments (hence the use of keyword-only notation in the abbreviated signature). The full function signature is largely the same as that of the Popen constructor - most of the arguments to this function are passed through to that interface. (timeout, input, check, and capture_output are not.)

If capture_output is true, stdout and stderr will be captured. When used, the internal Popen object is automatically created with stdout=PIPE and stderr=PIPE. The stdout and stderr arguments may not be supplied at the same time as capture_output. If you wish to capture and combine both streams into one, use stdout=PIPE and stderr=STDOUT instead of capture_output.

The timeout argument is passed to Popen.communicate(). If the timeout expires, the child process will be killed and waited for. The TimeoutExpired exception will be re-raised after the child process has terminated.

The input argument is passed to Popen.communicate() and thus to the subprocess’s stdin. If used it must be a byte sequence, or a string if encoding or errors is specified or text is true. When used, the internal Popen object is automatically created with stdin=PIPE, and the stdin argument may not be used as well.

If check is true, and the process exits with a non-zero exit code, a CalledProcessError exception will be raised. Attributes of that exception hold the arguments, the exit code, and stdout and stderr if they were captured.

If encoding or errors are specified, or text is true, file objects for stdin, stdout and stderr are opened in text mode using the specified encoding and errors or the io.TextIOWrapper default. The universal_newlines argument is equivalent to text and is provided for backwards compatibility. By default, file objects are opened in binary mode.

If env is not None, it must be a mapping that defines the environment variables for the new process; these are used instead of the default behavior of inheriting the current process’ environment. It is passed directly to Popen.

Examples:


>>> subprocess.run(["ls", "-l"])  # doesn't capture output
CompletedProcess(args=['ls', '-l'], returncode=0)

>>> subprocess.run("exit 1", shell=True, check=True)
Traceback (most recent call last):
  ...

➡ `subprocess.CalledProcessError: Command 'exit 1' returned non-zero exit status 1`

>>> subprocess.run(["ls", "-l", "/dev/null"], capture_output=True)
CompletedProcess(args=['ls', '-l', '/dev/null'], returncode=0,
stdout=b'crw-rw-rw- 1 root root 1, 3 Jan 23 16:23 /dev/null\n', stderr=b'')


New in version 3.5.


Changed in version 3.6: Added encoding and errors parameters


Changed in version 3.7: Added the text parameter, as a more understandable alias of universal_newlines. Added the capture_output parameter.


✅ `class subprocess.CompletedProcess`
The return value from run(), representing a process that has finished.

➡ ↪ `args`
The arguments used to launch the process. This may be a list or a string.

➡ ↪ `returncode`
Exit status of the child process. Typically, an exit status of 0 indicates that it ran successfully.

A negative value -N indicates that the child was terminated by signal N (POSIX only).

➡ ↪ `stdout`
Captured stdout from the child process. A bytes sequence, or a string if run() was called with an encoding, errors, or text=True. None if stdout was not captured.

If you ran the process with stderr=subprocess.STDOUT, stdout and stderr will be combined in this attribute, and stderr will be None.

➡ ↪ `stderr`
Captured stderr from the child process. A bytes sequence, or a string if run() was called with an encoding, errors, or text=True. None if stderr was not captured.

➡ ↪ `check_returncode()`
If returncode is non-zero, raise a CalledProcessError.

New in version 3.5.


➡ `subprocess.DEVNULL`
Special value that can be used as the stdin, stdout or stderr argument to Popen and indicates that the special file os.devnull will be used.

New in version 3.3.


➡ `subprocess.PIPE`
Special value that can be used as the stdin, stdout or stderr argument to Popen and indicates that a pipe to the standard stream should be opened. Most useful with Popen.communicate().

➡ `subprocess.STDOUT`
Special value that can be used as the stderr argument to Popen and indicates that standard error should go into the same handle as standard output.

✅ `exception subprocess.SubprocessError`
Base class for all other exceptions from this module.

New in version 3.3.


✅ `exception subprocess.TimeoutExpired`
Subclass of SubprocessError, raised when a timeout expires while waiting for a child process.

➡ ↪ `cmd`
Command that was used to spawn the child process.

➡ ↪ `timeout`
Timeout in seconds.

➡ ↪ `output`
Output of the child process if it was captured by run() or check_output(). Otherwise, None.

➡ ↪ `stdout`
Alias for output, for symmetry with stderr.

➡ ↪ `stderr`
Stderr output of the child process if it was captured by run(). Otherwise, None.

New in version 3.3.


Changed in version 3.5: stdout and stderr attributes added


✅ `exception subprocess.CalledProcessError`
Subclass of SubprocessError, raised when a process run by check_call() or check_output() returns a non-zero exit status.

➡ ↪ `returncode`
Exit status of the child process. If the process exited due to a signal, this will be the negative signal number.

➡ ↪ `cmd`
Command that was used to spawn the child process.

➡ ↪ `output`
Output of the child process if it was captured by run() or check_output(). Otherwise, None.

➡ ↪ `stdout`
Alias for output, for symmetry with stderr.

➡ ↪ `stderr`
Stderr output of the child process if it was captured by run(). Otherwise, None.

Changed in version 3.5: stdout and stderr attributes added


#### ◾Frequently Used Arguments

To support a wide variety of use cases, the Popen constructor (and the convenience functions) accept a large number of optional arguments. For most typical use cases, many of these arguments can be safely left at their default values. The arguments that are most commonly needed are:



`args` is required for all calls and should be a string, or a sequence of program arguments. Providing a sequence of arguments is generally preferred, as it allows the module to take care of any required escaping and quoting of arguments (e.g. to permit spaces in file names). If passing a single string, either shell must be True (see below) or else the string must simply name the program to be executed without specifying any arguments.

`stdin`, `stdout` and `stderr` specify the executed program’s standard input, standard output and standard error file handles, respectively. Valid values are PIPE, DEVNULL, an existing file descriptor (a positive integer), an existing file object, and None. PIPE indicates that a new pipe to the child should be created. DEVNULL indicates that the special file os.devnull will be used. With the default settings of None, no redirection will occur; the child’s file handles will be inherited from the parent. Additionally, stderr can be STDOUT, which indicates that the stderr data from the child process should be captured into the same file handle as for stdout.

If `encoding` or `errors` are specified, or `text` (also known as universal_newlines) is true, the file objects stdin, stdout and stderr will be opened in text mode using the encoding and errors specified in the call or the defaults for io.TextIOWrapper.

For stdin, line ending characters '\n' in the input will be converted to the default line separator os.linesep. For stdout and stderr, all line endings in the output will be converted to '\n'. For more information see the documentation of the `io.TextIOWrapper` class when the newline argument to its constructor is None.

If text mode is not used, stdin, stdout and stderr will be opened as binary streams. No encoding or line ending conversion is performed.

New in version 3.6: Added encoding and errors parameters.


New in version 3.7: Added the text parameter as an alias for universal_newlines.


Note:
 The newlines attribute of the file objects Popen.stdin, Popen.stdout and Popen.stderr are not updated by the Popen.communicate() method.
 

If `shell` is True, the specified command will be executed through the shell. This can be useful if you are using Python primarily for the enhanced control flow it offers over most system shells and still want convenient access to other shell features such as shell pipes, filename wildcards, environment variable expansion, and expansion of ~ to a user’s home directory. However, note that Python itself offers implementations of many shell-like features (in particular, glob, fnmatch, os.walk(), os.path.expandvars(), os.path.expanduser(), and shutil).

Changed in version 3.3: When universal_newlines is True, the class uses the encoding locale.getpreferredencoding(False) instead of locale.getpreferredencoding(). See the io.TextIOWrapper class for more information on this change.


Note:
 Read the Security Considerations section before using shell=True.
 

These options, along with all of the other options, are described in more detail in the Popen constructor documentation.

#### ◾Popen Constructor

The underlying process creation and management in this module is handled by the Popen class. It offers a lot of flexibility so that developers are able to handle the less common cases not covered by the convenience functions.

✅ `class subprocess.Popen(args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, universal_newlines=None, startupinfo=None, creationflags=0, restore_signals=True, start_new_session=False, pass_fds=(), *, group=None, extra_groups=None, user=None, umask=-1, encoding=None, errors=None, text=None, pipesize=-1)`
Execute a child program in a new process. On POSIX, the class uses os.execvpe()-like behavior to execute the child program. On Windows, the class uses the Windows CreateProcess() function. The arguments to Popen are as follows.

args should be a sequence of program arguments or else a single string or path-like object. By default, the program to execute is the first item in args if args is a sequence. If args is a string, the interpretation is platform-dependent and described below. See the shell and executable arguments for additional differences from the default behavior. Unless otherwise stated, it is recommended to pass args as a sequence.

Warning:
 For maximum reliability, use a fully-qualified path for the executable. To search for an unqualified name on PATH, use shutil.which(). On all platforms, passing sys.executable is the recommended way to launch the current Python interpreter again, and use the -m command-line format to launch an installed module.
 
Resolving the path of executable (or the first item of args) is platform dependent. For POSIX, see os.execvpe(), and note that when resolving or searching for the executable path, cwd overrides the current working directory and env can override the PATH environment variable. For Windows, see the documentation of the lpApplicationName and lpCommandLine parameters of WinAPI CreateProcess, and note that when resolving or searching for the executable path with shell=False, cwd does not override the current working directory and env cannot override the PATH environment variable. Using a full path avoids all of these variations.

An example of passing some arguments to an external program as a sequence is:


Popen(["/usr/bin/git", "commit", "-m", "Fixes a bug."])


On POSIX, if args is a string, the string is interpreted as the name or path of the program to execute. However, this can only be done if not passing arguments to the program.

Note:
 It may not be obvious how to break a shell command into a sequence of arguments, especially in complex cases. shlex.split() can illustrate how to determine the correct tokenization for args:
 

>>> import shlex, subprocess
>>> command_line = input()
/bin/vikings -input eggs.txt -output "spam spam.txt" -cmd "echo '$MONEY'"
>>> args = shlex.split(command_line)
>>> print(args)
['/bin/vikings', '-input', 'eggs.txt', '-output', 'spam spam.txt', '-cmd', "echo '$MONEY'"]
>>> p = subprocess.Popen(args) # Success!


Note in particular that options (such as -input) and arguments (such as eggs.txt) that are separated by whitespace in the shell go in separate list elements, while arguments that need quoting or backslash escaping when used in the shell (such as filenames containing spaces or the echo command shown above) are single list elements.

On Windows, if args is a sequence, it will be converted to a string in a manner described in Converting an argument sequence to a string on Windows. This is because the underlying `CreateProcess()` operates on strings.

Changed in version 3.6: args parameter accepts a path-like object if shell is False and a sequence containing path-like objects on POSIX.


Changed in version 3.8: args parameter accepts a path-like object if shell is False and a sequence containing bytes and path-like objects on Windows.


The shell argument (which defaults to False) specifies whether to use the shell as the program to execute. If shell is True, it is recommended to pass args as a string rather than as a sequence.

On POSIX with shell=True, the shell defaults to /bin/sh. If args is a string, the string specifies the command to execute through the shell. This means that the string must be formatted exactly as it would be when typed at the shell prompt. This includes, for example, quoting or backslash escaping filenames with spaces in them. If args is a sequence, the first item specifies the command string, and any additional items will be treated as additional arguments to the shell itself. That is to say, Popen does the equivalent of:


Popen(['/bin/sh', '-c', args[0], args[1], ...])


On Windows with shell=True, the COMSPEC environment variable specifies the default shell. The only time you need to specify shell=True on Windows is when the command you wish to execute is built into the shell (e.g. dir or copy). You do not need shell=True to run a batch file or console-based executable.

Note:
 Read the Security Considerations section before using shell=True.
 

bufsize will be supplied as the corresponding argument to the open() function when creating the stdin/stdout/stderr pipe file objects:

• 0 means unbuffered (read and write are one system call and can return short)
• 1 means line buffered (only usable if universal_newlines=True i.e., in a text mode)
• any other positive value means use a buffer of approximately that size
• negative bufsize (the default) means the system default of io.DEFAULT_BUFFER_SIZE will be used.

Changed in version 3.3.1: bufsize now defaults to -1 to enable buffering by default to match the behavior that most code expects. In versions prior to Python 3.2.4 and 3.3.1 it incorrectly defaulted to 0 which was unbuffered and allowed short reads. This was unintentional and did not match the behavior of Python 2 as most code expected.


The executable argument specifies a replacement program to execute. It is very seldom needed. When shell=False, executable replaces the program to execute specified by args. However, the original args is still passed to the program. Most programs treat the program specified by args as the command name, which can then be different from the program actually executed. On POSIX, the args name becomes the display name for the executable in utilities such as ps. If shell=True, on POSIX the executable argument specifies a replacement shell for the default /bin/sh.

Changed in version 3.6: executable parameter accepts a path-like object on POSIX.


Changed in version 3.8: executable parameter accepts a bytes and path-like object on Windows.


stdin, stdout and stderr specify the executed program’s standard input, standard output and standard error file handles, respectively. Valid values are PIPE, DEVNULL, an existing file descriptor (a positive integer), an existing file object, and None. PIPE indicates that a new pipe to the child should be created. DEVNULL indicates that the special file os.devnull will be used. With the default settings of None, no redirection will occur; the child’s file handles will be inherited from the parent. Additionally, stderr can be STDOUT, which indicates that the stderr data from the applications should be captured into the same file handle as for stdout.

If preexec_fn is set to a callable object, this object will be called in the child process just before the child is executed. (POSIX only)

Warning:
 The preexec_fn parameter is not safe to use in the presence of threads in your application. The child process could deadlock before exec is called. If you must use it, keep it trivial! Minimize the number of libraries you call into.
 

Note:
 If you need to modify the environment for the child use the env parameter rather than doing it in a preexec_fn. The start_new_session parameter can take the place of a previously common use of preexec_fn to call os.setsid() in the child.
 

Changed in version 3.8: The preexec_fn parameter is no longer supported in subinterpreters. The use of the parameter in a subinterpreter raises RuntimeError. The new restriction may affect applications that are deployed in mod_wsgi, uWSGI, and other embedded environments.


If close_fds is true, all file descriptors except 0, 1 and 2 will be closed before the child process is executed. Otherwise when close_fds is false, file descriptors obey their inheritable flag as described in Inheritance of File Descriptors.

On Windows, if close_fds is true then no handles will be inherited by the child process unless explicitly passed in the handle_list element of STARTUPINFO.lpAttributeList, or by standard handle redirection.

Changed in version 3.2: The default for close_fds was changed from False to what is described above.


Changed in version 3.7: On Windows the default for close_fds was changed from False to True when redirecting the standard handles. It’s now possible to set close_fds to True when redirecting the standard handles.


pass_fds is an optional sequence of file descriptors to keep open between the parent and child. Providing any pass_fds forces close_fds to be True. (POSIX only)

Changed in version 3.2: The pass_fds parameter was added.


If cwd is not None, the function changes the working directory to cwd before executing the child. cwd can be a string, bytes or path-like object. On POSIX, the function looks for executable (or for the first item in args) relative to cwd if the executable path is a relative path.

Changed in version 3.6: cwd parameter accepts a path-like object on POSIX.


Changed in version 3.7: cwd parameter accepts a path-like object on Windows.


Changed in version 3.8: cwd parameter accepts a bytes object on Windows.


If restore_signals is true (the default) all signals that Python has set to SIG_IGN are restored to SIG_DFL in the child process before the exec. Currently this includes the SIGPIPE, SIGXFZ and SIGXFSZ signals. (POSIX only)

Changed in version 3.2: restore_signals was added.


If start_new_session is true the setsid() system call will be made in the child process prior to the execution of the subprocess. (POSIX only)

Changed in version 3.2: start_new_session was added.


If group is not None, the setregid() system call will be made in the child process prior to the execution of the subprocess. If the provided value is a string, it will be looked up via grp.getgrnam() and the value in gr_gid will be used. If the value is an integer, it will be passed verbatim. (POSIX only)

Availability: POSIX

New in version 3.9.


If extra_groups is not None, the setgroups() system call will be made in the child process prior to the execution of the subprocess. Strings provided in extra_groups will be looked up via grp.getgrnam() and the values in gr_gid will be used. Integer values will be passed verbatim. (POSIX only)

Availability: POSIX

New in version 3.9.


If user is not None, the setreuid() system call will be made in the child process prior to the execution of the subprocess. If the provided value is a string, it will be looked up via pwd.getpwnam() and the value in pw_uid will be used. If the value is an integer, it will be passed verbatim. (POSIX only)

Availability: POSIX

New in version 3.9.


If umask is not negative, the umask() system call will be made in the child process prior to the execution of the subprocess.

Availability: POSIX

New in version 3.9.


If env is not None, it must be a mapping that defines the environment variables for the new process; these are used instead of the default behavior of inheriting the current process’ environment.

Note:
 If specified, env must provide any variables required for the program to execute. On Windows, in order to run a side-by-side assembly the specified env must include a valid SystemRoot.
 

If encoding or errors are specified, or text is true, the file objects stdin, stdout and stderr are opened in text mode with the specified encoding and errors, as described above in Frequently Used Arguments. The universal_newlines argument is equivalent to text and is provided for backwards compatibility. By default, file objects are opened in binary mode.

New in version 3.6: encoding and errors were added.


New in version 3.7: text was added as a more readable alias for universal_newlines.


If given, startupinfo will be a STARTUPINFO object, which is passed to the underlying CreateProcess function. creationflags, if given, can be one or more of the following flags:


• CREATE_NEW_CONSOLE
• CREATE_NEW_PROCESS_GROUP
• ABOVE_NORMAL_PRIORITY_CLASS
• BELOW_NORMAL_PRIORITY_CLASS
• HIGH_PRIORITY_CLASS
• IDLE_PRIORITY_CLASS
• NORMAL_PRIORITY_CLASS
• REALTIME_PRIORITY_CLASS
• CREATE_NO_WINDOW
• DETACHED_PROCESS
• CREATE_DEFAULT_ERROR_MODE
• CREATE_BREAKAWAY_FROM_JOB

pipesize can be used to change the size of the pipe when PIPE is used for stdin, stdout or stderr. The size of the pipe is only changed on platforms that support this (only Linux at this time of writing). Other platforms will ignore this parameter.

New in version 3.10: The pipesize parameter was added.


Popen objects are supported as context managers via the with statement: on exit, standard file descriptors are closed, and the process is waited for.


with Popen(["ifconfig"], stdout=PIPE) as proc:
    log.write(proc.stdout.read())


Popen and the other functions in this module that use it raise an auditing event subprocess.Popen with arguments executable, args, cwd, and env. The value for args may be a single string or a list of strings, depending on platform.

Changed in version 3.2: Added context manager support.


Changed in version 3.6: Popen destructor now emits a ResourceWarning warning if the child process is still running.


Changed in version 3.8: Popen can use os.posix_spawn() in some cases for better performance. On Windows Subsystem for Linux and QEMU User Emulation, Popen constructor using os.posix_spawn() no longer raise an exception on errors like missing program, but the child process fails with a non-zero returncode.


#### ◾Exceptions

Exceptions raised in the child process, before the new program has started to execute, will be re-raised in the parent.

The most common exception raised is OSError. This occurs, for example, when trying to execute a non-existent file. Applications should prepare for OSError exceptions. Note that, when shell=True, OSError will be raised by the child only if the selected shell itself was not found. To determine if the shell failed to find the requested application, it is necessary to check the return code or output from the subprocess.

A ValueError will be raised if Popen is called with invalid arguments.

check_call() and check_output() will raise CalledProcessError if the called process returns a non-zero return code.

All of the functions and methods that accept a timeout parameter, such as call() and Popen.communicate() will raise TimeoutExpired if the timeout expires before the process exits.

Exceptions defined in this module all inherit from SubprocessError.



New in version 3.3: The SubprocessError base class was added.




### ===🗝 ◦ Security Considerations

Unlike some other popen functions, this implementation will never implicitly call a system shell. This means that all characters, including shell metacharacters, can safely be passed to child processes. If the shell is invoked explicitly, via shell=True, it is the application’s responsibility to ensure that all whitespace and metacharacters are quoted appropriately to avoid shell injection vulnerabilities. On some platforms, it is possible to use shlex.quote() for this escaping.


### ===🗝 ◦ Popen Objects

➡ `Popen.poll()`
➡ `Popen.wait(timeout=None)`
➡ `Popen.communicate(input=None, timeout=None)`
➡ `Popen.send_signal(signal)`
➡ `Popen.terminate()`
➡ `Popen.kill()`
➡ `Popen.args`
➡ `Popen.stdin`
➡ `Popen.stdout`
➡ `Popen.stderr`
➡ `Popen.pid`
➡ `Popen.returncode`

Instances of the Popen class have the following methods:

➡ `Popen.poll()`
Check if child process has terminated. Set and return returncode attribute. Otherwise, returns None.

➡ `Popen.wait(timeout=None)`
Wait for child process to terminate. Set and return returncode attribute.

If the process does not terminate after timeout seconds, raise a TimeoutExpired exception. It is safe to catch this exception and retry the wait.

Note:
 This will deadlock when using stdout=PIPE or stderr=PIPE and the child process generates enough output to a pipe such that it blocks waiting for the OS pipe buffer to accept more data. Use Popen.communicate() when using pipes to avoid that.
 

Note:
 The function is implemented using a busy loop (non-blocking call and short sleeps). Use the asyncio module for an asynchronous wait: see asyncio.create_subprocess_exec.
 

Changed in version 3.3: timeout was added.


➡ `Popen.communicate(input=None, timeout=None)`
Interact with process: Send data to stdin. Read data from stdout and stderr, until end-of-file is reached. Wait for process to terminate and set the returncode attribute. The optional `input` argument should be data to be sent to the child process, or None, if no data should be sent to the child. If streams were opened in text mode, input must be a string. Otherwise, it must be bytes.

communicate() returns a tuple (stdout_data, stderr_data). The data will be strings if streams were opened in text mode; otherwise, bytes.

Note that if you want to send data to the process’s stdin, you need to create the Popen object with stdin=PIPE. Similarly, to get anything other than None in the result tuple, you need to give stdout=PIPE and/or stderr=PIPE too.

If the process does not terminate after `timeout` seconds, a TimeoutExpired exception will be raised. Catching this exception and retrying communication will not lose any output.

The child process is not killed if the `timeout` expires, so in order to cleanup properly a well-behaved application should kill the child process and finish communication:


```py
proc = subprocess.Popen(...)
try:
    outs, errs = proc.communicate(timeout=15)
except TimeoutExpired:
    proc.kill()
    outs, errs = proc.communicate()
```


Note:
 The data read is buffered in memory, so do not use this method if the data size is large or unlimited.
 

Changed in version 3.3: timeout was added.


➡ `Popen.send_signal(signal)`
Sends the signal signal to the child.

Do nothing if the process completed.

Note:
 On Windows, SIGTERM is an alias for terminate(). CTRL_C_EVENT and CTRL_BREAK_EVENT can be sent to processes started with a creationflags parameter which includes CREATE_NEW_PROCESS_GROUP.
 


➡ `Popen.terminate()`
Stop the child. On POSIX OSs the method sends SIGTERM to the child. On Windows the Win32 API function TerminateProcess() is called to stop the child.

➡ `Popen.kill()`
Kills the child. On POSIX OSs the function sends SIGKILL to the child. On Windows kill() is an alias for terminate().

The following attributes are also available:

➡ `Popen.args`
The args argument as it was passed to Popen – a sequence of program arguments or else a single string.

New in version 3.3.


➡ `Popen.stdin`
If the stdin argument was PIPE, this attribute is a writeable stream object as returned by open(). If the encoding or errors arguments were specified or the universal_newlines argument was True, the stream is a text stream, otherwise it is a byte stream. If the stdin argument was not PIPE, this attribute is None.

➡ `Popen.stdout`
If the stdout argument was PIPE, this attribute is a readable stream object as returned by open(). Reading from the stream provides output from the child process. If the encoding or errors arguments were specified or the universal_newlines argument was True, the stream is a text stream, otherwise it is a byte stream. If the stdout argument was not PIPE, this attribute is None.

➡ `Popen.stderr`
If the stderr argument was PIPE, this attribute is a readable stream object as returned by open(). Reading from the stream provides error output from the child process. If the encoding or errors arguments were specified or the universal_newlines argument was True, the stream is a text stream, otherwise it is a byte stream. If the stderr argument was not PIPE, this attribute is None.

Warning:
 Use communicate() rather than .stdin.write, .stdout.read or .stderr.read to avoid deadlocks due to any of the other OS pipe buffers filling up and blocking the child process.
 


➡ `Popen.pid`
The process ID of the child process.

Note that if you set the shell argument to True, this is the process ID of the spawned shell.

➡ `Popen.returncode`
The child return code, set by poll() and wait() (and indirectly by communicate()). A None value indicates that the process hasn’t terminated yet.

A negative value -N indicates that the child was terminated by signal N (POSIX only).


### ===🗝 ◦ Windows Popen Helpers

The STARTUPINFO class and following constants are only available on Windows.

✅ `class subprocess.STARTUPINFO(*, dwFlags=0, hStdInput=None, hStdOutput=None, hStdError=None, wShowWindow=0, lpAttributeList=None)`
Partial support of the Windows STARTUPINFO structure is used for Popen creation. The following attributes can be set by passing them as keyword-only arguments.

Changed in version 3.7: Keyword-only argument support was added.


➡ ↪ `dwFlags`
A bit field that determines whether certain STARTUPINFO attributes are used when the process creates a window.


si = subprocess.STARTUPINFO()
si.dwFlags = subprocess.STARTF_USESTDHANDLES | subprocess.STARTF_USESHOWWINDOW


➡ ↪ `hStdInput`
If dwFlags specifies STARTF_USESTDHANDLES, this attribute is the standard input handle for the process. If STARTF_USESTDHANDLES is not specified, the default for standard input is the keyboard buffer.

➡ ↪ `hStdOutput`
If dwFlags specifies STARTF_USESTDHANDLES, this attribute is the standard output handle for the process. Otherwise, this attribute is ignored and the default for standard output is the console window’s buffer.

➡ ↪ `hStdError`
If dwFlags specifies STARTF_USESTDHANDLES, this attribute is the standard error handle for the process. Otherwise, this attribute is ignored and the default for standard error is the console window’s buffer.

➡ ↪ `wShowWindow`
If dwFlags specifies STARTF_USESHOWWINDOW, this attribute can be any of the values that can be specified in the nCmdShow parameter for the ShowWindow function, except for SW_SHOWDEFAULT. Otherwise, this attribute is ignored.

SW_HIDE is provided for this attribute. It is used when Popen is called with shell=True.

➡ ↪ `lpAttributeList`
A dictionary of additional attributes for process creation as given in STARTUPINFOEX, see UpdateProcThreadAttribute.

Supported attributes:

➡ ↪ `handle_list`
Sequence of handles that will be inherited. close_fds must be true if non-empty.

The handles must be temporarily made inheritable by os.set_handle_inheritable() when passed to the Popen constructor, else OSError will be raised with Windows error ERROR_INVALID_PARAMETER (87).

Warning:
 In a multithreaded process, use caution to avoid leaking handles that are marked inheritable when combining this feature with concurrent calls to other process creation functions that inherit all handles such as os.system(). This also applies to standard handle redirection, which temporarily creates inheritable handles.
 

New in version 3.7.


#### ◾Windows Constants

The subprocess module exposes the following constants.

➡ `subprocess.STD_INPUT_HANDLE`
The standard input device. Initially, this is the console input buffer, CONIN$.

➡ `subprocess.STD_OUTPUT_HANDLE`
The standard output device. Initially, this is the active console screen buffer, CONOUT$.

➡ `subprocess.STD_ERROR_HANDLE`
The standard error device. Initially, this is the active console screen buffer, CONOUT$.

➡ `subprocess.SW_HIDE`
Hides the window. Another window will be activated.

➡ `subprocess.STARTF_USESTDHANDLES`
Specifies that the STARTUPINFO.hStdInput, STARTUPINFO.hStdOutput, and STARTUPINFO.hStdError attributes contain additional information.

➡ `subprocess.STARTF_USESHOWWINDOW`
Specifies that the STARTUPINFO.wShowWindow attribute contains additional information.

➡ `subprocess.CREATE_NEW_CONSOLE`
The new process has a new console, instead of inheriting its parent’s console (the default).

➡ `subprocess.CREATE_NEW_PROCESS_GROUP`
A Popen creationflags parameter to specify that a new process group will be created. This flag is necessary for using os.kill() on the subprocess.

This flag is ignored if CREATE_NEW_CONSOLE is specified.

➡ `subprocess.ABOVE_NORMAL_PRIORITY_CLASS`
A Popen creationflags parameter to specify that a new process will have an above average priority.

New in version 3.7.


➡ `subprocess.BELOW_NORMAL_PRIORITY_CLASS`
A Popen creationflags parameter to specify that a new process will have a below average priority.

New in version 3.7.


➡ `subprocess.HIGH_PRIORITY_CLASS`
A Popen creationflags parameter to specify that a new process will have a high priority.

New in version 3.7.


➡ `subprocess.IDLE_PRIORITY_CLASS`
A Popen creationflags parameter to specify that a new process will have an idle (lowest) priority.

New in version 3.7.


➡ `subprocess.NORMAL_PRIORITY_CLASS`
A Popen creationflags parameter to specify that a new process will have an normal priority. (default)

New in version 3.7.


➡ `subprocess.REALTIME_PRIORITY_CLASS`
A Popen creationflags parameter to specify that a new process will have realtime priority. You should almost never use REALTIME_PRIORITY_CLASS, because this interrupts system threads that manage mouse input, keyboard input, and background disk flushing. This class can be appropriate for applications that “talk” directly to hardware or that perform brief tasks that should have limited interruptions.

New in version 3.7.


➡ `subprocess.CREATE_NO_WINDOW`
A Popen creationflags parameter to specify that a new process will not create a window.

New in version 3.7.


➡ `subprocess.DETACHED_PROCESS`
A Popen creationflags parameter to specify that a new process will not inherit its parent’s console. This value cannot be used with CREATE_NEW_CONSOLE.

New in version 3.7.


➡ `subprocess.CREATE_DEFAULT_ERROR_MODE`
A Popen creationflags parameter to specify that a new process does not inherit the error mode of the calling process. Instead, the new process gets the default error mode. This feature is particularly useful for multithreaded shell applications that run with hard errors disabled.

New in version 3.7.


➡ `subprocess.CREATE_BREAKAWAY_FROM_JOB`
A Popen creationflags parameter to specify that a new process is not associated with the job.

New in version 3.7.


### ===🗝 ◦ Older high-level API

Prior to Python 3.5, these three functions comprised the high level API to subprocess. You can now use run() in many cases, but lots of existing code calls these functions.

➡ `subprocess.call(args, *, stdin=None, stdout=None, stderr=None, shell=False, cwd=None, timeout=None, **other_popen_kwargs)`
Run the command described by args. Wait for command to complete, then return the returncode attribute.

Code needing to capture stdout or stderr should use run() instead:


    run(...).returncode


To suppress stdout or stderr, supply a value of DEVNULL.

The arguments shown above are merely some common ones. The full function signature is the same as that of the Popen constructor - this function passes all supplied arguments other than timeout directly through to that interface.

Note:
 Do not use stdout=PIPE or stderr=PIPE with this function. The child process will block if it generates enough output to a pipe to fill up the OS pipe buffer as the pipes are not being read from.
 

Changed in version 3.3: timeout was added.


➡ `subprocess.check_call(args, *, stdin=None, stdout=None, stderr=None, shell=False, cwd=None, timeout=None, **other_popen_kwargs)`
Run command with arguments. Wait for command to complete. If the return code was zero then return, otherwise raise CalledProcessError. The CalledProcessError object will have the return code in the returncode attribute. If check_call() was unable to start the process it will propagate the exception that was raised.

Code needing to capture stdout or stderr should use run() instead:


    run(..., check=True)


To suppress stdout or stderr, supply a value of DEVNULL.

The arguments shown above are merely some common ones. The full function signature is the same as that of the Popen constructor - this function passes all supplied arguments other than timeout directly through to that interface.

Note:
 Do not use stdout=PIPE or stderr=PIPE with this function. The child process will block if it generates enough output to a pipe to fill up the OS pipe buffer as the pipes are not being read from.
 

Changed in version 3.3: timeout was added.


➡ `subprocess.check_output(args, *, stdin=None, stderr=None, shell=False, cwd=None, encoding=None, errors=None, universal_newlines=None, timeout=None, text=None, **other_popen_kwargs)`
Run command with arguments and return its output.

If the return code was non-zero it raises a CalledProcessError. The CalledProcessError object will have the return code in the returncode attribute and any output in the output attribute.

This is equivalent to:


    run(..., check=True, stdout=PIPE).stdout


The arguments shown above are merely some common ones. The full function signature is largely the same as that of run() - most arguments are passed directly through to that interface. One API deviation from run() behavior exists: passing input=None will behave the same as input=b'' (or input='', depending on other arguments) rather than using the parent’s standard input file handle.

By default, this function will return the data as encoded bytes. The actual encoding of the output data may depend on the command being invoked, so the decoding to text will often need to be handled at the application level.

This behaviour may be overridden by setting text, encoding, errors, or universal_newlines to True as described in Frequently Used Arguments and run().

To also capture standard error in the result, use stderr=subprocess.STDOUT:


>>> subprocess.check_output(
...     "ls non_existent_file; exit 0",
...     stderr=subprocess.STDOUT,
...     shell=True)
'ls: non_existent_file: No such file or directory\n'


New in version 3.1.


Changed in version 3.3: timeout was added.


Changed in version 3.4: Support for the input keyword argument was added.


Changed in version 3.6: encoding and errors were added. See run() for details.


New in version 3.7: text was added as a more readable alias for universal_newlines.


### ===🗝 ◦ Replacing Older Functions with the subprocess Module

➡ ◾Replacing /bin/sh shell command substitution
➡ ◾Replacing shell pipeline
➡ ◾Replacing os.system()
➡ ◾Replacing the os.spawn family
➡ ◾Replacing os.popen(), os.popen2(), os.popen3()
➡ ◾Replacing functions from the popen2 module


In this section, “a becomes b” means that b can be used as a replacement for a.

Note:
 All “a” functions in this section fail (more or less) silently if the executed program cannot be found; the “b” replacements raise OSError instead.
 
In addition, the replacements using check_output() will fail with a CalledProcessError if the requested operation produces a non-zero return code. The output is still available as the output attribute of the raised exception.

In the following examples, we assume that the relevant functions have already been imported from the subprocess module.


➡ Replacing /bin/sh shell command substitution


    output=$(mycmd myarg)


becomes:


    output = check_output(["mycmd", "myarg"])



➡ Replacing shell pipeline


    output=$(dmesg | grep hda)


becomes:


```py
p1 = Popen(["dmesg"], stdout=PIPE)
p2 = Popen(["grep", "hda"], stdin=p1.stdout, stdout=PIPE)
p1.stdout.close()  # Allow p1 to receive a SIGPIPE if p2 exits.
output = p2.communicate()[0]

```

The p1.stdout.close() call after starting the p2 is important in order for p1 to receive a SIGPIPE if p2 exits before p1.

Alternatively, for trusted input, the shell’s own pipeline support may still be used directly:


    output=$(dmesg | grep hda)


becomes:


    output = check_output("dmesg | grep hda", shell=True)



➡ Replacing os.system()


```py
sts = os.system("mycmd" + " myarg")
# becomes
retcode = call("mycmd" + " myarg", shell=True)
```


Notes:

• Calling the program through the shell is usually not required.
• The call() return value is encoded differently to that of os.system().
• The os.system() function ignores SIGINT and SIGQUIT signals while the command is running, but the caller must do this separately when using the subprocess module.

A more realistic example would look like this:


```py
try:
    retcode = call("mycmd" + " myarg", shell=True)
    if retcode < 0:
        print("Child was terminated by signal", -retcode, file=sys.stderr)
    else:
        print("Child returned", retcode, file=sys.stderr)
except OSError as e:
    print("Execution failed:", e, file=sys.stderr)
```



➡ Replacing the os.spawn family

P_NOWAIT example:


```py
pid = os.spawnlp(os.P_NOWAIT, "/bin/mycmd", "mycmd", "myarg")
==>
pid = Popen(["/bin/mycmd", "myarg"]).pid
```


P_WAIT example:


```py
retcode = os.spawnlp(os.P_WAIT, "/bin/mycmd", "mycmd", "myarg")
==>
retcode = call(["/bin/mycmd", "myarg"])
```


Vector example:


```py
os.spawnvp(os.P_NOWAIT, path, args)
==>
Popen([path] + args[1:])
```


Environment example:


```py
os.spawnlpe(os.P_NOWAIT, "/bin/mycmd", "mycmd", "myarg", env)
==>
Popen(["/bin/mycmd", "myarg"], env={"PATH": "/usr/bin"})
```



➡ Replacing os.popen(), os.popen2(), os.popen3()


```py
(child_stdin, child_stdout) = os.popen2(cmd, mode, bufsize)
==>
p = Popen(cmd, shell=True, bufsize=bufsize,
          stdin=PIPE, stdout=PIPE, close_fds=True)
(child_stdin, child_stdout) = (p.stdin, p.stdout)



(child_stdin,
 child_stdout,
 child_stderr) = os.popen3(cmd, mode, bufsize)
==>
p = Popen(cmd, shell=True, bufsize=bufsize,
          stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)
(child_stdin,
 child_stdout,
 child_stderr) = (p.stdin, p.stdout, p.stderr)



(child_stdin, child_stdout_and_stderr) = os.popen4(cmd, mode, bufsize)
==>
p = Popen(cmd, shell=True, bufsize=bufsize,
          stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)
(child_stdin, child_stdout_and_stderr) = (p.stdin, p.stdout)
```


Return code handling translates as follows:


```py
pipe = os.popen(cmd, 'w')
...
rc = pipe.close()
if rc is not None and rc >> 8:
    print("There were some errors")
==>
process = Popen(cmd, stdin=PIPE)
...
process.stdin.close()
if process.wait() != 0:
    print("There were some errors")
```



➡ Replacing functions from the popen2 module

Note:
 If the cmd argument to popen2 functions is a string, the command is executed through /bin/sh. If it is a list, the command is directly executed.
 


```py
(child_stdout, child_stdin) = popen2.popen2("somestring", bufsize, mode)
==>
p = Popen("somestring", shell=True, bufsize=bufsize,
          stdin=PIPE, stdout=PIPE, close_fds=True)
(child_stdout, child_stdin) = (p.stdout, p.stdin)



(child_stdout, child_stdin) = popen2.popen2(["mycmd", "myarg"], bufsize, mode)
==>
p = Popen(["mycmd", "myarg"], bufsize=bufsize,
          stdin=PIPE, stdout=PIPE, close_fds=True)
(child_stdout, child_stdin) = (p.stdout, p.stdin)
```


popen2.Popen3 and popen2.Popen4 basically work as subprocess.Popen, except that:

• Popen raises an exception if the execution fails.
• The capturestderr argument is replaced with the stderr argument.
• stdin=PIPE and stdout=PIPE must be specified.
• popen2 closes all file descriptors by default, but you have to specify close_fds=True with Popen to guarantee this behavior on all platforms or past Python versions.


### ===🗝 ◦ Legacy Shell Invocation Functions

This module also provides the following legacy functions from the 2.x commands module. These operations implicitly invoke the system shell and none of the guarantees described above regarding security and exception handling consistency are valid for these functions.

➡ `subprocess.getstatusoutput(cmd)`
Return (exitcode, output) of executing cmd in a shell.

Execute the string cmd in a shell with Popen.check_output() and return a 2-tuple (exitcode, output). The locale encoding is used; see the notes on Frequently Used Arguments for more details.

A trailing newline is stripped from the output. The exit code for the command can be interpreted as the return code of subprocess. Example:


>>> subprocess.getstatusoutput('ls /bin/ls')
(0, '/bin/ls')
>>> subprocess.getstatusoutput('cat /bin/junk')
(1, 'cat: /bin/junk: No such file or directory')
>>> subprocess.getstatusoutput('/bin/junk')
(127, 'sh: /bin/junk: not found')
>>> subprocess.getstatusoutput('/bin/kill $$')
(-15, '')


Availability: POSIX & Windows.

Changed in version 3.3.4: Windows support was added.


The function now returns (exitcode, output) instead of (status, output) as it did in Python 3.3.3 and earlier. exitcode has the same value as returncode.

➡ `subprocess.getoutput(cmd)`
Return output (stdout and stderr) of executing cmd in a shell.

Like getstatusoutput(), except the exit code is ignored and the return value is a string containing the command’s output. Example:


>>> subprocess.getoutput('ls /bin/ls')
'/bin/ls'


Availability: POSIX & Windows.

Changed in version 3.3.4: Windows support added


#### ◾Converting an argument sequence to a string on Windows

On Windows, an args sequence is converted to a string that can be parsed using the following rules (which correspond to the rules used by the MS C runtime):

1. Arguments are delimited by white space, which is either a space or a tab.

2. A string surrounded by double quotation marks is interpreted as a single argument, regardless of white space contained within. A quoted string can be embedded in an argument.

3. A double quotation mark preceded by a backslash is interpreted as a literal double quotation mark.

4. Backslashes are interpreted literally, unless they immediately precede a double quotation mark.

5. If backslashes immediately precede a double quotation mark, every pair of backslashes is interpreted as a literal backslash. If the number of backslashes is odd, the last backslash escapes the next double quotation mark as described in rule 3.

See also:
 shlexModule which provides function to parse and escape command lines.



## ==⚡ • sched — Event scheduler

Source code: Lib/sched.py


The sched module defines a class which implements a general purpose event scheduler:

✅ `class sched.scheduler(timefunc=time.monotonic, delayfunc=time.sleep)`

The scheduler class defines a generic interface to scheduling events. It needs two functions to actually deal with the “outside world” — timefunc should be callable without arguments, and return a number (the “time”, in any units whatsoever). The delayfunc function should be callable with one argument, compatible with the output of timefunc, and should delay that many time units. delayfunc will also be called with the argument 0 after each event is run to allow other threads an opportunity to run in multi-threaded applications.


Changed in version 3.3: timefunc and delayfunc parameters are optional.


Changed in version 3.3: scheduler class can be safely used in multi-threaded environments.

Example:


>>> import sched, time
>>> s = sched.scheduler(time.time, time.sleep)
>>> def print_time(a='default'):
...     print("From print_time", time.time(), a)
...
>>> def print_some_times():
...     print(time.time())
...     s.enter(10, 1, print_time)
...     s.enter(5, 2, print_time, argument=('positional',))
...     s.enter(5, 1, print_time, kwargs={'a': 'keyword'})
...     s.run()
...     print(time.time())
...
>>> print_some_times()
930343690.257
From print_time 930343695.274 positional
From print_time 930343695.275 keyword
From print_time 930343700.273 default
930343700.276



### ===🗝 ◦ Scheduler Objects

scheduler instances have the following methods and attributes:
scheduler.enterabs(time, priority, action, argument=(), kwargs={})
Schedule a new event. The time argument should be a numeric type compatible with the return value of the timefunc function passed to the constructor. Events scheduled for the same time will be executed in the order of their priority. A lower number represents a higher priority.

Executing the event means executing `action(*argument, **kwargs)`. argument is a sequence holding the positional arguments for action. kwargs is a dictionary holding the keyword arguments for action.

Return value is an event which may be used for later cancellation of the event (see cancel()).


Changed in version 3.3: argument parameter is optional.


Changed in version 3.3: kwargs parameter was added.

➡ `scheduler.enter(delay, priority, action, argument=(), kwargs={})`
Schedule an event for delay more time units. Other than the relative time, the other arguments, the effect and the return value are the same as those for enterabs().


Changed in version 3.3: argument parameter is optional.


Changed in version 3.3: kwargs parameter was added.

➡ `scheduler.cancel(event)`
Remove the event from the queue. If event is not an event currently in the queue, this method will raise a ValueError.

➡ `scheduler.empty()`
Return True if the event queue is empty.

➡ `scheduler.run(blocking=True)`
Run all scheduled events. This method will wait (using the delayfunc() function passed to the constructor) for the next event, then execute it and so on until there are no more scheduled events.

If blocking is false executes the scheduled events due to expire soonest (if any) and then return the deadline of the next scheduled call in the scheduler (if any).

Either action or delayfunc can raise an exception. In either case, the scheduler will maintain a consistent state and propagate the exception. If an exception is raised by action, the event will not be attempted in future calls to run().

If a sequence of events takes longer to run than the time available before the next event, the scheduler will simply fall behind. No events will be dropped; the calling code is responsible for canceling events which are no longer pertinent.


Changed in version 3.3: blocking parameter was added.

➡ `scheduler.queue`
Read-only attribute returning a list of upcoming events in the order they will be run. Each event is shown as a named tuple with the following fields: time, priority, action, argument, kwargs.



## ==⚡ • queue — A synchronized queue class

队列已实现加锁同步功能。

Source code: Lib/queue.py

✅ `class queue.Queue(maxsize=0)`
✅ `class queue.LifoQueue(maxsize=0)`
✅ `class queue.PriorityQueue(maxsize=0)`
✅ `class queue.SimpleQueue`
✅ `exception queue.Empty`
✅ `exception queue.Full`

The queue module implements multi-producer, multi-consumer queues. It is especially useful in threaded programming when information must be exchanged safely between multiple threads. The Queue class in this module implements all the required locking semantics.

The module implements three types of queue, which differ only in the order in which the entries are retrieved. In a FIFO queue, the first tasks added are the first retrieved. In a LIFO queue, the most recently added entry is the first retrieved (operating like a stack). With a priority queue, the entries are kept sorted (using the heapq module) and the lowest valued entry is retrieved first.

Internally, those three types of queues use locks to temporarily block competing threads; however, they are not designed to handle reentrancy within a thread.

In addition, the module implements a “simple” FIFO queue type, SimpleQueue, whose specific implementation provides additional guarantees in exchange for the smaller functionality.

The queue module defines the following classes and exceptions:

✅ `class queue.Queue(maxsize=0)`
Constructor for a FIFO queue. maxsize is an integer that sets the upperbound limit on the number of items that can be placed in the queue. Insertion will block once this size has been reached, until queue items are consumed. If maxsize is less than or equal to zero, the queue size is infinite.

✅ `class queue.LifoQueue(maxsize=0)`
Constructor for a LIFO queue. maxsize is an integer that sets the upperbound limit on the number of items that can be placed in the queue. Insertion will block once this size has been reached, until queue items are consumed. If maxsize is less than or equal to zero, the queue size is infinite.

✅ `class queue.PriorityQueue(maxsize=0)`
Constructor for a priority queue. maxsize is an integer that sets the upperbound limit on the number of items that can be placed in the queue. Insertion will block once this size has been reached, until queue items are consumed. If maxsize is less than or equal to zero, the queue size is infinite.

The lowest valued entries are retrieved first (the lowest valued entry is the one returned by sorted(list(entries))[0]). A typical pattern for entries is a tuple in the form: (priority_number, data).

If the data elements are not comparable, the data can be wrapped in a class that ignores the data item and only compares the priority number:


```py
from dataclasses import dataclass, field
from typing import Any

@dataclass(order=True)
class PrioritizedItem:
    priority: int
    item: Any=field(compare=False)
```


✅ `class queue.SimpleQueue`
Constructor for an unbounded FIFO queue. Simple queues lack advanced functionality such as task tracking.


New in version 3.7.

✅ `exception queue.Empty`
Exception raised when non-blocking get() (or get_nowait()) is called on a Queue object which is empty.

✅ `exception queue.Full`
Exception raised when non-blocking put() (or put_nowait()) is called on a Queue object which is full.


### ===🗝 ◦ Queue Objects

➡ `Queue.qsize()`
➡ `Queue.empty()`
➡ `Queue.full()`
➡ `Queue.put(item, block=True, timeout=None)`
➡ `Queue.put_nowait(item)`
➡ `Queue.get(block=True, timeout=None)`
➡ `Queue.get_nowait()`
➡ `Queue.task_done()`
➡ `Queue.join()`

Queue objects (Queue, LifoQueue, or PriorityQueue) provide the public methods described below.

➡ `Queue.qsize()`
Return the approximate size of the queue. Note, qsize() > 0 doesn’t guarantee that a subsequent get() will not block, nor will qsize() < maxsize guarantee that put() will not block.

➡ `Queue.empty()`
Return True if the queue is empty, False otherwise. If empty() returns True it doesn’t guarantee that a subsequent call to put() will not block. Similarly, if empty() returns False it doesn’t guarantee that a subsequent call to get() will not block.

➡ `Queue.full()`
Return True if the queue is full, False otherwise. If full() returns True it doesn’t guarantee that a subsequent call to get() will not block. Similarly, if full() returns False it doesn’t guarantee that a subsequent call to put() will not block.

➡ `Queue.put(item, block=True, timeout=None)`
Put item into the queue. If optional args block is true and timeout is None (the default), block if necessary until a free slot is available. If timeout is a positive number, it blocks at most timeout seconds and raises the Full exception if no free slot was available within that time. Otherwise (block is false), put an item on the queue if a free slot is immediately available, else raise the Full exception (timeout is ignored in that case).

➡ `Queue.put_nowait(item)`
Equivalent to put(item, False).

➡ `Queue.get(block=True, timeout=None)`
Remove and return an item from the queue. If optional args block is true and timeout is None (the default), block if necessary until an item is available. If timeout is a positive number, it blocks at most timeout seconds and raises the Empty exception if no item was available within that time. Otherwise (block is false), return an item if one is immediately available, else raise the Empty exception (timeout is ignored in that case).

Prior to 3.0 on POSIX systems, and for all versions on Windows, if block is true and timeout is None, this operation goes into an uninterruptible wait on an underlying lock. This means that no exceptions can occur, and in particular a SIGINT will not trigger a KeyboardInterrupt.

➡ `Queue.get_nowait()`
Equivalent to get(False).

Two methods are offered to support tracking whether enqueued tasks have been fully processed by daemon consumer threads.

➡ `Queue.task_done()`
Indicate that a formerly enqueued task is complete. Used by queue consumer threads. For each get() used to fetch a task, a subsequent call to task_done() tells the queue that the processing on the task is complete.

If a join() is currently blocking, it will resume when all items have been processed (meaning that a task_done() call was received for every item that had been put() into the queue).

Raises a ValueError if called more times than there were items placed in the queue.

➡ `Queue.join()`
Blocks until all items in the queue have been gotten and processed.

The count of unfinished tasks goes up whenever an item is added to the queue. The count goes down whenever a consumer thread calls task_done() to indicate that the item was retrieved and all work on it is complete. When the count of unfinished tasks drops to zero, join() unblocks.

Example of how to wait for enqueued tasks to be completed:


```py
import threading, queue

q = queue.Queue()

def worker():
    while True:
        item = q.get()
        print(f'Working on {item}')
        print(f'Finished {item}')
        q.task_done()

# turn-on the worker thread
threading.Thread(target=worker, daemon=True).start()

# send thirty task requests to the worker
for item in range(30):
    q.put(item)
print('All task requests sent\n', end='')

# block until all tasks are done
q.join()
print('All work completed')
```



### ===🗝 ◦ SimpleQueue Objects

➡ `SimpleQueue.qsize()`
➡ `SimpleQueue.empty()`
➡ `SimpleQueue.put(item, block=True, timeout=None)`
➡ `SimpleQueue.put_nowait(item)`
➡ `SimpleQueue.get(block=True, timeout=None)`
➡ `SimpleQueue.get_nowait()`

SimpleQueue objects provide the public methods described below.

➡ `SimpleQueue.qsize()`
Return the approximate size of the queue. Note, qsize() > 0 doesn’t guarantee that a subsequent get() will not block.

➡ `SimpleQueue.empty()`
Return True if the queue is empty, False otherwise. If empty() returns False it doesn’t guarantee that a subsequent call to get() will not block.

➡ `SimpleQueue.put(item, block=True, timeout=None)`
Put item into the queue. The method never blocks and always succeeds (except for potential low-level errors such as failure to allocate memory). The optional args block and timeout are ignored and only provided for compatibility with Queue.put().


CPython implementation detail: This method has a C implementation which is reentrant. That is, a put() or get() call can be interrupted by another put() call in the same thread without deadlocking or corrupting internal state inside the queue. This makes it appropriate for use in destructors such as __del__ methods or weakref callbacks.

➡ `SimpleQueue.put_nowait(item)`
Equivalent to put(item), provided for compatibility with Queue.put_nowait().

➡ `SimpleQueue.get(block=True, timeout=None)`
Remove and return an item from the queue. If optional args block is true and timeout is None (the default), block if necessary until an item is available. If timeout is a positive number, it blocks at most timeout seconds and raises the Empty exception if no item was available within that time. Otherwise (block is false), return an item if one is immediately available, else raise the Empty exception (timeout is ignored in that case).

➡ `SimpleQueue.get_nowait()`
Equivalent to get(False).

See also:
 Class multiprocessing.QueueA queue class for use in a multi-processing (rather than multi-threading) context.
collections.deque is an alternative implementation of unbounded queues with fast atomic append() and popleft() operations that do not require locking and also support indexing.



## ==⚡ • contextvars — Context Variables

✅ `class contextvars.ContextVar(name[, *, default])`
    ➡ `name`
    ➡ `get([default])`
    ➡ `set(value)`
    ➡ `reset(token)`
✅ `class contextvars.Token`
    ➡ `var`
    ➡ `old_value`
    ➡ `MISSING`
➡ `contextvars.copy_context()`
✅ `class contextvars.Context`
    ➡ `run(callable, *args, **kwargs)`
    ➡ `copy()`
    ➡ `context[var]`
    ➡ `get(var[, default])`
    ➡ `iter(context)`
    ➡ `len(proxy)`
    ➡ `keys()`
    ➡ `values()`
    ➡ `items()`

This module provides APIs to manage, store, and access context-local state. The ContextVar class is used to declare and work with Context Variables. The copy_context() function and the Context class should be used to manage the current context in asynchronous frameworks.

Context managers that have state should use Context Variables instead of threading.local() to prevent their state from bleeding to other code unexpectedly, when used in concurrent code.

See also PEP 567 for additional details.

New in version 3.7.

### ===🗝 ◦ Context Variables

✅ `class contextvars.ContextVar(name[, *, default])`
This class is used to declare a new Context Variable, e.g.:


var: ContextVar[int] = ContextVar('var', default=42)


The required name parameter is used for introspection and debug purposes.

The optional keyword-only default parameter is returned by ContextVar.get() when no value for the variable is found in the current context.

Important: Context Variables should be created at the top module level and never in closures. Context objects hold strong references to context variables which prevents context variables from being properly garbage collected.

➡ `name`
The name of the variable. This is a read-only property.

New in version 3.7.1.


➡ `get([default])`
Return a value for the context variable for the current context.

If there is no value for the variable in the current context, the method will:
• return the value of the default argument of the method, if provided; or
• return the default value for the context variable, if it was created with one; or
• raise a LookupError.

➡ `set(value)`
Call to set a new value for the context variable in the current context.

The required value argument is the new value for the context variable.

Returns a Token object that can be used to restore the variable to its previous value via the ContextVar.reset() method.

➡ `reset(token)`
Reset the context variable to the value it had before the ContextVar.set() that created the token was used.

For example:

```py
var = ContextVar('var')

token = var.set('new value')
# code that uses 'var'; var.get() returns 'new value'.
var.reset(token)

# After the reset call the var has no value again, so
# var.get() would raise a LookupError.
```


✅ `class contextvars.Token`
Token objects are returned by the ContextVar.set() method. They can be passed to the ContextVar.reset() method to revert the value of the variable to what it was before the corresponding set.

➡ `var`
A read-only property. Points to the ContextVar object that created the token.

➡ `old_value`
A read-only property. Set to the value the variable had before the ContextVar.set() method call that created the token. It points to Token.MISSING is the variable was not set before the call.

➡ `MISSING`
A marker object used by Token.old_value.


### ===🗝 ◦ Manual Context Management

➡ `contextvars.copy_context()`
Returns a copy of the current Context object.

The following snippet gets a copy of the current context and prints all variables and their values that are set in it:


```py
ctx: Context = copy_context()
print(list(ctx.items()))
```


The function has an O(1) complexity, i.e. works equally fast for contexts with a few context variables and for contexts that have a lot of them.

✅ `class contextvars.Context`
A mapping of ContextVars to their values.

Context() creates an empty context with no values in it. To get a copy of the current context use the copy_context() function.

Context implements the collections.abc.Mapping interface.

➡ `run(callable, *args, **kwargs)`
Execute callable(*args, **kwargs) code in the context object the run method is called on. Return the result of the execution or propagate an exception if one occurred.

Any changes to any context variables that callable makes will be contained in the context object:


```py
var = ContextVar('var')
var.set('spam')

def main():
    # 'var' was set to 'spam' before
    # calling 'copy_context()' and 'ctx.run(main)', so:
    # var.get() == ctx[var] == 'spam'

    var.set('ham')

    # Now, after setting 'var' to 'ham':
    # var.get() == ctx[var] == 'ham'

ctx = copy_context()

# Any changes that the 'main' function makes to 'var'
# will be contained in 'ctx'.
ctx.run(main)

# The 'main()' function was run in the 'ctx' context,
# so changes to 'var' are contained in it:
# ctx[var] == 'ham'

# However, outside of 'ctx', 'var' is still set to 'spam':
# var.get() == 'spam'
```


The method raises a RuntimeError when called on the same context object from more than one OS thread, or when called recursively.

➡ `copy()`
Return a shallow copy of the context object.
var in context
Return True if the context has a value for var set; return False otherwise.

➡ `context[var]`
Return the value of the var ContextVar variable. If the variable is not set in the context object, a KeyError is raised.

➡ `get(var[, default])`
Return the value for var if var has the value in the context object. Return default otherwise. If default is not given, return None.

➡ `iter(context)`
Return an iterator over the variables stored in the context object.

➡ `len(proxy)`
Return the number of variables set in the context object.

➡ `keys()`
Return a list of all variables in the context object.

➡ `values()`
Return a list of all variables’ values in the context object.

➡ `items()`
Return a list of 2-tuples containing all variables and their values in the context object.


### ===🗝 ◦ asyncio support

Context variables are natively supported in asyncio and are ready to be used without any extra configuration. For example, here is a simple echo server, that uses a context variable to make the address of a remote client available in the Task that handles that client:


```py
import asyncio
import contextvars

client_addr_var = contextvars.ContextVar('client_addr')

def render_goodbye():
    # The address of the currently handled client can be accessed
    # without passing it explicitly to this function.

    client_addr = client_addr_var.get()
    return f'Good bye, client @ {client_addr}\n'.encode()

async def handle_request(reader, writer):
    addr = writer.transport.get_extra_info('socket').getpeername()
    client_addr_var.set(addr)

    # In any code that we call is now possible to get
    # client's address by calling 'client_addr_var.get()'.

    while True:
        line = await reader.readline()
        print(line)
        if not line.strip():
            break
        writer.write(line)

    writer.write(render_goodbye())
    writer.close()

async def main():
    srv = await asyncio.start_server(
        handle_request, '127.0.0.1', 8081)

    async with srv:
        await srv.serve_forever()

asyncio.run(main())

# To test it you can use telnet:
#     telnet 127.0.0.1 8081
```


## ==⚡ • `_thread` — Low-level threading API

✅ `exception _thread.error`
➡ `_thread.LockType`
➡ `_thread.start_new_thread(function, args[, kwargs])`
➡ `_thread.interrupt_main(signum=signal.SIGINT, /)`
➡ `_thread.exit()`
➡ `_thread.allocate_lock()`
➡ `_thread.get_ident()`
➡ `_thread.get_native_id()`
➡ `_thread.stack_size([size])`
➡ `_thread.TIMEOUT_MAX`

➡ `lock.acquire(waitflag=1, timeout=-1)`
➡ `lock.release()`
➡ `lock.locked()`

This module provides low-level primitives for working with multiple threads (also called light-weight processes or tasks) — multiple threads of control sharing their global data space. For synchronization, simple locks (also called mutexes or binary semaphores) are provided. The threading module provides an easier to use and higher-level threading API built on top of this module.

Changed in version 3.7: This module used to be optional, it is now always available.


This module defines the following constants and functions:

✅ `exception _thread.error`
Raised on thread-specific errors.

Changed in version 3.3: This is now a synonym of the built-in RuntimeError.


➡ `_thread.LockType`
This is the type of lock objects.

➡ `_thread.start_new_thread(function, args[, kwargs])`
Start a new thread and return its identifier. The thread executes the function function with the argument list args (which must be a tuple). The optional kwargs argument specifies a dictionary of keyword arguments.

When the function returns, the thread silently exits.

When the function terminates with an unhandled exception, sys.unraisablehook() is called to handle the exception. The object attribute of the hook argument is function. By default, a stack trace is printed and then the thread exits (but other threads continue to run).

When the function raises a SystemExit exception, it is silently ignored.

Changed in version 3.8: sys.unraisablehook() is now used to handle unhandled exceptions.


➡ `_thread.interrupt_main(signum=signal.SIGINT, /)`
Simulate the effect of a signal arriving in the main thread. A thread can use this function to interrupt the main thread, though there is no guarantee that the interruption will happen immediately.

If given, signum is the number of the signal to simulate. If signum is not given, signal.SIGINT is simulated.

If the given signal isn’t handled by Python (it was set to signal.SIG_DFL or signal.SIG_IGN), this function does nothing.

Changed in version 3.10: The signum argument is added to customize the signal number.


Note:
 This does not emit the corresponding signal but schedules a call to the associated handler (if it exists). If you want to truly emit the signal, use signal.raise_signal().
 

➡ `_thread.exit()`
Raise the SystemExit exception. When not caught, this will cause the thread to exit silently.

➡ `_thread.allocate_lock()`
Return a new lock object. Methods of locks are described below. The lock is initially unlocked.

➡ `_thread.get_ident()`
Return the ‘thread identifier’ of the current thread. This is a nonzero integer. Its value has no direct meaning; it is intended as a magic cookie to be used e.g. to index a dictionary of thread-specific data. Thread identifiers may be recycled when a thread exits and another thread is created.

➡ `_thread.get_native_id()`
Return the native integral Thread ID of the current thread assigned by the kernel. This is a non-negative integer. Its value may be used to uniquely identify this particular thread system-wide (until the thread terminates, after which the value may be recycled by the OS).

Availability: Windows, FreeBSD, Linux, macOS, OpenBSD, NetBSD, AIX.

New in version 3.8.


➡ `_thread.stack_size([size])`
Return the thread stack size used when creating new threads. The optional size argument specifies the stack size to be used for subsequently created threads, and must be 0 (use platform or configured default) or a positive integer value of at least 32,768 (32 KiB). If size is not specified, 0 is used. If changing the thread stack size is unsupported, a RuntimeError is raised. If the specified stack size is invalid, a ValueError is raised and the stack size is unmodified. 32 KiB is currently the minimum supported stack size value to guarantee sufficient stack space for the interpreter itself. Note that some platforms may have particular restrictions on values for the stack size, such as requiring a minimum stack size > 32 KiB or requiring allocation in multiples of the system memory page size - platform documentation should be referred to for more information (4 KiB pages are common; using multiples of 4096 for the stack size is the suggested approach in the absence of more specific information).

Availability: Windows, systems with POSIX threads.

➡ `_thread.TIMEOUT_MAX`
The maximum value allowed for the timeout parameter of Lock.acquire(). Specifying a timeout greater than this value will raise an OverflowError.

New in version 3.2.


Lock objects have the following methods:

➡ `lock.acquire(waitflag=1, timeout=-1)`
Without any optional argument, this method acquires the lock unconditionally, if necessary waiting until it is released by another thread (only one thread at a time can acquire a lock — that’s their reason for existence).

If the integer waitflag argument is present, the action depends on its value: if it is zero, the lock is only acquired if it can be acquired immediately without waiting, while if it is nonzero, the lock is acquired unconditionally as above.

If the floating-point timeout argument is present and positive, it specifies the maximum wait time in seconds before returning. A negative timeout argument specifies an unbounded wait. You cannot specify a timeout if waitflag is zero.

The return value is True if the lock is acquired successfully, False if not.

Changed in version 3.2: The timeout parameter is new.


Changed in version 3.2: Lock acquires can now be interrupted by signals on POSIX.


➡ `lock.release()`
Releases the lock. The lock must have been acquired earlier, but not necessarily by the same thread.

➡ `lock.locked()`
Return the status of the lock: True if it has been acquired by some thread, False if not.

In addition to these methods, lock objects can also be used via the with statement, e.g.:


```py
import _thread

a_lock = _thread.allocate_lock()

with a_lock:
    print("a_lock is locked while this executes")
```


Caveats:

• Threads interact strangely with interrupts: the KeyboardInterrupt exception will be received by an arbitrary thread. (When the signal module is available, interrupts always go to the main thread.)

• Calling sys.exit() or raising the SystemExit exception is equivalent to calling `_thread.exit()`.

• It is not possible to interrupt the acquire() method on a lock — the KeyboardInterrupt exception will happen after the lock has been acquired.

• When the main thread exits, it is system defined whether the other threads survive. On most systems, they are killed without executing try … finally clauses or executing object destructors.

• When the main thread exits, it does not do any of its usual cleanup (except that try … finally clauses are honored), and the standard I/O files are not flushed.




# =🚩 Networking and Interprocess Communication
- [Interprocess Communication and Networking](https://docs.python.org/3.9/library/ipc.html)

The modules described in this chapter provide mechanisms for networking and inter-processes communication.

Some modules only work for two processes that are on the same machine, e.g. signal and mmap. Other modules support networking protocols that two or more processes can use to communicate across machines.

The list of modules described in this chapter is:

- `asyncio` — Asynchronous I/O
- `socket` — Low-level networking interface
- `ssl` — TLS/SSL wrapper for socket objects
- `select` — Waiting for I/O completion
- `selectors` — High-level I/O multiplexing
- `asyncore` — Asynchronous socket handler
- `asynchat` — Asynchronous socket command/response handler
- `signal` — Set handlers for asynchronous events
- `mmap` — Memory-mapped file support

## ==⚡ • asyncio — Asynchronous I/O

asyncio is a library to write concurrent code using the async/await syntax.

asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.

asyncio is often a perfect fit for IO-bound and high-level structured network code.

asyncio provides a set of high-level APIs to:

• run Python coroutines concurrently and have full control over their execution;
• perform network IO and IPC;
• control subprocesses;
• distribute tasks via queues;
• synchronize concurrent code;

Additionally, there are low-level APIs for library and framework developers to:

• create and manage event loops, which provide asynchronous APIs for networking, running subprocesses, handling OS signals, etc;
• implement efficient protocols using transports;
• bridge callback-based libraries and code with async/await syntax.

Reference

High-level APIs

• Coroutines and Tasks
• Streams
• Synchronization Primitives
• Subprocesses
• Queues
• Exceptions


Low-level APIs

• Event Loop
• Futures
• Transports and Protocols
• Policies
• Platform Support


Guides and Tutorials

• High-level API Index
• Low-level API Index
• Developing with asyncio

Note:
 The source code for asyncio can be found in Lib/asyncio/.

### ===🗝 • Coroutines and Tasks

This section outlines high-level asyncio APIs to work with coroutines and Tasks.

• Coroutines
• Awaitables
• Running an asyncio Program
• Creating Tasks
• Sleeping
• Running Tasks Concurrently
• Shielding From Cancellation
• Timeouts
• Waiting Primitives
• Running in Threads
• Scheduling From Other Threads
• Introspection
• Task Object
• Generator-based Coroutines
We say that an object is an awaitable object if it can be used in an `await` expression. Many asyncio APIs are designed to accept awaitables.

• Awaitables Objects

    ➡ `Coroutines` (Decorator)
    ➡ `Tasks`
    ➡ `Futures`

✅ `class asyncio.Task(coro, *, loop=None, name=None)`
➡ ↪ `cancel(msg=None)`
➡ ↪ `cancelled()`
➡ ↪ `done()`
➡ ↪ `result()`
➡ ↪ `exception()`
➡ ↪ `add_done_callback(callback, *, context=None)`
➡ ↪ `remove_done_callback(callback)`
➡ ↪ `get_stack(*, limit=None)`
➡ ↪ `print_stack(*, limit=None, file=None)`
➡ ↪ `get_coro()`
➡ ↪ `get_name()`
➡ ↪ `set_name(value)`

➡ `asyncio.run(coro, *, debug=False)`
➡ `asyncio.create_task(coro, *, name=None)`
➡ `asyncio.as_completed(aws, *, timeout=None)`
➡ `asyncio.run_coroutine_threadsafe(coro, loop)`
➡ `asyncio.current_task(loop=None)`
➡ `asyncio.all_tasks(loop=None)`
➡ `asyncio.iscoroutine(obj)`
➡ `asyncio.iscoroutinefunction(func)`

➡ `awaitable asyncio.gather(*aws, return_exceptions=False)`
➡ `awaitable asyncio.shield(aw)`
➡ `coroutine asyncio.sleep(delay, result=None)`
➡ `coroutine asyncio.wait_for(aw, timeout)`
➡ `coroutine asyncio.wait(aws, *, timeout=None, return_when=ALL_COMPLETED)`
➡ `coroutine asyncio.to_thread(func, /, *args, **kwargs)`


#### • Coroutines

Coroutines declared with the async/await syntax is the preferred way of writing asyncio applications. For example, the following snippet of code (requires Python 3.7+) prints “hello”, waits 1 second, and then prints “world”:


>>> import asyncio

>>> async def main():
...     print('hello')`
...     await asyncio.sleep(1)`
...     print('world')`
>>> asyncio.run(main())`
hello
world


Note that simply calling a coroutine will not schedule it to be executed:


>>> main()`
<coroutine object main at 0x1053bb7c8>


To actually run a coroutine, asyncio provides three main mechanisms:

• The asyncio.run() function to run the top-level entry point “main()” function (see the above example.)


• Awaiting on a coroutine. The following snippet of code will print “hello” after waiting for 1 second, and then print “world” after waiting for another 2 seconds:


```py
import asyncio
import time

async def say_after(delay, what):
    await asyncio.sleep(delay)
    print(what)

async def main():
    print(f"started at {time.strftime('%X')}")

    await say_after(1, 'hello')
    await say_after(2, 'world')

    print(f"finished at {time.strftime('%X')}")

asyncio.run(main())
```


Expected output:


started at 17:13:52
hello
world
finished at 17:13:55



• The asyncio.create_task() function to run coroutines concurrently as asyncio Tasks.

Let’s modify the above example and run two say_after coroutines concurrently:


```py
async def main():
    task1 = asyncio.create_task(
        say_after(1, 'hello'))

    task2 = asyncio.create_task(
        say_after(2, 'world'))

    print(f"started at {time.strftime('%X')}")

    # Wait until both tasks are completed (should take
    # around 2 seconds.)
    await task1
    await task2

    print(f"finished at {time.strftime('%X')}")
```


Note that expected output now shows that the snippet runs 1 second faster than before:


started at 17:14:32
hello
world
finished at 17:14:34




#### • Awaitables

We say that an object is an awaitable object if it can be used in an `await` expression. Many asyncio APIs are designed to accept awaitables.

There are three main types of awaitable objects: coroutines, Tasks, and Futures.

➡ `Coroutines`

Python coroutines are awaitables and therefore can be awaited from other coroutines:


```py
import asyncio

async def nested():
    return 42

async def main():
    # Nothing happens if we just call "nested()".
    # A coroutine object is created but not awaited,
    # so it *won't run at all*.
    nested()

    # Let's do it differently now and await it:
    print(await nested())  # will print "42".

asyncio.run(main())
```


Important:
 In this documentation the term “coroutine” can be used for two closely related concepts:

• a coroutine function: an `async def function;`
• a coroutine object: an object returned by calling a coroutine function.

asyncio also supports legacy generator-based coroutines.

➡ `Tasks`

Tasks are used to schedule coroutines concurrently.

When a coroutine is wrapped into a Task with functions like asyncio.create_task() the coroutine is automatically scheduled to run soon:


```py
import asyncio

async def nested():
    return 42

async def main():
    # Schedule nested() to run soon concurrently
    # with "main()".
    task = asyncio.create_task(nested())

    # "task" can now be used to cancel "nested()", or
    # can simply be awaited to wait until it is complete:
    await task

asyncio.run(main())
```


➡ `Futures`

A Future is a special low-level awaitable object that represents an eventual result of an asynchronous operation.

When a Future object is awaited it means that the coroutine will wait until the Future is resolved in some other place.

Future objects in asyncio are needed to allow callback-based code to be used with async/await.

Normally there is no need to create Future objects at the application level code.

Future objects, sometimes exposed by libraries and some asyncio APIs, can be awaited:


```py
async def main():
    await function_that_returns_a_future_object()

    # this is also valid:
    await asyncio.gather(
        function_that_returns_a_future_object(),
        some_python_coroutine()
    )
```


A good example of a low-level function that returns a Future object is loop.run_in_executor().


#### • Running an asyncio Program

➡ `asyncio.run(coro, *, debug=False)`
Execute the coroutine coro and return the result.

This function runs the passed coroutine, taking care of managing the asyncio event loop, finalizing asynchronous generators, and closing the threadpool.

This function cannot be called when another asyncio event loop is running in the same thread.

If debug is True, the event loop will be run in debug mode.

This function always creates a new event loop and closes it at the end. It should be used as a main entry point for asyncio programs, and should ideally only be called once.

Example:


```py
async def main():
    await asyncio.sleep(1)
    print('hello')

asyncio.run(main())
```


New in version 3.7.


Changed in version 3.9: Updated to use loop.shutdown_default_executor().


Note:
 The source code for asyncio.run() can be found in Lib/asyncio/runners.py.
 


#### • Creating Tasks

➡ `asyncio.create_task(coro, *, name=None)`
Wrap the coro coroutine into a Task and schedule its execution. Return the Task object.

If name is not None, it is set as the name of the task using Task.set_name().

The task is executed in the loop returned by get_running_loop(), RuntimeError is raised if there is no running loop in current thread.

This function has been added in Python 3.7. Prior to Python 3.7, the low-level asyncio.ensure_future() function can be used instead:


```py
async def coro():
    ...

# In Python 3.7+
task = asyncio.create_task(coro())
...

# This works in all Python versions but is less readable
task = asyncio.ensure_future(coro())
...
```


Important:
 Save a reference to the result of this function, to avoid a task disappearing mid execution.
 

New in version 3.7.


Changed in version 3.8: Added the name parameter.



#### • Sleeping

➡ `coroutine asyncio.sleep(delay, result=None)`
Block for delay seconds.

If result is provided, it is returned to the caller when the coroutine completes.

sleep() always suspends the current task, allowing other tasks to run.

Setting the delay to 0 provides an optimized path to allow other tasks to run. This can be used by long-running functions to avoid blocking the event loop for the full duration of the function call.


Deprecated since version 3.8, removed in version 3.10: The loop parameter. This function has been implicitly getting the current running loop since 3.7. See What’s New in 3.10’s Removed section for more information.

Example of coroutine displaying the current date every second for 5 seconds:


```py
import asyncio
import datetime

async def display_date():
    loop = asyncio.get_running_loop()
    end_time = loop.time() + 5.0
    while True:
        print(datetime.datetime.now())
        if (loop.time() + 1.0) >= end_time:
            break
        await asyncio.sleep(1)

asyncio.run(display_date())
```


#### • Running Tasks Concurrently

➡ `awaitable asyncio.gather(*aws, return_exceptions=False)`
Run awaitable objects in the aws sequence concurrently.

If any awaitable in aws is a coroutine, it is automatically scheduled as a Task.

If all awaitables are completed successfully, the result is an aggregate list of returned values. The order of result values corresponds to the order of awaitables in aws.

If return_exceptions is False (default), the first raised exception is immediately propagated to the task that awaits on gather(). Other awaitables in the aws sequence won’t be cancelled and will continue to run.

If return_exceptions is True, exceptions are treated the same as successful results, and aggregated in the result list.

If gather() is cancelled, all submitted awaitables (that have not completed yet) are also cancelled.

If any Task or Future from the aws sequence is cancelled, it is treated as if it raised CancelledError – the gather() call is not cancelled in this case. This is to prevent the cancellation of one submitted Task/Future to cause other Tasks/Futures to be cancelled.


Deprecated since version 3.8, removed in version 3.10: The loop parameter. This function has been implicitly getting the current running loop since 3.7. See What’s New in 3.10’s Removed section for more information.

Example:


```py
import asyncio

async def factorial(name, number):
    f = 1
    for i in range(2, number + 1):
        print(f"Task {name}: Compute factorial({number}), currently i={i}...")
        await asyncio.sleep(1)
        f *= i
    print(f"Task {name}: factorial({number}) = {f}")
    return f

async def main():
    # Schedule three calls *concurrently*:
    L = await asyncio.gather(
        factorial("A", 2),
        factorial("B", 3),
        factorial("C", 4),
    )
    print(L)

asyncio.run(main())

# Expected output:
#
#     Task A: Compute factorial(2), currently i=2...
#     Task B: Compute factorial(3), currently i=2...
#     Task C: Compute factorial(4), currently i=2...
#     Task A: factorial(2) = 2
#     Task B: Compute factorial(3), currently i=3...
#     Task C: Compute factorial(4), currently i=3...
#     Task B: factorial(3) = 6
#     Task C: Compute factorial(4), currently i=4...
#     Task C: factorial(4) = 24
#     [2, 6, 24]
```


Note:
 If return_exceptions is False, cancelling gather() after it has been marked done won’t cancel any submitted awaitables. For instance, gather can be marked done after propagating an exception to the caller, therefore, calling gather.cancel() after catching an exception (raised by one of the awaitables) from gather won’t cancel any other awaitables.
 

Changed in version 3.7: If the gather itself is cancelled, the cancellation is propagated regardless of return_exceptions.



Deprecated since version 3.8, removed in version 3.10: The loop parameter. This function has been implicitly getting the current running loop since 3.7. See What’s New in 3.10’s Removed section for more information.


Deprecated since version 3.10: Deprecation warning is emitted if no positional arguments are provided or not all positional arguments are Future-like objects and there is no running event loop.


#### • Shielding From Cancellation

➡ `awaitable asyncio.shield(aw)`
Protect an awaitable object from being cancelled.

If aw is a coroutine it is automatically scheduled as a Task.

The statement:


    res = await shield(something())


is equivalent to:


    res = await something()


except that if the coroutine containing it is cancelled, the Task running in something() is not cancelled. From the point of view of something(), the cancellation did not happen. Although its caller is still cancelled, so the “await” expression still raises a CancelledError.

If something() is cancelled by other means (i.e. from within itself) that would also cancel shield().

If it is desired to completely ignore cancellation (not recommended) the shield() function should be combined with a try/except clause, as follows:


```py
try:
    res = await shield(something())
except CancelledError:
    res = None
```



Deprecated since version 3.8, removed in version 3.10: The loop parameter. This function has been implicitly getting the current running loop since 3.7. See What’s New in 3.10’s Removed section for more information.


Deprecated since version 3.10: Deprecation warning is emitted if aw is not Future-like object and there is no running event loop.


#### • Timeouts

➡ `coroutine asyncio.wait_for(aw, timeout)`
Wait for the aw awaitable to complete with a timeout.

If aw is a coroutine it is automatically scheduled as a Task.

timeout can either be None or a float or int number of seconds to wait for. If timeout is None, block until the future completes.

If a timeout occurs, it cancels the task and raises asyncio.TimeoutError.

To avoid the task cancellation, wrap it in shield().

The function will wait until the future is actually cancelled, so the total wait time may exceed the timeout. If an exception happens during cancellation, it is propagated.

If the wait is cancelled, the future aw is also cancelled.


Deprecated since version 3.8, removed in version 3.10: The loop parameter. This function has been implicitly getting the current running loop since 3.7. See What’s New in 3.10’s Removed section for more information.

Example:


```py
async def eternity():
    # Sleep for one hour
    await asyncio.sleep(3600)
    print('yay!')

async def main():
    # Wait for at most 1 second
    try:
        await asyncio.wait_for(eternity(), timeout=1.0)
    except asyncio.TimeoutError:
        print('timeout!')

asyncio.run(main())

# Expected output:
#
#     timeout!
```


Changed in version 3.7: When aw is cancelled due to a timeout, wait_for waits for aw to be cancelled. Previously, it raised asyncio.TimeoutError immediately.



Deprecated since version 3.8, removed in version 3.10: The loop parameter. This function has been implicitly getting the current running loop since 3.7. See What’s New in 3.10’s Removed section for more information.


#### • Waiting Primitives

➡ `coroutine asyncio.wait(aws, *, timeout=None, return_when=ALL_COMPLETED)`
Run awaitable objects in the aws iterable concurrently and block until the condition specified by return_when.

The aws iterable must not be empty.

Returns two sets of Tasks/Futures: (done, pending).

Usage:


    done, pending = await asyncio.wait(aws)


➡ ↪ `timeout` (a float or int), if specified, can be used to control the maximum number of seconds to wait before returning.

Note that this function does not raise asyncio.TimeoutError. Futures or Tasks that aren’t done when the timeout occurs are simply returned in the second set.

➡ ↪ `return_when` indicates when this function should return. It must be one of the following constants:


| Constant         | Description |
|------------------|-------------|
| FIRST_COMPLETED  | The function will return when any future finishes or is cancelled. 
| FIRST_EXCEPTION  | The function will return when any future finishes by raising an exception. If no future raises an exception then it is equivalent to ALL_COMPLETED. 
| ALL_COMPLETED    | The function will return when all futures finish or are cancelled. 

Unlike wait_for(), wait() does not cancel the futures when a timeout occurs.


Deprecated since version 3.8: If any awaitable in aws is a coroutine, it is automatically scheduled as a Task. Passing coroutines objects to wait() directly is deprecated as it leads to confusing behavior.


Deprecated since version 3.8, removed in version 3.10: The loop parameter. This function has been implicitly getting the current running loop since 3.7. See What’s New in 3.10’s Removed section for more information.

Note:
 wait() schedules coroutines as Tasks automatically and later returns those implicitly created Task objects in (done, pending) sets. Therefore the following code won’t work as expected:
 

```py
async def foo():
    return 42

coro = foo()
done, pending = await asyncio.wait({coro})

if coro in done:
    # This branch will never be run!
```


Here is how the above snippet can be fixed:


```py
async def foo():
    return 42

task = asyncio.create_task(foo())
done, pending = await asyncio.wait({task})

if task in done:
    # Everything will work as expected now.
```



Deprecated since version 3.8, removed in version 3.10: The loop parameter. This function has been implicitly getting the current running loop since 3.7. See What’s New in 3.10’s Removed section for more information.


Deprecated since version 3.8, will be removed in version 3.11: Passing coroutine objects to wait() directly is deprecated.

➡ `asyncio.as_completed(aws, *, timeout=None)`
Run awaitable objects in the aws iterable concurrently. Return an iterator of coroutines. Each coroutine returned can be awaited to get the earliest next result from the iterable of the remaining awaitables.

Raises asyncio.TimeoutError if the timeout occurs before all Futures are done.


Deprecated since version 3.8, removed in version 3.10: The loop parameter. This function has been implicitly getting the current running loop since 3.7. See What’s New in 3.10’s Removed section for more information.

Example:


```py
for coro in as_completed(aws):
    earliest_result = await coro
    # ...
```



Deprecated since version 3.8, removed in version 3.10: The loop parameter. This function has been implicitly getting the current running loop since 3.7. See What’s New in 3.10’s Removed section for more information.


Deprecated since version 3.10: Deprecation warning is emitted if not all awaitable objects in the aws iterable are Future-like objects and there is no running event loop.


#### • Running in Threads

➡ `coroutine asyncio.to_thread(func, /, *args, **kwargs)`
Asynchronously run function func in a separate thread.

Any `*args` and `**kwargs` supplied for this function are directly passed to func. Also, the current contextvars.Context is propagated, allowing context variables from the event loop thread to be accessed in the separate thread.

Return a coroutine that can be awaited to get the eventual result of func.

This coroutine function is primarily intended to be used for executing IO-bound functions/methods that would otherwise block the event loop if they were ran in the main thread. For example:


```py
def blocking_io():
    print(f"start blocking_io at {time.strftime('%X')}")
    # Note that time.sleep() can be replaced with any blocking
    # IO-bound operation, such as file operations.
    time.sleep(1)
    print(f"blocking_io complete at {time.strftime('%X')}")

async def main():
    print(f"started main at {time.strftime('%X')}")

    await asyncio.gather(
        asyncio.to_thread(blocking_io),
        asyncio.sleep(1))

    print(f"finished main at {time.strftime('%X')}")


asyncio.run(main())

# Expected output:
#
# started main at 19:50:53
# start blocking_io at 19:50:53
# blocking_io complete at 19:50:54
# finished main at 19:50:54
```


Directly calling blocking_io() in any coroutine would block the event loop for its duration, resulting in an additional 1 second of run time. Instead, by using asyncio.to_thread(), we can run it in a separate thread without blocking the event loop.

Note:
 Due to the GIL, asyncio.to_thread() can typically only be used to make IO-bound functions non-blocking. However, for extension modules that release the GIL or alternative Python implementations that don’t have one, asyncio.to_thread() can also be used for CPU-bound functions.
 

New in version 3.9.



#### • Scheduling From Other Threads

➡ `asyncio.run_coroutine_threadsafe(coro, loop)`
Submit a coroutine to the given event loop. Thread-safe.

Return a concurrent.futures.Future to wait for the result from another OS thread.

This function is meant to be called from a different OS thread than the one where the event loop is running. Example:


```py
# Create a coroutine
coro = asyncio.sleep(1, result=3)

# Submit the coroutine to a given loop
future = asyncio.run_coroutine_threadsafe(coro, loop)

# Wait for the result with an optional timeout argument
assert future.result(timeout) == 3
```


If an exception is raised in the coroutine, the returned Future will be notified. It can also be used to cancel the task in the event loop:


```py
try:
    result = future.result(timeout)
except concurrent.futures.TimeoutError:
    print('The coroutine took too long, cancelling the task...')
    future.cancel()
except Exception as exc:
    print(f'The coroutine raised an exception: {exc!r}')
else:
    print(f'The coroutine returned: {result!r}')
```


See the concurrency and multithreading section of the documentation.

Unlike other asyncio functions this function requires the loop argument to be passed explicitly.

New in version 3.5.1.



#### • Introspection

➡ `asyncio.current_task(loop=None)`
Return the currently running Task instance, or None if no task is running.

If loop is None get_running_loop() is used to get the current loop.

New in version 3.7.

➡ `asyncio.all_tasks(loop=None)`
Return a set of not yet finished Task objects run by the loop.

If loop is None, get_running_loop() is used for getting current loop.

New in version 3.7.



#### • Task Object

✅ `class asyncio.Task(coro, *, loop=None, name=None)`
A Future-like object that runs a Python coroutine. Not thread-safe.

Tasks are used to run coroutines in event loops. If a coroutine awaits on a Future, the Task suspends the execution of the coroutine and waits for the completion of the Future. When the Future is done, the execution of the wrapped coroutine resumes.

Event loops use cooperative scheduling: an event loop runs one Task at a time. While a Task awaits for the completion of a Future, the event loop runs other Tasks, callbacks, or performs IO operations.

Use the high-level asyncio.create_task() function to create Tasks, or the low-level loop.create_task() or ensure_future() functions. Manual instantiation of Tasks is discouraged.

To cancel a running Task use the cancel() method. Calling it will cause the Task to throw a CancelledError exception into the wrapped coroutine. If a coroutine is awaiting on a Future object during cancellation, the Future object will be cancelled.

cancelled() can be used to check if the Task was cancelled. The method returns True if the wrapped coroutine did not suppress the CancelledError exception and was actually cancelled.

asyncio.Task inherits from Future all of its APIs except Future.set_result() and Future.set_exception().

Tasks support the contextvars module. When a Task is created it copies the current context and later runs its coroutine in the copied context.

Changed in version 3.7: Added support for the contextvars module.


Changed in version 3.8: Added the name parameter.



Deprecated since version 3.8, removed in version 3.10: The loop parameter.


Deprecated since version 3.10: Deprecation warning is emitted if loop is not specified and there is no running event loop.

➡ ↪ `cancel(msg=None)`
Request the Task to be cancelled.

This arranges for a CancelledError exception to be thrown into the wrapped coroutine on the next cycle of the event loop.

The coroutine then has a chance to clean up or even deny the request by suppressing the exception with a try … … except CancelledError … finally block. Therefore, unlike Future.cancel(), Task.cancel() does not guarantee that the Task will be cancelled, although suppressing cancellation completely is not common and is actively discouraged.

Changed in version 3.9: Added the msg parameter.


The following example illustrates how coroutines can intercept the cancellation request:


```py
async def cancel_me():
    print('cancel_me(): before sleep')

    try:
        # Wait for 1 hour
        await asyncio.sleep(3600)
    except asyncio.CancelledError:
        print('cancel_me(): cancel sleep')
        raise
    finally:
        print('cancel_me(): after sleep')

async def main():
    # Create a "cancel_me" Task
    task = asyncio.create_task(cancel_me())

    # Wait for 1 second
    await asyncio.sleep(1)

    task.cancel()
    try:
        await task
    except asyncio.CancelledError:
        print("main(): cancel_me is cancelled now")

asyncio.run(main())

# Expected output:
#
#     cancel_me(): before sleep
#     cancel_me(): cancel sleep
#     cancel_me(): after sleep
#     main(): cancel_me is cancelled now
```

➡ ↪ `cancelled()`
Return True if the Task is cancelled.

The Task is cancelled when the cancellation was requested with cancel() and the wrapped coroutine propagated the CancelledError exception thrown into it.

➡ ↪ `done()`
Return True if the Task is done.

A Task is done when the wrapped coroutine either returned a value, raised an exception, or the Task was cancelled.

➡ ↪ `result()`
Return the result of the Task.

If the Task is done, the result of the wrapped coroutine is returned (or if the If the Task is done, the result of the wrapped coroutine is returned (or if the coroutine raised an exception, that exception is re-raised.)

If the Task has been cancelled, this method raises a CancelledError exception.

If the Task’s result isn’t yet available, this method raises a InvalidStateError exception.

➡ ↪ `exception()`
Return the exception of the Task.

If the wrapped coroutine raised an exception that exception is returned. If the wrapped coroutine returned normally this method returns None.

If the Task has been cancelled, this method raises a CancelledError exception.

If the Task isn’t done yet, this method raises an InvalidStateError exception.

➡ ↪ `add_done_callback(callback, *, context=None)`
Add a callback to be run when the Task is done.

This method should only be used in low-level callback-based code.

See the documentation of Future.add_done_callback() for more details.

➡ ↪ `remove_done_callback(callback)`
Remove callback from the callbacks list.

This method should only be used in low-level callback-based code.

See the documentation of Future.remove_done_callback() for more details.

➡ ↪ `get_stack(*, limit=None)`
Return the list of stack frames for this Task.

If the wrapped coroutine is not done, this returns the stack where it is suspended. If the coroutine has completed successfully or was cancelled, this returns an empty list. If the coroutine was terminated by an exception, this returns the list of traceback frames.

The frames are always ordered from oldest to newest.

Only one stack frame is returned for a suspended coroutine.

The optional limit argument sets the maximum number of frames to return; by default all available frames are returned. The ordering of the returned list differs depending on whether a stack or a traceback is returned: the newest frames of a stack are returned, but the oldest frames of a traceback are returned. (This matches the behavior of the The optional limit argument sets the maximum number of frames to return; by default all available frames are returned. The ordering of the returned list differs depending on whether a stack or a traceback is returned: the newest frames of a stack are returned, but the oldest frames of a traceback are returned. (This matches the behavior of the traceback module.)

➡ ↪ `print_stack(*, limit=None, file=None)`
Print the stack or traceback for this Task.

This produces output similar to that of the traceback module for the frames retrieved by get_stack().

The limit argument is passed to get_stack() directly.

The file argument is an I/O stream to which the output is written; by default output is written to sys.stderr.

➡ ↪ `get_coro()`
Return the coroutine object wrapped by the Task.

New in version 3.8.

➡ ↪ `get_name()`
Return the name of the Task.

If no name has been explicitly assigned to the Task, the default asyncio Task implementation generates a default name during instantiation.

New in version 3.8.

➡ ↪ `set_name(value)`
Set the name of the Task.

The value argument can be any object, which is then converted to a string.

In the default Task implementation, the name will be visible in the repr() output of a task object.

New in version 3.8.


#### • Generator-based Coroutines

Note:
 Support for generator-based coroutines is deprecated and is scheduled for removal in Python 3.10.
 

Generator-based coroutines predate async/await syntax. They are Python generators that use yield from expressions to await on Futures and other coroutines.

Generator-based coroutines should be decorated with @asyncio.coroutine, although this is not enforced.
@asyncio.coroutine
Decorator to mark generator-based coroutines.

This decorator enables legacy generator-based coroutines to be compatible with async/await code:


```py
@asyncio.coroutine
def old_style_coroutine():
    yield from asyncio.sleep(1)

async def main():
    await old_style_coroutine()
```


This decorator should not be used for async def coroutines.


Deprecated since version 3.8, will be removed in version 3.11: Use async def instead.

➡ `asyncio.iscoroutine(obj)`
Return True if obj is a coroutine object.

This method is different from inspect.iscoroutine() because it returns True for generator-based coroutines.

➡ `asyncio.iscoroutinefunction(func)`
Return True if func is a coroutine function.

This method is different from inspect.iscoroutinefunction() because it returns True for generator-based coroutine functions decorated with @coroutine.



### ===🗝 • Streams

### ===🗝 • Synchronization Primitives

### ===🗝 • Subprocesses

### ===🗝 • Queues

### ===🗝 • Exceptions


### ===🗝 • Event Loop

### ===🗝 • Futures

### ===🗝 • Transports and Protocols

### ===🗝 • Policies

### ===🗝 • Platform Support


### ===🗝 • High-level API Index

### ===🗝 • Low-level API Index

### ===🗝 • Developing with asyncio


19.5.1 Base Event Loop

Run an event loop

    AbstractEventLoop.run_forever()
    AbstractEventLoop.run_until_complete(future)
    AbstractEventLoop.is_running()
    AbstractEventLoop.is_closed()
    AbstractEventLoop.close()
    AbstractEventLoop.stop()

    coroutine AbstractEventLoop.shutdown_asyncgens()

Calls

    AbstractEventLoop.call_soon(callback, *args, context=None)
    AbstractEventLoop.call_soon_threadsafe(callback, *args, context=None)

Delayed calls

    AbstractEventLoop.call_later(delay, callback, *args, context=None)
    AbstractEventLoop.call_at(when, callback, *args, context=None)
    AbstractEventLoop.time()

Fetures

    AbstractEventLoop.create_future()

Tasks

    AbstractEventLoop.create_task(coro)
    AbstractEventLoop.set_task_factory(factory)
    AbstractEventLoop.get_task_factory()

Creating connections

    coroutine AbstractEventLoop.create_connection(protocol_factory, host=None, port=None, *, 
        ssl=None, family=0, proto=0, flags=0, sock=None, local_addr=None, server_hostname=None, ssl_handshake_timeout=None)
    
    coroutine AbstractEventLoop.create_datagram_endpoint(protocol_factory, local_addr=None, 
        remote_addr=None, *, family=0, proto=0, flags=0, reuse_address=None, reuse_port=None, allow_broadcast=None, sock=None)
    
    coroutine AbstractEventLoop.create_unix_connection(protocol_factory, path=None, *, 
        ssl=None, sock=None, server_hostname=None, ssl_handshake_timeout=None)

Creating listening connections

    coroutine AbstractEventLoop.create_server(protocol_factory, host=None, port=None, *, 
        family=socket.AF_UNSPEC, flags=socket.AI_PASSIVE, sock=None, backlog=100, ssl=None, reuse_address=None, reuse_port=None, 
        ssl_handshake_timeout=None, start_serving=True)

    coroutine AbstractEventLoop.create_unix_server(protocol_factory, path=None, *, sock=None, backlog=100, 
        ssl=None, ssl_handshake_timeout=None, start_serving=True)

    coroutine BaseEventLoop.connect_accepted_socket(protocol_factory, sock, *, ssl=None, ssl_handshake_timeout=None)

File Transferring

    coroutine AbstractEventLoop.sendfile(transport, file, offset=0, count=None, *, fallback=True)

TLS Upgrade

    coroutine AbstractEventLoop.start_tls(transport, protocol, sslcontext, *, server_side=False, server_hostname=None, 
        ssl_handshake_timeout=None)

Watch file descriptors

    AbstractEventLoop.add_reader(fd, callback, *args)
    AbstractEventLoop.remove_reader(fd)
    AbstractEventLoop.add_writer(fd, callback, *args)
    AbstractEventLoop.remove_writer(fd)

Low-level socket operations

    coroutine AbstractEventLoop.sock_recv(sock, nbytes)
    coroutine AbstractEventLoop.sock_recv_into(sock, buf)
    coroutine AbstractEventLoop.sock_sendall(sock, data)
    coroutine AbstractEventLoop.sock_connect(sock, address)
    coroutine AbstractEventLoop.sock_accept(sock)
    coroutine AbstractEventLoop.sock_sendfile(sock, file, offset=0, count=None, *, fallback=True)


Resolve host name

    coroutine AbstractEventLoop.getaddrinfo(host, port, *, family=0, type=0, proto=0, flags=0)
    coroutine AbstractEventLoop.getnameinfo(sockaddr, flags=0)


19.5.1.13. Connect pipes

    coroutine AbstractEventLoop.connect_read_pipe(protocol_factory, pipe)
    coroutine AbstractEventLoop.connect_write_pipe(protocol_factory, pipe)


19.5.1.14. UNIX signals

    AbstractEventLoop.add_signal_handler(signum, callback, *args)
    AbstractEventLoop.remove_signal_handler(sig)


19.5.1.15. Executor

    AbstractEventLoop.run_in_executor(executor, func, *args)
    AbstractEventLoop.set_default_executor(executor)


19.5.1.16. Error Handling API

    AbstractEventLoop.set_exception_handler(handler)
    AbstractEventLoop.get_exception_handler()
    AbstractEventLoop.default_exception_handler(context)
    AbstractEventLoop.call_exception_handler(context)

19.5.1.17. Debug mode

    AbstractEventLoop.get_debug()
    AbstractEventLoop.set_debug(enabled: bool)


19.5.1.18. Server

    class asyncio.Server

    Server.close()
    Server.get_loop()
    coroutine Server.start_serving()
    coroutine Server.serve_forever()
    Server.is_serving()
    coroutine Server.wait_closed()


19.5.1.19. Handle

    class asyncio.Handle
    class asyncio.TimerHandle


    exception asyncio.SendfileNotAvailableError


19.5.1.21. Event loop examples


19.5.1.21.1. Hello World with call_soon()

Example using the AbstractEventLoop.call_soon() method to schedule a callback. The callback displays "Hello World" and then stops the event loop:


    import asyncio

    def hello_world(loop):
        print('Hello World')
        loop.stop()

    loop = asyncio.get_event_loop()

    # Schedule a call to hello_world()
    loop.call_soon(hello_world, loop)

    # Blocking call interrupted by loop.stop()
    loop.run_forever()
    loop.close()


19.5.1.21.2. Display the current date with call_later()

Example of callback displaying the current date every second. The callback uses the AbstractEventLoop.call_later() method to reschedule itself during 5 seconds, and then stops the event loop:


    import asyncio
    import datetime

    def display_date(end_time, loop):
        print(datetime.datetime.now())
        if (loop.time() + 1.0) < end_time:
            loop.call_later(1, display_date, end_time, loop)
        else:
            loop.stop()

    loop = asyncio.get_event_loop()

    # Schedule the first call to display_date()
    end_time = loop.time() + 5.0
    loop.call_soon(display_date, end_time, loop)

    # Blocking call interrupted by loop.stop()
    loop.run_forever()
    loop.close()


19.5.1.21.3. Watch a file descriptor for read events

Wait until a file descriptor received some data using the AbstractEventLoop.add_reader() method and then close the event loop:


    import asyncio
    from socket import socketpair

    # Create a pair of connected file descriptors
    rsock, wsock = socketpair()
    loop = asyncio.get_event_loop()

    def reader():
        data = rsock.recv(100)
        print("Received:", data.decode())
        # We are done: unregister the file descriptor
        loop.remove_reader(rsock)
        # Stop the event loop
        loop.stop()

    # Register the file descriptor for read event
    loop.add_reader(rsock, reader)

    # Simulate the reception of data from the network
    loop.call_soon(wsock.send, 'abc'.encode())

    # Run the event loop
    loop.run_forever()

    # We are done, close sockets and the event loop
    rsock.close()
    wsock.close()
    loop.close()


19.5.1.21.4. Set signal handlers for SIGINT and SIGTERM

Register handlers for signals SIGINT and SIGTERM using the AbstractEventLoop.add_signal_handler() method:


    import asyncio
    import functools
    import os
    import signal

    def ask_exit(signame):
        print("got signal %s: exit" % signame)
        loop.stop()

    loop = asyncio.get_event_loop()
    for signame in ('SIGINT', 'SIGTERM'):
        loop.add_signal_handler(getattr(signal, signame),
                                functools.partial(ask_exit, signame))

    print("Event loop running forever, press Ctrl+C to interrupt.")
    print("pid %s: send SIGINT or SIGTERM to exit." % os.getpid())

    try:
        loop.run_forever()
    finally:
        loop.close()


19.5.5. Streams (coroutine based API)

Source code: Lib/asyncio/streams.py


19.5.5.1. Stream functions

    coroutine asyncio.open_connection(host=None, port=None, *, loop=None, limit=None, 
        ssl=None, family=0, proto=0, flags=0, sock=None, local_addr=None, server_hostname=None, ssl_handshake_timeout=None)

    coroutine asyncio.start_server(client_connected_cb, host=None, port=None, *, loop=None, limit=None, 
        family=socket.AF_UNSPEC, flags=socket.AI_PASSIVE, sock=None, backlog=100, ssl=None, reuse_address=None, reuse_port=None,
        ssl_handshake_timeout=None, start_serving=True)

    coroutine asyncio.open_unix_connection(path=None, *, loop=None, limit=None, ssl=None, sock=None, server_hostname=None, 
        ssl_handshake_timeout=None)

    coroutine asyncio.start_unix_server(client_connected_cb, path=None, *, loop=None, limit=None, sock=None, backlog=100, 
        ssl=None, ssl_handshake_timeout=None, start_serving=True)


19.5.5.2. StreamReader

    class asyncio.StreamReader(limit=None, loop=None)

    StreamReader.exception()
    StreamReader.feed_eof()
    StreamReader.feed_data(data)
    StreamReader.set_exception(exc)
    StreamReader.set_transport(transport)
    StreamReader.coroutine read(n=-1)
    StreamReader.coroutine readline()
    StreamReader.coroutine readexactly(n)
    StreamReader.coroutine readuntil(separator=b'\n')
    StreamReader.at_eof()


19.5.5.3. StreamWriter

    class asyncio.StreamWriter(transport, protocol, reader, loop)

    StreamWriter.transport
    StreamWriter.can_write_eof()
    StreamWriter.close()
    StreamWriter.is_closing()
    StreamWriter.coroutine wait_closed()
    StreamWriter.coroutine drain()
    StreamWriter.w.write(data)
    StreamWriter.await w.drain()
    StreamWriter.get_extra_info(name, default=None)
    StreamWriter.write(data)
    StreamWriter.writelines(data)
    StreamWriter.write_eof()


19.5.5.4. StreamReaderProtocol

    class asyncio.StreamReaderProtocol(stream_reader, client_connected_cb=None, loop=None)

19.5.5.5. IncompleteReadError

    exception asyncio.IncompleteReadError

    IncompleteReadError.expected
    IncompleteReadError.partial


19.5.5.6. LimitOverrunError

    exception asyncio.LimitOverrunError

    LimitOverrunError.consumed


19.5.5.7. Stream examples


19.5.5.7.1. TCP echo client using streams

TCP echo client using the asyncio.open_connection() function:


    import asyncio

    async def tcp_echo_client(message, loop):
        reader, writer = await asyncio.open_connection('127.0.0.1', 8888,
                                                       loop=loop)

        print('Send: %r' % message)
        writer.write(message.encode())

        data = await reader.read(100)
        print('Received: %r' % data.decode())

        print('Close the socket')
        writer.close()

    message = 'Hello World!'
    loop = asyncio.get_event_loop()
    loop.run_until_complete(tcp_echo_client(message, loop))
    loop.close()



19.5.5.7.2. TCP echo server using streams

TCP echo server using the asyncio.start_server() function:


    import asyncio

    async def handle_echo(reader, writer):
        data = await reader.read(100)
        message = data.decode()
        addr = writer.get_extra_info('peername')
        print("Received %r from %r" % (message, addr))

        print("Send: %r" % message)
        writer.write(data)
        await writer.drain()

        print("Close the client socket")
        writer.close()

    loop = asyncio.get_event_loop()
    coro = asyncio.start_server(handle_echo, '127.0.0.1', 8888, loop=loop)
    server = loop.run_until_complete(coro)

    # Serve requests until Ctrl+C is pressed
    print('Serving on {}'.format(server.sockets[0].getsockname()))
    try:
        loop.run_forever()
    except KeyboardInterrupt:
        pass

    # Close the server
    server.close()
    loop.run_until_complete(server.wait_closed())
    loop.close()



19.5.5.7.3. Get HTTP headers

Simple example querying HTTP headers of the URL passed on the command line:


    import asyncio
    import urllib.parse
    import sys

    @asyncio.coroutine
    def print_http_headers(url):
        url = urllib.parse.urlsplit(url)
        if url.scheme == 'https':
            connect = asyncio.open_connection(url.hostname, 443, ssl=True)
        else:
            connect = asyncio.open_connection(url.hostname, 80)
        reader, writer = await connect
        query = ('HEAD {path} HTTP/1.0\r\n'
                 'Host: {hostname}\r\n'
                 '\r\n').format(path=url.path or '/', hostname=url.hostname)
        writer.write(query.encode('latin-1'))
        while True:
            line = await reader.readline()
            if not line:
                break
            line = line.decode('latin1').rstrip()
            if line:
                print('HTTP header> %s' % line)

        # Ignore the body, close the socket
        writer.close()

    url = sys.argv[1]
    loop = asyncio.get_event_loop()
    task = asyncio.ensure_future(print_http_headers(url))
    loop.run_until_complete(task)
    loop.close()


Usage:


    python example.py http://example.com/path/page.html
    python example.py https://example.com/path/page.html



19.5.5.7.4. Register an open socket to wait for data using streams

Coroutine waiting until a socket receives data using the open_connection() function:


    import asyncio
    from socket import socketpair

    async def wait_for_data(loop):
        # Create a pair of connected sockets
        rsock, wsock = socketpair()

        # Register the open socket to wait for data
        reader, writer = await asyncio.open_connection(sock=rsock, loop=loop)

        # Simulate the reception of data from the network
        loop.call_soon(wsock.send, 'abc'.encode())

        # Wait for data
        data = await reader.read(100)

        # Got data, we are done: close the socket
        print("Received:", data.decode())
        writer.close()

        # Close the second socket
        wsock.close()

    loop = asyncio.get_event_loop()
    loop.run_until_complete(wait_for_data(loop))
    loop.close()



19.5.8. Queues

Source code: Lib/asyncio/queues.py

非线程安全 Queues:
- Queue 
- PriorityQueue
- LifoQueue


19.5.8.1. Queue

    class asyncio.Queue(maxsize=0, *, loop=None)
    Queue.empty()
    Queue.full()
    coroutine Queue.get()
    Queue.get_nowait()
    coroutine Queue.join()
    coroutine Queue.put(item)
    Queue.put_nowait(item)
    Queue.qsize()
    Queue.task_done()
    Queue.maxsize


19.5.8.2. PriorityQueue

    class asyncio.PriorityQueue

19.5.8.3. LifoQueue
    
    class asyncio.LifoQueue

19.5.8.3.1. Exceptions

    exception asyncio.QueueEmpty
    exception asyncio.QueueFull


19.5.2. Event loops

Source code: Lib/asyncio/events.py


19.5.2.1. Event loop functions

    asyncio.get_event_loop()
    asyncio.set_event_loop(loop)
    asyncio.new_event_loop()
    asyncio.get_running_loop()

19.5.2.2. Available event loops

    class asyncio.SelectorEventLoop

    class asyncio.ProactorEventLoop


19.5.2.3. Platform support

19.5.2.3.1. Windows

not supported

    create_unix_connection() 
    create_unix_server()
    add_signal_handler()
    remove_signal_handler()
    EventLoopPolicy.set_child_watcher()
    ProactorEventLoop.create_datagram_endpoint() (UDP) is not supported
    ProactorEventLoop.add_reader() and add_writer() are not supported


ProactorEventLoop supports subprocesses.

SelectorEventLoop specific limits:

    •SelectSelector is used which only supports sockets and is limited to 512 sockets.
    •add_reader() and add_writer() only accept file descriptors of sockets
    •Pipes are not supported (ex: connect_read_pipe(), connect_write_pipe())
    •Subprocesses are not supported (ex: subprocess_exec(), subprocess_shell())


19.5.2.3.2. Mac OS X

Character devices like PTY are only well supported since Mavericks (Mac OS 10.9). They are not supported at all on Mac OS 10.5 and older.

On Mac OS 10.6, 10.7 and 10.8, the default event loop is SelectorEventLoop which uses selectors.KqueueSelector. selectors.KqueueSelector does not support character devices on these versions. The SelectorEventLoop can be used with SelectSelector or PollSelector to support character devices on these versions of Mac OS X. Example:


    import asyncio
    import selectors

    selector = selectors.SelectSelector()
    loop = asyncio.SelectorEventLoop(selector)
    asyncio.set_event_loop(loop)


19.5.2.5. Event loop policy interface

    class asyncio.AbstractEventLoopPolicy

    AbstractEventLoopPolicy.get_event_loop()
    AbstractEventLoopPolicy.set_event_loop(loop)
    AbstractEventLoopPolicy.new_event_loop()


19.5.2.6. Access to the global loop policy

    asyncio.get_event_loop_policy()
    asyncio.set_event_loop_policy(policy)

19.5.3. Tasks and coroutines

    Source code: Lib/asyncio/tasks.py
    Source code: Lib/asyncio/coroutines.py


19.5.3.1. Coroutines

Things a coroutine can do:

•result = await future or result = yield from future – suspends the coroutine until the future is done, then returns the future’s result, or raises an exception. 
•result = await coroutine or result = yield from coroutine – wait for another coroutine to produce a result
•return expression – produce a result to the coroutine that is waiting for this one using await or yield from.
•raise exception – raise an exception in the coroutine that is waiting for this one using await or yield from.

    @asyncio.coroutine

    asyncio.run(coro, *, debug=False)

19.5.3.1.1. Example: Hello World coroutine


    import asyncio

    async def hello_world():
        print("Hello World!")

    asyncio.run(hello_world())


19.5.3.1.2. Example: Coroutine displaying the current date


    import asyncio
    import datetime

    async def display_date():
        loop = asyncio.get_running_loop()
        end_time = loop.time() + 5.0
        while True:
            print(datetime.datetime.now())
            if (loop.time() + 1.0) >= end_time:
                break
            await asyncio.sleep(1)

    asyncio.run(display_date())



19.5.3.1.3. Example: Chain coroutines


    import asyncio

    async def compute(x, y):
        print("Compute %s + %s ..." % (x, y))
        await asyncio.sleep(1.0)
        return x + y

    async def print_sum(x, y):
        result = await compute(x, y)
        print("%s + %s = %s" % (x, y, result))

    loop = asyncio.get_event_loop()
    loop.run_until_complete(print_sum(1, 2))
    loop.close()


19.5.3.2. InvalidStateError

    exception asyncio.InvalidStateError


19.5.3.3. TimeoutError

    exception asyncio.TimeoutError



19.5.3.4. Future

    class asyncio.Future(*, loop=None)

    Future.cancel()
    Future.cancelled()
    Future.done()
    Future.result()
    Future.exception()
    Future.add_done_callback(callback, *, context=None)
    Future.remove_done_callback(fn)
    Future.set_result(result)
    Future.set_exception(exception)
    Future.get_loop()

19.5.3.4.1. Example: Future with run_until_complete()


    import asyncio

    async def slow_operation(future):
        await asyncio.sleep(1)
        future.set_result('Future is done!')

    loop = asyncio.get_event_loop()
    future = asyncio.Future()
    asyncio.ensure_future(slow_operation(future))
    loop.run_until_complete(future)
    print(future.result())
    loop.close()



19.5.3.4.2. Example: Future with run_forever()


    import asyncio

    async def slow_operation(future):
        await asyncio.sleep(1)
        future.set_result('Future is done!')

    def got_result(future):
        print(future.result())
        loop.stop()

    loop = asyncio.get_event_loop()
    future = asyncio.Future()
    asyncio.ensure_future(slow_operation(future))
    future.add_done_callback(got_result)
    try:
        loop.run_forever()
    finally:
        loop.close()



19.5.3.5. Task

    AbstractEventLoop.create_task(coro)

    class asyncio.Task(coro, *, loop=None)

    classmethod all_tasks(loop=None)
    classmethod current_task(loop=None)

    Task.cancel()
    Task.get_stack(*, limit=None)
    Task.print_stack(*, limit=None, file=None)


19.5.3.6. Task functions

    asyncio.current_task(loop=None)
    asyncio.all_tasks(loop=None)
    asyncio.as_completed(fs, *, loop=None, timeout=None)
    asyncio.ensure_future(coro_or_future, *, loop=None)
    asyncio.wrap_future(future, *, loop=None)
    asyncio.gather(*coros_or_futures, loop=None, return_exceptions=False)
    asyncio.iscoroutine(obj)
    asyncio.iscoroutinefunction(func)
    asyncio.run_coroutine_threadsafe(coro, loop)

    coroutine asyncio.sleep(delay, result=None, *, loop=None)
    coroutine asyncio.shield(arg, *, loop=None)
    coroutine asyncio.wait(futures, *, loop=None, timeout=None, return_when=ALL_COMPLETED)
    coroutine asyncio.wait_for(fut, timeout, *, loop=None)


## ==⚡ • socket — Low-level networking interface

    socket.accept()
    socket.AF_ALGsocket.SOL_ALGALG_*
    socket.AF_CANsocket.PF_CANSOL_CAN_*CAN_*
    socket.AF_LINK
    socket.AF_RDSsocket.PF_RDSsocket.SOL_RDSRDS_*
    socket.AF_UNIXsocket.AF_INETsocket.AF_INET6
    socket.AF_VSOCKsocket.IOCTL_VM_SOCKETS_GET_LOCAL_CIDVMADDR*SO_VM*
    socket.BDADDR_ANYsocket.BDADDR_LOCAL
    socket.bind(address)
    socket.CAN_BCMCAN_BCM_*
    socket.CAN_ISOTP
    socket.CAN_RAW_FD_FRAMES
    socket.close()
    socket.close(fd)
    socket.CMSG_LEN(length)
    socket.CMSG_SPACE(length)
    socket.connect(address)
    socket.connect_ex(address)
    socket.create_connection(address[, timeout[, source_address]])
    socket.detach()
    socket.dup()
    socket.family
    socket.fileno()
    socket.fromfd(fd, family, type, proto=0)
    socket.fromshare(data)
    socket.get_inheritable()
    socket.getaddrinfo(host, port, family=0, type=0, proto=0, flags=0)
    socket.getblocking()
    socket.getdefaulttimeout()
    socket.getfqdn([name])
    socket.gethostbyaddr(ip_address)
    socket.gethostbyname(hostname)
    socket.gethostbyname_ex(hostname)
    socket.gethostname()
    socket.getnameinfo(sockaddr, flags)
    socket.getpeername()
    socket.getprotobyname(protocolname)
    socket.getservbyname(servicename[, protocolname])
    socket.getservbyport(port[, protocolname])
    socket.getsockname()
    socket.getsockopt(level, optname[, buflen])
    socket.gettimeout()
    socket.has_ipv6
    socket.HCI_FILTERsocket.HCI_TIME_STAMPsocket.HCI_DATA_DIR
    socket.htonl(x)
    socket.htons(x)
    socket.if_indextoname(if_index)
    socket.if_nameindex()
    socket.if_nametoindex(if_name)
    socket.inet_aton(ip_string)
    socket.inet_ntoa(packed_ip)
    socket.inet_ntop(address_family, packed_ip)
    socket.inet_pton(address_family, ip_string)
    socket.ioctl(control, option)
    socket.listen([backlog])
    socket.makefile(mode='r', buffering=None, *, encoding=None, errors=None, newline=None)
    socket.ntohl(x)
    socket.ntohs(x)
    socket.proto
    socket.recv(bufsize[, flags])
    socket.recv_into(buffer[, nbytes[, flags]])
    socket.recvfrom(bufsize[, flags])
    socket.recvfrom_into(buffer[, nbytes[, flags]])
    socket.recvmsg(bufsize[, ancbufsize[, flags]])
    socket.recvmsg_into(buffers[, ancbufsize[, flags]])
    socket.send(bytes[, flags])
    socket.sendall(bytes[, flags])
    socket.sendfile(file, offset=0, count=None)
    socket.sendmsg(buffers[, ancdata[, flags[, address]]])
    socket.sendmsg_afalg([msg, ]*, op[, iv[, assoclen[, flags]]])
    socket.sendto(bytes, address)socket.sendto(bytes, flags, address)
    socket.set_inheritable(inheritable)
    socket.setblocking(flag)
    socket.setdefaulttimeout(timeout)
    socket.sethostname(name)
    socket.setsockopt(level, optname, None, optlen: int)
    socket.setsockopt(level, optname, value: buffer)
    socket.setsockopt(level, optname, value: int)
    socket.settimeout(value)
    socket.share(process_id)
    socket.shutdown(how)
    socket.SIO_RCVALLsocket.SIO_KEEPALIVE_VALSsocket.SIO_LOOPBACK_FAST_PATHRCVALL_*
    socket.SOCK_CLOEXECsocket.SOCK_NONBLOCK
    socket.SOCK_STREAMsocket.SOCK_DGRAMsocket.SOCK_RAWsocket.SOCK_RDMsocket.SOCK_SEQPACKET
    socket.socket(family=AF_INET, type=SOCK_STREAM, proto=0, fileno=None)
    socket.socket(socket.AF_CAN, socket.SOCK_DGRAM, socket.CAN_BCM)
    socket.socketpair([family[, type[, proto]]])
    socket.SocketType
    socket.type

## ==⚡ • ssl — TLS/SSL wrapper for socket objects
## ==⚡ • select — Waiting for I/O completion
## ==⚡ • selectors — High-level I/O multiplexing
## ==⚡ • asyncore — Asynchronous socket handler
## ==⚡ • asynchat — Asynchronous socket command/response handler
## ==⚡ • signal — Set handlers for asynchronous events
## ==⚡ • mmap — Memory-mapped file support



# =🚩 Internet Data Handling

This chapter describes modules which support handling data formats commonly used on the internet.

➡ `• email — An email and MIME handling package`
◦ email.message: Representing an email message
◦ email.parser: Parsing email messages
◾ FeedParser API
◾ Parser API
◾ Additional notes

◦ email.generator: Generating MIME documents
◦ email.policy: Policy Objects
◦ email.errors: Exception and Defect classes
◦ email.headerregistry: Custom Header Objects
◦ email.contentmanager: Managing MIME Content
◾ Content Manager Instances

◦ email: Examples
◦ email.message.Message: Representing an email message using the compat32 API
◦ email.mime: Creating email and MIME objects from scratch
◦ email.header: Internationalized headers
◦ email.charset: Representing character sets
◦ email.encoders: Encoders
◦ email.utils: Miscellaneous utilities
◦ email.iterators: Iterators

➡ `• json — JSON encoder and decoder`
◦ Basic Usage
◦ Encoders and Decoders
◦ Exceptions
◦ Standard Compliance and Interoperability
◾ Character Encodings
◾ Infinite and NaN Number Values
◾ Repeated Names Within an Object
◾ Top-level Non-Object, Non-Array Values
◾ Implementation Limitations

◦ Command Line Interface
◾ Command line options


➡ `• mailcap — Mailcap file handling`
➡ `• mailbox — Manipulate mailboxes in various formats`
◦ Mailbox objects
◾ Maildir
◾ mbox
◾ MH
◾ Babyl
◾ MMDF

◦ Message objects
◾ MaildirMessage
◾ mboxMessage
◾ MHMessage
◾ BabylMessage
◾ MMDFMessage

◦ Exceptions
◦ Examples

➡ `• mimetypes — Map filenames to MIME types`
◦ MimeTypes Objects

➡ `• base64 — Base16, Base32, Base64, Base85 Data Encodings`
◦ Security Considerations

➡ `• binhex — Encode and decode binhex4 files`
◦ Notes

➡ `• binascii — Convert between binary and ASCII`
➡ `• quopri — Encode and decode MIME quoted-printable data`
➡ `• uu — Encode and decode uuencode files`

## ==⚡ • email — An email and MIME handling package

## ==⚡ • json — JSON encoder and decoder

## ==⚡ • mailcap — Mailcap file handling

## ==⚡ • mailbox — Manipulate mailboxes in various formats

## ==⚡ • mimetypes — Map filenames to MIME types

## ==⚡ • base64 — Base16, Base32, Base64, Base85 Data Encodings

## ==⚡ • binhex — Encode and decode binhex4 files

## ==⚡ • binascii — Convert between binary and ASCII

## ==⚡ • quopri — Encode and decode MIME quoted-printable data

## ==⚡ • uu — Encode and decode uuencode files


# =🚩 Structured Markup Processing Tools
- The Python Standard Library » Structured Markup Processing Tools
 
Python supports a variety of modules to work with various forms of structured data markup. This includes modules to work with the Standard Generalized Markup Language (SGML) and the Hypertext Markup Language (HTML), and several interfaces for working with the Extensible Markup Language (XML).

➡ `• html — HyperText Markup Language support`
➡ `• html.parser — Simple HTML and XHTML parser`
◦ Example HTML Parser Application
◦ HTMLParser Methods
◦ Examples

➡ `• html.entities — Definitions of HTML general entities`
➡ `• XML Processing Modules`
◦ XML vulnerabilities
◦ The defusedxml Package

➡ `• xml.etree.ElementTree — The ElementTree XML API`
◦ Tutorial
◾ XML tree and elements
◾ Parsing XML
◾ Pull API for non-blocking parsing
◾ Finding interesting elements
◾ Modifying an XML File
◾ Building XML documents
◾ Parsing XML with Namespaces
◾ Additional resources

◦ XPath support
◾ Example
◾ Supported XPath syntax

◦ Reference
◾ Functions

◦ XInclude support
◾ Example

◦ Reference
◾ Functions
◾ Element Objects
◾ ElementTree Objects
◾ QName Objects
◾ TreeBuilder Objects
◾ XMLParser Objects
◾ XMLPullParser Objects
◾ Exceptions


➡ `• xml.dom — The Document Object Model API`
◦ Module Contents
◦ Objects in the DOM
◾ DOMImplementation Objects
◾ Node Objects
◾ NodeList Objects
◾ DocumentType Objects
◾ Document Objects
◾ Element Objects
◾ Attr Objects
◾ NamedNodeMap Objects
◾ Comment Objects
◾ Text and CDATASection Objects
◾ ProcessingInstruction Objects
◾ Exceptions

◦ Conformance
◾ Type Mapping
◾ Accessor Methods


➡ `• xml.dom.minidom — Minimal DOM implementation`
◦ DOM Objects
◦ DOM Example
◦ minidom and the DOM standard

➡ `• xml.dom.pulldom — Support for building partial DOM trees`
◦ DOMEventStream Objects

➡ `• xml.sax — Support for SAX2 parsers`
◦ SAXException Objects

➡ `• xml.sax.handler — Base classes for SAX handlers`
◦ ContentHandler Objects
◦ DTDHandler Objects
◦ EntityResolver Objects
◦ ErrorHandler Objects
◦ LexicalHandler Objects

➡ `• xml.sax.saxutils — SAX Utilities`
➡ `• xml.sax.xmlreader — Interface for XML parsers`
◦ XMLReader Objects
◦ IncrementalParser Objects
◦ Locator Objects
◦ InputSource Objects
◦ The Attributes Interface
◦ The AttributesNS Interface

➡ `• xml.parsers.expat — Fast XML parsing using Expat`
◦ XMLParser Objects
◦ ExpatError Exceptions
◦ Example
◦ Content Model Descriptions
◦ Expat error constants


## ==⚡ • html — HyperText Markup Language support

Source code: Lib/html/__init__.py


This module defines utilities to manipulate HTML.
➡ `html.escape(s, quote=True)`
Convert the characters &, < and > in string s to HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML. If the optional flag quote is true, the characters (") and (') are also translated; this helps for inclusion in an HTML attribute value delimited by quotes, as in <a href="...">.


New in version 3.2.
➡ `html.unescape(s)`
Convert all named and numeric character references (e.g. &gt;, &#62;, &#x3e;) in the string s to the corresponding Unicode characters. This function uses the rules defined by the HTML 5 standard for both valid and invalid character references, and the list of HTML 5 named character references.

New in version 3.4.

Submodules in the html package are:

➡ **html.parser** – HTML/XHTML parser with lenient parsing mode
➡ **html.entities** – HTML entity definitions


## ==⚡ • html.parser — Simple HTML and XHTML parser

Source code: Lib/html/parser.py

This module defines a class HTMLParser which serves as the basis for parsing text files formatted in HTML (HyperText Mark-up Language) and XHTML.
➡ `class html.parser.HTMLParser(*, convert_charrefs=True)`
Create a parser instance able to parse invalid markup.

If convert_charrefs is True (the default), all character references (except the ones in script/style elements) are automatically converted to the corresponding Unicode characters.

An HTMLParser instance is fed HTML data and calls handler methods when start tags, end tags, text, comments, and other markup elements are encountered. The user should subclass HTMLParser and override its methods to implement the desired behavior.

This parser does not check that end tags match start tags or call the end-tag handler for elements which are closed implicitly by closing an outer element.


Changed in version 3.4: convert_charrefs keyword argument added.


Changed in version 3.5: The default value for argument convert_charrefs is now True.

Example HTML Parser Application

As a basic example, below is a simple HTML parser that uses the HTMLParser class to print out start tags, end tags, and data as they are encountered:


from html.parser import HTMLParser

```py
class MyHTMLParser(HTMLParser):
    def handle_starttag(self, tag, attrs):
        print("Encountered a start tag:", tag)

    def handle_endtag(self, tag):
        print("Encountered an end tag :", tag)

    def handle_data(self, data):
        print("Encountered some data  :", data)

parser = MyHTMLParser()
parser.feed('<html><head><title>Test</title></head>'
            '<body><h1>Parse me!</h1></body></html>')
```

The output will then be:

    Encountered a start tag: html
    Encountered a start tag: head
    Encountered a start tag: title
    Encountered some data  : Test
    Encountered an end tag : title
    Encountered an end tag : head
    Encountered a start tag: body
    Encountered a start tag: h1
    Encountered some data  : Parse me!
    Encountered an end tag : h1
    Encountered an end tag : body
    Encountered an end tag : html


## ==⚡ • html.entities — Definitions of HTML general entities
Source code: Lib/html/entities.py


This module defines four dictionaries, html5, name2codepoint, codepoint2name, and entitydefs.
➡ `html.entities.html5`
A dictionary that maps HTML5 named character references to the equivalent Unicode character(s), e.g. html5['gt;'] == '>'. Note that the trailing semicolon is included in the name (e.g. 'gt;'), however some of the names are accepted by the standard even without the semicolon: in this case the name is present with and without the ';'. See also html.unescape().


New in version 3.3.
➡ `html.entities.entitydefs`
A dictionary mapping XHTML 1.0 entity definitions to their replacement text in ISO Latin-1.
➡ `html.entities.name2codepoint`
A dictionary that maps HTML entity names to the Unicode code points.
➡ `html.entities.codepoint2name`
A dictionary that maps Unicode code points to HTML entity names.



## ==⚡ • XML Processing Modules
◦  XML vulnerabilities
◦ The defusedxml Package

## ==⚡ • xml.etree.ElementTree — The ElementTree XML API
◦  Tutorial
◾XML tree and elements
◾Parsing XML
◾Pull API for non-blocking parsing
◾Finding interesting elements
◾Modifying an XML File
◾Building XML documents
◾Parsing XML with Namespaces
◾Additional resources

◦ XPath support
◾Example
◾Supported XPath syntax

◦ Reference
◾Functions

◦ XInclude support
◾Example

◦ Reference
◾Functions
◾Element Objects
◾ElementTree Objects
◾QName Objects
◾TreeBuilder Objects
◾XMLParser Objects
◾XMLPullParser Objects
◾Exceptions


## ==⚡ • xml.dom — The Document Object Model API
◦  Module Contents
◦ Objects in the DOM
◾DOMImplementation Objects
◾Node Objects
◾NodeList Objects
◾DocumentType Objects
◾Document Objects
◾Element Objects
◾Attr Objects
◾NamedNodeMap Objects
◾Comment Objects
◾Text and CDATASection Objects
◾ProcessingInstruction Objects
◾Exceptions

◦ Conformance
◾Type Mapping
◾Accessor Methods


## ==⚡ • xml.dom.minidom — Minimal DOM implementation
◦  DOM Objects
◦ DOM Example
◦ minidom and the DOM standard

## ==⚡ • xml.dom.pulldom — Support for building partial DOM trees
◦  DOMEventStream Objects

## ==⚡ • xml.sax — Support for SAX2 parsers
◦  SAXException Objects

## ==⚡ • xml.sax.handler — Base classes for SAX handlers
◦  ContentHandler Objects
◦ DTDHandler Objects
◦ EntityResolver Objects
◦ ErrorHandler Objects

## ==⚡ • xml.sax.saxutils — SAX Utilities
## ==⚡ • xml.sax.xmlreader — Interface for XML parsers
◦  XMLReader Objects
◦ IncrementalParser Objects
◦ Locator Objects
◦ InputSource Objects
◦ The Attributes Interface
◦ The AttributesNS Interface

## ==⚡ • xml.parsers.expat — Fast XML parsing using Expat
◦  XMLParser Objects
◦ ExpatError Exceptions
◦ Example
◦ Content Model Descriptions
◦ Expat error constants



# =🚩 Internet Protocols and Support
- https://docs.python.org/3.9/library/internet.html

- `webbrowser` — Convenient Web-browser controller
- `cgi` — Common Gateway Interface support
- `cgitb` — Traceback manager for CGI scripts
- `wsgiref` — WSGI Utilities and Reference Implementation
- `urllib` — URL handling modules
- `urllib.request` — Extensible library for opening URLs
- `urllib.response` — Response classes used by urllib
- `urllib.parse` — Parse URLs into components
- `urllib.error` — Exception classes raised by urllib.request
- `urllib.robotparser` — Parser for robots.txt
- `http` — HTTP modules
- `http.client` — HTTP protocol client
- `ftplib` — FTP protocol client
- `poplib` — POP3 protocol client
- `imaplib` — IMAP4 protocol client
- `nntplib` — NNTP protocol client
- `smtplib` — SMTP protocol client
- `smtpd` — SMTP Server
- `telnetlib` — Telnet client
- `uuid` — UUID objects according to RFC 4122
- `socketserver` — A framework for network servers
- `http.server` — HTTP servers
- `http.cookies` — HTTP state management
- `http.cookiejar` — Cookie handling for HTTP clients
- `xmlrpc` — XMLRPC server and client modules
- `xmlrpc.client` — XML-RPC client access
- `xmlrpc.server` — Basic XML-RPC servers
- `ipaddress` — IPv4/IPv6 manipulation library


The modules described in this chapter implement internet protocols and support for related technology. They are all implemented in Python. Most of these modules require the presence of the system-dependent module socket, which is currently supported on most popular platforms. Here is an overview:

➡ `• webbrowser — Convenient web-browser controller`
◦ Browser Controller Objects

➡ `• cgi — Common Gateway Interface support`
◦ Introduction
◦ Using the cgi module
◦ Higher Level Interface
◦ Functions
◦ Caring about security
◦ Installing your CGI script on a Unix system
◦ Testing your CGI script
◦ Debugging CGI scripts
◦ Common problems and solutions

➡ `• cgitb — Traceback manager for CGI scripts`
➡ `• wsgiref — WSGI Utilities and Reference Implementation`
◦ wsgiref.util – WSGI environment utilities
◦ wsgiref.headers – WSGI response header tools
◦ wsgiref.simple_server – a simple WSGI HTTP server
◦ wsgiref.validate — WSGI conformance checker
◦ wsgiref.handlers – server/gateway base classes
◦ Examples

➡ `• urllib — URL handling modules`
➡ `• urllib.request — Extensible library for opening URLs`
◦ Request Objects
◦ OpenerDirector Objects
◦ BaseHandler Objects
◦ HTTPRedirectHandler Objects
◦ HTTPCookieProcessor Objects
◦ ProxyHandler Objects
◦ HTTPPasswordMgr Objects
◦ HTTPPasswordMgrWithPriorAuth Objects
◦ AbstractBasicAuthHandler Objects
◦ HTTPBasicAuthHandler Objects
◦ ProxyBasicAuthHandler Objects
◦ AbstractDigestAuthHandler Objects
◦ HTTPDigestAuthHandler Objects
◦ ProxyDigestAuthHandler Objects
◦ HTTPHandler Objects
◦ HTTPSHandler Objects
◦ FileHandler Objects
◦ DataHandler Objects
◦ FTPHandler Objects
◦ CacheFTPHandler Objects
◦ UnknownHandler Objects
◦ HTTPErrorProcessor Objects
◦ Examples
◦ Legacy interface
◦ urllib.request Restrictions

➡ `• urllib.response — Response classes used by urllib`
➡ `• urllib.parse — Parse URLs into components`
◦ URL Parsing
◦ Parsing ASCII Encoded Bytes
◦ Structured Parse Results
◦ URL Quoting

➡ `• urllib.error — Exception classes raised by urllib.request`
➡ `• urllib.robotparser — Parser for robots.txt`
➡ `• http — HTTP modules`
◦ HTTP status codes

➡ `• http.client — HTTP protocol client`
◦ HTTPConnection Objects
◦ HTTPResponse Objects
◦ Examples
◦ HTTPMessage Objects

➡ `• ftplib — FTP protocol client`
◦ FTP Objects
◦ FTP_TLS Objects

➡ `• poplib — POP3 protocol client`
◦ POP3 Objects
◦ POP3 Example

➡ `• imaplib — IMAP4 protocol client`
◦ IMAP4 Objects
◦ IMAP4 Example

➡ `• nntplib — NNTP protocol client`
◦ NNTP Objects
◾ Attributes
◾ Methods

◦ Utility functions

➡ `• smtplib — SMTP protocol client`
◦ SMTP Objects
◦ SMTP Example

➡ `• smtpd — SMTP Server`
◦ SMTPServer Objects
◦ DebuggingServer Objects
◦ PureProxy Objects
◦ MailmanProxy Objects
◦ SMTPChannel Objects

➡ `• telnetlib — Telnet client`
◦ Telnet Objects
◦ Telnet Example

➡ `• uuid — UUID objects according to RFC 4122`
◦ Example

➡ `• socketserver — A framework for network servers`
◦ Server Creation Notes
◦ Server Objects
◦ Request Handler Objects
◦ Examples
◾ socketserver.TCPServer Example
◾ socketserver.UDPServer Example
◾ Asynchronous Mixins


➡ `• http.server — HTTP servers`
➡ `• http.cookies — HTTP state management`
◦ Cookie Objects
◦ Morsel Objects
◦ Example

➡ `• http.cookiejar — Cookie handling for HTTP clients`
◦ CookieJar and FileCookieJar Objects
◦ FileCookieJar subclasses and co-operation with web browsers
◦ CookiePolicy Objects
◦ DefaultCookiePolicy Objects
◦ Cookie Objects
◦ Examples

➡ `• xmlrpc — XMLRPC server and client modules`
➡ `• xmlrpc.client — XML-RPC client access`
◦ ServerProxy Objects
◦ DateTime Objects
◦ Binary Objects
◦ Fault Objects
◦ ProtocolError Objects
◦ MultiCall Objects
◦ Convenience Functions
◦ Example of Client Usage
◦ Example of Client and Server Usage

➡ `• xmlrpc.server — Basic XML-RPC servers`
◦ SimpleXMLRPCServer Objects
◾ SimpleXMLRPCServer Example

◦ CGIXMLRPCRequestHandler
◦ Documenting XMLRPC server
◦ DocXMLRPCServer Objects
◦ DocCGIXMLRPCRequestHandler

➡ `• ipaddress — IPv4/IPv6 manipulation library`
◦ Convenience factory functions
◦ IP Addresses
◾ Address objects
◾ Conversion to Strings and Integers
◾ Operators
◾ Comparison operators
◾ Arithmetic operators


◦ IP Network definitions
◾ Prefix, net mask and host mask
◾ Network objects
◾ Operators
◾ Logical operators
◾ Iteration
◾ Networks as containers of addresses


◦ Interface objects
◾ Operators
◾ Logical operators


◦ Other Module Level Functions
◦ Custom Exceptions

## ==⚡ • webbrowser — Convenient web-browser controller
## ==⚡ • cgi — Common Gateway Interface support
## ==⚡ • cgitb — Traceback manager for CGI scripts
## ==⚡ • wsgiref — WSGI Utilities and Reference Implementation
## ==⚡ • urllib — URL handling modules
https://docs.python.org/3/library/urllib.html

在 Python 2 有 urllib 和 urlib2 两个库来实现网络客户端请求 request 的发送。而在 Python 3 中，统一为 urllib。

urllib中包括了四个模块，包括:

- urllib.request 可以用来发送 request 和获取 request 的结果
- urllib.error 包含了 urllib.request 产生的异常
- urllib.parse 用来解析和处理URL
- urllib.robotparse 用来解析页面的 robots.txt 文件


### ===🗝 urlopen() 示例

    import urllib.request
    response = urllib.request.urlopen('http://python.org/')
    html = response.read()

### ===🗝 Request() 示例

    import urllib.request
    req = urllib.request.Request('http://python.org/')
    response = urllib.request.urlopen(req)
    the_page = response.read()

### ===🗝 POST 发送数据

    #! /usr/bin/env python3
    import urllib.parse
    import urllib.request
    url = 'http://localhost/login.php'
    user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
    values = {
    'act' : 'login',
    'login[email]' : 'yzhang@i9i8.com',
    'login[password]' : '123456'
    }
    data = urllib.parse.urlencode(values)
    req = urllib.request.Request(url, data)
    req.add_header('Referer', 'http://www.python.org/')
    req.add_header('User-Agent', user_agent)
    response = urllib.request.urlopen(req)
    the_page = response.read()
    print(the_page.decode("utf8"))

### ===🗝 发送数据和 header

    #! /usr/bin/env python3
    import urllib.parse
    import urllib.request
    url = 'http://localhost/login.php'
    user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
    values = {
    'act' : 'login',
    'login[email]' : 'yzhang@i9i8.com',
    'login[password]' : '123456'
    }
    headers = { 'User-Agent' : user_agent }
    data = urllib.parse.urlencode(values)
    req = urllib.request.Request(url, data, headers)
    response = urllib.request.urlopen(req)
    the_page = response.read()
    print(the_page.decode("utf8"))
    
### ===🗝 URLError HTTPError 异常处理1

    #! /usr/bin/env python3
    from urllib.request import Request, urlopen
    from urllib.error import URLError, HTTPError
    req = Request("//www.jb51.net /")
    try:
        response = urlopen(req)
    except HTTPError as e:
        print('The server couldn't fulfill the request.')
        print('Error code: ', e.code)
    except URLError as e:
        print('We failed to reach a server.')
        print('Reason: ', e.reason)
    else:
        print("good!")
        print(response.read().decode("utf8"))


### ===🗝 URLError HTTPError 异常处理2

    #! /usr/bin/env python3
    from urllib.request import Request, urlopen
    from urllib.error import URLError
    req = Request("//www.jb51.net /")
    try:
        response = urlopen(req)
    except URLError as e:
        if hasattr(e, 'reason'):
            print('We failed to reach a server.')
            print('Reason: ', e.reason)
        elif hasattr(e, 'code'):
            print('The server couldn't fulfill the request.')
            print('Error code: ', e.code)
    else:
        print("good!")
        print(response.read().decode("utf8"))


### ===🗝 HTTP Auth 认证

    #! /usr/bin/env python3
    import urllib.request
    # create a password manager
    password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm()
    # Add the username and password.
    # If we knew the realm, we could use it instead of None.
    top_level_url = "https://www.jb51.net /"
    password_mgr.add_password(None, top_level_url, 'rekfan', 'xxxxxx')
    handler = urllib.request.HTTPBasicAuthHandler(password_mgr)
    # create "opener" (OpenerDirector instance)
    opener = urllib.request.build_opener(handler)
    # use the opener to fetch a URL
    a_url = "https://www.jb51.net /"
    x = opener.open(a_url)
    print(x.read())
    # Install the opener.
    # Now all calls to urllib.request.urlopen use our opener.
    urllib.request.install_opener(opener)
    a = urllib.request.urlopen(a_url).read().decode('utf8')
    print(a)

### ===🗝 ProxyHandler 使用代理

    #! /usr/bin/env python3
    import urllib.request
    proxy_support = urllib.request.ProxyHandler({'sock5': 'localhost:1080'})
    opener = urllib.request.build_opener(proxy_support)
    urllib.request.install_opener(opener)
     
    a = urllib.request.urlopen("//www.jb51.net ").read().decode("utf8")
    print(a)


### ===🗝 timeout 超时

    #! /usr/bin/env python3
    import socket
    import urllib.request
    # timeout in seconds
    timeout = 2
    socket.setdefaulttimeout(timeout)
    # this call to urllib.request.urlopen now uses the default timeout
    # we have set in the socket module
    req = urllib.request.Request('//www.jb51.net/')
    a = urllib.request.urlopen(req).read()
    print(a)


### ===🗝 urllib.request.urlopen

输出结果它是一个 http.client.HTTPResponse 类型的对象，type(response) 可以获取类型信息。


利用以上最基本的 urlopen() 方法，我们可以完成最基本的简单网页的 GET 请求抓取，根据 urlopen() 函数的 API 定义可以传入需要的参数：

    urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)

data 参数是可选的，如果要添加 data ，它要是字节流编码格式的内容，即 bytes 类型，通过 bytes() 函数可以进行转化，另外如果你传递了这个 data 参数，它的请求方式就不再是 GET 方式请求，而是 POST。

timeout 参数可以设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间还没有得到响应，就会抛出异常，如果不指定，就会使用全局默认时间。它支持 HTTP 、 HTTPS 、 FTP 请求。

在这里我们设置了超时时间是 1 秒，程序 1 秒过后服务器依然没有响应，就抛出了 urllib.error.URLError 异常，错误原因是 timed out 。

还有 context 参数，它必须是 ssl.SSLContext 类型，用来指定 SSL 设置。cafile 和 capath 两个参数是指定CA证书和它的路径，这个在请求 HTTPS 链接时会有用。

cadefault 参数现在已经弃用了，默认为 False。


### ===🗝 urllib.request.Request

利用 urlopen() 方法可以实现最基本的请求发起，但这几个简单的参数并不足以构建一个完整的请求，如果请求中需要加入 headers 等信息，我们就可以利用更强大的 Request 类来构建一个请求。然后再用 urlopen() 方法来发送这个请求，只不过这次 urlopen() 方法的参数不再是一个 URL，而是一个 Request 对象，通过构造这个这个数据结构，一方面我们可以将请求独立成一个对象，另一方面可配置参数更加丰富和灵活。

看一下 Request 它的构造方法如下。

    class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)

第一个参数是请求链接，这个是必传参数，其他的都是可选参数。

data 参数如果要传必须传 bytes （字节流）类型的，如果是一个字典，可以先用 urllib.parse.urlencode() 编码。

headers 参数是一个字典，你可以在构造 Request 时通过 headers 参数传递，也可以通过调用 Request 对象的 add_header() 方法来添加请求头。请求头最常用的用法就是通过修改 User-Agent 来伪装浏览器，默认的 User-Agent 是 Python-urllib ，你可以通过修改它来伪装浏览器，比如要伪装火狐浏览器，你可以把它设置为 

    Mozilla/5.0 (X11; U; Linux i686)Gecko/20071127 Firefox/2.0.0.11

origin_req_host 指的是请求方的 host 名称或者 IP 地址。

unverifiable 指的是这个请求是否是无法验证的，默认是 False 。意思就是说用户没有足够权限来选择接收这个请求的结果。例如我们请求一个HTML文档中的图片，但是我们没有自动抓取图像的权限，这时 unverifiable 的值就是 True 。

method 是一个字符串，它用来指示请求使用的方法，比如 GET ， POST ， PUT 等等。

### ===🗝 http.client.HTTPResponse
https://docs.python.org/3/library/urllib.request.html#module-urllib.response
https://docs.python.org/3/library/http.client.html#http.client.HTTPResponse

主要包含的方法有 read() 、 readinto() 、getheader(name) 、 getheaders() 、 fileno() 等函数和 msg 、 version 、 status 、 reason 、 debuglevel 、 closed 等属性。 得到这个对象之后，赋值为 response ，然后就可以用 response 调用这些方法和属性，得到返回结果的一系列信息。例如 response.read() 就可以得到返回的网页内容， response.status 就可以得到返回结果的状态码，如200代表请求成功，404代表网页未找到等。

    dir(response)

    [
        '__IOBase_closed', 
        ..., 
        'begin', 'chunk_left', 'chunked', 'close', 'closed', 'code', 'debuglevel', 'detach', 'fileno', 'flush', 'fp',
        'getcode', 'getheader', 'getheaders', 'geturl', 'headers', 'info', 'isatty', 'isclosed', 'length', 'msg', 'peek',
        'read', 'read1', 'readable', 'readinto', 'readinto1', 'readline', 'readlines', 'reason', 'seek', 'seekable', 'status',
        'tell', 'truncate', 'url', 'version', 'will_close', 'writable', 'write', 'writelines'
    ]


属性
- r.status_code HTTP 请求的返回状态，200表示连接成功，404表示失败 
- r.text HTTP响应内容的字符串形式，即url对应的页面内容 
- r.encoding 从HTTP header中猜测的响应内容编码方式( 
- r.apparent_encoding 从内容中分析出的响应内容编码方式（备选编码方式） 
- r.content HTTP响应内容的二进制形式

get方法获取网上资源的基本流程：用r.status_code检查返回状态 , 
情况1：如果是200，可以用r.text、r.encoding、r.apparent_encoding、r.content解析返回的内容； 
情况2：返回的是404或者其他，则访问不成功

r.encoding 和 r.apparent_encoding的区别 

- r.encoding 如果header中不存在charset，则认为编码为ISO-8859-1 
- r.apparent_encoding:根据网页内容分析出的编码方式 

综上所述，r.apparent_encoding 比 r.encoding 更为准确

**r.raise_for_status()**Requests库用于检验异常

    isZip = res.headers.get('Content-Encoding')=="gzip"

    gzip.decompress(content).decode("utf-8") 


### ===🗝 Handler 
https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler

urllib.request 高级特性，比如 Cookies 处理，就需要更强大的工具 Handler 登场了。

简而言之你可以把它理解为各种处理器，有专门处理登录验证的，有处理 Cookies 的，有处理代理设置的，利用它们我们几乎可以做到任何 HTTP 请求中所有的事情。

首先介绍下 urllib.request.BaseHandler ，它是所有其他 Handler 的父类，它提供了最基本的 Handler 的方法，例如 default_open() 、 protocol_request() 等。

接下来就有各种 Handler 类继承这个 BaseHandler ，列举如下：

- HTTPDefaultErrorHandler 用于处理HTTP响应错误，错误都会抛出 HTTPError 类型的异常。
- HTTPRedirectHandler 用于处理重定向。
- HTTPCookieProcessor 用于处理 Cookie 。
- ProxyHandler 用于设置代理，默认代理为空。
- HTTPPasswordMgr 用于管理密码，它维护了用户名密码的表。
- HTTPBasicAuthHandler 用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。

另外一个比较重要的就是 OpenerDirector ，可以称之为 Opener， urllib.request.urlopen() 这个方法它就是一个 Opener。

Request 、 urlopen() 相当于类库为你封装好了极其常用的请求方法，利用它们两个我们就可以完成基本的请求，需要实现更高级的功能，就要更底层的 API，在这里我们就用到了比 urlopen() 更底层的对象，也就是 Opener 。

Opener 可以使用 open() 方法，返回的类型和 urlopen() 如出一辙。它就是利用 Handler 来构建 Opener。

### HTTPBasicAuthHandler 认证实例

    import urllib.request
    auth_handler = urllib.request.HTTPBasicAuthHandler()
    auth_handler.add_password(realm='PDQ Application',
                              uri='https://mahler:8092/site-updates.py',
                              user='klem',
                              passwd='kadidd!ehopper')
    opener = urllib.request.build_opener(auth_handler)
    urllib.request.install_opener(opener)
    urllib.request.urlopen('http://www.example.com/login.html')

此处代码为实例代码，用于说明 Handler 和 Opener 的使用方法。在这里，首先实例化了一个 HTTPBasicAuthHandler 对象，然后利用 add_password() 添加进去用户名和密码，相当于建立了一个处理认证的处理器。

接下来利用 urllib.request.build_opener() 方法来利用这个处理器构建一个 Opener ，那么这个 Opener 在发送请求的时候就具备了认证功能了。接下来利用 Opener 的 open() 方法打开链接，就可以完成认证了。
    
### ProxyHandler 代理实例

    import urllib.request
    proxy_handler = urllib.request.ProxyHandler({
        'http': 'http://218.202.111.10:80',
        'https': 'https://180.250.163.34:8888'
    })
    opener = urllib.request.build_opener(proxy_handler)
    response = opener.open('https://www.baidu.com')
    print(response.read())

在这里使用了 ProxyHandler ， ProxyHandler 的参数是一个字典，key是协议类型，比如 http 还是 https 等，value是代理链接，可以添加多个代理。

然后利用 build_opener() 方法利用这个 Handler 构造一个 Opener ，然后发送请求即可。



### HTTPCookieProcessor Cookie 设置

我们先用一个实例来感受一下怎样将网站的 Cookie 获取下来。

    import http.cookiejar, urllib.request
    cookie = http.cookiejar.CookieJar()
    handler = urllib.request.HTTPCookieProcessor(cookie)
    opener = urllib.request.build_opener(handler)
    response = opener.open('http://www.baidu.com')
    for item in cookie:
       print(item.name+"="+item.value)

首先我们必须声明一个 CookieJar 对象，接下来我们就需要利用 HTTPCookieProcessor 来构建一个 handler ，最后利用 build_opener 方法构建出 opener ，执行 open() 即可。

    BAIDUID=2E65A683F8A8BA3DF521469DF8EFF1E1:FG=1
    BIDUPSID=2E65A683F8A8BA3DF521469DF8EFF1E1
    H_PS_PSSID=20987_1421_18282_17949_21122_17001_21227_21189_21161_20927
    PSTM=1474900615
    BDSVRTM=0
    BD_HOME=0

可以看到输出了每一条 Cookie 的名称还有值，还可以从文件中读写 Cookie。

    filename = 'cookie.txt'
    cookie = http.cookiejar.MozillaCookieJar(filename)
    handler = urllib.request.HTTPCookieProcessor(cookie)
    opener = urllib.request.build_opener(handler)
    response = opener.open('http://www.baidu.com')
    cookie.save(ignore_discard=True, ignore_expires=True)

这时的 CookieJar 就需要换成 MozillaCookieJar ，生成文件时需要用到它，它是 CookieJar 的子类，可以用来处理 Cookie 和文件相关的事件，读取和保存 Cookie ，它可以将 Cookie 保存成 Mozilla 型的格式。

运行之后可以发现生成了一个 cookie.txt 文件。

    # Netscape HTTP Cookie File
    # http://curl.haxx.se/rfc/cookie_spec.html
    # This is a generated file! Do not edit.
    .baidu.com TRUE / FALSE 3622386254 BAIDUID 05A
    E39B5F56C1DEC474325CDA522D44F:FG=1
    .baidu.com TRUE / FALSE 3622386254 BIDUPSID 05
    AE39B5F56C1DEC474325CDA522D44F
    .baidu.com TRUE / FALSE H_PS_PSSID 19638_1453
    _17710_18240_21091_18560_17001_21191_21161
    .baidu.com TRUE / FALSE 3622386254 PSTM 147490
    2606
    www.baidu.com FALSE / FALSE BDSVRTM 0
    www.baidu.com FALSE / FALSE BD_HOME 0

另外还有一个 LWPCookieJar ，同样可以读取和保存 Cookie ，但是保存的格式和 MozillaCookieJar 的不一样，它会保存成与libwww-perl的Set-Cookie3文件格式的 Cookie 。

那么在声明时就改为

    cookie = http.cookiejar.LWPCookieJar(filename)


    #LWP-Cookies-2.0
    Set-Cookie3: BAIDUID="0CE9C56F598E69DB375B7C294AE5C591:FG=1"; path="/"; domain=".baidu.com"; path_spec; domain_dot; expires="208
    4-10-14 18:25:19Z"; version=0
    Set-Cookie3: BIDUPSID=0CE9C56F598E69DB375B7C294AE5C591; path="/"; domain=".baidu.com"; path_spec; domain_dot; expires="2084-10-1
    4 18:25:19Z"; version=0
    Set-Cookie3: H_PS_PSSID=20048_1448_18240_17944_21089_21192_21161_20929; path="/"; domain=".baidu.com"; path_spec; domain_dot; di
    scard; version=0
    Set-Cookie3: PSTM=1474902671; path="/"; domain=".baidu.com"; path_spec; domain_dot; expires="2084-10-14 18:25:19Z"; version=0
    Set-Cookie3: BDSVRTM=0; path="/"; domain="www.baidu.com"; path_spec; discard; version=0
    Set-Cookie3: BD_HOME=0; path="/"; domain="www.baidu.com"; path_spec; discard; version=0

生成的内容如下：由此看来生成的格式还是有比较大的差异的。
那么生成了 Cookie 文件，怎样从文件读取并利用呢？
下面我们以 LWPCookieJar 格式为例来感受一下：

    cookie = http.cookiejar.LWPCookieJar()
    cookie.load('cookie.txt', ignore_discard=True, ignore_expires=True)
    handler = urllib.request.HTTPCookieProcessor(cookie)
    opener = urllib.request.build_opener(handler)
    response = opener.open('http://www.baidu.com')
    print(response.read().decode('utf-8'))

前提是我们首先利用上面的方式生成了 LWPCookieJar 格式的 Cookie ，然后利用 load() 方法，传入文件名称，后面同样的方法构建 handler 和 opener 即可。

## ==⚡ • urllib.request — Extensible library for opening URLs
## ==⚡ • urllib.response — Response classes used by urllib
## ==⚡ • urllib.parse — Parse URLs into components
## ==⚡ • urllib.error — Exception classes raised by urllib.request
## ==⚡ • urllib.robotparser — Parser for robots.txt
## ==⚡ • http — HTTP modules
## ==⚡ • http.client — HTTP protocol client
## ==⚡ • ftplib — FTP protocol client
## ==⚡ • poplib — POP3 protocol client
## ==⚡ • imaplib — IMAP4 protocol client
## ==⚡ • nntplib — NNTP protocol client
## ==⚡ • smtplib — SMTP protocol client
## ==⚡ • smtpd — SMTP Server
## ==⚡ • telnetlib — Telnet client
## ==⚡ • uuid — UUID objects according to RFC 4122
## ==⚡ • socketserver — A framework for network servers
## ==⚡ • http.server — HTTP servers
## ==⚡ • http.cookies — HTTP state management
## ==⚡ • http.cookiejar — Cookie handling for HTTP clients
## ==⚡ • xmlrpc — XMLRPC server and client modules
## ==⚡ • xmlrpc.client — XML-RPC client access
## ==⚡ • xmlrpc.server — Basic XML-RPC servers
## ==⚡ • ipaddress — IPv4/IPv6 manipulation library



# =🚩 Multimedia Services

The modules described in this chapter implement various algorithms or interfaces that are mainly useful for multimedia applications. They are available at the discretion of the installation. Here’s an overview:

➡ `• audioop — Manipulate raw audio data`
➡ `• aifc — Read and write AIFF and AIFC files`
➡ `• sunau — Read and write Sun AU files`
◦ AU_read Objects
◦ AU_write Objects

➡ `• wave — Read and write WAV files`
◦ Wave_read Objects
◦ Wave_write Objects

➡ `• chunk — Read IFF chunked data`
➡ `• colorsys — Conversions between color systems`
➡ `• imghdr — Determine the type of an image`
➡ `• sndhdr — Determine type of sound file`
➡ `• ossaudiodev — Access to OSS-compatible audio devices`
◦ Audio Device Objects
◦ Mixer Device Objects

## ==⚡ • audioop — Manipulate raw audio data

Help on built-in module audioop:

NAME

    audioop

CLASSES

    builtins.Exception(builtins.BaseException)
        error
    
    class error(builtins.Exception)
     |  Method resolution order:
     |      error
     |      builtins.Exception
     |      builtins.BaseException
     |      builtins.object
     |  
     |  Data descriptors defined here:
     |  
     |  __weakref__
     |      list of weak references to the object (if defined)
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from builtins.Exception:
     |  
     |  __init__(self, /, *args, **kwargs)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |  
     |  ----------------------------------------------------------------------
     |  Static methods inherited from builtins.Exception:
     |  
     |  __new__(*args, **kwargs) from builtins.type
     |      Create and return a new object.  See help(type) for accurate signature.
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from builtins.BaseException:
     |  
     |  __delattr__(self, name, /)
     |      Implement delattr(self, name).
     |  
     |  __getattribute__(self, name, /)
     |      Return getattr(self, name).
     |  
     |  __reduce__(...)
     |      Helper for pickle.
     |  
     |  __repr__(self, /)
     |      Return repr(self).
     |  
     |  __setattr__(self, name, value, /)
     |      Implement setattr(self, name, value).
     |  
     |  __setstate__(...)
     |  
     |  __str__(self, /)
     |      Return str(self).
     |  
     |  with_traceback(...)
     |      Exception.with_traceback(tb) --
     |      set self.__traceback__ to tb and return self.
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from builtins.BaseException:
     |  
     |  __cause__
     |      exception cause
     |  
     |  __context__
     |      exception context
     |  
     |  __dict__
     |  
     |  __suppress_context__
     |  
     |  __traceback__
     |  
     |  args

FUNCTIONS

    add(fragment1, fragment2, width, /)
        Return a fragment which is the addition of the two samples passed as parameters.
    
    adpcm2lin(fragment, width, state, /)
        Decode an Intel/DVI ADPCM coded fragment to a linear fragment.
    
    alaw2lin(fragment, width, /)
        Convert sound fragments in a-LAW encoding to linearly encoded sound fragments.
    
    avg(fragment, width, /)
        Return the average over all samples in the fragment.
    
    avgpp(fragment, width, /)
        Return the average peak-peak value over all samples in the fragment.
    
    bias(fragment, width, bias, /)
        Return a fragment that is the original fragment with a bias added to each sample.
    
    byteswap(fragment, width, /)
        Convert big-endian samples to little-endian and vice versa.
    
    cross(fragment, width, /)
        Return the number of zero crossings in the fragment passed as an argument.
    
    findfactor(fragment, reference, /)
        Return a factor F such that rms(add(fragment, mul(reference, -F))) is minimal.
    
    findfit(fragment, reference, /)
        Try to match reference as well as possible to a portion of fragment.
    
    findmax(fragment, length, /)
        Search fragment for a slice of specified number of samples with maximum energy.
    
    getsample(fragment, width, index, /)
        Return the value of sample index from the fragment.
    
    lin2adpcm(fragment, width, state, /)
        Convert samples to 4 bit Intel/DVI ADPCM encoding.
    
    lin2alaw(fragment, width, /)
        Convert samples in the audio fragment to a-LAW encoding.
    
    lin2lin(fragment, width, newwidth, /)
        Convert samples between 1-, 2-, 3- and 4-byte formats.
    
    lin2ulaw(fragment, width, /)
        Convert samples in the audio fragment to u-LAW encoding.
    
    max(fragment, width, /)
        Return the maximum of the absolute value of all samples in a fragment.
    
    maxpp(fragment, width, /)
        Return the maximum peak-peak value in the sound fragment.
    
    minmax(fragment, width, /)
        Return the minimum and maximum values of all samples in the sound fragment.
    
    mul(fragment, width, factor, /)
        Return a fragment that has all samples in the original fragment multiplied by the floating-point value factor.
    
    ratecv(fragment, width, nchannels, inrate, outrate, state, weightA=1, weightB=0, /)
        Convert the frame rate of the input fragment.
    
    reverse(fragment, width, /)
        Reverse the samples in a fragment and returns the modified fragment.
    
    rms(fragment, width, /)
        Return the root-mean-square of the fragment, i.e. sqrt(sum(S_i^2)/n).
    
    tomono(fragment, width, lfactor, rfactor, /)
        Convert a stereo fragment to a mono fragment.
    
    tostereo(fragment, width, lfactor, rfactor, /)
        Generate a stereo fragment from a mono fragment.
    
    ulaw2lin(fragment, width, /)
        Convert sound fragments in u-LAW encoding to linearly encoded sound fragments.

FILE
    (built-in)




The `audioop` module contains some useful operations on sound fragments. It operates on sound fragments consisting of signed integer samples 8, 16, 24 or 32 bits wide, stored in bytes-like objects. All scalar items are integers, unless specified otherwise.


Changed in version 3.4: Support for 24-bit samples was added. All functions now accept any bytes-like object. String input now results in an immediate error.

This module provides support for a-LAW, u-LAW and Intel/DVI ADPCM encodings.

A few of the more complicated operations only take 16-bit samples, otherwise the sample size (in bytes) is always a parameter of the operation.

The module defines the following variables and functions:

✅ `exception audioop.error`
This exception is raised on all errors, such as unknown number of bytes per sample, etc.

➡ `audioop.add(fragment1, fragment2, width)`
Return a fragment which is the addition of the two samples passed as parameters. width is the sample width in bytes, either 1, 2, 3 or 4. Both fragments should have the same length. Samples are truncated in case of overflow.

➡ `audioop.adpcm2lin(adpcmfragment, width, state)`
Decode an Intel/DVI ADPCM coded fragment to a linear fragment. See the description of lin2adpcm() for details on ADPCM coding. Return a tuple (sample, newstate) where the sample has the width specified in width.

➡ `audioop.alaw2lin(fragment, width)`
Convert sound fragments in a-LAW encoding to linearly encoded sound fragments. a-LAW encoding always uses 8 bits samples, so width refers only to the sample width of the output fragment here.

➡ `audioop.avg(fragment, width)`
Return the average over all samples in the fragment.

➡ `audioop.avgpp(fragment, width)`
Return the average peak-peak value over all samples in the fragment. No filtering is done, so the usefulness of this routine is questionable.

➡ `audioop.bias(fragment, width, bias)`
Return a fragment that is the original fragment with a bias added to each sample. Samples wrap around in case of overflow.

➡ `audioop.byteswap(fragment, width)`
“Byteswap” all samples in a fragment and returns the modified fragment. Converts big-endian samples to little-endian and vice versa.

New in version 3.4.


➡ `audioop.cross(fragment, width)`
Return the number of zero crossings in the fragment passed as an argument.

➡ `audioop.findfactor(fragment, reference)`
Return a factor F such that rms(add(fragment, mul(reference, -F))) is minimal, i.e., return the factor with which you should multiply reference to make it match as well as possible to fragment. The fragments should both contain 2-byte samples.

The time taken by this routine is proportional to len(fragment).

➡ `audioop.findfit(fragment, reference)`
Try to match reference as well as possible to a portion of fragment (which should be the longer fragment). This is (conceptually) done by taking slices out of fragment, using findfactor() to compute the best match, and minimizing the result. The fragments should both contain 2-byte samples. Return a tuple (offset, factor) where offset is the (integer) offset into fragment where the optimal match started and factor is the (floating-point) factor as per findfactor().

➡ `audioop.findmax(fragment, length)`
Search fragment for a slice of length length samples (not bytes!) with maximum energy, i.e., return i for which `rms(fragment[i*2:(i+length)*2])` is maximal. The fragments should both contain 2-byte samples.

The routine takes time proportional to len(fragment).

➡ `audioop.getsample(fragment, width, index)`
Return the value of sample index from the fragment.

➡ `audioop.lin2adpcm(fragment, width, state)`
Convert samples to 4 bit Intel/DVI ADPCM encoding. ADPCM coding is an adaptive coding scheme, whereby each 4 bit number is the difference between one sample and the next, divided by a (varying) step. The Intel/DVI ADPCM algorithm has been selected for use by the IMA, so it may well become a standard.

`state` is a tuple containing the state of the coder. The coder returns a tuple (adpcmfrag, newstate), and the newstate should be passed to the next call of lin2adpcm(). In the initial call, None can be passed as the state. adpcmfrag is the ADPCM coded fragment packed 2 4-bit values per byte.

➡ `audioop.lin2alaw(fragment, width)`
Convert samples in the audio fragment to a-LAW encoding and return this as a bytes object. a-LAW is an audio encoding format whereby you get a dynamic range of about 13 bits using only 8 bit samples. It is used by the Sun audio hardware, among others.

➡ `audioop.lin2lin(fragment, width, newwidth)`
Convert samples between 1-, 2-, 3- and 4-byte formats.

Note:
 In some audio formats, such as .WAV files, 16, 24 and 32 bit samples are signed, but 8 bit samples are unsigned. So when converting to 8 bit wide samples for these formats, you need to also add 128 to the result:
 


```py
new_frames = audioop.lin2lin(frames, old_width, 1)
new_frames = audioop.bias(new_frames, 1, 128)
```


The same, in reverse, has to be applied when converting from 8 to 16, 24 or 32 bit width samples.

➡ `audioop.lin2ulaw(fragment, width)`
Convert samples in the audio fragment to u-LAW encoding and return this as a bytes object. u-LAW is an audio encoding format whereby you get a dynamic range of about 14 bits using only 8 bit samples. It is used by the Sun audio hardware, among others.

➡ `audioop.max(fragment, width)`
Return the maximum of the absolute value of all samples in a fragment.

➡ `audioop.maxpp(fragment, width)`
Return the maximum peak-peak value in the sound fragment.

➡ `audioop.minmax(fragment, width)`
Return a tuple consisting of the minimum and maximum values of all samples in the sound fragment.

➡ `audioop.mul(fragment, width, factor)`
Return a fragment that has all samples in the original fragment multiplied by the floating-point value factor. Samples are truncated in case of overflow.

➡ `audioop.ratecv(fragment, width, nchannels, inrate, outrate, state[, weightA[, weightB]])`
Convert the frame rate of the input fragment.

state is a tuple containing the state of the converter. The converter returns a tuple (newfragment, newstate), and newstate should be passed to the next call of ratecv(). The initial call should pass None as the state.

The weightA and weightB arguments are parameters for a simple digital filter and default to 1 and 0 respectively.

➡ `audioop.reverse(fragment, width)`
Reverse the samples in a fragment and returns the modified fragment.

➡ `audioop.rms(fragment, width)`
Return the root-mean-square of the fragment, i.e. sqrt(sum(S_i^2)/n).

This is a measure of the power in an audio signal.

➡ `audioop.tomono(fragment, width, lfactor, rfactor)`
Convert a stereo fragment to a mono fragment. The left channel is multiplied by lfactor and the right channel by rfactor before adding the two channels to give a mono signal.

➡ `audioop.tostereo(fragment, width, lfactor, rfactor)`
Generate a stereo fragment from a mono fragment. Each pair of samples in the stereo fragment are computed from the mono sample, whereby left channel samples are multiplied by lfactor and right channel samples by rfactor.

➡ `audioop.ulaw2lin(fragment, width)`
Convert sound fragments in u-LAW encoding to linearly encoded sound fragments. u-LAW encoding always uses 8 bits samples, so width refers only to the sample width of the output fragment here.

Note that operations such as mul() or max() make no distinction between mono and stereo fragments, i.e. all samples are treated equal. If this is a problem the stereo fragment should be split into two mono fragments first and recombined later. Here is an example of how to do that:


```py
def mul_stereo(sample, width, lfactor, rfactor):

    lsample = audioop.tomono(sample, width, 1, 0)
    rsample = audioop.tomono(sample, width, 0, 1)
    lsample = audioop.mul(lsample, width, lfactor)
    rsample = audioop.mul(rsample, width, rfactor)
    lsample = audioop.tostereo(lsample, width, 1, 0)
    rsample = audioop.tostereo(rsample, width, 0, 1)
    return audioop.add(lsample, rsample, width)
```

If you use the ADPCM coder to build network packets and you want your protocol to be stateless (i.e. to be able to tolerate packet loss) you should not only transmit the data but also the state. Note that you should send the initial state (the one you passed to lin2adpcm()) along to the decoder, not the final state (as returned by the coder). If you want to use struct.Struct to store the state in binary you can code the first element (the predicted value) in 16 bits and the second (the delta index) in 8.

The ADPCM coders have never been tried against other ADPCM coders, only against themselves. It could well be that I misinterpreted the standards in which case they will not be interoperable with the respective standards.

The find*() routines might look a bit funny at first sight. They are primarily meant to do echo cancellation. A reasonably fast way to do this is to pick the most energetic piece of the output sample, locate that in the input sample and subtract the whole output sample from the input sample:


```py
def echocancel(outputdata, inputdata):
    pos = audioop.findmax(outputdata, 800)    # one tenth second
    out_test = outputdata[pos*2:]
    in_test = inputdata[pos*2:]

    ipos, factor = audioop.findfit(in_test, out_test)    # Optional (for better cancellation):
    # factor = audioop.findfactor(in_test[ipos*2:ipos*2+len(out_test)],

    #              out_test)    prefill = '\0'*(pos+ipos)*2

    postfill = '\0'*(len(inputdata)-len(prefill)-len(outputdata))    outputdata = prefill + audioop.mul(outputdata, 2, -factor) + postfill

    return audioop.add(inputdata, outputdata, 2)
```


## ==⚡ • aifc — Read and write AIFF and AIFC files

## ==⚡ • sunau — Read and write Sun AU files

## ==⚡ • wave — Read and write WAV files

Source code: Lib/wave.py

The wave module provides a convenient interface to the WAV sound format. It does not support compression/decompression, but it does support mono/stereo.

The wave module defines the following function and exception:

✅ `wave.open(file, mode=None)`
If `file` is a string, open the file by that name, otherwise treat it as a file-like object. mode can be:

➡ 'rb' Read only mode.
➡ 'wb' Write only mode.

Note that it does not allow read/write WAV files.

A mode of 'rb' returns a Wave_read object, while a mode of 'wb' returns a Wave_write object. If mode is omitted and a file-like object is passed as file, file.mode is used as the default value for mode.

If you pass in a file-like object, the wave object will not close it when its close() method is called; it is the caller’s responsibility to close the file object.

The open() function may be used in a with statement. When the with block completes, the `Wave_read.close()` or `Wave_write.close()` method is called.

Changed in version 3.4: Added support for unseekable files.

✅ `exception wave.Error`
An error raised when something is impossible because it violates the WAV specification or hits an implementation deficiency.


### ===🗝 Wave_read Objects

Wave_read objects, as returned by open(), have the following methods:

✅ `Wave_read.close()`
Close the stream if it was opened by wave, and make the instance unusable. This is called automatically on object collection.

✅ `Wave_read.getnchannels()`
Returns number of audio channels (1 for mono, 2 for stereo).

✅ `Wave_read.getsampwidth()`
Returns sample width in bytes.

✅ `Wave_read.getframerate()`
Returns sampling frequency.

✅ `Wave_read.getnframes()`
Returns number of audio frames.

✅ `Wave_read.getcomptype()`
Returns compression type ('NONE' is the only supported type).

✅ `Wave_read.getcompname()`
Human-readable version of getcomptype(). Usually 'not compressed' parallels 'NONE'.

✅ `Wave_read.getparams()`
Returns a namedtuple() (nchannels, sampwidth, framerate, nframes, comptype, compname), equivalent to output of the `get*()` methods.

✅ `Wave_read.readframes(n)`
Reads and returns at most n frames of audio, as a bytes object.

✅ `Wave_read.rewind()`
Rewind the file pointer to the beginning of the audio stream.

The following two methods are defined for compatibility with the aifc module, and don’t do anything interesting.

✅ `Wave_read.getmarkers()`
Returns None.

✅ `Wave_read.getmark(id)`
Raise an error.

The following two methods define a term “position” which is compatible between them, and is otherwise implementation dependent.

✅ `Wave_read.setpos(pos)`
Set the file pointer to the specified position.

✅ `Wave_read.tell()`
Return current file pointer position.


### ===🗝 Wave_write Objects

For seekable output streams, the wave header will automatically be updated to reflect the number of frames actually written. For unseekable streams, the nframes value must be accurate when the first frame data is written. An accurate nframes value can be achieved either by calling `setnframes()` or `setparams()` with the number of frames that will be written before close() is called and then using `writeframesraw()` to write the frame data, or by calling `writeframes()` with all of the frame data to be written. In the latter case `writeframes()` will calculate the number of frames in the data and set nframes accordingly before writing the frame data.

Wave_write objects, as returned by open(), have the following methods:

Changed in version 3.4: Added support for unseekable files.


✅ `Wave_write.close()`
Make sure nframes is correct, and close the file if it was opened by wave. This method is called upon object collection. It will raise an exception if the output stream is not seekable and nframes does not match the number of frames actually written.

✅ `Wave_write.setnchannels(n)`
Set the number of channels.

✅ `Wave_write.setsampwidth(n)`
Set the sample width to n bytes.

✅ `Wave_write.setframerate(n)`
Set the frame rate to n.

Changed in version 3.2: A non-integral input to this method is rounded to the nearest integer.


✅ `Wave_write.setnframes(n)`
Set the number of frames to n. This will be changed later if the number of frames actually written is different (this update attempt will raise an error if the output stream is not seekable).

✅ `Wave_write.setcomptype(type, name)`
Set the compression type and description. At the moment, only compression type NONE is supported, meaning no compression.

✅ `Wave_write.setparams(tuple)`
The tuple should be (nchannels, sampwidth, framerate, nframes, comptype, compname), with values valid for the set*() methods. Sets all parameters.

✅ `Wave_write.tell()`
Return current position in the file, with the same disclaimer for the Wave_read.tell() and Wave_read.setpos() methods.

✅ `Wave_write.writeframesraw(data)`
Write audio frames, without correcting nframes.

Changed in version 3.4: Any bytes-like object is now accepted.


✅ `Wave_write.writeframes(data)`
Write audio frames and make sure nframes is correct. It will raise an error if the output stream is not seekable and the total number of frames that have been written after data has been written does not match the previously set value for nframes.

Changed in version 3.4: Any bytes-like object is now accepted.


Note that it is invalid to set any parameters after calling writeframes() or writeframesraw(), and any attempt to do so will raise wave.Error.

## ==⚡ • chunk — Read IFF chunked data

## ==⚡ • colorsys — Conversions between color systems

## ==⚡ • imghdr — Determine the type of an image

Source code: Lib/imghdr.py

The imghdr module determines the type of image contained in a file or byte stream.

The imghdr module defines the following function:

➡ `imghdr.what(file, h=None)`
Tests the image data contained in the file named by file, and returns a string describing the image type. If optional h is provided, the file argument is ignored and h is assumed to contain the byte stream to test.


Changed in version 3.6: Accepts a path-like object.

The following image types are recognized, as listed below with the return value from what():


| Value  |            Image format           |
|--------|-----------------------------------|
| 'rgb'  | SGI ImgLib Files                  |
| 'gif'  | GIF 87a and 89a Files             |
| 'pbm'  | Portable Bitmap Files             |
| 'pgm'  | Portable Graymap Files            |
| 'ppm'  | Portable Pixmap Files             |
| 'tiff' | TIFF Files                        |
| 'rast' | Sun Raster Files                  |
| 'xbm'  | X Bitmap Files                    |
| 'jpeg' | JPEG data in JFIF or Exif formats |
| 'bmp'  | BMP files                         |
| 'png'  | Portable Network Graphics         |
| 'webp' | WebP files                        |
| 'exr'  | OpenEXR Files                     |


New in version 3.5: The exr and webp formats were added.

You can extend the list of file types imghdr can recognize by appending to this variable:

➡ `imghdr.tests`
A list of functions performing the individual tests. Each function takes two arguments: the byte-stream and an open file-like object. When what() is called with a byte-stream, the file-like object will be None.

The test function should return a string describing the image type if the test succeeded, or None if it failed.

Example:


>>> import imghdr
>>> imghdr.what('bass.gif')
'gif'


## ==⚡ • sndhdr — Determine type of sound file

Source code: Lib/sndhdr.py


The `sndhdr` provides utility functions which attempt to determine the type of sound data which is in a file. When these functions are able to determine what type of sound data is stored in a file, they return a `namedtuple()`, containing five attributes: `(filetype, framerate, nchannels, nframes, sampwidth)`. The value for type indicates the data type and will be one of the strings `'aifc', 'aiff', 'au', 'hcom', 'sndr', 'sndt', 'voc', 'wav', '8svx', 'sb', 'ub', or 'ul'`. The `sampling_rate` will be either the actual value or 0 if unknown or difficult to decode. Similarly, channels will be either the number of channels or 0 if it cannot be determined or if the value is difficult to decode. The value for frames will be either the number of frames or -1. The last item in the tuple, bits_per_sample, will either be the sample size in bits or 'A' for A-LAW or 'U' for u-LAW.

➡ `sndhdr.what(filename)`
Determines the type of sound data stored in the file filename using whathdr(). If it succeeds, returns a namedtuple as described above, otherwise None is returned.

Changed in version 3.5: Result changed from a tuple to a namedtuple.


➡ `sndhdr.whathdr(filename)`
Determines the type of sound data stored in a file based on the file header. The name of the file is given by filename. This function returns a namedtuple as described above on success, or None.

Changed in version 3.5: Result changed from a tuple to a namedtuple.


## ==⚡ • ossaudiodev — Access to OSS-compatible audio devices



# =🚩 Internationalization

The modules described in this chapter help you write software that is independent of language and locale by providing mechanisms for selecting a language to be used in program messages or by tailoring output to match local conventions.

The list of modules described in this chapter is:

➡ `• gettext — Multilingual internationalization services`
◦ GNU gettext API
◦ Class-based API
◾ The NullTranslations class
◾ The GNUTranslations class
◾ Solaris message catalog support
◾ The Catalog constructor

◦ Internationalizing your programs and modules
◾ Localizing your module
◾ Localizing your application
◾ Changing languages on the fly
◾ Deferred translations

◦ Acknowledgements

➡ `• locale — Internationalization services`
◦ Background, details, hints, tips and caveats
◦ For extension writers and programs that embed Python
◦ Access to message catalogs

## ==⚡ • gettext — Multilingual internationalization services

## ==⚡ • locale — Internationalization services


# =🚩 Program Frameworks

The modules described in this chapter are frameworks that will largely dictate the structure of your program. Currently the modules described here are all oriented toward writing command-line interfaces.

The full list of modules described in this chapter is:

➡ `• turtle — Turtle graphics`
◦ Introduction
◦ Overview of available Turtle and Screen methods
◾ Turtle methods
◾ Methods of TurtleScreen/Screen

◦ Methods of RawTurtle/Turtle and corresponding functions
◾ Turtle motion
◾ Tell Turtle’s state
◾ Settings for measurement
◾ Pen control
◾ Drawing state
◾ Color control
◾ Filling
◾ More drawing control

◾ Turtle state
◾ Visibility
◾ Appearance

◾ Using events
◾ Special Turtle methods
◾ Compound shapes

◦ Methods of TurtleScreen/Screen and corresponding functions
◾ Window control
◾ Animation control
◾ Using screen events
◾ Input methods
◾ Settings and special methods
◾ Methods specific to Screen, not inherited from TurtleScreen

◦ Public classes
◦ Help and configuration
◾ How to use help
◾ Translation of docstrings into different languages
◾ How to configure Screen and Turtles

◦ turtledemo — Demo scripts
◦ Changes since Python 2.6
◦ Changes since Python 3.0

➡ `• cmd — Support for line-oriented command interpreters`
◦ Cmd Objects
◦ Cmd Example


➡ `• shlex — Simple lexical analysis`
◦ shlex Objects
◦ Parsing Rules
◦ Improved Compatibility with Shells

## ==⚡ • turtle — Turtle graphics

## ==⚡ • cmd — Support for line-oriented command interpreters


Source code: Lib/cmd.py


The Cmd class provides a simple framework for writing line-oriented command interpreters. These are often useful for test harnesses, administrative tools, and prototypes that will later be wrapped in a more sophisticated interface.

✅ `class cmd.Cmd(completekey='tab', stdin=None, stdout=None)`
A Cmd instance or subclass instance is a line-oriented interpreter framework. There is no good reason to instantiate Cmd itself; rather, it’s useful as a superclass of an interpreter class you define yourself in order to inherit Cmd’s methods and encapsulate action methods.

The optional argument *completekey* is the readline name of a completion key; it defaults to Tab. If completekey is not None and readline is available, command completion is done automatically.

The optional arguments *stdin* and *stdout* specify the input and output file objects that the Cmd instance or subclass instance will use for input and output. If not specified, they will default to sys.stdin and sys.stdout.

If you want a given *stdin* to be used, make sure to set the instance’s use_rawinput attribute to False, otherwise *stdin* will be ignored.


### ===🗝 Cmd Objects

A Cmd instance has the following methods:

➡ `Cmd.cmdloop(intro=None)`
Repeatedly issue a prompt, accept input, parse an initial prefix off the received input, and dispatch to action methods, passing them the remainder of the line as argument.

The optional argument is a banner or intro string to be issued before the first prompt (this overrides the intro class attribute).

If the readline module is loaded, input will automatically inherit bash-like history-list editing (e.g. Control-P scrolls back to the last command, Control-N forward to the next one, Control-F moves the cursor to the right non-destructively, Control-B moves the cursor to the left non-destructively, etc.).

An end-of-file on input is passed back as the string 'EOF'.

An interpreter instance will recognize a command name foo if and only if it has a method do_foo(). As a special case, a line beginning with the character '?' is dispatched to the method do_help(). As another special case, a line beginning with the character '!' is dispatched to the method do_shell() (if such a method is defined).

This method will return when the postcmd() method returns a true value. The stop argument to postcmd() is the return value from the command’s corresponding do_*() method.

If completion is enabled, completing commands will be done automatically, and completing of commands args is done by calling complete_foo() with arguments text, line, begidx, and endidx. text is the string prefix we are attempting to match: all returned matches must begin with it. line is the current input line with leading whitespace removed, begidx and endidx are the beginning and ending indexes of the prefix text, which could be used to provide different completion depending upon which position the argument is in.

All subclasses of Cmd inherit a predefined do_help(). This method, called with an argument 'bar', invokes the corresponding method help_bar(), and if that is not present, prints the docstring of do_bar(), if available. With no argument, do_help() lists all available help topics (that is, all commands with corresponding help_*() methods or commands that have docstrings), and also lists any undocumented commands.

➡ `Cmd.onecmd(str)`
Interpret the argument as though it had been typed in response to the prompt. This may be overridden, but should not normally need to be; see the precmd() and postcmd() methods for useful execution hooks. The return value is a flag indicating whether interpretation of commands by the interpreter should stop. If there is a do_*() method for the command str, the return value of that method is returned, otherwise the return value from the default() method is returned.

➡ `Cmd.emptyline()`
Method called when an empty line is entered in response to the prompt. If this method is not overridden, it repeats the last nonempty command entered.

➡ `Cmd.default(line)`
Method called on an input line when the command prefix is not recognized. If this method is not overridden, it prints an error message and returns.

➡ `Cmd.completedefault(text, line, begidx, endidx)`
Method called to complete an input line when no command-specific complete_*() method is available. By default, it returns an empty list.

➡ `Cmd.precmd(line)`
Hook method executed just before the command line line is interpreted, but after the input prompt is generated and issued. This method is a stub in Cmd; it exists to be overridden by subclasses. The return value is used as the command which will be executed by the onecmd() method; the precmd() implementation may re-write the command or simply return line unchanged.

➡ `Cmd.postcmd(stop, line)`
Hook method executed just after a command dispatch is finished. This method is a stub in Cmd; it exists to be overridden by subclasses. line is the command line which was executed, and stop is a flag which indicates whether execution will be terminated after the call to postcmd(); this will be the return value of the onecmd() method. The return value of this method will be used as the new value for the internal flag which corresponds to stop; returning false will cause interpretation to continue.

➡ `Cmd.preloop()`
Hook method executed once when cmdloop() is called. This method is a stub in Cmd; it exists to be overridden by subclasses.

➡ `Cmd.postloop()`
Hook method executed once when cmdloop() is about to return. This method is a stub in Cmd; it exists to be overridden by subclasses.

Instances of Cmd subclasses have some public instance variables:

➡ `Cmd.prompt`
The prompt issued to solicit input.

➡ `Cmd.identchars`
The string of characters accepted for the command prefix.

➡ `Cmd.lastcmd`
The last nonempty command prefix seen.

➡ `Cmd.cmdqueue`
A list of queued input lines. The cmdqueue list is checked in cmdloop() when new input is needed; if it is nonempty, its elements will be processed in order, as if entered at the prompt.

➡ `Cmd.intro`
A string to issue as an intro or banner. May be overridden by giving the cmdloop() method an argument.

➡ `Cmd.doc_header`
The header to issue if the help output has a section for documented commands.

➡ `Cmd.misc_header`
The header to issue if the help output has a section for miscellaneous help topics (that is, there are help_*() methods without corresponding do_*() methods).

➡ `Cmd.undoc_header`
The header to issue if the help output has a section for undocumented commands (that is, there are do_*() methods without corresponding help_*() methods).

➡ `Cmd.ruler`
The character used to draw separator lines under the help-message headers. If empty, no ruler line is drawn. It defaults to '='.

➡ `Cmd.use_rawinput`
A flag, defaulting to true. If true, cmdloop() uses input() to display a prompt and read the next command; if false, sys.stdout.write() and sys.stdin.readline() are used. (This means that by importing readline, on systems that support it, the interpreter will automatically support Emacs-like line editing and command-history keystrokes.)


### ===🗝 Cmd Example

The `cmd` module is mainly useful for building custom shells that let a user work with a program interactively.

This section presents a simple example of how to build a shell around a few of the commands in the `turtle` module.

Basic `turtle` commands such as forward() are added to a Cmd subclass with method named do_forward(). The argument is converted to a number and dispatched to the `turtle` module. The docstring is used in the help utility provided by the shell.

The example also includes a basic record and playback facility implemented with the `precmd()` method which is responsible for converting the input to lowercase and writing the commands to a file. The `do_playback()` method reads the file and adds the recorded commands to the cmdqueue for immediate playback:


```py
import cmd, sys
from turtle import *

class TurtleShell(cmd.Cmd):
    intro = 'Welcome to the turtle shell.   Type help or ? to list commands.\n'
    prompt = '(turtle) '
    file = None

    # ----- basic turtle commands -----
    def do_forward(self, arg):
        'Move the turtle forward by the specified distance:  FORWARD 10'
        forward(*parse(arg))
    def do_right(self, arg):
        'Turn turtle right by given number of degrees:  RIGHT 20'
        right(*parse(arg))
    def do_left(self, arg):
        'Turn turtle left by given number of degrees:  LEFT 90'
        left(*parse(arg))
    def do_goto(self, arg):
        'Move turtle to an absolute position with changing orientation.  GOTO 100 200'
        goto(*parse(arg))
    def do_home(self, arg):
        'Return turtle to the home position:  HOME'
        home()
    def do_circle(self, arg):
        'Draw circle with given radius an options extent and steps:  CIRCLE 50'
        circle(*parse(arg))
    def do_position(self, arg):
        'Print the current turtle position:  POSITION'
        print('Current position is %d %d\n' % position())
    def do_heading(self, arg):
        'Print the current turtle heading in degrees:  HEADING'
        print('Current heading is %d\n' % (heading(),))
    def do_color(self, arg):
        'Set the color:  COLOR BLUE'
        color(arg.lower())
    def do_undo(self, arg):
        'Undo (repeatedly) the last turtle action(s):  UNDO'
    def do_reset(self, arg):
        'Clear the screen and return turtle to center:  RESET'
        reset()
    def do_bye(self, arg):
        'Stop recording, close the turtle window, and exit:  BYE'
        print('Thank you for using Turtle')
        self.close()
        bye()
        return True

    # ----- record and playback -----
    def do_record(self, arg):
        'Save future commands to filename:  RECORD rose.cmd'
        self.file = open(arg, 'w')
    def do_playback(self, arg):
        'Playback commands from a file:  PLAYBACK rose.cmd'
        self.close()
        with open(arg) as f:
            self.cmdqueue.extend(f.read().splitlines())
    def precmd(self, line):
        line = line.lower()
        if self.file and 'playback' not in line:
            print(line, file=self.file)
        return line
    def close(self):
        if self.file:
            self.file.close()
            self.file = None

def parse(arg):
    'Convert a series of zero or more numbers to an argument tuple'
    return tuple(map(int, arg.split()))

if __name__ == '__main__':
    TurtleShell().cmdloop()
```


Here is a sample session with the turtle shell showing the help functions, using blank lines to repeat commands, and the simple record and playback facility:


    Welcome to the turtle shell.   Type help or ? to list commands.

    (turtle) ?

    Documented commands (type help <topic>):
    ========================================
    bye     color    goto     home  playback  record  right
    circle  forward  heading  left  position  reset   undo

    (turtle) help forward
    Move the turtle forward by the specified distance:  FORWARD 10
    (turtle) record spiral.cmd
    (turtle) position
    Current position is 0 0

    (turtle) heading
    Current heading is 0

    (turtle) reset
    (turtle) circle 20
    (turtle) right 30
    (turtle) circle 40
    (turtle) right 30
    (turtle) circle 60
    (turtle) right 30
    (turtle) circle 80
    (turtle) right 30
    (turtle) circle 100
    (turtle) right 30
    (turtle) circle 120
    (turtle) right 30
    (turtle) circle 120
    (turtle) heading
    Current heading is 180

    (turtle) forward 100
    (turtle)
    (turtle) right 90
    (turtle) forward 100
    (turtle)
    (turtle) right 90
    (turtle) forward 400
    (turtle) right 90
    (turtle) forward 500
    (turtle) right 90
    (turtle) forward 400
    (turtle) right 90
    (turtle) forward 300
    (turtle) playback spiral.cmd
    Current position is 0 0

    Current heading is 0

    Current heading is 180

    (turtle) bye
    Thank you for using Turtle

## ==⚡ • shlex — Simple lexical analysis

Source code: Lib/shlex.py


The shlex class makes it easy to write lexical analyzers for simple syntaxes resembling that of the Unix shell. This will often be useful for writing minilanguages, (for example, in run control files for Python applications) or for parsing quoted strings.

The shlex module defines the following functions:

➡ `shlex.split(s, comments=False, posix=True)`
Split the string s using shell-like syntax. If comments is False (the default), the parsing of comments in the given string will be disabled (setting the commenters attribute of the shlex instance to the empty string). This function operates in POSIX mode by default, but uses non-POSIX mode if the posix argument is false.

Note:
 Since the split() function instantiates a shlex instance, passing None for s will read the string to split from standard input.
 


Deprecated since version 3.9: Passing None for s will raise an exception in future Python versions.

➡ `shlex.join(split_command)`
Concatenate the tokens of the list split_command and return a string. This function is the inverse of split().


>>> from shlex import join
>>> print(join(['echo', '-n', 'Multiple words']))
echo -n 'Multiple words'


The returned value is shell-escaped to protect against injection vulnerabilities (see quote()).


New in version 3.8.

➡ `shlex.quote(s)`
Return a shell-escaped version of the string s. The returned value is a string that can safely be used as one token in a shell command line, for cases where you cannot use a list.

Warning:
 The shlex module is only designed for Unix shells.
 
The quote() function is not guaranteed to be correct on non-POSIX compliant shells or shells from other operating systems such as Windows. Executing commands quoted by this module on such shells can open up the possibility of a command injection vulnerability.

Consider using functions that pass command arguments with lists such as subprocess.run() with shell=False.

This idiom would be unsafe:


>>> filename = 'somefile; rm -rf ~'
>>> command = 'ls -l {}'.format(filename)
>>> print(command)  # executed by a shell: boom!
ls -l somefile; rm -rf ~


quote() lets you plug the security hole:


>>> from shlex import quote
>>> command = 'ls -l {}'.format(quote(filename))
>>> print(command)
ls -l 'somefile; rm -rf ~'
>>> remote_command = 'ssh home {}'.format(quote(command))
>>> print(remote_command)
ssh home 'ls -l '"'"'somefile; rm -rf ~'"'"''


The quoting is compatible with UNIX shells and with split():


>>> from shlex import split
>>> remote_command = split(remote_command)
>>> remote_command
['ssh', 'home', "ls -l 'somefile; rm -rf ~'"]
>>> command = split(remote_command[-1])
>>> command
['ls', '-l', 'somefile; rm -rf ~']



New in version 3.3.

The shlex module defines the following class:

✅ `class shlex.shlex(instream=None, infile=None, posix=False, punctuation_chars=False)`
A shlex instance or subclass instance is a lexical analyzer object. The initialization argument, if present, specifies where to read characters from. It must be a file-/stream-like object with `read()` and `readline()` methods, or a string. If no argument is given, input will be taken from `sys.stdin`. The second optional argument is a filename string, which sets the initial value of the infile attribute. If the instream argument is omitted or equal to `sys.stdin`, this second argument defaults to “stdin”. The posix argument defines the operational mode: when posix is not true (default), the shlex instance will operate in compatibility mode. When operating in POSIX mode, shlex will try to be as close as possible to the POSIX shell parsing rules. The punctuation_chars argument provides a way to make the behaviour even closer to how real shells parse. This can take a number of values: the default value, False, preserves the behaviour seen under Python 3.5 and earlier. If set to True, then parsing of the characters ();<>|& is changed: any run of these characters (considered punctuation characters) is returned as a single token. If set to a non-empty string of characters, those characters will be used as the punctuation characters. Any characters in the wordchars attribute that appear in punctuation_chars will be removed from wordchars. See Improved Compatibility with Shells for more information. punctuation_chars can be set only upon shlex instance creation and can’t be modified later.


Changed in version 3.6: The punctuation_chars parameter was added.

See also:
 Module configparserParser for configuration files similar to the Windows .ini files.

### ===🗝 shlex Objects

A shlex instance has the following methods:

➡ `shlex.get_token()`
Return a token. If tokens have been stacked using push_token(), pop a token off the stack. Otherwise, read one from the input stream. If reading encounters an immediate end-of-file, eof is returned (the empty string ('') in non-POSIX mode, and None in POSIX mode).

➡ `shlex.push_token(str)`
Push the argument onto the token stack.

➡ `shlex.read_token()`
Read a raw token. Ignore the pushback stack, and do not interpret source requests. (This is not ordinarily a useful entry point, and is documented here only for the sake of completeness.)

➡ `shlex.sourcehook(filename)`
When shlex detects a source request (see source below) this method is given the following token as argument, and expected to return a tuple consisting of a filename and an open file-like object.

Normally, this method first strips any quotes off the argument. If the result is an absolute pathname, or there was no previous source request in effect, or the previous source was a stream (such as sys.stdin), the result is left alone. Otherwise, if the result is a relative pathname, the directory part of the name of the file immediately before it on the source inclusion stack is prepended (this behavior is like the way the C preprocessor handles #include "file.h").

The result of the manipulations is treated as a filename, and returned as the first component of the tuple, with open() called on it to yield the second component. (Note: this is the reverse of the order of arguments in instance initialization!)

This hook is exposed so that you can use it to implement directory search paths, addition of file extensions, and other namespace hacks. There is no corresponding ‘close’ hook, but a shlex instance will call the close() method of the sourced input stream when it returns EOF.

For more explicit control of source stacking, use the push_source() and pop_source() methods.

➡ `shlex.push_source(newstream, newfile=None)`
Push an input source stream onto the input stack. If the filename argument is specified it will later be available for use in error messages. This is the same method used internally by the sourcehook() method.

➡ `shlex.pop_source()`
Pop the last-pushed input source from the input stack. This is the same method used internally when the lexer reaches EOF on a stacked input stream.

➡ `shlex.error_leader(infile=None, lineno=None)`
This method generates an error message leader in the format of a Unix C compiler error label; the format is '"%s", line %d: ', where the %s is replaced with the name of the current source file and the %d with the current input line number (the optional arguments can be used to override these).

This convenience is provided to encourage shlex users to generate error messages in the standard, parseable format understood by Emacs and other Unix tools.

Instances of shlex subclasses have some public instance variables which either control lexical analysis or can be used for debugging:

➡ `shlex.commenters`
The string of characters that are recognized as comment beginners. All characters from the comment beginner to end of line are ignored. Includes just '#' by default.

➡ `shlex.wordchars`
The string of characters that will accumulate into multi-character tokens. By default, includes all ASCII alphanumerics and underscore. In POSIX mode, the accented characters in the Latin-1 set are also included. If punctuation_chars is not empty, the characters `~-./*?=`, which can appear in filename specifications and command line parameters, will also be included in this attribute, and any characters which appear in punctuation_chars will be removed from wordchars if they are present there. If whitespace_split is set to True, this will have no effect.

➡ `shlex.whitespace`
Characters that will be considered whitespace and skipped. Whitespace bounds tokens. By default, includes space, tab, linefeed and carriage-return.

➡ `shlex.escape`
Characters that will be considered as escape. This will be only used in POSIX mode, and includes just '\' by default.

➡ `shlex.quotes`
Characters that will be considered string quotes. The token accumulates until the same quote is encountered again (thus, different quote types protect each other as in the shell.) By default, includes ASCII single and double quotes.

➡ `shlex.escapedquotes`
Characters in quotes that will interpret escape characters defined in escape. This is only used in POSIX mode, and includes just '"' by default.

➡ `shlex.whitespace_split`
If True, tokens will only be split in whitespaces. This is useful, for example, for parsing command lines with shlex, getting tokens in a similar way to shell arguments. When used in combination with punctuation_chars, tokens will be split on whitespace in addition to those characters.


Changed in version 3.8: The punctuation_chars attribute was made compatible with the whitespace_split attribute.

➡ `shlex.infile`
The name of the current input file, as initially set at class instantiation time or stacked by later source requests. It may be useful to examine this when constructing error messages.

➡ `shlex.instream`
The input stream from which this shlex instance is reading characters.

➡ `shlex.source`
This attribute is None by default. If you assign a string to it, that string will be recognized as a lexical-level inclusion request similar to the source keyword in various shells. That is, the immediately following token will be opened as a filename and input will be taken from that stream until EOF, at which point the close() method of that stream will be called and the input source will again become the original input stream. Source requests may be stacked any number of levels deep.

➡ `shlex.debug`
If this attribute is numeric and 1 or more, a shlex instance will print verbose progress output on its behavior. If you need to use this, you can read the module source code to learn the details.

➡ `shlex.lineno`
Source line number (count of newlines seen so far plus one).

➡ `shlex.token`
The token buffer. It may be useful to examine this when catching exceptions.

➡ `shlex.eof`
Token used to determine end of file. This will be set to the empty string (''), in non-POSIX mode, and to None in POSIX mode.

➡ `shlex.punctuation_chars`
A read-only property. Characters that will be considered punctuation. Runs of punctuation characters will be returned as a single token. However, note that no semantic validity checking will be performed: for example, ‘>>>’ could be returned as a token, even though it may not be recognised as such by shells.


New in version 3.6.


### ===🗝 Parsing Rules

When operating in non-POSIX mode, shlex will try to obey to the following rules.

• Quote characters are not recognized within words (Do"Not"Separate is parsed as the single word Do"Not"Separate);

• Escape characters are not recognized;

• Enclosing characters in quotes preserve the literal value of all characters within the quotes;

• Closing quotes separate words ("Do"Separate is parsed as "Do" and Separate);

• If whitespace_split is False, any character not declared to be a word character, whitespace, or a quote will be returned as a single-character token. If it is True, shlex will only split words in whitespaces;

• EOF is signaled with an empty string ('');

• It’s not possible to parse empty strings, even if quoted.

When operating in POSIX mode, shlex will try to obey to the following parsing rules.

• Quotes are stripped out, and do not separate words ("Do"Not"Separate" is parsed as the single word DoNotSeparate);

• Non-quoted escape characters (e.g. '\') preserve the literal value of the next character that follows;

• Enclosing characters in quotes which are not part of escapedquotes (e.g. "'") preserve the literal value of all characters within the quotes;

• Enclosing characters in quotes which are part of escapedquotes (e.g. '"') preserves the literal value of all characters within the quotes, with the exception of the characters mentioned in escape. The escape characters retain its special meaning only when followed by the quote in use, or the escape character itself. Otherwise the escape character will be considered a normal character.

• EOF is signaled with a None value;

• Quoted empty strings ('') are allowed.


### ===🗝 Improved Compatibility with Shells


New in version 3.6.

The shlex class provides compatibility with the parsing performed by common Unix shells like bash, dash, and sh. To take advantage of this compatibility, specify the punctuation_chars argument in the constructor. This defaults to False, which preserves pre-3.6 behaviour. However, if it is set to True, then parsing of the characters ();<>|& is changed: any run of these characters is returned as a single token. While this is short of a full parser for shells (which would be out of scope for the standard library, given the multiplicity of shells out there), it does allow you to perform processing of command lines more easily than you could otherwise. To illustrate, you can see the difference in the following snippet:


 >>> import shlex
 >>> text = "a && b; c && d || e; f >'abc'; (def \"ghi\")"
 >>> s = shlex.shlex(text, posix=True)
 >>> s.whitespace_split = True
 >>> list(s)
 ['a', '&&', 'b;', 'c', '&&', 'd', '||', 'e;', 'f', '>abc;', '(def', 'ghi)']
 >>> s = shlex.shlex(text, posix=True, punctuation_chars=True)
 >>> s.whitespace_split = True
 >>> list(s)
 ['a', '&&', 'b', ';', 'c', '&&', 'd', '||', 'e', ';', 'f', '>', 'abc', ';',
 '(', 'def', 'ghi', ')']


Of course, tokens will be returned which are not valid for shells, and you’ll need to implement your own error checks on the returned tokens.

Instead of passing True as the value for the punctuation_chars parameter, you can pass a string with specific characters, which will be used to determine which characters constitute punctuation. For example:


>>> import shlex
>>> s = shlex.shlex("a && b || c", punctuation_chars="|")
>>> list(s)
['a', '&', '&', 'b', '||', 'c']


Note:
 When punctuation_chars is specified, the wordchars attribute is augmented with the characters `~-./*?=`. That is because these characters can appear in file names (including wildcards) and command-line arguments (e.g. --color=auto). Hence:
 

```sh
>>> import shlex
>>> s = shlex.shlex('~/a && b-c --color=auto || d *.py?',
...                 punctuation_chars=True)
>>> list(s)
['~/a', '&&', 'b-c', '--color=auto', '||', 'd', '*.py?']
```


However, to match the shell as closely as possible, it is recommended to always use posix and whitespace_split when using punctuation_chars, which will negate wordchars entirely.

For best effect, punctuation_chars should be set in conjunction with posix=True. (Note that posix=False is the default for shlex.)


# =🚩 Graphical User Interfaces with Tk

Tk/Tcl has long been an integral part of Python. It provides a robust and platform independent windowing toolkit, that is available to Python programmers using the tkinter package, and its extension, the tkinter.tix and the tkinter.ttk modules.

The tkinter package is a thin object-oriented layer on top of Tcl/Tk. To use tkinter, you don’t need to write Tcl code, but you will need to consult the Tk documentation, and occasionally the Tcl documentation. tkinter is a set of wrappers that implement the Tk widgets as Python classes.

tkinter’s chief virtues are that it is fast, and that it usually comes bundled with Python. Although its standard documentation is weak, good material is available, which includes: references, tutorials, a book and others. tkinter is also famous for having an outdated look and feel, which has been vastly improved in Tk 8.5. Nevertheless, there are many other GUI libraries that you could be interested in. The Python wiki lists several alternative GUI frameworks and tools.

➡ `• tkinter — Python interface to Tcl/Tk`
◦ Architecture
◦ Tkinter Modules
◦ Tkinter Life Preserver
◾ A Hello World Program
◾ Important Tk Concepts
◾ Understanding How Tkinter Wraps Tcl/Tk
◾ How do I…? What option does…?
◾ Navigating the Tcl/Tk Reference Manual

◦ Threading model
◦ Handy Reference
◾ Setting Options
◾ The Packer
◾ Packer Options
◾ Coupling Widget Variables
◾ The Window Manager
◾ Tk Option Data Types
◾ Bindings and Events
◾ The index Parameter
◾ Images

◦ File Handlers

➡ `• tkinter.colorchooser — Color choosing dialog`
➡ `• tkinter.font — Tkinter font wrapper`
➡ `• Tkinter Dialogs`
◦ tkinter.simpledialog — Standard Tkinter input dialogs
◦ tkinter.filedialog — File selection dialogs
◾ Native Load/Save Dialogs

◦ tkinter.commondialog — Dialog window templates

➡ `• tkinter.messagebox — Tkinter message prompts`
➡ `• tkinter.scrolledtext — Scrolled Text Widget`
➡ `• tkinter.dnd — Drag and drop support`
➡ `• tkinter.ttk — Tk themed widgets`
◦ Using Ttk
◦ Ttk Widgets
◦ Widget
◾ Standard Options
◾ Scrollable Widget Options
◾ Label Options
◾ Compatibility Options
◾ Widget States
◾ ttk.Widget

◦ Combobox
◾ Options
◾ Virtual events
◾ ttk.Combobox

◦ Spinbox
◾ Options
◾ Virtual events
◾ ttk.Spinbox

◦ Notebook
◾ Options
◾ Tab Options
◾ Tab Identifiers
◾ Virtual Events
◾ ttk.Notebook

◦ Progressbar
◾ Options
◾ ttk.Progressbar

◦ Separator
◾ Options

◦ Sizegrip
◾ Platform-specific notes
◾ Bugs

◦ Treeview
◾ Options
◾ Item Options
◾ Tag Options
◾ Column Identifiers
◾ Virtual Events
◾ ttk.Treeview

◦ Ttk Styling
◾ Layouts


➡ `• tkinter.tix — Extension widgets for Tk`
◦ Using Tix
◦ Tix Widgets
◾ Basic Widgets
◾ File Selectors
◾ Hierarchical ListBox
◾ Tabular ListBox
◾ Manager Widgets
◾ Image Types
◾ Miscellaneous Widgets
◾ Form Geometry Manager

◦ Tix Commands

➡ `• IDLE`
◦ Menus
◾ File menu (Shell and Editor)
◾ Edit menu (Shell and Editor)
◾ Format menu (Editor window only)
◾ Run menu (Editor window only)
◾ Shell menu (Shell window only)
◾ Debug menu (Shell window only)
◾ Options menu (Shell and Editor)
◾ Window menu (Shell and Editor)
◾ Help menu (Shell and Editor)
◾ Context Menus

◦ Editing and navigation
◾ Editor windows
◾ Key bindings
◾ Automatic indentation
◾ Completions
◾ Calltips
◾ Code Context
◾ Python Shell window
◾ Text colors

◦ Startup and code execution
◾ Command line usage
◾ Startup failure
◾ Running user code
◾ User output in Shell
◾ Developing tkinter applications
◾ Running without a subprocess

◦ Help and preferences
◾ Help sources
◾ Setting preferences
◾ IDLE on macOS
◾ Extensions

## ==⚡ • tkinter — Python interface to Tcl/Tk
- Tcl Package User Guide https://wiki.tcl-lang.org/page/Tcl+Package+User+Guide

Source code: Lib/tkinter/__init__.py

The tkinter package (“Tk interface”) is the standard Python interface to the Tcl/Tk GUI toolkit. Both Tk and tkinter are available on most Unix platforms, including macOS, as well as on Windows systems.

Running python -m tkinter from the command line should open a window demonstrating a simple Tk interface, letting you know that tkinter is properly installed on your system, and also showing what version of Tcl/Tk is installed, so you can read the Tcl/Tk documentation specific to that version.

Tkinter supports a range of Tcl/Tk versions, built either with or without thread support. The official Python binary release bundles Tcl/Tk 8.6 threaded. See the source code for the `_tkinter` module for more information about supported versions.

Tkinter is not a thin wrapper, but adds a fair amount of its own logic to make the experience more pythonic. This documentation will concentrate on these additions and changes, and refer to the official Tcl/Tk documentation for details that are unchanged.

Note:
 Tcl/Tk 8.5 (2007) introduced a modern set of themed user interface components along with a new API to use them. Both old and new APIs are still available. Most documentation you will find online still uses the old API and can be woefully outdated.
 

See also:
• TkDocsExtensive tutorial on creating user interfaces with Tkinter. Explains key concepts, and illustrates recommended approaches using the modern API.
• Tkinter 8.5 reference: a GUI for PythonReference documentation for Tkinter 8.5 detailing available classes, methods, and options.

Tcl/Tk Resources:
• Tk commandsComprehensive reference to each of the underlying Tcl/Tk commands used by Tkinter.
• Tcl/Tk Home PageAdditional documentation, and links to Tcl/Tk core development.

Books:
• Modern Tkinter for Busy Python Developers By Mark Roseman. (ISBN 978-1999149567)
• Python and Tkinter Programming By Alan Moore. (ISBN 978-1788835886)
• Programming Python By Mark Lutz; has excellent coverage of Tkinter. (ISBN 978-0596158101)
• Tcl and the Tk Toolkit (2nd edition) By John Ousterhout, inventor of Tcl/Tk, and Ken Jones; does not cover Tkinter. (ISBN 978-0321336330)


### ===🗝 Architecture

Tcl/Tk is not a single library but rather consists of a few distinct modules, each with separate functionality and its own official documentation. Python’s binary releases also ship an add-on module together with it.

#### Tcl
Tcl is a dynamic interpreted programming language, just like Python. Though it can be used on its own as a general-purpose programming language, it is most commonly embedded into C applications as a scripting engine or an interface to the Tk toolkit. The Tcl library has a C interface to create and manage one or more instances of a Tcl interpreter, run Tcl commands and scripts in those instances, and add custom commands implemented in either Tcl or C. Each interpreter has an event queue, and there are facilities to send events to it and process them. Unlike Python, Tcl’s execution model is designed around cooperative multitasking, and Tkinter bridges this difference (see Threading model for details).

#### Tk
Tk is a Tcl package implemented in C that adds custom commands to create and manipulate GUI widgets. Each Tk object embeds its own Tcl interpreter instance with Tk loaded into it. Tk’s widgets are very customizable, though at the cost of a dated appearance. Tk uses Tcl’s event queue to generate and process GUI events.

#### Ttk
Themed Tk (Ttk) is a newer family of Tk widgets that provide a much better appearance on different platforms than many of the classic Tk widgets. Ttk is distributed as part of Tk, starting with Tk version 8.5. Python bindings are provided in a separate module, tkinter.ttk.

Internally, Tk and Ttk use facilities of the underlying operating system, i.e., Xlib on Unix/X11, Cocoa on macOS, GDI on Windows.

When your Python application uses a class in Tkinter, e.g., to create a widget, the tkinter module first assembles a Tcl/Tk command string. It passes that Tcl command string to an internal `_tkinter` binary module, which then calls the Tcl interpreter to evaluate it. The Tcl interpreter will then call into the Tk and/or Ttk packages, which will in turn make calls to Xlib, Cocoa, or GDI.


### ===🗝 Tkinter Modules

Support for Tkinter is spread across several modules. Most applications will need the main tkinter module, as well as the tkinter.ttk module, which provides the modern themed widget set and API:


```py
from tkinter import *
from tkinter import ttk
```

✅ `class tkinter.Tk(screenName=None, baseName=None, className='Tk', useTk=1)`
The Tk class is instantiated without arguments. This creates a toplevel widget of Tk which usually is the main window of an application. Each instance has its own associated Tcl interpreter.


➡ `tkinter.Tcl(screenName=None, baseName=None, className='Tk', useTk=0)`
The Tcl() function is a factory function which creates an object much like that created by the Tk class, except that it does not initialize the Tk subsystem. This is most often useful when driving the Tcl interpreter in an environment where one doesn’t want to create extraneous toplevel windows, or where one cannot (such as Unix/Linux systems without an X server). An object created by the Tcl() object can have a Toplevel window created (and the Tk subsystem initialized) by calling its loadtk() method.

The modules that provide Tk support include:

➡ `tkinter`
Main Tkinter module.

➡ `tkinter.colorchooser`
Dialog to let the user choose a color.

➡ `tkinter.commondialog`
Base class for the dialogs defined in the other modules listed here.

➡ `tkinter.filedialog`
Common dialogs to allow the user to specify a file to open or save.

➡ `tkinter.font`
Utilities to help work with fonts.

➡ `tkinter.messagebox`
Access to standard Tk dialog boxes.

➡ `tkinter.scrolledtext`
Text widget with a vertical scroll bar built in.

➡ `tkinter.simpledialog`
Basic dialogs and convenience functions.

➡ `tkinter.ttk`
Themed widget set introduced in Tk 8.5, providing modern alternatives for many of the classic widgets in the main tkinter module.

Additional modules:

➡ `_tkinterA`
binary module that contains the low-level interface to Tcl/Tk. It is automatically imported by the main tkinter module, and should never be used directly by application programmers. It is usually a shared library (or DLL), but might in some cases be statically linked with the Python interpreter.

➡ `idlelib`
Python’s Integrated Development and Learning Environment (IDLE). Based on tkinter.

➡ `tkinter.constants`
Symbolic constants that can be used in place of strings when passing various parameters to Tkinter calls. Automatically imported by the main tkinter module.

➡ `tkinter.dnd`
(experimental) Drag-and-drop support for tkinter. This will become deprecated when it is replaced with the Tk DND.

➡ `tkinter.tix`
(deprecated) An older third-party Tcl/Tk package that adds several new widgets. Better alternatives for most can be found in tkinter.ttk.

➡ `turtle`
Turtle graphics in a Tk window.

### ===🗝 Tkinter Life Preserver

This section is not designed to be an exhaustive tutorial on either Tk or Tkinter. For that, refer to one of the external resources noted earlier. Instead, this section provides a very quick orientation to what a Tkinter application looks like, identifies foundational Tk concepts, and explains how the Tkinter wrapper is structured.

The remainder of this section will help you to identify the classes, methods, and options you’ll need in your Tkinter application, and where to find more detailed documentation on them, including in the official Tcl/Tk reference manual.


#### A Hello World Program

We’ll start by walking through a “Hello World” application in Tkinter. This isn’t the smallest one we could write, but has enough to illustrate some key concepts you’ll need to know.


```py
from tkinter import *
from tkinter import ttk
root = Tk()
frm = ttk.Frame(root, padding=10)
frm.grid()
ttk.Label(frm, text="Hello World!").grid(column=0, row=0)
ttk.Button(frm, text="Quit", command=root.destroy).grid(column=1, row=0)
root.mainloop()
```


After the imports, the next line creates an instance of the Tk class, which initializes Tk and creates its associated Tcl interpreter. It also creates a toplevel window, known as the root window, which serves as the main window of the application.

The following line creates a frame widget, which in this case will contain a label and a button we’ll create next. The frame is fit inside the root window.

The next line creates a label widget holding a static text string. The `grid()` method is used to specify the relative layout (position) of the label within its containing frame widget, similar to how tables in HTML work.

A button widget is then created, and placed to the right of the label. When pressed, it will call the `destroy()` method of the root window.

Finally, the `mainloop()` method puts everything on the display, and responds to user input until the program terminates.


#### Important Tk Concepts

Even this simple program illustrates the following key Tk concepts:

➡ `widgets`
A Tkinter user interface is made up of individual widgets. Each widget is represented as a Python object, instantiated from classes like `ttk.Frame`, `ttk.Label`, and `ttk.Button`.

➡ `widget hierarchy`
Widgets are arranged in a hierarchy. The label and button were contained within a frame, which in turn was contained within the root window. When creating each child widget, its parent widget is passed as the first argument to the widget constructor.

➡ `configuration options`
Widgets have configuration options, which modify their appearance and behavior, such as the text to display in a label or button. Different classes of widgets will have different sets of options.

➡ `geometry management`
Widgets aren’t automatically added to the user interface when they are created. A geometry manager like grid controls where in the user interface they are placed.

➡ `event loop`
Tkinter reacts to user input, changes from your program, and even refreshes the display only when actively running an event loop. If your program isn’t running the event loop, your user interface won’t update.

#### Understanding How Tkinter Wraps Tcl/Tk

When your application uses Tkinter’s classes and methods, internally Tkinter is assembling strings representing Tcl/Tk commands, and executing those commands in the Tcl interpreter attached to your applicaton’s Tk instance.

Whether it’s trying to navigate reference documentation, trying to find the right method or option, adapting some existing code, or debugging your Tkinter application, there are times that it will be useful to understand what those underlying Tcl/Tk commands look like.

To illustrate, here is the Tcl/Tk equivalent of the main part of the Tkinter script above.


```sh
ttk::frame .frm -padding 10
grid .frm
grid [ttk::label .frm.lbl -text "Hello World!"] -column 0 -row 0
grid [ttk::button .frm.btn -text "Quit" -command "destroy ."] -column 1 -row 0
```


Tcl’s syntax is similar to many shell languages, where the first word is the command to be executed, with arguments to that command following it, separated by spaces. Without getting into too many details, notice the following:

• The commands used to create widgets (like `ttk::frame`) correspond to widget classes in Tkinter.

• Tcl widget options (like `-text`) correspond to keyword arguments in Tkinter.

• Widgets are referred to by a pathname in Tcl (like `.frm.btn`), whereas Tkinter doesn’t use names but object references.

• A widget’s place in the widget hierarchy is encoded in its (hierarchical) pathname, which uses a . (dot) as a path separator. The pathname for the root window is just . (dot). In Tkinter, the hierarchy is defined not by pathname but by specifying the` parent widget` when creating each child widget.

• Operations which are implemented as separate commands in Tcl (like `grid` or `destroy`) are represented as methods on Tkinter widget objects. As you’ll see shortly, at other times Tcl uses what appear to be method calls on `widget objects`, which more closely mirror what would is used in Tkinter.



#### How do I…? What option does…?

If you’re not sure how to do something in Tkinter, and you can’t immediately find it in the tutorial or reference documentation you’re using, there are a few strategies that can be helpful.

First, remember that the details of how individual widgets work may vary across different versions of both Tkinter and Tcl/Tk. If you’re searching documentation, make sure it corresponds to the Python and Tcl/Tk versions installed on your system.

When searching for how to use an API, it helps to know the exact name of the class, option, or method that you’re using. Introspection, either in an interactive Python shell or with print(), can help you identify what you need.

To find out what configuration options are available on any widget, call its configure() method, which returns a dictionary containing a variety of information about each object, including its default and current values. Use keys() to get just the names of each option.


```py
btn = ttk.Button(frm, ...)
print(btn.configure().keys())
```


As most widgets have many configuration options in common, it can be useful to find out which are specific to a particular widget class. Comparing the list of options to that of a simpler widget, like a frame, is one way to do that.


```py
print(set(btn.configure().keys()) - set(frm.configure().keys()))
```


Similarly, you can find the available methods for a widget object using the standard dir() function. If you try it, you’ll see there are over 200 common widget methods, so again identifying those specific to a widget class is helpful.


```py
print(dir(btn))
print(set(dir(btn)) - set(dir(frm)))
```



#### Navigating the Tcl/Tk Reference Manual

As noted, the official Tk commands reference manual (man pages) is often the most accurate description of what specific operations on widgets do. Even when you know the name of the option or method that you need, you may still have a few places to look.

While all operations in Tkinter are implemented as method calls on widget objects, you’ve seen that many Tcl/Tk operations appear as commands that take a widget pathname as its first parameter, followed by optional parameters, e.g.


destroy .
grid .frm.btn -column 0 -row 0


Others, however, look more like methods called on a widget object (in fact, when you create a widget in Tcl/Tk, it creates a Tcl command with the name of the widget pathname, with the first parameter to that command being the name of a method to call).


.frm.btn invoke
.frm.lbl configure -text "Goodbye"


In the official Tcl/Tk reference documentation, you’ll find most operations that look like method calls on the man page for a specific widget (e.g., you’ll find the invoke() method on the `ttk::button` man page), while functions that take a widget as a parameter often have their own man page (e.g., grid).

You’ll find many common options and methods in the options or `ttk::widget` man pages, while others are found in the man page for a specific widget class.

You’ll also find that many Tkinter methods have compound names, e.g., winfo_x(), winfo_height(), winfo_viewable(). You’d find documentation for all of these in the winfo man page.

Note:
 Somewhat confusingly, there are also methods on all Tkinter widgets that don’t actually operate on the widget, but operate at a global scope, independent of any widget. Examples are methods for accessing the clipboard or the system bell. (They happen to be implemented as methods in the base Widget class that all Tkinter widgets inherit from).
 


### ===🗝 Threading model

Python and Tcl/Tk have very different threading models, which tkinter tries to bridge. If you use threads, you may need to be aware of this.

A Python interpreter may have many threads associated with it. In Tcl, multiple threads can be created, but each thread has a separate Tcl interpreter instance associated with it. Threads can also create more than one interpreter instance, though each interpreter instance can be used only by the one thread that created it.

Each Tk object created by tkinter contains a Tcl interpreter. It also keeps track of which thread created that interpreter. Calls to tkinter can be made from any Python thread. Internally, if a call comes from a thread other than the one that created the Tk object, an event is posted to the interpreter’s event queue, and when executed, the result is returned to the calling Python thread.

Tcl/Tk applications are normally event-driven, meaning that after initialization, the interpreter runs an event loop (i.e. `Tk.mainloop()`) and responds to events. Because it is single-threaded, event handlers must respond quickly, otherwise they will block other events from being processed. To avoid this, any long-running computations should not run in an event handler, but are either broken into smaller pieces using timers, or run in another thread. This is different from many GUI toolkits where the GUI runs in a completely separate thread from all application code including event handlers.

If the Tcl interpreter is not running the event loop and processing events, any tkinter calls made from threads other than the one running the Tcl interpreter will fail.

A number of special cases exist:


• Tcl/Tk libraries can be built so they are not thread-aware. In this case, tkinter calls the library from the originating Python thread, even if this is different than the thread that created the Tcl interpreter. A global lock ensures only one call occurs at a time.
• While tkinter allows you to create more than one instance of a Tk object (with its own interpreter), all interpreters that are part of the same thread share a common event queue, which gets ugly fast. In practice, don’t create more than one instance of Tk at a time. Otherwise, it’s best to create them in separate threads and ensure you’re running a thread-aware Tcl/Tk build.
• Blocking event handlers are not the only way to prevent the Tcl interpreter from reentering the event loop. It is even possible to run multiple nested event loops or abandon the event loop entirely. If you’re doing anything tricky when it comes to events or threads, be aware of these possibilities.
• There are a few select tkinter functions that presently work only when called from the thread that created the Tcl interpreter.


### ===🗝 Handy Reference


#### Setting Options

Options control things like the color and border width of a widget. Options can be set in three ways:
At object creation time, using keyword arguments

    fred = Button(self, fg="red", bg="blue")

After object creation, treating the option name like a dictionary index

    fred["fg"] = "red"
    fred["bg"] = "blue"

Use the config() method to update multiple attrs subsequent to object creation

    fred.config(fg="red", bg="blue")


For a complete explanation of a given option and its behavior, see the Tk man pages for the widget in question.

Note that the man pages list “STANDARD OPTIONS” and “WIDGET SPECIFIC OPTIONS” for each widget. The former is a list of options that are common to many widgets, the latter are the options that are idiosyncratic to that particular widget. The Standard Options are documented on the options(3) man page.

No distinction between standard and widget-specific options is made in this document. Some options don’t apply to some kinds of widgets. Whether a given widget responds to a particular option depends on the class of the widget; buttons have a command option, labels do not.

The options supported by a given widget are listed in that widget’s man page, or can be queried at runtime by calling the config() method without arguments, or by calling the keys() method on that widget. The return value of these calls is a dictionary whose key is the name of the option as a string (for example, 'relief') and whose values are 5-tuples.

Some options, like bg are synonyms for common options with long names (bg is shorthand for “background”). Passing the config() method the name of a shorthand option will return a 2-tuple, not 5-tuple. The 2-tuple passed back will contain the name of the synonym and the “real” option (such as ('bg', 'background')).

| Index |             Meaning              | Example  |
|-------|----------------------------------|----------|
|     0 | option  name                     | 'relief' |
|     1 | option name for database lookup  | 'relief' |
|     2 | option class for database lookup | 'Relief' |
|     3 | default value                    | 'raised' |
|     4 | current value                    | 'groove' |

Example:


>>> print(fred.config())
{'relief': ('relief', 'relief', 'Relief', 'raised', 'groove')}


Of course, the dictionary printed will include all the options available and their values. This is meant only as an example.


#### The Packer

The packer is one of Tk’s geometry-management mechanisms. Geometry managers are used to specify the relative positioning of widgets within their container - their mutual master. In contrast to the more cumbersome placer (which is used less commonly, and we do not cover here), the packer takes qualitative relationship specification - above, to the left of, filling, etc - and works everything out to determine the exact placement coordinates for you.

The size of any master widget is determined by the size of the “slave widgets” inside. The packer is used to control where slave widgets appear inside the master into which they are packed. You can pack widgets into frames, and frames into other frames, in order to achieve the kind of layout you desire. Additionally, the arrangement is dynamically adjusted to accommodate incremental changes to the configuration, once it is packed.

Note that widgets do not appear until they have had their geometry specified with a geometry manager. It’s a common early mistake to leave out the geometry specification, and then be surprised when the widget is created but nothing appears. A widget will appear only after it has had, for example, the packer’s pack() method applied to it.

The pack() method can be called with keyword-option/value pairs that control where the widget is to appear within its container, and how it is to behave when the main application window is resized. Here are some examples:


```py
fred.pack()                     # defaults to side = "top"
fred.pack(side="left")
fred.pack(expand=1)
```



#### Packer Options

For more extensive information on the packer and the options that it can take, see the man pages and page 183 of John Ousterhout’s book.
anchorAnchor type. Denotes where the packer is to place each slave in its parcel.expandBoolean, 0 or 1.fillLegal values: 'x', 'y', 'both', 'none'.ipadx and ipadyA distance - designating internal padding on each side of the slave widget.padx and padyA distance - designating external padding on each side of the slave widget.sideLegal values are: 'left', 'right', 'top', 'bottom'.

#### Coupling Widget Variables

The current-value setting of some widgets (like text entry widgets) can be connected directly to application variables by using special options. These options are variable, textvariable, onvalue, offvalue, and value. This connection works both ways: if the variable changes for any reason, the widget it’s connected to will be updated to reflect the new value.

Unfortunately, in the current implementation of tkinter it is not possible to hand over an arbitrary Python variable to a widget through a variable or textvariable option. The only kinds of variables for which this works are variables that are subclassed from a class called Variable, defined in tkinter.

There are many useful subclasses of Variable already defined: StringVar, IntVar, DoubleVar, and BooleanVar. To read the current value of such a variable, call the get() method on it, and to change its value you call the set() method. If you follow this protocol, the widget will always track the value of the variable, with no further intervention on your part.

For example:


```py
import tkinter as tk

class App(tk.Frame):
    def __init__(self, master):
        super().__init__(master)
        self.pack()

        self.entrythingy = tk.Entry()
        self.entrythingy.pack()

        # Create the application variable.
        self.contents = tk.StringVar()
        # Set it to some value.
        self.contents.set("this is a variable")
        # Tell the entry widget to watch this variable.
        self.entrythingy["textvariable"] = self.contents

        # Define a callback for when the user hits return.
        # It prints the current value of the variable.
        self.entrythingy.bind('<Key-Return>',
                             self.print_contents)

    def print_contents(self, event):
        print("Hi. The current entry content is:",
              self.contents.get())

root = tk.Tk()
myapp = App(root)
myapp.mainloop()
```



#### The Window Manager

In Tk, there is a utility command, wm, for interacting with the window manager. Options to the wm command allow you to control things like titles, placement, icon bitmaps, and the like. In tkinter, these commands have been implemented as methods on the Wm class. Toplevel widgets are subclassed from the Wm class, and so can call the Wm methods directly.

To get at the toplevel window that contains a given widget, you can often just refer to the widget’s master. Of course if the widget has been packed inside of a frame, the master won’t represent a toplevel window. To get at the toplevel window that contains an arbitrary widget, you can call the `_root()` method. This method begins with an underscore to denote the fact that this function is part of the implementation, and not an interface to Tk functionality.

Here are some examples of typical usage:


```py
import tkinter as tk

class App(tk.Frame):
    def __init__(self, master=None):
        super().__init__(master)
        self.pack()

# create the application
myapp = App()

#
# here are method calls to the window manager class
#
myapp.master.title("My Do-Nothing Application")
myapp.master.maxsize(1000, 400)

# start the program
myapp.mainloop()
```


#### Tk Option Data Types

➡ `anchor` Legal values are points of the compass: "n", "ne", "e", "se", "s", "sw", "w", "nw", and also "center".

➡ `bitmap` There are eight built-in, named bitmaps: 'error', 'gray25', 'gray50', 'hourglass', 'info', 'questhead', 'question', 'warning'. To specify an X bitmap filename, give the full path to the file, preceded with an @, as in "@/usr/contrib/bitmap/gumby.bit".

➡ `boolean` You can pass integers 0 or 1 or the strings "yes" or "no".

➡ `callback` This is any Python function that takes no arguments. For example:



```py
def print_it():
    print("hi there")
fred["command"] = print_it
```

➡ `color` Colors can be given as the names of X colors in the rgb.txt file, or as strings representing RGB values in 4 bit: "#RGB", 8 bit: "#RRGGBB", 12 bit” "#RRRGGGBBB", or 16 bit "#RRRRGGGGBBBB" ranges, where R,G,B here represent any legal hex digit. See page 160 of Ousterhout’s book for details.

➡ `cursor` The standard X cursor names from cursorfont.h can be used, without the XC_ prefix. For example to get a hand cursor (XC_hand2), use the string "hand2". You can also specify a bitmap and mask file of your own. See page 179 of Ousterhout’s book.

➡ `distance` Screen distances can be specified in either pixels or absolute distances. Pixels are given as numbers and absolute distances as strings, with the trailing character denoting units: c for centimetres, i for inches, m for millimetres, p for printer’s points. For example, 3.5 inches is expressed as "3.5i".

➡ `font` Tk uses a list font name format, such as {courier 10 bold}. Font sizes with positive numbers are measured in points; sizes with negative numbers are measured in pixels.

➡ `geometry` This is a string of the form widthxheight, where width and height are measured in pixels for most widgets (in characters for widgets displaying text). For example: fred["geometry"] = "200x100".

➡ `justIfy` Legal values are the strings: "left", "center", "right", and "fill".

➡ `regIon` This is a string with four space-delimited elements, each of which is a legal distance (see above). For example: "2 3 4 5" and "3i 2i 4.5i 2i" and "3c 2c 4c 10.43c" are all legal regions.

➡ `reLief` Determines what the border style of a widget will be. Legal values are: "raised", "sunken", "flat", "groove", and "ridge".

➡ `scrollcomMand` This is almost always the set() method of some scrollbar widget, but can be any widget method that takes a single argument.

➡ `Wrap` Must be one of: "none", "char", or "word".


#### Bindings and Events

The bind method from the widget command allows you to watch for certain events and to have a callback function trigger when that event type occurs. The form of the bind method is:


```py
def bind(self, sequence, func, add=''):
```

where:
sequenceis a string that denotes the target kind of event. (See the bind man page and page 201 of John Ousterhout’s book for details).funcis a Python function, taking one argument, to be invoked when the event occurs. An Event instance will be passed as the argument. (Functions deployed this way are commonly known as callbacks.)addis optional, either '' or '+'. Passing an empty string denotes that this binding is to replace any other bindings that this event is associated with. Passing a '+' means that this function is to be added to the list of functions bound to this event type.
For example:


```py
def turn_red(self, event):
    event.widget["activeforeground"] = "red"

self.button.bind("<Enter>", self.turn_red)
```


Notice how the widget field of the event is being accessed in the turn_red() callback. This field contains the widget that caught the X event. The following table lists the other event fields you can access, and how they are denoted in Tk, which can be useful when referring to the Tk man pages.

| Tk | Tkinter Event Field | Tk | Tkinter Event Field |
|----|---------------------|----|---------------------|
| %f | focus               | %A | char                |
| %h | height              | %E | send_event          |
| %k | keycode             | %K | keysym              |
| %s | state               | %N | keysym_num          |
| %t | time                | %T | type                |
| %w | width               | %W | widget              |
| %x | x                   | %X | x_root              |
| %y | y                   | %Y | y_root              |


#### The index Parameter

A number of widgets require “index” parameters to be passed. These are used to point at a specific place in a Text widget, or to particular characters in an Entry widget, or to particular menu items in a Menu widget.

➡ Entry widget indexes (index, view index, etc.)
Entry widgets have options that refer to character positions in the text being displayed. You can use these tkinter functions to access these special points in text widgets:

➡ Text widget indexes
The index notation for Text widgets is very rich and is best described in the Tk man pages.Menu indexes (menu.invoke(), menu.entryconfig(), etc.)

Some options and methods for menus manipulate specific menu entries. Anytime a menu index is needed for an option or a parameter, you may pass in:

• an integer which refers to the numeric position of the entry in the widget, counted from the top, starting with 0;

• the string "active", which refers to the menu position that is currently under the cursor;

• the string "last" which refers to the last menu item;

• An integer preceded by @, as in @6, where the integer is interpreted as a y pixel coordinate in the menu’s coordinate system;

• the string "none", which indicates no menu entry at all, most often used with menu.activate() to deactivate all entries, and finally,

• a text string that is pattern matched against the label of the menu entry, as scanned from the top of the menu to the bottom. Note that this index type is considered after all the others, which means that matches for menu items labelled last, active, or none may be interpreted as the above literals, instead.


#### Images

Images of different formats can be created through the corresponding subclass of tkinter.Image:

• BitmapImage for images in XBM format.
• PhotoImage for images in PGM, PPM, GIF and PNG formats. The latter is supported starting with Tk 8.6.

Either type of image is created through either the file or the data option (other options are available as well).

The image object can then be used wherever an image option is supported by some widget (e.g. labels, buttons, menus). In these cases, Tk will not keep a reference to the image. When the last Python reference to the image object is deleted, the image data is deleted as well, and Tk will display an empty box wherever the image was used.

See also:
 The Pillow package adds support for formats such as BMP, JPEG, TIFF, and WebP, among others.
 


### ===🗝 File Handlers

Tk allows you to register and unregister a callback function which will be called from the Tk mainloop when I/O is possible on a file descriptor. Only one handler may be registered per file descriptor. Example code:


```py
import tkinter
widget = tkinter.Tk()
mask = tkinter.READABLE | tkinter.WRITABLE
widget.tk.createfilehandler(file, mask, callback)
...
widget.tk.deletefilehandler(file)
```


This feature is not available on Windows.

Since you don’t know how many bytes are available for reading, you may not want to use the BufferedIOBase or TextIOBase read() or readline() methods, since these will insist on reading a predefined number of bytes. For sockets, the recv() or recvfrom() methods will work fine; for other files, use raw reads or os.read(file.fileno(), maxbytecount).

➡ `Widget.tk.createfilehandler(file, mask, func)`
Registers the file handler callback function func. The file argument may either be an object with a fileno() method (such as a file or socket object), or an integer file descriptor. The mask argument is an ORed combination of any of the three constants below. The callback is called as follows:


    callback(file, mask)


➡ `Widget.tk.deletefilehandler(file)`
Unregisters a file handler.

➡ `tkinter.READABLEtkinter.WRITABLEtkinter.EXCEPTION`
Constants used in the mask arguments.



## ==⚡ • tkinter.colorchooser — Color choosing dialog
## ==⚡ • tkinter.font — Tkinter font wrapper
## ==⚡ • Tkinter Dialogs
## ==⚡ • tkinter.messagebox — Tkinter message prompts
## ==⚡ • tkinter.scrolledtext — Scrolled Text Widget
## ==⚡ • tkinter.dnd — Drag and drop support
## ==⚡ • tkinter.ttk — Tk themed widgets
## ==⚡ • tkinter.tix — Extension widgets for Tk
## ==⚡ • IDLE

# =🚩 Development Tools
- 3.10.2 Documentation » The Python Standard Library » Development Tools

The modules described in this chapter help you write software. For example, the pydoc module takes a module and generates documentation based on the module’s contents. The doctest and unittest modules contains frameworks for writing unit tests that automatically exercise code and verify that the expected output is produced. 2to3 can translate Python 2.x source code into valid Python 3.x code.

The list of modules described in this chapter is:

### • typing — Support for type hints
◦Relevant PEPs
◦Type aliases
◦NewType
◦Callable
◦Generics
◦User-defined generic types
◦The Any type
◦Nominal vs structural subtyping
◦Module contents
◾Special typing primitives
◾Special types
◾Special forms
◾Building generic types
◾Other special directives

◾Generic concrete collections
◾Corresponding to built-in types
◾Corresponding to types in collections
◾Other concrete types

◾Abstract Base Classes
◾Corresponding to collections in collections.abc
◾Corresponding to other types in collections.abc
◾Asynchronous programming
◾Context manager types

◾Protocols
◾Functions and decorators
◾Introspection helpers
◾Constant


### • pydoc — Documentation generator and online help system
### • Python Development Mode
### • Effects of the Python Development Mode
### • ResourceWarning Example
### • Bad file descriptor error example
### • doctest — Test interactive Python examples
◦Simple Usage: Checking Examples in Docstrings

◦Simple Usage: Checking Examples in a Text File
◦How It Works
◾Which Docstrings Are Examined?
◾How are Docstring Examples Recognized?
◾What’s the Execution Context?
◾What About Exceptions?
◾Option Flags
◾Directives
◾Warnings

◦Basic API
◦Unittest API
◦Advanced API
◾DocTest Objects
◾Example Objects
◾DocTestFinder objects
◾DocTestParser objects
◾DocTestRunner objects
◾OutputChecker objects

◦Debugging
◦Soapbox

### • unittest — Unit testing framework
◦Basic example
◦Command-Line Interface
◾Command-line options

◦Test Discovery
◦Organizing test code
◦Re-using old test code
◦Skipping tests and expected failures
◦Distinguishing test iterations using subtests
◦Classes and functions
◾Test cases
◾Deprecated aliases

◾Grouping tests
◾Loading and running tests
◾load_tests Protocol


◦Class and Module Fixtures
◾setUpClass and tearDownClass
◾setUpModule and tearDownModule

◦Signal Handling

### • unittest.mock — mock object library
◦Quick Guide
◦The Mock Class
◾Calling
◾Deleting Attributes
◾Mock names and the name attribute
◾Attaching Mocks as Attributes

◦The patchers
◾patch
◾patch.object
◾patch.dict
◾patch.multiple
◾patch methods: start and stop
◾patch builtins
◾TEST_PREFIX
◾Nesting Patch Decorators
◾Where to patch
◾Patching Descriptors and Proxy Objects

◦MagicMock and magic method support
◾Mocking Magic Methods
◾Magic Mock

◦Helpers
◾sentinel
◾DEFAULT
◾call
◾create_autospec
◾ANY
◾FILTER_DIR
◾mock_open
◾Autospeccing
◾Sealing mocks


### • unittest.mock — getting started
◦Using Mock
◾Mock Patching Methods
◾Mock for Method Calls on an Object
◾Mocking Classes
◾Naming your mocks
◾Tracking all Calls
◾Setting Return Values and Attributes
◾Raising exceptions with mocks
◾Side effect functions and iterables
◾Mocking asynchronous iterators
◾Mocking asynchronous context manager
◾Creating a Mock from an Existing Object

◦Patch Decorators
◦Further Examples
◾Mocking chained calls
◾Partial mocking
◾Mocking a Generator Method
◾Applying the same patch to every test method
◾Mocking Unbound Methods
◾Checking multiple calls with mock
◾Coping with mutable arguments
◾Nesting Patches
◾Mocking a dictionary with MagicMock
◾Mock subclasses and their attributes
◾Mocking imports with patch.dict
◾Tracking order of calls and less verbose call assertions
◾More complex argument matching


### • 2to3 - Automated Python 2 to 3 code translation
◦Using 2to3
◦Fixers
◦lib2to3 - 2to3’s library

### • test — Regression tests package for Python
◦Writing Unit Tests for the test package
◦Running tests using the command-line interface

### • test.support — Utilities for the Python test suite
### • test.support.socket_helper — Utilities for socket tests
### • test.support.script_helper — Utilities for the Python execution tests
### • test.support.bytecode_helper — Support tools for testing correct bytecode generation
### • test.support.threading_helper — Utilities for threading tests
### • test.support.os_helper — Utilities for os tests
### • test.support.import_helper — Utilities for import tests
### • test.support.warnings_helper — Utilities for warnings tests


## ==⚡  typing — Support for type hints

## ==⚡  pydoc — Documentation generator and online help system


Source code: Lib/pydoc.py

```sh
# Python 3.10 Module Docs (64-bit)
C:\Python310\python.exe -m pydoc -b

> python -m pydoc
pydoc - the Python documentation tool

pydoc <name> ...
    Show text documentation on something.  <name> may be the name of a
    Python keyword, topic, function, module, or package, or a dotted
    reference to a class or function within a module or module in a
    package.  If <name> contains a '/', it is used as the path to a
    Python source file to document. If name is 'keywords', 'topics',
    or 'modules', a listing of these things is displayed.

pydoc -k <keyword>
    Search for a keyword in the synopsis lines of all available modules.

pydoc -n <hostname>
    Start an HTTP server with the given hostname (default: localhost).

pydoc -p <port>
    Start an HTTP server on the given port on the local machine.  Port
    number 0 can be used to get an arbitrary unused port.

pydoc -b
    Start an HTTP server on an arbitrary unused port and open a Web browser
    to interactively browse documentation.  This option can be used in
    combination with -n and/or -p.

pydoc -w <name> ...
    Write out the HTML documentation for a module to a file in the current
    directory.  If <name> contains a '/', it is treated as a filename; if
    it names a directory, documentation is written for all the contents.
```


The `pydoc` module automatically generates documentation from Python modules. The documentation can be presented as pages of text on the console, served to a web browser, or saved to HTML files.

For modules, classes, functions and methods, the displayed documentation is derived from the `docstring` (i.e. the __doc__ attribute) of the object, and recursively of its documentable members. If there is no `docstring`, `pydoc` tries to obtain a description from the block of comment lines just above the definition of the class, function or method in the source file, or at the top of the module (see `inspect.getcomments()`).

The built-in function `help()` invokes the online help system in the interactive interpreter, which uses `pydoc` to generate its documentation as text on the console. The same text documentation can also be viewed from outside the Python interpreter by running `pydoc` as a script at the operating system’s command prompt. For example, running


    pydoc sys


at a shell prompt will display documentation on the sys module, in a style similar to the manual pages shown by the Unix `man` command. The argument to `pydoc` can be the name of a function, module, or package, or a dotted reference to a class, method, or function within a module or module in a package. If the argument to `pydoc` looks like a path (that is, it contains the path separator for your operating system, such as a slash in Unix), and refers to an existing Python source file, then documentation is produced for that file.

Note:
 In order to find objects and their documentation, `pydoc` imports the module(s) to be documented. Therefore, any code on module level will be executed on that occasion. Use an if __name__ == '__main__': guard to only execute code when a file is invoked as a script and not just imported.
 

When printing output to the console, `pydoc` attempts to paginate the output for easier reading. If the PAGER environment variable is set, `pydoc` will use its value as a pagination program.

Specifying a `-w` flag before the argument will cause HTML documentation to be written out to a file in the current directory, instead of displaying text on the console.

Specifying a `-k` flag before the argument will search the synopsis lines of all available modules for the keyword given as the argument, again in a manner similar to the Unix `man` command. The synopsis line of a module is the first line of its documentation string.

You can also use `pydoc` to start an HTTP server on the local machine that will serve documentation to visiting web browsers. `pydoc -p 1234` will start a HTTP server on port 1234, allowing you to browse the documentation at http://localhost:1234/ in your preferred web browser. Specifying 0 as the port number will select an arbitrary unused port.

`pydoc -n <hostname>` will start the server listening at the given hostname. By default the hostname is ‘localhost’ but if you want the server to be reached from other machines, you may want to change the host name that the server responds to. During development this is especially useful if you want to run `pydoc` from within a container.

`pydoc -b` will start the server and additionally open a web browser to a module index page. Each served page has a navigation bar at the top where you can Get help on an individual item, Search all modules with a keyword in their synopsis line, and go to the Module index, Topics and Keywords pages.

When `pydoc` generates documentation, it uses the current environment and path to locate modules. Thus, invoking `pydoc` spam documents precisely the version of the module you would get if you started the Python interpreter and typed import spam.

Module docs for core modules are assumed to reside in https://docs.python.org/X.Y/library/ where X and Y are the major and minor version numbers of the Python interpreter. This can be overridden by setting the `PYTHONDOCS` environment variable to a different URL or to a local directory containing the Library Reference Manual pages.

Changed in version 3.2: Added the -b option.
Changed in version 3.3: The -g command line option was removed.
Changed in version 3.4: `pydoc` now uses `inspect.signature()` rather than `inspect.getfullargspec()` to extract signature information from callables.
Changed in version 3.7: Added the `-n` option.

## ==⚡  Python Development Mode

## ==⚡  Effects of the Python Development Mode

## ==⚡  ResourceWarning Example

## ==⚡  Bad file descriptor error example

## ==⚡  doctest — Test interactive Python examples


Source code: Lib/doctest.py


The `doctest` module searches for pieces of text that look like interactive Python sessions, and then executes those sessions to verify that they work exactly as shown. There are several common ways to use doctest:
↪• To check that a module’s docstrings are up-to-date by verifying that all interactive examples still work as documented.
↪• To perform regression testing by verifying that interactive examples from a test file or a test object work as expected.
↪• To write tutorial documentation for a package, liberally illustrated with input-output examples. Depending on whether the examples or the expository text are emphasized, this has the flavor of “literate testing” or “executable documentation”.

Here’s a complete but small example module:


```py
"""
This is the "example" module.

The example module supplies one function, factorial().  For example,

>>> factorial(5)
120
"""

def factorial(n):
    """Return the factorial of n, an exact integer >= 0.

    >>> [factorial(n) for n in range(6)]
    [1, 1, 2, 6, 24, 120]
    >>> factorial(30)
    265252859812191058636308480000000
    >>> factorial(-1)
    Traceback (most recent call last):
        ...
    ValueError: n must be >= 0

    Factorials of floats are OK, but the float must be an exact integer:
    >>> factorial(30.1)
    Traceback (most recent call last):
        ...
    ValueError: n must be exact integer
    >>> factorial(30.0)
    265252859812191058636308480000000

    It must also not be ridiculously large:
    >>> factorial(1e100)
    Traceback (most recent call last):
        ...
    OverflowError: n too large
    """

    import math
    if not n >= 0:
        raise ValueError("n must be >= 0")
    if math.floor(n) != n:
        raise ValueError("n must be exact integer")
    if n+1 == n:  # catch a value like 1e300
        raise OverflowError("n too large")
    result = 1
    factor = 2
    while factor <= n:
        result *= factor
        factor += 1
    return result


if __name__ == "__main__":
    import doctest
    doctest.testmod()
```


If you run `example.py` directly from the command line, doctest works its magic:


```sh
$ python example.py
$
```


There’s no output! That’s normal, and it means all the examples worked. Pass `-v` to the script, and doctest prints a detailed log of what it’s trying, and prints a summary at the end:


```sh
$ python example.py -v
Trying:
    factorial(5)
Expecting:
    120
ok
Trying:
    [factorial(n) for n in range(6)]
Expecting:
    [1, 1, 2, 6, 24, 120]
ok
```


And so on, eventually ending with:


```sh
Trying:
    factorial(1e100)
Expecting:
    Traceback (most recent call last):
        ...
    OverflowError: n too large
ok
2 items passed all tests:
   1 tests in __main__
   8 tests in __main__.factorial
9 tests in 2 items.
9 passed and 0 failed.
Test passed.
$
```


That’s all you need to know to start making productive use of doctest! Jump in. The following sections provide full details. Note that there are many examples of doctests in the standard Python test suite and libraries. Especially useful examples can be found in the standard test file `Lib/test/test_doctest.py`.


### ===🗝 Simple Usage: Checking Examples in Docstrings

The simplest way to start using doctest (but not necessarily the way you’ll continue to do it) is to end each module M with:


    if __name__ == "__main__":
        import doctest
        doctest.testmod()


doctest then examines docstrings in module M.

Running the module as a script causes the examples in the docstrings to get executed and verified:


    python M.py


This won’t display anything unless an example fails, in which case the failing example(s) and the cause(s) of the failure(s) are printed to stdout, and the final line of output is ***Test Failed*** N failures., where N is the number of examples that failed.

Run it with the -v switch instead:


    python M.py -v


and a detailed report of all examples tried is printed to standard output, along with assorted summaries at the end.

You can force verbose mode by passing verbose=True to testmod(), or prohibit it by passing verbose=False. In either of those cases, sys.argv is not examined by testmod() (so passing -v or not has no effect).

There is also a command line shortcut for running testmod(). You can instruct the Python interpreter to run the doctest module directly from the standard library and pass the module name(s) on the command line:


    python -m doctest -v example.py


This will import example.py as a standalone module and run testmod() on it. Note that this may not work correctly if the file is part of a package and imports other submodules from that package.

For more information on testmod(), see section Basic API.


### ===🗝 Simple Usage: Checking Examples in a Text File

Another simple application of doctest is testing interactive examples in a text file. This can be done with the `testfile()` function:


```py
import doctest
doctest.testfile("example.txt")
```


That short script executes and verifies any interactive Python examples contained in the file `example.txt`. The file content is treated as if it were a single giant docstring; the file doesn’t need to contain a Python program! For example, perhaps `example.txt` contains this:


The ``example`` module
======================

Using ``factorial``
-------------------

This is an example text file in reStructuredText format.  First import
``factorial`` from the ``example`` module:

    >>> from example import factorial

Now use it:

    >>> factorial(6)
    120


Running `doctest.testfile("example.txt")` then finds the error in this documentation:


```sh
File "./example.txt", line 14, in example.txt
Failed example:
    factorial(6)
Expected:
    120
Got:
    720
```


As with `testmod()`, `testfile()` won’t display anything unless an example fails. If an example does fail, then the failing example(s) and the cause(s) of the failure(s) are printed to stdout, using the same format as `testmod()`.

By default, `testfile()` looks for files in the calling module’s directory. See section Basic API for a description of the optional arguments that can be used to tell it to look for files in other locations.

Like `testmod()`, `testfile()`’s verbosity can be set with the `-v` command-line switch or with the optional keyword argument verbose.

There is also a command line shortcut for running `testfile()`. You can instruct the Python interpreter to run the doctest module directly from the standard library and pass the file name(s) on the command line:


    python -m doctest -v example.txt


Because the file name does not end with `.py`, doctest infers that it must be run with `testfile()`, not `testmod()`.

For more information on `testfile()`, see section Basic API.


### ===🗝 How It Works

This section examines in detail how doctest works: which docstrings it looks at, how it finds interactive examples, what execution context it uses, how it handles exceptions, and how option flags can be used to control its behavior. This is the information that you need to know to write doctest examples; for information about actually running doctest on these examples, see the following sections.


### ===🗝 Which Docstrings Are Examined?

The module docstring, and all function, class and method docstrings are searched. Objects imported into the module are not searched.

In addition, if M.__test__ exists and “is true”, it must be a `dict`, and each entry maps a (string) name to a function object, class object, or string. Function and class object docstrings found from M.__test__ are searched, and strings are treated as if they were docstrings. In output, a key K in M.__test__ appears with name


<name of M>.__test__.K


Any classes found are recursively searched similarly, to test docstrings in their contained methods and nested classes.


CPython implementation detail: Prior to version 3.4, extension modules written in C were not fully searched by doctest.


### ===🗝 How are Docstring Examples Recognized?

In most cases a copy-and-paste of an interactive console session works fine, but doctest isn’t trying to do an exact emulation of any specific Python shell.


>>> # comments are ignored
>>> x = 12
>>> x
12
>>> if x == 13:
...     print("yes")
... else:
...     print("no")
...     print("NO")
...     print("NO!!!")
...
no
NO
NO!!!
>>>


Any expected output must immediately follow the final '>>> ' or '... ' line containing the code, and the expected output (if any) extends to the next '>>> ' or all-whitespace line.

The fine print:

↪• Expected output cannot contain an all-whitespace line, since such a line is taken to signal the end of expected output. If expected output does contain a blank line, put <BLANKLINE> in your doctest example each place a blank line is expected.


↪• All hard tab characters are expanded to spaces, using 8-column tab stops. Tabs in output generated by the tested code are not modified. Because any hard tabs in the sample output are expanded, this means that if the code output includes hard tabs, the only way the doctest can pass is if the `NORMALIZE_WHITESPACE` option or directive is in effect. Alternatively, the test can be rewritten to capture the output and compare it to an expected value as part of the test. This handling of tabs in the source was arrived at through trial and error, and has proven to be the least error prone way of handling them. It is possible to use a different algorithm for handling tabs by writing a custom `DocTestParser` class.


↪• Output to `stdout` is captured, but not output to `stderr` (exception tracebacks are captured via a different means).


↪• If you continue a line via backslashing in an interactive session, or for any other reason use a backslash, you should use a raw docstring, which will preserve your backslashes exactly as you type them:


>>> def f(x):
...     r'''Backslashes in a raw docstring: m\n'''
>>> print(f.__doc__)
Backslashes in a raw docstring: m\n


Otherwise, the backslash will be interpreted as part of the string. For example, the \n above would be interpreted as a newline character. Alternatively, you can double each backslash in the doctest version (and not use a raw string):


>>> def f(x):
...     '''Backslashes in a raw docstring: m\\n'''
>>> print(f.__doc__)
Backslashes in a raw docstring: m\n



↪• The starting column doesn’t matter:


>>> assert "Easy!"
      >>> import math
          >>> math.floor(1.9)
          1


and as many leading whitespace characters are stripped from the expected output as appeared in the initial '>>> ' line that started the example.



### ===🗝 What’s the Execution Context?

By default, each time doctest finds a docstring to test, it uses a shallow copy of M’s globals, so that running tests doesn’t change the module’s real globals, and so that one test in M can’t leave behind crumbs that accidentally allow another test to work. This means examples can freely use any names defined at top-level in M, and names defined earlier in the docstring being run. Examples cannot see names defined in other docstrings.

You can force use of your own dict as the execution context by passing `globs=your_dict` to `testmod()` or `testfile()` instead.


### ===🗝 What About Exceptions?

No problem, provided that the traceback is the only output produced by the example: just paste in the traceback. [1] Since tracebacks contain details that are likely to change rapidly (for example, exact file paths and line numbers), this is one case where doctest works hard to be flexible in what it accepts.

Simple example:


>>> [1, 2, 3].remove(42)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ValueError: list.remove(x): x not in list


That doctest succeeds if ValueError is raised, with the `list.remove(x): x not in list` detail as shown.

The expected output for an exception must start with a traceback header, which may be either of the following two lines, indented the same as the first line of the example:


Traceback (most recent call last):
Traceback (innermost last):


The traceback header is followed by an optional traceback stack, whose contents are ignored by doctest. The traceback stack is typically omitted, or copied verbatim from an interactive session.

The traceback stack is followed by the most interesting part: the line(s) containing the exception type and detail. This is usually the last line of a traceback, but can extend across multiple lines if the exception has a multi-line detail:


>>> raise ValueError('multi\n    line\ndetail')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ValueError: multi
    line
detail


The last three lines (starting with ValueError) are compared against the exception’s type and detail, and the rest are ignored.

Best practice is to omit the traceback stack, unless it adds significant documentation value to the example. So the last example is probably better as:


>>> raise ValueError('multi\n    line\ndetail')
Traceback (most recent call last):
    ...
ValueError: multi
    line
detail


Note that tracebacks are treated very specially. In particular, in the rewritten example, the use of ... is independent of doctest’s ELLIPSIS option. The ellipsis in that example could be left out, or could just as well be three (or three hundred) commas or digits, or an indented transcript of a Monty Python skit.

Some details you should read once, but won’t need to remember:

↪• Doctest can’t guess whether your expected output came from an exception traceback or from ordinary printing. So, e.g., an example that expects ValueError: 42 is prime will pass whether ValueError is actually raised or if the example merely prints that traceback text. In practice, ordinary output rarely begins with a traceback header line, so this doesn’t create real problems.
↪• Each line of the traceback stack (if present) must be indented further than the first line of the example, or start with a non-alphanumeric character. The first line following the traceback header indented the same and starting with an alphanumeric is taken to be the start of the exception detail. Of course this does the right thing for genuine tracebacks.
↪• When the `IGNORE_EXCEPTION_DETAIL` doctest option is specified, everything following the leftmost colon and any module information in the exception name is ignored.
↪• The interactive shell omits the traceback header line for some `SyntaxErrors`. But doctest uses the traceback header line to distinguish exceptions from non-exceptions. So in the rare case where you need to test a SyntaxError that omits the traceback header, you will need to manually add the traceback header line to your test example.

↪• For some `SyntaxErrors`, Python displays the character position of the syntax error, using a ^ marker:


>>> 1 1
  File "<stdin>", line 1
    1 1
      ^
SyntaxError: invalid syntax


Since the lines showing the position of the error come before the exception type and detail, they are not checked by doctest. For example, the following test would pass, even though it puts the ^ marker in the wrong location:


>>> 1 1
  File "<stdin>", line 1
    1 1
    ^
SyntaxError: invalid syntax




### ===🗝 Option Flags

A number of option flags control various aspects of doctest’s behavior. Symbolic names for the flags are supplied as module constants, which can be bitwise ORed together and passed to various functions. The names can also be used in doctest directives, and may be passed to the doctest command line interface via the -o option.


New in version 3.4: The -o command line option.

The first group of options define test semantics, controlling aspects of how doctest decides whether actual output matches an example’s expected output:

➡ `doctest.DONT_ACCEPT_TRUE_FOR_1`
By default, if an expected output block contains just 1, an actual output block containing just 1 or just True is considered to be a match, and similarly for 0 versus False. When `DONT_ACCEPT_TRUE_FOR_1` is specified, neither substitution is allowed. The default behavior caters to that Python changed the return type of many functions from integer to boolean; doctests expecting “little integer” output still work in these cases. This option will probably go away, but not for several years.

➡ `doctest.DONT_ACCEPT_BLANKLINE`
By default, if an expected output block contains a line containing only the string <BLANKLINE>, then that line will match a blank line in the actual output. Because a genuinely blank line delimits the expected output, this is the only way to communicate that a blank line is expected. When DONT_ACCEPT_BLANKLINE is specified, this substitution is not allowed.

➡ `doctest.NORMALIZE_WHITESPACE`
When specified, all sequences of whitespace (blanks and newlines) are treated as equal. Any sequence of whitespace within the expected output will match any sequence of whitespace within the actual output. By default, whitespace must match exactly. NORMALIZE_WHITESPACE is especially useful when a line of expected output is very long, and you want to wrap it across multiple lines in your source.

➡ `doctest.ELLIPSIS`
When specified, an ellipsis marker (...) in the expected output can match any substring in the actual output. This includes substrings that span line boundaries, and empty substrings, so it’s best to keep usage of this simple. Complicated uses can lead to the same kinds of “oops, it matched too much!” surprises that .* is prone to in regular expressions.

➡ `doctest.IGNORE_EXCEPTION_DETAIL`
When specified, an example that expects an exception passes if an exception of the expected type is raised, even if the exception detail does not match. For example, an example expecting ValueError: 42 will pass if the actual exception raised is `ValueError: 3*14`, but will fail, e.g., if `TypeError` is raised.

It will also ignore the module name used in Python 3 doctest reports. Hence both of these variations will work with the flag specified, regardless of whether the test is run under Python 2.7 or Python 3.2 (or later versions):


>>> raise CustomError('message')
Traceback (most recent call last):
CustomError: message

>>> raise CustomError('message')
Traceback (most recent call last):
my_module.CustomError: message


Note that ELLIPSIS can also be used to ignore the details of the exception message, but such a test may still fail based on whether or not the module details are printed as part of the exception name. Using `IGNORE_EXCEPTION_DETAIL` and the details from Python 2.3 is also the only clear way to write a doctest that doesn’t care about the exception detail yet continues to pass under Python 2.3 or earlier (those releases do not support doctest directives and ignore them as irrelevant comments). For example:


>>> (1, 2)[3] = 'moo'
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: object doesn't support item assignment


passes under Python 2.3 and later Python versions with the flag specified, even though the detail changed in Python 2.4 to say “does not” instead of “doesn’t”.


Changed in version 3.2: IGNORE_EXCEPTION_DETAIL now also ignores any information relating to the module containing the exception under test.

➡ `doctest.SKIP`
When specified, do not run the example at all. This can be useful in contexts where doctest examples serve as both documentation and test cases, and an example should be included for documentation purposes, but should not be checked. E.g., the example’s output might be random; or the example might depend on resources which would be unavailable to the test driver.

The SKIP flag can also be used for temporarily “commenting out” examples.

➡ `doctest.COMPARISON_FLAGS`
A bitmask or’ing together all the comparison flags above.

The second group of options controls how test failures are reported:

➡ `doctest.REPORT_UDIFF`
When specified, failures that involve multi-line expected and actual outputs are displayed using a unified diff.

➡ `doctest.REPORT_CDIFF`
When specified, failures that involve multi-line expected and actual outputs will be displayed using a context diff.

➡ `doctest.REPORT_NDIFF`
When specified, differences are computed by difflib.Differ, using the same algorithm as the popular ndiff.py utility. This is the only method that marks differences within lines as well as across lines. For example, if a line of expected output contains digit 1 where actual output contains letter l, a line is inserted with a caret marking the mismatching column positions.

➡ `doctest.REPORT_ONLY_FIRST_FAILURE`
When specified, display the first failing example in each doctest, but suppress output for all remaining examples. This will prevent doctest from reporting correct examples that break because of earlier failures; but it might also hide incorrect examples that fail independently of the first failure. When REPORT_ONLY_FIRST_FAILURE is specified, the remaining examples are still run, and still count towards the total number of failures reported; only the output is suppressed.

➡ `doctest.FAIL_FAST`
When specified, exit after the first failing example and don’t attempt to run the remaining examples. Thus, the number of failures reported will be at most 1. This flag may be useful during debugging, since examples after the first failure won’t even produce debugging output.

The doctest command line accepts the option -f as a shorthand for -o FAIL_FAST.


New in version 3.4.

➡ `doctest.REPORTING_FLAGS`
A bitmask or’ing together all the reporting flags above.

There is also a way to register new option flag names, though this isn’t useful unless you intend to extend doctest internals via subclassing:

➡ `doctest.register_optionflag(name)`
Create a new option flag with a given name, and return the new flag’s integer value. register_optionflag() can be used when subclassing OutputChecker or DocTestRunner to create new options that are supported by your subclasses. register_optionflag() should always be called using the following idiom:


MY_FLAG = register_optionflag('MY_FLAG')



### ===🗝 Directives

Doctest directives may be used to modify the option flags for an individual example. Doctest directives are special Python comments following an example’s source code:

directive             ::=  "#" "doctest:" directive_options
directive_options     ::=  directive_option ("," directive_option)\*
directive_option      ::=  on_or_off directive_option_name
on_or_off             ::=  "+" \| "-"
directive_option_name ::=  "DONT_ACCEPT_BLANKLINE" \| "NORMALIZE_WHITESPACE" \| ...


Whitespace is not allowed between the + or - and the directive option name. The directive option name can be any of the option flag names explained above.

An example’s doctest directives modify doctest’s behavior for that single example. Use + to enable the named behavior, or - to disable it.

For example, this test passes:


>>> print(list(range(20))) 
[0,   1,  2,  3,  4,  5,  6,  7,  8,  9,
10,  11, 12, 13, 14, 15, 16, 17, 18, 19]


Without the directive it would fail, both because the actual output doesn’t have two blanks before the single-digit list elements, and because the actual output is on a single line. This test also passes, and also requires a directive to do so:


>>> print(list(range(20))) 
[0, 1, ..., 18, 19]


Multiple directives can be used on a single physical line, separated by commas:


>>> print(list(range(20))) 
[0,    1, ...,   18,    19]


If multiple directive comments are used for a single example, then they are combined:


>>> print(list(range(20))) 
...                        
[0,    1, ...,   18,    19]


As the previous example shows, you can add ... lines to your example containing only directives. This can be useful when an example is too long for a directive to comfortably fit on the same line:


>>> print(list(range(5)) + list(range(10, 20)) + list(range(30, 40)))
... 
[0, ..., 4, 10, ..., 19, 30, ..., 39]


Note that since all options are disabled by default, and directives apply only to the example they appear in, enabling options (via + in a directive) is usually the only meaningful choice. However, option flags can also be passed to functions that run doctests, establishing different defaults. In such cases, disabling an option via - in a directive can be useful.


### ===🗝 Warnings

doctest is serious about requiring exact matches in expected output. If even a single character doesn’t match, the test fails. This will probably surprise you a few times, as you learn exactly what Python does and doesn’t guarantee about output. For example, when printing a set, Python doesn’t guarantee that the element is printed in any particular order, so a test like


>>> foo()
{"Hermione", "Harry"}


is vulnerable! One workaround is to do


>>> foo() == {"Hermione", "Harry"}
True


instead. Another is to do


>>> d = sorted(foo())
>>> d
['Harry', 'Hermione']


Note:
 Before Python 3.6, when printing a dict, Python did not guarantee that the key-value pairs was printed in any particular order.
 

There are others, but you get the idea.

Another bad idea is to print things that embed an object address, like


>>> id(1.0) # certain to fail some of the time
7948648
>>> class C: pass
>>> C()   # the default repr() for instances embeds an address
<__main__.C instance at 0x00AC18F0>


The ELLIPSIS directive gives a nice approach for the last example:


>>> C() 
<__main__.C instance at 0x...>


Floating-point numbers are also subject to small output variations across platforms, because Python defers to the platform C library for float formatting, and C libraries vary widely in quality here.


>>> 1./7  # risky
0.14285714285714285
>>> print(1./7) # safer
0.142857142857
>>> print(round(1./7, 6)) # much safer
0.142857


Numbers of the form I/2.**J are safe across all platforms, and I often contrive doctest examples to produce numbers of that form:


>>> 3./4  # utterly safe
0.75


Simple fractions are also easier for people to understand, and that makes for better documentation.


### ===🗝 Basic API

The functions `testmod()` and `testfile()` provide a simple interface to doctest that should be sufficient for most basic uses. For a less formal introduction to these two functions, see sections Simple Usage: Checking Examples in Docstrings and Simple Usage: Checking Examples in a Text File.

➡ `doctest.testfile(filename, module_relative=True, name=None, package=None, globs=None, verbose=None, report=True, optionflags=0, extraglobs=None, raise_on_error=False, parser=DocTestParser(), encoding=None)`

All arguments except `filename` are optional, and should be specified in keyword form.

Test examples in the file named filename. Return (failure_count, test_count).

Optional argument `module_relative` specifies how the filename should be interpreted:

↪• If `module_relative` is True (the default), then filename specifies an OS-independent module-relative path. By default, this path is relative to the calling module’s directory; but if the package argument is specified, then it is relative to that package. To ensure OS-independence, filename should use / characters to separate path segments, and may not be an absolute path (i.e., it may not begin with /).
↪• If `module_relative` is False, then filename specifies an OS-specific path. The path may be absolute or relative; relative paths are resolved with respect to the current working directory.

Optional argument `name` gives the name of the test; by default, or if None, os.path.basename(filename) is used.

Optional argument `package` is a Python package or the name of a Python package whose directory should be used as the base directory for a module-relative filename. If no package is specified, then the calling module’s directory is used as the base directory for module-relative filenames. It is an error to specify package if module_relative is False.

Optional argument `globs` gives a dict to be used as the globals when executing examples. A new shallow copy of this dict is created for the doctest, so its examples start with a clean slate. By default, or if None, a new empty dict is used.

Optional argument `extraglobs` gives a dict merged into the globals used to execute examples. This works like dict.update(): if globs and extraglobs have a common key, the associated value in extraglobs appears in the combined dict. By default, or if None, no extra globals are used. This is an advanced feature that allows parameterization of doctests. For example, a doctest can be written for a base class, using a generic name for the class, then reused to test any number of subclasses by passing an extraglobs dict mapping the generic name to the subclass to be tested.

Optional argument `verbose` prints lots of stuff if true, and prints only failures if false; by default, or if None, it’s true if and only if '-v' is in sys.argv.

Optional argument `report` prints a summary at the end when true, else prints nothing at the end. In verbose mode, the summary is detailed, else the summary is very brief (in fact, empty if all tests passed).

Optional argument `optionflags` (default value 0) takes the bitwise OR of option flags. See section Option Flags.

Optional argument `raise_on_error` defaults to false. If true, an exception is raised upon the first failure or unexpected exception in an example. This allows failures to be post-mortem debugged. Default behavior is to continue running examples.

Optional argument `parser` specifies a DocTestParser (or subclass) that should be used to extract tests from the files. It defaults to a normal parser (i.e., DocTestParser()).

Optional argument `encoding` specifies an encoding that should be used to convert the file to unicode.

➡ `doctest.testmod(m=None, name=None, globs=None, verbose=None, report=True, optionflags=0, extraglobs=None, raise_on_error=False, exclude_empty=False)`
All arguments are optional, and all except for m should be specified in keyword form.

Test examples in docstrings in functions and classes reachable from module m (or module __main__ if m is not supplied or is None), starting with m.__doc__.

Also test examples reachable from dict m.__test__, if it exists and is not None. m.__test__ maps names (strings) to functions, classes and strings; function and class docstrings are searched for examples; strings are searched directly, as if they were docstrings.

Only docstrings attached to objects belonging to module m are searched.

Return (failure_count, test_count).

Optional argument `name` gives the name of the module; by default, or if None, m.__name__ is used.

Optional argument `exclude_empty` defaults to false. If true, objects for which no doctests are found are excluded from consideration. The default is a backward compatibility hack, so that code still using doctest.master.summarize() in conjunction with testmod() continues to get output for objects with no tests. The exclude_empty argument to the newer DocTestFinder constructor defaults to true.

Optional arguments` extraglobs`, verbose, report, optionflags, raise_on_error, and globs are the same as for function testfile() above, except that globs defaults to m.__dict__.

➡ `doctest.run_docstring_examples(f, globs, verbose=False, name="NoName", compileflags=None, optionflags=0)`
Test examples associated with object f; for example, f may be a string, a module, a function, or a class object.

A shallow copy of dictionary argument globs is used for the execution context.

Optional argument `name` is used in failure messages, and defaults to "NoName".

If optional argument verbose is true, output is generated even if there are no failures. By default, output is generated only in case of an example failure.

Optional argument `compileflags` gives the set of flags that should be used by the Python compiler when running the examples. By default, or if None, flags are deduced corresponding to the set of future features found in globs.

Optional argument `optionflags` works as for function testfile() above.


### ===🗝 Unittest API

As your collection of doctest’ed modules grows, you’ll want a way to run all their doctests systematically. doctest provides two functions that can be used to create unittest test suites from modules and text files containing doctests. To integrate with unittest test discovery, include a `load_tests()` function in your test module:


```py
import unittest
import doctest
import my_module_with_doctests

def load_tests(loader, tests, ignore):
    tests.addTests(doctest.DocTestSuite(my_module_with_doctests))
    return tests
```


There are two main functions for creating unittest.TestSuite instances from text files and modules with doctests:

➡ `doctest.DocFileSuite(*paths, module_relative=True, package=None, setUp=None, tearDown=None, globs=None, optionflags=0, parser=DocTestParser(), encoding=None)`
Convert doctest tests from one or more text files to a unittest.TestSuite.

The returned unittest.TestSuite is to be run by the unittest framework and runs the interactive examples in each file. If an example in any file fails, then the synthesized unit test fails, and a failureException exception is raised showing the name of the file containing the test and a (sometimes approximate) line number.

Pass one or more paths (as strings) to text files to be examined.

Options may be provided as keyword arguments:

Optional argument `module_relative` specifies how the filenames in paths should be interpreted:
↪• If module_relative is True (the default), then each filename in paths specifies an OS-independent module-relative path. By default, this path is relative to the calling module’s directory; but if the package argument is specified, then it is relative to that package. To ensure OS-independence, each filename should use / characters to separate path segments, and may not be an absolute path (i.e., it may not begin with /).
↪• If module_relative is False, then each filename in paths specifies an OS-specific path. The path may be absolute or relative; relative paths are resolved with respect to the current working directory.

Optional argument `package` is a Python package or the name of a Python package whose directory should be used as the base directory for module-relative filenames in paths. If no package is specified, then the calling module’s directory is used as the base directory for module-relative filenames. It is an error to specify package if module_relative is False.

Optional argument `setUp` specifies a set-up function for the test suite. This is called before running the tests in each file. The setUp function will be passed a DocTest object. The setUp function can access the test globals as the globs attribute of the test passed.

Optional argument `tearDown` specifies a tear-down function for the test suite. This is called after running the tests in each file. The tearDown function will be passed a DocTest object. The setUp function can access the test globals as the globs attribute of the test passed.

Optional argument `globs` is a dictionary containing the initial global variables for the tests. A new copy of this dictionary is created for each test. By default, globs is a new empty dictionary.

Optional argument `optionflags` specifies the default doctest options for the tests, created by or-ing together individual option flags. See section Option Flags. See function set_unittest_reportflags() below for a better way to set reporting options.

Optional argument `parser` specifies a DocTestParser (or subclass) that should be used to extract tests from the files. It defaults to a normal parser (i.e., DocTestParser()).

Optional argument `encoding` specifies an encoding that should be used to convert the file to unicode.

The global __file__ is added to the globals provided to doctests loaded from a text file using DocFileSuite().

➡ `doctest.DocTestSuite(module=None, globs=None, extraglobs=None, test_finder=None, setUp=None, tearDown=None, checker=None)`
Convert doctest tests for a module to a unittest.TestSuite.

The returned unittest.TestSuite is to be run by the unittest framework and runs each doctest in the module. If any of the doctests fail, then the synthesized unit test fails, and a failureException exception is raised showing the name of the file containing the test and a (sometimes approximate) line number.

Optional argument `module` provides the module to be tested. It can be a module object or a (possibly dotted) module name. If not specified, the module calling this function is used.

Optional argument `globs` is a dictionary containing the initial global variables for the tests. A new copy of this dictionary is created for each test. By default, globs is a new empty dictionary.

Optional argument `extraglobs` specifies an extra set of global variables, which is merged into globs. By default, no extra globals are used.

Optional argument `test_finder` is the DocTestFinder object (or a drop-in replacement) that is used to extract doctests from the module.

Optional arguments` setUp`, tearDown, and optionflags are the same as for function DocFileSuite() above.

This function uses the same search technique as testmod().


Changed in version 3.5: DocTestSuite() returns an empty unittest.TestSuite if module contains no docstrings instead of raising ValueError.

Under the covers, DocTestSuite() creates a unittest.TestSuite out of doctest.DocTestCase instances, and DocTestCase is a subclass of unittest.TestCase. DocTestCase isn’t documented here (it’s an internal detail), but studying its code can answer questions about the exact details of unittest integration.

Similarly, DocFileSuite() creates a unittest.TestSuite out of doctest.DocFileCase instances, and DocFileCase is a subclass of DocTestCase.

So both ways of creating a unittest.TestSuite run instances of DocTestCase. This is important for a subtle reason: when you run doctest functions yourself, you can control the doctest options in use directly, by passing option flags to doctest functions. However, if you’re writing a unittest framework, unittest ultimately controls when and how tests get run. The framework author typically wants to control doctest reporting options (perhaps, e.g., specified by command line options), but there’s no way to pass options through unittest to doctest test runners.

For this reason, doctest also supports a notion of doctest reporting flags specific to unittest support, via this function:

➡ `doctest.set_unittest_reportflags(flags)`
Set the doctest reporting flags to use.

Argument flags takes the bitwise OR of option flags. See section Option Flags. Only “reporting flags” can be used.

This is a module-global setting, and affects all future doctests run by module unittest: the runTest() method of DocTestCase looks at the option flags specified for the test case when the DocTestCase instance was constructed. If no reporting flags were specified (which is the typical and expected case), doctest’s unittest reporting flags are bitwise ORed into the option flags, and the option flags so augmented are passed to the DocTestRunner instance created to run the doctest. If any reporting flags were specified when the DocTestCase instance was constructed, doctest’s unittest reporting flags are ignored.

The value of the unittest reporting flags in effect before the function was called is returned by the function.


### ===🗝 Advanced API

The basic API is a simple wrapper that’s intended to make doctest easy to use. It is fairly flexible, and should meet most users’ needs; however, if you require more fine-grained control over testing, or wish to extend doctest’s capabilities, then you should use the advanced API.

The advanced API revolves around two container classes, which are used to store the interactive examples extracted from doctest cases:
↪• Example: A single Python statement, paired with its expected output.
↪• DocTest: A collection of Examples, typically extracted from a single docstring or text file.

Additional processing classes are defined to find, parse, and run, and check doctest examples:
↪• DocTestFinder: Finds all docstrings in a given module, and uses a DocTestParser to create a DocTest from every docstring that contains interactive examples.
↪• DocTestParser: Creates a DocTest object from a string (such as an object’s docstring).
↪• DocTestRunner: Executes the examples in a DocTest, and uses an OutputChecker to verify their output.
↪• OutputChecker: Compares the actual output from a doctest example with the expected output, and decides whether they match.

The relationships among these processing classes are summarized in the following diagram:


                                list of:
    +------+                   +---------+
    |module| --DocTestFinder-> | DocTest | --DocTestRunner-> results
    +------+    |        ^     +---------+     |       ^    (printed)
                |        |     | Example |     |       |
                v        |     |   ...   |     v       |
               DocTestParser   | Example |   OutputChecker
                               +---------+



### ===🗝 DocTest Objects
class doctest.DocTest(examples, globs, name, filename, lineno, docstring)
A collection of doctest examples that should be run in a single namespace. The constructor arguments are used to initialize the attributes of the same names.

DocTest defines the following attributes. They are initialized by the constructor, and should not be modified directly.
examples
A list of Example objects encoding the individual interactive Python examples that should be run by this test.
globs
The namespace (aka globals) that the examples should be run in. This is a dictionary mapping names to values. Any changes to the namespace made by the examples (such as binding new variables) will be reflected in globs after the test is run.
name
A string name identifying the DocTest. Typically, this is the name of the object or file that the test was extracted from.
filename
The name of the file that this DocTest was extracted from; or None if the filename is unknown, or if the DocTest was not extracted from a file.
lineno
The line number within filename where this DocTest begins, or None if the line number is unavailable. This line number is zero-based with respect to the beginning of the file.
docstring
The string that the test was extracted from, or None if the string is unavailable, or if the test was not extracted from a string.


### ===🗝 Example Objects
class doctest.Example(source, want, exc_msg=None, lineno=0, indent=0, options=None)
A single interactive example, consisting of a Python statement and its expected output. The constructor arguments are used to initialize the attributes of the same names.

Example defines the following attributes. They are initialized by the constructor, and should not be modified directly.
source
A string containing the example’s source code. This source code consists of a single Python statement, and always ends with a newline; the constructor adds a newline when necessary.
want
The expected output from running the example’s source code (either from stdout, or a traceback in case of exception). want ends with a newline unless no output is expected, in which case it’s an empty string. The constructor adds a newline when necessary.
exc_msg
The exception message generated by the example, if the example is expected to generate an exception; or None if it is not expected to generate an exception. This exception message is compared against the return value of traceback.format_exception_only(). exc_msg ends with a newline unless it’s None. The constructor adds a newline if needed.
lineno
The line number within the string containing this example where the example begins. This line number is zero-based with respect to the beginning of the containing string.
indent
The example’s indentation in the containing string, i.e., the number of space characters that precede the example’s first prompt.
options
A dictionary mapping from option flags to True or False, which is used to override default options for this example. Any option flags not contained in this dictionary are left at their default value (as specified by the DocTestRunner’s optionflags). By default, no options are set.


### ===🗝 DocTestFinder objects
class doctest.DocTestFinder(verbose=False, parser=DocTestParser(), recurse=True, exclude_empty=True)
A processing class used to extract the DocTests that are relevant to a given object, from its docstring and the docstrings of its contained objects. DocTests can be extracted from modules, classes, functions, methods, staticmethods, classmethods, and properties.

The optional argument verbose can be used to display the objects searched by the finder. It defaults to False (no output).

The optional argument parser specifies the DocTestParser object (or a drop-in replacement) that is used to extract doctests from docstrings.

If the optional argument recurse is false, then DocTestFinder.find() will only examine the given object, and not any contained objects.

If the optional argument exclude_empty is false, then DocTestFinder.find() will include tests for objects with empty docstrings.

DocTestFinder defines the following method:
find(obj[, name][, module][, globs][, extraglobs])
Return a list of the DocTests that are defined by obj’s docstring, or by any of its contained objects’ docstrings.

The optional argument name specifies the object’s name; this name will be used to construct names for the returned DocTests. If name is not specified, then obj.__name__ is used.

The optional parameter module is the module that contains the given object. If the module is not specified or is None, then the test finder will attempt to automatically determine the correct module. The object’s module is used:
↪• As a default namespace, if globs is not specified.
↪• To prevent the DocTestFinder from extracting DocTests from objects that are imported from other modules. (Contained objects with modules other than module are ignored.)
↪• To find the name of the file containing the object.
↪• To help find the line number of the object within its file.

If module is False, no attempt to find the module will be made. This is obscure, of use mostly in testing doctest itself: if module is False, or is None but cannot be found automatically, then all objects are considered to belong to the (non-existent) module, so all contained objects will (recursively) be searched for doctests.

The globals for each DocTest is formed by combining globs and extraglobs (bindings in extraglobs override bindings in globs). A new shallow copy of the globals dictionary is created for each DocTest. If globs is not specified, then it defaults to the module’s __dict__, if specified, or {} otherwise. If extraglobs is not specified, then it defaults to {}.


### ===🗝 DocTestParser objects
class doctest.DocTestParser
A processing class used to extract interactive examples from a string, and use them to create a DocTest object.

DocTestParser defines the following methods:
get_doctest(string, globs, name, filename, lineno)
Extract all doctest examples from the given string, and collect them into a DocTest object.

globs, name, filename, and lineno are attributes for the new DocTest object. See the documentation for DocTest for more information.
get_examples(string, name='<string>')
Extract all doctest examples from the given string, and return them as a list of Example objects. Line numbers are 0-based. The optional argument name is a name identifying this string, and is only used for error messages.
parse(string, name='<string>')
Divide the given string into examples and intervening text, and return them as a list of alternating Examples and strings. Line numbers for the Examples are 0-based. The optional argument name is a name identifying this string, and is only used for error messages.


### ===🗝 DocTestRunner objects
class doctest.DocTestRunner(checker=None, verbose=None, optionflags=0)
A processing class used to execute and verify the interactive examples in a DocTest.

The comparison between expected outputs and actual outputs is done by an OutputChecker. This comparison may be customized with a number of option flags; see section Option Flags for more information. If the option flags are insufficient, then the comparison may also be customized by passing a subclass of OutputChecker to the constructor.

The test runner’s display output can be controlled in two ways. First, an output function can be passed to TestRunner.run(); this function will be called with strings that should be displayed. It defaults to sys.stdout.write. If capturing the output is not sufficient, then the display output can be also customized by subclassing DocTestRunner, and overriding the methods report_start(), report_success(), report_unexpected_exception(), and report_failure().

The optional keyword argument checker specifies the OutputChecker object (or drop-in replacement) that should be used to compare the expected outputs to the actual outputs of doctest examples.

The optional keyword argument verbose controls the DocTestRunner’s verbosity. If verbose is True, then information is printed about each example, as it is run. If verbose is False, then only failures are printed. If verbose is unspecified, or None, then verbose output is used iff the command-line switch -v is used.

The optional keyword argument optionflags can be used to control how the test runner compares expected output to actual output, and how it displays failures. For more information, see section Option Flags.

DocTestParser defines the following methods:
report_start(out, test, example)
Report that the test runner is about to process the given example. This method is provided to allow subclasses of DocTestRunner to customize their output; it should not be called directly.

example is the example about to be processed. test is the test containing example. out is the output function that was passed to DocTestRunner.run().
report_success(out, test, example, got)
Report that the given example ran successfully. This method is provided to allow subclasses of DocTestRunner to customize their output; it should not be called directly.

example is the example about to be processed. got is the actual output from the example. test is the test containing example. out is the output function that was passed to DocTestRunner.run().
report_failure(out, test, example, got)
Report that the given example failed. This method is provided to allow subclasses of DocTestRunner to customize their output; it should not be called directly.

example is the example about to be processed. got is the actual output from the example. test is the test containing example. out is the output function that was passed to DocTestRunner.run().
report_unexpected_exception(out, test, example, exc_info)
Report that the given example raised an unexpected exception. This method is provided to allow subclasses of DocTestRunner to customize their output; it should not be called directly.

example is the example about to be processed. exc_info is a tuple containing information about the unexpected exception (as returned by sys.exc_info()). test is the test containing example. out is the output function that was passed to DocTestRunner.run().
run(test, compileflags=None, out=None, clear_globs=True)
Run the examples in test (a DocTest object), and display the results using the writer function out.

The examples are run in the namespace test.globs. If clear_globs is true (the default), then this namespace will be cleared after the test runs, to help with garbage collection. If you would like to examine the namespace after the test completes, then use clear_globs=False.

compileflags gives the set of flags that should be used by the Python compiler when running the examples. If not specified, then it will default to the set of future-import flags that apply to globs.

The output of each example is checked using the DocTestRunner’s output checker, and the results are formatted by the DocTestRunner.report_*() methods.
summarize(verbose=None)
Print a summary of all the test cases that have been run by this DocTestRunner, and return a named tuple TestResults(failed, attempted).

The optional verbose argument controls how detailed the summary is. If the verbosity is not specified, then the DocTestRunner’s verbosity is used.


### ===🗝 OutputChecker objects
class doctest.OutputChecker
A class used to check the whether the actual output from a doctest example matches the expected output. OutputChecker defines two methods: check_output(), which compares a given pair of outputs, and returns True if they match; and output_difference(), which returns a string describing the differences between two outputs.

OutputChecker defines the following methods:
check_output(want, got, optionflags)
Return True iff the actual output from an example (got) matches the expected output (want). These strings are always considered to match if they are identical; but depending on what option flags the test runner is using, several non-exact match types are also possible. See section Option Flags for more information about option flags.
output_difference(example, got, optionflags)
Return a string describing the differences between the expected output for a given example (example) and the actual output (got). optionflags is the set of option flags used to compare want and got.


### ===🗝 Debugging

Doctest provides several mechanisms for debugging doctest examples:

↪• Several functions convert doctests to executable Python programs, which can be run under the Python debugger, pdb.


↪• The DebugRunner class is a subclass of DocTestRunner that raises an exception for the first failing example, containing information about that example. This information can be used to perform post-mortem debugging on the example.


↪• The unittest cases generated by DocTestSuite() support the debug() method defined by unittest.TestCase.


↪• You can add a call to pdb.set_trace() in a doctest example, and you’ll drop into the Python debugger when that line is executed. Then you can inspect current values of variables, and so on. For example, suppose a.py contains just this module docstring:


"""
>>> def f(x):
...     g(x*2)
>>> def g(x):
...     print(x+3)
...     import pdb; pdb.set_trace()
>>> f(3)
9
"""


Then an interactive Python session may look like this:


>>> import a, doctest
>>> doctest.testmod(a)
--Return--
> <doctest a[1]>(3)g()->None
-> import pdb; pdb.set_trace()
(Pdb) list
  1     def g(x):
  2         print(x+3)
  3  ->     import pdb; pdb.set_trace()
[EOF]
(Pdb) p x
6
(Pdb) step
--Return--
> <doctest a[0]>(2)f()->None
-> g(x*2)
(Pdb) list
  1     def f(x):
  2  ->     g(x*2)
[EOF]
(Pdb) p x
3
(Pdb) step
--Return--
> <doctest a[2]>(1)?()->None
-> f(3)
(Pdb) cont
(0, 3)
>>>



Functions that convert doctests to Python code, and possibly run the synthesized code under the debugger:

➡ `doctest.script_from_examples(s)`
Convert text with examples to a script.

Argument s is a string containing doctest examples. The string is converted to a Python script, where doctest examples in s are converted to regular code, and everything else is converted to Python comments. The generated script is returned as a string. For example,


```py
import doctest
print(doctest.script_from_examples(r"""
    Set x and y to 1 and 2.
    >>> x, y = 1, 2

    Print their sum:
    >>> print(x+y)
    3
"""))


displays:


# Set x and y to 1 and 2.
x, y = 1, 2
#
# Print their sum:
print(x+y)
# Expected:
## 3
```


This function is used internally by other functions (see below), but can also be useful when you want to transform an interactive Python session into a Python script.

➡ `doctest.testsource(module, name)`
Convert the doctest for an object to a script.

Argument module is a module object, or dotted name of a module, containing the object whose doctests are of interest. Argument name is the name (within the module) of the object with the doctests of interest. The result is a string, containing the object’s docstring converted to a Python script, as described for script_from_examples() above. For example, if module a.py contains a top-level function f(), then


import a, doctest
print(doctest.testsource(a, "a.f"))


prints a script version of function f()’s docstring, with doctests converted to code, and the rest placed in comments.

➡ `doctest.debug(module, name, pm=False)`
Debug the doctests for an object.

The module and name arguments are the same as for function testsource() above. The synthesized Python script for the named object’s docstring is written to a temporary file, and then that file is run under the control of the Python debugger, pdb.

A shallow copy of module.__dict__ is used for both local and global execution context.

Optional argument `pm` controls whether post-mortem debugging is used. If pm has a true value, the script file is run directly, and the debugger gets involved only if the script terminates via raising an unhandled exception. If it does, then post-mortem debugging is invoked, via pdb.post_mortem(), passing the traceback object from the unhandled exception. If pm is not specified, or is false, the script is run under the debugger from the start, via passing an appropriate exec() call to pdb.run().

➡ `doctest.debug_src(src, pm=False, globs=None)`
Debug the doctests in a string.

This is like function debug() above, except that a string containing doctest examples is specified directly, via the src argument.

Optional argument `pm` has the same meaning as in function debug() above.

Optional argument `globs` gives a dictionary to use as both local and global execution context. If not specified, or None, an empty dictionary is used. If specified, a shallow copy of the dictionary is used.

The DebugRunner class, and the special exceptions it may raise, are of most interest to testing framework authors, and will only be sketched here. See the source code, and especially DebugRunner’s docstring (which is a doctest!) for more details:
class doctest.DebugRunner(checker=None, verbose=None, optionflags=0)
A subclass of DocTestRunner that raises an exception as soon as a failure is encountered. If an unexpected exception occurs, an UnexpectedException exception is raised, containing the test, the example, and the original exception. If the output doesn’t match, then a DocTestFailure exception is raised, containing the test, the example, and the actual output.

For information about the constructor parameters and methods, see the documentation for DocTestRunner in section Advanced API.

There are two exceptions that may be raised by DebugRunner instances:
exception doctest.DocTestFailure(test, example, got)
An exception raised by DocTestRunner to signal that a doctest example’s actual output did not match its expected output. The constructor arguments are used to initialize the attributes of the same names.

DocTestFailure defines the following attributes:
DocTestFailure.test
The DocTest object that was being run when the example failed.
DocTestFailure.example
The Example that failed.
DocTestFailure.got
The example’s actual output.
exception doctest.UnexpectedException(test, example, exc_info)
An exception raised by DocTestRunner to signal that a doctest example raised an unexpected exception. The constructor arguments are used to initialize the attributes of the same names.

UnexpectedException defines the following attributes:
UnexpectedException.test
The DocTest object that was being run when the example failed.
UnexpectedException.example
The Example that failed.
UnexpectedException.exc_info
A tuple containing information about the unexpected exception, as returned by sys.exc_info().


### ===🗝 Soapbox

As mentioned in the introduction, doctest has grown to have three primary uses:
1.Checking examples in docstrings.
2.Regression testing.
3.Executable documentation / literate testing.

These uses have different requirements, and it is important to distinguish them. In particular, filling your docstrings with obscure test cases makes for bad documentation.

When writing a docstring, choose docstring examples with care. There’s an art to this that needs to be learned—it may not be natural at first. Examples should add genuine value to the documentation. A good example can often be worth many words. If done with care, the examples will be invaluable for your users, and will pay back the time it takes to collect them many times over as the years go by and things change. I’m still amazed at how often one of my doctest examples stops working after a “harmless” change.

Doctest also makes an excellent tool for regression testing, especially if you don’t skimp on explanatory text. By interleaving prose and examples, it becomes much easier to keep track of what’s actually being tested, and why. When a test fails, good prose can make it much easier to figure out what the problem is, and how it should be fixed. It’s true that you could write extensive comments in code-based testing, but few programmers do. Many have found that using doctest approaches instead leads to much clearer tests. Perhaps this is simply because doctest makes writing prose a little easier than writing code, while writing comments in code is a little harder. I think it goes deeper than just that: the natural attitude when writing a doctest-based test is that you want to explain the fine points of your software, and illustrate them with examples. This in turn naturally leads to test files that start with the simplest features, and logically progress to complications and edge cases. A coherent narrative is the result, instead of a collection of isolated functions that test isolated bits of functionality seemingly at random. It’s a different attitude, and produces different results, blurring the distinction between testing and explaining.

Regression testing is best confined to dedicated objects or files. There are several options for organizing tests:
↪• Write text files containing test cases as interactive examples, and test the files using testfile() or DocFileSuite(). This is recommended, although is easiest to do for new projects, designed from the start to use doctest.
↪• Define functions named _regrtest_topic that consist of single docstrings, containing test cases for the named topics. These functions can be included in the same file as the module, or separated out into a separate test file.
↪• Define a __test__ dictionary mapping from regression test topics to docstrings containing test cases.

When you have placed your tests in a module, the module can itself be the test runner. When a test fails, you can arrange for your test runner to re-run only the failing doctest while you debug the problem. Here is a minimal example of such a test runner:


```py
if __name__ == '__main__':
    import doctest
    flags = doctest.REPORT_NDIFF|doctest.FAIL_FAST
    if len(sys.argv) > 1:
        name = sys.argv[1]
        if name in globals():
            obj = globals()[name]
        else:
            obj = __test__[name]
        doctest.run_docstring_examples(obj, globals(), name=name,
                                       optionflags=flags)
    else:
        fail, total = doctest.testmod(optionflags=flags)
        print("{} failures out of {} tests".format(fail, total))
```


Footnotes

[1] Examples containing both expected output and an exception are not supported. Trying to guess where one ends and the other begins is too error-prone, and that also makes for a confusing test. 

## ==⚡  unittest — Unit testing framework

## ==⚡  unittest.mock — mock object library

## ==⚡  unittest.mock — getting started

## ==⚡  2to3 - Automated Python 2 to 3 code translation

## ==⚡  test — Regression tests package for Python

## ==⚡  test.support — Utilities for the Python test suite

## ==⚡  test.support.socket_helper — Utilities for socket tests

## ==⚡  test.support.script_helper — Utilities for the Python execution tests

## ==⚡  test.support.bytecode_helper — Support tools for testing correct bytecode generation

## ==⚡  test.support.threading_helper — Utilities for threading tests

## ==⚡  test.support.os_helper — Utilities for os tests

## ==⚡  test.support.import_helper — Utilities for import tests

## ==⚡  test.support.warnings_helper — Utilities for warnings tests


# =🚩 Debugging and Profiling

These libraries help you with Python development: the debugger enables you to step through code, analyze stack frames and set breakpoints etc., and the profilers run code and give you a detailed breakdown of execution times, allowing you to identify bottlenecks in your programs. Auditing events provide visibility into runtime behaviors that would otherwise require intrusive debugging or patching.

➡ `• Audit events table`
➡ `• bdb — Debugger framework`
➡ `• faulthandler — Dump the Python traceback`
◦ Dumping the traceback
◦ Fault handler state
◦ Dumping the tracebacks after a timeout
◦ Dumping the traceback on a user signal
◦ Issue with file descriptors
◦ Example

➡ `• pdb — The Python Debugger`
◦ Debugger Commands

➡ `• The Python Profilers`
◦ Introduction to the profilers
◦ Instant User’s Manual
◦ profile and cProfile Module Reference
◦ The Stats Class
◦ What Is Deterministic Profiling?
◦ Limitations
◦ Calibration
◦ Using a custom timer

➡ `• timeit — Measure execution time of small code snippets`
◦ Basic Examples
◦ Python Interface
◦ Command-Line Interface
◦ Examples

➡ `• trace — Trace or track Python statement execution`
◦ Command-Line Usage
◾ Main options
◾ Modifiers
◾ Filters

◦ Programmatic Interface

➡ `• tracemalloc — Trace memory allocations`
◦ Examples
◾ Display the top 10
◾ Compute differences
◾ Get the traceback of a memory block
◾ Pretty top
◾ Record the current and peak size of all traced memory blocks


◦ API
◾ Functions
◾ DomainFilter
◾ Filter
◾ Frame
◾ Snapshot
◾ Statistic
◾ StatisticDiff
◾ Trace
◾ Traceback

## ==⚡ • Audit events table

This table contains all events raised by `sys.audit()` or `PySys_Audit()` calls throughout the CPython runtime and the standard library. These calls were added in 3.8.0 or later (see PEP 578).

See `sys.addaudithook()` and `PySys_AddAuditHook()` for information on handling these events.


CPython implementation detail: This table is generated from the CPython documentation, and may not represent events raised by other implementations. See your runtime specific documentation for actual events raised.

    |           Audit event            |             Arguments              |
    |----------------------------------|------------------------------------|
    | array.__new__                    | typecode, initializer              |
    | builtins.breakpoint              | breakpointhook                     |
    | builtins.id                      | id                                 |
    | builtins.input                   | prompt                             |
    | builtins.input/result            | result                             |
    | code.__new__                     | code, filename, name, argcount, posonlyargcount, kwonlyargcount, nlocals, stacksize, flags |
    | compile                          | source, filename                   |
    | cpython._PySys_ClearAuditHooks   |                                    |
    | cpython.PyInterpreterState_Clear |                                    |
    | cpython.PyInterpreterState_New   |                                    |
    | cpython.run_command              | command                            |
    | cpython.run_file                 | filename                           |
    | cpython.run_interactivehook      | hook                               |
    | cpython.run_module               | module-name                        |
    | cpython.run_startup              | filename                           |
    | cpython.run_stdin                |                                    |
    | ctypes.addressof                 | obj                                |
    | ctypes.call_function             | func_pointer, arguments            |
    | ctypes.cdata                     | address                            |
    | ctypes.cdata/buffer              | pointer, size, offset              |
    | ctypes.create_string_buffer      | init, size                         |
    | ctypes.create_unicode_buffer     | init, size                         |
    | ctypes.dlopen                    | name                               |
    | ctypes.dlsym                     | library, name                      |
    | ctypes.dlsym/handle              | handle, name                       |
    | ctypes.get_errno                 |                                    |
    | ctypes.get_last_error            |                                    |
    | ctypes.seh_exception             | code                               |
    | ctypes.set_errno                 | errno                              |
    | ctypes.set_last_error            | error                              |
    | ctypes.string_at                 | address, size                      |
    | ctypes.wstring_at                | address, size                      |
    | ensurepip.bootstrap              | root                               |
    | exec code_object                 |                                    |
    | fcntl.fcntl                      | fd, cmd, arg                       |
    | fcntl.flock                      | fd, operation                      |
    | fcntl.ioctl                      | fd, request, arg                   |
    | fcntl.lockf                      | fd, cmd, len, start, whence        |
    | ftplib.connect                   | self, host, port                   |
    | ftplib.sendcmd                   | self, cmd                          |
    | function.__new__                 | code                               |
    | gc.get_objects                   | generation                         |
    | gc.get_referents                 | objs                               |
    | gc.get_referrers                 | objs                               |
    | glob.glob                        | pathname, recursive                |
    | glob.glob/2                      | pathname, recursive, root_dir, dir_fd |
    | glob.glob/2                      | pathname, recursive, root_dir, dir_fd |
    | http.client.                     | connect self, host, port           |
    | http.client.                     | send self, data                    |
    | imaplib.open                     | self, host, port                   |
    | imaplib.send                     | self, data                         |
    | import                           | module, filename, sys.path, sys.meta_path, sys.path_hooks |
    | marshal.dumps                    | value, version                     |
    | marshal.load                     |                                    |
    | marshal.loads                    | bytes                              |
    | mmap.__new__                     | fileno, length, access, offset     |
    | msvcrt.get_osfhandle             | fd                                 |
    | msvcrt.locking                   | fd, mode, nbytes                   |
    | msvcrt.open_osfhandle            | handle, flags                      |
    | nntplib.connect                  | self, host, port                   |
    | nntplib.putline                  | self, line                         |
    | object.__delattr__               | obj, name                          |
    | object.__getattr__               | obj, name                          |
    | object.__setattr__               | obj, name, value                   |
    | open                             | file, mode, flags                  |
    | os.add_dll_directory             | path                               |
    | os.chdir                         | path                               |
    | os.chflags                       | path, flags                        |
    | os.chmod                         | path, mode, dir_fd                 |
    | os.chown                         | path, uid, gid, dir_fd             |
    | os.exec                          | path, args, env                    |
    | os.fork                          |                                    |
    | os.forkpty                       |                                    |
    | os.fwalk                         | top, topdown, onerror, follow_symlinks, dir_fd |
    | os.getxattr                      | path, attribute                    |
    | os.kill                          | pid, sig                           |
    | os.killpg                        | pgid, sig                          |
    | os.link                          | src, dst, src_dir_fd, dst_dir_fd   |
    | os.listdir                       | path                               |
    | os.listxattr                     | path                               |
    | os.lockf                         | fd, cmd, len                       |
    | os.mkdir                         | path, mode, dir_fd                 |
    | os.posix_spawn                   | path, argv, env                    |
    | os.putenv                        | key, value                         |
    | os.remove                        | path, dir_fd                       |
    | os.removexattr                   | path, attribute                    |
    | os.rename                        | src, dst, src_dir_fd, dst_dir_fd   |
    | os.rmdir                         | path, dir_fd                       |
    | os.scandir                       | path                               |
    | os.setxattr                      | path, attribute, value, flags      |
    | os.spawn                         | mode, path, args, env              |
    | os.startfile                     | path, operation                    |
    | os.startfile/2                   | path, operation, arguments, cwd, show_cmd |
    | os.symlink                       | src, dst, dir_fd                   |
    | os.system                        | command                            |
    | os.truncate                      | fd, length                         |
    | os.unsetenv                      | key                                |
    | os.utime                         | path, times, ns, dir_fd            |
    | os.walk                          | top, topdown, onerror, followlinks |
    | pathlib.Path.                    | glob self, pattern                 |
    | pathlib.Path.                    | rglob self, pattern                |
    | pdb.Pdb                          |                                    |
    | pickle.find_class                | module, name                       |
    | poplib.connect                   | self, host, port                   |
    | poplib.putline                   | self, line                         |
    | pty.spawn                        | argv                               |
    | resource.prlimit                 | pid, resource, limits              |
    | resource.setrlimit               | resource, limits                   |
    | setopencodehook                  |                                    |
    | shutil.chown                     | path, user, group                  |
    | shutil.copyfile                  | src, dst                           |
    | shutil.copymode                  | src, dst                           |
    | shutil.copystat                  | src, dst                           |
    | shutil.copytree                  | src, dst                           |
    | shutil.make_archive              | base_name, format, root_dir, base_dir |
    | shutil.move                      | src, dst                           |
    | shutil.rmtree                    | path                               |
    | shutil.unpack_archive            | filename, extract_dir, format      |
    | signal.pthread_kill              | thread_id, signalnum               |
    | smtplib.connect                  | self, host, port                   |
    | smtplib.send                     | self, data                         |
    | socket.__new__                   | self, family, type, protocol       |
    | socket.bind                      | self, address                      |
    | socket.connect                   | self, address                      |
    | socket.getaddrinfo               | host, port, family, type, protocol |
    | socket.gethostbyaddr             | ip_address                         |
    | socket.gethostbyname             | hostname                           |
    | socket.gethostname               |                                    |
    | socket.getnameinfo               | sockaddr                           |
    | socket.getservbyname             | servicename, protocolname          |
    | socket.getservbyport             | port, protocolname                 |
    | socket.sendmsg                   | self, address                      |
    | socket.sendto                    | self, address                      |
    | socket.sethostname               | name                               |
    | sqlite3.connect                  | database                           |
    | sqlite3.connect/handle           | connection_handle                  |
    | sqlite3.enable_load_extension    | connection, enabled                |
    | sqlite3.load_extension           | connection, path                   |
    | subprocess.Popen                 | executable, args, cwd, env         |
    | sys._current_exceptions          |                                    |
    | sys._current_frames              |                                    |
    | sys._getframe                    |                                    |
    | sys.addaudithook                 |                                    |
    | sys.excepthook                   | hook, type, value, traceback       |
    | sys.set_asyncgen_hooks_finalizer |                                    |
    | sys.set_asyncgen_hooks_firstiter |                                    |
    | sys.setprofile                   |                                    |
    | sys.settrace                     |                                    |
    | sys.unraisablehook               | hook, unraisable                   |
    | syslog.closelog                  |                                    |
    | syslog.openlog                   | ident, logoption, facility         |
    | syslog.setlogmask                | maskpri                            |
    | syslog.syslog                    | priority, message                  |
    | telnetlib.Telnet.open            | self, host, port                   |
    | telnetlib.Telnet.write           | self, buffer                       |
    | tempfile.mkdtemp                 | fullpath                           |
    | tempfile.mkstemp                 | fullpath                           |
    | urllib.Request                   | fullurl, data, headers, method     |
    | webbrowser.open                  | url                                |
    | winreg.ConnectRegistry           | computer_name, key                 |
    | winreg.CreateKey                 | key, sub_key, access               |
    | winreg.DeleteKey                 | key, sub_key, access               |
    | winreg.DeleteValue               | key, value                         |
    | winreg.DisableReflectionKey      | key                                |
    | winreg.EnableReflectionKey       | key                                |
    | winreg.EnumKey                   | key, index                         |
    | winreg.EnumValue                 | key, index                         |
    | winreg.ExpandEnvironmentStrings  | str                                |
    | winreg.LoadKey                   | key, sub_key, file_name            |
    | winreg.OpenKey                   | key, sub_key, access               |
    | winreg.OpenKey/result            | key                                |
    | winreg.PyHKEY.                   | Detach key                         |
    | winreg.QueryInfoKey              | key                                |
    | winreg.QueryReflectionKey        | key                                |
    | winreg.QueryValue                | key, sub_key, value_name           |
    | winreg.SaveKey                   | key, file_name                     |
    | winreg.SetValue                  | key, sub_key, type, value          |

The following events are raised internally and do not correspond to any public API of CPython:

    |       Audit event        |                     Arguments                     |
    |--------------------------|---------------------------------------------------|
    | _winapi.CreateJunction   | src_path, dst_path                                |
    | _winapi.CreateNamedPipe  | name, open_mode, pipe_mode                        |
    | _winapi.CreatePipe       |                                                   |
    | _winapi.CreateProcess    | application_name, command_line, current_directory |
    | _winapi.OpenProcess      | process_id, desired_access                        |
    | _winapi.TerminateProcess | handle, exit_code                                 |
    | ctypes.PyObj_FromPtr     | obj                                               |
    | _winapi.CreateFile       | file_name, desired_access, share_mode, creation_disposition, flags_and_attributes |


## ==⚡ • bdb — Debugger framework
## ==⚡ • faulthandler — Dump the Python traceback
## ==⚡ • pdb — The Python Debugger

## ==⚡ • The Python Profilers

Source code: Lib/profile.py and Lib/pstats.py


### ===🗝 Introduction to the profilers

cProfile and profile provide deterministic profiling of Python programs. A profile is a set of statistics that describes how often and for how long various parts of the program executed. These statistics can be formatted into reports via the pstats module.

The Python standard library provides two different implementations of the same profiling interface:

1. cProfile is recommended for most users; it’s a C extension with reasonable overhead that makes it suitable for profiling long-running programs. Based on lsprof, contributed by Brett Rosen and Ted Czotter.

2. profile, a pure Python module whose interface is imitated by cProfile, but which adds significant overhead to profiled programs. If you’re trying to extend the profiler in some way, the task might be easier with this module. Originally designed and written by Jim Roskind.

Note:
 The profiler modules are designed to provide an execution profile for a given program, not for benchmarking purposes (for that, there is timeit for reasonably accurate results). This particularly applies to benchmarking Python code against C code: the profilers introduce overhead for Python code, but not for C-level functions, and so the C code would seem faster than any Python one.
 


### ===🗝 Instant User’s Manual

This section is provided for users that “don’t want to read the manual.” It provides a very brief overview, and allows a user to rapidly perform profiling on an existing application.

To profile a function that takes a single argument, you can do:


```py
import cProfile
import re

cProfile.run('re.compile("foo|bar")')
```


(Use profile instead of cProfile if the latter is not available on your system.)

The above action would run re.compile() and print profile results like the following:


          197 function calls (192 primitive calls) in 0.002 seconds

    Ordered by: standard name

    ncalls  tottime  percall  cumtime  percall filename:lineno(function)
         1    0.000    0.000    0.001    0.001 <string>:1(<module>)
         1    0.000    0.000    0.001    0.001 re.py:212(compile)
         1    0.000    0.000    0.001    0.001 re.py:268(_compile)
         1    0.000    0.000    0.000    0.000 sre_compile.py:172(_compile_charset)
         1    0.000    0.000    0.000    0.000 sre_compile.py:201(_optimize_charset)
         4    0.000    0.000    0.000    0.000 sre_compile.py:25(_identityfunction)
       3/1    0.000    0.000    0.000    0.000 sre_compile.py:33(_compile)


The first line indicates that 197 calls were monitored. Of those calls, 192 were primitive, meaning that the call was not induced via recursion. 

The next line: Ordered by: standard name, indicates that the text string in the far right column was used to sort the output. 

The column headings include:

➡ `ncalls` for the number of calls.
➡ `tottime` for the total time spent in the given function (and excluding time made in calls to sub-functions)
➡ `percallis` the quotient of `tottime` divided by ncalls.
➡ `cumtime` is the cumulative time spent in this and all subfunctions (from invocation till exit). This figure is accurate even for recursive functions.
➡ `percall` is the quotient of `cumtime` divided by primitive calls.
➡ `filename:lineno(function)` provides the respective data of each function

When there are two numbers in the first column (for example 3/1), it means that the function recursed. The second value is the number of primitive calls and the former is the total number of calls. Note that when the function does not recurse, these two values are the same, and only the single figure is printed.

Instead of printing the output at the end of the profile run, you can save the results to a file by specifying a filename to the run() function:


```py
import cProfile
import re

cProfile.run('re.compile("foo|bar")', 'restats')
```


The `pstats.Stats` class reads profile results from a file and formats them in various ways.

The files cProfile and profile can also be invoked as a script to profile another script. For example:


python -m cProfile [-o output_file] [-s sort_order] (-m module | myscript.py)


-o writes the profile results to a file instead of to stdout

-s specifies one of the sort_stats() sort values to sort the output by. This only applies when -o is not supplied.

-m specifies that a module is being profiled instead of a script.


New in version 3.7: Added the -m option to cProfile.


New in version 3.8: Added the -m option to profile.

The `pstats` module’s Stats class has a variety of methods for manipulating and printing the data saved into a profile results file:


```py
import pstats
from pstats import SortKey
p = pstats.Stats('restats')
p.strip_dirs().sort_stats(-1).print_stats()
```


The `strip_dirs()` method removed the extraneous path from all the module names. 

The `sort_stats()` method sorted all the entries according to the standard module/line/name string that is printed. 

The `print_stats()` method printed out all the statistics. You might try the following sort calls:


```py
p.sort_stats(SortKey.NAME)
p.print_stats()
```


The first call will actually sort the list by function name, and the second call will print out the statistics. The following are some interesting calls to experiment with:


```py
p.sort_stats(SortKey.CUMULATIVE).print_stats(10)
```


This sorts the profile by cumulative time in a function, and then only prints the ten most significant lines. If you want to understand what algorithms are taking time, the above line is what you would use.

If you were looking to see what functions were looping a lot, and taking a lot of time, you would do:


```py
p.sort_stats(SortKey.TIME).print_stats(10)
```


to sort according to time spent within each function, and then print the statistics for the top ten functions.

You might also try:


```py
p.sort_stats(SortKey.FILENAME).print_stats('__init__')
```


This will sort all the statistics by file name, and then print out statistics for only the class init methods (since they are spelled with __init__ in them). As one final example, you could try:


```py
p.sort_stats(SortKey.TIME, SortKey.CUMULATIVE).print_stats(.5, 'init')
```


This line sorts statistics with a primary key of time, and a secondary key of cumulative time, and then prints out some of the statistics. To be specific, the list is first culled down to 50% (re: .5) of its original size, then only lines containing init are maintained, and that sub-sub-list is printed.

If you wondered what functions called the above functions, you could now (p is still sorted according to the last criteria) do:


```py
p.print_callers(.5, 'init')
```


and you would get a list of callers for each of the listed functions.

If you want more functionality, you’re going to have to read the manual, or guess what the following functions do:


```py
p.print_callees()
p.add('restats')
```


Invoked as a script, the pstats module is a statistics browser for reading and examining profile dumps. It has a simple line-oriented interface (implemented using cmd) and interactive help.


### ===🗝 profile and cProfile Module Reference

Both the profile and cProfile modules provide the following functions:

➡ `profile.run(command, filename=None, sort=-1)`
This function takes a single argument that can be passed to the exec() function, and an optional file name. In all cases this routine executes:


```py
exec(command, __main__.__dict__, __main__.__dict__)

```

and gathers profiling statistics from the execution. If no file name is present, then this function automatically creates a Stats instance and prints a simple profiling report. If the sort value is specified, it is passed to this Stats instance to control how the results are sorted.

➡ `profile.runctx(command, globals, locals, filename=None, sort=-1)`
This function is similar to run(), with added arguments to supply the globals and locals dictionaries for the command string. This routine executes:


```py
exec(command, globals, locals)

```

and gathers profiling statistics as in the run() function above.
class profile.Profile(timer=None, timeunit=0.0, subcalls=True, builtins=True)
This class is normally only used if more precise control over profiling is needed than what the cProfile.run() function provides.

A custom timer can be supplied for measuring how long code takes to run via the timer argument. This must be a function that returns a single number representing the current time. If the number is an integer, the timeunit specifies a multiplier that specifies the duration of each unit of time. For example, if the timer returns times measured in thousands of seconds, the time unit would be .001.

Directly using the Profile class allows formatting profile results without writing the profile data to a file:


```py
import cProfile, pstats, io
from pstats import SortKey
pr = cProfile.Profile()
pr.enable()
# ... do something ...
pr.disable()
s = io.StringIO()
sortby = SortKey.CUMULATIVE
ps = pstats.Stats(pr, stream=s).sort_stats(sortby)
ps.print_stats()
print(s.getvalue())
```


The Profile class can also be used as a context manager (supported only in cProfile module. see Context Manager Types):


```py
import cProfile

with cProfile.Profile() as pr:
    # ... do something ...

pr.print_stats()
```


Changed in version 3.8: Added context manager support.


➡ `enable()`
Start collecting profiling data. Only in cProfile.

➡ `disable()`
Stop collecting profiling data. Only in cProfile.

➡ `create_stats()`
Stop collecting profiling data and record the results internally as the current profile.

➡ `print_stats(sort=-1)`
Create a Stats object based on the current profile and print the results to stdout.

➡ `dump_stats(filename)`
Write the results of the current profile to filename.

➡ `run(cmd)`
Profile the cmd via exec().

➡ `runctx(cmd, globals, locals)`
Profile the cmd via exec() with the specified global and local environment.

➡ `runcall(func, /, *args, **kwargs)`
Profile `func(*args, **kwargs)`

Note that profiling will only work if the called command/function actually returns. If the interpreter is terminated (e.g. via a sys.exit() call during the called command/function execution) no profiling results will be printed.


### ===🗝 The Stats Class

Analysis of the profiler data is done using the Stats class.

✅ `class pstats.Stats(*filenames or profile, stream=sys.stdout)`
This class constructor creates an instance of a “statistics object” from a filename (or list of filenames) or from a Profile instance. Output will be printed to the stream specified by stream.

The file selected by the above constructor must have been created by the corresponding version of profile or cProfile. To be specific, there is no file compatibility guaranteed with future versions of this profiler, and there is no compatibility with files produced by other profilers, or the same profiler run on a different operating system. If several files are provided, all the statistics for identical functions will be coalesced, so that an overall view of several processes can be considered in a single report. If additional files need to be combined with data in an existing Stats object, the add() method can be used.

Instead of reading the profile data from a file, a cProfile.Profile or profile.Profile object can be used as the profile data source.

Stats objects have the following methods:

➡ `strip_dirs()`
This method for the Stats class removes all leading path information from file names. It is very useful in reducing the size of the printout to fit within (close to) 80 columns. This method modifies the object, and the stripped information is lost. After performing a strip operation, the object is considered to have its entries in a “random” order, as it was just after object initialization and loading. If strip_dirs() causes two function names to be indistinguishable (they are on the same line of the same filename, and have the same function name), then the statistics for these two entries are accumulated into a single entry.

➡ `add(*filenames)`
This method of the Stats class accumulates additional profiling information into the current profiling object. Its arguments should refer to filenames created by the corresponding version of `profile.run()` or `cProfile.run()`. Statistics for identically named (re: file, line, name) functions are automatically accumulated into single function statistics.

➡ `dump_stats(filename)`
Save the data loaded into the Stats object to a file named filename. The file is created if it does not exist, and is overwritten if it already exists. This is equivalent to the method of the same name on the profile.Profile and cProfile.Profile classes.

➡ `sort_stats(*keys)`
This method modifies the Stats object by sorting it according to the supplied criteria. The argument can be either a string or a `SortKey` enum identifying the basis of a sort (example: 'time', 'name', SortKey.TIME or SortKey.NAME). The SortKey enums argument have advantage over the string argument in that it is more robust and less error prone.

When more than one key is provided, then additional keys are used as secondary criteria when there is equality in all keys selected before them. For example, sort_stats(SortKey.NAME, SortKey.FILE) will sort all the entries according to their function name, and resolve all ties (identical function names) by sorting by file name.

For the string argument, abbreviations can be used for any key names, as long as the abbreviation is unambiguous.

The following are the valid string and SortKey:

| Valid String Arg |   Valid enum Arg   |       Meaning        |
|------------------|--------------------|----------------------|
| 'calls'          | SortKey.CALLS      | call count           |
| 'cumulative'     | SortKey.CUMULATIVE | cumulative time      |
| 'cumtime'        | N/A                | cumulative time      |
| 'file'           | N/A                | file name            |
| 'filename'       | SortKey.FILENAME   | file name            |
| 'module'         | N/A                | file name            |
| 'ncalls'         | N/A                | call count           |
| 'pcalls'         | SortKey.PCALLS     | primitive call count |
| 'line'           | SortKey.LINE       | line number          |
| 'name'           | SortKey.NAME       | function name        |
| 'nfl'            | SortKey.NFL        | name/file/line       |
| 'stdname'        | SortKey.STDNAME    | standard name        |
| 'time'           | SortKey.TIME       | internal time        |
| 'tottime'        | N/A                | internal time        |

Note that all sorts on statistics are in descending order (placing most time consuming items first), where as name, file, and line number searches are in ascending order (alphabetical). The subtle distinction between SortKey.NFL and SortKey.STDNAME is that the standard name is a sort of the name as printed, which means that the embedded line numbers get compared in an odd way. For example, lines 3, 20, and 40 would (if the file names were the same) appear in the string order 20, 3 and 40. In contrast, SortKey.NFL does a numeric compare of the line numbers. In fact, `sort_stats(SortKey.NFL)` is the same as `sort_stats(SortKey.NAME, SortKey.FILENAME, SortKey.LINE)`.

For backward-compatibility reasons, the numeric arguments -1, 0, 1, and 2 are permitted. They are interpreted as 'stdname', 'calls', 'time', and 'cumulative' respectively. If this old style format (numeric) is used, only one sort key (the numeric key) will be used, and additional arguments will be silently ignored.


New in version 3.7: Added the SortKey enum.

➡ `reverse_order()`
This method for the Stats class reverses the ordering of the basic list within the object. Note that by default ascending vs descending order is properly selected based on the sort key of choice.

➡ `print_stats(*restrictions)`
This method for the Stats class prints out a report as described in the profile.run() definition.

The order of the printing is based on the last sort_stats() operation done on the object (subject to caveats in add() and strip_dirs()).

The arguments provided (if any) can be used to limit the list down to the significant entries. Initially, the list is taken to be the complete set of profiled functions. Each restriction is either an integer (to select a count of lines), or a decimal fraction between 0.0 and 1.0 inclusive (to select a percentage of lines), or a string that will interpreted as a regular expression (to pattern match the standard name that is printed). If several restrictions are provided, then they are applied sequentially. For example:


print_stats(.1, 'foo:')


would first limit the printing to first 10% of list, and then only print functions that were part of filename `.*foo:`. In contrast, the command:


print_stats('foo:', .1)


would limit the list to all functions having file names `.*foo:`, and then proceed to only print the first 10% of them.

➡ `print_callers(*restrictions)`
This method for the Stats class prints a list of all functions that called each function in the profiled database. The ordering is identical to that provided by print_stats(), and the definition of the restricting argument is also identical. Each caller is reported on its own line. The format differs slightly depending on the profiler that produced the stats:

• With profile, a number is shown in parentheses after each caller to show how many times this specific call was made. For convenience, a second non-parenthesized number repeats the cumulative time spent in the function at the right.

• With cProfile, each caller is preceded by three numbers: the number of times this specific call was made, and the total and cumulative times spent in the current function while it was invoked by this specific caller.

➡ `print_callees(*restrictions)`
This method for the Stats class prints a list of all function that were called by the indicated function. Aside from this reversal of direction of calls (re: called vs was called by), the arguments and ordering are identical to the print_callers() method.

➡ `get_stats_profile()`
This method returns an instance of StatsProfile, which contains a mapping of function names to instances of FunctionProfile. Each FunctionProfile instance holds information related to the function’s profile such as how long the function took to run, how many times it was called, etc…


New in version 3.9: Added the following dataclasses: StatsProfile, FunctionProfile. Added the following function: get_stats_profile.


### ===🗝 What Is Deterministic Profiling?

Deterministic profiling is meant to reflect the fact that all function call, function return, and exception events are monitored, and precise timings are made for the intervals between these events (during which time the user’s code is executing). In contrast, statistical profiling (which is not done by this module) randomly samples the effective instruction pointer, and deduces where time is being spent. The latter technique traditionally involves less overhead (as the code does not need to be instrumented), but provides only relative indications of where time is being spent.

In Python, since there is an interpreter active during execution, the presence of instrumented code is not required in order to do deterministic profiling. Python automatically provides a hook (optional callback) for each event. In addition, the interpreted nature of Python tends to add so much overhead to execution, that deterministic profiling tends to only add small processing overhead in typical applications. The result is that deterministic profiling is not that expensive, yet provides extensive run time statistics about the execution of a Python program.

Call count statistics can be used to identify bugs in code (surprising counts), and to identify possible inline-expansion points (high call counts). Internal time statistics can be used to identify “hot loops” that should be carefully optimized. Cumulative time statistics should be used to identify high level errors in the selection of algorithms. Note that the unusual handling of cumulative times in this profiler allows statistics for recursive implementations of algorithms to be directly compared to iterative implementations.


### ===🗝 Limitations

One limitation has to do with accuracy of timing information. There is a fundamental problem with deterministic profilers involving accuracy. The most obvious restriction is that the underlying “clock” is only ticking at a rate (typically) of about .001 seconds. Hence no measurements will be more accurate than the underlying clock. If enough measurements are taken, then the “error” will tend to average out. Unfortunately, removing this first error induces a second source of error.

The second problem is that it “takes a while” from when an event is dispatched until the profiler’s call to get the time actually gets the state of the clock. Similarly, there is a certain lag when exiting the profiler event handler from the time that the clock’s value was obtained (and then squirreled away), until the user’s code is once again executing. As a result, functions that are called many times, or call many functions, will typically accumulate this error. The error that accumulates in this fashion is typically less than the accuracy of the clock (less than one clock tick), but it can accumulate and become very significant.

The problem is more important with profile than with the lower-overhead cProfile. For this reason, profile provides a means of calibrating itself for a given platform so that this error can be probabilistically (on the average) removed. After the profiler is calibrated, it will be more accurate (in a least square sense), but it will sometimes produce negative numbers (when call counts are exceptionally low, and the gods of probability work against you :-). ) Do not be alarmed by negative numbers in the profile. They should only appear if you have calibrated your profiler, and the results are actually better than without calibration.


### ===🗝 Calibration

The profiler of the profile module subtracts a constant from each event handling time to compensate for the overhead of calling the time function, and socking away the results. By default, the constant is 0. The following procedure can be used to obtain a better constant for a given platform (see Limitations).


```py
import profile
pr = profile.Profile()
for i in range(5):
    print(pr.calibrate(10000)
    )
```


The method executes the number of Python calls given by the argument, directly and again under the profiler, measuring the time for both. It then computes the hidden overhead per profiler event, and returns that as a float. For example, on a 1.8Ghz Intel Core i5 running macOS, and using Python’s time.process_time() as the timer, the magical number is about 4.04e-6.

The object of this exercise is to get a fairly consistent result. If your computer is very fast, or your timer function has poor resolution, you might have to pass 100000, or even 1000000, to get consistent results.

When you have a consistent answer, there are three ways you can use it:


```py
import profile

# 1. Apply computed bias to all Profile instances created hereafter.
profile.Profile.bias = your_computed_bias

# 2. Apply computed bias to a specific Profile instance.
pr = profile.Profile()
pr.bias = your_computed_bias

# 3. Specify computed bias in instance constructor.
pr = profile.Profile(bias=your_computed_bias)
```


If you have a choice, you are better off choosing a smaller constant, and then your results will “less often” show up as negative in profile statistics.


### ===🗝 Using a custom timer

If you want to change how current time is determined (for example, to force use of wall-clock time or elapsed process time), pass the timing function you want to the Profile class constructor:


```py
pr = profile.Profile(your_time_func)

```

The resulting profiler will then call your_time_func. Depending on whether you are using profile.Profile or cProfile.Profile, your_time_func’s return value will be interpreted differently:


➡ `profile.Profile`
your_time_func should return a single number, or a list of numbers whose sum is the current time (like what os.times() returns). If the function returns a single time number, or the list of returned numbers has length 2, then you will get an especially fast version of the dispatch routine.

Be warned that you should calibrate the profiler class for the timer function that you choose (see Calibration). For most machines, a timer that returns a lone integer value will provide the best results in terms of low overhead during profiling. (os.times() is pretty bad, as it returns a tuple of floating point values). If you want to substitute a better timer in the cleanest fashion, derive a class and hardwire a replacement dispatch method that best handles your timer call, along with the appropriate calibration constant.

➡ `cProfile.Profile`
your_time_func should return a single number. If it returns integers, you can also invoke the class constructor with a second argument specifying the real duration of one unit of time. For example, if your_integer_time_func returns times measured in thousands of seconds, you would construct the Profile instance as follows:


    pr = cProfile.Profile(your_integer_time_func, 0.001)


As the cProfile.Profile class cannot be calibrated, custom timer functions should be used with care and should be as fast as possible. For the best results with a custom timer, it might be necessary to hard-code it in the C source of the internal `_lsprof` module.

Python 3.3 adds several new functions in time that can be used to make precise measurements of process or wall-clock time. For example, see time.perf_counter().


## ==⚡ • timeit — Measure execution time of small code snippets
## ==⚡ • trace — Trace or track Python statement execution
## ==⚡ • tracemalloc — Trace memory allocations


# =🚩 Software Packaging and Distribution

These libraries help you with publishing and installing Python software. While these modules are designed to work in conjunction with the Python Package Index, they can also be used with a local index server, or without any index server at all.

➡ `• distutils — Building and installing Python modules`
➡ `• ensurepip — Bootstrapping the pip installer`
◦ Command line interface
◦ Module API

➡ `• venv — Creation of virtual environments`
◦ Creating virtual environments
◦ API
◦ An example of extending EnvBuilder

➡ `• zipapp — Manage executable Python zip archives`
◦ Basic Example
◦ Command-Line Interface
◦ Python API
◦ Examples
◦ Specifying the Interpreter
◦ Creating Standalone Applications with zipapp
◾ Making a Windows executable
◾ Caveats

◦ The Python Zip Application Archive Format

## ==⚡ • distutils — Building and installing Python modules
- PEP 632, Deprecate distutils module https://www.python.org/dev/peps/pep-0632
- Setup Tools https://setuptools.readthedocs.io/en/latest/
- Python Packaging User Guide https://packaging.python.org/

The distutils package is deprecated and slated for removal in Python 3.12. Use setup
tools or check PEP 632 for potential alternatives

distutils is deprecated with removal planned for Python 3.12. See the What’s New entry for more information.


The distutils package provides support for building and installing additional modules into a Python installation. The new modules may be either 100%-pure Python, or may be extension modules written in C, or may be collections of Python packages which include modules coded in both Python and C.

Most Python users will not want to use this module directly, but instead use the cross-version tools maintained by the Python Packaging Authority. In particular, setuptools is an enhanced alternative to distutils that provides:

• support for declaring project dependencies

• additional mechanisms for configuring which files to include in source releases (including plugins for integration with version control systems)

• the ability to declare project “entry points”, which can be used as the basis for application plugin systems

• the ability to automatically generate Windows command line executables at installation time rather than needing to prebuild them

• consistent behaviour across all supported Python versions


The recommended pip installer runs all setup.py scripts with setuptools, even if the script itself only imports distutils. Refer to the Python Packaging User Guide for more information.

For the benefits of packaging tool authors and users seeking a deeper understanding of the details of the current packaging and distribution system, the legacy distutils based user documentation and API reference remain available:

• Installing Python Modules (Legacy version)
• Distributing Python Modules (Legacy version)

## ==⚡ • Installing Python Modules (Legacy version)

## ==⚡ • Distributing Python Modules (Legacy version)

Distributing Python Modules (Legacy version)
Authors:
Greg Ward, Anthony Baxter 

Email:
distutils-sig@python.org 

See also:
 Distributing Python Modules
 The up to date module distribution documentations

Note:
 The entire distutils package has been deprecated and will be removed in Python 3.12. This documentation is retained as a reference only, and will be removed with the package. See the What’s New entry for more information.
 

Note:
 This document is being retained solely until the setuptools documentation at https://setuptools.readthedocs.io/en/latest/setuptools.html independently covers all of the relevant information currently included here.
 

Note:
 This guide only covers the basic tools for building and distributing extensions that are provided as part of this version of Python. Third party tools offer easier to use and more secure alternatives. Refer to the quick recommendations section in the Python Packaging User Guide for more information.
 

This document describes the Python Distribution Utilities (“Distutils”) from the module developer’s point of view, describing the underlying capabilities that setuptools builds on to allow Python developers to make Python modules and extensions readily available to a wider audience.

• 1. An Introduction to Distutils
◦ 1.1. Concepts & Terminology
◦ 1.2. A Simple Example
◦ 1.3. General Python terminology
◦ 1.4. Distutils-specific terminology

• 2. Writing the Setup Script
◦ 2.1. Listing whole packages
◦ 2.2. Listing individual modules
◦ 2.3. Describing extension modules
◦ 2.4. Relationships between Distributions and Packages
◦ 2.5. Installing Scripts
◦ 2.6. Installing Package Data
◦ 2.7. Installing Additional Files
◦ 2.8. Additional meta-data
◦ 2.9. Debugging the setup script

• 3. Writing the Setup Configuration File
• 4. Creating a Source Distribution
◦ 4.1. Specifying the files to distribute
◦ 4.2. Manifest-related options

• 5. Creating Built Distributions
◦ 5.1. Creating RPM packages
◦ 5.2. Cross-compiling on Windows

• 6. Distutils Examples
◦ 6.1. Pure Python distribution (by module)
◦ 6.2. Pure Python distribution (by package)
◦ 6.3. Single extension module
◦ 6.4. Checking a package
◦ 6.5. Reading the metadata

• 7. Extending Distutils
◦ 7.1. Integrating new commands
◦ 7.2. Adding new distribution types

• 8. Command Reference
◦ 8.1. Installing modules: the install command family
◦ 8.2. Creating a source distribution: the sdist command

• 9. API Reference
◦ 9.1. distutils.core — Core Distutils functionality
◦ 9.2. distutils.ccompiler — CCompiler base class
◦ 9.3. distutils.unixccompiler — Unix C Compiler
◦ 9.4. distutils.msvccompiler — Microsoft Compiler
◦ 9.5. distutils.bcppcompiler — Borland Compiler
◦ 9.6. distutils.cygwincompiler — Cygwin Compiler
◦ 9.7. distutils.archive_util — Archiving utilities
◦ 9.8. distutils.dep_util — Dependency checking
◦ 9.9. distutils.dir_util — Directory tree operations
◦ 9.10. distutils.file_util — Single file operations
◦ 9.11. distutils.util — Miscellaneous other utility functions
◦ 9.12. distutils.dist — The Distribution class
◦ 9.13. distutils.extension — The Extension class
◦ 9.14. distutils.debug — Distutils debug mode
◦ 9.15. distutils.errors — Distutils exceptions
◦ 9.16. distutils.fancy_getopt — Wrapper around the standard getopt module
◦ 9.17. distutils.filelist — The FileList class
◦ 9.18. distutils.log — Simple PEP 282-style logging
◦ 9.19. distutils.spawn — Spawn a sub-process
◦ 9.20. distutils.sysconfig — System configuration information
◦ 9.21. distutils.text_file — The TextFile class
◦ 9.22. distutils.version — Version number classes
◦ 9.23. distutils.cmd — Abstract base class for Distutils commands
◦ 9.24. Creating a new Distutils command
◦ 9.25. distutils.command — Individual Distutils commands
◦ 9.26. distutils.command.bdist — Build a binary installer
◦ 9.27. distutils.command.bdist_packager — Abstract base class for packagers
◦ 9.28. distutils.command.bdist_dumb — Build a “dumb” installer
◦ 9.29. distutils.command.bdist_msi — Build a Microsoft Installer binary package
◦ 9.30. distutils.command.bdist_rpm — Build a binary distribution as a Redhat RPM and SRPM
◦ 9.31. distutils.command.sdist — Build a source distribution
◦ 9.32. distutils.command.build — Build all files of a package
◦ 9.33. distutils.command.build_clib — Build any C libraries in a package
◦ 9.34. distutils.command.build_ext — Build any extensions in a package
◦ 9.35. distutils.command.build_py — Build the .py/.pyc files of a package
◦ 9.36. distutils.command.build_scripts — Build the scripts of a package
◦ 9.37. distutils.command.clean — Clean a package build area
◦ 9.38. distutils.command.config — Perform package configuration
◦ 9.39. distutils.command.install — Install a package
◦ 9.40. distutils.command.install_data — Install data files from a package
◦ 9.41. distutils.command.install_headers — Install C/C++ header files from a package
◦ 9.42. distutils.command.install_lib — Install library files from a package
◦ 9.43. distutils.command.install_scripts — Install script files from a package
◦ 9.44. distutils.command.register — Register a module with the Python Package Index
◦ 9.45. distutils.command.check — Check the meta-data of a package

### ===🗝 • 1. An Introduction to Distutils

Note:
 This document is being retained solely until the setuptools documentation at https://setuptools.readthedocs.io/en/latest/setuptools.html independently covers all of the relevant information currently included here.
 

This document covers using the Distutils to distribute your Python modules, concentrating on the role of developer/distributor: if you’re looking for information on installing Python modules, you should refer to the Installing Python Modules (Legacy version) chapter.


Note:
 The entire distutils package has been deprecated and will be removed in Python 3.12. This documentation is retained as a reference only, and will be removed with the package. See the What’s New entry for more information.
 

See also:
 Installing Python ModulesThe up to date module installation documentation. For regular Python usage, you almost certainly want that document rather than this one.

Note:
 This document is being retained solely until the setuptools documentation at https://setuptools.readthedocs.io/en/latest/setuptools.html independently covers all of the relevant information currently included here.
 

Note:
 This guide only covers the basic tools for building and distributing extensions that are provided as part of this version of Python. Third party tools offer easier to use and more secure alternatives. Refer to the quick recommendations section in the Python Packaging User Guide for more information.
 


#### ◦ Introduction

In Python 2.0, the distutils API was first added to the standard library. This provided Linux distro maintainers with a standard way of converting Python projects into Linux distro packages, and system administrators with a standard way of installing them directly onto target systems.

In the many years since Python 2.0 was released, tightly coupling the build system and package installer to the language runtime release cycle has turned out to be problematic, and it is now recommended that projects use the pip package installer and the setuptools build system, rather than using distutils directly.

See Installing Python Modules and Distributing Python Modules for more details.

This legacy documentation is being retained only until we’re confident that the setuptools documentation covers everything needed.


#### ◦ Distutils based source distributions

If you download a module source distribution, you can tell pretty quickly if it was packaged and distributed in the standard way, i.e. using the Distutils. First, the distribution’s name and version number will be featured prominently in the name of the downloaded archive, e.g. foo-1.0.tar.gz or widget-0.9.7.zip. Next, the archive will unpack into a similarly-named directory: foo-1.0 or widget-0.9.7. Additionally, the distribution will contain a setup script setup.py, and a file named README.txt or possibly just README, which should explain that building and installing the module distribution is a simple matter of running one command from a terminal:


    python setup.py install


For Windows, this command should be run from a command prompt window (Start ‣ Accessories):


    setup.py install


If all these things are true, then you already know how to build and install the modules you’ve just downloaded: Run the command above. Unless you need to install things in a non-standard way or customize the build process, you don’t really need this manual. Or rather, the above command is everything you need to get out of this manual.


#### ◦ Standard Build and Install

As described in section Distutils based source distributions, building and installing a module distribution using the Distutils is usually one simple command to run from a terminal:


    python setup.py install



#### ◦ Platform variations

You should always run the setup command from the distribution root directory, i.e. the top-level subdirectory that the module source distribution unpacks into. For example, if you’ve just downloaded a module source distribution foo-1.0.tar.gz onto a Unix system, the normal thing to do is:


    gunzip -c foo-1.0.tar.gz | tar xf -    # unpacks into directory foo-1.0
    cd foo-1.0
    python setup.py install


On Windows, you’d probably download foo-1.0.zip. If you downloaded the archive file to C:\Temp, then it would unpack into C:\Temp\foo-1.0; you can use either an archive manipulator with a graphical user interface (such as WinZip) or a command-line tool (such as unzip or pkunzip) to unpack the archive. Then, open a command prompt window and run:


    cd c:\Temp\foo-1.0
    python setup.py install



#### ◦ Splitting the job up

Running setup.py install builds and installs all modules in one run. If you prefer to work incrementally—especially useful if you want to customize the build process, or if things are going wrong—you can use the setup script to do one thing at a time. This is particularly helpful when the build and install will be done by different users—for example, you might want to build a module distribution and hand it off to a system administrator for installation (or do it yourself, with super-user privileges).

For example, you can build everything in one step, and then install everything in a second step, by invoking the setup script twice:


    python setup.py build
    python setup.py install


If you do this, you will notice that running the install command first runs the build command, which—in this case—quickly notices that it has nothing to do, since everything in the build directory is up-to-date.

You may not need this ability to break things down often if all you do is install modules downloaded off the ‘net, but it’s very handy for more advanced tasks. If you get into distributing your own Python modules and extensions, you’ll run lots of individual Distutils commands on their own.


#### ◦ How building works

As implied above, the build command is responsible for putting the files to install into a build directory. By default, this is build under the distribution root; if you’re excessively concerned with speed, or want to keep the source tree pristine, you can change the build directory with the --build-base option. For example:


    python setup.py build --build-base=/path/to/pybuild/foo-1.0


(Or you could do this permanently with a directive in your system or personal Distutils configuration file; see section Distutils Configuration Files.) Normally, this isn’t necessary.

The default layout for the build tree is as follows:


--- build/ --- lib/
or
--- build/ --- lib.<plat>/
               temp.<plat>/


where <plat> expands to a brief description of the current OS/hardware platform and Python version. The first form, with just a lib directory, is used for “pure module distributions”—that is, module distributions that include only pure Python modules. If a module distribution contains any extensions (modules written in C/C++), then the second form, with two <plat> directories, is used. In that case, the temp.plat directory holds temporary files generated by the compile/link process that don’t actually get installed. In either case, the lib (or lib.plat) directory contains all Python modules (pure Python and extensions) that will be installed.

In the future, more directories will be added to handle Python scripts, documentation, binary executables, and whatever else is needed to handle the job of installing Python modules and applications.


#### ◦ How installation works

After the build command runs (whether you run it explicitly, or the install command does it for you), the work of the install command is relatively simple: all it has to do is copy everything under build/lib (or build/lib.plat) to your chosen installation directory.

If you don’t choose an installation directory—i.e., if you just run setup.py install—then the install command installs to the standard location for third-party Python modules. This location varies by platform and by how you built/installed Python itself. On Unix (and macOS, which is also Unix-based), it also depends on whether the module distribution being installed is pure Python or contains extensions (“non-pure”):



|     Platform    |      Standard installation location     |               Default value                |
|-----------------|-----------------------------------------|--------------------------------------------|
| Unix (pure)     | prefix/lib/pythonX.Y/site-packages      | /usr/local/lib/pythonX.Y/site-packages (1) |
| Unix (non-pure) | exec-prefix/lib/pythonX.Y/site-packages | /usr/local/lib/pythonX.Y/site-packages (1) |
| Windows         | prefix\Lib\site-packages                | C:\PythonXY\Lib\site-packages (2)          |

Notes:

(1). Most Linux distributions include Python as a standard part of the system, so prefix and exec-prefix are usually both /usr on Linux. If you build Python yourself on Linux (or any Unix-like system), the default prefix and exec-prefix are /usr/local.

(2). The default installation directory on Windows was C:\Program Files\Python under Python 1.6a1, 1.5.2, and earlier.

prefix and exec-prefix stand for the directories that Python is installed to, and where it finds its libraries at run-time. They are always the same under Windows, and very often the same under Unix and macOS. You can find out what your Python installation uses for prefix and exec-prefix by running Python in interactive mode and typing a few simple commands. Under Unix, just type python at the shell prompt. Under Windows, choose Start ‣ Programs ‣ Python X.Y ‣ Python (command line). Once the interpreter is started, you type Python code at the prompt. For example, on my Linux system, I type the three Python statements shown below, and get the output as shown, to find out my prefix and exec-prefix:


Python 2.4 (#26, Aug  7 2004, 17:19:02)
Type "help", "copyright", "credits" or "license" for more information.
>>> import sys
>>> sys.prefix
'/usr'
>>> sys.exec_prefix
'/usr'


A few other placeholders are used in this document: X.Y stands for the version of Python, for example 3.2; abiflags will be replaced by the value of sys.abiflags or the empty string for platforms which don’t define ABI flags; distname will be replaced by the name of the module distribution being installed. Dots and capitalization are important in the paths; for example, a value that uses python3.2 on UNIX will typically use Python32 on Windows.

If you don’t want to install modules to the standard location, or if you don’t have permission to write there, then you need to read about alternate installations in section Alternate Installation. If you want to customize your installation directories more heavily, see section Custom Installation on custom installations.


#### ◦ Alternate Installation

Often, it is necessary or desirable to install modules to a location other than the standard location for third-party Python modules. For example, on a Unix system you might not have permission to write to the standard third-party module directory. Or you might wish to try out a module before making it a standard part of your local Python installation. This is especially true when upgrading a distribution already present: you want to make sure your existing base of scripts still works with the new version before actually upgrading.

The Distutils install command is designed to make installing module distributions to an alternate location simple and painless. The basic idea is that you supply a base directory for the installation, and the install command picks a set of directories (called an installation scheme) under this base directory in which to install files. The details differ across platforms, so read whichever of the following sections applies to you.

Note that the various alternate installation schemes are mutually exclusive: you can pass --user, or --home, or --prefix and --exec-prefix, or --install-base and --install-platbase, but you can’t mix from these groups.


Alternate installation: the user scheme

This scheme is designed to be the most convenient solution for users that don’t have write permission to the global site-packages directory or don’t want to install into it. It is enabled with a simple option:


    python setup.py install --user


Files will be installed into subdirectories of site.USER_BASE (written as userbase hereafter). This scheme installs pure Python modules and extension modules in the same location (also known as site.USER_SITE). Here are the values for UNIX, including macOS:


| Type of file |            Installation directory           |
|--------------|---------------------------------------------|
| modules      | userbase/lib/pythonX.Y/site-packages        |
| scripts      | userbase/bin                                |
| data         | userbase                                    |
| C headers    | userbase/include/pythonX.Yabiflags/distname |

And here are the values used on Windows:


| Type of file |        Installation directory       |
|--------------|-------------------------------------|
| modules      | userbase\PythonXY\site-packages     |
| scripts      | userbase\PythonXY\Scripts           |
| data         | userbase                            |
| C headers    | userbase\PythonXY\Include{distname} |

The advantage of using this scheme compared to the other ones described below is that the user site-packages directory is under normal conditions always included in sys.path (see site for more information), which means that there is no additional step to perform after running the setup.py script to finalize the installation.

The build_ext command also has a --user option to add userbase/include to the compiler search path for header files and userbase/lib to the compiler search path for libraries as well as to the runtime search path for shared C libraries (rpath).


Alternate installation: the home scheme

The idea behind the “home scheme” is that you build and maintain a personal stash of Python modules. This scheme’s name is derived from the idea of a “home” directory on Unix, since it’s not unusual for a Unix user to make their home directory have a layout similar to /usr/ or /usr/local/. This scheme can be used by anyone, regardless of the operating system they are installing for.

Installing a new module distribution is as simple as


    python setup.py install --home=<dir>


where you can supply any directory you like for the --home option. On Unix, lazy typists can just type a tilde ( ~ ); the install command will expand this to your home directory:


    python setup.py install --home=~


To make Python find the distributions installed with this scheme, you may have to modify Python’s search path or edit sitecustomize (see site) to call site.addsitedir() or edit sys.path.

The --home option defines the installation base directory. Files are installed to the following directories under the installation base as follows:


| Type of file |    Installation directory    |
|--------------|------------------------------|
| modules      | home/lib/python              |
| scripts      | home/bin                     |
| data         | home                         |
| C headers    | home/include/python/distname |

(Mentally replace slashes with backslashes if you’re on Windows.)


Alternate installation: Unix (the prefix scheme)

The “prefix scheme” is useful when you wish to use one Python installation to perform the build/install (i.e., to run the setup script), but install modules into the third-party module directory of a different Python installation (or something that looks like a different Python installation). If this sounds a trifle unusual, it is—that’s why the user and home schemes come before. However, there are at least two known cases where the prefix scheme will be useful.

First, consider that many Linux distributions put Python in /usr, rather than the more traditional /usr/local. This is entirely appropriate, since in those cases Python is part of “the system” rather than a local add-on. However, if you are installing Python modules from source, you probably want them to go in /usr/local/lib/python2.X rather than /usr/lib/python2.X. This can be done with


    /usr/bin/python setup.py install --prefix=/usr/local


Another possibility is a network filesystem where the name used to write to a remote directory is different from the name used to read it: for example, the Python interpreter accessed as /usr/local/bin/python might search for modules in /usr/local/lib/python2.X, but those modules would have to be installed to, say, /mnt/@server/export/lib/python2.X. This could be done with


    /usr/local/bin/python setup.py install --prefix=/mnt/@server/export


In either case, the --prefix option defines the installation base, and the --exec-prefix option defines the platform-specific installation base, which is used for platform-specific files. (Currently, this just means non-pure module distributions, but could be expanded to C libraries, binary executables, etc.) If --exec-prefix is not supplied, it defaults to --prefix. Files are installed as follows:



|    Type of file   |           Installation directory          |
|-------------------|-------------------------------------------|
| Python modules    | prefix/lib/pythonX.Y/site-packages        |
| extension modules | exec-prefix/lib/pythonX.Y/site-packages   |
| scripts           | prefix/bin                                |
| data              | prefix                                    |
| C headers         | prefix/include/pythonX.Yabiflags/distname |

There is no requirement that --prefix or --exec-prefix actually point to an alternate Python installation; if the directories listed above do not already exist, they are created at installation time.

Incidentally, the real reason the prefix scheme is important is simply that a standard Unix installation uses the prefix scheme, but with --prefix and --exec-prefix supplied by Python itself as sys.prefix and sys.exec_prefix. Thus, you might think you’ll never use the prefix scheme, but every time you run python setup.py install without any other options, you’re using it.

Note that installing extensions to an alternate Python installation has no effect on how those extensions are built: in particular, the Python header files (Python.h and friends) installed with the Python interpreter used to run the setup script will be used in compiling extensions. It is your responsibility to ensure that the interpreter used to run extensions installed in this way is compatible with the interpreter used to build them. The best way to do this is to ensure that the two interpreters are the same version of Python (possibly different builds, or possibly copies of the same build). (Of course, if your --prefix and --exec-prefix don’t even point to an alternate Python installation, this is immaterial.)


Alternate installation: Windows (the prefix scheme)

Windows has no concept of a user’s home directory, and since the standard Python installation under Windows is simpler than under Unix, the --prefix option has traditionally been used to install additional packages in separate locations on Windows.


    python setup.py install --prefix="\Temp\Python"


to install modules to the \Temp\Python directory on the current drive.

The installation base is defined by the --prefix option; the --exec-prefix option is not supported under Windows, which means that pure Python modules and extension modules are installed into the same location. Files are installed as follows:


| Type of file |  Installation directory  |
|--------------|--------------------------|
| modules      | prefix\Lib\site-packages |
| scripts      | prefix\Scripts           |
| data         | prefix                   |
| C headers    | prefix\Include{distname} |


Custom Installation

Sometimes, the alternate installation schemes described in section Alternate Installation just don’t do what you want. You might want to tweak just one or two directories while keeping everything under the same base directory, or you might want to completely redefine the installation scheme. In either case, you’re creating a custom installation scheme.

To create a custom installation scheme, you start with one of the alternate schemes and override some of the installation directories used for the various types of files, using these options:


|    Type of file   |  Override option  |
|-------------------|-------------------|
| Python modules    | --install-purelib |
| extension modules | --install-platlib |
| all modules       | --install-lib     |
| scripts           | --install-scripts |
| data              | --install-data    |
| C headers         | --install-headers |

These override options can be relative, absolute, or explicitly defined in terms of one of the installation base directories. (There are two installation base directories, and they are normally the same—they only differ when you use the Unix “prefix scheme” and supply different --prefix and --exec-prefix options; using --install-lib will override values computed or given for --install-purelib and --install-platlib, and is recommended for schemes that don’t make a difference between Python and extension modules.)

For example, say you’re installing a module distribution to your home directory under Unix—but you want scripts to go in ~/scripts rather than ~/bin. As you might expect, you can override this directory with the --install-scripts option; in this case, it makes most sense to supply a relative path, which will be interpreted relative to the installation base directory (your home directory, in this case):


    python setup.py install --home=~ --install-scripts=scripts


Another Unix example: suppose your Python installation was built and installed with a prefix of /usr/local/python, so under a standard installation scripts will wind up in /usr/local/python/bin. If you want them in /usr/local/bin instead, you would supply this absolute directory for the --install-scripts option:


    python setup.py install --install-scripts=/usr/local/bin


(This performs an installation using the “prefix scheme”, where the prefix is whatever your Python interpreter was installed with— /usr/local/python in this case.)

If you maintain Python on Windows, you might want third-party modules to live in a subdirectory of prefix, rather than right in prefix itself. This is almost as easy as customizing the script installation directory—you just have to remember that there are two types of modules to worry about, Python and extension modules, which can conveniently be both controlled by one option:


    python setup.py install --install-lib=Site


The specified installation directory is relative to prefix. Of course, you also have to ensure that this directory is in Python’s module search path, such as by putting a .pth file in a site directory (see site). See section Modifying Python’s Search Path to find out how to modify Python’s search path.

If you want to define an entire installation scheme, you just have to supply all of the installation directory options. The recommended way to do this is to supply relative paths; for example, if you want to maintain all Python module-related files under python in your home directory, and you want a separate directory for each platform that you use your home directory from, you might define the following installation scheme:


    python setup.py install --home=~ \
                        --install-purelib=python/lib \
                        --install-platlib=python/lib.$PLAT \
                        --install-scripts=python/scripts
                        --install-data=python/data


or, equivalently,


    python setup.py install --home=~/python \
                        --install-purelib=lib \
                        --install-platlib='lib.$PLAT' \
                        --install-scripts=scripts
                        --install-data=data


$PLAT is not (necessarily) an environment variable—it will be expanded by the Distutils as it parses your command line options, just as it does when parsing your configuration file(s).

Obviously, specifying the entire installation scheme every time you install a new module distribution would be very tedious. Thus, you can put these options into your Distutils config file (see section Distutils Configuration Files):


[install]
install-base=$HOME
install-purelib=python/lib
install-platlib=python/lib.$PLAT
install-scripts=python/scripts
install-data=python/data


or, equivalently,


[install]
install-base=$HOME/python
install-purelib=lib
install-platlib=lib.$PLAT
install-scripts=scripts
install-data=data


Note that these two are not equivalent if you supply a different installation base directory when you run the setup script. For example,


    python setup.py install --install-base=/tmp


would install pure modules to /tmp/python/lib in the first case, and to /tmp/lib in the second case. (For the second case, you probably want to supply an installation base of /tmp/python.)

You probably noticed the use of $HOME and $PLAT in the sample configuration file input. These are Distutils configuration variables, which bear a strong resemblance to environment variables. In fact, you can use environment variables in config files on platforms that have such a notion but the Distutils additionally define a few extra variables that may not be in your environment, such as $PLAT. (And of course, on systems that don’t have environment variables, such as Mac OS 9, the configuration variables supplied by the Distutils are the only ones you can use.) See section Distutils Configuration Files for details.

Note:
 When a virtual environment is activated, any options that change the installation path will be ignored from all distutils configuration files to prevent inadvertently installing projects outside of the virtual environment.
 


#### ◦ Modifying Python’s Search Path

When the Python interpreter executes an import statement, it searches for both Python code and extension modules along a search path. A default value for the path is configured into the Python binary when the interpreter is built. You can determine the path by importing the sys module and printing the value of sys.path.


>$ python
Python 2.2 (#11, Oct  3 2002, 13:31:27)
[GCC 2.96 20000731 (Red Hat Linux 7.3 2.96-112)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import sys
>>> sys.path
['', '/usr/local/lib/python2.3', '/usr/local/lib/python2.3/plat-linux2',
 '/usr/local/lib/python2.3/lib-tk', '/usr/local/lib/python2.3/lib-dynload',
 '/usr/local/lib/python2.3/site-packages']
>>>


The null string in sys.path represents the current working directory.

The expected convention for locally installed packages is to put them in the …/site-packages/ directory, but you may want to install Python modules into some arbitrary directory. For example, your site may have a convention of keeping all software related to the web server under /www. Add-on Python modules might then belong in /www/python, and in order to import them, this directory must be added to sys.path. There are several different ways to add the directory.

The most convenient way is to add a path configuration file to a directory that’s already on Python’s path, usually to the .../site-packages/ directory. Path configuration files have an extension of .pth, and each line must contain a single path that will be appended to sys.path. (Because the new paths are appended to sys.path, modules in the added directories will not override standard modules. This means you can’t use this mechanism for installing fixed versions of standard modules.)

Paths can be absolute or relative, in which case they’re relative to the directory containing the .pth file. See the documentation of the site module for more information.

A slightly less convenient way is to edit the site.py file in Python’s standard library, and modify sys.path. site.py is automatically imported when the Python interpreter is executed, unless the -S switch is supplied to suppress this behaviour. So you could simply edit site.py and add two lines to it:


    import sys
    sys.path.append('/www/python/')


However, if you reinstall the same major version of Python (perhaps when upgrading from 2.2 to 2.2.2, for example) site.py will be overwritten by the stock version. You’d have to remember that it was modified and save a copy before doing the installation.

There are two environment variables that can modify sys.path. PYTHONHOME sets an alternate value for the prefix of the Python installation. For example, if PYTHONHOME is set to /www/python, the search path will be set to ['', '/www/python/lib/pythonX.Y/', '/www/python/lib/pythonX.Y/plat-linux2', ...].

The PYTHONPATH variable can be set to a list of paths that will be added to the beginning of sys.path. For example, if PYTHONPATH is set to /www/python:/opt/py, the search path will begin with ['/www/python', '/opt/py']. (Note that directories must exist in order to be added to sys.path; the site module removes paths that don’t exist.)

Finally, sys.path is just a regular Python list, so any Python application can modify it by adding or removing entries.


#### ◦ Distutils Configuration Files

As mentioned above, you can use Distutils configuration files to record personal or site preferences for any Distutils options. That is, any option to any command can be stored in one of two or three (depending on your platform) configuration files, which will be consulted before the command-line is parsed. This means that configuration files will override default values, and the command-line will in turn override configuration files. Furthermore, if multiple configuration files apply, values from “earlier” files are overridden by “later” files.


Location and names of config files

The names and locations of the configuration files vary slightly across platforms. On Unix and macOS, the three configuration files (in the order they are processed) are:


| Type of file |              Location and filename               |
|--------------|--------------------------------------------------|
| system       | prefix/lib/pythonver/distutils/distutils.cfg (1) |
| personal     | $HOME/.pydistutils.cfg (2)                       |
| local        | setup.cfg (3)                                    |

And on Windows, the configuration files are:


| Type of file | Location and filename                  |
| system       | prefix\Lib\distutils\distutils.cfg (4) |
| personal     | %HOME%\pydistutils.cfg (5)             |
| local        | setup.cfg (3)                          |

On all platforms, the “personal” file can be temporarily disabled by passing the –no-user-cfg option.

Notes:

(1). Strictly speaking, the system-wide configuration file lives in the directory where the Distutils are installed; under Python 1.6 and later on Unix, this is as shown. For Python 1.5.2, the Distutils will normally be installed to prefix/lib/python1.5/site-packages/distutils, so the system configuration file should be put there under Python 1.5.2.

(2). On Unix, if the HOME environment variable is not defined, the user’s home directory will be determined with the getpwuid() function from the standard pwd module. This is done by the os.path.expanduser() function used by Distutils.

(3). I.e., in the current directory (usually the location of the setup script).

(4). (See also note (1).) Under Python 1.6 and later, Python’s default “installation prefix” is C:\Python, so the system configuration file is normally C:\Python\Lib\distutils\distutils.cfg. Under Python 1.5.2, the default prefix was C:\Program Files\Python, and the Distutils were not part of the standard library—so the system configuration file would be C:\Program Files\Python\distutils\distutils.cfg in a standard Python 1.5.2 installation under Windows.

(5). On Windows, if the HOME environment variable is not defined, USERPROFILE then HOMEDRIVE and HOMEPATH will be tried. This is done by the os.path.expanduser() function used by Distutils.


#### ◦ Syntax of config files

The Distutils configuration files all have the same syntax. The config files are grouped into sections. There is one section for each Distutils command, plus a global section for global options that affect every command. Each section consists of one option per line, specified as option=value.

For example, the following is a complete config file that just forces all commands to run quietly by default:


    [global]
    verbose=0


If this is installed as the system config file, it will affect all processing of any Python module distribution by any user on the current system. If it is installed as your personal config file (on systems that support them), it will affect only module distributions processed by you. And if it is used as the setup.cfg for a particular module distribution, it affects only that distribution.

You could override the default “build base” directory and make the build* commands always forcibly rebuild all files with the following:


    [build]
    build-base=blib
    force=1


which corresponds to the command-line arguments


    python setup.py build --build-base=blib --force


except that including the build command on the command-line means that command will be run. Including a particular command in config files has no such implication; it only means that if the command is run, the options in the config file will apply. (Or if other commands that derive values from it are run, they will use the values in the config file.)

You can find out the complete list of options for any command using the --help option, e.g.:


    python setup.py build --help


and you can find out the complete list of global options by using --help without a command:


    python setup.py --help


See also the “Reference” section of the “Distributing Python Modules” manual.


#### ◦ Building Extensions: Tips and Tricks

Whenever possible, the Distutils try to use the configuration information made available by the Python interpreter used to run the setup.py script. For example, the same compiler and linker flags used to compile Python will also be used for compiling extensions. Usually this will work well, but in complicated situations this might be inappropriate. This section discusses how to override the usual Distutils behaviour.


Tweaking compiler/linker flags

Compiling a Python extension written in C or C++ will sometimes require specifying custom flags for the compiler and linker in order to use a particular library or produce a special kind of object code. This is especially true if the extension hasn’t been tested on your platform, or if you’re trying to cross-compile Python.

In the most general case, the extension author might have foreseen that compiling the extensions would be complicated, and provided a Setup file for you to edit. This will likely only be done if the module distribution contains many separate extension modules, or if they often require elaborate sets of compiler flags in order to work.

A Setup file, if present, is parsed in order to get a list of extensions to build. Each line in a Setup describes a single module. Lines have the following structure:


    module ... [sourcefile ...] [cpparg ...] [library ...]


Let’s examine each of the fields in turn.
•module is the name of the extension module to be built, and should be a valid Python identifier. You can’t just change this in order to rename a module (edits to the source code would also be needed), so this should be left alone.
•sourcefile is anything that’s likely to be a source code file, at least judging by the filename. Filenames ending in .c are assumed to be written in C, filenames ending in .C, .cc, and .c++ are assumed to be C++, and filenames ending in .m or .mm are assumed to be in Objective C.
•cpparg is an argument for the C preprocessor, and is anything starting with -I, -D, -U or -C.
•library is anything ending in .a or beginning with -l or -L.

If a particular platform requires a special library on your platform, you can add it by editing the Setup file and running python setup.py build. For example, if the module defined by the line


    foo foomodule.c


must be linked with the math library libm.a on your platform, simply add -lm to the line:


    foo foomodule.c -lm


Arbitrary switches intended for the compiler or the linker can be supplied with the -Xcompiler arg and -Xlinker arg options:


    foo foomodule.c -Xcompiler -o32 -Xlinker -shared -lm


The next option after -Xcompiler and -Xlinker will be appended to the proper command line, so in the above example the compiler will be passed the -o32 option, and the linker will be passed -shared. If a compiler option requires an argument, you’ll have to supply multiple -Xcompiler options; for example, to pass -x c++ the Setup file would have to contain -Xcompiler -x -Xcompiler c++.

Compiler flags can also be supplied through setting the CFLAGS environment variable. If set, the contents of CFLAGS will be added to the compiler flags specified in the Setup file.


#### ◦ Using non-Microsoft compilers on Windows


#### ◦ Borland/CodeGear C++

This subsection describes the necessary steps to use Distutils with the Borland C++ compiler version 5.5. First you have to know that Borland’s object file format (OMF) is different from the format used by the Python version you can download from the Python or ActiveState web site. (Python is built with Microsoft Visual C++, which uses COFF as the object file format.) For this reason you have to convert Python’s library python25.lib into the Borland format. You can do this as follows:


    coff2omf python25.lib python25_bcpp.lib


The coff2omf program comes with the Borland compiler. The file python25.lib is in the Libs directory of your Python installation. If your extension uses other libraries (zlib, …) you have to convert them too.

The converted files have to reside in the same directories as the normal libraries.

How does Distutils manage to use these libraries with their changed names? If the extension needs a library (eg. foo) Distutils checks first if it finds a library with suffix `_bcpp` (eg. foo_bcpp.lib) and then uses this library. In the case it doesn’t find such a special library it uses the default name (foo.lib.) [1]

To let Distutils compile your extension with Borland C++ you now have to type:


    python setup.py build --compiler=bcpp


If you want to use the Borland C++ compiler as the default, you could specify this in your personal or system-wide configuration file for Distutils (see section Distutils Configuration Files.)

See also:
 C++Builder CompilerInformation about the free C++ compiler from Borland, including links to the download pages.Creating Python Extensions Using Borland’s Free CompilerDocument describing how to use Borland’s free command-line C++ compiler to build Python.

#### ◦ GNU C / Cygwin / MinGW

This section describes the necessary steps to use Distutils with the GNU C/C++ compilers in their Cygwin and MinGW distributions. [2] For a Python interpreter that was built with Cygwin, everything should work without any of these following steps.

Not all extensions can be built with MinGW or Cygwin, but many can. Extensions most likely to not work are those that use C++ or depend on Microsoft Visual C extensions.

To let Distutils compile your extension with Cygwin you have to type:


    python setup.py build --compiler=cygwin


and for Cygwin in no-cygwin mode [3] or for MinGW type:


    python setup.py build --compiler=mingw32


If you want to use any of these options/compilers as default, you should consider writing it in your personal or system-wide configuration file for Distutils (see section Distutils Configuration Files.)


#### ◦ Older Versions of Python and MinGW

The following instructions only apply if you’re using a version of Python inferior to 2.4.1 with a MinGW inferior to 3.0.0 (with binutils-2.13.90-20030111-1).

These compilers require some special libraries. This task is more complex than for Borland’s C++, because there is no program to convert the library. First you have to create a list of symbols which the Python DLL exports. (You can find a good program for this task at https://sourceforge.net/projects/mingw/files/MinGW/Extension/pexports/).


    pexports python25.dll >python25.def


The location of an installed python25.dll will depend on the installation options and the version and language of Windows. In a “just for me” installation, it will appear in the root of the installation directory. In a shared installation, it will be located in the system directory.

Then you can create from these information an import library for gcc.


    /cygwin/bin/dlltool --dllname python25.dll --def python25.def --output-lib libpython25.a


The resulting library has to be placed in the same directory as python25.lib. (Should be the libs directory under your Python installation directory.)

If your extension uses other libraries (zlib,…) you might have to convert them too. The converted files have to reside in the same directories as the normal libraries do.

See also:
 Building Python modules on MS Windows platform with MinGWInformation about building the required libraries for the MinGW environment.

Footnotes

[1] This also means you could replace all existing COFF-libraries with OMF-libraries of the same name. 

[2] Check https://www.sourceware.org/cygwin/ for more information 

[3] Then you have no POSIX emulation available, but you also don’t need cygwin1.dll. 



#### ◦ 1.1. Concepts & Terminology


Using the Distutils is quite simple, both for module developers and for users/administrators installing third-party modules. As a developer, your responsibilities (apart from writing solid, well-documented and well-tested code, of course!) are:

• write a setup script (setup.py by convention)
• (optional) write a setup configuration file
• create a source distribution
• (optional) create one or more built (binary) distributions

Each of these tasks is covered in this document.

Not all module developers have access to a multitude of platforms, so it’s not always feasible to expect them to create a multitude of built distributions. It is hoped that a class of intermediaries, called packagers, will arise to address this need. Packagers will take source distributions released by module developers, build them on one or more platforms, and release the resulting built distributions. Thus, users on the most popular platforms will be able to install most popular Python module distributions in the most natural way for their platform, without having to run a single setup script or compile a line of code.




#### ◦ 1.2. A Simple Example


The setup script is usually quite simple, although since it’s written in Python, there are no arbitrary limits to what you can do with it, though you should be careful about putting arbitrarily expensive operations in your setup script. Unlike, say, Autoconf-style configure scripts, the setup script may be run multiple times in the course of building and installing your module distribution.

If all you want to do is distribute a module called `foo`, contained in a file `foo.py`, then your setup script can be as simple as this:


```py
from distutils.core import setup
setup(name='foo',
      version='1.0',
      py_modules=['foo'],
      )
```


Some observations:

• most information that you supply to the Distutils is supplied as keyword arguments to the setup() function

• those keyword arguments fall into two categories: package metadata (name, version number) and information about what’s in the package (a list of pure Python modules, in this case)

• modules are specified by module name, not filename (the same will hold true for packages and extensions)

• it’s recommended that you supply a little more metadata, in particular your name, email address and a URL for the project (see section Writing the Setup Script for an example)


To create a source distribution for this module, you would create a setup script, setup.py, containing the above code, and run this command from a terminal:


    python setup.py sdist


For Windows, open a command prompt window (Start ‣ Accessories) and change the command to:


    setup.py sdist


sdist will create an archive file (e.g., tarball on Unix, ZIP file on Windows) containing your setup script setup.py, and your module foo.py. The archive file will be named foo-1.0.tar.gz (or .zip), and will unpack into a directory foo-1.0.

If an end-user wishes to install your foo module, all they have to do is download foo-1.0.tar.gz (or .zip), unpack it, and—from the foo-1.0 directory—run


    python setup.py install


which will ultimately copy foo.py to the appropriate directory for third-party modules in their Python installation.

This simple example demonstrates some fundamental concepts of the Distutils. First, both developers and installers have the same basic user interface, i.e. the setup script. The difference is which Distutils commands they use: the sdist command is almost exclusively for module developers, while install is more often for installers (although most developers will want to install their own code occasionally).

Other useful built distribution formats are RPM, implemented by the bdist_rpm command, Solaris pkgtool (bdist_pkgtool), and HP-UX swinstall (bdist_sdux). For example, the following command will create an RPM file called `foo-1.0.noarch.rpm`:


    python setup.py bdist_rpm


(The bdist_rpm command uses the rpm executable, therefore this has to be run on an RPM-based system such as Red Hat Linux, SuSE Linux, or Mandrake Linux.)

You can find out what distribution formats are available at any time by running


    python setup.py bdist --help-formats


#### ◦ 1.3. General Python terminology


If you’re reading this document, you probably have a good idea of what modules, extensions, and so forth are. Nevertheless, just to be sure that everyone is operating from a common starting point, we offer the following glossary of common Python terms:

➡ `module`
the basic unit of code reusability in Python: a block of code imported by some other code. Three types of modules concern us here: pure Python modules, extension modules, and packages.

➡ `pure Python module`
a module written in Python and contained in a single .py file (and possibly associated .pyc files). Sometimes referred to as a “pure module.”

➡ `extension module`
a module written in the low-level language of the Python implementation: C/C++ for Python, Java for Jython. Typically contained in a single dynamically loadable pre-compiled file, e.g. a shared object (.so) file for Python extensions on Unix, a DLL (given the .pyd extension) for Python extensions on Windows, or a Java class file for Jython extensions. (Note that currently, the Distutils only handles C/C++ extensions for Python.)

➡ `package`
a module that contains other modules; typically contained in a directory in the filesystem and distinguished from other directories by the presence of a file __init__.py.

➡ `root package`
the root of the hierarchy of packages. (This isn’t really a package, since it doesn’t have an __init__.py file. But we have to call it something.) The vast majority of the standard library is in the root package, as are many small, standalone third-party modules that don’t belong to a larger module collection. Unlike regular packages, modules in the root package can be found in many directories: in fact, every directory listed in sys.path contributes modules to the root package.



#### ◦ 1.4. Distutils-specific terminology


The following terms apply more specifically to the domain of distributing Python modules using the Distutils:

➡ `module distribution`
a collection of Python modules distributed together as a single downloadable resource and meant to be installed en masse. Examples of some well-known module distributions are NumPy, SciPy, Pillow, or mxBase. (This would be called a package, except that term is already taken in the Python context: a single module distribution may contain zero, one, or many Python packages.)

➡ `pure module distribution`
a module distribution that contains only pure Python modules and packages. Sometimes referred to as a “pure distribution.”

➡ `non-pure module distribution`
a module distribution that contains at least one extension module. Sometimes referred to as a “non-pure distribution.”

➡ `distribution root`
the top-level directory of your source tree (or source distribution); the directory where setup.py exists. Generally setup.py will be run from this directory.




### ===🗝 • 2. Writing the Setup Script



The setup script is the centre of all activity in building, distributing, and installing modules using the Distutils. The main purpose of the setup script is to describe your module distribution to the Distutils, so that the various commands that operate on your modules do the right thing. As we saw in section A Simple Example above, the setup script consists mainly of a call to setup(), and most information supplied to the Distutils by the module developer is supplied as keyword arguments to setup().

Here’s a slightly more involved example, which we’ll follow for the next couple of sections: the Distutils’ own setup script. (Keep in mind that although the Distutils are included with Python 1.6 and later, they also have an independent existence so that Python 1.5.2 users can use them to install other module distributions. The Distutils’ own setup script, shown here, is used to install the package into Python 1.5.2.)


```py
#!/usr/bin/env python

from distutils.core import setup

setup(name='Distutils',
      version='1.0',
      description='Python Distribution Utilities',
      author='Greg Ward',
      author_email='gward@python.net',
      url='https://www.python.org/sigs/distutils-sig/',
      packages=['distutils', 'distutils.command'],
     )
```


There are only two differences between this and the trivial one-file distribution presented in section A Simple Example: more metadata, and the specification of pure Python modules by package, rather than by module. This is important since the Distutils consist of a couple of dozen modules split into (so far) two packages; an explicit list of every module would be tedious to generate and difficult to maintain. For more information on the additional meta-data, see section Additional meta-data.

Note that any pathnames (files or directories) supplied in the setup script should be written using the Unix convention, i.e. slash-separated. The Distutils will take care of converting this platform-neutral representation into whatever is appropriate on your current platform before actually using the pathname. This makes your setup script portable across operating systems, which of course is one of the major goals of the Distutils. In this spirit, all pathnames in this document are slash-separated.

This, of course, only applies to pathnames given to Distutils functions. If you, for example, use standard Python functions such as glob.glob() or os.listdir() to specify files, you should be careful to write portable code instead of hardcoding path separators:


```py
glob.glob(os.path.join('mydir', 'subdir', '*.html'))
os.listdir(os.path.join('mydir', 'subdir'))
```



#### ◦ 2.1. Listing whole packages

The packages option tells the Distutils to process (build, distribute, install, etc.) all pure Python modules found in each package mentioned in the packages list. In order to do this, of course, there has to be a correspondence between package names and directories in the filesystem. The default correspondence is the most obvious one, i.e. package distutils is found in the directory distutils relative to the distribution root. Thus, when you say packages = ['foo'] in your setup script, you are promising that the Distutils will find a file foo/__init__.py (which might be spelled differently on your system, but you get the idea) relative to the directory where your setup script lives. If you break this promise, the Distutils will issue a warning but still process the broken package anyway.

If you use a different convention to lay out your source directory, that’s no problem: you just have to supply the package_dir option to tell the Distutils about your convention. For example, say you keep all Python source under lib, so that modules in the “root package” (i.e., not in any package at all) are in lib, modules in the foo package are in lib/foo, and so forth. Then you would put


      package_dir = {'': 'lib'}


in your setup script. The keys to this dictionary are package names, and an empty package name stands for the root package. The values are directory names relative to your distribution root. In this case, when you say packages = ['foo'], you are promising that the file lib/foo/__init__.py exists.

Another possible convention is to put the foo package right in lib, the foo.bar package in lib/bar, etc. This would be written in the setup script as


      package_dir = {'foo': 'lib'}


A package: dir entry in the package_dir dictionary implicitly applies to all packages below package, so the foo.bar case is automatically handled here. In this example, having packages = ['foo', 'foo.bar'] tells the Distutils to look for lib/__init__.py and lib/bar/__init__.py. (Keep in mind that although package_dir applies recursively, you must explicitly list all packages in packages: the Distutils will not recursively scan your source tree looking for any directory with an __init__.py file.)



#### ◦ 2.2. Listing individual modules

For a small module distribution, you might prefer to list all modules rather than listing packages—especially the case of a single module that goes in the “root package” (i.e., no package at all). This simplest case was shown in section A Simple Example; here is a slightly more involved example:


      py_modules = ['mod1', 'pkg.mod2']


This describes two modules, one of them in the “root” package, the other in the pkg package. Again, the default package/directory layout implies that these two modules can be found in mod1.py and pkg/mod2.py, and that pkg/__init__.py exists as well. And again, you can override the package/directory correspondence using the package_dir option.


#### ◦ 2.3. Describing extension modules

Just as writing Python extension modules is a bit more complicated than writing pure Python modules, describing them to the Distutils is a bit more complicated. Unlike pure modules, it’s not enough just to list modules or packages and expect the Distutils to go out and find the right files; you have to specify the extension name, source file(s), and any compile/link requirements (include directories, libraries to link with, etc.).

All of this is done through another keyword argument to setup(), the ext_modules option. ext_modules is just a list of Extension instances, each of which describes a single extension module. Suppose your distribution includes a single extension, called foo and implemented by foo.c. If no additional instructions to the compiler/linker are needed, describing this extension is quite simple:


      Extension('foo', ['foo.c'])


The Extension class can be imported from distutils.core along with setup(). Thus, the setup script for a module distribution that contains only this one extension and nothing else might be:


```py
from distutils.core import setup, Extension
setup(name='foo',
      version='1.0',
      ext_modules=[Extension('foo', ['foo.c'])],
      )
```


The Extension class (actually, the underlying extension-building machinery implemented by the build_ext command) supports a great deal of flexibility in describing Python extensions, which is explained in the following sections.


2.3.1. Extension names and packages


The first argument to the Extension constructor is always the name of the extension, including any package names. For example,


      Extension('foo', ['src/foo1.c', 'src/foo2.c'])


describes an extension that lives in the root package, while


      Extension('pkg.foo', ['src/foo1.c', 'src/foo2.c'])


describes the same extension in the pkg package. The source files and resulting object code are identical in both cases; the only difference is where in the filesystem (and therefore where in Python’s namespace hierarchy) the resulting extension lives.

If you have a number of extensions all in the same package (or all under the same base package), use the ext_package keyword argument to setup(). For example,


```py
setup(...,
      ext_package='pkg',
      ext_modules=[Extension('foo', ['foo.c']),
                   Extension('subpkg.bar', ['bar.c'])],
     )
```


will compile foo.c to the extension pkg.foo, and bar.c to pkg.subpkg.bar.



2.3.2. Extension source files

The second argument to the Extension constructor is a list of source files. Since the Distutils currently only support C, C++, and Objective-C extensions, these are normally C/C++/Objective-C source files. (Be sure to use appropriate extensions to distinguish C++ source files: .cc and .cpp seem to be recognized by both Unix and Windows compilers.)

However, you can also include SWIG interface (.i) files in the list; the build_ext command knows how to deal with SWIG extensions: it will run SWIG on the interface file and compile the resulting C/C++ file into your extension.

This warning notwithstanding, options to SWIG can be currently passed like this:


```py
setup(...,
      ext_modules=[Extension('_foo', ['foo.i'],
                             swig_opts=['-modern', '-I../include'])],
      py_modules=['foo'],
     )
```


Or on the commandline like this:


> python setup.py build_ext --swig-opts="-modern -I../include"


On some platforms, you can include non-source files that are processed by the compiler and included in your extension. Currently, this just means Windows message text (.mc) files and resource definition (.rc) files for Visual C++. These will be compiled to binary resource (.res) files and linked into the executable.


2.3.3. Preprocessor options

Three optional arguments to Extension will help if you need to specify include directories to search or preprocessor macros to define/undefine: include_dirs, define_macros, and undef_macros.

For example, if your extension requires header files in the include directory under your distribution root, use the include_dirs option:


      Extension('foo', ['foo.c'], include_dirs=['include'])


You can specify absolute directories there; if you know that your extension will only be built on Unix systems with X11R6 installed to /usr, you can get away with


      Extension('foo', ['foo.c'], include_dirs=['/usr/include/X11'])


You should avoid this sort of non-portable usage if you plan to distribute your code: it’s probably better to write C code like


      #include <X11/Xlib.h>


If you need to include header files from some other Python extension, you can take advantage of the fact that header files are installed in a consistent way by the Distutils install_headers command. For example, the Numerical Python header files are installed (on a standard Unix installation) to /usr/local/include/python1.5/Numerical. (The exact location will differ according to your platform and Python installation.) Since the Python include directory—/usr/local/include/python1.5 in this case—is always included in the search path when building Python extensions, the best approach is to write C code like


      #include <Numerical/arrayobject.h>


If you must put the Numerical include directory right into your header search path, though, you can find that directory using the Distutils distutils.sysconfig module:


```py
from distutils.sysconfig import get_python_inc
incdir = os.path.join(get_python_inc(plat_specific=1), 'Numerical')
setup(...,
      Extension(..., include_dirs=[incdir]),
      )
```


Even though this is quite portable—it will work on any Python installation, regardless of platform—it’s probably easier to just write your C code in the sensible way.

You can define and undefine pre-processor macros with the define_macros and undef_macros options. define_macros takes a list of (name, value) tuples, where name is the name of the macro to define (a string) and value is its value: either a string or None. (Defining a macro FOO to None is the equivalent of a bare #define FOO in your C source: with most compilers, this sets FOO to the string 1.) undef_macros is just a list of macros to undefine.

For example:


```py
Extension(...,
          define_macros=[('NDEBUG', '1'),
                         ('HAVE_STRFTIME', None)],
          undef_macros=['HAVE_FOO', 'HAVE_BAR'])
```


is the equivalent of having this at the top of every C source file:


```c
#define NDEBUG 1
#define HAVE_STRFTIME
#undef HAVE_FOO
#undef HAVE_BAR
```



2.3.4. Library options

You can also specify the libraries to link against when building your extension, and the directories to search for those libraries. The libraries option is a list of libraries to link against, library_dirs is a list of directories to search for libraries at link-time, and runtime_library_dirs is a list of directories to search for shared (dynamically loaded) libraries at run-time.

For example, if you need to link against libraries known to be in the standard library search path on target systems


```py
Extension(...,
          libraries=['gdbm', 'readline'])
```


If you need to link with libraries in a non-standard location, you’ll have to include the location in library_dirs:


```py
Extension(...,
          library_dirs=['/usr/X11R6/lib'],
          libraries=['X11', 'Xt'])

```

(Again, this sort of non-portable construct should be avoided if you intend to distribute your code.)


2.3.5. Other options

There are still some other options which can be used to handle special cases.

The optional option is a boolean; if it is true, a build failure in the extension will not abort the build process, but instead simply not install the failing extension.

The extra_objects option is a list of object files to be passed to the linker. These files must not have extensions, as the default extension for the compiler is used.

extra_compile_args and extra_link_args can be used to specify additional command line options for the respective compiler and linker command lines.

export_symbols is only useful on Windows. It can contain a list of symbols (functions or variables) to be exported. This option is not needed when building compiled extensions: Distutils will automatically add initmodule to the list of exported symbols.

The depends option is a list of files that the extension depends on (for example header files). The build command will call the compiler on the sources to rebuild extension if any on this files has been modified since the previous build.


#### ◦ 2.4. Relationships between Distributions and Packages

A distribution may relate to packages in three specific ways:

1. It can require packages or modules.
2. It can provide packages or modules.
3. It can obsolete packages or modules.

These relationships can be specified using keyword arguments to the distutils.core.setup() function.

Dependencies on other Python modules and packages can be specified by supplying the requires keyword argument to setup(). The value must be a list of strings. Each string specifies a package that is required, and optionally what versions are sufficient.

To specify that any version of a module or package is required, the string should consist entirely of the module or package name. Examples include 'mymodule' and 'xml.parsers.expat'.

If specific versions are required, a sequence of qualifiers can be supplied in parentheses. Each qualifier may consist of a comparison operator and a version number. The accepted comparison operators are:


<    >    ==
<=   >=   !=


These can be combined by using multiple qualifiers separated by commas (and optional whitespace). In this case, all of the qualifiers must be matched; a logical AND is used to combine the evaluations.

Let’s look at a bunch of examples:


| Requires Expression | Explanation |
|---------------------|-------------|
| ==1.0               | Only version 1.0 is compatible 
| >1.0, !=1.5.1, <2.0 | Any version after 1.0 and before 2.0 is compatible, except 1.5.1 

Now that we can specify dependencies, we also need to be able to specify what we provide that other distributions can require. This is done using the provides keyword argument to setup(). The value for this keyword is a list of strings, each of which names a Python module or package, and optionally identifies the version. If the version is not specified, it is assumed to match that of the distribution.

Some examples:


| Provides Expression | Explanation |
|---------------------|-------------|
| mypkg               | Provide mypkg, using the distribution version 
| mypkg (1.1)         | Provide mypkg version 1.1, regardless of the distribution version 

A package can declare that it obsoletes other packages using the obsoletes keyword argument. The value for this is similar to that of the requires keyword: a list of strings giving module or package specifiers. Each specifier consists of a module or package name optionally followed by one or more version qualifiers. Version qualifiers are given in parentheses after the module or package name.

The versions identified by the qualifiers are those that are obsoleted by the distribution being described. If no qualifiers are given, all versions of the named module or package are understood to be obsoleted.



#### ◦ 2.5. Installing Scripts

So far we have been dealing with pure and non-pure Python modules, which are usually not run by themselves but imported by scripts.

Scripts are files containing Python source code, intended to be started from the command line. Scripts don’t require Distutils to do anything very complicated. The only clever feature is that if the first line of the script starts with #! and contains the word “python”, the Distutils will adjust the first line to refer to the current interpreter location. By default, it is replaced with the current interpreter location. The --executable (or -e) option will allow the interpreter path to be explicitly overridden.

The scripts option simply is a list of files to be handled in this way. From the PyXML setup script:


```py
setup(...,
      scripts=['scripts/xmlproc_parse', 'scripts/xmlproc_val']
      )
```



Changed in version 3.1: All the scripts will also be added to the MANIFEST file if no template is provided. See Specifying the files to distribute.




#### ◦ 2.6. Installing Package Data

Often, additional files need to be installed into a package. These files are often data that’s closely related to the package’s implementation, or text files containing documentation that might be of interest to programmers using the package. These files are called package data.

Package data can be added to packages using the package_data keyword argument to the setup() function. The value must be a mapping from package name to a list of relative path names that should be copied into the package. The paths are interpreted as relative to the directory containing the package (information from the package_dir mapping is used if appropriate); that is, the files are expected to be part of the package in the source directories. They may contain glob patterns as well.

The path names may contain directory portions; any necessary directories will be created in the installation.

For example, if a package should contain a subdirectory with several data files, the files can be arranged like this in the source tree:


```py
setup.py
src/
    mypkg/
        __init__.py
        module.py
        data/
            tables.dat
            spoons.dat
            forks.dat
```


The corresponding call to setup() might be:


```py
setup(...,
      packages=['mypkg'],
      package_dir={'mypkg': 'src/mypkg'},
      package_data={'mypkg': ['data/*.dat']},
      )

```


Changed in version 3.1: All the files that match package_data will be added to the MANIFEST file if no template is provided. See Specifying the files to distribute.



#### ◦ 2.7. Installing Additional Files

The data_files option can be used to specify additional files needed by the module distribution: configuration files, message catalogs, data files, anything which doesn’t fit in the previous categories.

data_files specifies a sequence of (directory, files) pairs in the following way:


```py
setup(...,
      data_files=[('bitmaps', ['bm/b1.gif', 'bm/b2.gif']),
                  ('config', ['cfg/data.cfg'])],
     )
```


Each (directory, files) pair in the sequence specifies the installation directory and the files to install there.

Each file name in files is interpreted relative to the setup.py script at the top of the package source distribution. Note that you can specify the directory where the data files will be installed, but you cannot rename the data files themselves.

The directory should be a relative path. It is interpreted relative to the installation prefix (Python’s sys.prefix for system installations; site.USER_BASE for user installations). Distutils allows directory to be an absolute installation path, but this is discouraged since it is incompatible with the wheel packaging format. No directory information from files is used to determine the final location of the installed file; only the name of the file is used.

You can specify the data_files options as a simple sequence of files without specifying a target directory, but this is not recommended, and the install command will print a warning in this case. To install data files directly in the target directory, an empty string should be given as the directory.


Changed in version 3.1: All the files that match data_files will be added to the MANIFEST file if no template is provided. See Specifying the files to distribute.



#### ◦ 2.8. Additional meta-data


The setup script may include additional meta-data beyond the name and version. This information includes:


|    Meta-Data     |                 Description                  |        Value        |
|------------------|----------------------------------------------|---------------------|
| name             | name of the package                          | short string (1)    |
| version          | version of this release                      | short string (1)(2) |
| author           | package author’s name                        | short string (3)    |
| author_email     | email address of the package author          | email address (3)   |
| maintainer       | package maintainer’s name                    | short string (3)    |
| maintainer_email | email address of the package maintainer      | email address (3)   |
| url              | home page for the package                    | URL (1)             |
| description      | short, summary description of the package    | short string        |
| long_description | longer description of the package            | long string (4)     |
| download_url     | location where the package may be downloaded | URL                 |
| classifiers      | a list of classifiers list                   | of strings (6)(7)   |
| platforms        | a list of platforms list                     | of strings (6)(8)   |
| keywords         | a list of keywords list                      | of strings (6)(8)   |
| license          | license for the package                      | short string (5)    |

Notes:

(1). These fields are required.

(2). It is recommended that versions take the form major.minor[.patch[.sub]].

(3). Either the author or the maintainer must be identified. If maintainer is provided, distutils lists it as the author in PKG-INFO.

(4). The long_description field is used by PyPI when you publish a package, to build its project page.

(5). The license field is a text indicating the license covering the package where the license is not a selection from the “License” Trove classifiers. See the Classifier field. Notice that there’s a licence distribution option which is deprecated but still acts as an alias for license.

(6). This field must be a list.

(7). The valid classifiers are listed on PyPI.

(8). To preserve backward compatibility, this field also accepts a string. If you pass a comma-separated string 'foo, bar', it will be converted to ['foo', 'bar'], Otherwise, it will be converted to a list of one string.


**‘short string’** A single line of text, not more than 200 characters.

**‘long string’** Multiple lines of plain text in reStructuredText format (see http://docutils.sourceforge.net/).

**‘list of strings’** See below.

Encoding the version information is an art in itself. Python packages generally adhere to the version format major.minor[.patch][sub]. The major number is 0 for initial, experimental releases of software. It is incremented for releases that represent major milestones in a package. The minor number is incremented when important new features are added to the package. The patch number increments when bug-fix releases are made. Additional trailing version information is sometimes used to indicate sub-releases. These are “a1,a2,…,aN” (for alpha releases, where functionality and API may change), “b1,b2,…,bN” (for beta releases, which only fix bugs) and “pr1,pr2,…,prN” (for final pre-release release testing). Some examples:

**0.1.0** the first, experimental release of a package

**1.0.1a2** the second alpha release of the first patch version of 1.0

`classifiers` must be specified in a list:


```py
setup(...,
      classifiers=[
          'Development Status :: 4 - Beta',
          'Environment :: Console',
          'Environment :: Web Environment',
          'Intended Audience :: End Users/Desktop',
          'Intended Audience :: Developers',
          'Intended Audience :: System Administrators',
          'License :: OSI Approved :: Python Software Foundation License',
          'Operating System :: MacOS :: MacOS X',
          'Operating System :: Microsoft :: Windows',
          'Operating System :: POSIX',
          'Programming Language :: Python',
          'Topic :: Communications :: Email',
          'Topic :: Office/Business',
          'Topic :: Software Development :: Bug Tracking',
          ],
      )
```



Changed in version 3.7: setup now warns when classifiers, keywords or platforms fields are not specified as a list or a string.


#### ◦ 2.9. Debugging the setup script


Sometimes things go wrong, and the setup script doesn’t do what the developer wants.

Distutils catches any exceptions when running the setup script, and print a simple error message before the script is terminated. The motivation for this behaviour is to not confuse administrators who don’t know much about Python and are trying to install a package. If they get a big long traceback from deep inside the guts of Distutils, they may think the package or the Python installation is broken because they don’t read all the way down to the bottom and see that it’s a permission problem.

On the other hand, this doesn’t help the developer to find the cause of the failure. For this purpose, the DISTUTILS_DEBUG environment variable can be set to anything except an empty string, and distutils will now print detailed information about what it is doing, dump the full traceback when an exception occurs, and print the whole command line when an external program (like a C compiler) fails.



### ===🗝 • 3. Writing the Setup Configuration File

Often, it’s not possible to write down everything needed to build a distribution a priori: you may need to get some information from the user, or from the user’s system, in order to proceed. As long as that information is fairly simple—a list of directories to search for C header files or libraries, for example—then providing a configuration file, `setup.cfg`, for users to edit is a cheap and easy way to solicit it. Configuration files also let you provide default values for any command option, which the installer can then override either on the command-line or by editing the config file.

The setup configuration file is a useful middle-ground between the setup script—which, ideally, would be opaque to installers [1]—and the command-line to the setup script, which is outside of your control and entirely up to the installer. In fact, `setup.cfg` (and any other Distutils configuration files present on the target system) are processed after the contents of the setup script, but before the command-line. This has several useful consequences:

• installers can override some of what you put in `setup.py` by editing `setup.cfg`
• you can provide non-standard defaults for options that are not easily set in `setup.py`
• installers can override anything in `setup.cfg` using the command-line options to `setup.py`

The basic syntax of the configuration file is simple:


    [command]
    option=value
    ...


where command is one of the Distutils commands (e.g. build_py, install), and option is one of the options that command supports. Any number of options can be supplied for each command, and any number of command sections can be included in the file. Blank lines are ignored, as are comments, which run from a '#' character until the end of the line. Long option values can be split across multiple lines simply by indenting the continuation lines.

You can find out the list of options supported by a particular command with the universal --help option, e.g.


```sh
$ python setup.py --help build_ext
[...]
Options for 'build_ext' command:
  --build-lib (-b)     directory for compiled extension modules
  --build-temp (-t)    directory for temporary files (build by-products)
  --inplace (-i)       ignore build-lib and put compiled extensions into the
                       source directory alongside your pure Python modules
  --include-dirs (-I)  list of directories to search for header files
  --define (-D)        C preprocessor macros to define
  --undef (-U)         C preprocessor macros to undefine
  --swig-opts          list of SWIG command line options
[...]
```


Note that an option spelled --foo-bar on the command-line is spelled foo_bar in configuration files.

For example, say you want your extensions to be built “in-place”—that is, you have an extension pkg.ext, and you want the compiled extension file (ext.so on Unix, say) to be put in the same source directory as your pure Python modules pkg.mod1 and pkg.mod2. You can always use the --inplace option on the command-line to ensure this:


    python setup.py build_ext --inplace


But this requires that you always specify the build_ext command explicitly, and remember to provide --inplace. An easier way is to “set and forget” this option, by encoding it in `setup.cfg`, the configuration file for this distribution:


    [build_ext]
    inplace=1


This will affect all builds of this module distribution, whether or not you explicitly specify build_ext. If you include `setup.cfg` in your source distribution, it will also affect end-user builds—which is probably a bad idea for this option, since always building extensions in-place would break installation of the module distribution. In certain peculiar cases, though, modules are built right in their installation directory, so this is conceivably a useful ability. (Distributing extensions that expect to be built in their installation directory is almost always a bad idea, though.)

Another example: certain commands take a lot of options that don’t change from run to run; for example, bdist_rpm needs to know everything required to generate a “spec” file for creating an RPM distribution. Some of this information comes from the setup script, and some is automatically generated by the Distutils (such as the list of files installed). But some of it has to be supplied as options to bdist_rpm, which would be very tedious to do on the command-line for every run. Hence, here is a snippet from the Distutils’ own `setup.cfg`:


```yaml
[bdist_rpm]
release = 1
packager = Greg Ward <gward@python.net>
doc_files = CHANGES.txt
            README.txt
            USAGE.txt
            doc/
            examples/
```


Note that the doc_files option is simply a whitespace-separated string split across multiple lines for readability.

See also:
 Syntax of config files in “Installing Python Modules”More information on the configuration files is available in the manual for system administrators.

Footnotes

[1] This ideal probably won’t be achieved until auto-configuration is fully supported by the Distutils. 


### ===🗝 • 4. Creating a Source Distribution

As shown in section A Simple Example, you use the sdist command to create a source distribution. In the simplest case,


python setup.py sdist


(assuming you haven’t specified any sdist options in the setup script or config file), sdist creates the archive of the default format for the current platform. The default format is a gzip’ed tar file (.tar.gz) on Unix, and ZIP file on Windows.

You can specify as many formats as you like using the `--formats` option, for example:


    python setup.py sdist --formats=gztar,zip


to create a gzipped tarball and a zip file. The available formats are:


| Format |         Description          |  Notes  |
|--------|------------------------------|---------|
| zip    | zip file (.zip)              | (1),(3) |
| gztar  | gzip’ed tar file (.tar.gz)   | (2)     |
| bztar  | bzip2’ed tar file (.tar.bz2) | (5)     |
| xztar  | xz’ed tar file (.tar.xz)     | (5)     |
| ztar   | compressed tar file (.tar.Z) | (4),(5) |
| tar    | tar file (.tar)              | (5)     |


Changed in version 3.5: Added support for the xztar format.

Notes:

(1). default on Windows

(2). default on Unix

(3). requires either external zip utility or zipfile module (part of the standard Python library since Python 1.6)

(4). requires the compress program. Notice that this format is now pending for deprecation and will be removed in the future versions of Python.

(5). deprecated by PEP 527; PyPI only accepts .zip and .tar.gz files.


When using any tar format (gztar, bztar, xztar, ztar or tar), under Unix you can specify the owner and group names that will be set for each member of the archive.

For example, if you want all files of the archive to be owned by root:


  python setup.py sdist --owner=root --group=root


#### ◦ 4.1. Specifying the files to distribute

If you don’t supply an explicit list of files (or instructions on how to generate one), the sdist command puts a minimal default set into the source distribution:

• all Python source files implied by the py_modules and packages options

• all C source files mentioned in the ext_modules or libraries options

• scripts identified by the scripts option See Installing Scripts.

• anything that looks like a test script: `test/test*.py` (currently, the Distutils don’t do anything with test scripts except include them in source distributions, but in the future there will be a standard for testing Python module distributions)

• Any of the standard README files (README, README.txt, or README.rst), setup.py (or whatever you called your setup script), and setup.cfg.

• all files that matches the package_data metadata. See Installing Package Data.

• all files that matches the data_files metadata. See Installing Additional Files.

Sometimes this is enough, but usually you will want to specify additional files to distribute. The typical way to do this is to write a manifest template, called MANIFEST.in by default. The manifest template is just a list of instructions for how to generate your manifest file, MANIFEST, which is the exact list of files to include in your source distribution. The sdist command processes this template and generates a manifest based on its instructions and what it finds in the filesystem.

If you prefer to roll your own manifest file, the format is simple: one filename per line, regular files (or symlinks to them) only. If you do supply your own MANIFEST, you must specify everything: the default set of files described above does not apply in this case.


Changed in version 3.1: An existing generated MANIFEST will be regenerated without sdist comparing its modification time to the one of MANIFEST.in or setup.py.


Changed in version 3.1.3: MANIFEST files start with a comment indicating they are generated. Files without this comment are not overwritten or removed.


Changed in version 3.2.2: sdist will read a MANIFEST file if no MANIFEST.in exists, like it used to do.


Changed in version 3.7: README.rst is now included in the list of distutils standard READMEs.

The manifest template has one command per line, where each command specifies a set of files to include or exclude from the source distribution. For an example, again we turn to the Distutils’ own manifest template:


    include *.txt
    recursive-include examples *.txt *.py
    prune examples/sample?/build


The meanings should be fairly clear: include all files in the distribution root matching `*.txt`, all files anywhere under the examples directory matching `*.txt` or `*.py`, and exclude all directories matching examples/sample?/build. All of this is done after the standard include set, so you can exclude files from the standard set with explicit instructions in the manifest template. (Or, you can use the --no-defaults option to disable the standard set entirely.) There are several other commands available in the manifest template mini-language; see section Creating a source distribution: the sdist command.

The order of commands in the manifest template matters: initially, we have the list of default files as described above, and each command in the template adds to or removes from that list of files. Once we have fully processed the manifest template, we remove files that should not be included in the source distribution:
•all files in the Distutils “build” tree (default build/)
•all files in directories named RCS, CVS, .svn, .hg, .git, .bzr or `_darcs`

Now we have our complete list of files, which is written to the manifest for future reference, and then used to build the source distribution archive(s).

You can disable the default set of included files with the --no-defaults option, and you can disable the standard exclude set with --no-prune.

Following the Distutils’ own manifest template, let’s trace how the sdist command builds the list of files to include in the Distutils source distribution:

(1). include all Python source files in the distutils and distutils/command subdirectories (because packages corresponding to those two directories were mentioned in the packages option in the setup script—see section Writing the Setup Script)

(2). include README.txt, setup.py, and setup.cfg (standard files)

(3). include `test/test*.py` (standard files)

(4). include `*.txt` in the distribution root (this will find README.txt a second time, but such redundancies are weeded out later)

(5). include anything matching `*.txt` or `*.py` in the sub-tree under examples,

(6). exclude all files in the sub-trees starting at directories matching examples/sample?/build—this may exclude files included by the previous two steps, so it’s important that the prune command in the manifest template comes after the recursive-include command

(7). exclude the entire build tree, and any RCS, CVS, .svn, .hg, .git, .bzr and `_darcs` directories

Just like in the setup script, file and directory names in the manifest template should always be slash-separated; the Distutils will take care of converting them to the standard representation on your platform. That way, the manifest template is portable across operating systems.




#### ◦ 4.2. Manifest-related options

The normal course of operations for the `sdist` command is as follows:

• if the manifest file (MANIFEST by default) exists and the first line does not have a comment indicating it is generated from MANIFEST.in, then it is used as is, unaltered

• if the manifest file doesn’t exist or has been previously automatically generated, read MANIFEST.in and create the manifest

• if neither MANIFEST nor MANIFEST.in exist, create a manifest with just the default file set

• use the list of files now in MANIFEST (either just generated or read in) to create the source distribution archive(s)

There are a couple of options that modify this behaviour. First, use the --no-defaults and --no-prune to disable the standard “include” and “exclude” sets.

Second, you might just want to (re)generate the manifest, but not create a source distribution:


    python setup.py sdist --manifest-only


-o is a shortcut for --manifest-only.







### ===🗝 • 5. Creating Built Distributions



#### ◦ 5.1. Creating RPM packages



#### ◦ 5.2. Cross-compiling on Windows







### ===🗝 • 6. Distutils Examples


This chapter provides a number of basic examples to help get started with distutils. Additional information about using distutils can be found in the Distutils Cookbook.

See also:
 Distutils CookbookCollection of recipes showing how to achieve more control over distutils.


#### ◦ 6.1. Pure Python distribution (by module)

If you’re just distributing a couple of modules, especially if they don’t live in a particular package, you can specify them individually using the py_modules option in the setup script.

In the simplest case, you’ll have two files to worry about: a setup script and the single module you’re distributing, foo.py in this example:


      <root>/
        setup.py
        foo.py


(In all diagrams in this section, <root> will refer to the distribution root directory.) A minimal setup script to describe this situation would be:


```py
from distutils.core import setup
setup(name='foo',
      version='1.0',
      py_modules=['foo'],
      )
```


Note that the name of the distribution is specified independently with the name option, and there’s no rule that says it has to be the same as the name of the sole module in the distribution (although that’s probably a good convention to follow). However, the distribution name is used to generate filenames, so you should stick to letters, digits, underscores, and hyphens.

Since py_modules is a list, you can of course specify multiple modules, eg. if you’re distributing modules foo and bar, your setup might look like this:


      <root>/
        setup.py
        foo.py
        bar.py


and the setup script might be


```py
from distutils.core import setup
setup(name='foobar',
      version='1.0',
      py_modules=['foo', 'bar'],
      )
```


You can put module source files into another directory, but if you have enough modules to do that, it’s probably easier to specify modules by package rather than listing them individually.


#### ◦ 6.2. Pure Python distribution (by package)

If you have more than a couple of modules to distribute, especially if they are in multiple packages, it’s probably easier to specify whole packages rather than individual modules. This works even if your modules are not in a package; you can just tell the Distutils to process modules from the root package, and that works the same as any other package (except that you don’t have to have an __init__.py file).

The setup script from the last example could also be written as


```py
from distutils.core import setup
setup(name='foobar',
      version='1.0',
      packages=[''],
      )
```


(The empty string stands for the root package.)

If those two files are moved into a subdirectory, but remain in the root package, e.g.:


      <root>/
        setup.py
        src/      foo.py
                  bar.py


then you would still specify the root package, but you have to tell the Distutils where source files in the root package live:


```py
from distutils.core import setup
setup(name='foobar',
      version='1.0',
      package_dir={'': 'src'},
      packages=[''],
      )
```


More typically, though, you will want to distribute multiple modules in the same package (or in sub-packages). For example, if the foo and bar modules belong in package foobar, one way to layout your source tree is


      <root>/
        setup.py
        foobar/
           __init__.py
           foo.py
           bar.py


This is in fact the default layout expected by the Distutils, and the one that requires the least work to describe in your setup script:


```py
from distutils.core import setup
setup(name='foobar',
      version='1.0',
      packages=['foobar'],
      )
```


If you want to put modules in directories not named for their package, then you need to use the package_dir option again. For example, if the src directory holds modules in the foobar package:


      <root>/
        setup.py
        src/
           __init__.py
           foo.py
           bar.py


an appropriate setup script would be


```py
from distutils.core import setup
setup(name='foobar',
      version='1.0',
      package_dir={'foobar': 'src'},
      packages=['foobar'],
      )
```


Or, you might put modules from your main package right in the distribution root:


      <root>/
        setup.py
        __init__.py
        foo.py
        bar.py


in which case your setup script would be


```py
from distutils.core import setup
setup(name='foobar',
      version='1.0',
      package_dir={'foobar': ''},
      packages=['foobar'],
      )
```


(The empty string also stands for the current directory.)

If you have sub-packages, they must be explicitly listed in packages, but any entries in package_dir automatically extend to sub-packages. (In other words, the Distutils does not scan your source tree, trying to figure out which directories correspond to Python packages by looking for __init__.py files.) Thus, if the default layout grows a sub-package:


      <root>/
        setup.py
        foobar/
           __init__.py
           foo.py
           bar.py
           subfoo/
               __init__.py
               blah.py


then the corresponding setup script would be


```py
from distutils.core import setup
setup(name='foobar',
      version='1.0',
      packages=['foobar', 'foobar.subfoo'],
      )
```



#### ◦ 6.3. Single extension module


Extension modules are specified using the ext_modules option. package_dir has no effect on where extension source files are found; it only affects the source for pure Python modules. The simplest case, a single extension module in a single C source file, is:


      <root>/
        setup.py
        foo.c


If the foo extension belongs in the root package, the setup script for this could be


```py
from distutils.core import setup
from distutils.extension import Extension
setup(name='foobar',
      version='1.0',
      ext_modules=[Extension('foo', ['foo.c'])],
      )
```


If the extension actually belongs in a package, say foopkg, then

With exactly the same source tree layout, this extension can be put in the foopkg package simply by changing the name of the extension:


```py
from distutils.core import setup
from distutils.extension import Extension
setup(name='foobar',
      version='1.0',
      ext_modules=[Extension('foopkg.foo', ['foo.c'])],
      )
```



#### ◦ 6.4. Checking a package

The check command allows you to verify if your package meta-data meet the minimum requirements to build a distribution.

To run it, just call it using your setup.py script. If something is missing, check will display a warning.

Let’s take an example with a simple script:


```py
from distutils.core import setup

setup(name='foobar')
```


Running the check command will display some warnings:


```sh
$ python setup.py check
running check
warning: check: missing required meta-data: version, url
warning: check: missing meta-data: either (author and author_email) or
         (maintainer and maintainer_email) should be supplied
```


If you use the reStructuredText syntax in the long_description field and docutils is installed you can check if the syntax is fine with the check command, using the restructuredtext option.

For example, if the setup.py script is changed like this:


```py
from distutils.core import setup

desc = """\
My description
==============

This is the description of the ``foobar`` package.
"""

setup(name='foobar', version='1', author='tarek',
    author_email='tarek@ziade.org',
    url='http://example.com', long_description=desc)
```


Where the long description is broken, check will be able to detect it by using the docutils parser:


```sh
$ python setup.py check --restructuredtext
running check
warning: check: Title underline too short. (line 2)
warning: check: Could not finish the parsing.
```



#### ◦ 6.5. Reading the metadata


The distutils.core.setup() function provides a command-line interface that allows you to query the metadata fields of a project through the setup.py script of a given project:


$ python setup.py --name
distribute


This call reads the name metadata by running the distutils.core.setup() function. Although, when a source or binary distribution is created with Distutils, the metadata fields are written in a static file called PKG-INFO. When a Distutils-based project is installed in Python, the PKG-INFO file is copied alongside the modules and packages of the distribution under NAME-VERSION-pyX.X.egg-info, where NAME is the name of the project, VERSION its version as defined in the Metadata, and pyX.X the major and minor version of Python like 2.7 or 3.2.

You can read back this static file, by using the distutils.dist.DistributionMetadata class and its read_pkg_file() method:


>>> from distutils.dist import DistributionMetadata
>>> metadata = DistributionMetadata()
>>> metadata.read_pkg_file(open('distribute-0.6.8-py2.7.egg-info'))
>>> metadata.name
'distribute'
>>> metadata.version
'0.6.8'
>>> metadata.description
'Easily download, build, install, upgrade, and uninstall Python packages'


Notice that the class can also be instantiated with a metadata file path to loads its values:


>>> pkg_info_path = 'distribute-0.6.8-py2.7.egg-info'
>>> DistributionMetadata(pkg_info_path).name
'distribute'







### ===🗝 • 7. Extending Distutils



#### ◦ 7.1. Integrating new commands



#### ◦ 7.2. Adding new distribution types







### ===🗝 • 8. Command Reference



#### ◦ 8.1. Installing modules: the install command family



#### ◦ 8.2. Creating a source distribution: the sdist command







### ===🗝 • 9. distutils API Reference




## ==⚡ • ensurepip — Bootstrapping the pip installer

## ==⚡ • venv — Creation of virtual environments

## ==⚡ • zipapp — Manage executable Python zip archives


New in version 3.5.

Source code: Lib/zipapp.py


This module provides tools to manage the creation of zip files containing Python code, which can be executed directly by the Python interpreter. The module provides both a Command-Line Interface and a Python API.


### ===🗝 Basic Example

The following example shows how the Command-Line Interface can be used to create an executable archive from a directory containing Python code. When run, the archive will execute the `main` function from the module `myapp` in the archive.


```py
$ python -m zipapp myapp -m "myapp:main"
$ python myapp.pyz
<output from myapp>
```

Python 命令后面的 -m 指定要打包的目录 `myapp`、同时也是模块名称。后面一个 -m 选项指定要运行的模块及函数，格式可以是 `package.module:fun`，最终会生成 myapp.pyz 打包文件，会包含一个内容如下的 `__main__.py` 文件：

```py
# -*- coding: utf-8 -*-
import mod_root
mod_root.main()
```

所以，项目目录结构应该如下：

    + project 
        + myapp
            - __init__.py

并且模块中应该有类似以下的主函数：

```py
def main():
    ...
    pass

# if __name__ == '__main__':
#     main()
```

如果主目录下已经编写好 __main__.py 脚本，则不能再在命令行中指定入口函数。


### ===🗝 Command-Line Interface

When called as a program from the command line, the following form is used:


    $ python -m zipapp source [options]


If source is a directory, this will create an archive from the contents of source. If source is a file, it should be an archive, and it will be copied to the target archive (or the contents of its shebang line will be displayed if the `–info` option is specified).

The following options are understood:

➡ -o <output>, --output=<output>
Write the output to a file named output. If this option is not specified, the output filename will be the same as the input source, with the extension .pyz added. If an explicit filename is given, it is used as is (so a .pyz extension should be included if required).

An output filename must be specified if the source is an archive (and in that case, output must not be the same as source).

➡ -p <interpreter>, --python=<interpreter>
Add a #! line to the archive specifying interpreter as the command to run. Also, on POSIX, make the archive executable. The default is to write no #! line, and not make the file executable.

➡ -m <mainfn>, --main=<mainfn>
Write a __main__.py file to the archive that executes mainfn. The `mainfn` argument should have the form “`pkg.mod:fn`”, where “`pkg.mod`” is a package/module in the archive, and “fn” is a callable in the given module. The __main__.py file will execute that callable.

--main cannot be specified when copying an archive.

➡ -c, --compress
Compress files with the deflate method, reducing the size of the output file. By default, files are stored uncompressed in the archive.

--compress has no effect when copying an archive.

New in version 3.7.

➡ --info
Display the interpreter embedded in the archive, for diagnostic purposes. In this case, any other options are ignored and SOURCE must be an archive, not a directory.

➡ -h, --help
Print a short usage message and exit.


### ===🗝 Python API

The module defines two convenience functions:

✅ zipapp.create_archive(source, target=None, interpreter=None, main=None, filter=None, compressed=False)

➡ Create an application archive from `source`. The source can be any of the following:

• The name of a directory, or a path-like object referring to a directory, in which case a new application archive will be created from the content of that directory.

• The name of an existing application archive file, or a path-like object referring to such a file, in which case the file is copied to the target (modifying it to reflect the value given for the interpreter argument). The file name should include the .pyz extension, if required.

• A file object open for reading in bytes mode. The content of the file should be an application archive, and the file object is assumed to be positioned at the start of the archive.

➡ The `target` argument determines where the resulting archive will be written:

• If it is the name of a file, or a path-like object, the archive will be written to that file.

• If it is an open file object, the archive will be written to that file object, which must be open for writing in bytes mode.

• If the target is omitted (or None), the source must be a directory and the target will be a file with the same name as the source, with a .pyz extension added.

➡ The `interpreter` argument specifies the name of the Python interpreter with which the archive will be executed. It is written as a “shebang” line at the start of the archive. On POSIX, this will be interpreted by the OS, and on Windows it will be handled by the Python launcher. Omitting the interpreter results in no shebang line being written. If an interpreter is specified, and the target is a filename, the executable bit of the target file will be set.

➡ The `main` argument specifies the name of a callable which will be used as the main program for the archive. It can only be specified if the source is a directory, and the source does not already contain a __main__.py file. The main argument should take the form “pkg.module:callable” and the archive will be run by importing “pkg.module” and executing the given callable with no arguments. It is an error to omit main if the source is a directory and does not contain a __main__.py file, as otherwise the resulting archive would not be executable.

➡ The optional `filter` argument specifies a callback function that is passed a Path object representing the path to the file being added (relative to the source directory). It should return True if the file is to be added.

➡ The optional `compressed` argument determines whether files are compressed. If set to True, files in the archive are compressed with the deflate method; otherwise, files are stored uncompressed. This argument has no effect when copying an existing archive.

If a file object is specified for `source` or `target`, it is the caller’s responsibility to close it after calling create_archive.

When copying an existing archive, file objects supplied only need read and readline, or write methods. When creating an archive from a directory, if the target is a file object it will be passed to the zipfile.ZipFile class, and must supply the methods needed by that class.

New in version 3.7: Added the filter and compressed arguments.


✅ zipapp.get_interpreter(archive)

Return the interpreter specified in the `#!` line at the start of the archive. If there is no `#!` line, return None. The archive argument can be a filename or a file-like object open for reading in bytes mode. It is assumed to be at the start of the archive.


### ===🗝 Examples

Pack up a directory into an archive, and run it.


$ python -m zipapp myapp
$ python myapp.pyz
<output from myapp>


The same can be done using the create_archive() function:


>>> import zipapp
>>> zipapp.create_archive('myapp', 'myapp.pyz')


To make the application directly executable on POSIX, specify an interpreter to use.


$ python -m zipapp myapp -p "/usr/bin/env python"
$ ./myapp.pyz
<output from myapp>


To replace the shebang line on an existing archive, create a modified archive using the create_archive() function:


>>> import zipapp
>>> zipapp.create_archive('old_archive.pyz', 'new_archive.pyz', '/usr/bin/python3')


To update the file in place, do the replacement in memory using a BytesIO object, and then overwrite the source afterwards. Note that there is a risk when overwriting a file in place that an error will result in the loss of the original file. This code does not protect against such errors, but production code should do so. Also, this method will only work if the archive fits in memory:


>>> import zipapp
>>> import io
>>> temp = io.BytesIO()
>>> zipapp.create_archive('myapp.pyz', temp, '/usr/bin/python2')
>>> with open('myapp.pyz', 'wb') as f:
>>>     f.write(temp.getvalue())



### ===🗝 Specifying the Interpreter

Note that if you specify an interpreter and then distribute your application archive, you need to ensure that the interpreter used is portable. The Python launcher for Windows supports most common forms of POSIX #! line, but there are other issues to consider:

• If you use “`/usr/bin/env python`” (or other forms of the “python” command, such as “`/usr/bin/python`”), you need to consider that your users may have either Python 2 or Python 3 as their default, and write your code to work under both versions.

• If you use an explicit version, for example “`/usr/bin/env python3`” your application will not work for users who do not have that version. (This may be what you want if you have not made your code Python 2 compatible).

• There is no way to say “`python X.Y or later`”, so be careful of using an exact version like “`/usr/bin/env python3.4`” as you will need to change your shebang line for users of Python 3.5, for example.


Typically, you should use an “/usr/bin/env python2” or “/usr/bin/env python3”, depending on whether your code is written for Python 2 or 3.


### ===🗝 Creating Standalone Applications with zipapp

Using the `zipapp` module, it is possible to create self-contained Python programs, which can be distributed to end users who only need to have a suitable version of Python installed on their system. The key to doing this is to bundle all of the application’s dependencies into the archive, along with the application code.

The steps to create a standalone archive are as follows:

1. Create your application in a directory as normal, so you have a myapp directory containing a __main__.py file, and any supporting application code.


2. Install all of your application’s dependencies into the myapp directory, using pip:


    $ python -m pip install -r requirements.txt --target myapp


(this assumes you have your project requirements in a requirements.txt file - if not, you can just list the dependencies manually on the pip command line).


3. Optionally, delete the .dist-info directories created by pip in the myapp directory. These hold metadata for pip to manage the packages, and as you won’t be making any further use of pip they aren’t required - although it won’t do any harm if you leave them.


4. Package the application using:


    $ python -m zipapp -p "interpreter" myapp


This will produce a standalone executable, which can be run on any machine with the appropriate interpreter available. See Specifying the Interpreter for details. It can be shipped to users as a single file.

On Unix, the `myapp.pyz` file is executable as it stands. You can rename the file to remove the .pyz extension if you prefer a “plain” command name. On Windows, the myapp.pyz[w] file is executable by virtue of the fact that the Python interpreter registers the `.pyz` and `.pyzw` file extensions when installed.


### ===🗝 Making a Windows executable

On Windows, registration of the .pyz extension is optional, and furthermore, there are certain places that don’t recognise registered extensions “transparently” (the simplest example is that `subprocess.run(['myapp'])` won’t find your application - you need to explicitly specify the extension).

On Windows, therefore, it is often preferable to create an executable from the zipapp. This is relatively easy, although it does require a C compiler. The basic approach relies on the fact that zipfiles can have arbitrary data prepended, and Windows exe files can have arbitrary data appended. So by creating a suitable launcher and tacking the .pyz file onto the end of it, you end up with a single-file executable that runs your application.

A suitable launcher can be as simple as the following:


```cpp
#define Py_LIMITED_API 1
#include "Python.h"

#define WIN32_LEAN_AND_MEAN
#include <windows.h>

#ifdef WINDOWS
int WINAPI wWinMain(
    HINSTANCE hInstance,      /* handle to current instance */
    HINSTANCE hPrevInstance,  /* handle to previous instance */
    LPWSTR lpCmdLine,         /* pointer to command line */
    int nCmdShow              /* show state of window */
)
#else
int wmain()
#endif
{
    wchar_t **myargv = _alloca((__argc + 1) * sizeof(wchar_t*));
    myargv[0] = __wargv[0];
    memcpy(myargv + 1, __wargv, __argc * sizeof(wchar_t *));
    return Py_Main(__argc+1, myargv);
}
```


If you define the WINDOWS preprocessor symbol, this will generate a GUI executable, and without it, a console executable.

To compile the executable, you can either just use the standard MSVC command line tools, or you can take advantage of the fact that distutils knows how to compile Python source:


>>> from distutils.ccompiler import new_compiler
>>> import distutils.sysconfig
>>> import sys
>>> import os
>>> from pathlib import Path

>>> def compile(src):
>>>     src = Path(src)
>>>     cc = new_compiler()
>>>     exe = src.stem
>>>     cc.add_include_dir(distutils.sysconfig.get_python_inc())
>>>     cc.add_library_dir(os.path.join(sys.base_exec_prefix, 'libs'))
>>>     # First the CLI executable
>>>     objs = cc.compile([str(src)])
>>>     cc.link_executable(objs, exe)
>>>     # Now the GUI executable
>>>     cc.define_macro('WINDOWS')
>>>     objs = cc.compile([str(src)])
>>>     cc.link_executable(objs, exe + 'w')

>>> if __name__ == "__main__":
>>>     compile("zastub.c")


The resulting launcher uses the “Limited ABI”, so it will run unchanged with any version of Python 3.x. All it needs is for Python (python3.dll) to be on the user’s PATH.

For a fully standalone distribution, you can distribute the launcher with your application appended, bundled with the Python “embedded” distribution. This will run on any PC with the appropriate architecture (32 bit or 64 bit).


### ===🗝 Caveats

There are some limitations to the process of bundling your application into a single file. In most, if not all, cases they can be addressed without needing major changes to your application.

1. If your application depends on a package that includes a C extension, that package cannot be run from a zip file (this is an OS limitation, as executable code must be present in the filesystem for the OS loader to load it). In this case, you can exclude that dependency from the zipfile, and either require your users to have it installed, or ship it alongside your zipfile and add code to your __main__.py to include the directory containing the unzipped module in `sys.path`. In this case, you will need to make sure to ship appropriate binaries for your target architecture(s) (and potentially pick the correct version to add to `sys.path` at runtime, based on the user’s machine).

2. If you are shipping a Windows executable as described above, you either need to ensure that your users have python3.dll on their PATH (which is not the default behaviour of the installer) or you should bundle your application with the embedded distribution.

3. The suggested launcher above uses the Python embedding API. This means that in your application, `sys.executable` will be your application, and not a conventional Python interpreter. Your code and its dependencies need to be prepared for this possibility. For example, if your application uses the multiprocessing module, it will need to call `multiprocessing.set_executable()` to let the module know where to find the standard Python interpreter.


### ===🗝 The Python Zip Application Archive Format

Python has been able to execute zip files which contain a __main__.py file since version 2.6. In order to be executed by Python, an application archive simply has to be a standard zip file containing a __main__.py file which will be run as the entry point for the application. As usual for any Python script, the parent of the script (in this case the zip file) will be placed on sys.path and thus further modules can be imported from the zip file.

The zip file format allows arbitrary data to be prepended to a zip file. The zip application format uses this ability to prepend a standard POSIX “shebang” line to the file (#!/path/to/interpreter).

Formally, the Python zip application format is therefore:

1. An optional shebang line, containing the characters b'#!' followed by an interpreter name, and then a newline (b'\n') character. The interpreter name can be anything acceptable to the OS “shebang” processing, or the Python launcher on Windows. The interpreter should be encoded in UTF-8 on Windows, and in sys.getfilesystemencoding() on POSIX.

2. Standard zipfile data, as generated by the zipfile module. The zipfile content must include a file called __main__.py (which must be in the “root” of the zipfile - i.e., it cannot be in a subdirectory). The zipfile data can be compressed or uncompressed.


If an application archive has a shebang line, it may have the executable bit set on POSIX systems, to allow it to be executed directly.

There is no requirement that the tools in this module are used to create application archives - the module is a convenience, but archives in the above format created by any means are acceptable to Python.


# =🚩 Python Runtime Services
- The Python Standard Library » Python Runtime Services

The modules described in this chapter provide a wide range of services related to the Python interpreter and its interaction with its environment. Here’s an overview:

➡ `• sys — System-specific parameters and functions`
➡ `• sysconfig — Provide access to Python’s configuration information`
◦ Configuration variables
◦ Installation paths
◦ Other functions
◦ Using sysconfig as a script

➡ `• builtins — Built-in objects`
➡ `• __main__ — Top-level code environment`
◦ __name__ == '__main__'
◾ What is the “top-level code environment”?
◾ Idiomatic Usage
◾ Packaging Considerations

◦ __main__.py in Python Packages
◾ Idiomatic Usage

◦ import __main__

➡ `• warnings — Warning control`
◦ Warning Categories
◦ The Warnings Filter
◾ Describing Warning Filters
◾ Default Warning Filter
◾ Overriding the default filter

◦ Temporarily Suppressing Warnings
◦ Testing Warnings
◦ Updating Code For New Versions of Dependencies
◦ Available Functions
◦ Available Context Managers

➡ `• dataclasses — Data Classes`
◦ Module contents
◦ Post-init processing
◦ Class variables
◦ Init-only variables
◦ Frozen instances
◦ Inheritance
◦ Re-ordering of keyword-only parameters in __init__()
◦ Default factory functions
◦ Mutable default values

➡ `• contextlib — Utilities for with-statement contexts`
◦ Utilities
◦ Examples and Recipes
◾ Supporting a variable number of context managers
◾ Catching exceptions from __enter__ methods
◾ Cleaning up in an __enter__ implementation
◾ Replacing any use of try-finally and flag variables
◾ Using a context manager as a function decorator

◦ Single use, reusable and reentrant context managers
◾ Reentrant context managers
◾ Reusable context managers


➡ `• abc — Abstract Base Classes`
➡ `• atexit — Exit handlers`
◦ atexit Example

➡ `• traceback — Print or retrieve a stack traceback`
◦ TracebackException Objects
◦ StackSummary Objects
◦ FrameSummary Objects
◦ Traceback Examples

➡ `• __future__ — Future statement definitions`
➡ `• gc — Garbage Collector interface`
➡ `• inspect — Inspect live objects`
◦ Types and members
◦ Retrieving source code
◦ Introspecting callables with the Signature object
◦ Classes and functions
◦ The interpreter stack
◦ Fetching attributes statically
◦ Current State of Generators and Coroutines
◦ Code Objects Bit Flags
◦ Command Line Interface

➡ `• site — Site-specific configuration hook`
◦ Readline configuration
◦ Module contents
◦ Command Line Interface



## ==⚡ sys — System-specific parameters and functions
- https://docs.python.org/3.9/library/sys.html

利用 sys.modules 获取当前文件模块定义的类型：

```py
import sys
class Foobar:
    pass
def str_to_class(str):
    return getattr(sys.modules[__name__], str)
print (str_to_class("Foobar"))
print (type(Foobar))
```

- `sys` — System-specific parameters and functions
- `sysconfig` — Provide access to Python’s configuration information
- `builtins` — Built-in objects
- `__main__` — Top-level script environment
- `warnings` — Warning control
- `dataclasses` — Data Classes
- `contextlib` — Utilities for with-statement contexts
- `abc` — Abstract Base Classes
- `atexit` — Exit handlers
- `traceback` — Print or retrieve a stack traceback
- `__future__` — Future statement definitions
- `gc` — Garbage Collector interface
- `inspect` — Inspect live objects
- `site` — Site-specific configuration hook

This module provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available.

    sys.argv

The list of command line arguments passed to a Python script. argv[0] is the script name (it is operating system dependent whether this is a full pathname or not). If the command was executed using the -c command line option to the interpreter, argv[0] is set to the string '-c'. If no script name was passed to the Python interpreter, argv[0] is the empty string.

    sys.exit([arg])

Exit from Python. This is implemented by raising the SystemExit exception, so cleanup actions specified by finally clauses of try statements are honored, and it is possible to intercept the exit attempt at an outer level.

    sys.platform

This string contains a platform identifier that can be used to append platform-specific components to sys.path, for instance.

For Unix systems, except on Linux and AIX, this is the lowercased OS name as returned by uname -s with the first part of the version as returned by uname -r appended, e.g. 'sunos5' or 'freebsd8', at the time when Python was built. Unless you want to test for a specific system version, it is therefore recommended to use the following idiom:

    if sys.platform.startswith('freebsd'):
        # FreeBSD-specific code here...
    elif sys.platform.startswith('linux'):
        # Linux-specific code here...
    elif sys.platform.startswith('aix'):
        # AIX-specific code here...

For other systems, the values are:

    |     System     | platform value |
    |----------------|----------------|
    | AIX            | 'aix'          |
    | Linux          | 'linux'        |
    | Windows        | 'win32'        |
    | Windows/Cygwin | 'cygwin'       |
    | macOS          | 'darwin'       |

I/O

    sys.stdin
    sys.stdout
    sys.stderr

File objects used by the interpreter for standard input, output and errors:

- `stdin` is used for all interactive input (including calls to input());
- `stdout-` is used for the output of print() and expression statements and for the prompts of input();
- The interpreter’s own prompts and its error messages go to `stderr`.

    sys.__stdin__
    sys.__stdout__
    sys.__stderr__

These objects contain the original values of stdin, stderr and stdout at the start of the program. They are used during finalization, and could be useful to print to the actual standard stream no matter if the sys.std* object has been redirected.

This module provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available.
➡ **sys.abiflags**
On POSIX systems where Python was built with the standard configure script, this contains the ABI flags as specified by PEP 3149.


Changed in version 3.8: Default flags became an empty string (m flag for pymalloc has been removed).
New in version 3.2.


➡ **sys.addaudithook(hook)**
Append the callable hook to the list of active auditing hooks for the current (sub)interpreter.

When an auditing event is raised through the sys.audit() function, each hook will be called in the order it was added with the event name and the tuple of arguments. Native hooks added by PySys_AddAuditHook() are called first, followed by hooks added in the current (sub)interpreter. Hooks can then log the event, raise an exception to abort the operation, or terminate the process entirely.

Calling sys.addaudithook() will itself raise an auditing event named sys.addaudithook with no arguments. If any existing hooks raise an exception derived from RuntimeError, the new hook will not be added and the exception suppressed. As a result, callers cannot assume that their hook has been added unless they control all existing hooks.

See the audit events table for all events raised by CPython, and PEP 578 for the original design discussion.

New in version 3.8.

Changed in version 3.8.1: Exceptions derived from Exception but not RuntimeError are no longer suppressed.


CPython implementation detail: When tracing is enabled (see settrace()), Python hooks are only traced if the callable has a __cantrace__ member that is set to a true value. Otherwise, trace functions will skip the hook.
➡ **sys.argv**
The list of command line arguments passed to a Python script. argv[0] is the script name (it is operating system dependent whether this is a full pathname or not). If the command was executed using the -c command line option to the interpreter, argv[0] is set to the string '-c'. If no script name was passed to the Python interpreter, argv[0] is the empty string.

To loop over the standard input, or the list of files given on the command line, see the fileinput module.

Note:
 On Unix, command line arguments are passed by bytes from OS. Python decodes them with filesystem encoding and “surrogateescape” error handler. When you need original bytes, you can get it by [os.fsencode(arg) for arg in sys.argv].
 
➡ `sys.audit(event, *args)`
Raise an auditing event and trigger any active auditing hooks. event is a string identifying the event, and args may contain optional arguments with more information about the event. The number and types of arguments for a given event are considered a public and stable API and should not be modified between releases.

For example, one auditing event is named os.chdir. This event has one argument called path that will contain the requested new working directory.

sys.audit() will call the existing auditing hooks, passing the event name and arguments, and will re-raise the first exception from any hook. In general, if an exception is raised, it should not be handled and the process should be terminated as quickly as possible. This allows hook implementations to decide how to respond to particular events: they can merely log the event or abort the operation by raising an exception.

Hooks are added using the sys.addaudithook() or PySys_AddAuditHook() functions.

The native equivalent of this function is PySys_Audit(). Using the native function is preferred when possible.

See the audit events table for all events raised by CPython.
New in version 3.8.


➡ **sys.base_exec_prefix**
Set during Python startup, before site.py is run, to the same value as exec_prefix. If not running in a virtual environment, the values will stay the same; if site.py finds that a virtual environment is in use, the values of prefix and exec_prefix will be changed to point to the virtual environment, whereas base_prefix and base_exec_prefix will remain pointing to the base Python installation (the one which the virtual environment was created from).
New in version 3.3.


➡ **sys.base_prefix**
Set during Python startup, before site.py is run, to the same value as prefix. If not running in a virtual environment, the values will stay the same; if site.py finds that a virtual environment is in use, the values of prefix and exec_prefix will be changed to point to the virtual environment, whereas base_prefix and base_exec_prefix will remain pointing to the base Python installation (the one which the virtual environment was created from).
New in version 3.3.


➡ **sys.byteorder**
An indicator of the native byte order. This will have the value 'big' on big-endian (most-significant byte first) platforms, and 'little' on little-endian (least-significant byte first) platforms.
➡ **sys.builtin_module_names**
A tuple of strings giving the names of all modules that are compiled into this Python interpreter. (This information is not available in any other way — modules.keys() only lists the imported modules.)
➡ **sys.call_tracing(func, args)**
Call `func(*args)`, while tracing is enabled. The tracing state is saved, and restored afterwards. This is intended to be called from a debugger from a checkpoint, to recursively debug some other code.
➡ **sys.copyright**
A string containing the copyright pertaining to the Python interpreter.
➡ `sys._clear_type_cache()`
Clear the internal type cache. The type cache is used to speed up attribute and method lookups. Use the function only to drop unnecessary references during reference leak debugging.

This function should be used for internal and specialized purposes only.
➡ `sys._current_frames()`
Return a dictionary mapping each thread’s identifier to the topmost stack frame currently active in that thread at the time the function is called. Note that functions in the traceback module can build the call stack given such a frame.

This is most useful for debugging deadlock: this function does not require the deadlocked threads’ cooperation, and such threads’ call stacks are frozen for as long as they remain deadlocked. The frame returned for a non-deadlocked thread may bear no relationship to that thread’s current activity by the time calling code examines the frame.

This function should be used for internal and specialized purposes only.
Raises an auditing event `sys._current_frames` with no arguments.

➡ `sys.breakpointhook()`
This hook function is called by built-in breakpoint(). By default, it drops you into the pdb debugger, but it can be set to any other function so that you can choose which debugger gets used.

The signature of this function is dependent on what it calls. For example, the default binding (e.g. pdb.set_trace()) expects no arguments, but you might bind it to a function that expects additional arguments (positional and/or keyword). The built-in breakpoint() function passes its `*args` and `**kws` straight through. Whatever breakpointhooks() returns is returned from breakpoint().

The default implementation first consults the environment variable PYTHONBREAKPOINT. If that is set to "0" then this function returns immediately; i.e. it is a no-op. If the environment variable is not set, or is set to the empty string, pdb.set_trace() is called. Otherwise this variable should name a function to run, using Python’s dotted-import nomenclature, e.g. package.subpackage.module.function. In this case, package.subpackage.module would be imported and the resulting module must have a callable named function(). This is run, passing in `*args` and `**kws`, and whatever function() returns, sys.breakpointhook() returns to the built-in breakpoint() function.

Note that if anything goes wrong while importing the callable named by PYTHONBREAKPOINT, a RuntimeWarning is reported and the breakpoint is ignored.

Also note that if sys.breakpointhook() is overridden programmatically, PYTHONBREAKPOINT is not consulted.

New in version 3.7.

➡ **sys._debugmallocstats()**
Print low-level information to stderr about the state of CPython’s memory allocator.

If Python is configured –with-pydebug, it also performs some expensive internal consistency checks.

New in version 3.3.



CPython implementation detail: This function is specific to CPython. The exact output format is not defined here, and may change.
➡ **sys.dllhandle**
Integer specifying the handle of the Python DLL.

Availability: Windows.
➡ **sys.displayhook(value)**
If value is not None, this function prints repr(value) to sys.stdout, and saves value in `builtins._`. If repr(value) is not encodable to sys.stdout.encoding with sys.stdout.errors error handler (which is probably 'strict'), encode it to sys.stdout.encoding with 'backslashreplace' error handler.

sys.displayhook is called on the result of evaluating an expression entered in an interactive Python session. The display of these values can be customized by assigning another one-argument function to sys.displayhook.

Pseudo-code:

    def displayhook(value):
        if value is None:
            return
        # Set '_' to None to avoid recursion
        builtins._ = None
        text = repr(value)
        try:
            sys.stdout.write(text)
        except UnicodeEncodeError:
            bytes = text.encode(sys.stdout.encoding, 'backslashreplace')
            if hasattr(sys.stdout, 'buffer'):
                sys.stdout.buffer.write(bytes)
            else:
                text = bytes.decode(sys.stdout.encoding, 'strict')
                sys.stdout.write(text)
        sys.stdout.write("\n")
        builtins._ = value

Changed in version 3.2: Use 'backslashreplace' error handler on UnicodeEncodeError.


➡ **sys.dont_write_bytecode**
If this is true, Python won’t try to write .pyc files on the import of source modules. This value is initially set to True or False depending on the -B command line option and the PYTHONDONTWRITEBYTECODE environment variable, but you can set it yourself to control bytecode file generation.
➡ **sys.pycache_prefix**
If this is set (not None), Python will write bytecode-cache .pyc files to (and read them from) a parallel directory tree rooted at this directory, rather than from __pycache__ directories in the source code tree. Any __pycache__ directories in the source code tree will be ignored and new .pyc files written within the pycache prefix. Thus if you use compileall as a pre-build step, you must ensure you run it with the same pycache prefix (if any) that you will use at runtime.

A relative path is interpreted relative to the current working directory.

This value is initially set based on the value of the -X pycache_prefix=PATH command-line option or the PYTHONPYCACHEPREFIX environment variable (command-line takes precedence). If neither are set, it is None.

New in version 3.8.

➡ **sys.excepthook(type, value, traceback)**
This function prints out a given traceback and exception to sys.stderr.

When an exception is raised and uncaught, the interpreter calls sys.excepthook with three arguments, the exception class, exception instance, and a traceback object. In an interactive session this happens just before control is returned to the prompt; in a Python program this happens just before the program exits. The handling of such top-level exceptions can be customized by assigning another three-argument function to sys.excepthook.

Raise an auditing event sys.excepthook with arguments hook, type, value, traceback when an uncaught exception occurs. If no hook has been set, hook may be None. If any hook raises an exception derived from RuntimeError the call to the hook will be suppressed. Otherwise, the audit hook exception will be reported as unraisable and sys.excepthook will be called.

See also:
 The sys.unraisablehook() function handles unraisable exceptions and the threading.excepthook() function handles exception raised by threading.Thread.run().
 
➡ **sys.__breakpointhook__**
➡ **sys.__displayhook__**
➡ **sys.__excepthook__**
➡ **sys.__unraisablehook__**

These objects contain the original values of breakpointhook, displayhook, excepthook, and unraisablehook at the start of the program. They are saved so that breakpointhook, displayhook and excepthook, unraisablehook can be restored in case they happen to get replaced with broken or alternative objects.

New in version 3.7: __breakpointhook__
New in version 3.8: __unraisablehook__

➡ **sys.exc_info()**
This function returns a tuple of three values that give information about the exception that is currently being handled. The information returned is specific both to the current thread and to the current stack frame. If the current stack frame is not handling an exception, the information is taken from the calling stack frame, or its caller, and so on until a stack frame is found that is handling an exception. Here, “handling an exception” is defined as “executing an except clause.” For any stack frame, only information about the exception being currently handled is accessible.

If no exception is being handled anywhere on the stack, a tuple containing three None values is returned. Otherwise, the values returned are (type, value, traceback). Their meaning is: type gets the type of the exception being handled (a subclass of BaseException); value gets the exception instance (an instance of the exception type); traceback gets a traceback object which encapsulates the call stack at the point where the exception originally occurred.
➡ **sys.exec_prefix**
A string giving the site-specific directory prefix where the platform-dependent Python files are installed; by default, this is also '/usr/local'. This can be set at build time with the --exec-prefix argument to the configure script. Specifically, all configuration files (e.g. the pyconfig.h header file) are installed in the directory exec_prefix/lib/pythonX.Y/config, and shared library modules are installed in exec_prefix/lib/pythonX.Y/lib-dynload, where X.Y is the version number of Python, for example 3.2.

Note:
 If a virtual environment is in effect, this value will be changed in site.py to point to the virtual environment. The value for the Python installation will still be available, via base_exec_prefix.
 
➡ **sys.executable**
A string giving the absolute path of the executable binary for the Python interpreter, on systems where this makes sense. If Python is unable to retrieve the real path to its executable, sys.executable will be an empty string or None.
➡ **sys.exit([arg])**
Exit from Python. This is implemented by raising the SystemExit exception, so cleanup actions specified by finally clauses of try statements are honored, and it is possible to intercept the exit attempt at an outer level.

The optional argument arg can be an integer giving the exit status (defaulting to zero), or another type of object. If it is an integer, zero is considered “successful termination” and any nonzero value is considered “abnormal termination” by shells and the like. Most systems require it to be in the range 0–127, and produce undefined results otherwise. Some systems have a convention for assigning specific meanings to specific exit codes, but these are generally underdeveloped; Unix programs generally use 2 for command line syntax errors and 1 for all other kind of errors. If another type of object is passed, None is equivalent to passing zero, and any other object is printed to stderr and results in an exit code of 1. In particular, sys.exit("some error message") is a quick way to exit a program when an error occurs.

Since exit() ultimately “only” raises an exception, it will only exit the process when called from the main thread, and the exception is not intercepted.

Changed in version 3.6: If an error occurs in the cleanup after the Python interpreter has caught SystemExit (such as an error flushing buffered data in the standard streams), the exit status is changed to 120.

➡ **sys.flags**

The named tuple flags exposes the status of command line flags. The attributes are read only.

|      attribute      |    flag   |
|---------------------|-----------|
| debug               | -d        |
| inspect             | -i        |
| interactive         | -i        |
| isolated            | -I        |
| optimize            | -O or -OO |
| dont_write_bytecode | -B        |
| no_user_site        | -s        |
| no_site             | -S        |
| ignore_environment  | -E        |
| verbose             | -v        |
| bytes_warning       | -b        |
| quiet               | -q        |
| hash_randomization  | -R        |
| dev_mode            | -X dev    |
| utf8_mode           | -X utf8   |

dev - (Python Development Mode)

Changed in version 3.2: Added quiet attribute for the new -q flag.

New in version 3.2.3: The hash_randomization attribute.

Changed in version 3.3: Removed obsolete division_warning attribute.

Changed in version 3.4: Added isolated attribute for -I isolated flag.

Changed in version 3.7: Added the dev_mode attribute for the new Python Development Mode and the utf8_mode attribute for the new -X utf8 flag.

➡ **sys.float_info**
A named tuple holding information about the float type. It contains low level information about the precision and internal representation. The values correspond to the various floating-point constants defined in the standard header file float.h for the ‘C’ programming language; see section 5.2.4.2.2 of the 1999 ISO/IEC C standard [C99], ‘Characteristics of floating types’, for details.


| attribute | float.h macro | explanation |


↪ epsilon *DBL_EPSILON* 
difference between 1.0 and the least value greater than 1.0 that is representable as a float

See also math.ulp().
 
↪ dig *DBL_DIG* maximum number of decimal digits that can be faithfully represented in a float; see below 
↪ mant_dig *DBL_MANT_DIG* float precision: the number of base-radix digits in the significand of a float 
↪ max *DBL_MAX* maximum representable positive finite float 
↪ max_exp *DBL_MAX_EXP* maximum integer e such that `radix**(e-1)` is a representable finite float 
↪ max_10_exp *DBL_MAX_10_EXP* maximum integer e such that `10**e` is in the range of representable finite floats 
↪ min *DBL_MIN* 
minimum representable positive normalized float

Use math.ulp(0.0) to get the smallest positive denormalized representable float.
 
↪ min_exp *DBL_MIN_EXP* minimum integer e such that `radix**(e-1)` is a normalized float 
↪ min_10_exp *DBL_MIN_10_EXP* minimum integer e such that `10**e` is a normalized float 
↪ radix *FLT_RADIX* radix of exponent representation 
↪ rounds *FLT_ROUNDS* integer constant representing the rounding mode used for arithmetic operations. This reflects the value of the system FLT_ROUNDS macro at interpreter startup time. See section 5.2.4.2.2 of the C99 standard for an explanation of the possible values and their meanings. 

The attribute sys.float_info.dig needs further explanation. If s is any string representing a decimal number with at most sys.float_info.dig significant digits, then converting s to a float and back again will recover a string representing the same decimal value:


>>> import sys
>>> sys.float_info.dig
15
>>> s = '3.14159265358979'    # decimal string with 15 significant digits
>>> format(float(s), '.15g')  # convert to float and back -> same value
'3.14159265358979'


But for strings with more than sys.float_info.dig significant digits, this isn’t always true:


>>> s = '9876543211234567'    # 16 significant digits is too many!
>>> format(float(s), '.16g')  # conversion changes value
'9876543211234568'

➡ **sys.float_repr_style**
A string indicating how the repr() function behaves for floats. If the string has value 'short' then for a finite float x, repr(x) aims to produce a short string with the property that float(repr(x)) == x. This is the usual behaviour in Python 3.1 and later. Otherwise, float_repr_style has value 'legacy' and repr(x) behaves in the same way as it did in versions of Python prior to 3.1.

New in version 3.1.

➡ **sys.getallocatedblocks()**
Return the number of memory blocks currently allocated by the interpreter, regardless of their size. This function is mainly useful for tracking and debugging memory leaks. Because of the interpreter’s internal caches, the result can vary from call to call; you may have to call `_clear_type_cache()` and `gc.collect()` to get more predictable results.

If a Python build or implementation cannot reasonably compute this information, getallocatedblocks() is allowed to return 0 instead.

New in version 3.4.

➡ **sys.getandroidapilevel()**
Return the build time API version of Android as an integer.

Availability: Android.

New in version 3.7.

➡ **sys.getdefaultencoding()**
Return the name of the current default string encoding used by the Unicode implementation.
➡ **sys.getdlopenflags()**
Return the current value of the flags that are used for dlopen() calls. Symbolic names for the flag values can be found in the os module (RTLD_xxx constants, e.g. os.RTLD_LAZY).

Availability: Unix.
➡ **sys.getfilesystemencoding()**
Return the name of the encoding used to convert between Unicode filenames and bytes filenames. For best compatibility, str should be used for filenames in all cases, although representing filenames as bytes is also supported. Functions accepting or returning filenames should support either str or bytes and internally convert to the system’s preferred representation.

This encoding is always ASCII-compatible.

os.fsencode() and os.fsdecode() should be used to ensure that the correct encoding and errors mode are used.
•In the UTF-8 mode, the encoding is utf-8 on any platform.
•On macOS, the encoding is 'utf-8'.
•On Unix, the encoding is the locale encoding.
•On Windows, the encoding may be 'utf-8' or 'mbcs', depending on user configuration.
•On Android, the encoding is 'utf-8'.
•On VxWorks, the encoding is 'utf-8'.


Changed in version 3.2: getfilesystemencoding() result cannot be None anymore.
Changed in version 3.6: Windows is no longer guaranteed to return 'mbcs'. See PEP 529 and `_enablelegacywindowsfsencoding()` for more information.
Changed in version 3.7: Return ‘utf-8’ in the UTF-8 mode.


➡ **sys.getfilesystemencodeerrors()**
Return the name of the error mode used to convert between Unicode filenames and bytes filenames. The encoding name is returned from getfilesystemencoding().

os.fsencode() and os.fsdecode() should be used to ensure that the correct encoding and errors mode are used.

New in version 3.6.

➡ **sys.getrefcount(object)**
Return the reference count of the object. The count returned is generally one higher than you might expect, because it includes the (temporary) reference as an argument to getrefcount().
➡ **sys.getrecursionlimit()**
Return the current value of the recursion limit, the maximum depth of the Python interpreter stack. This limit prevents infinite recursion from causing an overflow of the C stack and crashing Python. It can be set by setrecursionlimit().
➡ **sys.getsizeof(object[, default])**
Return the size of an object in bytes. The object can be any type of object. All built-in objects will return correct results, but this does not have to hold true for third-party extensions as it is implementation specific.

Only the memory consumption directly attributed to the object is accounted for, not the memory consumption of objects it refers to.

If given, default will be returned if the object does not provide means to retrieve the size. Otherwise a TypeError will be raised.

getsizeof() calls the object’s __sizeof__ method and adds an additional garbage collector overhead if the object is managed by the garbage collector.

See recursive sizeof recipe for an example of using getsizeof() recursively to find the size of containers and all their contents.
➡ **sys.getswitchinterval()**
Return the interpreter’s “thread switch interval”; see setswitchinterval().

New in version 3.2.

➡ **sys._getframe([depth])**
Return a `frame object` from the call stack. If optional integer depth is given, return the `frame object` that many calls below the top of the stack. If that is deeper than the call stack, ValueError is raised. The default for depth is zero, returning the frame at the top of the call stack.

Raises an auditing event `sys._getframe` with no arguments.


CPython implementation detail: This function should be used for internal and specialized purposes only. It is not guaranteed to exist in all implementations of Python.
➡ **sys.getprofile()**
Get the profiler function as set by setprofile().
➡ **sys.gettrace()**
Get the trace function as set by settrace().


CPython implementation detail: The gettrace() function is intended only for implementing debuggers, profilers, coverage tools and the like. Its behavior is part of the implementation platform, rather than part of the language definition, and thus may not be available in all Python implementations.
➡ **sys.getwindowsversion()**
Return a named tuple describing the Windows version currently running. The named elements are major, minor, build, platform, service_pack, service_pack_minor, service_pack_major, suite_mask, product_type and platform_version. service_pack contains a string, platform_version a 3-tuple and all other values are integers. The components can also be accessed by name, so sys.getwindowsversion()[0] is equivalent to sys.getwindowsversion().major. For compatibility with prior versions, only the first 5 elements are retrievable by indexing.

platform will be 2 (VER_PLATFORM_WIN32_NT).

product_type may be one of the following values:

|           Constant           |                       Meaning                        |
|------------------------------|------------------------------------------------------|
| 1 (VER_NT_WORKSTATION)       | The system is a workstation.                         |
| 2 (VER_NT_DOMAIN_CONTROLLER) | The system is a domain controller.                   |
| 3 (VER_NT_SERVER)            | The system is a server, but not a domain controller. |

This function wraps the Win32 GetVersionEx() function; see the Microsoft documentation on OSVERSIONINFOEX() for more information about these fields.

platform_version returns the major version, minor version and build number of the current operating system, rather than the version that is being emulated for the process. It is intended for use in logging rather than for feature detection.

Note:
 platform_version derives the version from kernel32.dll which can be of a different version than the OS version. Please use platform module for achieving accurate OS version.
 

Availability: Windows.


Changed in version 3.2: Changed to a named tuple and added service_pack_minor, service_pack_major, suite_mask, and product_type.

Changed in version 3.6: Added platform_version

➡ **sys.get_asyncgen_hooks()**
Returns an asyncgen_hooks object, which is similar to a namedtuple of the form (firstiter, finalizer), where firstiter and finalizer are expected to be either None or functions which take an asynchronous generator iterator as an argument, and are used to schedule finalization of an asynchronous generator by an event loop.

New in version 3.6: See PEP 525 for more details.


Note:
 This function has been added on a provisional basis (see PEP 411 for details.)
 
➡ **sys.get_coroutine_origin_tracking_depth()**
Get the current coroutine origin tracking depth, as set by set_coroutine_origin_tracking_depth().

New in version 3.7.


Note:
 This function has been added on a provisional basis (see PEP 411 for details.) Use it only for debugging purposes.
 
➡ **sys.hash_info**
A named tuple giving parameters of the numeric hash implementation. For more details about hashing of numeric types, see Hashing of numeric types.


| attribute |                           explanation                           |
|-----------|-----------------------------------------------------------------|
| width     | width in bits used for hash values                              |
| modulus   | prime modulus P used for numeric hash scheme                    |
| inf       | hash value returned for a positive infinity                     |
| nan       | hash value returned for a nan                                   |
| imag      | multiplier used for the imaginary part of a complex number      |
| algorithm | name of the algorithm for hashing of str, bytes, and memoryview |
| hash_bits | internal output size of the hash algorithm                      |
| seed_bits | size of the seed key of the hash algorithm                      |

>>> sys.hash_info
sys.hash_info(width=64, modulus=2305843009213693951, inf=314159, nan=0, imag=1000003, algorithm='siphash24', hash_bits=64, seed_bits=128, cutoff=0)

New in version 3.2.

Changed in version 3.4: Added algorithm, hash_bits and seed_bits


➡ **sys.hexversion**
The version number encoded as a single integer. This is guaranteed to increase with each version, including proper support for non-production releases. For example, to test that the Python interpreter is at least version 1.5.2, use:

    if sys.hexversion >= 0x010502F0:
        # use some advanced feature
        ...
    else:
        # use an alternative implementation or warn the user
        ...

This is called hexversion since it only really looks meaningful when viewed as the result of passing it to the built-in hex() function. The named tuple sys.version_info may be used for a more human-friendly encoding of the same information.

More details of hexversion can be found at API and ABI Versioning.
➡ **sys.implementation**
An object containing information about the implementation of the currently running Python interpreter. The following attributes are required to exist in all Python implementations.

name is the implementation’s identifier, e.g. 'cpython'. The actual string is defined by the Python implementation, but it is guaranteed to be lower case.

version is a named tuple, in the same format as sys.version_info. It represents the version of the Python implementation. This has a distinct meaning from the specific version of the Python language to which the currently running interpreter conforms, which sys.version_info represents. For example, for PyPy 1.8 sys.implementation.version might be sys.version_info(1, 8, 0, 'final', 0), whereas sys.version_info would be sys.version_info(2, 7, 2, 'final', 0). For CPython they are the same value, since it is the reference implementation.

hexversion is the implementation version in hexadecimal format, like sys.hexversion.

cache_tag is the tag used by the import machinery in the filenames of cached modules. By convention, it would be a composite of the implementation’s name and version, like 'cpython-33'. However, a Python implementation may use some other value if appropriate. If cache_tag is set to None, it indicates that module caching should be disabled.

sys.implementation may contain additional attributes specific to the Python implementation. These non-standard attributes must start with an underscore, and are not described here. Regardless of its contents, sys.implementation will not change during a run of the interpreter, nor between implementation versions. (It may change between Python language versions, however.) See PEP 421 for more information.

New in version 3.3.


Note:
 The addition of new required attributes must go through the normal PEP process. See PEP 421 for more information.
 
➡ **sys.int_info**
A named tuple that holds information about Python’s internal representation of integers. The attributes are read only.

|   Attribute    |                      Explanation                      |
|----------------|-------------------------------------------------------|
| bits_per_digit | number of bits held in each digit.                    |
| sizeof_digit   | size in bytes of the C type used to represent a digit |

Python integers are stored internally in base `2**int_info.bits_per_digit` 

New in version 3.1.

➡ `sys.__interactivehook__`
When this attribute exists, its value is automatically called (with no arguments) when the interpreter is launched in interactive mode. This is done after the PYTHONSTARTUP file is read, so that you can set this hook there. The site module sets this.

Raises an auditing event cpython.run_interactivehook with the hook object as the argument when the hook is called on startup.

New in version 3.4.

➡ **sys.intern(string)**
Enter string in the table of “interned” strings and return the interned string – which is string itself or a copy. Interning strings is useful to gain a little performance on dictionary lookup – if the keys in a dictionary are interned, and the lookup key is interned, the key comparisons (after hashing) can be done by a pointer compare instead of a string compare. Normally, the names used in Python programs are automatically interned, and the dictionaries used to hold module, class or instance attributes have interned keys.

Interned strings are not immortal; you must keep a reference to the return value of intern() around to benefit from it.
➡ **sys.is_finalizing()**
Return True if the Python interpreter is shutting down, False otherwise.

New in version 3.5.

➡ **sys.last_typesys.last_valuesys.last_traceback**
These three variables are not always defined; they are set when an exception is not handled and the interpreter prints an error message and a stack traceback. Their intended use is to allow an interactive user to import a debugger module and engage in post-mortem debugging without having to re-execute the command that caused the error. (Typical use is import pdb; pdb.pm() to enter the post-mortem debugger; see pdb module for more information.)

The meaning of the variables is the same as that of the return values from exc_info() above.
➡ **sys.maxsize**
An integer giving the maximum value a variable of type Py_ssize_t can take. It’s usually 2**31 - 1 on a 32-bit platform and 2**63 - 1 on a 64-bit platform.
➡ **sys.maxunicode**
An integer giving the value of the largest Unicode code point, i.e. 1114111 (0x10FFFF in hexadecimal).


Changed in version 3.3: Before PEP 393, sys.maxunicode used to be either 0xFFFF or 0x10FFFF, depending on the configuration option that specified whether Unicode characters were stored as UCS-2 or UCS-4.
➡ **sys.meta_path**
A list of meta path finder objects that have their find_spec() methods called to see if one of the objects can find the module to be imported. The find_spec() method is called with at least the absolute name of the module being imported. If the module to be imported is contained in a package, then the parent package’s __path__ attribute is passed in as a second argument. The method returns a module spec, or None if the module cannot be found.

See also:
 importlib.abc.MetaPathFinderThe abstract base class defining the interface of finder objects on meta_path.importlib.machinery.ModuleSpecThe concrete class which find_spec() should return instances of.

Changed in version 3.4: Module specs were introduced in Python 3.4, by PEP 451. Earlier versions of Python looked for a method called find_module(). This is still called as a fallback if a meta_path entry doesn’t have a find_spec() method.
➡ **sys.modules**
This is a dictionary that maps module names to modules which have already been loaded. This can be manipulated to force reloading of modules and other tricks. However, replacing the dictionary will not necessarily work as expected and deleting essential items from the dictionary may cause Python to fail.
➡ **sys.path**
A list of strings that specifies the search path for modules. Initialized from the environment variable PYTHONPATH, plus an installation-dependent default.

As initialized upon program startup, the first item of this list, path[0], is the directory containing the script that was used to invoke the Python interpreter. If the script directory is not available (e.g. if the interpreter is invoked interactively or if the script is read from standard input), path[0] is the empty string, which directs Python to search modules in the current directory first. Notice that the script directory is inserted before the entries inserted as a result of PYTHONPATH.

A program is free to modify this list for its own purposes. Only strings and bytes should be added to sys.path; all other data types are ignored during import.

See also:
 Module site This describes how to use .pth files to extend sys.path.
 
➡ **sys.path_hooks**
A list of callables that take a path argument to try to create a finder for the path. If a finder can be created, it is to be returned by the callable, else raise ImportError.

Originally specified in PEP 302.
➡ **sys.path_importer_cache**
A dictionary acting as a cache for finder objects. The keys are paths that have been passed to sys.path_hooks and the values are the finders that are found. If a path is a valid file system path but no finder is found on sys.path_hooks then None is stored.

Originally specified in PEP 302.


Changed in version 3.3: None is stored instead of imp.NullImporter when no finder is found.
➡ **sys.platform**
This string contains a platform identifier that can be used to append platform-specific components to sys.path, for instance.

For Unix systems, except on Linux and AIX, this is the lowercased OS name as returned by uname -s with the first part of the version as returned by uname -r appended, e.g. 'sunos5' or 'freebsd8', at the time when Python was built. Unless you want to test for a specific system version, it is therefore recommended to use the following idiom:


    if sys.platform.startswith('freebsd'):
        # FreeBSD-specific code here...
    elif sys.platform.startswith('linux'):
        # Linux-specific code here...
    elif sys.platform.startswith('aix'):
        # AIX-specific code here...


For other systems, the values are:

|     System     | platform value |
|----------------|----------------|
| AIX            | 'aix'          |
| Linux          | 'linux'        |
| Windows        | 'win32'        |
| Windows/Cygwin | 'cygwin'       |
| macOS          | 'darwin'       |


Changed in version 3.3: On Linux, sys.platform doesn’t contain the major version anymore. It is always 'linux', instead of 'linux2' or 'linux3'. Since older Python versions include the version number, it is recommended to always use the startswith idiom presented above.


Changed in version 3.8: On AIX, sys.platform doesn’t contain the major version anymore. It is always 'aix', instead of 'aix5' or 'aix7'. Since older Python versions include the version number, it is recommended to always use the startswith idiom presented above.

See also:
 os.name has a coarser granularity. os.uname() gives system-dependent version information.
 
The platform module provides detailed checks for the system’s identity.
➡ **sys.platlibdir**
Name of the platform-specific library directory. It is used to build the path of standard library and the paths of installed extension modules.

It is equal to "lib" on most platforms. On Fedora and SuSE, it is equal to "lib64" on 64-bit platforms which gives the following sys.path paths (where X.Y is the Python major.minor version):
•/usr/lib64/pythonX.Y/: Standard library (like os.py of the os module)
•/usr/lib64/pythonX.Y/lib-dynload/: C extension modules of the standard library (like the errno module, the exact filename is platform specific)
•/usr/lib/pythonX.Y/site-packages/ (always use lib, not sys.platlibdir): Third-party modules
•/usr/lib64/pythonX.Y/site-packages/: C extension modules of third-party packages

New in version 3.9.

➡ **sys.prefix**
A string giving the site-specific directory prefix where the platform independent Python files are installed; by default, this is the string '/usr/local'. This can be set at build time with the --prefix argument to the configure script. The main collection of Python library modules is installed in the directory prefix/lib/pythonX.Y while the platform independent header files (all except pyconfig.h) are stored in prefix/include/pythonX.Y, where X.Y is the version number of Python, for example 3.2.

Note:
 If a virtual environment is in effect, this value will be changed in site.py to point to the virtual environment. The value for the Python installation will still be available, via base_prefix.
 
➡ **sys.ps1sys.ps2**
Strings specifying the primary and secondary prompt of the interpreter. These are only defined if the interpreter is in interactive mode. Their initial values in this case are '>>> ' and '... '. If a non-string object is assigned to either variable, its str() is re-evaluated each time the interpreter prepares to read a new interactive command; this can be used to implement a dynamic prompt.
➡ **sys.setdlopenflags(n)**
Set the flags used by the interpreter for dlopen() calls, such as when the interpreter loads extension modules. Among other things, this will enable a lazy resolving of symbols when importing a module, if called as sys.setdlopenflags(0). To share symbols across extension modules, call as sys.setdlopenflags(os.RTLD_GLOBAL). Symbolic names for the flag values can be found in the os module (RTLD_xxx constants, e.g. os.RTLD_LAZY).

Availability: Unix.
➡ **sys.setprofile(profilefunc)**
Set the system’s profile function, which allows you to implement a Python source code profiler in Python. See chapter The Python Profilers for more information on the Python profiler. The system’s profile function is called similarly to the system’s trace function (see settrace()), but it is called with different events, for example it isn’t called for each executed line of code (only on call and return, but the return event is reported even when an exception has been set). The function is thread-specific, but there is no way for the profiler to know about context switches between threads, so it does not make sense to use this in the presence of multiple threads. Also, its return value is not used, so it can simply return None. Error in the profile function will cause itself unset.

Profile functions should have three arguments: frame, event, and arg. frame is the current stack frame. event is a string: 'call', 'return', 'c_call', 'c_return', or 'c_exception'. arg depends on the event type.

Raises an auditing event sys.setprofile with no arguments.

The events have the following meaning:
'call'A function is called (or some other code block entered). The profile function is called; arg is None.'return'A function (or other code block) is about to return. The profile function is called; arg is the value that will be returned, or None if the event is caused by an exception being raised.'c_call'A C function is about to be called. This may be an extension function or a built-in. arg is the C function object.'c_return'A C function has returned. arg is the C function object.'c_exception'A C function has raised an exception. arg is the C function object.sys.setrecursionlimit(limit)
Set the maximum depth of the Python interpreter stack to limit. This limit prevents infinite recursion from causing an overflow of the C stack and crashing Python.

The highest possible limit is platform-dependent. A user may need to set the limit higher when they have a program that requires deep recursion and a platform that supports a higher limit. This should be done with care, because a too-high limit can lead to a crash.

If the new limit is too low at the current recursion depth, a RecursionError exception is raised.


Changed in version 3.5.1: A RecursionError exception is now raised if the new limit is too low at the current recursion depth.
➡ **sys.setswitchinterval(interval)**
Set the interpreter’s thread switch interval (in seconds). This floating-point value determines the ideal duration of the “timeslices” allocated to concurrently running Python threads. Please note that the actual value can be higher, especially if long-running internal functions or methods are used. Also, which thread becomes scheduled at the end of the interval is the operating system’s decision. The interpreter doesn’t have its own scheduler.

New in version 3.2.

➡ **sys.settrace(tracefunc)**
Set the system’s trace function, which allows you to implement a Python source code debugger in Python. The function is thread-specific; for a debugger to support multiple threads, it must register a trace function using `settrace()` for each thread being debugged or use `threading.settrace()`.

Trace functions should have three arguments: 

↪ `frame` - frame is the current stack frame.
↪ `event` - event is a string: 'call', 'line', 'return', 'exception' or 'opcode'. 
↪ `arg`   - arg depends on the event type.

The trace function is invoked (with event set to 'call') whenever a new local scope is entered; it should return a reference to a local trace function to be used for the new scope, or None if the scope shouldn’t be traced.

The local trace function should return a reference to itself (or to another function for further tracing in that scope), or None to turn off tracing in that scope.

If there is any error occurred in the trace function, it will be unset, just like `settrace(None)` is called.

The events have the following meaning:
↪ `call`
    A function is called (or some other code block entered). The global trace function is called; arg is None; the return value specifies the local trace function.
↪ `line`
    The interpreter is about to execute a new line of code or re-execute the condition of a loop. The local trace function is called; arg is None; the return value specifies the new local trace function. See Objects/lnotab_notes.txt for a detailed explanation of how this works. Per-line events may be disabled for a frame by setting f_trace_lines to False on that frame.
↪ `return`
    A function (or other code block) is about to return. The local trace function is called; arg is the value that will be returned, or None if the event is caused by an exception being raised. The trace function’s return value is ignored.
↪ `exception`
    An exception has occurred. The local trace function is called; arg is a tuple (exception, value, traceback); the return value specifies the new local trace function.
↪ `opcode`
    The interpreter is about to execute a new opcode (see dis for opcode details). The local trace function is called; arg is None; the return value specifies the new local trace function. Per-opcode events are not emitted by default: they must be explicitly requested by setting f_trace_opcodes to True on the frame.

Note that as an exception is propagated down the chain of callers, an '`exception`' event is generated at each level.

For more fine-grained usage, it’s possible to set a trace function by assigning frame.f_trace = tracefunc explicitly, rather than relying on it being set indirectly via the return value from an already installed trace function. This is also required for activating the trace function on the current frame, which settrace() doesn’t do. Note that in order for this to work, a global tracing function must have been installed with settrace() in order to enable the runtime tracing machinery, but it doesn’t need to be the same tracing function (e.g. it could be a low overhead tracing function that simply returns None to disable itself immediately on each frame).

For more information on code and `frame objects`, refer to The standard type hierarchy.

Raises an auditing event sys.settrace with no arguments.


CPython implementation detail: The settrace() function is intended only for implementing debuggers, profilers, coverage tools and the like. Its behavior is part of the implementation platform, rather than part of the language definition, and thus may not be available in all Python implementations.

Changed in version 3.7: 'opcode' event type added; f_trace_lines and f_trace_opcodes attributes added to frames

➡ **sys.set_asyncgen_hooks(firstiter, finalizer)**
Accepts two optional keyword arguments which are callables that accept an asynchronous generator iterator as an argument. The firstiter callable will be called when an asynchronous generator is iterated for the first time. The finalizer will be called when an asynchronous generator is about to be garbage collected.

Raises an auditing event sys.set_asyncgen_hooks_firstiter with no arguments.

Raises an auditing event sys.set_asyncgen_hooks_finalizer with no arguments.

Two auditing events are raised because the underlying API consists of two calls, each of which must raise its own event.

New in version 3.6: See PEP 525 for more details, and for a reference example of a finalizer method see the implementation of asyncio.Loop.shutdown_asyncgens in Lib/asyncio/base_events.py


Note:
 This function has been added on a provisional basis (see PEP 411 for details.)
 
➡ **sys.set_coroutine_origin_tracking_depth(depth)**
Allows enabling or disabling coroutine origin tracking. When enabled, the cr_origin attribute on coroutine objects will contain a tuple of (filename, line number, function name) tuples describing the traceback where the coroutine object was created, with the most recent call first. When disabled, cr_origin will be None.

To enable, pass a depth value greater than zero; this sets the number of frames whose information will be captured. To disable, pass set depth to zero.

This setting is thread-specific.

New in version 3.7.


Note:
 This function has been added on a provisional basis (see PEP 411 for details.) Use it only for debugging purposes.
 
➡ **sys._enablelegacywindowsfsencoding()**
Changes the default filesystem encoding and errors mode to ‘mbcs’ and ‘replace’ respectively, for consistency with versions of Python prior to 3.6.

This is equivalent to defining the PYTHONLEGACYWINDOWSFSENCODING environment variable before launching Python.

Availability: Windows.

New in version 3.6: See PEP 529 for more details.

➡ **sys.stdin`
➡ `sys.stdout`
➡ `sys.stderr**
File objects used by the interpreter for standard input, output and errors:
•stdin is used for all interactive input (including calls to input());
•stdout is used for the output of print() and expression statements and for the prompts of input();
•The interpreter’s own prompts and its error messages go to stderr.

These streams are regular text files like those returned by the open() function. Their parameters are chosen as follows:

•The character encoding is platform-dependent. Non-Windows platforms use the locale encoding (see locale.getpreferredencoding()).

On Windows, UTF-8 is used for the console device. Non-character devices such as disk files and pipes use the system locale encoding (i.e. the ANSI codepage). Non-console character devices such as NUL (i.e. where isatty() returns True) use the value of the console input and output codepages at startup, respectively for stdin and stdout/stderr. This defaults to the system locale encoding if the process is not initially attached to a console.

The special behaviour of the console can be overridden by setting the environment variable PYTHONLEGACYWINDOWSSTDIO before starting Python. In that case, the console codepages are used as for any other character device.

Under all platforms, you can override the character encoding by setting the PYTHONIOENCODING environment variable before starting Python or by using the new -X utf8 command line option and PYTHONUTF8 environment variable. However, for the Windows console, this only applies when PYTHONLEGACYWINDOWSSTDIO is also set.


•When interactive, the stdout stream is line-buffered. Otherwise, it is block-buffered like regular text files. The stderr stream is line-buffered in both cases. You can make both streams unbuffered by passing the -u command-line option or setting the PYTHONUNBUFFERED environment variable.



Changed in version 3.9: Non-interactive stderr is now line-buffered instead of fully buffered.

Note:
 To write or read binary data from/to the standard streams, use the underlying binary buffer object. For example, to write bytes to stdout, use sys.stdout.buffer.write(b'abc').
 
However, if you are writing a library (and do not control in which context its code will be executed), be aware that the standard streams may be replaced with file-like objects like io.StringIO which do not support the buffer attribute.
➡ `sys.__stdin__`
➡ `sys.__stdout__`
➡ `sys.__stderr__`
These objects contain the original values of stdin, stderr and stdout at the start of the program. They are used during finalization, and could be useful to print to the actual standard stream no matter if the sys.std* object has been redirected.

It can also be used to restore the actual files to known working file objects in case they have been overwritten with a broken object. However, the preferred way to do this is to explicitly save the previous stream before replacing it, and restore the saved object.

Note:
 Under some conditions stdin, stdout and stderr as well as the original values __stdin__, __stdout__ and __stderr__ can be None. It is usually the case for Windows GUI apps that aren’t connected to a console and Python apps started with pythonw.
 
➡ **sys.thread_info**
A named tuple holding information about the thread implementation.

|Attribute| Explanation|
name Name of the thread implementation:

•'nt': Windows threads
•'pthread': POSIX threads
•'solaris': Solaris threads
 
lock Name of the lock implementation:

•'semaphore': a lock uses a semaphore
•'mutex+cond': a lock uses a mutex and a condition variable
•None if this information is unknown
 
version Name and version of the thread library. It is a string, or None if this information is unknown. 

New in version 3.3.

➡ **sys.tracebacklimit**
When this variable is set to an integer value, it determines the maximum number of levels of traceback information printed when an unhandled exception occurs. The default is 1000. When set to 0 or less, all traceback information is suppressed and only the exception type and value are printed.
➡ **sys.unraisablehook(unraisable, /)**
Handle an unraisable exception.

Called when an exception has occurred but there is no way for Python to handle it. For example, when a destructor raises an exception or during garbage collection (gc.collect()).

The unraisable argument has the following attributes:
• *exc_type*: Exception type.
• *exc_value*: Exception value, can be None.
• *exc_traceback*: Exception traceback, can be None.
• *err_msg*: Error message, can be None.
• *object*: Object causing the exception, can be None.

The default hook formats err_msg and object as: f'{err_msg}: {object!r}'; use “Exception ignored in” error message if err_msg is None.

*sys.unraisablehook()* can be overridden to control how unraisable exceptions are handled.

Storing exc_value using a custom hook can create a reference cycle. It should be cleared explicitly to break the reference cycle when the exception is no longer needed.

Storing object using a custom hook can resurrect it if it is set to an object which is being finalized. Avoid storing object after the custom hook completes to avoid resurrecting objects.

See also excepthook() which handles uncaught exceptions.

Raise an auditing event sys.unraisablehook with arguments hook, unraisable when an exception that cannot be handled occurs. The unraisable object is the same as what will be passed to the hook. If no hook has been set, hook may be None.

New in version 3.8.

➡ **sys.version**
A string containing the version number of the Python interpreter plus additional information on the build number and compiler used. This string is displayed when the interactive interpreter is started. Do not extract version information out of it, rather, use version_info and the functions provided by the platform module.
➡ **sys.api_version**
The C API version for this interpreter. Programmers may find this useful when debugging version conflicts between Python and extension modules.
➡ **sys.version_info**
A tuple containing the five components of the version number: major, minor, micro, releaselevel, and serial. All values except releaselevel are integers; the release level is 'alpha', 'beta', 'candidate', or 'final'. The version_info value corresponding to the Python version 2.0 is (2, 0, 0, 'final', 0). The components can also be accessed by name, so sys.version_info[0] is equivalent to sys.version_info.major and so on.


Changed in version 3.1: Added named component attributes.
➡ **sys.warnoptions**
This is an implementation detail of the warnings framework; do not modify this value. Refer to the warnings module for more information on the warnings framework.
➡ **sys.winver**
The version number used to form registry keys on Windows platforms. This is stored as string resource 1000 in the Python DLL. The value is normally the first three characters of version. It is provided in the sys module for informational purposes; modifying this value has no effect on the registry keys used by Python.

Availability: Windows.
➡ **sys._xoptions**
A dictionary of the various implementation-specific flags passed through the -X command-line option. Option names are either mapped to their values, if given explicitly, or to True. Example:


>$ ./python -Xa=b -Xc
Python 3.2a3+ (py3k, Oct 16 2010, 20:14:50)
[GCC 4.4.3] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import sys
>>> sys._xoptions
{'a': 'b', 'c': True}

CPython implementation detail: This is a CPython-specific way of accessing options passed through -X. Other implementations may export them through other means, or not at all.


## ==⚡ sysconfig — Provide access to Python’s configuration information

New in version 3.2.

Source code: Lib/sysconfig.py


The `sysconfig` module provides access to Python’s configuration information like the list of installation paths and the configuration variables relevant for the current platform.


### ===🗝 Configuration variables

A Python distribution contains a Makefile and a `pyconfig.h` header file that are necessary to build both the Python binary itself and third-party C extensions compiled using distutils.

`sysconfig` puts all variables found in these files in a dictionary that can be accessed using `get_config_vars()` or `get_config_var()`.

Notice that on Windows, it’s a much smaller set.

➡ `sysconfig.get_config_vars(*args)`
With no arguments, return a dictionary of all configuration variables relevant for the current platform.

With arguments, return a list of values that result from looking up each argument in the configuration variable dictionary.

For each argument, if the value is not found, return None.

➡ `sysconfig.get_config_var(name)`
Return the value of a single variable name. Equivalent to get_config_vars().get(name).

If name is not found, return None.

Example of usage:


>>> import sysconfig
>>> sysconfig.get_config_var('Py_ENABLE_SHARED')
0
>>> sysconfig.get_config_var('LIBDIR')
'/usr/local/lib'
>>> sysconfig.get_config_vars('AR', 'CXX')
['ar', 'g++']



### ===🗝 Installation paths

Python uses an installation scheme that differs depending on the platform and on the installation options. These schemes are stored in sysconfig under unique identifiers based on the value returned by os.name.

Every new component that is installed using distutils or a Distutils-based system will follow the same scheme to copy its file in the right places.

Python currently supports six schemes:

• `posix_prefix`: scheme for POSIX platforms like Linux or macOS. This is the default scheme used when Python or a component is installed.
• `posix_home`: scheme for POSIX platforms used when a home option is used upon installation. This scheme is used when a component is installed through Distutils with a specific home prefix.
• `posix_user`: scheme for POSIX platforms used when a component is installed through Distutils and the user option is used. This scheme defines paths located under the user home directory.
• `nt`: scheme for NT platforms like Windows.
• `nt_user`: scheme for NT platforms, when the user option is used.
• `osx_framework_user`: scheme for macOS, when the user option is used.

Each scheme is itself composed of a series of paths and each path has a unique identifier. Python currently uses eight paths:

• `stdlib`: directory containing the standard Python library files that are not platform-specific.
• `platstdlib`: directory containing the standard Python library files that are platform-specific.
• `platlib`: directory for site-specific, platform-specific files.
• `purelib`: directory for site-specific, non-platform-specific files.
• `include`: directory for non-platform-specific header files.
• `platinclude`: directory for platform-specific header files.
• `scripts`: directory for script files.
• `data`: directory for data files.

sysconfig provides some functions to determine these paths.

➡ `sysconfig.get_scheme_names()`
Return a tuple containing all schemes currently supported in sysconfig.

➡ `sysconfig.get_default_scheme()`
Return the default scheme name for the current platform.


Changed in version 3.10: This function was previously named `_get_default_scheme()` and considered an implementation detail.

➡ `sysconfig.get_preferred_scheme(key)`
Return a preferred scheme name for an installation layout specified by key.

key must be either "prefix", "home", or "user".

The return value is a scheme name listed in `get_scheme_names()`. It can be passed to sysconfig functions that take a scheme argument, such as `get_paths()`.

New in version 3.10.


➡ `sysconfig._get_preferred_schemes()`
Return a dict containing preferred scheme names on the current platform. Python implementers and redistributors may add their preferred schemes to the `_INSTALL_SCHEMES` module-level global value, and modify this function to return those scheme names, to e.g. provide different schemes for system and language package managers to use, so packages installed by either do not mix with those by the other.

End users should not use this function, but `get_default_scheme()` and `get_preferred_scheme()` instead.


New in version 3.10.

➡ `sysconfig.get_path_names()`
Return a tuple containing all path names currently supported in sysconfig.

➡ `sysconfig.get_path(name[, scheme[, vars[, expand]]])`
Return an installation path corresponding to the path name, from the install scheme named scheme.

name has to be a value from the list returned by `get_path_names()`.

sysconfig stores installation paths corresponding to each path name, for each platform, with variables to be expanded. For instance the stdlib path for the nt scheme is: {base}/Lib.

`get_path()` will use the variables returned by `get_config_vars()` to expand the path. All variables have default values for each platform so one may call this function and get the default value.

If scheme is provided, it must be a value from the list returned by get_scheme_names(). Otherwise, the default scheme for the current platform is used.

If vars is provided, it must be a dictionary of variables that will update the dictionary return by get_config_vars().

If expand is set to False, the path will not be expanded using the variables.

If name is not found, raise a KeyError.

➡ `sysconfig.get_paths([scheme[, vars[, expand]]])`
Return a dictionary containing all installation paths corresponding to an installation scheme. See get_path() for more information.

If scheme is not provided, will use the default scheme for the current platform.

If vars is provided, it must be a dictionary of variables that will update the dictionary used to expand the paths.

If expand is set to false, the paths will not be expanded.

If scheme is not an existing scheme, get_paths() will raise a KeyError.


### ===🗝 Other functions

➡ `sysconfig.get_python_version()`
Return the MAJOR.MINOR Python version number as a string. Similar to '%d.%d' % sys.version_info[:2].

➡ `sysconfig.get_platform()`
Return a string that identifies the current platform.

This is used mainly to distinguish platform-specific build directories and platform-specific built distributions. Typically includes the OS name and version and the architecture (as supplied by ‘os.uname()’), although the exact information included depends on the OS; e.g., on Linux, the kernel version isn’t particularly important.

Examples of returned values:

• linux-i586
• linux-alpha (?)
• solaris-2.6-sun4u

Windows will return one of:

• win-amd64 (64bit Windows on AMD64, aka x86_64, Intel64, and EM64T)
• win32 (all others - specifically, sys.platform is returned)

macOS can return:

• macosx-10.6-ppc
• macosx-10.4-ppc64
• macosx-10.3-i386
• macosx-10.4-fat

For other non-POSIX platforms, currently just returns sys.platform.

➡ `sysconfig.is_python_build()`
Return True if the running Python interpreter was built from source and is being run from its built location, and not from a location resulting from e.g. running make install or installing via a binary installer.

➡ `sysconfig.parse_config_h(fp[, vars])`
Parse a config.h-style file.

fp is a file-like object pointing to the config.h-like file.

A dictionary containing name/value pairs is returned. If an optional dictionary is passed in as the second argument, it is used instead of a new dictionary, and updated with the values read in the file.

➡ `sysconfig.get_config_h_filename()`
Return the path of pyconfig.h.

➡ `sysconfig.get_makefile_filename()`
Return the path of Makefile.


### ===🗝 Using sysconfig as a script

You can use sysconfig as a script with Python’s -m option:


```sh
$ python -m sysconfig
Platform: "macosx-10.4-i386"
Python version: "3.2"
Current installation scheme: "posix_prefix"

Paths:
        data = "/usr/local"
        include = "/Users/tarek/Dev/svn.python.org/py3k/Include"
        platinclude = "."
        platlib = "/usr/local/lib/python3.2/site-packages"
        platstdlib = "/usr/local/lib/python3.2"
        purelib = "/usr/local/lib/python3.2/site-packages"
        scripts = "/usr/local/bin"
        stdlib = "/usr/local/lib/python3.2"

Variables:
        AC_APPLE_UNIVERSAL_BUILD = "0"
        AIX_GENUINE_CPLUSPLUS = "0"
        AR = "ar"
        ARFLAGS = "rc"
        ...
```


This call will print in the standard output the information returned by `get_platform()`, `get_python_version()`, `get_path()` and `get_config_vars()`.



## ==⚡ builtins — Built-in objects

This module provides direct access to all ‘built-in’ identifiers of Python; for example, `builtins.open` is the full name for the built-in function `open()`. See Built-in Functions and Built-in Constants for documentation.

This module is not normally accessed explicitly by most applications, but can be useful in modules that provide objects with the same name as a built-in value, but in which the built-in of that name is also needed. For example, in a module that wants to implement an `open()` function that wraps the built-in `open()`, this module can be used directly:


```py
import builtins

def open(path):
    f = builtins.open(path, 'r')
    return UpperCaser(f)

class UpperCaser:
    '''Wrapper around a file that converts output to upper-case.'''

    def __init__(self, f):
        self._f = f

    def read(self, count=-1):
        return self._f.read(count).upper()

    # ...
```


As an implementation detail, most modules have the name __builtins__ made available as part of their globals. The value of __builtins__ is normally either this module or the value of this module’s __dict__ attribute. Since this is an implementation detail, it may not be used by alternate implementations of Python.


## ==⚡ __main__ — Top-level script environment

In Python, the special name __main__ is used for two important constructs:

1. the name of the top-level environment of the program, which can be checked using the __name__ == '__main__' expression; and

2. the __main__.py file in Python packages.

Both of these mechanisms are related to Python modules; how users interact with them and how they interact with each other. They are explained in detail below. If you’re new to Python modules, see the tutorial section Modules for an introduction.


### ===🗝 __name__ == '__main__'

When a Python module or package is imported, __name__ is set to the module’s name. Usually, this is the name of the Python file itself without the .py extension:


>>> import configparser
>>> configparser.__name__
'configparser'


If the file is part of a package, __name__ will also include the parent package’s path:


>>> from concurrent.futures import process
>>> process.__name__
'concurrent.futures.process'


However, if the module is executed in the top-level code environment, its __name__ is set to the string '__main__'.


### ===🗝 What is the “top-level code environment”?

__main__ is the name of the environment where top-level code is run. “Top-level code” is the first user-specified Python module that starts running. It’s “top-level” because it imports all other modules that the program needs. Sometimes “top-level code” is called an entry point to the application.

The top-level code environment can be:

• the scope of an interactive prompt:


>>> __name__
'__main__'



• the Python module passed to the Python interpreter as a file argument:

```sh
$ python3 helloworld.py
Hello, world!
```

• the Python module or package passed to the Python interpreter with the -m argument:



```sh
$ python3 -m tarfile
usage: tarfile.py [-h] [-v] (...)
```

• Python code read by the Python interpreter from standard input:


```sh
$ echo "import this" | python3
The Zen of Python, by Tim Peters

Beautiful is better than ugly.
Explicit is better than implicit.
...
```

• Python code passed to the Python interpreter with the -c argument:

```sh
$ python3 -c "import this"
The Zen of Python, by Tim Peters

Beautiful is better than ugly.
Explicit is better than implicit.
...
```


In each of these situations, the top-level module’s __name__ is set to '__main__'.

As a result, a module can discover whether or not it is running in the top-level environment by checking its own __name__, which allows a common idiom for conditionally executing code when the module is not initialized from an import statement:


if __name__ == '__main__':
    # Execute when the module is not initialized from an import statement.
    ...


See also:
 For a more detailed look at how __name__ is set in all situations, see the tutorial section Modules.
 

### ===🗝 Idiomatic Usage

Some modules contain code that is intended for script use only, like parsing command-line arguments or fetching data from standard input. If a module like this was imported from a different module, for example to unit test it, the script code would unintentionally execute as well.

This is where using the if __name__ == '__main__' code block comes in handy. Code within this block won’t run unless the module is executed in the top-level environment.

Putting as few statements as possible in the block below if __name___ == '__main__' can improve code clarity and correctness. Most often, a function named main encapsulates the program’s primary behavior:


```py
# echo.py

import shlex
import sys

def echo(phrase: str) -> None:
   """A dummy wrapper around print."""
   # for demonstration purposes, you can imagine that there is some
   # valuable and reusable logic inside this function
   print(phrase)

def main() -> int:
    """Echo the input arguments to standard output"""
    phrase = shlex.join(sys.argv)
    echo(phrase)
    return 0

if __name__ == '__main__':
    sys.exit(main())  # next section explains the use of sys.exit
```


Note that if the module didn’t encapsulate code inside the main function but instead put it directly within the if __name__ == '__main__' block, the phrase variable would be global to the entire module. This is error-prone as other functions within the module could be unintentionally using the global variable instead of a local name. A main function solves this problem.

Using a main function has the added benefit of the echo function itself being isolated and importable elsewhere. When echo.py is imported, the echo and main functions will be defined, but neither of them will be called, because __name__ != '__main__'.


### ===🗝 Packaging Considerations

main functions are often used to create command-line tools by specifying them as entry points for console scripts. When this is done, pip inserts the function call into a template script, where the return value of main is passed into sys.exit(). For example:


sys.exit(main())


Since the call to main is wrapped in sys.exit(), the expectation is that your function will return some value acceptable as an input to sys.exit(); typically, an integer or None (which is implicitly returned if your function does not have a return statement).

By proactively following this convention ourselves, our module will have the same behavior when run directly (i.e. python3 echo.py) as it will have if we later package it as a console script entry-point in a pip-installable package.

In particular, be careful about returning strings from your main function. sys.exit() will interpret a string argument as a failure message, so your program will have an exit code of 1, indicating failure, and the string will be written to sys.stderr. The echo.py example from earlier exemplifies using the sys.exit(main()) convention.

See also:
 Python Packaging User Guide contains a collection of tutorials and references on how to distribute and install Python packages with modern tools.
 


### ===🗝 __main__.py in Python Packages

If you are not familiar with Python packages, see section Packages of the tutorial. Most commonly, the __main__.py file is used to provide a command-line interface for a package. Consider the following hypothetical package, “bandclass”:


bandclass
  ├── __init__.py
  ├── __main__.py
  └── student.py


__main__.py will be executed when the package itself is invoked directly from the command line using the -m flag. For example:


$ python3 -m bandclass


This command will cause __main__.py to run. How you utilize this mechanism will depend on the nature of the package you are writing, but in this hypothetical case, it might make sense to allow the teacher to search for students:


```py
# bandclass/__main__.py

import sys
from .student import search_students

student_name = sys.argv[2] if len(sys.argv) >= 2 else ''
print(f'Found student: {search_students(student_name)}')
```


Note that from .student import search_students is an example of a relative import. This import style can be used when referencing modules within a package. For more details, see Intra-package References in the Modules section of the tutorial.


### ===🗝 Idiomatic Usage

The contents of __main__.py typically isn’t fenced with if __name__ == '__main__' blocks. Instead, those files are kept short, functions to execute from other modules. Those other modules can then be easily unit-tested and are properly reusable.

If used, an if __name__ == '__main__' block will still work as expected for a __main__.py file within a package, because its __name__ attribute will include the package’s path if imported:


>>> import asyncio.__main__
>>> asyncio.__main__.__name__
'asyncio.__main__'


This won’t work for __main__.py files in the root directory of a .zip file though. Hence, for consistency, minimal __main__.py like the venv one mentioned above are preferred.

See also:
 See venv for an example of a package with a minimal __main__.py in the standard library. It doesn’t contain a if __name__ == '__main__' block. You can invoke it with python3 -m venv [directory].
 
See runpy for more details on the -m flag to the interpreter executable.

See `zipapp` for how to run applications packaged as .zip files. In this case Python looks for a __main__.py file in the root directory of the archive.


### ===🗝 import __main__

Regardless of which module a Python program was started with, other modules running within that same program can import the top-level environment’s scope (namespace) by importing the __main__ module. This doesn’t import a __main__.py file but rather whichever module that received the special name '__main__'.

Here is an example module that consumes the __main__ namespace:


```py
# namely.py

import __main__

def did_user_define_their_name():
    return 'my_name' in dir(__main__)

def print_user_name():
    if not did_user_define_their_name():
        raise ValueError('Define the variable `my_name`!')

    if '__file__' in dir(__main__):
        print(__main__.my_name, "found in file", __main__.__file__)
    else:
        print(__main__.my_name)
```


Example usage of this module could be as follows:


```py
# start.py

import sys

from namely import print_user_name

# my_name = "Dinsdale"

def main():
    try:
        print_user_name()
    except ValueError as ve:
        return str(ve)

if __name__ == "__main__":
    sys.exit(main())
```


Now, if we started our program, the result would look like this:


```sh
$ python3 start.py
Define the variable `my_name`!
```


The exit code of the program would be 1, indicating an error. Uncommenting the line with my_name = "Dinsdale" fixes the program and now it exits with status code 0, indicating success:


```sh
$ python3 start.py
Dinsdale found in file /path/to/start.py
```


Note that importing __main__ doesn’t cause any issues with unintentionally running top-level code meant for script use which is put in the `if __name__ == "__main__"` block of the start module. Why does this work?

Python inserts an empty __main__ module in `sys.modules` at interpreter startup, and populates it by running top-level code. 

In our example this is the start module which runs line by line and imports namely. In turn, namely imports __main__ (which is really start). That’s an import cycle! Fortunately, since the partially populated __main__ module is present in `sys.modules`, Python passes that to namely. See Special considerations for __main__ in the import system’s reference for details on how this works.

The Python REPL is another example of a “top-level environment”, so anything defined in the REPL becomes part of the __main__ scope:


>>> import namely
>>> namely.did_user_define_their_name()
False
>>> namely.print_user_name()
Traceback (most recent call last):
...
ValueError: Define the variable `my_name`!
>>> my_name = 'Jabberwocky'
>>> namely.did_user_define_their_name()
True
>>> namely.print_user_name()
Jabberwocky


Note that in this case the __main__ scope doesn’t contain a __file__ attribute as it’s interactive.

The __main__ scope is used in the implementation of pdb and rlcompleter.


## ==⚡ warnings — Warning control
◦ Warning Categories
◦ The Warnings Filter
◾Describing Warning Filters
◾Default Warning Filter
◾Overriding the default filter

◦ Temporarily Suppressing Warnings
◦ Testing Warnings
◦ Updating Code For New Versions of Dependencies
◦ Available Functions
◦ Available Context Managers

## ==⚡ dataclasses — Data Classes
◦ Module-level decorators, classes, and functions
◦ Post-init processing
◦ Class variables
◦ Init-only variables
◦ Frozen instances
◦ Inheritance
◦ Default factory functions
◦ Mutable default values
◦ Exceptions

## ==⚡ contextlib — Utilities for with-statement contexts
◦ Utilities
◦ Examples and Recipes
◾Supporting a variable number of context managers
◾Catching exceptions from __enter__ methods
◾Cleaning up in an __enter__ implementation
◾Replacing any use of try-finally and flag variables
◾Using a context manager as a function decorator

◦ Single use, reusable and reentrant context managers
◾Reentrant context managers
◾Reusable context managers


## ==⚡ abc — Abstract Base Classes

Source code: Lib/abc.py

ABC，Abstract Base Class（抽象基类），主要定义接口基本的抽象方法，为子类定义提供统一 API 接口。抽象基类可以不实现具体的方法（当然也可以实现，只不过子类如果想调用抽象基类中定义的方法需要使用super()）而是将其留给派生类实现。

This module provides the infrastructure for defining abstract base classes (ABCs) in Python, as outlined in PEP 3119; see the PEP for why this was added to Python. (See also PEP 3141 and the numbers module regarding a type hierarchy for numbers based on ABCs.)

The collections module has some concrete classes that derive from ABCs; these can, of course, be further derived. In addition, the collections.abc submodule has some ABCs that can be used to test whether a class or instance provides a particular interface, for example, if it is hashable or if it is a mapping.

This module provides the metaclass ABCMeta for defining ABCs and a helper class ABC to alternatively define ABCs through inheritance:

✅ `class abc.ABC`
A helper class that has ABCMeta as its metaclass. With this class, an abstract base class can be created by simply deriving from ABC avoiding sometimes confusing metaclass usage, for example:


```py
from abc import ABC

class MyABC(ABC):
    pass
```


Note that the type of ABC is still ABCMeta, therefore inheriting from ABC requires the usual precautions regarding metaclass usage, as multiple inheritance may lead to metaclass conflicts. One may also define an abstract base class by passing the metaclass keyword and using ABCMeta directly, for example:


```py
from abc import ABCMeta

class MyABC(metaclass=ABCMeta):
    pass
```


New in version 3.4.


✅ `class abc.ABCMeta`
Metaclass for defining Abstract Base Classes (ABCs).

Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as “virtual subclasses” – these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won’t show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). [1]

Classes created with a metaclass of ABCMeta have the following method:

➡ `register(subclass)`
Register subclass as a “virtual subclass” of this ABC. For example:


```py
from abc import ABC

class MyABC(ABC):
    pass

MyABC.register(tuple)

assert issubclass(tuple, MyABC)
assert isinstance((), MyABC)
```



Changed in version 3.3: Returns the registered subclass, to allow usage as a class decorator.


Changed in version 3.4: To detect calls to register(), you can use the get_cache_token() function.

You can also override this method in an abstract base class:

➡ `__subclasshook__(subclass)`
(Must be defined as a class method.)

Check whether subclass is considered a subclass of this ABC. This means that you can customize the behavior of issubclass further without the need to call register() on every class you want to consider a subclass of the ABC. (This class method is called from the __subclasscheck__() method of the ABC.)

This method should return True, False or NotImplemented. If it returns True, the subclass is considered a subclass of this ABC. If it returns False, the subclass is not considered a subclass of this ABC, even if it would normally be one. If it returns NotImplemented, the subclass check is continued with the usual mechanism.

For a demonstration of these concepts, look at this example ABC definition:


```py
class Foo:
    def __getitem__(self, index):
        ...
    def __len__(self):
        ...
    def get_iterator(self):
        return iter(self)

class MyIterable(ABC):

    @abstractmethod
    def __iter__(self):
        while False:
            yield None

    def get_iterator(self):
        return self.__iter__()

    @classmethod
    def __subclasshook__(cls, C):
        if cls is MyIterable:
            if any("__iter__" in B.__dict__ for B in C.__mro__):
                return True
        return NotImplemented

MyIterable.register(Foo)
```


The ABC MyIterable defines the standard iterable method, __iter__(), as an abstract method. The implementation given here can still be called from subclasses. The get_iterator() method is also part of the MyIterable abstract base class, but it does not have to be overridden in non-abstract derived classes.

The __subclasshook__() class method defined here says that any class that has an __iter__() method in its __dict__ (or in that of one of its base classes, accessed via the __mro__ list) is considered a MyIterable too.

Finally, the last line makes Foo a virtual subclass of MyIterable, even though it does not define an __iter__() method (it uses the old-style iterable protocol, defined in terms of __len__() and __getitem__()). Note that this will not make get_iterator available as a method of Foo, so it is provided separately.

The abc module also provides the following decorator:

✅ `@abc.abstractmethod`
A decorator indicating abstract methods.

Using this decorator requires that the class’s metaclass is ABCMeta or is derived from it. A class that has a metaclass derived from ABCMeta cannot be instantiated unless all of its abstract methods and properties are overridden. The abstract methods can be called using any of the normal ‘super’ call mechanisms. abstractmethod() may be used to declare abstract methods for properties and descriptors.

Dynamically adding abstract methods to a class, or attempting to modify the abstraction status of a method or class once it is created, are only supported using the update_abstractmethods() function. The abstractmethod() only affects subclasses derived using regular inheritance; “virtual subclasses” registered with the ABC’s register() method are not affected.

When abstractmethod() is applied in combination with other method descriptors, it should be applied as the innermost decorator, as shown in the following usage examples:


```py
class C(ABC):
    @abstractmethod
    def my_abstract_method(self, ...):
        ...
    @classmethod
    @abstractmethod
    def my_abstract_classmethod(cls, ...):
        ...
    @staticmethod
    @abstractmethod
    def my_abstract_staticmethod(...):
        ...

    @property
    @abstractmethod
    def my_abstract_property(self):
        ...
    @my_abstract_property.setter
    @abstractmethod
    def my_abstract_property(self, val):
        ...

    @abstractmethod
    def _get_x(self):
        ...
    @abstractmethod
    def _set_x(self, val):
        ...
    x = property(_get_x, _set_x)
```


In order to correctly interoperate with the abstract base class machinery, the descriptor must identify itself as abstract using __isabstractmethod__. In general, this attribute should be True if any of the methods used to compose the descriptor are abstract. For example, Python’s built-in property does the equivalent of:


```py
class Descriptor:
    ...
    @property
    def __isabstractmethod__(self):
        return any(getattr(f, '__isabstractmethod__', False) for
                   f in (self._fget, self._fset, self._fdel))
```


Note:
 Unlike Java abstract methods, these abstract methods may have an implementation. This implementation can be called via the super() mechanism from the class that overrides it. This could be useful as an end-point for a super-call in a framework that uses cooperative multiple-inheritance.
 

The abc module also supports the following legacy decorators:
New in version 3.2.

✅ `@abc.abstractclassmethod`


Deprecated since version 3.3: It is now possible to use classmethod with abstractmethod(), making this decorator redundant.

A subclass of the built-in classmethod(), indicating an abstract classmethod. Otherwise it is similar to abstractmethod().

This special case is deprecated, as the classmethod() decorator is now correctly identified as abstract when applied to an abstract method:


```py
class C(ABC):
    @classmethod
    @abstractmethod
    def my_abstract_classmethod(cls, ...):
        ...
```

New in version 3.2.

✅ `@abc.abstractstaticmethod`


Deprecated since version 3.3: It is now possible to use staticmethod with abstractmethod(), making this decorator redundant.

A subclass of the built-in staticmethod(), indicating an abstract staticmethod. Otherwise it is similar to abstractmethod().

This special case is deprecated, as the staticmethod() decorator is now correctly identified as abstract when applied to an abstract method:


```py
class C(ABC):
    @staticmethod
    @abstractmethod
    def my_abstract_staticmethod(...):
        ...

```

✅ `@abc.abstractproperty`

Deprecated since version 3.3: It is now possible to use property, property.getter(), property.setter() and property.deleter() with abstractmethod(), making this decorator redundant.

A subclass of the built-in property(), indicating an abstract property.

This special case is deprecated, as the property() decorator is now correctly identified as abstract when applied to an abstract method:


```py
class C(ABC):
    @property
    @abstractmethod
    def my_abstract_property(self):
        ...
```


The above example defines a read-only property; you can also define a read-write abstract property by appropriately marking one or more of the underlying methods as abstract:


```py
class C(ABC):
    @property
    def x(self):
        ...

    @x.setter
    @abstractmethod
    def x(self, val):
        ...
```


If only some components are abstract, only those components need to be updated to create a concrete property in a subclass:


```py
class D(C):
    @C.x.setter
    def x(self, val):
        ...
```


The abc module also provides the following functions:

➡ `abc.get_cache_token()`
Returns the current abstract base class cache token.

The token is an opaque object (that supports equality testing) identifying the current version of the abstract base class cache for virtual subclasses. The token changes with every call to ABCMeta.register() on any ABC.

New in version 3.4.


➡ `abc.update_abstractmethods(cls)`
A function to recalculate an abstract class’s abstraction status. This function should be called if a class’s abstract methods have been implemented or changed after it was created. Usually, this function should be called from within a class decorator.

Returns cls, to allow usage as a class decorator.

If cls is not an instance of ABCMeta, does nothing.

Note:
 This function assumes that cls’s superclasses are already updated. It does not update any subclasses.
 

New in version 3.10.


Footnotes

[1] C++ programmers should note that Python’s virtual base class concept is not the same as C++’s. 



## ==⚡ atexit — Exit handlers


The `atexit` module defines functions to register and unregister cleanup functions. Functions thus registered are automatically executed upon normal interpreter termination. `atexit` runs these functions in the reverse order in which they were registered; if you register A, B, and C, at interpreter termination time they will be run in the order C, B, A.

Note: The functions registered via this module are not called when the program is killed by a signal not handled by Python, when a Python fatal internal error is detected, or when `os._exit()` is called.

Changed in version 3.7: When used with C-API subinterpreters, registered functions are local to the interpreter they were registered in.


➡ `atexit.register(func, *args, **kwargs)`
Register `func` as a function to be executed at termination. Any optional arguments that are to be passed to `func` must be passed as arguments to `register()`. It is possible to register the same function and arguments more than once.

At normal program termination (for instance, if `sys.exit()` is called or the main module’s execution completes), all functions registered are called in last in, first out order. The assumption is that lower level modules will normally be imported before higher level modules and thus must be cleaned up later.

If an exception is raised during execution of the exit handlers, a traceback is printed (unless SystemExit is raised) and the exception information is saved. After all exit handlers have had a chance to run, the last exception to be raised is re-raised.

This function returns func, which makes it possible to use it as a decorator.

➡ `atexit.unregister(func)`
Remove func from the list of functions to be run at interpreter shutdown. unregister() silently does nothing if func was not previously registered. If func has been registered more than once, every occurrence of that function in the atexit call stack will be removed. Equality comparisons (==) are used internally during unregistration, so function references do not need to have matching identities.

See also:
 Module readlineUseful example of atexit to read and write readline history files.

### ===🗝 atexit Example

The following simple example demonstrates how a module can initialize a counter from a file when it is imported and save the counter’s updated value automatically when the program terminates without relying on the application making an explicit call into this module at termination.


```py
try:
    with open('counterfile') as infile:
        _count = int(infile.read())
except FileNotFoundError:
    _count = 0

def incrcounter(n):
    global _count
    _count = _count + n

def savecounter():
    with open('counterfile', 'w') as outfile:
        outfile.write('%d' % _count)

import atexit

atexit.register(savecounter)
```


Positional and keyword arguments may also be passed to register() to be passed along to the registered function when it is called:


```py
def goodbye(name, adjective):
    print('Goodbye %s, it was %s to meet you.' % (name, adjective))

import atexit


atexit.register(goodbye, 'Donny', 'nice')
# or:

atexit.register(goodbye, adjective='nice', name='Donny')
```


Usage as a decorator:


```py
import atexit

@atexit.register
def goodbye():
    print('You are now leaving the Python sector.')
```


This only works with functions that can be called without arguments.



## ==⚡ traceback — Print or retrieve a stack traceback
◦ TracebackException Objects
◦ StackSummary Objects
◦ FrameSummary Objects
◦ Traceback Examples

## ==⚡ __future__ — Future statement definitions

Source code: Lib/__future__.py

__future__ 模块为了在老版本的 Python 代码中兼顾新特性的一种方法。Python 2.1 开始，当一个新的语言特性首次出现在发行版中时，如果该新特性与旧版本不兼容，则该新特性默认会被禁用。要想启用这个新特性，必须使用 "`from __future__import xxx`" 进行导入。

比如，新除法特性，在 Python 3.0 之前的版本使用 `from __future__ import division` 引入使用 Python 3.0 中的除法。同样是 7/3，在 Python 2 除法中会截断小数部分，7/3 = 2，导入 division 特性后则会得到 2.333...

又以打印函数为例，使用 `from __future__ import print_function` 引用 Python 3 中的 print 函数。

引入新特性的导入语言必需在代码文件的开始一行。

参考 Python 3.10 源代码定义的 `_Feature` 对象，可以看到有 10 种前卫特性的引入版本 OptionalRelease 和强制实施版本 MandatoryRelease，其中 Python 3.7.0 引入的类型注解功能特性 Annotations，在 3.11.0 alpha 版本就正式采用：

```py
nested_scopes    = _Feature((2, 1, 0, "beta",  1),
                            (2, 2, 0, "alpha", 0), CO_NESTED)

generators       = _Feature((2, 2, 0, "alpha", 1),
                            (2, 3, 0, "final", 0), CO_GENERATOR_ALLOWED)

division         = _Feature((2, 2, 0, "alpha", 2),
                            (3, 0, 0, "alpha", 0), CO_FUTURE_DIVISION)

absolute_import  = _Feature((2, 5, 0, "alpha", 1),
                            (3, 0, 0, "alpha", 0), CO_FUTURE_ABSOLUTE_IMPORT)

with_statement   = _Feature((2, 5, 0, "alpha", 1),
                            (2, 6, 0, "alpha", 0), CO_FUTURE_WITH_STATEMENT)

print_function   = _Feature((2, 6, 0, "alpha", 2),
                            (3, 0, 0, "alpha", 0), CO_FUTURE_PRINT_FUNCTION)

unicode_literals = _Feature((2, 6, 0, "alpha", 2),
                            (3, 0, 0, "alpha", 0), CO_FUTURE_UNICODE_LITERALS)

barry_as_FLUFL   = _Feature((3, 1, 0, "alpha", 2),
                            (4, 0, 0, "alpha", 0), CO_FUTURE_BARRY_AS_BDFL)

generator_stop   = _Feature((3, 5, 0, "beta", 1),
                            (3, 7, 0, "alpha", 0), CO_FUTURE_GENERATOR_STOP)

annotations      = _Feature((3, 7, 0, "beta", 1),
                            (3, 11, 0, "alpha", 0), CO_FUTURE_ANNOTATIONS)
```

__future__ is a real module, and serves three purposes:

• To avoid confusing existing tools that analyze `import` statements and expect to find the modules they’re importing.

• To ensure that future statements run under releases prior to 2.1 at least yield runtime exceptions (the import of __future__ will fail, because there was no module of that name prior to 2.1).

• To document when incompatible changes were introduced, and when they will be — or were — made mandatory. This is a form of executable documentation, and can be inspected programmatically via importing __future__ and examining its contents.

Each statement in __future__.py is of the form:


```py
FeatureName = _Feature(OptionalRelease, MandatoryRelease,
                       CompilerFlag)
```


where, normally, `OptionalRelease` is less than `MandatoryRelease`, and both are 5-tuples of the same form as `sys.version_info`:


```py
(PY_MAJOR_VERSION, # the 2 in 2.1.0a3; an int
 PY_MINOR_VERSION, # the 1; an int
 PY_MICRO_VERSION, # the 0; an int
 PY_RELEASE_LEVEL, # "alpha", "beta", "candidate" or "final"; string
 PY_RELEASE_SERIAL # the 3; an int
)
```

`OptionalRelease` records the first release in which the feature was accepted.

In the case of a MandatoryRelease that has not yet occurred, MandatoryRelease predicts the release in which the feature will become part of the language.

Else MandatoryRelease records when the feature became part of the language; in releases at or after that, modules no longer need a future statement to use the feature in question, but may continue to use such imports.

`MandatoryRelease` may also be `None`, meaning that a planned feature got dropped.

Instances of class `_Feature` have two corresponding methods, `getOptionalRelease()` and `getMandatoryRelease()`.

`CompilerFlag` is the (bitfield) flag that should be passed in the fourth argument to the built-in function `compile()` to enable the feature in dynamically compiled code. This flag is stored in the `compiler_flag` attribute on `_Feature` instances.

No feature description will ever be deleted from __future__. Since its introduction in Python 2.1 the following features have found their way into the language using this mechanism:


|     feature      | optional | mandatory |                       effect                      |
|------------------|----------|-----------|---------------------------------------------------|
| nested_scopes    | 2.1.0b1  |   in  2.2 | PEP 227: Statically Nested Scopes                 |
| generators       | 2.2.0a1  |   in  2.3 | PEP 255: Simple Generators                        |
| division         | 2.2.0a2  |   in  3.0 | PEP 238: Changing the Division Operator           |
| absolute_import  | 2.5.0a1  |   in  3.0 | PEP 328: Imports: Multi-Line & Absolute/Relative  |
| with_statement   | 2.5.0a1  |   in  2.6 | PEP 343: The “with” Statement                     |
| print_function   | 2.6.0a2  |   in  3.0 | PEP 3105: Make print a function                   |
| unicode_literals | 2.6.0a2  |   in  3.0 | PEP 3112: Bytes literals in Python 3000           |
| generator_stop   | 3.5.0b1  |   in  3.7 | PEP 479: StopIteration handling inside generators |
| annotations      | 3.7.0b1  |   in 3.11 | PEP 563: Postponed evaluation of annotations      |

See also:
 Future statementsHow the compiler treats future imports.


## ==⚡ gc — Garbage Collector interface


This module provides an interface to the optional garbage collector. It provides the ability to disable the collector, tune the collection frequency, and set debugging options. It also provides access to unreachable objects that the collector found but cannot free. Since the collector supplements the reference counting already used in Python, you can disable the collector if you are sure your program does not create reference cycles. Automatic collection can be disabled by calling gc.disable(). To debug a leaking program call gc.set_debug(gc.DEBUG_LEAK). Notice that this includes gc.DEBUG_SAVEALL, causing garbage-collected objects to be saved in gc.garbage for inspection.

The gc module provides the following functions:

➡ `gc.enable()`
Enable automatic garbage collection.

➡ `gc.disable()`
Disable automatic garbage collection.

➡ `gc.isenabled()`
Return True if automatic collection is enabled.

➡ `gc.collect(generation=2)`
With no arguments, run a full collection. The optional argument generation may be an integer specifying which generation to collect (from 0 to 2). A ValueError is raised if the generation number is invalid. The number of unreachable objects found is returned.

The free lists maintained for a number of built-in types are cleared whenever a full collection or collection of the highest generation (2) is run. Not all items in some free lists may be freed due to the particular implementation, in particular float.

➡ `gc.set_debug(flags)`
Set the garbage collection debugging flags. Debugging information will be written to sys.stderr. See below for a list of debugging flags which can be combined using bit operations to control debugging.

➡ `gc.get_debug()`
Return the debugging flags currently set.

➡ `gc.get_objects(generation=None)`
Returns a list of all objects tracked by the collector, excluding the list returned. If generation is not None, return only the objects tracked by the collector that are in that generation.

Changed in version 3.8: New generation parameter.


Raises an auditing event gc.get_objects with argument generation.

➡ `gc.get_stats()`
Return a list of three per-generation dictionaries containing collection statistics since interpreter start. The number of keys may change in the future, but currently each dictionary will contain the following items:

• collections is the number of times this generation was collected;
• collected is the total number of objects collected inside this generation;
• uncollectable is the total number of objects which were found to be uncollectable (and were therefore moved to the garbage list) inside this generation.

New in version 3.4.


➡ `gc.set_threshold(threshold0[, threshold1[, threshold2]])`
Set the garbage collection thresholds (the collection frequency). Setting threshold0 to zero disables collection.

The GC classifies objects into three generations depending on how many collection sweeps they have survived. New objects are placed in the youngest generation (generation 0). If an object survives a collection it is moved into the next older generation. Since generation 2 is the oldest generation, objects in that generation remain there after a collection. In order to decide when to run, the collector keeps track of the number object allocations and deallocations since the last collection. When the number of allocations minus the number of deallocations exceeds threshold0, collection starts. Initially only generation 0 is examined. If generation 0 has been examined more than threshold1 times since generation 1 has been examined, then generation 1 is examined as well. With the third generation, things are a bit more complicated, see Collecting the oldest generation for more information.

➡ `gc.get_count()`
Return the current collection counts as a tuple of (count0, count1, count2).

➡ `gc.get_threshold()`
Return the current collection thresholds as a tuple of (threshold0, threshold1, threshold2).

➡ `gc.get_referrers(*objs)`
Return the list of objects that directly refer to any of objs. This function will only locate those containers which support garbage collection; extension types which do refer to other objects but do not support garbage collection will not be found.

Note that objects which have already been dereferenced, but which live in cycles and have not yet been collected by the garbage collector can be listed among the resulting referrers. To get only currently live objects, call collect() before calling get_referrers().

Warning:
 Care must be taken when using objects returned by get_referrers() because some of them could still be under construction and hence in a temporarily invalid state. Avoid using get_referrers() for any purpose other than debugging.
 

Raises an auditing event gc.get_referrers with argument objs.

➡ `gc.get_referents(*objs)`
Return a list of objects directly referred to by any of the arguments. The referents returned are those objects visited by the arguments’ C-level tp_traverse methods (if any), and may not be all objects actually directly reachable. tp_traverse methods are supported only by objects that support garbage collection, and are only required to visit objects that may be involved in a cycle. So, for example, if an integer is directly reachable from an argument, that integer object may or may not appear in the result list.

Raises an auditing event gc.get_referents with argument objs.

➡ `gc.is_tracked(obj)`
Returns `True` if the object is currently tracked by the garbage collector, `False` otherwise. As a general rule, instances of atomic types aren’t tracked and instances of non-atomic types (containers, user-defined objects…) are. However, some type-specific optimizations can be present in order to suppress the garbage collector footprint of simple instances (e.g. dicts containing only atomic keys and values):


>>> gc.is_tracked(0)
False
>>> gc.is_tracked("a")
False
>>> gc.is_tracked([])
True
>>> gc.is_tracked({})
False
>>> gc.is_tracked({"a": 1})
False
>>> gc.is_tracked({"a": []})
True


New in version 3.1.


➡ `gc.is_finalized(obj)`
Returns True if the given object has been finalized by the garbage collector, False otherwise.


>>> x = None
>>> class Lazarus:
...     def __del__(self):
...         global x
...         x = self
...
>>> lazarus = Lazarus()
>>> gc.is_finalized(lazarus)
False
>>> del lazarus
>>> gc.is_finalized(x)
True


New in version 3.9.


➡ `gc.freeze()`
Freeze all the objects tracked by gc - move them to a permanent generation and ignore all the future collections. This can be used before a POSIX fork() call to make the gc copy-on-write friendly or to speed up collection. Also collection before a POSIX fork() call may free pages for future allocation which can cause copy-on-write too so it’s advised to disable gc in parent process and freeze before fork and enable gc in child process.

New in version 3.7.


➡ `gc.unfreeze()`
Unfreeze the objects in the permanent generation, put them back into the oldest generation.

New in version 3.7.


➡ `gc.get_freeze_count()`
Return the number of objects in the permanent generation.

New in version 3.7.


The following variables are provided for read-only access (you can mutate the values but should not rebind them):

➡ `gc.garbage`
A list of objects which the collector found to be unreachable but could not be freed (uncollectable objects). Starting with Python 3.4, this list should be empty most of the time, except when using instances of C extension types with a non-NULL tp_del slot.

If DEBUG_SAVEALL is set, then all unreachable objects will be added to this list rather than freed.

Changed in version 3.2: If this list is non-empty at interpreter shutdown, a ResourceWarning is emitted, which is silent by default. If DEBUG_UNCOLLECTABLE is set, in addition all uncollectable objects are printed.


Changed in version 3.4: Following PEP 442, objects with a __del__() method don’t end up in gc.garbage anymore.


➡ `gc.callbacks`
A list of callbacks that will be invoked by the garbage collector before and after collection. The callbacks will be called with two arguments, phase and info.

phase can be one of two values:

- “start”: The garbage collection is about to start.
- “stop”: The garbage collection has finished.

info is a dict providing more information for the callback. The following keys are currently defined:

- “generation”: The oldest generation being collected.
- “collected”: When phase is “stop”, the number of objects successfully collected.
- “uncollectable”: When phase is “stop”, the number of objects that could not be collected and were put in garbage.

Applications can add their own callbacks to this list. The primary use cases are:



Gathering statistics about garbage collection, such as how often various generations are collected, and how long the collection takes.

Allowing applications to identify and clear their own uncollectable types when they appear in garbage.

New in version 3.3.


The following constants are provided for use with set_debug():

➡ `gc.DEBUG_STATS`
Print statistics during collection. This information can be useful when tuning the collection frequency.

➡ `gc.DEBUG_COLLECTABLE`
Print information on collectable objects found.

➡ `gc.DEBUG_UNCOLLECTABLE`
Print information of uncollectable objects found (objects which are not reachable but cannot be freed by the collector). These objects will be added to the garbage list.

Changed in version 3.2: Also print the contents of the garbage list at interpreter shutdown, if it isn’t empty.


➡ `gc.DEBUG_SAVEALL`
When set, all unreachable objects found will be appended to garbage rather than being freed. This can be useful for debugging a leaking program.

➡ `gc.DEBUG_LEAK`
The debugging flags necessary for the collector to print information about a leaking program (equal to DEBUG_COLLECTABLE | DEBUG_UNCOLLECTABLE | DEBUG_SAVEALL).



## ==⚡ inspect — Inspect live objects
◦ Types and members
◦ Retrieving source code
◦ Introspecting callables with the Signature object
◦ Classes and functions
◦ The interpreter stack
◦ Fetching attributes statically
◦ Current State of Generators and Coroutines
◦ Code Objects Bit Flags
◦ Command Line Interface


Source code: Lib/inspect.py


The `inspect` module provides several useful functions to help get information about live objects such as modules, classes, methods, functions, tracebacks, frame objects, and code objects. For example, it can help you examine the contents of a class, retrieve the source code of a method, extract and format the argument list for a function, or get all the information you need to display a detailed traceback.

There are four main kinds of services provided by this module: type checking, getting source code, inspecting classes and functions, and examining the interpreter stack.


### ===🗝 Types and members

The `getmembers()` function retrieves the members of an object such as a class or module. The functions whose names begin with “is” are mainly provided as convenient choices for the second argument to `getmembers()`. They also help you determine when you can expect to find the following special attributes:


| Type | Attribute| Description

module 
    __doc__ documentation string 
    __file__ filename (missing for built-in modules) 
class 
    __doc__ documentation string 
    __name__ name with which this class was defined 
    __qualname__ qualified name 
    __module__ name of module in which this class was defined 
method 
    __doc__ documentation string 
    __name__ name with which this method was defined 
    __qualname__ qualified name 
    __func__ function object containing implementation of method 
    __self__ instance to which this method is bound, or None 
    __module__ name of module in which this method was defined 
function 
    __doc__ documentation string 
    __name__ name with which this function was defined 
    __qualname__ qualified name 
    __code__ code object containing compiled function bytecode 
    __defaults__ tuple of any default values for positional or keyword parameters 
    __kwdefaults__ mapping of any default values for keyword-only parameters 
    __globals__ global namespace in which this function was defined 
    __builtins__ builtins namespace 
    __annotations__ mapping of parameters names to annotations; "return" key is reserved for return annotations. 
    __module__ name of module in which this function was defined 
traceback 
    `tb_frame` frame object at this level 
    `tb_lasti` index of last attempted instruction in bytecode 
    `tb_lineno` current line number in Python source code 
    `tb_next` next inner traceback object (called by this level) 
frame 
    `f_back` next outer frame object (this frame’s caller) 
    `f_builtins` builtins namespace seen by this frame 
    `f_code` code object being executed in this frame 
    `f_globals` global namespace seen by this frame 
    `f_lasti` index of last attempted instruction in bytecode 
    `f_lineno` current line number in Python source code 
    `f_locals` local namespace seen by this frame 
    `f_trace` tracing function for this frame, or None 
code 
    `co_argcount` number of arguments (not including keyword only arguments, * or ** args) 
    `co_code` string of raw compiled bytecode 
    `co_cellvars` tuple of names of cell variables (referenced by containing scopes) 
    `co_consts` tuple of constants used in the bytecode 
    `co_filename` name of file in which this code object was created 
    `co_firstlineno` number of first line in Python source code 
    `co_flags` bitmap of CO_* flags, read more here 
    `co_lnotab` encoded mapping of line numbers to bytecode indices 
    `co_freevars` tuple of names of free variables (referenced via a function’s closure) 
    `co_posonlyargcount` number of positional only arguments 
    `co_kwonlyargcount` number of keyword only arguments (not including ** arg) 
    `co_name` name with which this code object was defined 
    `co_names` tuple of names of local variables 
    `co_nlocals` number of local variables 
    `co_stacksize` virtual machine stack space required 
    `co_varnames` tuple of names of arguments and local variables 
generator 
    `__name__` name 
    `__qualname__` qualified name 
    `gi_frame` frame 
    `gi_running` is the generator running? 
    `gi_code` code 
    `gi_yieldfrom` object being iterated by yield from, or None 
coroutine 
    `__name__` name 
    `__qualname__` qualified name 
    `cr_await` object being awaited on, or None 
    `cr_frame` frame 
    `cr_running` is the coroutine running? 
    `cr_code` code 
    `cr_origin` where coroutine was created, or None. See sys.set_coroutine_origin_tracking_depth() 
builtin 
    `__doc__` documentation string 
    `__name__` original name of this function or method 
    `__qualname__` qualified name 
    `__self__` instance to which a method is bound, or None 

Changed in version 3.5: Add __qualname__ and gi_yieldfrom attributes to generators.


The __name__ attribute of generators is now set from the function name, instead of the code name, and it can now be modified.

Changed in version 3.7: Add cr_origin attribute to coroutines.


Changed in version 3.10: Add __builtins__ attribute to functions.


➡ `inspect.getmembers(object[, predicate])`
Return all the members of an object in a list of (name, value) pairs sorted by name. If the optional predicate argument—which will be called with the value object of each member—is supplied, only members for which the predicate returns a true value are included.

Note:
 getmembers() will only return class attributes defined in the metaclass when the argument is a class and those attributes have been listed in the metaclass’ custom __dir__().
 

➡ `inspect.getmodulename(path)`
Return the name of the module named by the file path, without including the names of enclosing packages. The file extension is checked against all of the entries in importlib.machinery.all_suffixes(). If it matches, the final path component is returned with the extension removed. Otherwise, None is returned.

Note that this function only returns a meaningful name for actual Python modules - paths that potentially refer to Python packages will still return None.

Changed in version 3.3: The function is based directly on importlib.


➡ `inspect.ismodule(object)`
Return True if the object is a module.

➡ `inspect.isclass(object)`
Return True if the object is a class, whether built-in or created in Python code.

➡ `inspect.ismethod(object)`
Return True if the object is a bound method written in Python.

➡ `inspect.isfunction(object)`
Return True if the object is a Python function, which includes functions created by a lambda expression.

➡ `inspect.isgeneratorfunction(object)`
Return True if the object is a Python generator function.

Changed in version 3.8: Functions wrapped in functools.partial() now return True if the wrapped function is a Python generator function.


➡ `inspect.isgenerator(object)`
Return True if the object is a generator.

➡ `inspect.iscoroutinefunction(object)`
Return True if the object is a coroutine function (a function defined with an async def syntax).

New in version 3.5.


Changed in version 3.8: Functions wrapped in functools.partial() now return True if the wrapped function is a coroutine function.


➡ `inspect.iscoroutine(object)`
Return True if the object is a coroutine created by an async def function.

New in version 3.5.


➡ `inspect.isawaitable(object)`
Return True if the object can be used in await expression.

Can also be used to distinguish generator-based coroutines from regular generators:


def gen():
    yield
@types.coroutine
def gen_coro():
    yield

assert not isawaitable(gen())
assert isawaitable(gen_coro())


New in version 3.5.


➡ `inspect.isasyncgenfunction(object)`
Return True if the object is an asynchronous generator function, for example:


>>> async def agen():
...     yield 1
...
>>> inspect.isasyncgenfunction(agen)
True


New in version 3.6.


Changed in version 3.8: Functions wrapped in functools.partial() now return True if the wrapped function is a asynchronous generator function.


➡ `inspect.isasyncgen(object)`
Return True if the object is an asynchronous generator iterator created by an asynchronous generator function.

New in version 3.6.


➡ `inspect.istraceback(object)`
Return True if the object is a traceback.

➡ `inspect.isframe(object)`
Return True if the object is a frame.

➡ `inspect.iscode(object)`
Return True if the object is a code.

➡ `inspect.isbuiltin(object)`
Return True if the object is a built-in function or a bound built-in method.

➡ `inspect.isroutine(object)`
Return True if the object is a user-defined or built-in function or method.

➡ `inspect.isabstract(object)`
Return True if the object is an abstract base class.

➡ `inspect.ismethoddescriptor(object)`
Return True if the object is a method descriptor, but not if ismethod(), isclass(), isfunction() or isbuiltin() are true.

This, for example, is true of int.__add__. An object passing this test has a __get__() method but not a __set__() method, but beyond that the set of attributes varies. A __name__ attribute is usually sensible, and __doc__ often is.

Methods implemented via descriptors that also pass one of the other tests return False from the ismethoddescriptor() test, simply because the other tests promise more – you can, e.g., count on having the __func__ attribute (etc) when an object passes ismethod().

➡ `inspect.isdatadescriptor(object)`
Return True if the object is a data descriptor.

Data descriptors have a __set__ or a __delete__ method. Examples are properties (defined in Python), getsets, and members. The latter two are defined in C and there are more specific tests available for those types, which is robust across Python implementations. Typically, data descriptors will also have __name__ and __doc__ attributes (properties, getsets, and members have both of these attributes), but this is not guaranteed.

➡ `inspect.isgetsetdescriptor(object)`
Return True if the object is a getset descriptor.


CPython implementation detail: getsets are attributes defined in extension modules via PyGetSetDef structures. For Python implementations without such types, this method will always return False.

➡ `inspect.ismemberdescriptor(object)`
Return True if the object is a member descriptor.


CPython implementation detail: Member descriptors are attributes defined in extension modules via PyMemberDef structures. For Python implementations without such types, this method will always return False.


### ===🗝 Retrieving source code

➡ `inspect.getdoc(object)`
Get the documentation string for an object, cleaned up with cleandoc(). If the documentation string for an object is not provided and the object is a class, a method, a property or a descriptor, retrieve the documentation string from the inheritance hierarchy.

Changed in version 3.5: Documentation strings are now inherited if not overridden.


➡ `inspect.getcomments(object)`
Return in a single string any lines of comments immediately preceding the object’s source code (for a class, function, or method), or at the top of the Python source file (if the object is a module). If the object’s source code is unavailable, return None. This could happen if the object has been defined in C or the interactive shell.

➡ `inspect.getfile(object)`
Return the name of the (text or binary) file in which an object was defined. This will fail with a TypeError if the object is a built-in module, class, or function.

➡ `inspect.getmodule(object)`
Try to guess which module an object was defined in.

➡ `inspect.getsourcefile(object)`
Return the name of the Python source file in which an object was defined. This will fail with a TypeError if the object is a built-in module, class, or function.

➡ `inspect.getsourcelines(object)`
Return a list of source lines and starting line number for an object. The argument may be a module, class, method, function, traceback, frame, or code object. The source code is returned as a list of the lines corresponding to the object and the line number indicates where in the original source file the first line of code was found. An OSError is raised if the source code cannot be retrieved.

Changed in version 3.3: OSError is raised instead of IOError, now an alias of the former.


➡ `inspect.getsource(object)`
Return the text of the source code for an object. The argument may be a module, class, method, function, traceback, frame, or code object. The source code is returned as a single string. An OSError is raised if the source code cannot be retrieved.

Changed in version 3.3: OSError is raised instead of IOError, now an alias of the former.


➡ `inspect.cleandoc(doc)`
Clean up indentation from docstrings that are indented to line up with blocks of code.

All leading whitespace is removed from the first line. Any leading whitespace that can be uniformly removed from the second line onwards is removed. Empty lines at the beginning and end are subsequently removed. Also, all tabs are expanded to spaces.


### ===🗝 Introspecting callables with the Signature object

New in version 3.3.


The Signature object represents the call signature of a callable object and its return annotation. To retrieve a Signature object, use the signature() function.

➡ `inspect.signature(callable, *, follow_wrapped=True, globals=None, locals=None, eval_str=False)`
Return a Signature object for the given callable:


>>> from inspect import signature
>>> def foo(a, *, b:int, **kwargs):
...     pass

>>> sig = signature(foo)

>>> str(sig)
'(a, *, b:int, **kwargs)'

>>> str(sig.parameters['b'])
'b:int'

>>> sig.parameters['b'].annotation
<class 'int'>


Accepts a wide range of Python callables, from plain functions and classes to *functools.partial()* objects.

For objects defined in modules using stringized annotations (from __future__ import annotations), signature() will attempt to automatically un-stringize the annotations using `inspect.get_annotations()`. The global, locals, and eval_str parameters are passed into `inspect.get_annotations()` when resolving the annotations; see the documentation for `inspect.get_annotations()` for instructions on how to use these parameters.

Raises ValueError if no signature can be provided, and TypeError if that type of object is not supported. Also, if the annotations are stringized, and eval_str is not false, the eval() call(s) to un-stringize the annotations could potentially raise any kind of exception.

A slash(/) in the signature of a function denotes that the parameters prior to it are positional-only. For more info, see the FAQ entry on positional-only parameters.

New in version 3.5: follow_wrapped parameter. Pass False to get a signature of callable specifically (callable.__wrapped__ will not be used to unwrap decorated callables.)


New in version 3.10: globals, locals, and eval_str parameters.


Note:
 Some callables may not be introspectable in certain implementations of Python. For example, in CPython, some built-in functions defined in C provide no metadata about their arguments.
 
✅ `class inspect.Signature(parameters=None, *, return_annotation=Signature.empty)`
A Signature object represents the call signature of a function and its return annotation. For each parameter accepted by the function it stores a Parameter object in its parameters collection.

The optional parameters *argument* is a sequence of Parameter objects, which is validated to check that there are no parameters with duplicate names, and that the parameters are in the right order, i.e. positional-only first, then positional-or-keyword, and that parameters with defaults follow parameters without defaults.

The optional *return_annotation* argument, can be an arbitrary Python object, is the “return” annotation of the callable.

Signature objects are immutable. Use Signature.replace() to make a modified copy.

Changed in version 3.5: Signature objects are picklable and hashable.

➡ **empty**
A special class-level marker to specify absence of a return annotation.
parameters
An ordered mapping of parameters’ names to the corresponding Parameter objects. Parameters appear in strict definition order, including keyword-only parameters.

Changed in version 3.7: Python only explicitly guaranteed that it preserved the declaration order of keyword-only parameters as of version 3.7, although in practice this order had always been preserved in Python 3.

➡ `return_annotation`
The “return” annotation for the callable. If the callable has no “return” annotation, this attribute is set to Signature.empty.

➡ `bind(*args, **kwargs)`
Create a mapping from positional and keyword arguments to parameters. Returns BoundArguments if `*args` and `**kwargs` match the signature, or raises a TypeError.

➡ `bind_partial(*args, **kwargs)`
Works the same way as Signature.bind(), but allows the omission of some required arguments (mimics functools.partial() behavior.) Returns BoundArguments, or raises a TypeError if the passed arguments do not match the signature.

➡ `replace(*[, parameters][, return_annotation])`
Create a new Signature instance based on the instance replace was invoked on. It is possible to pass different parameters and/or return_annotation to override the corresponding properties of the base signature. To remove return_annotation from the copied Signature, pass in Signature.empty.


>>> def test(a, b):
...     pass
>>> sig = signature(test)
>>> new_sig = sig.replace(return_annotation="new return anno")
>>> str(new_sig)
"(a, b) -> 'new return anno'"

➡ `classmethod from_callable(obj, *, follow_wrapped=True, globalns=None, localns=None)`
Return a Signature (or its subclass) object for a given callable obj. Pass follow_wrapped=False to get a signature of obj without unwrapping its __wrapped__ chain. globalns and localns will be used as the namespaces when resolving annotations.

This method simplifies subclassing of Signature:


```py
class MySignature(Signature):
    pass
sig = MySignature.from_callable(min)
assert isinstance(sig, MySignature)
```


New in version 3.5.


New in version 3.10: globalns and localns parameters.

✅ `class inspect.Parameter(name, kind, *, default=Parameter.empty, annotation=Parameter.empty)`
Parameter objects are immutable. Instead of modifying a Parameter object, you can use Parameter.replace() to create a modified copy.

Changed in version 3.5: Parameter objects are picklable and hashable.

➡ **empty**
A special class-level marker to specify absence of default values and annotations.
➡ **name**
The name of the parameter as a string. The name must be a valid Python identifier.


CPython implementation detail: CPython generates implicit parameter names of the form .0 on the code objects used to implement comprehensions and generator expressions.

Changed in version 3.6: These parameter names are exposed by this module as names like implicit0.

➡ **default**
The default value for the parameter. If the parameter has no default value, this attribute is set to Parameter.empty.
➡ **annotation**
The annotation for the parameter. If the parameter has no annotation, this attribute is set to Parameter.empty.
➡ **kind**
Describes how argument values are bound to the parameter. Possible values (accessible via Parameter, like Parameter.KEYWORD_ONLY):

|Name| Meaning

↪ POSITIONAL_ONLY Value must be supplied as a positional argument. Positional only parameters are those which appear before a / entry (if present) in a Python function definition. 

↪ POSITIONAL_OR_KEYWORD Value may be supplied as either a keyword or positional argument (this is the standard binding behaviour for functions implemented in Python.) 
VAR_POSITIONAL A tuple of positional arguments that aren’t bound to any other parameter. This corresponds to a `*args` parameter in a Python function definition. 

↪ KEYWORD_ONLY Value must be supplied as a keyword argument. Keyword only parameters are those which appear after a * or `*args` entry in a Python function definition. 

↪ VAR_KEYWORD A dict of keyword arguments that aren’t bound to any other parameter. This corresponds to a `**kwargs` parameter in a Python function definition. 

Example: print all keyword-only arguments without default values:


>>> def foo(a, b, *, c, d=10):
...     pass

>>> sig = signature(foo)
>>> for param in sig.parameters.values():
...     if (param.kind == param.KEYWORD_ONLY and
...                        param.default is param.empty):
...         print('Parameter:', param)
Parameter: c

➡ `kind.description`
Describes a enum value of Parameter.kind.

New in version 3.8.


Example: print all descriptions of arguments:


>>> def foo(a, b, *, c, d=10):
...     pass

>>> sig = signature(foo)
>>> for param in sig.parameters.values():
...     print(param.kind.description)
positional or keyword
positional or keyword
keyword-only
keyword-only

➡ `replace(*[, name][, kind][, default][, annotation])`
Create a new Parameter instance based on the instance replaced was invoked on. To override a Parameter attribute, pass the corresponding argument. To remove a default value or/and an annotation from a Parameter, pass Parameter.empty.


>>> from inspect import Parameter
>>> param = Parameter('foo', Parameter.KEYWORD_ONLY, default=42)
>>> str(param)
'foo=42'

>>> str(param.replace()) # Will create a shallow copy of 'param'
'foo=42'

>>> str(param.replace(default=Parameter.empty, annotation='spam'))
"foo:'spam'"


Changed in version 3.4: In Python 3.3 Parameter objects were allowed to have name set to None if their kind was set to POSITIONAL_ONLY. This is no longer permitted.

✅ `class inspect.BoundArguments`
Result of a Signature.bind() or Signature.bind_partial() call. Holds the mapping of arguments to the function’s parameters.

➡ `arguments`
A mutable mapping of parameters’ names to arguments’ values. Contains only explicitly bound arguments. Changes in arguments will reflect in args and kwargs.

Should be used in conjunction with Signature.parameters for any argument processing purposes.

Note:
 Arguments for which Signature.bind() or Signature.bind_partial() relied on a default value are skipped. However, if needed, use BoundArguments.apply_defaults() to add them.
 

Changed in version 3.9: arguments is now of type dict. Formerly, it was of type collections.OrderedDict.


➡ `args`
A tuple of positional arguments values. Dynamically computed from the arguments attribute.

➡ `kwargs`
A dict of keyword arguments values. Dynamically computed from the arguments attribute.

➡ `signature`
A reference to the parent Signature object.

➡ `apply_defaults()`
Set default values for missing arguments.

For variable-positional arguments (`*args`) the default is an empty tuple.

For variable-keyword arguments (`**kwargs`) the default is an empty dict.


>>> def foo(a, b='ham', *args): pass
>>> ba = inspect.signature(foo).bind('spam')
>>> ba.apply_defaults()
>>> ba.arguments
{'a': 'spam', 'b': 'ham', 'args': ()}


New in version 3.5.


The args and kwargs properties can be used to invoke functions:


```py
def test(a, *, b):
    ...

sig = signature(test)
ba = sig.bind(10, b=20)
test(*ba.args, **ba.kwargs)
```


See also:
 PEP 362 - Function Signature Object.
 The detailed specification, implementation details and examples.

### ===🗝 Classes and functions

➡ `inspect.getclasstree(classes, unique=False)`
Arrange the given list of classes into a hierarchy of nested lists. Where a nested list appears, it contains classes derived from the class whose entry immediately precedes the list. Each entry is a 2-tuple containing a class and a tuple of its base classes. If the unique argument is true, exactly one entry appears in the returned structure for each class in the given list. Otherwise, classes using multiple inheritance and their descendants will appear multiple times.

➡ `inspect.getargspec(func)`
Get the names and default values of a Python function’s parameters. A named tuple ArgSpec(args, varargs, keywords, defaults) is returned. args is a list of the parameter names. varargs and keywords are the names of the * and ** parameters or None. defaults is a tuple of default argument values or None if there are no default arguments; if this tuple has n elements, they correspond to the last n elements listed in args.


Deprecated since version 3.0: Use getfullargspec() for an updated API that is usually a drop-in replacement, but also correctly handles function annotations and keyword-only parameters.

Alternatively, use signature() and Signature Object, which provide a more structured introspection API for callables.

➡ `inspect.getfullargspec(func)`
Get the names and default values of a Python function’s parameters. A named tuple is returned:

➡ `FullArgSpec(args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations)`

*args* is a list of the positional parameter names.
*varargs* is the name of the * parameter or None if arbitrary positional arguments are not accepted.
*varkw* is the name of the ** parameter or None if arbitrary keyword arguments are not accepted.
*defaults* is an n-tuple of default argument values corresponding to the last n positional parameters, or None if there are no such defaults defined.
*kwonlyargs* is a list of keyword-only parameter names in declaration order.
*kwonlydefaults* is a dictionary mapping parameter names from kwonlyargs to the default *values* used if no argument is supplied.
*annotations* is a dictionary mapping parameter names to annotations. The special key "return" is used to report the function return value annotation (if any).

Note that signature() and Signature Object provide the recommended API for callable introspection, and support additional behaviours (like positional-only arguments) that are sometimes encountered in extension module APIs. This function is retained primarily for use in code that needs to maintain compatibility with the Python 2 inspect module API.

Changed in version 3.4: This function is now based on signature(), but still ignores __wrapped__ attributes and includes the already bound first parameter in the signature output for bound methods.


Changed in version 3.6: This method was previously documented as deprecated in favour of signature() in Python 3.5, but that decision has been reversed in order to restore a clearly supported standard interface for single-source Python 2/3 code migrating away from the legacy getargspec() API.


Changed in version 3.7: Python only explicitly guaranteed that it preserved the declaration order of keyword-only parameters as of version 3.7, although in practice this order had always been preserved in Python 3.


➡ `inspect.getargvalues(frame)`
Get information about arguments passed into a particular frame. A named tuple ArgInfo(args, varargs, keywords, locals) is returned. args is a list of the argument names. varargs and keywords are the names of the * and ** arguments or None. locals is the locals dictionary of the given frame.

Note:
 This function was inadvertently marked as deprecated in Python 3.5.
 

➡ `inspect.formatargspec(args[, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations[, formatarg, formatvarargs, formatvarkw, formatvalue, formatreturns, formatannotations]])`
Format a pretty argument spec from the values returned by getfullargspec().

The first seven arguments are (args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations).

The other six arguments are functions that are called to turn argument names, * argument name, ** argument name, default values, return annotation and individual annotations into strings, respectively.

For example:


>>> from inspect import formatargspec, getfullargspec
>>> def f(a: int, b: float):
...     pass
...
>>> formatargspec(*getfullargspec(f))
'(a: int, b: float)'



Deprecated since version 3.5: Use signature() and Signature Object, which provide a better introspecting API for callables.

➡ `inspect.formatargvalues(args[, varargs, varkw, locals, formatarg, formatvarargs, formatvarkw, formatvalue])`
Format a pretty argument spec from the four values returned by getargvalues(). The format* arguments are the corresponding optional formatting functions that are called to turn names and values into strings.

Note:
 This function was inadvertently marked as deprecated in Python 3.5.
 

➡ `inspect.getmro(cls)`
Return a tuple of class cls’s base classes, including cls, in method resolution order. No class appears more than once in this tuple. Note that the method resolution order depends on cls’s type. Unless a very peculiar user-defined metatype is in use, cls will be the first element of the tuple.

➡ `inspect.getcallargs(func, /, *args, **kwds)`
Bind the args and kwds to the argument names of the Python function or method func, as if it was called with them. For bound methods, bind also the first argument (typically named self) to the associated instance. A dict is returned, mapping the argument names (including the names of the * and ** arguments, if any) to their values from args and kwds. In case of invoking func incorrectly, i.e. whenever `func(*args, **kwds)` would raise an exception because of incompatible signature, an exception of the same type and the same or similar message is raised. For example:


>>> from inspect import getcallargs
>>> def f(a, b=1, *pos, **named):
...     pass
>>> getcallargs(f, 1, 2, 3) == {'a': 1, 'named': {}, 'b': 2, 'pos': (3,)}
True
>>> getcallargs(f, a=2, x=4) == {'a': 2, 'named': {'x': 4}, 'b': 1, 'pos': ()}
True
>>> getcallargs(f)
Traceback (most recent call last):
...
TypeError: f() missing 1 required positional argument: 'a'


New in version 3.2.



Deprecated since version 3.5: Use Signature.bind() and Signature.bind_partial() instead.

➡ `inspect.getclosurevars(func)`
Get the mapping of external name references in a Python function or method func to their current values. A named tuple ClosureVars(nonlocals, globals, builtins, unbound) is returned. nonlocals maps referenced names to lexical closure variables, globals to the function’s module globals and builtins to the builtins visible from the function body. unbound is the set of names referenced in the function that could not be resolved at all given the current module globals and builtins.

TypeError is raised if func is not a Python function or method.

New in version 3.3.


➡ `inspect.unwrap(func, *, stop=None)`
Get the object wrapped by func. It follows the chain of __wrapped__ attributes returning the last object in the chain.

stop is an optional callback accepting an object in the wrapper chain as its sole argument that allows the unwrapping to be terminated early if the callback returns a true value. If the callback never returns a true value, the last object in the chain is returned as usual. For example, signature() uses this to stop unwrapping if any object in the chain has a __signature__ attribute defined.

ValueError is raised if a cycle is encountered.

New in version 3.4.


➡ `inspect.get_annotations(obj, *, globals=None, locals=None, eval_str=False)`
Compute the annotations dict for an object.

obj may be a callable, class, or module. Passing in an object of any other type raises TypeError.

Returns a dict. get_annotations() returns a new dict every time it’s called; calling it twice on the same object will return two different but equivalent dicts.

This function handles several details for you:


• If *eval_str* is true, values of type str will be un-stringized using eval(). This is intended for use with stringized annotations (from __future__ import annotations).

• If *obj* doesn’t have an annotations dict, returns an empty dict. (Functions and methods always have an annotations dict; classes, modules, and other types of callables may not.)

• Ignores inherited annotations on classes. If a class doesn’t have its own annotations dict, returns an empty dict.

• All accesses to object members and dict values are done using getattr() and dict.get() for safety.

• Always, always, always returns a freshly-created dict.

*eval_str* controls whether or not values of type str are replaced with the result of calling eval() on those values:

• If *eval_str* is true, eval() is called on values of type str. (Note that get_annotations doesn’t catch exceptions; if eval() raises an exception, it will unwind the stack past the get_annotations call.)

• If *eval_str* is false (the default), values of type str are unchanged.

globals and locals are passed in to eval(); see the documentation for eval() for more information. If globals or locals is None, this function may replace that value with a context-specific default, contingent on type(obj):

• If *obj* is a module, globals defaults to obj.__dict__.

• If *obj* is a class, globals defaults to sys.modules[obj.__module__].__dict__ and locals defaults to the obj class namespace.

• If *obj* is a callable, globals defaults to obj.__globals__, although if obj is a wrapped function (using functools.update_wrapper()) it is first unwrapped.

Calling get_annotations is best practice for accessing the annotations dict of any object. See Annotations Best Practices for more information on annotations best practices.

New in version 3.10.



### ===🗝 The interpreter stack

When the following functions return “frame records,” each record is a named tuple FrameInfo(frame, filename, lineno, function, code_context, index). The tuple contains the frame object, the filename, the line number of the current line, the function name, a list of lines of context from the source code, and the index of the current line within that list.

Changed in version 3.5: Return a named tuple instead of a tuple.


Note:
 Keeping references to frame objects, as found in the first element of the frame records these functions return, can cause your program to create reference cycles. Once a reference cycle has been created, the lifespan of all objects which can be accessed from the objects which form the cycle can become much longer even if Python’s optional cycle detector is enabled. If such cycles must be created, it is important to ensure they are explicitly broken to avoid the delayed destruction of objects and increased memory consumption which occurs.
 
Though the cycle detector will catch these, destruction of the frames (and local variables) can be made deterministic by removing the cycle in a finally clause. This is also important if the cycle detector was disabled when Python was compiled or using gc.disable(). For example:


```py
def handle_stackframe_without_leak():
    frame = inspect.currentframe()
    try:
        # do something with the frame
    finally:
        del frame
```


If you want to keep the frame around (for example to print a traceback later), you can also break reference cycles by using the frame.clear() method.

The optional context argument supported by most of these functions specifies the number of lines of context to return, which are centered around the current line.

➡ `inspect.getframeinfo(frame, context=1)`
Get information about a frame or traceback object. A named tuple Traceback(filename, lineno, function, code_context, index) is returned.

➡ `inspect.getouterframes(frame, context=1)`
Get a list of frame records for a frame and all outer frames. These frames represent the calls that lead to the creation of frame. The first entry in the returned list represents frame; the last entry represents the outermost call on frame’s stack.

Changed in version 3.5: A list of named tuples FrameInfo(frame, filename, lineno, function, code_context, index) is returned.


➡ `inspect.getinnerframes(traceback, context=1)`
Get a list of frame records for a traceback’s frame and all inner frames. These frames represent calls made as a consequence of frame. The first entry in the list represents traceback; the last entry represents where the exception was raised.

Changed in version 3.5: A list of named tuples FrameInfo(frame, filename, lineno, function, code_context, index) is returned.


➡ `inspect.currentframe()`
Return the frame object for the caller’s stack frame.


CPython implementation detail: This function relies on Python stack frame support in the interpreter, which isn’t guaranteed to exist in all implementations of Python. If running in an implementation without Python stack frame support this function returns None.

➡ `inspect.stack(context=1)`
Return a list of frame records for the caller’s stack. The first entry in the returned list represents the caller; the last entry represents the outermost call on the stack.

Changed in version 3.5: A list of named tuples FrameInfo(frame, filename, lineno, function, code_context, index) is returned.


➡ `inspect.trace(context=1)`
Return a list of frame records for the stack between the current frame and the frame in which an exception currently being handled was raised in. The first entry in the list represents the caller; the last entry represents where the exception was raised.

Changed in version 3.5: A list of named tuples FrameInfo(frame, filename, lineno, function, code_context, index) is returned.



### ===🗝 Fetching attributes statically

Both getattr() and hasattr() can trigger code execution when fetching or checking for the existence of attributes. Descriptors, like properties, will be invoked and __getattr__() and __getattribute__() may be called.

For cases where you want passive introspection, like documentation tools, this can be inconvenient. getattr_static() has the same signature as getattr() but avoids executing code when it fetches attributes.

➡ `inspect.getattr_static(obj, attr, default=None)`
Retrieve attributes without triggering dynamic lookup via the descriptor protocol, __getattr__() or __getattribute__().

Note: this function may not be able to retrieve all attributes that getattr can fetch (like dynamically created attributes) and may find attributes that getattr can’t (like descriptors that raise AttributeError). It can also return descriptors objects instead of instance members.

If the instance __dict__ is shadowed by another member (for example a property) then this function will be unable to find instance members.

New in version 3.2.


`getattr_static()` does not resolve descriptors, for example slot descriptors or getset descriptors on objects implemented in C. The descriptor object is returned instead of the underlying attribute.

You can handle these with code like the following. Note that for arbitrary getset descriptors invoking these may trigger code execution:


```py
# example code for resolving the builtin descriptor types
class _foo:
    __slots__ = ['foo']

slot_descriptor = type(_foo.foo)
getset_descriptor = type(type(open(__file__)).name)
wrapper_descriptor = type(str.__dict__['__add__'])
descriptor_types = (slot_descriptor, getset_descriptor, wrapper_descriptor)

result = getattr_static(some_object, 'foo')
if type(result) in descriptor_types:
    try:
        result = result.__get__()
    except AttributeError:
        # descriptors can raise AttributeError to
        # indicate there is no underlying value
        # in which case the descriptor itself will
        # have to do
        pass

```


### ===🗝 Current State of Generators and Coroutines

When implementing coroutine schedulers and for other advanced uses of generators, it is useful to determine whether a generator is currently executing, is waiting to start or resume or execution, or has already terminated. getgeneratorstate() allows the current state of a generator to be determined easily.

➡ `inspect.getgeneratorstate(generator)`
Get current state of a generator-iterator.

Possible states are:

• GEN_CREATED: Waiting to start execution.

• GEN_RUNNING: Currently being executed by the interpreter.

• GEN_SUSPENDED: Currently suspended at a yield expression.

• GEN_CLOSED: Execution has completed.

New in version 3.2.


➡ `inspect.getcoroutinestate(coroutine)`
Get current state of a coroutine object. The function is intended to be used with coroutine objects created by async def functions, but will accept any coroutine-like object that has cr_running and cr_frame attributes.

Possible states are:

• CORO_CREATED: Waiting to start execution.

• CORO_RUNNING: Currently being executed by the interpreter.

• CORO_SUSPENDED: Currently suspended at an await expression.

• CORO_CLOSED: Execution has completed.

New in version 3.5.


The current internal state of the generator can also be queried. This is mostly useful for testing purposes, to ensure that internal state is being updated as expected:

➡ `inspect.getgeneratorlocals(generator)`
Get the mapping of live local variables in generator to their current values. A dictionary is returned that maps from variable names to values. This is the equivalent of calling locals() in the body of the generator, and all the same caveats apply.

If generator is a generator with no currently associated frame, then an empty dictionary is returned. TypeError is raised if generator is not a Python generator object.


CPython implementation detail: This function relies on the generator exposing a Python stack frame for introspection, which isn’t guaranteed to be the case in all implementations of Python. In such cases, this function will always return an empty dictionary.

New in version 3.3.


➡ `inspect.getcoroutinelocals(coroutine)`
This function is analogous to getgeneratorlocals(), but works for coroutine objects created by async def functions.

New in version 3.5.



### ===🗝 Code Objects Bit Flags

Python code objects have a co_flags attribute, which is a bitmap of the following flags:

➡ `inspect.CO_OPTIMIZED`
The code object is optimized, using fast locals.

➡ `inspect.CO_NEWLOCALS`
If set, a new dict will be created for the frame’s f_locals when the code object is executed.

➡ `inspect.CO_VARARGS`
The code object has a variable positional parameter (*args-like).

➡ `inspect.CO_VARKEYWORDS`
The code object has a variable keyword parameter (**kwargs-like).

➡ `inspect.CO_NESTED`
The flag is set when the code object is a nested function.

➡ `inspect.CO_GENERATOR`
The flag is set when the code object is a generator function, i.e. a generator object is returned when the code object is executed.

➡ `inspect.CO_NOFREE`
The flag is set if there are no free or cell variables.

➡ `inspect.CO_COROUTINE`
The flag is set when the code object is a coroutine function. When the code object is executed it returns a coroutine object. See PEP 492 for more details.

New in version 3.5.


➡ `inspect.CO_ITERABLE_COROUTINE`
The flag is used to transform generators into generator-based coroutines. Generator objects with this flag can be used in await expression, and can yield from coroutine objects. See PEP 492 for more details.

New in version 3.5.


➡ `inspect.CO_ASYNC_GENERATOR`
The flag is set when the code object is an asynchronous generator function. When the code object is executed it returns an asynchronous generator object. See PEP 525 for more details.

New in version 3.6.


Note:
 The flags are specific to CPython, and may not be defined in other Python implementations. Furthermore, the flags are an implementation detail, and can be removed or deprecated in future Python releases. It’s recommended to use public APIs from the inspect module for any introspection needs.
 


### ===🗝 Command Line Interface

The `inspect` module also provides a basic introspection capability from the command line.

By default, accepts the name of a module and prints the source of that module. A class or function within the module can be printed instead by appended a colon and the qualified name of the target object.

➡ `--details`
Print information about the specified object rather than the source code


## ==⚡ site — Site-specific configuration hook
◦ Readline configuration
◦ Module contents
◦ Command Line Interface


Source code: Lib/site.py


This module is automatically imported during initialization. The automatic import can be suppressed using the interpreter’s `-S` option.

Importing this module will append site-specific paths to the module search path and add a few builtins, unless `-S` was used. In that case, this module can be safely imported with no automatic modifications to the module search path or additions to the builtins. To explicitly trigger the usual site-specific additions, call the site.main() function.


Changed in version 3.3: Importing the module used to trigger paths manipulation even when using `-S`.

It starts by constructing up to four directories from a head and a tail part. For the head part, it uses sys.prefix and sys.exec_prefix; empty heads are skipped. For the tail part, it uses the empty string and then lib/site-packages (on Windows) or lib/pythonX.Y/site-packages (on Unix and macOS). For each of the distinct head-tail combinations, it sees if it refers to an existing directory, and if so, adds it to sys.path and also inspects the newly added path for configuration files.


Changed in version 3.5: Support for the “site-python” directory has been removed.

If a file named “pyvenv.cfg” exists one directory above sys.executable, sys.prefix and sys.exec_prefix are set to that directory and it is also checked for site-packages (sys.base_prefix and sys.base_exec_prefix will always be the “real” prefixes of the Python installation). If “pyvenv.cfg” (a bootstrap configuration file) contains the key “include-system-site-packages” set to anything other than “true” (case-insensitive), the system-level prefixes will not be searched for site-packages; otherwise they will.

A path configuration file is a file whose name has the form name.pth and exists in one of the four directories mentioned above; its contents are additional items (one per line) to be added to sys.path. Non-existing items are never added to sys.path, and no check is made that the item refers to a directory rather than a file. No item is added to sys.path more than once. Blank lines and lines beginning with # are skipped. Lines starting with import (followed by space or tab) are executed.

Note:
 An executable line in a .pth file is run at every Python startup, regardless of whether a particular module is actually going to be used. Its impact should thus be kept to a minimum. The primary intended purpose of executable lines is to make the corresponding module(s) importable (load 3rd-party import hooks, adjust PATH etc). Any other initialization is supposed to be done upon a module’s actual import, if and when it happens. Limiting a code chunk to a single line is a deliberate measure to discourage putting anything more complex here.
 

For example, suppose sys.prefix and sys.exec_prefix are set to /usr/local. The Python X.Y library is then installed in /usr/local/lib/pythonX.Y. Suppose this has a subdirectory /usr/local/lib/pythonX.Y/site-packages with three subsubdirectories, foo, bar and spam, and two path configuration files, foo.pth and bar.pth. Assume foo.pth contains the following:


```py
# foo package configuration

foo
bar
bletch
```


and bar.pth contains:


```py
# bar package configuration

bar
```


Then the following version-specific directories are added to sys.path, in this order:


    /usr/local/lib/pythonX.Y/site-packages/bar
    /usr/local/lib/pythonX.Y/site-packages/foo


Note that bletch is omitted because it doesn’t exist; the bar directory precedes the foo directory because bar.pth comes alphabetically before foo.pth; and spam is omitted because it is not mentioned in either path configuration file.

After these path manipulations, an attempt is made to import a module named sitecustomize, which can perform arbitrary site-specific customizations. It is typically created by a system administrator in the site-packages directory. If this import fails with an ImportError or its subclass exception, and the exception’s name attribute equals to 'sitecustomize', it is silently ignored. If Python is started without output streams available, as with pythonw.exe on Windows (which is used by default to start IDLE), attempted output from sitecustomize is ignored. Any other exception causes a silent and perhaps mysterious failure of the process.

After this, an attempt is made to import a module named usercustomize, which can perform arbitrary user-specific customizations, if ENABLE_USER_SITE is true. This file is intended to be created in the user site-packages directory (see below), which is part of sys.path unless disabled by -s. If this import fails with an ImportError or its subclass exception, and the exception’s name attribute equals to 'usercustomize', it is silently ignored.

Note that for some non-Unix systems, sys.prefix and sys.exec_prefix are empty, and the path manipulations are skipped; however the import of sitecustomize and usercustomize is still attempted.


### ===🗝 Readline configuration

On systems that support readline, this module will also import and configure the rlcompleter module, if Python is started in interactive mode and without the -S option. The default behavior is enable tab-completion and to use ~/.python_history as the history save file. To disable it, delete (or override) the sys.__interactivehook__ attribute in your sitecustomize or usercustomize module or your PYTHONSTARTUP file.


Changed in version 3.4: Activation of rlcompleter and history was made automatic.


### ===🗝 Module contents

➡ `site.PREFIXES`
A list of prefixes for site-packages directories.

➡ `site.ENABLE_USER_SITE`
Flag showing the status of the user site-packages directory. True means that it is enabled and was added to sys.path. False means that it was disabled by user request (with -s or PYTHONNOUSERSITE). None means it was disabled for security reasons (mismatch between user or group id and effective id) or by an administrator.

➡ `site.USER_SITE`
Path to the user site-packages for the running Python. Can be None if getusersitepackages() hasn’t been called yet. Default value is ~/.local/lib/pythonX.Y/site-packages for UNIX and non-framework macOS builds, ~/Library/Python/X.Y/lib/python/site-packages for macOS framework builds, and %APPDATA%\Python\PythonXY\site-packages on Windows. This directory is a site directory, which means that .pth files in it will be processed.

➡ `site.USER_BASE`
Path to the base directory for the user site-packages. Can be None if getuserbase() hasn’t been called yet. Default value is ~/.local for UNIX and macOS non-framework builds, ~/Library/Python/X.Y for macOS framework builds, and %APPDATA%\Python for Windows. This value is used by Distutils to compute the installation directories for scripts, data files, Python modules, etc. for the user installation scheme. See also PYTHONUSERBASE.

➡ `site.main()`
Adds all the standard site-specific directories to the module search path. This function is called automatically when this module is imported, unless the Python interpreter was started with the -S flag.


Changed in version 3.3: This function used to be called unconditionally.

➡ `site.addsitedir(sitedir, known_paths=None)`
Add a directory to sys.path and process its .pth files. Typically used in sitecustomize or usercustomize (see above).

➡ `site.getsitepackages()`
Return a list containing all global site-packages directories.

New in version 3.2.


➡ `site.getuserbase()`
Return the path of the user base directory, USER_BASE. If it is not initialized yet, this function will also set it, respecting PYTHONUSERBASE.

New in version 3.2.


➡ `site.getusersitepackages()`
Return the path of the user-specific site-packages directory, USER_SITE. If it is not initialized yet, this function will also set it, respecting USER_BASE. To determine if the user-specific site-packages was added to sys.path ENABLE_USER_SITE should be used.

New in version 3.2.



### ===🗝 Command Line Interface

The site module also provides a way to get the user directories from the command line:


    $ python3 -m site --user-site
    /home/user/.local/lib/python3.3/site-packages


If it is called without arguments, it will print the contents of sys.path on the standard output, followed by the value of USER_BASE and whether the directory exists, then the same thing for USER_SITE, and finally the value of ENABLE_USER_SITE.

➡ `--user-base`
Print the path to the user base directory.

➡ `--user-site`
Print the path to the user site-packages directory.

If both options are given, user base and user site will be printed (always in this order), separated by os.pathsep.

If any option is given, the script will exit with one of these values: 0 if the user site-packages directory is enabled, 1 if it was disabled by the user, 2 if it is disabled for security reasons or by an administrator, and a value greater than 2 if there is an error.

See also:
 PEP 370 – Per user site-packages directory
 


# =🚩 Custom Python Interpreters

The modules described in this chapter allow writing interfaces similar to Python’s interactive interpreter. If you want a Python interpreter that supports some special feature in addition to the Python language, you should look at the code module. (The codeop module is lower-level, used to support compiling a possibly-incomplete chunk of Python code.)

The full list of modules described in this chapter is:

➡ `• code — Interpreter base classes`
◦ Interactive Interpreter Objects
◦ Interactive Console Objects

➡ `• codeop — Compile Python code`

## ==⚡ • code — Interpreter base classes

Source code: Lib/code.py

The `code` module provides facilities to implement read-eval-print loops in Python. Two classes and convenience functions are included which can be used to build applications which provide an interactive interpreter prompt.


✅ `class code.InteractiveInterpreter(locals=None)`
This class deals with parsing and interpreter state (the user’s namespace); it does not deal with input buffering or prompting or input file naming (the filename is always passed in explicitly). The optional locals argument specifies the dictionary in which code will be executed; it defaults to a newly created dictionary with key '__name__' set to '__console__' and key '__doc__' set to None.

✅ `class code.InteractiveConsole(locals=None, filename="<console>")`
Closely emulate the behavior of the interactive Python interpreter. This class builds on InteractiveInterpreter and adds prompting using the familiar sys.ps1 and sys.ps2, and input buffering.

➡ `code.interact(banner=None, readfunc=None, local=None, exitmsg=None)`
Convenience function to run a read-eval-print loop. This creates a new instance of InteractiveConsole and sets `readfunc` to be used as the `InteractiveConsole.raw_input()` method, if provided. If `local` is provided, it is passed to the InteractiveConsole constructor for use as the default namespace for the interpreter loop. 

The `interact()` method of the instance is then run with `banner` and `exitmsg` passed as the banner and exit message to use, if provided. The console object is discarded after use.

Changed in version 3.6: Added exitmsg parameter.

➡ `code.compile_command(source, filename="<input>", symbol="single")`
This function is useful for programs that want to emulate Python’s interpreter main loop (a.k.a. the read-eval-print loop). The tricky part is to determine when the user has entered an incomplete command that can be completed by entering more text (as opposed to a complete command or a syntax error). This function almost always makes the same decision as the real interpreter main loop.

`source` is the source string; `filename` is the optional filename from which source was read, defaulting to '<input>'; and `symbol` is the optional grammar start symbol, which should be 'single' (the default), 'eval' or 'exec'.

Returns a `code object` (the same as compile(source, filename, symbol)) if the command is complete and valid; `None` if the command is incomplete; raises `SyntaxError` if the command is complete and contains a syntax error, or raises `OverflowError` or `ValueError` if the command contains an invalid literal.


### ===🗝 Interactive Interpreter Objects

➡ `InteractiveInterpreter.runsource(source, filename="<input>", symbol="single")`
Compile and run some source in the interpreter. Arguments are the same as for compile_command(); the default for filename is '<input>', and for symbol is 'single'. One of several things can happen:

• The input is incorrect; compile_command() raised an exception (SyntaxError or OverflowError). A syntax traceback will be printed by calling the showsyntaxerror() method. runsource() returns False.

• The input is incomplete, and more input is required; compile_command() returned None. runsource() returns True.

• The input is complete; compile_command() returned a `code object`. The code is executed by calling the runcode() (which also handles run-time exceptions, except for SystemExit). runsource() returns False.

The return value can be used to decide whether to use sys.ps1 or sys.ps2 to prompt the next line.


➡ `InteractiveInterpreter.runcode(code)`
Execute a code object. When an exception occurs, showtraceback() is called to display a traceback. All exceptions are caught except SystemExit, which is allowed to propagate.

A note about KeyboardInterrupt: this exception may occur elsewhere in this code, and may not always be caught. The caller should be prepared to deal with it.

➡ `InteractiveInterpreter.showsyntaxerror(filename=None)`
Display the syntax error that just occurred. This does not display a stack trace because there isn’t one for syntax errors. If filename is given, it is stuffed into the exception instead of the default filename provided by Python’s parser, because it always uses '<string>' when reading from a string. The output is written by the write() method.

➡ `InteractiveInterpreter.showtraceback()`
Display the exception that just occurred. We remove the first stack item because it is within the interpreter object implementation. The output is written by the write() method.

Changed in version 3.5: The full chained traceback is displayed instead of just the primary traceback.


➡ `InteractiveInterpreter.write(data)`
Write a string to the standard error stream (sys.stderr). Derived classes should override this to provide the appropriate output handling as needed.


### ===🗝 Interactive Console Objects

The InteractiveConsole class is a subclass of InteractiveInterpreter, and so offers all the methods of the interpreter objects as well as the following additions.

➡ `InteractiveConsole.interact(banner=None, exitmsg=None)`
Closely emulate the interactive Python console. The optional banner argument specify the banner to print before the first interaction; by default it prints a banner similar to the one printed by the standard Python interpreter, followed by the class name of the console object in parentheses (so as not to confuse this with the real interpreter – since it’s so close!).

The optional exitmsg argument specifies an exit message printed when exiting. Pass the empty string to suppress the exit message. If exitmsg is not given or None, a default message is printed.

Changed in version 3.4: To suppress printing any banner, pass an empty string.

Changed in version 3.6: Print an exit message when exiting.


➡ `InteractiveConsole.push(line)`

Push a line of source text to the interpreter. The line should not have a trailing newline; it may have internal newlines. The line is appended to a buffer and the interpreter’s runsource() method is called with the concatenated contents of the buffer as source. If this indicates that the command was executed or invalid, the buffer is reset; otherwise, the command is incomplete, and the buffer is left as it was after the line was appended. The return value is True if more input is required, False if the line was dealt with in some way (this is the same as runsource()).

➡ `InteractiveConsole.resetbuffer()`
Remove any unhandled source text from the input buffer.

➡ `InteractiveConsole.raw_input(prompt="")`
Write a prompt and read a line. The returned line does not include the trailing newline. When the user enters the EOF key sequence, EOFError is raised. The base implementation reads from sys.stdin; a subclass may replace this with a different implementation.
    
## ==⚡ • codeop — Compile Python code

Source code: Lib/codeop.py


The `codeop` module provides utilities upon which the Python read-eval-print loop can be emulated, as is done in the code module. As a result, you probably don’t want to use the module directly; if you want to include such a loop in your program you probably want to use the code module instead.

There are two parts to this job:

1. Being able to tell if a line of input completes a Python statement: in short, telling whether to print ‘>>>’ or ‘...’ next.

2. Remembering which future statements the user has entered, so subsequent input can be compiled with these in effect.

The `codeop` module provides a way of doing each of these things, and a way of doing them both.

To do just the former:

➡ `codeop.compile_command(source, filename="<input>", symbol="single")`
Tries to compile source, which should be a string of Python code and return a code object if source is valid Python code. In that case, the filename attribute of the code object will be filename, which defaults to '<input>'. Returns None if source is not valid Python code, but is a prefix of valid Python code.

If there is a problem with source, an exception will be raised. SyntaxError is raised if there is invalid Python syntax, and OverflowError or ValueError if there is an invalid literal.

The symbol argument determines whether source is compiled as a statement ('single', the default), as a sequence of statements ('exec') or as an expression ('eval'). Any other value will cause ValueError to be raised.

Note:
 It is possible (but not likely) that the parser stops parsing with a successful outcome before reaching the end of the source; in this case, trailing symbols may be ignored instead of causing an error. For example, a backslash followed by two newlines may be followed by arbitrary garbage. This will be fixed once the API for the parser is better.
 
✅ `class codeop.Compile`
Instances of this class have __call__() methods identical in signature to the built-in function compile(), but with the difference that if the instance compiles program text containing a __future__ statement, the instance ‘remembers’ and compiles all subsequent program texts with the statement in force.

✅ `class codeop.CommandCompiler`
Instances of this class have __call__() methods identical in signature to compile_command(); the difference is that if the instance compiles program text containing a __future__ statement, the instance ‘remembers’ and compiles all subsequent program texts with the statement in force.


# =🚩 Importing Modules
- The Python Standard Library » Importing Modules

The modules described in this chapter provide new ways to import other Python modules and hooks for customizing the import process.

The full list of modules described in this chapter is:

### •zipimport — Import modules from Zip archives
◦ zipimporter Objects
◦ Examples

### • pkgutil — Package extension utility
### • modulefinder — Find modules used by a script
◦ Example usage of ModuleFinder

### • runpy — Locating and executing Python modules
### • importlib — The implementation of import
◦ Introduction
◦ Functions
◦ importlib.abc – Abstract base classes related to import
◦ importlib.resources – Resources
◦ importlib.machinery – Importers and path hooks
◦ importlib.util – Utility code for importers
◦ Examples
◾Importing programmatically
◾Checking if a module can be imported
◾Importing a source file directly
◾Setting up an importer
◾Approximating importlib.import_module()


### • Using importlib.metadata
◦ Overview
◦ Functional API
◾Entry points
◾Distribution metadata
◾Distribution versions
◾Distribution files
◾Distribution requirements

◦ Distributions
◦ Extending the search algorithm

## ==⚡ zipimport — Import modules from Zip archives

Source code: Lib/zipimport.py


This module adds the ability to import Python modules (`*.py`, `*.pyc`) and packages from ZIP-format archives. It is usually not needed to use the zipimport module explicitly; it is automatically used by the built-in import mechanism for `sys.path` items that are paths to ZIP archives.

Typically, `sys.path` is a list of directory names as strings. This module also allows an item of `sys.path` to be a string naming a ZIP file archive. The ZIP archive can contain a subdirectory structure to support package imports, and a path within the archive can be specified to only import from a subdirectory. For example, the path example.zip/lib/ would only import from the lib/ subdirectory within the archive.

Any files may be present in the ZIP archive, but only files .py and .pyc are available for import. ZIP import of dynamic modules (.pyd, .so) is disallowed. Note that if an archive only contains .py files, Python will not attempt to modify the archive by adding the corresponding .pyc file, meaning that if a ZIP archive doesn’t contain .pyc files, importing may be rather slow.

Changed in version 3.8: Previously, ZIP archives with an archive comment were not supported.


See also:
 PKZIP Application NoteDocumentation on the ZIP file format by Phil Katz, the creator of the format and algorithms used.
 PEP 273 - Import Modules from Zip ArchivesWritten by James C. Ahlstrom, who also provided an implementation. Python 2.3 follows the specification in PEP 273, but uses an implementation written by Just van Rossum that uses the import hooks described in PEP 302.
 PEP 302 - New Import HooksThe PEP to add the import hooks that help this module work.

This module defines an exception:
✅ `exception zipimport.ZipImportError`
Exception raised by zipimporter objects. It’s a subclass of ImportError, so it can be caught as ImportError, too.


### ===🗝 zipimporter Objects

zipimporter is the class for importing ZIP files.

✅ `class zipimport.zipimporter(archivepath)`
Create a new `zipimporter` instance. archivepath must be a path to a ZIP file, or to a specific path within a ZIP file. For example, an archivepath of foo/bar.zip/lib will look for modules in the lib directory inside the ZIP file foo/bar.zip (provided that it exists).

ZipImportError is raised if archivepath doesn’t point to a valid ZIP archive.
➡ `find_module(fullname[, path])`
Search for a module specified by fullname. fullname must be the fully qualified (dotted) module name. It returns the `zipimporter` instance itself if the module was found, or None if it wasn’t. The optional path argument is ignored—it’s there for compatibility with the importer protocol.
➡ `get_code(fullname)`
Return the code object for the specified module. Raise `ZipImportError` if the module couldn’t be found.
➡ `get_data(pathname)`
Return the data associated with pathname. Raise OSError if the file wasn’t found.

Changed in version 3.3: IOError used to be raised instead of OSError.

➡ `get_filename(fullname)`
Return the value __file__ would be set to if the specified module was imported. Raise `ZipImportError` if the module couldn’t be found.

New in version 3.1.

➡ `get_source(fullname)`
Return the source code for the specified module. Raise `ZipImportError` if the module couldn’t be found, return None if the archive does contain the module, but has no source for it.
➡ `is_package(fullname)`
Return True if the module specified by fullname is a package. Raise `ZipImportError` if the module couldn’t be found.
➡ `load_module(fullname)`
Load the module specified by fullname. fullname must be the fully qualified (dotted) module name. It returns the imported module, or raises `ZipImportError` if it wasn’t found.
➡ `archive`
The file name of the importer’s associated ZIP file, without a possible subpath.
➡ `prefix`
The subpath within the ZIP file where modules are searched. This is the empty string for `zipimporter` objects which point to the root of the ZIP file.

The archive and prefix attributes, when combined with a slash, equal the original archivepath argument given to the `zipimporter` constructor.


### ===🗝 Examples

Here is an example that imports a module from a ZIP archive - note that the zipimport module is not explicitly used.


>$ unzip -l example.zip
Archive:  example.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
     8467  11-26-02 22:30   jwzthreading.py
 --------                   -------
     8467                   1 file
$ ./python
Python 2.3 (#1, Aug 1 2003, 19:54:32)
>>> import sys
>>> sys.path.insert(0, 'example.zip')  # Add .zip file to front of path
>>> import jwzthreading
>>> jwzthreading.__file__
'example.zip/jwzthreading.py'


## ==⚡ pkgutil — Package extension utility

Source code: Lib/pkgutil.py


This module provides utilities for the import system, in particular package support.

✅ `class pkgutil.ModuleInfo(module_finder, name, ispkg)`
A namedtuple that holds a brief summary of a module’s info.

New in version 3.6.


➡ `pkgutil.extend_path(path, name)`
Extend the search path for the modules which comprise a package. Intended use is to place the following code in a package’s __init__.py:


    from pkgutil import extend_path
    __path__ = extend_path(__path__, __name__)


This will add to the package’s __path__ all subdirectories of directories on sys.path named after the package. This is useful if one wants to distribute different parts of a single logical package as multiple directories.

It also looks for `*.pkg` files beginning where * matches the name argument. This feature is similar to `*.pth` files (see the site module for more information), except that it doesn’t special-case lines starting with import. A `*.pkg` file is trusted at face value: apart from checking for duplicates, all entries found in a `*.pkg` file are added to the path, regardless of whether they exist on the filesystem. (This is a feature.)

If the input path is not a list (as is the case for frozen packages) it is returned unchanged. The input path is not modified; an extended copy is returned. Items are only appended to the copy at the end.

It is assumed that sys.path is a sequence. Items of sys.path that are not strings referring to existing directories are ignored. Unicode items on sys.path that cause errors when used as filenames may cause this function to raise an exception (in line with os.path.isdir() behavior).

✅ `class pkgutil.ImpImporter(dirname=None)`
PEP 302 Finder that wraps Python’s “classic” import algorithm.

If dirname is a string, a PEP 302 finder is created that searches that directory. If dirname is None, a PEP 302 finder is created that searches the current sys.path, plus any modules that are frozen or built-in.

Note that ImpImporter does not currently support being used by placement on sys.meta_path.


Deprecated since version 3.3: This emulation is no longer needed, as the standard import mechanism is now fully PEP 302 compliant and available in importlib.

✅ `class pkgutil.ImpLoader(fullname, file, filename, etc)`
Loader that wraps Python’s “classic” import algorithm.


Deprecated since version 3.3: This emulation is no longer needed, as the standard import mechanism is now fully PEP 302 compliant and available in importlib.

➡ `pkgutil.find_loader(fullname)`
Retrieve a module loader for the given fullname.

This is a backwards compatibility wrapper around importlib.util.find_spec() that converts most failures to ImportError and only returns the loader rather than the full ModuleSpec.


Changed in version 3.3: Updated to be based directly on importlib rather than relying on the package internal PEP 302 import emulation.


Changed in version 3.4: Updated to be based on PEP 451

➡ `pkgutil.get_importer(path_item)`
Retrieve a finder for the given path_item.

The returned finder is cached in sys.path_importer_cache if it was newly created by a path hook.

The cache (or part of it) can be cleared manually if a rescan of sys.path_hooks is necessary.


Changed in version 3.3: Updated to be based directly on importlib rather than relying on the package internal PEP 302 import emulation.

➡ `pkgutil.get_loader(module_or_name)`
Get a loader object for module_or_name.

If the module or package is accessible via the normal import mechanism, a wrapper around the relevant part of that machinery is returned. Returns None if the module cannot be found or imported. If the named module is not already imported, its containing package (if any) is imported, in order to establish the package __path__.


Changed in version 3.3: Updated to be based directly on importlib rather than relying on the package internal PEP 302 import emulation.


Changed in version 3.4: Updated to be based on PEP 451

➡ `pkgutil.iter_importers(fullname='')`
Yield finder objects for the given module name.

If fullname contains a ‘.’, the finders will be for the package containing fullname, otherwise they will be all registered top level finders (i.e. those on both sys.meta_path and sys.path_hooks).

If the named module is in a package, that package is imported as a side effect of invoking this function.

If no module name is specified, all top level finders are produced.


Changed in version 3.3: Updated to be based directly on importlib rather than relying on the package internal PEP 302 import emulation.

➡ `pkgutil.iter_modules(path=None, prefix='')`
Yields ModuleInfo for all submodules on path, or, if path is None, all top-level modules on sys.path.

path should be either None or a list of paths to look for modules in.

prefix is a string to output on the front of every module name on output.

Note:
 Only works for a finder which defines an iter_modules() method. This interface is non-standard, so the module also provides implementations for importlib.machinery.FileFinder and zipimport.zipimporter.
 


Changed in version 3.3: Updated to be based directly on importlib rather than relying on the package internal PEP 302 import emulation.

➡ `pkgutil.walk_packages(path=None, prefix='', onerror=None)`
Yields ModuleInfo for all modules recursively on path, or, if path is None, all accessible modules.

path should be either None or a list of paths to look for modules in.

prefix is a string to output on the front of every module name on output.

Note that this function must import all packages (not all modules!) on the given path, in order to access the __path__ attribute to find submodules.

onerror is a function which gets called with one argument (the name of the package which was being imported) if any exception occurs while trying to import a package. If no onerror function is supplied, ImportErrors are caught and ignored, while all other exceptions are propagated, terminating the search.

Examples:


```py
# list all modules python can access
walk_packages()

# list all submodules of ctypes
walk_packages(ctypes.__path__, ctypes.__name__ + '.')
```


Note:
 Only works for a finder which defines an iter_modules() method. This interface is non-standard, so the module also provides implementations for importlib.machinery.FileFinder and zipimport.zipimporter.
 


Changed in version 3.3: Updated to be based directly on importlib rather than relying on the package internal PEP 302 import emulation.

➡ `pkgutil.get_data(package, resource)`
Get a resource from a package.

This is a wrapper for the loader get_data API. The package argument should be the name of a package, in standard module format (foo.bar). The resource argument should be in the form of a relative filename, using / as the path separator. The parent directory name .. is not allowed, and nor is a rooted name (starting with a /).

The function returns a binary string that is the contents of the specified resource.

For packages located in the filesystem, which have already been imported, this is the rough equivalent of:


    d = os.path.dirname(sys.modules[package].__file__)
    data = open(os.path.join(d, resource), 'rb').read()


If the package cannot be located or loaded, or it uses a loader which does not support get_data, then None is returned. In particular, the loader for namespace packages does not support get_data.

➡ `pkgutil.resolve_name(name)`
Resolve a name to an object.

This functionality is used in numerous places in the standard library (see bpo-12915) - and equivalent functionality is also in widely used third-party packages such as setuptools, Django and Pyramid.

It is expected that name will be a string in one of the following formats, where W is shorthand for a valid Python identifier and dot stands for a literal period in these pseudo-regexes:

• W(.W)*
• W(.W)*:(W(.W)*)?

The first form is intended for backward compatibility only. It assumes that some part of the dotted name is a package, and the rest is an object somewhere within that package, possibly nested inside other objects. Because the place where the package stops and the object hierarchy starts can’t be inferred by inspection, repeated attempts to import must be done with this form.

In the second form, the caller makes the division point clear through the provision of a single colon: the dotted name to the left of the colon is a package to be imported, and the dotted name to the right is the object hierarchy within that package. Only one import is needed in this form. If it ends with the colon, then a module object is returned.

The function will return an object (which might be a module), or raise one of the following exceptions:
✅ `ValueError` – if name isn’t in a recognised format.
✅ `ImportError` – if an import failed when it shouldn’t have.
✅ `AttributeError` – If a failure occurred when traversing the object hierarchy within the imported package to get to the desired object.

New in version 3.9.



## ==⚡ modulefinder — Find modules used by a script

## ==⚡ runpy — Locating and executing Python modules

## ==⚡ importlib — The implementation of import

## ==⚡ Using importlib.metadata



# =🚩 Python Language Services

Python provides a number of modules to assist in working with the Python language. These modules support tokenizing, parsing, syntax analysis, bytecode disassembly, and various other facilities.

These modules include:

➡ `• ast — Abstract Syntax Trees`
◦ Abstract Grammar
◦ Node classes
◾ Literals
◾ Variables
◾ Expressions
◾ Subscripting
◾ Comprehensions

◾ Statements
◾ Imports

◾ Control flow
◾ Pattern matching
◾ Function and class definitions
◾ Async and await

◦ ast Helpers
◦ Compiler Flags
◦ Command-Line Usage

➡ `• symtable — Access to the compiler’s symbol tables`
◦ Generating Symbol Tables
◦ Examining Symbol Tables

➡ `• token — Constants used with Python parse trees`
➡ `• keyword — Testing for Python keywords`
➡ `• tokenize — Tokenizer for Python source`
◦ Tokenizing Input
◦ Command-Line Usage
◦ Examples

➡ `• tabnanny — Detection of ambiguous indentation`
➡ `• pyclbr — Python module browser support`
◦ Function Objects
◦ Class Objects

➡ `• py_compile — Compile Python source files`
◦ Command-Line Interface

➡ `• compileall — Byte-compile Python libraries`
◦ Command-line use
◦ Public functions

➡ `• dis — Disassembler for Python bytecode`
◦ Bytecode analysis
◦ Analysis functions
◦ Python Bytecode Instructions
◦ Opcode collections

➡ `• pickletools — Tools for pickle developers`
◦ Command line usage
◾ Command line options

◦ Programmatic Interface

## ==⚡ • ast — Abstract Syntax Trees

Source code: Lib/ast.py

The `ast` module helps Python applications to process trees of the Python abstract syntax grammar. The abstract syntax itself might change with each Python release; this module helps to find out programmatically what the current grammar looks like.

An abstract syntax tree can be generated by passing `ast.PyCF_ONLY_AST` as a flag to the `compile()` built-in function, or using the `parse()` helper provided in this module. The result will be a tree of objects whose classes all inherit from `ast.AST`. An abstract syntax tree can be compiled into a Python code object using the built-in `compile()` function.


### ===🗝 Abstract Grammar

The abstract grammar is currently defined as follows:

抽象语法描述语言 ASDL - Abstract syntax description language 是按树状结构排列的数据结构，是编译器将源代码语言转换为目标机器能理解的代码的程序中间表达形式


```cs
-- ASDL's 4 builtin types are:
-- identifier, int, string, constant

module Python
{
    mod = Module(stmt* body, type_ignore* type_ignores)
        | Interactive(stmt* body)
        | Expression(expr body)
        | FunctionType(expr* argtypes, expr returns)

    stmt = FunctionDef(identifier name, arguments args,
                       stmt* body, expr* decorator_list, expr? returns,
                       string? type_comment)
          | AsyncFunctionDef(identifier name, arguments args,
                             stmt* body, expr* decorator_list, expr? returns,
                             string? type_comment)

          | ClassDef(identifier name,
             expr* bases,
             keyword* keywords,
             stmt* body,
             expr* decorator_list)
          | Return(expr? value)

          | Delete(expr* targets)
          | Assign(expr* targets, expr value, string? type_comment)
          | AugAssign(expr target, operator op, expr value)
          -- 'simple' indicates that we annotate simple name without parens
          | AnnAssign(expr target, expr annotation, expr? value, int simple)

          -- use 'orelse' because else is a keyword in target languages
          | For(expr target, expr iter, stmt* body, stmt* orelse, string? type_comment)
          | AsyncFor(expr target, expr iter, stmt* body, stmt* orelse, string? type_comment)
          | While(expr test, stmt* body, stmt* orelse)
          | If(expr test, stmt* body, stmt* orelse)
          | With(withitem* items, stmt* body, string? type_comment)
          | AsyncWith(withitem* items, stmt* body, string? type_comment)

          | Match(expr subject, match_case* cases)

          | Raise(expr? exc, expr? cause)
          | Try(stmt* body, excepthandler* handlers, stmt* orelse, stmt* finalbody)
          | Assert(expr test, expr? msg)

          | Import(alias* names)
          | ImportFrom(identifier? module, alias* names, int? level)

          | Global(identifier* names)
          | Nonlocal(identifier* names)
          | Expr(expr value)
          | Pass | Break | Continue

          -- col_offset is the byte offset in the utf8 string the parser uses
          attributes (int lineno, int col_offset, int? end_lineno, int? end_col_offset)

          -- BoolOp() can use left & right?
    expr = BoolOp(boolop op, expr* values)
         | NamedExpr(expr target, expr value)
         | BinOp(expr left, operator op, expr right)
         | UnaryOp(unaryop op, expr operand)
         | Lambda(arguments args, expr body)
         | IfExp(expr test, expr body, expr orelse)
         | Dict(expr* keys, expr* values)
         | Set(expr* elts)
         | ListComp(expr elt, comprehension* generators)
         | SetComp(expr elt, comprehension* generators)
         | DictComp(expr key, expr value, comprehension* generators)
         | GeneratorExp(expr elt, comprehension* generators)
         -- the grammar constrains where yield expressions can occur
         | Await(expr value)
         | Yield(expr? value)
         | YieldFrom(expr value)
         -- need sequences for compare to distinguish between
         -- x < 4 < 3 and (x < 4) < 3
         | Compare(expr left, cmpop* ops, expr* comparators)
         | Call(expr func, expr* args, keyword* keywords)
         | FormattedValue(expr value, int conversion, expr? format_spec)
         | JoinedStr(expr* values)
         | Constant(constant value, string? kind)

         -- the following expression can appear in assignment context
         | Attribute(expr value, identifier attr, expr_context ctx)
         | Subscript(expr value, expr slice, expr_context ctx)
         | Starred(expr value, expr_context ctx)
         | Name(identifier id, expr_context ctx)
         | List(expr* elts, expr_context ctx)
         | Tuple(expr* elts, expr_context ctx)

         -- can appear only in Subscript
         | Slice(expr? lower, expr? upper, expr? step)

          -- col_offset is the byte offset in the utf8 string the parser uses
          attributes (int lineno, int col_offset, int? end_lineno, int? end_col_offset)

    expr_context = Load | Store | Del

    boolop = And | Or

    operator = Add | Sub | Mult | MatMult | Div | Mod | Pow | LShift
                 | RShift | BitOr | BitXor | BitAnd | FloorDiv

    unaryop = Invert | Not | UAdd | USub

    cmpop = Eq | NotEq | Lt | LtE | Gt | GtE | Is | IsNot | In | NotIn

    comprehension = (expr target, expr iter, expr* ifs, int is_async)

    excepthandler = ExceptHandler(expr? type, identifier? name, stmt* body)
                    attributes (int lineno, int col_offset, int? end_lineno, int? end_col_offset)

    arguments = (arg* posonlyargs, arg* args, arg? vararg, arg* kwonlyargs,
                 expr* kw_defaults, arg? kwarg, expr* defaults)

    arg = (identifier arg, expr? annotation, string? type_comment)
           attributes (int lineno, int col_offset, int? end_lineno, int? end_col_offset)

    -- keyword arguments supplied to call (NULL identifier for **kwargs)
    keyword = (identifier? arg, expr value)
               attributes (int lineno, int col_offset, int? end_lineno, int? end_col_offset)

    -- import name with optional 'as' alias.
    alias = (identifier name, identifier? asname)
             attributes (int lineno, int col_offset, int? end_lineno, int? end_col_offset)

    withitem = (expr context_expr, expr? optional_vars)

    match_case = (pattern pattern, expr? guard, stmt* body)

    pattern = MatchValue(expr value)
            | MatchSingleton(constant value)
            | MatchSequence(pattern* patterns)
            | MatchMapping(expr* keys, pattern* patterns, identifier? rest)
            | MatchClass(expr cls, pattern* patterns, identifier* kwd_attrs, pattern* kwd_patterns)

            | MatchStar(identifier? name)
            -- The optional "rest" MatchMapping parameter handles capturing extra mapping keys

            | MatchAs(pattern? pattern, identifier? name)
            | MatchOr(pattern* patterns)

             attributes (int lineno, int col_offset, int end_lineno, int end_col_offset)

    type_ignore = TypeIgnore(int lineno, string tag)
}
```

### ===🗝 Node classes


## ==⚡ • symtable — Access to the compiler’s symbol tables

Source code: Lib/symtable.py

Symbol tables are generated by the compiler from AST just before bytecode is generated. The symbol table is responsible for calculating the scope of every identifier in the code. symtable provides an interface to examine these tables.


### ===🗝 Generating Symbol Tables

➡ `symtable.symtable(code, filename, compile_type)`
Return the toplevel SymbolTable for the Python source code. filename is the name of the file containing the code. compile_type is like the mode argument to compile().


### ===🗝 Examining Symbol Tables

✅ `class symtable.SymbolTable`
A namespace table for a block. The constructor is not public.

➡ `get_type()`
Return the type of the symbol table. Possible values are 'class', 'module', and 'function'.

➡ `get_id()`
Return the table’s identifier.

➡ `get_name()`
Return the table’s name. This is the name of the class if the table is for a class, the name of the function if the table is for a function, or 'top' if the table is global (get_type() returns 'module').

➡ `get_lineno()`
Return the number of the first line in the block this table represents.

➡ `is_optimized()`
Return True if the locals in this table can be optimized.

➡ `is_nested()`
Return True if the block is a nested class or function.

➡ `has_children()`
Return True if the block has nested namespaces within it. These can be obtained with get_children().

➡ `get_identifiers()`
Return a list of names of symbols in this table.

➡ `lookup(name)`
Lookup name in the table and return a Symbol instance.

➡ `get_symbols()`
Return a list of Symbol instances for names in the table.

➡ `get_children()`
Return a list of the nested symbol tables.

✅ `class symtable.Function`
A namespace for a function or method. This class inherits SymbolTable.

➡ `get_parameters()`
Return a tuple containing names of parameters to this function.

➡ `get_locals()`
Return a tuple containing names of locals in this function.

➡ `get_globals()`
Return a tuple containing names of globals in this function.

➡ `get_nonlocals()`
Return a tuple containing names of nonlocals in this function.

➡ `get_frees()`
Return a tuple containing names of free variables in this function.

✅ `class symtable.Class`
A namespace of a class. This class inherits SymbolTable.

➡ `get_methods()`
Return a tuple containing the names of methods declared in the class.

✅ `class symtable.Symbol`
An entry in a SymbolTable corresponding to an identifier in the source. The constructor is not public.

➡ `get_name()`
Return the symbol’s name.

➡ `is_referenced()`
Return True if the symbol is used in its block.

➡ `is_imported()`
Return True if the symbol is created from an import statement.

➡ `is_parameter()`
Return True if the symbol is a parameter.

➡ `is_global()`
Return True if the symbol is global.

➡ `is_nonlocal()`
Return True if the symbol is nonlocal.

➡ `is_declared_global()`
Return True if the symbol is declared global with a global statement.

➡ `is_local()`
Return True if the symbol is local to its block.

➡ `is_annotated()`
Return True if the symbol is annotated.

New in version 3.6.

➡ `is_free()`
Return True if the symbol is referenced in its block, but not assigned to.

➡ `is_assigned()`
Return True if the symbol is assigned to in its block.

➡ `is_namespace()`
Return True if name binding introduces new namespace.

If the name is used as the target of a function or class statement, this will be true.

For example:

>>> table = symtable.symtable("def some_func(): pass", "string", "exec")
>>> table.lookup("some_func").is_namespace()
True


Note that a single name can be bound to multiple objects. If the result is True, the name may also be bound to other objects, like an int or list, that does not introduce a new namespace.

➡ `get_namespaces()`
Return a list of namespaces bound to this name.

➡ `get_namespace()`
Return the namespace bound to this name. If more than one namespace is bound, ValueError is raised.


## ==⚡ • token — Constants used with Python parse trees

Source code: Lib/token.py


This module provides constants which represent the numeric values of leaf nodes of the parse tree (terminal tokens). Refer to the file Grammar/Tokens in the Python distribution for the definitions of the names in the context of the language grammar. The specific numeric values which the names map to may change between Python versions.

The module also provides a mapping from numeric codes to names and some functions. The functions mirror definitions in the Python C header files.


➡ `token.tok_name`
Dictionary mapping the numeric values of the constants defined in this module back to name strings, allowing more human-readable representation of parse trees to be generated.

➡ `token.ISTERMINAL(x)`
Return True for terminal token values.

➡ `token.ISNONTERMINAL(x)`
Return True for non-terminal token values.

➡ `token.ISEOF(x)`
Return True if x is the marker indicating the end of input.

The token constants are:

|         Token          | value |         Token          | value |
|------------------------|-------|------------------------|-------|
| token.ENDMARKER        |       | token.TILDE            | "~"   |
| token.NAME             |       | token.CIRCUMFLEX       | "^"   |
| token.NUMBER           |       | token.LEFTSHIFT        | "<<"  |
| token.STRING           |       | token.RIGHTSHIFT       | ">>"  |
| token.NEWLINE          |       | token.DOUBLESTAR       | "**"  |
| token.INDENT           |       | token.PLUSEQUAL        | "+="  |
| token.DEDENT           |       | token.MINEQUAL         | "-="  |
| token.LPAR             | "("   | token.STAREQUAL        | "*="  |
| token.RPAR             | ")"   | token.SLASHEQUAL       | "/="  |
| token.LSQB             | "["   | token.PERCENTEQUAL     | "%="  |
| token.RSQB             | "]"   | token.AMPEREQUAL       | "&="  |
| token.COLON            | ":"   | token.VBAREQUAL        | "|="  |
| token.COMMA            | ","   | token.CIRCUMFLEXEQUAL  | "^="  |
| token.SEMI             | ";"   | token.LEFTSHIFTEQUAL   | "<<=" |
| token.PLUS             | "+"   | token.RIGHTSHIFTEQUAL  | ">>=" |
| token.MINUS            | "-"   | token.DOUBLESTAREQUAL  | "**=" |
| token.STAR             | "*"   | token.DOUBLESLASH      | "//"  |
| token.SLASH            | "/"   | token.DOUBLESLASHEQUAL | "//=" |
| token.VBAR             | "|"   | token.AT               | "@"   |
| token.AMPER            | "&"   | token.ATEQUAL          | "@="  |
| token.LESS             | "<"   | token.RARROW           | "->"  |
| token.GREATER          | ">"   | token.ELLIPSIS         | "..." |
| token.EQUAL            | "="   | token.COLONEQUAL       | ":="  |
| token.DOT              | "."   | token.OP               |       |
| token.PERCENT          | "%"   | token.AWAIT            |       |
| token.LBRACE           | "{"   | token.ASYNC            |       |
| token.RBRACE           | "}"   | token.TYPE_IGNORE      |       |
| token.EQEQUAL          | "=="  | token.TYPE_COMMENT     |       |
| token.NOTEQUAL         | "!="  | token.SOFT_KEYWORD     |       |
| token.LESSEQUAL        | "<="  | token.ERRORTOKEN       |       |
| token.GREATEREQUAL     | ">="  | token.N_TOKENS         |       |
|                        |       | token.NT_OFFSET        |       |

The following token type values aren’t used by the C tokenizer but are needed for the tokenize module.

➡ `token.COMMENT`
Token value used to indicate a comment.

➡ `token.NL`
Token value used to indicate a non-terminating newline. The NEWLINE token indicates the end of a logical line of Python code; NL tokens are generated when a logical line of code is continued over multiple physical lines.

➡ `token.ENCODING`
Token value that indicates the encoding used to decode the source bytes into text. The first token returned by tokenize.tokenize() will always be an ENCODING token.

➡ `token.TYPE_COMMENT`
Token value indicating that a type comment was recognized. Such tokens are only produced when `ast.parse()` is invoked with `type_comments=True`.

Changed in version 3.5: Added AWAIT and ASYNC tokens.


Changed in version 3.7: Added COMMENT, NL and ENCODING tokens.


Changed in version 3.7: Removed AWAIT and ASYNC tokens. “async” and “await” are now tokenized as NAME tokens.

Changed in version 3.8: Added TYPE_COMMENT, TYPE_IGNORE, COLONEQUAL. Added AWAIT and ASYNC tokens back (they’re needed to support parsing older Python versions for ast.parse() with feature_version set to 6 or lower).


## ==⚡ • keyword — Testing for Python keywords

Source code: Lib/keyword.py


This module allows a Python program to determine if a string is a keyword or soft keyword.

➡ `keyword.iskeyword(s)`
Return True if s is a Python keyword.

➡ `keyword.kwlist`
Sequence containing all the keywords defined for the interpreter. If any keywords are defined to only be active when particular __future__ statements are in effect, these will be included as well.

➡ `keyword.issoftkeyword(s)`
Return True if s is a Python soft keyword.

New in version 3.9.


➡ `keyword.softkwlist`
Sequence containing all the soft keywords defined for the interpreter. If any soft keywords are defined to only be active when particular __future__ statements are in effect, these will be included as well.

New in version 3.9.


## ==⚡ • tokenize — Tokenizer for Python source

Source code: Lib/tokenize.py

The `tokenize` module provides a `lexical scanner` for Python source code, implemented in Python. The scanner in this module returns comments as tokens as well, making it useful for implementing “pretty-printers”, including colorizers for on-screen displays.

To simplify token stream handling, all operator and delimiter tokens and Ellipsis are returned using the generic OP token type. The exact type can be determined by checking the exact_type property on the named tuple returned from `tokenize.tokenize()`.


### ===🗝 Tokenizing Input

The primary entry point is a generator:

➡ `tokenize.tokenize(readline)`
The tokenize() generator requires one argument, readline, which must be a callable object which provides the same interface as the `io.IOBase.readline()` method of file objects. Each call to the function should return one line of input as bytes.

The generator produces 5-tuples with these members: 

- the token type; the token string; 
- a 2-tuple `(srow, scol)` of ints specifying the row and column where the token begins in the source; 
- a 2-tuple `(erow, ecol)` of ints specifying the row and column where the token ends in the source; 
- and the line on which the token was found. The line passed (the last tuple item) is the physical line. 

The 5 tuple is returned as a named tuple with the field names: `type string start end line`.

The returned named tuple has an additional property named `exact_type` that contains the exact operator type for OP tokens. For all other token types `exact_type` equals the named tuple type field.

Changed in version 3.1: Added support for named tuples.

Changed in version 3.3: Added support for exact_type.

`tokenize()` determines the source encoding of the file by looking for a UTF-8 BOM or encoding cookie, according to PEP 263.

➡ `tokenize.generate_tokens(readline)`
Tokenize a source reading unicode strings instead of bytes.

Like tokenize(), the readline argument is a callable returning a single line of input. However, generate_tokens() expects readline to return a str object rather than bytes.

The result is an iterator yielding named tuples, exactly like tokenize(). It does not yield an ENCODING token.

All constants from the token module are also exported from tokenize.

Another function is provided to reverse the tokenization process. This is useful for creating tools that tokenize a script, modify the token stream, and write back the modified script.

➡ `tokenize.untokenize(iterable)`
Converts tokens back into Python source code. The iterable must return sequences with at least two elements, the token type and the token string. Any additional sequence elements are ignored.

The reconstructed script is returned as a single string. The result is guaranteed to tokenize back to match the input so that the conversion is lossless and round-trips are assured. The guarantee applies only to the token type and token string as the spacing between tokens (column positions) may change.

It returns bytes, encoded using the ENCODING token, which is the first token sequence output by tokenize(). If there is no encoding token in the input, it returns a str instead.

tokenize() needs to detect the encoding of source files it tokenizes. The function it uses to do this is available:

➡ `tokenize.detect_encoding(readline)`
The detect_encoding() function is used to detect the encoding that should be used to decode a Python source file. It requires one argument, readline, in the same way as the tokenize() generator.

It will call readline a maximum of twice, and return the encoding used (as a string) and a list of any lines (not decoded from bytes) it has read in.

It detects the encoding from the presence of a UTF-8 BOM or an encoding cookie as specified in PEP 263. If both a BOM and a cookie are present, but disagree, a SyntaxError will be raised. Note that if the BOM is found, 'utf-8-sig' will be returned as an encoding.

If no encoding is specified, then the default of 'utf-8' will be returned.

Use open() to open Python source files: it uses detect_encoding() to detect the file encoding.

➡ `tokenize.open(filename)`
Open a file in read only mode using the encoding detected by detect_encoding().

New in version 3.2.

✅ `exception tokenize.TokenError`
Raised when either a docstring or expression that may be split over several lines is not completed anywhere in the file, for example:


    """Beginning of
    docstring


or:


    [1,
     2,
     3


Note that unclosed single-quoted strings do not cause an error to be raised. They are tokenized as ERRORTOKEN, followed by the tokenization of their contents.


### ===🗝 Command-Line Usage


New in version 3.3.

The tokenize module can be executed as a script from the command line. It is as simple as:


    python -m tokenize [-e] [filename.py]

The following options are accepted:

➡ -h, --help: show this help message and exit
➡ -e, --exact: display token names using the exact type

If `filename.py` is specified its contents are tokenized to `stdout`. Otherwise, tokenization is performed on `stdin`.


### ===🗝 Examples

Example of a script rewriter that transforms float literals into Decimal objects:


```py
from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP
from io import BytesIO

def decistmt(s):
    """Substitute Decimals for floats in a string of statements.

    >>> from decimal import Decimal
    >>> s = 'print(+21.3e-5*-.1234/81.7)'
    >>> decistmt(s)
    "print (+Decimal ('21.3e-5')*-Decimal ('.1234')/Decimal ('81.7'))"

    The format of the exponent is inherited from the platform C library.
    Known cases are "e-007" (Windows) and "e-07" (not Windows).  Since
    we're only showing 12 digits, and the 13th isn't close to 5, the
    rest of the output should be platform-independent.

    >>> exec(s)  #doctest: +ELLIPSIS
    -3.21716034272e-0...7

    Output from calculations with Decimal should be identical across all
    platforms.

    >>> exec(decistmt(s))
    -3.217160342717258261933904529E-7
    """
    result = []
    g = tokenize(BytesIO(s.encode('utf-8')).readline)  # tokenize the string
    for toknum, tokval, _, _, _ in g:
        if toknum == NUMBER and '.' in tokval:  # replace NUMBER tokens
            result.extend([
                (NAME, 'Decimal'),
                (OP, '('),
                (STRING, repr(tokval)),
                (OP, ')')
            ])
        else:
            result.append((toknum, tokval))
    return untokenize(result).decode('utf-8')
```


Example of tokenizing from the command line. The script:


```py
def say_hello():
    print("Hello, World!")

say_hello()
```


will be tokenized to the following output where the first column is the range of the line/column coordinates where the token is found, the second column is the name of the token, and the final column is the value of the token (if any)


```sh
$ python -m tokenize hello.py
0,0-0,0:            ENCODING       'utf-8'
1,0-1,3:            NAME           'def'
1,4-1,13:           NAME           'say_hello'
1,13-1,14:          OP             '('
1,14-1,15:          OP             ')'
1,15-1,16:          OP             ':'
1,16-1,17:          NEWLINE        '\n'
2,0-2,4:            INDENT         '    '
2,4-2,9:            NAME           'print'
2,9-2,10:           OP             '('
2,10-2,25:          STRING         '"Hello, World!"'
2,25-2,26:          OP             ')'
2,26-2,27:          NEWLINE        '\n'
3,0-3,1:            NL             '\n'
4,0-4,0:            DEDENT         ''
4,0-4,9:            NAME           'say_hello'
4,9-4,10:           OP             '('
4,10-4,11:          OP             ')'
4,11-4,12:          NEWLINE        '\n'
5,0-5,0:            ENDMARKER      ''
```


The exact token type names can be displayed using the `-e` option:


```sh
$ python -m tokenize -e hello.py
0,0-0,0:            ENCODING       'utf-8'
1,0-1,3:            NAME           'def'
1,4-1,13:           NAME           'say_hello'
1,13-1,14:          LPAR           '('
1,14-1,15:          RPAR           ')'
1,15-1,16:          COLON          ':'
1,16-1,17:          NEWLINE        '\n'
2,0-2,4:            INDENT         '    '
2,4-2,9:            NAME           'print'
2,9-2,10:           LPAR           '('
2,10-2,25:          STRING         '"Hello, World!"'
2,25-2,26:          RPAR           ')'
2,26-2,27:          NEWLINE        '\n'
3,0-3,1:            NL             '\n'
4,0-4,0:            DEDENT         ''
4,0-4,9:            NAME           'say_hello'
4,9-4,10:           LPAR           '('
4,10-4,11:          RPAR           ')'
4,11-4,12:          NEWLINE        '\n'
5,0-5,0:            ENDMARKER      ''
```


Example of tokenizing a file programmatically, reading unicode strings instead of bytes with `generate_tokens()`:


```py
import tokenize

with tokenize.open('hello.py') as f:
    tokens = tokenize.generate_tokens(f.readline)
    for token in tokens:
        print(token)


Or reading bytes directly with tokenize():


import tokenize

with open('hello.py', 'rb') as f:
    tokens = tokenize.tokenize(f.readline)
    for token in tokens:
        print(token)

```

## ==⚡ • tabnanny — Detection of ambiguous indentation

Source code: Lib/tabnanny.py

缩进保姆 - 模糊缩进检测模块，tabnanny 是一个模糊缩进扫描检测程序。

Python 语言使用一致的缩进，这很重要，因为统一的缩进表示了有序，空格或Tab缩进不重要，但是统一的编进很重要。

For the time being this module is intended to be called as a script. However it is possible to import it into an IDE and use the function `check()` described below.

Note:
 The API provided by this module is likely to change in future releases; such changes may not be backward compatible.
 

✅ `tabnanny.check(file_or_dir)`
If `file_or_dir` is a directory and not a symbolic link, then recursively descend the directory tree named by `file_or_dir`, checking all .py files along the way. If `file_or_dir` is an ordinary Python source file, it is checked for whitespace related problems. The diagnostic messages are written to standard output using the print() function.

✅ `tabnanny.verbose`
Flag indicating whether to print verbose messages. This is incremented by the `-v` option if called as a script.

✅ `tabnanny.filename_only`
Flag indicating whether to print only the filenames of files containing whitespace related problems. This is set to true by the -q option if called as a script.

✅ `exception tabnanny.NannyNag`
Raised by `process_tokens()` if detecting an ambiguous indent. Captured and handled in check().

✅ `tabnanny.process_tokens(tokens)`
This function is used by check() to process tokens generated by the tokenize module.

See also:
 Module tokenizeLexical scanner for Python source code.
 
## ==⚡ • pyclbr — Python module browser support

Source code: Lib/pyclbr.py

The `pyclbr` module provides limited information about the functions, classes, and methods defined in a Python-coded module. 

The information is sufficient to implement a module browser. The information is extracted from the Python source code rather than by importing the module, so this module is safe to use with untrusted code. This restriction makes it impossible to use this module with modules not implemented in Python, including all standard and optional extension modules.

➡ `pyclbr.readmodule(module, path=None)`
Return a dictionary mapping module-level class names to class descriptors. If possible, descriptors for imported base classes are included. Parameter module is a string with the name of the module to read; it may be the name of a module within a package. If given, path is a sequence of directory paths prepended to `sys.path`, which is used to locate the module source code.

This function is the original interface and is only kept for back compatibility. It returns a filtered version of the following.

➡ `pyclbr.readmodule_ex(module, path=None)`
Return a dictionary-based tree containing a function or class descriptors for each function and class defined in the module with a `def` or `class` statement. The returned dictionary maps module-level function and class names to their descriptors. `Nested objects` are entered into the children dictionary of their parent. As with readmodule, module names the module to be read and path is prepended to `sys.path`. If the module being read is a package, the returned dictionary has a key '__path__' whose value is a list containing the package search path.

New in version 3.7: Descriptors for nested definitions. They are accessed through the new children attribute. Each has a new parent attribute.

The descriptors returned by these functions are instances of Function and Class classes. Users are not expected to create instances of these classes.


### ===🗝 Function Objects

Class Function instances describe functions defined by def statements. They have the following attributes:

➡ `Function.file`
Name of the file in which the function is defined.

➡ `Function.module`
The name of the module defining the function described.

➡ `Function.name`
The name of the function.

➡ `Function.lineno`
The line number in the file where the definition starts.

➡ `Function.parent`
For top-level functions, None. For nested functions, the parent.

New in version 3.7.


➡ `Function.children`
A dictionary mapping names to descriptors for nested functions and classes.

New in version 3.7.


➡ `Function.is_async`
True for functions that are defined with the async prefix, False otherwise.

New in version 3.10.



### ===🗝 Class Objects

Class Class instances describe classes defined by class statements. They have the same attributes as Functions and two more.

✅ `Class.file`
Name of the file in which the class is defined.

✅ `Class.module`
The name of the module defining the class described.

✅ `Class.name`
The name of the class.

✅ `Class.lineno`
The line number in the file where the definition starts.

✅ `Class.parent`
For top-level classes, None. For nested classes, the parent.

New in version 3.7.


✅ `Class.children`
A dictionary mapping names to descriptors for nested functions and classes.

New in version 3.7.


✅ `Class.super`
A list of Class objects which describe the immediate base classes of the class being described. Classes which are named as superclasses but which are not discoverable by readmodule_ex() are listed as a string with the class name instead of as Class objects.

✅ `Class.methods`
A dictionary mapping method names to line numbers. This can be derived from the newer children dictionary, but remains for back-compatibility.

## ==⚡ • py_compile — Compile Python source files

Source code: Lib/py_compile.py

The `py_compile` module provides a function to generate a byte-code file from a source file, and another function used when the module source file is invoked as a script.

Though not often needed, this function can be useful when installing modules for shared use, especially if some of the users may not have permission to write the byte-code cache files in the directory containing the source code.

✅ `exception py_compile.PyCompileError`
Exception raised when an error occurs while attempting to compile the file.

➡ `py_compile.compile(file, cfile=None, dfile=None, doraise=False, optimize=-1, invalidation_mode=PycInvalidationMode.TIMESTAMP, quiet=0)`
Compile a source file to byte-code and write out the byte-code cache file. The source code is loaded from the file named file. The byte-code is written to cfile, which defaults to the PEP 3147/PEP 488 path, ending in `.pyc`. 

For example, if `file` is `/foo/bar/baz.py` cfile will default to `/foo/bar/__pycache__/baz.cpython-32.pyc` for Python 3.2. If `dfile` is specified, it is used as the name of the source file in error messages instead of file. 

If `doraise` is true, a `PyCompileError` is raised when an error is encountered while compiling file. If doraise is false (the default), an error string is written to `sys.stderr`, but no exception is raised. This function returns the path to byte-compiled file, i.e. whatever cfile value was used.


The `doraise` and `quiet` arguments determine how errors are handled while compiling file. If quiet is 0 or 1, and doraise is false, the default behaviour is enabled: an error string is written to sys.stderr, and the function returns None instead of a path. If doraise is true, a PyCompileError is raised instead. However if quiet is 2, no message is written, and doraise has no effect.

If the path that `cfile` becomes (either explicitly specified or computed) is a symlink or non-regular file, FileExistsError will be raised. This is to act as a warning that import will turn those paths into regular files if it is allowed to write byte-compiled files to those paths. This is a side-effect of import using file renaming to place the final byte-compiled file into place to prevent concurrent file writing issues.

`optimize` controls the optimization level and is passed to the built-in `compile()` function. The default of -1 selects the optimization level of the current interpreter.

`invalidation_mode` should be a member of the `PycInvalidationMode` enum and controls how the generated bytecode cache is invalidated at runtime. The default is PycInvalidationMode.CHECKED_HASH if the `SOURCE_DATE_EPOCH` environment variable is set, otherwise the default is PycInvalidationMode.TIMESTAMP.

Changed in version 3.2: Changed default value of `cfile` to be PEP 3147-compliant. Previous default was file + 'c' ('o' if optimization was enabled). Also added the optimize parameter.


Changed in version 3.4: Changed code to use `importlib` for the byte-code cache file writing. This means file creation/writing semantics now match what `importlib` does, e.g. permissions, write-and-move semantics, etc. Also added the caveat that FileExistsError is raised if cfile is a symlink or non-regular file.


Changed in version 3.7: The invalidation_mode parameter was added as specified in PEP 552. If the SOURCE_DATE_EPOCH environment variable is set, invalidation_mode will be forced to PycInvalidationMode.CHECKED_HASH.


Changed in version 3.7.2: The SOURCE_DATE_EPOCH environment variable no longer overrides the value of the invalidation_mode argument, and determines its default value instead.


Changed in version 3.8: The quiet parameter was added.

✅ `class py_compile.PycInvalidationMode`
A enumeration of possible methods the interpreter can use to determine whether a bytecode file is up to date with a source file. The .pyc file indicates the desired invalidation mode in its header. See Cached bytecode invalidation for more information on how Python invalidates .pyc files at runtime.

New in version 3.7.

➡ `TIMESTAMP`
The .pyc file includes the timestamp and size of the source file, which Python will compare against the metadata of the source file at runtime to determine if the .pyc file needs to be regenerated.

➡ `CHECKED_HASH`
The .pyc file includes a hash of the source file content, which Python will compare against the source at runtime to determine if the .pyc file needs to be regenerated.

➡ `UNCHECKED_HASH`
Like CHECKED_HASH, the .pyc file includes a hash of the source file content. However, Python will at runtime assume the .pyc file is up to date and not validate the .pyc against the source file at all.

This option is useful when the .pycs are kept up to date by some system external to Python like a build system.


### ===🗝 Command-Line Interface

This module can be invoked as a script to compile several source files. The files named in filenames are compiled and the resulting bytecode is cached in the normal manner. This program does not search a directory structure to locate source files; it only compiles files named explicitly. The exit status is nonzero if one of the files could not be compiled.

➡ <file> ... <fileN>-
Positional arguments are files to compile. If - is the only parameter, the list of files is taken from standard input.

➡ -q, --quiet
Suppress errors output.

Changed in version 3.2: Added support for -.

Changed in version 3.10: Added support for -q.


See also:
 Module compileallUtilities to compile all Python source files in a directory tree.


## ==⚡ • compileall — Byte-compile Python libraries

## ==⚡ • dis — Disassembler for Python bytecode

## ==⚡ • pickletools — Tools for pickle developers


# =🚩 Miscellaneous Services
- https://docs.python.org/3.8/library/misc.html

The modules described in this chapter provide miscellaneous services that are available in all Python versions. Here’s an overview:

formatter — Generic output formatting

- The Formatter Interface
- Formatter Implementations
- The Writer Interface
- Writer Implementations

“formatter”模块现在已逐渐被完全弃用，仍将计划在 Python 3.6 中删除。

The `formatter` module has now graduated to full deprecation and is still slated for removal in Python 3.6.


# =🚩 MS Windows Specific Services

This chapter describes modules that are only available on MS Windows platforms.

➡ `• msilib — Read and write Microsoft Installer files`
◦ Database Objects
◦ View Objects
◦ Summary Information Objects
◦ Record Objects
◦ Errors
◦ CAB Objects
◦ Directory Objects
◦ Features
◦ GUI classes
◦ Precomputed tables

➡ `• msvcrt — Useful routines from the MS VC++ runtime`
◦ File Operations
◦ Console I/O
◦ Other Functions

➡ `• winreg — Windows registry access`
◦ Functions
◦ Constants
◾ HKEY_* Constants
◾ Access Rights
◾ 64-bit Specific

◾ Value Types

◦ Registry Handle Objects

➡ `• winsound — Sound-playing interface for Windows`

## ==⚡ • msilib — Read and write Microsoft Installer files

## ==⚡ • msvcrt — Useful routines from the MS VC++ runtime

## ==⚡ • winreg — Windows registry access

## ==⚡ • winsound — Sound-playing interface for Windows

The winsound module provides access to the basic sound-playing machinery provided by Windows platforms. It includes functions and several constants.

➡ `winsound.Beep(frequency, duration)`
Beep the PC’s speaker. The frequency parameter specifies frequency, in hertz, of the sound, and must be in the range 37 through 32,767. The duration parameter specifies the number of milliseconds the sound should last. If the system is not able to beep the speaker, RuntimeError is raised.

➡ `winsound.PlaySound(sound, flags)`
Call the underlying PlaySound() function from the Platform API. The sound parameter may be a filename, a system sound alias, audio data as a bytes-like object, or None. Its interpretation depends on the value of flags, which can be a bitwise ORed combination of the constants described below. If the sound parameter is None, any currently playing waveform sound is stopped. If the system indicates an error, RuntimeError is raised.

➡ `winsound.MessageBeep(type=MB_OK)`
Call the underlying MessageBeep() function from the Platform API. This plays a sound as specified in the registry. The type argument specifies which sound to play; possible values are -1, MB_ICONASTERISK, MB_ICONEXCLAMATION, MB_ICONHAND, MB_ICONQUESTION, and MB_OK, all described below. The value -1 produces a “simple beep”; this is the final fallback if a sound cannot be played otherwise. If the system indicates an error, RuntimeError is raised.

➡ `winsound.SND_FILENAME`
The sound parameter is the name of a WAV file. Do not use with SND_ALIAS.

➡ `winsound.SND_ALIAS`
The sound parameter is a sound association name from the registry. If the registry contains no such name, play the system default sound unless SND_NODEFAULT is also specified. If no default sound is registered, raise RuntimeError. Do not use with SND_FILENAME.

All Win32 systems support at least the following; most systems support many more:

|   PlaySound() name  | Corresponding Control Panel Sound name |
|---------------------|----------------------------------------|
| 'SystemAsterisk'    | Asterisk                               |
| 'SystemExclamation' | Exclamation                            |
| 'SystemExit'        | Exit Windows                           |
| 'SystemHand'        | Critical Stop                          |
| 'SystemQuestion'    | Question                               |

For example:


```py
import winsound
# Play Windows exit sound.
winsound.PlaySound("SystemExit", winsound.SND_ALIAS)

# Probably play Windows default sound, if any is registered (because
# "*" probably isn't the registered name of any sound).
winsound.PlaySound("*", winsound.SND_ALIAS)
```

异步播放文件需要等待，播放内存文件只能同步播放：

```py
import winsound, wave, time

wav = "C:\\coding\\md-code\\coding\\sub_snare03.wav"
ret = winsound.PlaySound(wav, winsound.SND_FILENAME|winsound.SND_ASYNC)
time.sleep(10) # wait sound play for file... 
help(winsound)

file = open(wavfile, "rb")
wav = file.read()
ret = winsound.PlaySound(wav, winsound.SND_MEMORY|winsound.SND_LOOP)
```



➡ `winsound.SND_LOOP`
Play the sound repeatedly. The SND_ASYNC flag must also be used to avoid blocking. Cannot be used with SND_MEMORY.

➡ `winsound.SND_MEMORY`
The sound parameter to PlaySound() is a memory image of a WAV file, as a bytes-like object.

Note:
 This module does not support playing from a memory image asynchronously, so a combination of this flag and SND_ASYNC will raise RuntimeError.
 

➡ `winsound.SND_PURGE`
Stop playing all instances of the specified sound.

Note:
 This flag is not supported on modern Windows platforms.
 

➡ `winsound.SND_ASYNC`
Return immediately, allowing sounds to play asynchronously.

➡ `winsound.SND_NODEFAULT`
If the specified sound cannot be found, do not play the system default sound.

➡ `winsound.SND_NOSTOP`
Do not interrupt sounds currently playing.

➡ `winsound.SND_NOWAIT`
Return immediately if the sound driver is busy.

Note:
 This flag is not supported on modern Windows platforms.
 

➡ `winsound.MB_ICONASTERISK`
Play the SystemDefault sound.

➡ `winsound.MB_ICONEXCLAMATION`
Play the SystemExclamation sound.

➡ `winsound.MB_ICONHAND`
Play the SystemHand sound.

➡ `winsound.MB_ICONQUESTION`
Play the SystemQuestion sound.

➡ `winsound.MB_OK`
Play the SystemDefault sound.



# =🚩 Unix Specific Services

## ==⚡ • posix — The most common POSIX system calls

## ==⚡ • pwd — The password database

## ==⚡ • spwd — The shadow password database

## ==⚡ • grp — The group database

## ==⚡ • crypt — Function to check Unix passwords

## ==⚡ • termios — POSIX style tty control

## ==⚡ • tty — Terminal control functions

## ==⚡ • pty — Pseudo-terminal utilities

## ==⚡ • fcntl — The fcntl and ioctl system calls

## ==⚡ • pipes — Interface to shell pipelines

## ==⚡ • resource — Resource usage information

## ==⚡ • nis — Interface to Sun’s NIS (Yellow Pages)

## ==⚡ • syslog — Unix syslog library routines

# =🚩 Superseded Modules

The modules described in this chapter are deprecated and only kept for backwards compatibility. They have been superseded by other modules.

➡ `• optparse — Parser for command line options`
◦ Background
◾ Terminology
◾ What are options for?
◾ What are positional arguments for?

◦ Tutorial
◾ Understanding option actions
◾ The store action
◾ Handling boolean (flag) options
◾ Other actions
◾ Default values
◾ Generating help
◾ Grouping Options

◾ Printing a version string
◾ How optparse handles errors
◾ Putting it all together

◦ Reference Guide
◾ Creating the parser
◾ Populating the parser
◾ Defining options
◾ Option attributes
◾ Standard option actions
◾ Standard option types
◾ Parsing arguments
◾ Querying and manipulating your option parser
◾ Conflicts between options
◾ Cleanup
◾ Other methods

◦ Option Callbacks
◾ Defining a callback option
◾ How callbacks are called
◾ Raising errors in a callback
◾ Callback example 1: trivial callback
◾ Callback example 2: check option order
◾ Callback example 3: check option order (generalized)
◾ Callback example 4: check arbitrary condition
◾ Callback example 5: fixed arguments
◾ Callback example 6: variable arguments

◦ Extending optparse
◾ Adding new types
◾ Adding new actions


➡ `• imp — Access the import internals`
◦ Examples


## ==⚡ • optparse — Parser for command line options

Source code: Lib/optparse.py

Deprecated since version 3.2: The optparse module is deprecated and will not be developed further; development will continue with the argparse module.


## ==⚡ • imp — Access the import internals

Source code: Lib/imp.py

Deprecated since version 3.4: The imp module is deprecated in favor of importlib.


# =🚩 Undocumented Modules

Undocumented Modules

Here’s a quick listing of modules that are currently undocumented, but that should be documented. Feel free to contribute documentation for them! (Send via email to docs@python.org.)

The idea and original contents for this chapter were taken from a posting by Fredrik Lundh; the specific contents of this chapter have been substantially revised.


Platform specific modules

These modules are used to implement the os.path module, and are not documented beyond this mention. There’s little need to document these.

➡ ntpath — Implementation of os.path on Win32 and Win64 platforms.
➡ posixpath — Implementation of os.path on POSIX.










