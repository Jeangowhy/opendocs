:icons: font
:stem: latexmath
:source-highlighter: highlight.js

[source]
----
while read -r path
do
pandoc -tasciidoc -rhtml "$path" >> $0
done<<EOF
https://www.web3d.org/documents/specifications/19775-1/V4.0/index.html
EOF
exit
----

== VRML 2.0

* http://www.graphics.stanford.edu/courses/cs248-98-winter/[CS 248: Introduction to Computer Graphics]
* http://www.graphics.stanford.edu/courses/cs248-98-winter/Assignments/VRML_Tutorial/[VRML 2.0 Tutorial Slides and Examples]
* http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/[The VRML Specification 2.0, 1996]
* https://www.web3d.org/documents/specifications/19776-2/V3.3/index.html[Extensible 3D (X3D) encodings ISO/IEC 19776-2:2015 Part 2:  Classic VRML encoding]
* http://www.agocg.ac.uk/train/vrml2rep/cover.htm[A Guide to VRML 2.0 and an Evaluation of VRML Modelling Tools, Neil Ashdown, Simon Forestiero, 1998]

.VRML，Virtual Reality Modeling Language
[NOTE]
******
虚拟现实建模语言（VRML，Virtual Reality Modeling Language）是一种用于创建三维虚拟世界的
建模语言。它允许用户在互联网上建立交互式的三维多媒体虚拟世界，并且具有平台无关性。使用文本格式
的 .wrl 或者 .vrml 扩展名文件描述 3D 场景，这是一种 ASCII 编码的文本文件。

VRML 最早于 1993 年 12 月由 Mark Pesce 和 Tony Parisi 开始设计，并在 1994 年 2 月完成
初始版本。1995 年 5月，VRML 1.0 规范正式出台，但缺少对一些关键特性如动作、交互和行为的支持。
1996 年 8 月，VRML 2.0 规范的第一版在 SIGGRAPH’96 会议上公布，并于 1997 年 4 月提交审议。
1997 年 12 月，VRML 作为国际标准正式发布，并于 1998 年 1 月获得 ISO 批准，国际标准号为
ISO／IEC 14772 1：1997，通常称为 VRML 97。

VRML 1.0只 能创建静态的 3D 景物，你可以在它们之间移动，来测览三维世界。VRML 2.0 增加了多种特性，
可以让物体旋转、行走、滚动、改变颜色和大小。其主要改进有：

1. 增强了静态世界；
2. 增加了交互性；
3. 增加了动画功能；
4. 增加了编程功能；
5. 增加了原形定义功能。

VRML 是一种用于对三维虚拟场景进行建模的描述性语言，它与以上所说的标记语言并没有什么关系。VRML 97
作为 ISO／IEC 国际标准，推动 Internet 交互式三维应用的发展。Netscape Navigator 4.x 和
Internet Explorer 4.x 浏览器开始就内置 VRML，配合脚本引擎实现可编程功能。无论过去如何将 VRML
吹的天花乱坠，目前为止，大多数现代浏览器已经不再直接支持 VRML，但您可以通过安装插件或使用专门的
VRML 阅读器来解决这个问题。也可以将 VRML 转换成某种格式，以便于获得各种 WebGL 框架支持。

VRML 目前由 Web3D 维护，并且作为 X3D 规范的一部分，因此 X3D 4.0 就相当 VRML 4.0。
Extensible 3D (X3D) encodings ISO/IEC 19776-2:2015。X3D 采用模块化架构（如 Core、
CAD、Medical 等模块），按需加载功能。支持 XML 编码，易于集成其他 Web 技术（SVG、HTML5）。
通过 X3DOM 框架直接在浏览器中渲染，与 JavaScript 深度交互，适合 WebGL 和 Three.js 集成。
VRML 转 X3D 工具 (Xj3D) 可将 .wrl 文件转换为 .x3d（保留大部分结构）。X3D 支持向后兼容
VRML97 语法（需设置 profile="Immersive"）。

VRML、X3D 核心差异对比
|===
|特性      |VRML                          |X3D
|标准规范   |VRML97（功能有限，无后续更新）    |ISO 标准，持续迭代（X3D 4.0）
|数据格式   |基于文本的 .wrl 文件            |支持 XML（.x3d）、JSON、二进制格式
|扩展性    |固定节点集，难以扩展              |模块化设计，支持自定义节点
|图形能力   |基础几何体、简单材质、有限光照     |支持 PBR 材质、动态光照、粒子系统
|交互与动画  |基本事件路由、时间传感器          |复杂事件链、脚本接口（ECMAScript）
|网络集成   |需插件（Cortona）              |原生 WebGL 支持（X3DOM 框架）
|性能优化   |低效，依赖CPU渲染                |硬件加速（GPU）、LOD 优化
|工具生态   |老旧工具（Blender VRML 导出插件） |现代工具链（如 FreeCAD、X3D-Edit）
|===

X3D is now 4th-generation VRML. X3D is a direct superset of VRML with four 
encodings: XML encoding .x3d, Classic VRML encoding .x3dv, VRML97 encoding 
.wrl and JavaScript Object Notation (JSON).  Programming language bindings 
are available for JavaScript and Java, with work in progress on C++/C# and 
Python.  X3D is designed so that all of these forms are functionally 
equivalent, you can choose to use any of them.

X3DOM (pronounced X-Freedom) is an open-source framework and runtime for 3D 
graphics on the Web. It can be freely used for non-commercial and commercial 
purposes, and is dual-licensed under MIT and GPL license.

• https://www.web3d.org/documents/specifications/19775-1/V4.0/index.html[Extensible 3D (X3D) 4.0]
• https://www.web3d.org/documents/specifications/19776-2/V3.3/index.html[X3D Part 2: Classic VRML encoding]
• https://sourceforge.net/projects/freewrl/files/freewrl-win32/[FreeWRL VRML/X3D browser]
• https://www.web3d.org/x3d/content/examples/HelloWorld.wrl[VRML HelloWorld]
• https://www.x3dom.org/[X3DOM]

从计算机编程语言范畴来看，VRML 就是提供了一种使用文本（领域语言）来描述 3D 场景，通过编译器或者
解释程序将场景描述转换成 3D 场景模型并通过渲染 2D 画面来模拟用户在文件中所描述的 3D 场景。因此
VRML 的应用不局限于 Web 环境，它也可以作为一种 3D 场景转换的中间文件。VRML 广泛应用于生活、
生产、科研教学、商务甚至军事等各种领域。它不仅支持数据和过程的三维表示，还能提供带有音响效果的结点，
用户能走进视听效果十分逼真的虚拟世界。VRML的出现使得互联网更加丰富多彩，用户可以在三维环境里随意
探寻互联网上丰富的信息资源。

VRML 基本语法。VRML文件可以包括四个主要成分：文件头、原型、造型和脚本、路由。文件头是 VRML 文件
的标志，所有 2.0 版本的 VRML 文件都以 `#VRML V2.0 utf8` 开头。VRML 文件描述的场景中包括几何
造型节点、外观节点、传感器节点等。通过这些节点，用户可以创建复杂的三维场景，并实现与用户的交互。
以下是一个简单的 VRML 示例，它在三维空间中建立一个立方体：

[source,vrml]
-------------
#VRML V2.0 utf8
Shape {
  appearance Appearance {
    material Material { }
  }
  geometry Box { }
}
-------------

VRML 初始设计为一种面向 Web 的三维造型语言，使用节点（Node）来描述三维对象和场景。每个节点可以
包含多个子节点，从而构成复杂的景物。VRML 文件是显式地定义和组织起来的3D多媒体对象集合，描述的是
基于时间的交互式 3D 多媒体信息的抽象功能行为。更新的 glTF 文档规范也有类似的结构。想想现在的
元宇宙（meta）概念，再看看当前的技术，人类始终狂热于生意。

虽然 VRML 2.0 于 1996 年成为 ISO 的国际标准，但 VRML 的最终版本，称为“VRML97”，于 1997
年标准化。大约在那个时候，人们对 VRML 的兴趣开始减弱，因为很明显 3D 在线世界，而不是像未来学家
所承诺的那样实用或有用。1996 年，CNET 写到 VRML 未能达到预期，称“带宽限制、硬件限制，以及最
糟糕的是，缺乏引人注目的应用程序可能会使 3D 技术暂时显得更虚拟。”

从发展的眼光来看，VRML 无疑是过时的、落后的产物。从是作为一个从 2025 年才开始认真阅读 VRML
文档的人，我可以从这里看到 VRML 和曾经的神经元那样沉睡在一堆论文的角落中，缺少的是一个契机
来使它的生命力再度活跃。或者说，人类的探索不会无缘由地成为垃圾被丢弃。
******

== VRML 2.0 Tutorial Slides and Examples

'''''

* <<Slide_01, 01 - What is VRML?>>
* <<Slide_02, 02 - Overview of Tutorial - Nuts & Bolts Approach>>
* <<Slide_03, 03 - Shape Nodes>>
* <<Slide_04, 04 - Shape Example>>
* <<Slide_05, 05 - Geometry>>
* <<Slide_06, 06 - Geometry Example>>
* <<Slide_07, 07 - Appearance and Material Properties>>
* <<Slide_08, 08 - Appearance and Material Example>>
* <<Slide_09, 09 - Transformations>>
* <<Slide_10, 10 - Transformation Example>>
* <<Slide_11, 11 - Nested Transformation Example>>
* <<Slide_12, 12 - Animating Objects>>
* <<Slide_13, 13 - Setting timers and interpolators: Example>>
* <<Slide_14, 14 - Attaching interpolators to transformations: Example>>
* <<Slide_15, 15 - Sensors & Triggers>>
* <<Slide_16, 16 - Sensor Example>>
* <<Slide_17, 17 - Titles, Navigation, Viewpoints>>
* <<Slide_18, 18 - Prototypes Example>>
* <<Silde_19, 19 - Level Of Detail Example>>

'''''

Copyright © 1997 Reid Gershbein

[[Slide_01]]
=== Slide 1 - What is VRML?

[NOTE]
******

* Virtual Reality Modeling Language (VRML)
* A language that describes 3D objects and environments.
** Geometry
** Appearance
** Animation
** Interactions

******

Slide 1 - 2/20/97 - Copyright © 1997 Reid Gershbein
CS248 - VRML Tutorial - Slide 2

[[Slide_02]]
=== Slide 2 - Overview of Tutorial - Nuts & Bolts Approach

******

* Shape
* Geometry
* Appearance and Material Properties
* Transformations
* Animation
* Sensors and Triggers

******

Slide 2 - 2/20/97 - Copyright © 1997 Reid Gershbein
CS248 - VRML Tutorial - Slide 3


[[Slide_03]]
=== Slide 3 - Shape Nodes

******

* Describes a piece of an object
* Two components:
** Geometry
** Appearance - Material

******

Slide 3 - 2/20/97 - Copyright © 1997 Reid Gershbein
CS248 - VRML Tutorial - Slide 4


[[Slide_04]]
=== Slide 4 - Shape Example

http://www.graphics.stanford.edu/courses/cs248-98-winter/Assignments/VRML_Tutorial/slide4.wrl

******

[source]
....
    #VRML V2.0 utf8

    Shape {
      appearance Appearance {
        material Material { diffuseColor .5 .1 .3 }
      }
      geometry Box { }
    }
....

******

Slide 4 - 2/20/97 - Copyright © 1997 Reid Gershbein
CS248 - VRML Tutorial - Slide 5


[[Slide_05]]
=== Slide 5 - Geometry

******

* Basic Primitives:
** Box
** Cone
** Cylinder
** Sphere
** IndexedFaceSet
*** _Polygons_

******

Slide 5 - 2/20/97 - Copyright © 1997 Reid Gershbein
CS248 - VRML Tutorial - Slide 6

[[Slide_06]]
=== Slide 6 - Geometry Example

http://www.graphics.stanford.edu/courses/cs248-98-winter/Assignments/VRML_Tutorial/slide6.wrl

******

[source]
....
Shape {
    appearance Appearance { material Material { } }

    geometry Cylinder {
        radius 2.5
        height 5
        top TRUE
        bottom FALSE
    }
}
....

******

Slide 6 - 2/20/97 - Copyright © 1997 Reid Gershbein
CS248 - VRML Tutorial - Slide 7

[[Slide_07]]
=== Slide 7 - Appearance and Material Properties

******

== 

* Describes surface properties of an object
** Color
** Shininess
** Transparency

******

Slide 7 - 2/20/97 - Copyright © 1997 Reid Gershbein
CS248 - VRML Tutorial - Slide 8

[[Slide_08]]
=== Slide 8 - Appearance and Material Example

http://www.graphics.stanford.edu/courses/cs248-98-winter/Assignments/VRML_Tutorial/slide8.wrl

******


[source]
....
#VRML V2.0 utf8

Shape {
	appearance Appearance {
		material Material {
			diffuseColor .5 .1 .3
			shininess .9
			transparency .3
		}
	}

	geometry Box {}
}
....

******

Slide 8 - 2/20/97 - Copyright © 1997 Reid Gershbein
CS248 - VRML Tutorial - Slide 9

[[Slide_09]]
=== Slide 9 - Transformations


******

* Translation
* Rotation
* Scaling
* Can be nested.
* Order in a node:
. Scale
. Rotate
. Translate

******

Slide 9 - 2/20/97 - Copyright © 1997 Reid Gershbein
CS248 - VRML Tutorial - Slide 10

[[Slide_10]]
=== Slide 10 - Transformation Example

http://www.graphics.stanford.edu/courses/cs248-98-winter/Assignments/VRML_Tutorial/slide10.wrl

******


[source]
....
#VRML V2.0 utf8

Transform {
	translation 2 1 -1
	children [
		Shape {
			appearance Appearance { material Material {} }
			geometry Cone {}
		}
	]
}
....

******

Slide 10 - 2/20/97 - Copyright © 1997 Reid Gershbein
CS248 - VRML Tutorial - Slide 11

[[Slide_11]]
=== Slide 11 - Nested Transformation Example

http://www.graphics.stanford.edu/courses/cs248-98-winter/Assignments/VRML_Tutorial/slide11.wrl

******


[source]
....
#VRML V2.0 utf8

Transform {
	rotation 1 0 0 1.54
	children [
		Transform {
			translation 2 1 -1
			children [
				Shape {
					appearance Appearance { material Material {} }
					geometry Cone {}
				}
			]
		}
	]
}
....

******

Slide 11 - 2/20/97 - Copyright © 1997 Reid Gershbein
CS248 - VRML Tutorial - Slide 12

[[Slide_12]]
=== Slide 12 - Animating Objects

******

* Timers
* Interpolators
** PositionInterpolator
*** Translation
** OrientationInterpolator
*** Rotation
** ScalarInterpolator
*** Scale
* Routing
** Connect timers to interpolators.
** Connect sensors to timers.

******

Slide 12 - 2/20/97 - Copyright © 1997 Reid Gershbein
CS248 - VRML Tutorial - Slide 13

[[Slide_13]]
=== Slide 13 - Setting timers and interpolators: Example

http://www.graphics.stanford.edu/courses/cs248-98-winter/Assignments/VRML_Tutorial/slide13.wrl

******


[source]
....
#VRML V2.0 utf8

DEF TIMER TimeSensor {
	cycleInterval 10
	loop TRUE
	startTime 1
}
DEF SPIN OrientationInterpolator {
	key [ 0, .5, 1 ]
	keyValue [ 0 0 1 0, 0 0 1 3.14, 0 0 1 6.28 ]
}
ROUTE TIMER.fraction_changed TO SPIN.set_fraction
....

******

Slide 13 - 2/20/97 - Copyright © 1997 Reid Gershbein
CS248 - VRML Tutorial - Slide 14

[[Slide_14]]
=== Slide 14 - Attaching interpolators to transformations: Example

http://www.graphics.stanford.edu/courses/cs248-98-winter/Assignments/VRML_Tutorial/slide14.wrl

******


[source]
....
#VRML V2.0 utf8

DEF TIMER TimeSensor {
	cycleInterval 10
	loop TRUE
	startTime 1
}
DEF SPIN OrientationInterpolator {
	key [ 0, .5, 1 ]
	keyValue [ 0 0 1 0, 0 0 1 3.14, 0 0 1 6.28 ]
}
ROUTE TIMER.fraction_changed TO SPIN.set_fraction
DEF OBJECT_TO_SPIN Transform {
	children [
		Shape {
			appearance Appearance {
				material Material {} 
			}
			geometry Cone {}
		}
	]
}
ROUTE SPIN.value_changed TO OBJECT_TO_SPIN.rotation
....

******

Slide 14 - 2/20/97 - Copyright © 1997 Reid Gershbein
CS248 - VRML Tutorial - Slide 15

[[Slide_15]]
=== Slide 15 - Sensors & Triggers

******

* Sensors
** Can trigger timers to start.
*** Allows animations to be triggered by user interaction.

******

Slide 15 - 2/20/97 - Copyright © 1997 Reid Gershbein
CS248 - VRML Tutorial - Slide 16

[[Slide_16]]
=== Slide 16 - Sensor Example

http://www.graphics.stanford.edu/courses/cs248-98-winter/Assignments/VRML_Tutorial/slide16.wrl

******


[source]
....
#VRML V2.0 utf8

DEF TIMER TimeSensor {
	cycleInterval 10
	startTime 0
	loop FALSE
}
DEF SPIN OrientationInterpolator {
	key [ 0, .5, 1 ]
	keyValue [ 0 0 1 0, 0 0 1 3.14, 0 0 1 6.28 ]
}
ROUTE TIMER.fraction_changed TO SPIN.set_fraction
DEF OBJECT_TO_SPIN Transform {
	children [
		DEF A_TOUCH_SENSOR TouchSensor {}

		Shape {
			appearance Appearance {
				material Material {} 
			}
			geometry Cone {}
		}
	]
}
ROUTE SPIN.value_changed TO OBJECT_TO_SPIN.rotation

ROUTE A_TOUCH_SENSOR.touchTime TO TIMER.startTime
....

******

Slide 16 - 2/20/97 - Copyright © 1997 Reid Gershbein

[[Slide_17]]
=== Slide 17 - Titles, Navigation, Viewpoints

******


[source]
....
# Set a page title
WorldInfo {
    title "James is Testing VRML"
}

# Set viewer to examiner
NavigationInfo {
    type "EXAMINE"
}

# Set some viewpoints
Viewpoint {
    position 0 0 10
    orientation 0 0 1 0
    description "Front"
}
Viewpoint {
    position 0 10 0
    orientation 1 0 0 -1.57
    description "Top"
}
....

******

Slide 17

[[Slide_18]]
=== Slide 18 - Prototypes Example

http://www.graphics.stanford.edu/courses/cs248-98-winter/Assignments/VRML_Tutorial/jed.wrl

******

[subs=normal]
....
\#VRML V2.0 utf8

# Set a page title
WorldInfo {
	title "James is Testing VRML"
}

# Set viewer to examiner
NavigationInfo {
	type "EXAMINE"
}

# Set some viewpoints
Viewpoint {
	position 0 0 10
	orientation 0 0 1 0 
	description "Front"
}
Viewpoint {
	position 0 10 0
	orientation 1 0 0 -1.57
	description "Top"
}

**# Prototypes**
PROTO JED_Cone [
	field SFColor color .5 .5 .5
]
{ 
	Shape {
		appearance Appearance {
			material Material {
				diffuseColor IS color
			}
		}
		geometry Cone{}
	}
}

Transform {
	translation 2 0 0
	children [
		JED_Cone{
			color .8 .2 .2
		}
	]
}
Transform {
	translation -2 0 0 
	children [
		JED_Cone{}
	]
}

[source]
....
******

Slide 18

[[Silde_19]]
=== Slide 19 - Level Of Detail Example

http://www.graphics.stanford.edu/courses/cs248-98-winter/Assignments/VRML_Tutorial/lod.wrl


******
[source,vrml]
....
#VRML V2.0 utf8


LOD {
	center 0 0 0 
	range [ 11, 13 ]
	level [

		Inline {
			url "extrude.wrl"
		}
		,
		Shape {
			appearance Appearance {material Material{}}
			geometry Cylinder{
				radius 2
				height 4
			}
		}
		,
		Shape {
			appearance Appearance {material Material{}}
			geometry Box{
				size 4 2 2
			}
		}
	]
}
....
******


Slide 19 - 2/98

== A Guide to VRML 2.0 and an Evaluation of VRML Modelling Tools

A Guide to VRML 2.0 and an Evaluation of VRML Modelling Tools
by Neil Ashdown, Simon Forestiero, 1998

*_Cover_*


**A Guide to VRML 2.0 +
and an Evaluation of VRML Modelling Tools**

'''''

Neil Ashdown +
Simon Forestiero +
1998 +

http://www.mmu.ac.uk

*Department of Architecture, Landscape +
and Three Dimensional Design* +
Manchester Metropolitan University +
Chatham Building +
Cavendish Street +
Manchester M15 6BR +

*http://www.mmu.ac.uk/art-des/arc/*
 

=== Contents

<<about, About this Report>>

<<contents, Contents>>

Part One: *A Guide to VRML 2.0*

* <<guide1, 1. Introduction>>
** <<heading2, 1.1 What is VRML>>
** <<heading3, 1.2 VRML 1.0>>
** <<heading4, 1.3 VRML 2.0>>
** <<heading5, 1.4 VRML97>>
** <<heading6, 1.5 Uses>>
* <<guide2, 2. VRML Browsers>>
** <<heading8, 2.1 General Information>>
** <<heading11, 2.2 CASUS Presenter>>
** <<heading16, 2.3 Community Place>>
** <<heading21, 2.4 Cosmo Player>>
** <<heading26, 2.5 VRMLview>>
** <<heading31, 2.6 VRwave>>
** <<heading36, 2.7 WorldView>>
** <<heading41, 2.8 Other Browsers>>
* <<guide3, 3. Language Details>>
** <<heading43, 3.1 Example 1: A Yellow Box>>
** <<heading49, 3.2 Example 2: An Animated and Interactive Scene>>
** <<heading67, 3.3 Complex Shapes>>
** <<heading68, 3.4 Controlling Detail>>
** <<heading69, 3.5 Other Nodes>>
** <<heading75, 3.6 Scripts>>
** <<heading76, 3.7 Prototypes>>
** <<heading77, 3.8 Node Summary>>
** <<heading78, 3.9 Online Tutorials>>
* <<guide4, 4. Creating VRML Worlds>>
** <<heading80, 4.1 Modelling Tools>>
** <<heading81, 4.2 File Translators>>
** <<heading87, 4.3 Validators>>
** <<heading91, 4.4 Object Libraries>>
** <<heading92, 4.5 Other Utilities>>
* <<guide5, 5. Publishing Worlds on the Web>>
** <<heading94, 5.1 MIME type>>
** <<heading95, 5.2 Integrating VRML with HTML>>
** <<heading97, 5.3 File Optimization>>
* <<guide6, 6. Future Development>>
** <<heading103, 6.1 Development Process>>
** <<heading104, 6.2 Mailing List>>
** <<heading105, 6.3 Conclusion>>
* <<guide7, 7. Further Information>>
** <<heading107, 7.1 Internet Resources>>
** <<heading108, 7.2 Books>>
** <<heading109, 7.3 SIMA Publications>>

Part Two: *An Evaluation of VRML Modelling Tools*

* <<eval1, 1. Introduction>>
** <<headinge2, 1.1 VRML Modelling Tools>>
** <<headinge3, 1.2 Evaluation Objectives>>
** <<headinge4, 1.3 Report Structure>>
* <<eval2, 2. Software Selection>>
** <<headinge6, 2.1 Testing Platform>>
** <<headinge7, 2.2 Selection Procedure>>
* <<eval3, 3. Evaluation Methodology>>
** <<headinge9, 3.1 Evaluation Criteria>>
** <<headinge10, 3.2 Testing Procedure>>
* <<eval4, 4. Software Overview>>
** <<headinge12, 4.1 Caligari TrueSpace 3.0>>
** <<headinge13, 4.2 Ligos V-Realm Builder 2.0>>
** <<headinge14, 4.3 Paragraph Internet Space Builder 2.1>>
** <<headinge15, 4.4 Platinum VRCreator 2.0>>
** <<headinge16, 4.5 Rendersoft VRML Editor 1.3>>
** <<headinge17, 4.6 Sculptware LLC SiteSculptor 1.0>>
* <<eval5, 5. Comparison of Features>>
* <<eval6, 6. Conclusions>>

<<appxa, Appendix A: *VRML Nodes*>>

* <<appxafie, Field Value Types>>
* <<appxanod, Node Reference>>


[[about]]
=== About this Report

This report is about the second version of the Virtual Reality Modelling
Language, VRML 2.0, the file format standard that has brought
interactive virtual worlds and a three dimensional interface to the
World Wide Web. The report has two parts:

*link:guide[Part 1: A Guide to VRML 2.0]*, introduces the
language and provides an overview of software and resources available
for viewing and creating VRML worlds. It has seven sections: +

* The _link:guide1[Introduction]_ describes what VRML is, how
it was developed and what it can be used for. It also explains the
differences between the different versions of VRML. +
* _link:guide2[VRML Browsers]_ summarizes the main features of
six different programs currently available for viewing VRML 2.0 worlds.
It includes screenshots and the addresses of the Web sites from where
the software can be downloaded. +
* _link:guide3[Language Details]_ introduces the features of
the language, primarily with the use of examples. It also lists the
locations of some online tutorials. +
* _link:guide4[Creating VRML Worlds]_ describes various
software tools and resources that can be used when creating VRML 2.0
worlds, including file translators, validators and object libraries. +
* _link:guide5[Publishing Worlds on the Web]_ discusses
techniques, such as integrating VRML with HTML and file optimization,
that are useful when making worlds available on the Web. +
* _link:guide6[Future Development]_ summarizes the current
state of VRML and discusses how it is likely to develop in the future. +
* _link:guide7[Further Information]_ lists other useful
sources of information on VRML 2.0, including Web sites, newsgroups,
books and reports.

*link:part2/eval.htm[Part 2: An Evaluation of VRML Modelling Tools]*,
compares six different VRML 2.0 modelling applications for the Windows95
system. It includes a description of the selection procedure, the
evaluation methodology, an overview of the each application,
screenshots, a table comparing the different features and a conclusion.

'''''

This report has been produced as part of the Support Initiative for
Multimedia Applications project, funded under the JISC New Technologies
Initaitive and administered by the http://www.agocg.ac.uk[Advisory Group
on Computer Graphics].

The author welcomes any comments, he can be contacted at
_N.Ashdown@mmu.ac.uk_.


==== _Part One: *A Guide to VRML 2.0*_

*Part One* +
*A Guide to VRML 2.0* +

'''''

* <<guide1, 1. Introduction>>
* <<guide2, 2. VRML Browsers>>
* <<guide3, 3. Language Details>>
* <<guide4, 4. Creating VRML Worlds>>
* <<guide5, 5. Publishing Worlds on the Web>>
* <<guide6, 6. Future Development>>
* <<guide7, 7. Further Information>>


[[guide1]]
=== 1. Introduction

* <<heading2, 1.1 What is VRML>>
* <<heading3, 1.2 VRML 1.0>>
* <<heading4, 1.3 VRML 2.0>>
* <<heading5, 1.4 VRML97>>
* <<heading6, 1.5 Uses>>

'''''

[[heading2]]
==== 1.1 What is VRML

The Virtual Reality Modelling Language (VRML) is the standard file
format for describing 3D objects and interactive scenes that can viewed
and explored on the World-Wide Web. VRML is pronounced as either
"V-R-M-L" or "ver-mal". VRML file names are given the extension, .wrl,
short for world.

image:http://www.agocg.ac.uk/train/vrml2rep/part1/fig11s.jpg[fig11s,width=200,height=167]

*Figure 1-1* A VRML Model of the National Gallery of Ireland with
intergrated HTML descriptions

In some ways, VRML can be considered as a three dimensional equivalent
of the Hypertext Markup Language (HTML), which is used to create the
hypertext documents of the Web. In particular, both languages share the
following important features:

* *They are written in a plain text format*. A VRML or HTML document can
be created or edited with an ordinary text editor.
* *Multi-platform*. Neither language is dependent on any particular
computer hardware or operating system.
* *Hyperlinking*. Both HTML documents and VRML worlds can contain links
to other documents, worlds or resources located anywhere on the Web,
identified by a Uniform Resource Locator (URL).
* *Inclusion of multimedia elements*. As well as text, a HTML document
can contain embedded images and other multimedia components such as
video, sound or Java applets, each referenced by a URL. Similarly, VRML
can use texture images, video, sound and program script files in a
world. A VRML world can also include other VRML files.
* *Open standards*. The HTML specification is controlled by the World
Wide Web Consortium, VRML is an ISO international standard.

However, the syntax and structure of each language are entirely
different. This is to be expected since each language is describing
entirely different things. HTML needs to describe the content, layout
and formatting of a two dimensional document. For this it uses a system
of markup tags. VRML, on the other hand is a modelling language, it
describes the geometry and positioning of objects within a three
dimensional space using 3D coordinates. It also specifies the appearance
of these objects and other properties such as lighting and interactions.

[[heading3]]
==== 1.2 VRML 1.0

The first version of VRML was developed between 1994 and 1995. The need
for a three dimensional equivalent of HTML was established at the First
International Conference on the World-Wide Web in the Spring of 1994. It
was here that the name VRML (originally Virtual Reality Markup Language)
was first coined.

Following the conference, an internet mailing list (www-vrml) was set up
to allow interested individuals to discuss the requirements of such a
language. The participants soon reached the consensus that in order to
develop a standard quickly, the first version of VRML should be be based
on one of the 3D modelling languages already in existence. For the same
reason, it was also decided to leave the more difficult problem of
defining methods for implementing animations and multi-participant
interactivity until later. The first version of VRML would only describe
static worlds.

A number of different formats were proposed as the basis of VRML 1.0,
but after some debate the members decided that Silicon Graphics' Open
Inventor ASCII format best met the agreed requirements. Discussions
continued on the mailing list on how to adapt the Open Inventor format,
which SGI had now placed in the public domain, to include platform
independence and hyperlinking features. The first draft of the VRML 1.0
specification was presented at the Second WWW Conference in October
1994. By the following Spring the first VRML compatible Web browser,
Silicon Graphics' WebSpace, was made freely available. This was soon
followed by other VRML browsers and authoring tools for most computer
platforms. Consequently, the number of Web sites containing VRML worlds
soon began to grow.

The VRML 1.0 specification allowed the following:

* Virtual 3D worlds created from primitive shapes, such as cubes, cones,
spheres and text, or with custom defined shapes.
* Material properties including texture maps to be applied to these
shapes. +
* Initial viewpoints and the ability for the user to examine or move
freely through a scene.
* Objects that are clickable hyperlinks to other VRML worlds or
documents. +
* Multiple light sources.
* Inline objects, where the geometry is defined in a separate VRML file.
* Objects with different levels of detail.
* Object definitions named and reused.

More information about VRML 1.0 can be found in the SIMA report "VRML in
Art and Design Higher Education" (see <<guide7, Section 7.0>>).

[[heading4]]
==== 1.3 VRML 2.0

With VRML now being actively supported by many major computer companies
and organisations, it became necessary for the development of VRML to be
organised more formally. Therefore, in 1995 the leaders of the VRML
community formed the VRML Architecture Group (VAG) to oversee its
development.

The limitations of the static VRML 1.0 worlds were soon becoming
apparent and there was demand for new features, especially for more
interactivity and behaviours. By the end of 1995, a number of companies,
including Silicon Graphics and Microsoft were already developing
replacements for VRML 1.0. Therefore, VAG decided that to avoid a
fragmentation of the standard, they would issue a request for proposals
and allow the VRML community to choose the best candidate for VRML 2.0
in a process similar to the development of VRML 1.0. The role of VAG
would be to produce a list a requirements and manage the selection
process, rather than defining the new specification themselves.

Six proposals were received: Active VRML from Microsoft, Dynamic Worlds
from GMD and others, HoloWeb from Sun, Moving Worlds from Silicon
Graphics and others, Out of this World from Apple, and Reactive Virtual
Environment from IBM Japan. Following a six week public review and
voting process in the early part of 1996, the Moving Worlds proposal
emerged as the clear favourite. Hence Moving Worlds was chosen as the
basis for VRML 2.0. After some amendments and revisions, the final VRML
2.0 specification was ready to be presented at the Siggraph96
conference.

The new features introduced in VRML 2.0 include:

* *Enhanced Static Worlds*. +
Sound, movie textures, fog and backgrounds can now be added to a VRML
world. There are new ways to define complex geometries such as terrains
and extruded shapes. The VRML 2.0 scene graph structure has also been
simplified.
* *Interaction and Animation*. +
VRML 1.0 worlds were static. VRML 2.0 allows objects within the scene to
move and repond to both user initiated and time based events. This is
achieved through sensors that detect or generate these events,
interpolators that describe what should happen when an event occurs and
routes which wire everything together. Complex animations and behaviours
can be programmed into a VRML 2.0 world using Netscape's JavaScript or
Sun Microsystems' Java languages.Collision detection and navigation
style information also help to improve the user interaction experience.
* *Prototyping New VRML Objects*. +
New VRML properties or objects can be defined and reused using the
prototyping mechanism.

All these features are described in more detail in
<<guide3, Section 3.0>>.

[[heading5]]
==== 1.4 VRML97

Following the selection of the Moving Worlds proposal as the basis of
VRML 2.0, VAG began working with the International Organization for
Standardization (ISO) and the International Electrotechnical Commission
(IEC) to turn VRML 2.0 into an international standard. This
collaboration resulted in ISO/IEC-14772-1:1997, also known as VRML97,
being formally approved by the ISO in December 1997, as an international
standard.

VRML97 is now the current version of VRML. The full specification
document can be found online at
_http://www.vrml.org/Specifications/VRML97/_

In fact there is no real difference in functionality between VRML 2.0
and VRML97. Changes were made only to the wording and layout of the
specification document, to ensure that it satsified the ISO
requirements. In this report, the name VRML 2.0 is generally used
instead of VRML97, for consistency.

In December 1996, the nonprofit VRML Consortium
(_http://www.vrml.org[www.vrml.org]_) replaced VAG as the body
responsible for the development and promotion of VRML. Consortium
corporate members include Apple, Cosmo Software (a SGI company),
Microsoft, IBM and Sony.

[[heading6]]
==== 1.5 Uses

VRML enables information and ideas to be presented in three dimensional
form and shared across computer networks. VRML worlds can be made
available on the Web or just a local Intranet. Worlds may be based on
real places or objects or be totally imaginary or abstract. A world may
be animated and highly interactive or be entirely static. Some example
applications are listed below.

* Advertising, _e.g. 3D banners replacing animated GIFs in a HTML page._ +
* Architecture, _e.g. a walk-through of building (see Figure 1-1)._ +
* Artistic, _e.g. virtual sculptures (see <<heading37, Figure 2-6>>)._ +
* Business, _e.g. 3D representation of databases._ +
* Manufacturing and Design, _e.g. a model of a product (see <<heading12, Figure 2-1>>)._ +
* Educational, _e.g. historical reconstructions._ +
* Entertainment, _e.g. animated cartoon characters (see <<heading22, Figure 2-3>>)._ +
* Geography, _e.g. terrain maps._ +
* Personal, _e.g. a 3D home page._ +
* Scientific, _e.g. simulations and data visualization._ +
* Virtual Communities, _e.g. multi-user games_. +

To see how VRML is currently being used, visit some of the recommended
sites listed at Cosmo Software, _http://cosmosoftware.com/galleries/_,
or at the Mining Co. VRML GuideSite, _http://vrml.miningco.com_. Example
sites are also shown in the figures in <<guide2, Section 2.0>>.

=== _Part One: *A Guide to VRML 2.0*_

[[guide2]]
=== 2. VRML Browsers

* <<heading8, 2.1 General Information>>
* <<heading11, 2.2 CASUS Presenter>>
* <<heading16, 2.3 Community Place>>
* <<heading21, 2.4 Cosmo Player>>
* <<heading26, 2.5 VRMLview>>
* <<heading31, 2.6 VRwave>>
* <<heading36, 2.7 WorldView>>
* <<heading41, 2.8 Other Browsers>>

'''''

[[heading8]]
==== 2.1 General Information

A Web browser, such as Netscape Navigator or Microsoft Internet
Explorer, can normally only display HTML text and images. In order to
view and interact with a VRML 2.0 world, a special _VRML browser_
program is required. The Web browser will automatically start an
installed VRML browser when a .wrl file is loaded. A number of different
VRML browsers are currently available for Windows, Macintosh and UNIX
computer systems. This section will describe the main features of six
different VRML 2.0 browsers: Fraunhofer Institute's *CASUS Presenter*,
Sony's *Community Place*, Silicon Graphics' *Cosmo Player*, SIM's
*VRMLview*, *VRwave* __ from IICM and Intervista's *WorldView*. All of
which are available via the internet.

[[heading9]]
===== Plug-ins and Helper Applications

Most VRML browsers operate as a _plug-in_, where the 3D scene and user
interface controls are actually displayed within the main Web browser
window. This allows a VRML scene to be embedded within a HTML document
(see Section 5.2 for more details). However, some VRML browsers display
the VRML world independently of the Web browser. They need to be set up
as a _helper application_ in order to work in conjunction with the web
browser.

[[heading10]]
===== Navigation and Interaction

VRML browsers usually offer the user more than one way of examining or
moving through the VRML scene. Although the methods of interaction are
implemented differently on each browser, they are typically based on
examine, fly, walk and click-and-seek themes. In examine mode, the user
can rotate an object or move it in relation to the viewpoint. A fly mode
simulates moving through the scene, with mouse or keyboard input
controlling speed and direction. Walk mode is similar to fly mode except
the user's viewpoint will follow the terrain. Finally, some browsers
implement a seek mode, where the user can click on a object with the
mouse pointer and the viewpoint will move automatically towards it. VRML
browsers also usually allow the user to switch between the viewpoints
defined within the VRML file.

Two other features found in all VRML browsers are an optional "headlamp"
to illuminate the scene immediately in front of the viewpoint and a
method to allow the user to select an object, usually by clicking on it
with the mouse pointer, in order to follow a hyperlink or to activate a
sensor.

[[heading11]]
==== 2.2 CASUS Presenter

Fraunhofer Institute for Computer Graphics, Darmstadt, Germany
(_http://www.igd.fhg.de[www.igd.fhg.de]_)

[[heading12]]
===== Current Version

CASUS Presenter 1.0a10 is available for SGI, Sun Solaris, Windows 95 and
NT platforms. It is free for non-commercial use, but note that it also
requires the Open Inventor run-time libraries to operate.

image:http://www.agocg.ac.uk/train/vrml2rep/part1/fig21s.jpg[fig21s,width=200,height=264]

*Figure 2-1* CASUS Presenter, model from Cosmo Software's Object Library

[[heading13]]
===== Features

CASUS Presenter is a stand-alone helper application, developed to test
Java animations within VRML. Not all VRML nodes are supported. It
accepts gzipped compressed .wrl files and the following file formats:

Textures: GIF, JPEG +
Movie: none +
Audio: MID, WAV (SGI and Solaris versions only) +
Scripts: Java (no security)

[[heading14]]
===== Additional Requirements

Java Development Kit (JDK) 1.0.2 or later +
Open Inventor runtime libraries 2.1 or 2.4 +
Kahlua Open Inventor Interface for Java (which can be downloaded for
free for non-commercial uses)

[[heading15]]
===== More Information

_http://www.igd.fhg.de/CP_

[[heading16]]
==== 2.3 Community Place

Sony Corporation (_http://www.sony.com[www.sony.com]_)

[[heading17]]
===== Current Version

Community Place preview version 2.0 is available for Windows 95 and NT,
as either a helper application or Netscape plug-in.

image:http://www.agocg.ac.uk/train/vrml2rep/part1/fig22s.jpg[fig22s,width=200,height=159]

*Figure 2-2* Community Place

[[heading18]]
===== Features

As well as displaying any VRML 2.0 file, Community Place also supports
Sony's Enhanced-VRML, which adds multi-user interaction, including chat,
to the VRML 2.0 specification. To take advantage of these multi-user
features, the browser needs to be connected via the internet to Sony's
Community Place Bureau multi-user server. Sony also produce a E-VRML
authoring tool called Community Place Conductor. The browser uses
Direct3D or Renderware for rendering and Microsoft's DirectX 5 sound
package for supporting ambient sound and voice chat.

Textures: BMP, GIF, JPEG +
Movie: BMP, GIF +
Audio: WAV, MOD +
Scripts: Java

[[heading19]]
===== Additional Requirements

Java Runtime Environment (JRE) +
Netscape Navigator 2.0 or 3.0 (not 4.0) for plug-in version.

[[heading20]]
===== More Information

_http://vs.spiw.com/vs/_

[[heading21]]
==== 2.4 Cosmo Player

Silicon Graphics, Inc. (_http://www.sgi.com[www.sgi.com]_)

[[heading22]]
===== Current Version

Cosmo Player version 2.1 for Windows 95 and NT, version 1.0b5 for
Windows 3.1 and version 1.0.2 for SGI. A Macintosh version will be made
available soon.

image:http://www.agocg.ac.uk/train/vrml2rep/part1/fig23s.jpg[fig23s,width=200,height=185]

*Figure 2-3* Cosmo Player (SGI version)

[[heading23]]
===== Features

The Cosmo Player plug-in is a popular VRML browser, due no doubt to its
full support of the VRML 2.0 specification and its inclusion as a
standard component of the Netscape Communicator distribution. It uses
OpenGL or Direct3D to take advantage of any hardware graphics
acceleration. Version 2.1 supports animations defined by JavaScript and
Java script nodes. It also allows the VRML scene to be controlled
externally by Java applets in HTML documents, via the External Authoring
Interface (EAI). The SGI version of CosmoPlayer currently only supports
VRMLScript, a limited implementation of JavaScript. The browser
automatically converts VRML 1.0 files and uncompresses gzipped worlds.

*Cosmo Player 2.1 for Windows 95/NT*

Textures: BMP, GIF, JPEG, PNG, RGB (SGI) +
Movie: Animated GIF, plus ActiveMovie formats* +
Audio: MIDI, WAV, plus ActiveMovie formats +
Scripts: Java, JavaScript, VRMLScript

*Cosmo Player 1.0.2 for SGI*

Textures: GIF, JPEG, PNG, RGB (SGI) +
Movie: MPEG, QuickTime, SGI Movie +
Audio: AIFC, AIFF, AU, MIDI, WAV +
Scripts: VRMLScript 

[[heading24]]
===== Additional Requirements

Netscape Communicator 4.0 or Microsoft Internet Explorer 4.0. +
The SGI version requires Netscape 2.0 or later. +
* Cosmo Player 2.1 uses Microsoft's ActiveMovie (now renamed DirectShow
and bundled with DirectX) to support various video and compressed audio
formats, including AVI, AIFF, MPEG, QuickTime and WAV.

[[heading25]]
===== More Information

** _http://cosmosoftware.com_

[[heading26]]
==== 2.5 VRMLview

Systems in Motion (_http://www.sim.no[www.sim.no]_)

[[heading27]]
===== Current Version

VRMLview 2.0b is available for Windows 95/NT, Linux, SGI and BeOS
systems.

image:http://www.agocg.ac.uk/train/vrml2rep/part1/fig24s.jpg[fig24s,width=200,height=187]

*Figure 2-4* VRMLview, model by Simon Forestiero

[[heading28]]
===== Features

VRMLview is a free stand-alone VRML model viewer. It can load both VRML
1.0 and 2.0 files, including gzip compressed files. However, these must
be local files, it cannot access resources from the Web. It does not
support scripting, sensor activation or animation, including movie
textures. Interactivity is limited to changing the viewpoint. Background
and fog nodes are also not supported. But VRMLview does have various
display options that are useful when checking static models, such as
surface normal and vertex highlighting, bounding boxes, wireframe and
surface shading modes. Light sources can be turned on or off and a
summary of the VRML scene graph can be displayed.

Textures: GIF, JPEG +
Movie: not supported +
Audio: not supported +
Scripts: not supported

[[heading29]]
===== Additional Requirements

None.

[[heading30]]
===== More Information

_http://www.sim.no/vrmlview.html_

[[heading31]]
==== 2.6 VRwave

Institute for Information Processing and Computer Supported New Media
(IICM), Graz University of Technology, Austria
(_http://www.iicm.edu[www.iicm.edu]_)

[[heading32]]
===== Current Version

VRwave 0.9 is available as source code (Java and C) or precompiled
binaries for SGI, Sun Solaris, Dec Alpha, HP-UX and Linux platforms.

image:http://www.agocg.ac.uk/train/vrml2rep/part1/fig25s.jpg[fig25s,width=200,height=164]

*Figure 2-5* VRwave

[[heading33]]
===== Features

VRwave can operate as a stand-alone helper application or as a Netscape
plug-in. It uses OpenGL or the work-alike 3D graphics library Mesa for
rendering. Not all VRML nodes are supported, though it does implement
the EAI.

Textures: GIF, JPEG +
Movie: not supported +
Audio: not supported +
Scripts: not supported

[[heading34]]
===== Additional Requirements

JDK 1.0.2 or 1.1 for helper application or compiling. +
Plug-in operation requires Netscape 3.0.

[[heading35]]
===== More Information

_http://www.iicm.edu/vrwave_ +
The software is also available from the HENSA mirror at: +
_http://www.hensa.ac.uk/ftp/mirrors/vrml/VRwave/_ +

[[heading36]]
==== 2.7 WorldView

Intervista Software (_http://www.intervista.com[www.intervista.com]_)

[[heading37]]
===== Current Version

WorldView versions 2.0 and 2.1 are available for Windows 95 and NT and
version 2.0 for the Macintosh.

image:http://www.agocg.ac.uk/train/vrml2rep/part1/fig26s.jpg[fig26s,width=200,height=173]

*Figure 2-6* WorldView, model by Vladimir Bulatov (Oregon State
University)

[[heading38]]
===== Features

Intervista's WorldView is another popular fully-featured plug-in browser
for Netscape Navigator (WorldView 2.0) and Microsoft Internet Explorer
(WorldView 2.1). It supports both JavaScript and Java animations,
spatialized sound and can also load VRML 1.0 files. WorldView offers the
user multiple navigation modes and controls, with the rendering achieved
through Microsoft's Direct3D or Apple's QuickDraw 3D. Microsoft's VRML
2.0 Viewer is based on WorldView.

Textures: +
Win__:__ BMP, GIF, JPEG, PNG, PPM, RAS +
Mac__:__ GIF, JPEG +
Movie: ActiveMovie formats +
Audio: WAV, plus ActiveMovie formats +
Scripts: JavaScript, Java

[[heading39]]
===== Additional Requirements

Netscape Navigator 3.x (the Windows version does not currently work with
Netscape Communicator 4.0) or Microsoft Internet Explorer 3.02 or later.

[[heading40]]
===== More Information

_http://www.intervista.com/worldview/_

[[heading41]]
==== 2.8 Other Browsers

Other VRML 2.0 browsers that are available include NewFire Torch, DpIV
from the Takshele Corporation and Live Picture Viewer. FreeWRL by Thomas
Lukka is GNU VRML browser for UNIX systems, written mostly in Perl.
Blaxxun Community Client Pro and Oz Virtual are special browsers that
support multi-user worlds. The SDSC VRML Repository
(_http://www.sdsc.edu/vrml[www.sdsc.edu/vrml]_) maintains a list of all
VRML browsers that are currently available.

For a more detailed comparison of VRML 2.0 browser features, check
Gregory Seidman's browser datasheet at
_http://www.cs.brown.edu/~gss/vrml/comparison_.

Advice on using more than one VRML plug-in at a time can be found at
_http://hiwaay.net/~crispen/vrmlworks/viewing2.html._


[[guide3]]
=== 3. Language Details

* <<heading43, 3.1 Example 1: A Yellow Box>>
* <<heading49, 3.2 Example 2: An Animated and Interactive Scene>>
* <<heading67, 3.3 Complex Shapes>>
* <<heading68, 3.4 Controlling Detail>>
* <<heading69, 3.5 Other Nodes>>
* <<heading75, 3.6 Scripts>>
* <<heading76, 3.7 Prototypes>>
* <<heading77, 3.8 Node Summary>>
* <<heading78, 3.9 Online Tutorials>>

'''''

This section will introduce the main features of the VRML 2.0 language
and demonstrate with examples how they can be used to create interactive
and animated virtual scenes. The example worlds used in this section can
also be found online at:

_http://www.mmu.ac.uk/art-des/arc/vrml2proj/_

For a more detailed explanation of all the many powerful capabilities of
VRML 2.0 language, the reader should refer to the online tutorials
listed in at the end of this section or to the other internet resources
and publications listed in <<guide7, Section 7>>. A list of all the
VRML 2.0 nodes can also be found in link:../appendix/appxa.htm[Appendix
A].

[[heading43]]
==== 3.1 Example 1: A Yellow Box

image:http://www.agocg.ac.uk/train/vrml2rep/part1/fig31s.jpg[fig31s,width=200,height=209]

*Figure 3-1* Example 1, viewed with WorldView

Figure 3-1 shows a simple VRML world, consisting of a single yellow box
shape. The VRML file that describes this scene is listed below. Because
a VRML file is written in plain text, any ordinary text editor (such as
WordPad or vi) can be used to create it. Simply save the file with a
.wrl extension, then open the file again with a VRML browser in order to
view the model in three-dimensions.


[source]
....
#VRML V2.0 utf8
# example1.wrl - a yellow box
Shape {
    geometry Box { }
    appearance Appearance {
        material Material {
            diffuseColor    1.0  1.0  0.0    # red, green, blue
        }
    }
}
....

[[heading44]]
===== The VRML 2.0 Header and Comments

The first line of the VRML file contains the header
*`+#VRML V2.0 utf+`*. This identifies it as a VRML 2.0 file, as opposed
to a VRML 1.0 file, for example. All VRML files must start with this
header. The *`utf`* keyword refers to the UTF-8 international
character set used in the file.

The second line starts with the `**`+#+`**' symbol. This indicates that
all the text with follows it, until the end of the line is a comment,
and should be ignored by the VRML software. In the example, the comment
describes the contents of the scene.

[[heading45]]
===== Nodes and the Scene Graph

The rest of a VRML file consists of a list of nodes, each of which
contain part of the information that describes the whole scene, for
example, it may define a shape or light source or particular properties
of an object, such as its colour or coordinates. Each node consists of a
name, that indicates the node's type, followed by a set a curly braces
containing zero or more fields, that define the attributes of that node.
A field statement consists of the field name followed by one or more
values. Fields can be written in any order. Node type names start with a
capital letter, field names begin with a lower case letter.

In the example, the nodes used are *`Shape`*, *`Box`*,
*`Appearance`* and *`Material`*. The *`Shape`* node contains two
fields, *`geometry`* and *`appearance`*, each of which require other
nodes as their values. The *`geometry`* field contains the *`Box`*
node and the *`appearance`* field takes the *`Appearance`* node as
its value. The *`Appearance`* node then in turn contains another node,
called *`Material`*. This hierachical structure of nodes contained
within other nodes is called the _scene graph_.

[[heading46]]
===== The Box node, Fields and Field Values

The *`Box`* node has a single field called *`size`*, which sets the
dimensions of the box shape that is to be drawn. The field name is
followed by three values that define the lengths of the sides, parallel
to the X, Y and Z axes respectively, with the centre of the box at the
origin. For example, the following node defines a box shape 3 units
wide, 2 units high and 5 units deep.

[source]
....
Box {
    size  3.0  2.0  5.0
}
....

However, the example world does not specify a size for the box,
therefore the VRML browser will use the default values for these
attributes when it displays the scene. A list of all the VRML nodes and
their default field values can be found in Appendix A . The different
data types the fields expect are also described.

[[heading47]]
===== VRML Units and Coordinate System

The VRML specification recommends that metres are used as units of
distance. Using a common unit of measurement makes it easier to share
models. However, using metres may not always be convenient. Time is
always defined in seconds and angles in radians.

VRML uses a right-handed coordinate system. By default, the viewer is on
the Z-axis looking towards the origin with the X-axis to the right and
Y-axis upwards.

[[heading48]]
===== Appearance and Material

Having defined the object's geometry through the *`Box`* node, it is
now necessary to describe the object's colour and surface properties
using the *`Appearance`* node. The Appearance node has three fields,
*`material`*, *`texture`* and *`textureTransform`*. The last two
fields are used when applying textures to an object, this is described
in more detail later. The *`material`* field takes the *`Material`*
node as its value. If the *`Appearance`* node is left undefined the
object will be rendered as pure white. The *`Material`* node specifies
the colour and reflective properties of an object with six fields.

* The *`diffuseColor`* field defines the base colour of the object,
that is reflected in all directions when it is illuminated. The colour
is specified using three floating point numbers ranging between 0.0 and
1.0 and representing the amounts of red, green and blue. The default
value is 0.8 0.8 0.8, which is a grey colour.

* The *`specularColor`* field defines the colour of the shiny
highlights on the object. By default this is 0.0 0.0 0.0 (black).
* The *`emissiveColor`* field specifies the colour given off by
objects, independent of any light sources. The default value is 0.0 0.0
0.0 (black).
* The *`shininess`* value controls the sharpness of the specular
highlight. Increasing this value will make the object appear more shiny.
the default value is 0.2.
* The *`transparency`* field specifies how "clear" the object is. A
value of 0.0 (default) indicates an opague object, 1.0 indicates full
transparency.
* The *`ambientIntensity`* value is used to simulate ambient
(indirect) lighting of the object. the default value is 0.2. In the
first example, only the *`diffuseColor`* field is required to define a
basic yellow colour for the box shape. The next example will show how
the other fields can be used to create different effects.


[[heading49]]
==== 3.2 Example 2: An Animated and Interactive Scene

image:http://www.agocg.ac.uk/train/vrml2rep/part1/fig32s.jpg[fig32s,width=200,height=172]

*Figure 3-2* Example 2, displayed by Cosmo Player

The second example VRML world (Figure 3-2) demonstrates more object
types, environmental and material properties and includes animations and
user interaction. The scene consists of a semi-transparent table top
with three objects positioned upon it. The first object is a globe,
consisting of a sphere on a base. The colour of the sphere will slowly
change. The second object is a texture-mapped box. The box also rotates
and forms a hyperlink to the AGOCG home page. The final object is an
inlined model of the Space Shuttle, which the user can manipulate.

The VRML file which describes this scene is listed below. The line
numbers to the left of the code are included for reference purposes, and
are not part of the VRML document.

[source]
....
1   #VRML V2.0 utf8
2   Group {
3       children [
4           # Define initial viewpoint
5           Viewpoint {
6               position    0 4 8
7               orientation 1 0 0 -0.46
8               description "VIEW1"
9           }
10  
11          # Make background a light blue colour
12          Background { skyColor [0.65, 0.86, 0.99]}
13  
14          # A point light source to illuminate scene
15          PointLight { location 10 0 10 }
16  
17          # Semi-transparent table top
18          Shape {
19              appearance Appearance {
20                  material Material { transparency  0.5 }
21              }
22              geometry Cylinder { 
23                  radius 5
24                  height 0.1
25              }
26          }
27  
28          # The globe: a sphere on a cone base
29          Transform {
30              translation  0.0  1.1 -2.0
31              children [
32                  Shape {                                 # The base          
33                      appearance Appearance {
34                          material DEF BaseMaterial Material { 
35                              diffuseColor    0.2  0.2  0.2
36                              specularColor   0.4  0.4  0.4
37                          }
38                      }
39                      geometry Cone { }
40                  }
41                  Transform {                 
42                      translation  0.0  0.5  0.0 
43                      children [
44                          Shape {                         # The sphere
45                              appearance Appearance {
46                                  material DEF SphereColour Material { }
47                              }
48                              geometry Sphere { }
49                          }
50                      ]
51                  }   
52              ]
53          }
54  
55          # Hyperlinked 'AGOCG' box
56          Anchor {
57              url "http://www.agocg.ac.uk"
58              description "AGOCG Home Page"
59              children [
60                  DEF BoxTransform Transform {
61                      translation  -2.6  1.1  1.0
62                      children [
63                          Shape { 
64                              appearance Appearance {
65                                  material USE BaseMaterial
66                                  texture ImageTexture {url "agocg_logo.gif"}
67                              }
68                              geometry Box { }
69                          }
70                      ]
71                  }
72              ]
73          }
74          
75          # Inline shuttle model
76          DEF ShuttleTransform Transform {
77              translation  2.6  1.6  1.0
78              scale 0.2 0.2 0.2
79              children [
80                  Inline { url "shuttle.wrl" }
81                  DEF ShuttleSensor SphereSensor { }
82              ]    
83          }
84  
85          # Clock to drive animations
86          DEF Clock TimeSensor {
87              cycleInterval 10.0      # 10 second animation
88              loop          TRUE      # animation repeats forever
89          }
90  
91          # Colour changes for sphere
92          DEF NewColour ColorInterpolator { 
93              key       [0.0,         0.33,        0.66,        1.0]
94              keyValue  [1.0 0.2 0.2, 0.2 1.0 0.2, 0.2 0.2 1.0, 1.0 0.2 0.2 ] 
95          }
96  
97          # Rotation for 'AGOCG' box
98          DEF BoxRotation OrientationInterpolator {
99              key      [0.0,              0.5,              1.0]
100             keyValue [0.0 1.0 0.0 0.00, 0.0 1.0 0.0 3.14, 0.0 1.0 0.0 6.28]
101         }
102     ]
103 }
104 
105 # Animate sphere
106 ROUTE Clock.fraction_changed TO NewColour.set_fraction
107 ROUTE NewColour.value_changed TO SphereColour.set_diffuseColor
108 
109 # Animate 'AGOCG' box
110 ROUTE Clock.fraction_changed TO BoxRotation.set_fraction
111 ROUTE BoxRotation.value_changed TO BoxTransform.set_rotation
112 
113 # Animate model shuttle
114 ROUTE ShuttleSensor.rotation_changed TO ShuttleTransform.set_rotation
....

[[heading50]]
===== Group Node (line 2)

The purpose of the *`Group`* node is to group together other nodes.
The grouped nodes are listed in the *`children`* field. In the
example, the *`Group`* node contains all the other nodes in the file
(the closing bracket of the node is on line 103), so forming a single
root node for the whole scene.

[[heading51]]
===== Viewpoint Node (line 5)

The *`Viewpoint`* node specifies the position and orientation of a
favoured view within the VRML scene. When a VRML browser loads a VRML
world containing a *`Viewpoint`* node, the scene will be initially
displayed from that viewpoint. A VRML file can contain any number of
*`Viewpoint`* nodes, although only one can be in use (bound) at a
time. So, when a browser loads a world with multiple viewpoint
definitions it will initially bind to the first viewpoint listed in the
file. However, most VRML browsers also provide a menu in their user
interface to allow the user to select between the different predefined
viewpoints.

[[heading52]]
===== Background Node (line 12)

In the example, the *`Background`* node is used to specify a simple
light blue background colour for the world. However, more complicated
backgrounds can be defined with this node, using colour gradients and
panorama images, such as mountain ranges or cityscapes.

[[heading53]]
===== Scene Illumination (line 15)

The example world contains a single point light source, located at (10 0
10). It is not essential to include a light source in a VRML world,
since VRML browsers usually have a "headlight" mode to illuminate the
scene immediately in front of the viewer. However, adding lights can
often enhance the appearance of a scene.

There are three types of lighting nodes in VRML. *`DirectionalLight`*
is used to simulate illumination from a far away light source, such as
the sun. Since all the light rays are parallel (i.e. have the same
direction vector), this is the easiest type of lighting for a browser to
calculate. The *`PointLight`* node is used in the example, this models
a point light source that sends out light with equal intensity in all
directions. As well as the *`location`* field, there are fields for
controlling how the intensity of the light diminishes with distance.
Finally, there is a *`SpotLight`* node that only illuminates objects 
within a specified cone-shaped space.

Note that the light from VRML light sources is not stopped when it hits
an object, but goes straight through, so there are no shadows. However,
there are various techniques that can be used to "fake" shadows, such as
texture mapping.

[[heading54]]
===== The Table Top (lines 18 to 26)

The table top is modelled using a flattened cylinder shape. The geometry
is defined using the *`Cylinder`* node. VRML has four primitive
geometry nodes: *`Box`*, *`Cone`*, *`Cylinder`* and *`Sphere`*.
All four are used in this example world. Each node has fields to specify
the dimensions of the shape and in addition, the *`Cone`* and
*`Cylinder`* nodes have fields which control whether particular sides
are shown. So, for example, by making the *`side`* and *`bottom`*
fields of the *`Cylinder`* node *`FALSE`*, a disc shape can be
produced.

To simulate a semi-transparent glass table top, the *`transparency`*
value in the shape's *`Material`* node is made 0.5.

[[heading55]]
===== Transform Node

In order to move (translate), rotate or scale an object the
*`Transform`* node is used. The effect of the node is to create a new
coordinate system, which is used by all the children nodes of the
transform. A *`Transform`* node can be a child of another transform,
so producing a hierarchy or stack of coordinate transformations. This is
demonstrated in the example world when defining the globe shape. The
first *`Transform`* node (line 29), moves the globe's base to (0, 1.1,
-2) in the world coordinate system. The second transform (line 41),
which is a child of the first, moves the sphere part of the globe
upwards by (0, 0.5, 0) relative to the position of the base part. Hence,
the globe is positioned at (0, 1.5, -2) in the world coordinate system.

[[heading56]]
===== Naming Nodes with DEF

VRML allows a name to given to a node, using the *`DEF`* syntax:

*`+DEF +`_`NodeName`_`+ +`_`NodeType`_`+ { ... }+`*

A node name must start with a letter and not contain any spaces. In line
34 the *`Material`* node for the globe's base is named
*`BaseMaterial`*, and in line 46 the sphere's *`Material`* node is
named *`SphereColour`*. Node names are required when reusing nodes and
when defining animations, both of which are described later.

[[heading57]]
===== Anchor Node (line 56)

The *`Anchor`* node creates a hyperlink to a resource specified by the
*`url`* field of the node. This could be another VRML world, a HTML
page or any other type Web document. In order to activate the link the
user must click on any of the objects defined in the *`children`*
field of the Anchor node. In the example this is the texture mapped
AGOCG box. The *`description`* field contains extra information about
the link that can be displayed by the VRML browser.

[[heading58]]
===== Instancing (line 65)

The box has the same shiny dark grey material properties as the globe
base, so instead of rewriting the whole *`Material`* node and its
values, the *`USE`* syntax is used:

*`+USE +`_`NodeName`_*

Where *_`NodeName`_* is the name of the node previously defined in the
file using the *`DEF`* syntax, which in our example is
*`BaseMaterial`*. This re-use of nodes, is called _instancing_. As
well as saving space within a file, instancing enables any changes made
to the properties of the original node to be carried forward to all of
its instances.

[[heading59]]
===== Texture Mapping (line 66)

The box shape has an image of the AGOCG logo on each of its faces. This
is achieved by using an *`ImageTexture`* node in the *`texture`*
field of the shape's *`Appearance`* node. The *`ImageTexture`* node
has a *`url`* field which specifies the location and name of the image
file to be applied to the shape. In the example a relative URL is used
because the agocg_logo.gif file is in the same directory as the .wrl
file. Note that the texture overrides the *`diffuseColor`* property of
the *`Material`* node.

Nearly all VRML browsers will accept JPEG and GIF (including
transparent) formats for texture mapping. The VRML specification also
says that the PNG format should be supported, although currently not all
browsers do (see <<guide2, Section 2.0>>).

The position, scale and orientation of the texture can be controlled
using the *`TextureTransform`* node in the *`Appearance`* node. By
default, a texture image will be repeated when mapped on a surface, this
can be turned off using *`repeatS`* and *`repeatT`* fields of the
*`ImageTexture`* node.

The *`PixelTexture`* and *`MovieTexture`* nodes can also be used
within the *`texture`* field of the *`Appearance`* node. The
*`PixelTexture`* node records the texture image data within the VRML
file itself, instead of an external image file. The pixel values are
specified in the uncompressed *`SFImage`* format (see
link:../appendix/appxa.htm[Appendix A]). The *`MovieTexture`* node is
used to specify a movie file (for example, in a MPEG format) for texture
mapping.

[[heading60]]
===== Inline Node (line 80)

The final object in the scene is a model of the Space Shuttle. The
Shuttle's geometry and appearance are actually defined in separate VRML
file called "shuttle.wrl". The *`Inline`* node is used to tell the
VRML browser to fetch the VRML file from the location specified in the
*`url`* field and insert it into this world. Inlining is useful
because it enables objects to be reused in different scenes and helps
make large worlds more manageable.

The *`scale`* field (line 78) of the Shuttle's *`Transform`* node is
used to reduce the size of the model by one-fifth, so that it can fit on
the table top.

[[heading61]]
===== SphereSensor Node (line 81)

The *`Transform`* node also contains a *`SphereSensor`* node, which
has been named *`ShuttleSensor`*. A sensor node detects viewer actions
with the mouse pointer. When a viewer clicks on a shape in the same
group as the sensor node, it generates events that can be used to start
or control animations.

The *`SphereSensor`* node senses the user's click-and-drag action over
the Shuttle shape, and computes a rotation value. This rotation value is
then used to change the orientation of the Shuttle model using the
*`ROUTE`* mechanism, which is described later. Other types of
pointing-device sensor nodes are *`TouchSensor`*, *`CylinderSensor`*
and *`PlaneSensor`*.

[[heading62]]
===== Events

Animation occurs in a VRML world, when the properties of a node, as
defined by its field values are changed. To change one of these values
it necessary to send the node an event created by another node. The
event received by a node is called the _eventIn_ and the event sent by a
node is called the _eventOut_. Each node type accepts and generates
different types of events, these are all listed in the
<<appxanod, Node Reference>> section in Appendix A. For
example, the *`SphereSensor`* node produces an eventOut called
*`rotation_changed`* which has a SFRotation value type.

A node field that has an implicit eventIn and eventOut is known as an
_exposedField_. The name of the eventIn is the same as the exposedField,
except that it has a *`set_`* prefix. Similarly, the name of the
eventOut has a *`_changed`*`+ +`suffix.

[[heading63]]
===== TimeSensor Node (line 86)

The *`TimeSensor`* node generates time related events that are used to
control the animations, essentially it is the clock for the VRML world.
In the example, the *`cycleInterval`* and *`loop`* fields are used
to define a 10 second animation that repeats continuously. It produces a
floating point *`fraction_changed`* eventOut value that changes from
0.0 to 1.0 over the 10 second period.

[[heading64]]
===== The Interpolator Nodes (lines 91-101)

Interpolator nodes describe the changes that occur during an animation.
They receive a single floating point eventIn value (*`set_fraction`*)
and using the *`key`* and *`keyValue`* values defined within the
node, compute a new (*`value_change`*) eventOut value, which can then
be used to change the properties of other nodes.

The *`key`* field is a list of floating point values, usually
representing points in fractional time. The *`keyValue`* field
contains a list of corresponding eventOut values for each key value.
When the eventIn value lies between key values, the eventOut value is
determined using linear interpolation.

In the example, the *`ColorInterpolator`* produces a colour value that
is used to vary the diffuse colour property of the sphere shape. The
*`keyValue`* field specifies four different RGB colours at fractional
times 0.0, 0.33, 0.66 and 1.0. So, at the start of the animation the
sphere is red, gradually changing to a green colour after 3.3 seconds,
then blue after 6.6 seconds, finally red again after 10 seconds, and
then repeating.

The *`OrientationInterpolator`* (line 98) defines a steady rotation of
the box shape about its vertical y-axis. One complete rotation
(2__[pi]__ radians) takes 10 seconds.

Other types of interpolator nodes are *`CoordinateInterpolator`*,
*`NormalInterpolator`*, *`PositionInterpolator`* and
*`ScalarInterpolator`*.

[[heading65]]
===== Routes

In order for a node to receive an event from another node, they must be
connected using a *`ROUTE`* statement. Routes are usually listed at
the bottom a VRML file. The general syntax is:

*`+ROUTE +`_`+Node1.eventOut+`_`+ TO +`_`+Node2.eventIn+`_*

Note that a node must be named with a *`DEF`*, for it to be able to
send or receive an event through a *`ROUTE`*. Also, the eventIn and
eventOut fields must have the same data type.

For example, the *`ROUTE`* statement in line 106, sends a SFFloat
value from the *`fraction_changed`* eventOut field of the
*`ClockSensor`* node (named *`Clock`*) to the *`set_fraction`*
eventIn field of the *`ColorInterpolator`* (*`NewColour`*) node. In
the next line, the SFColor value computed by the *`ColorInterpolator`*
node, as a result of receiving the *`set_fraction`* value, is sent to
the *`set_diffuseColor`* field of the sphere shape's *`Material`*
node (named *`SphereColour`*, line 46), causing the sphere's diffuse
colour properties to change.

Lines 110 and 111 contain the two *`ROUTE`* statements necessary to
link the *`Clock`* event to the *`OrientationInterpolator`*, and
then pass the SFRotation value it produces to the *`set_rotation`*
field of the *`Transform`* node that controls the orientation of the
box shape (line 60). Note how two separate animations statements share
the same eventOut of the *`TimeSensor`* node.

The final *`ROUTE`* statement in the VRML example (line 114), animates
the model of the Space Shuttle by sending the SFRotation value produced
by the *`SphereSensor`* node (line 80) to the Shuttle's
*`Transform`* node (line 76).

[[heading66]]
===== Summary

The two example worlds have been used to introduce many of the main
features of the VRML language, including:

* Creating shapes using *`Cube`*, *`Cone`*, *`Cylinder`* and
*`Sphere`*`+ +`geometry nodes.
* Defining backgrounds, light sources and viewpoints.
* Applying different material properties to shapes.
* Grouping nodes and applying transformations (translation, scale, rotation) 
* Texture mapping
* Hyperlinking
* Using inlines
* Naming nodes with *`DEF`* and reusing them with *`USE`*
* Creating animations and interactivity using sensor and interpolator
nodes and the *`ROUTE`* statement.

However, there are many more powerful features included in the VRML 2.0
specification. These are summarized in the following sections.

[[heading67]]
==== 3.3 Complex Shapes

The last example has shown how the primitive geometry nodes can be used
to create a variety of simple shapes. But, for more complex shapes, such
as the inlined Space Shuttle model, it is necessary to use the
*`IndexedFaceSet`* node, which explicitly defines the coordinates of
each face.

The following code demonstrates how the *`IndexedFaceSet`* node can be
used to define a single square shape on the Z=0 plane. First, the 3D
coordinates of the square are listed using the *`Coordinate`* node,
then the *`coordIndex`* field describes the order in which the
coordinate points should be joined together to form a single face. The
order is important because it defines which side of the face is the
front. By default, only the front face is rendered. Both sides are
rendered if the *`solid`* field is made *`FALSE`*.


[source]
....
#VRML V2.0 utf8
Shape {
    geometry IndexedFaceSet {
        coord Coordinate {
            point [ 0 0 0, 1 0 0, 1 1 0, 0 1 0 ]
        }
        coordIndex [ 0, 1, 2, 3, -1]  # -1 indicates end of index list
    }
}
....

There is also a *`PointSet`* node that defines a set of points and a
*`IndexedLineSet`* node for defining a 3D polyline. Text shapes are
added to a VRML world using the *`Text`* geometry node. The
*`FontStyle`* node is used to specify the characteristics of this
text.

Complex shapes can also be created using the *`Extrusion`* node. This
node describes a shape using a cross section that is extruded along a 3D
spine. Finally, there is the *`ElevationGrid`* geometry node that
provides an efficient method for specifying terrain surfaces, using a
regular grid of height points.


[[heading68]]
==== 3.4 Controlling Detail

The *`LOD`* (level of detail) node enables scenes to be displayed more
efficiently by the VRML browser, by providing high and low detailed
versions of an object. When a viewer is close to an object the high
detailed version is displayed, but when the viewer is far away the low
detailed version is used.

The *`LOD`* node groups together the nodes that form the alternative
representations of the object and specifies the viewing distances at
which the browser should switch between representations.

The *`Switch`* node can also be used to group together different
versions of an object. The value of the *`whichChoice`* field
determines which version is displayed.

[[heading69]]
==== 3.5 Other Nodes

[[heading70]]
===== Sound

Sound can be added to a VRML world with the *`Sound`* node. The node
specifies a sound's location, direction, loudness and range. The source
of the sound is defined using either an *`AudioClip`* or
*`MovieTexture`* node. The *`AudioClip`* node contains the URL of
the sound file (WAV or MIDI type 1 formats) and describes how it is
played.

[[heading71]]
===== Billboard

The *`Billboard`* node is a group node, that automatically rotates its
child shapes about a specified axis, so that the same side always faces
the viewer.

[[heading72]]
===== Geometric Property Nodes

The *`Color`* node contains a list of colours that can be used in
conjuction with the *`Coordinate`* node, to specify the colour
properties of individual vertices, faces, lines or points. The
*`TextureCoordinate`* node is used in the *`IndexedFaceSet`* and
*`ElevationGrid`* nodes to control texture mapping on these surfaces.
Vertex and face normals are used by the VRML browser to determine
surface shading. By default, the browser will generate the normals
automatically. However, it is possible to specify them explicitly within
the VRML file, using the *`Normal`* node.

[[heading73]]
===== Viewer Detection

Most VRML browsers implement collision detection in 'walk' or 'fly'
viewing modes, to prevent the user moving through objects. The
*`Collision`* grouping node can be used turn off collision detection
for its children or generate collision events that can be used for
animations or sound effects. A collision bounding box can also be
specified for a group of objects. This helps improve browser efficiency,
by reducing the number of collision detection calculations required. The
*`ProximitySensor`* node senses when the viewer enters or moves within
a defined box-shaped region. The *`VisibilitySensor`* detects when a
defined box-shaped region is visible from the viewer's current position
and orientation.

[[heading74]]
===== World and Avator Properties

Atmospheric effects can be added to the VRML world, using the *`Fog`*
node. The *`NavigationInfo`* node specifies the navigation mode that
should be used by the VRML Browser when it loads the world. The standard
navigation type values are 'EXAMINE', 'FLY', 'WALK', 'NONE' and 'ANY'.
The first three navigation types are discussed in
<<heading8, section 2.1>>, a 'NONE' value indicates that
navigation should be disabled. The *`NavigationInfo`* node is also
used to set the size and speed of the avator (the symbolic
representation of the viewer in the virtual world), whether the
headlight should be on or off and the maximum distance the avator can
see. Finally, the *`WorldInfo`* node can be used to provide the VRML
world with a title, and include additional information such as copyright
details.

[[heading75]]
==== 3.6 Scripts

The example world described above, used VRML sensor and interpolator
nodes to animate the objects. However, these nodes alone can only
describe relatively simple actions. More complex animations and
behaviours, such as a simulation of a bouncing ball or the switching of
a light source on and off, require special purpose sensors and
interpolators created with the general purpose *`Script`* node.

Like all VRML nodes, the *`Script`* node has a list of fields,
eventIns and eventOuts, forming the node's interface declaration. The
actions of the node are defined by a program script, that is specified
in the *`url`* field. Typically, a program script will describe how an
eventOut value is determined when an eventIn is received. Program
scripts may also communicate with external sources, such as a server or
a Java applet in a HTML page.

Program scripts can be written in any language that is supported by the
VRML browser (see <<guide2, section 2.0>>). This is usually Sun
Microsystem's Java language or Netscape's JavaScript (ECMAScript
standard). Some browsers support VRMLScript, a subset of JavaScript
language.

The Script node's *`url`* field specifies the location of the program
script code, or can contain the code itself if JavaScript is used. For
example, the following *`Script`* node uses JavaScript to describe the
same rotation action defined by the *`OrientationInterpolator`* node
in line 98 of the second example world.

[source]
....
DEF JSBoxRotation Script {
    eventIn SFFloat set_fraction
    eventOut SFRotation value_changed
    url "javascript:
        function set_fraction(fraction, timestamp) 
        {
            value_changed.x = 0;
            value_changed.y = 1;
            value_changed.z = 0;
            value_changed.angle = 2*Math.PI*fraction;
        }"
}
....


[[heading76]]
==== 3.7 Prototypes

VRML 2.0 allows new node types to be created with a *`PROTO`*
definition. Prototyping is a powerful mechanism that enables VRML worlds
to be described efficiently and new features to be added without
requiring any changes to the core VRML specification.

A protoype consists of the *`PROTO`* keyword followed by the name of
the new node type. Next is the interface declaration, which is enclosed
in square brackets. This is immediately followed by all the nodes and
routes that form the implementation of the prototype, enclosed within a
set of curly brackets. The interface declaration consists of list a
fields, exposed fields, eventIn and eventOuts and their default values.
The *`PROTO`* implementation accesses these values using the *`IS`*
syntax.

Once a prototype node has been declared, the new node type can be
throughout the rest of the VRML file in the same way as any other
standard node is used. For example, the following VRML file defines a
new node called *`ColouredBox`*, that creates a single Box shape. By
default, the *`diffuseColor`* property of the shape is grey, but it
can be changed with the *`colour`* field. The VRML file uses the
*`ColouredBox`* node only once, with the colour properties set to
yellow. Hence, the world will appear identical to example1.wrl.


[source]
....
#VRML V2.0 utf8
PROTO ColouredBox [
    field SFColor colour 0.8 0.8 0.8
] {
    Shape {
        appearance Appearance {
            material Material {
                diffuseColor IS colour 
           }
        }
        geometry Box { }
    }
}
ColouredBox { colour 1 1 0 }
....


A VRML file can use a prototype node defined in another file, by
including an *`EXTERNPROTO`* declaration. The *`EXTERNPROTO`*
declaration provides a name for the new node and specifies its
interface, but does not contain the implementation. Instead, the
*`url`* field provides the location of the file containing the
prototype definition. With this prototyping mechanism, libaries of
commonly used nodes can be created.


[[heading77]]
==== 3.8 Node Summary

The following table lists all 54 VRML 2.0 nodes by type. A full
definition of each node can be found in Appendix A.

[cols=3,opts=autowidth]
|=======
|*Grouping Nodes*         |*Sensors*                    |*Geometry*
|`Anchor`               |`CylinderSensor`           |`Box` 
|`Billboard`            |`PlaneSensor`              |`Cone` 
|`Collision`            |`ProximitySensor`          |`Cylinder` 
|`Group`                |`SphereSensor`             |`ElevationGrid` 
|`Inline`               |`TimeSensor`               |`Extrusion` 
|`LOD`                  |`TouchSensor`              |`IndexedFaceSet` 
|`Switch`               |`VisibilitySensor`         |`IndexedLineSet` 
|`Transform`            |                             |`PointSet`
|						              |*Interpolators*              |`Sphere`
|*Bindable Nodes*         |`ColorInterpolator`        |`Text`
|`Background`           |`CoordinateInterpolator`   |
|`Fog`                  |`NormalInterpolator`       |*Geometric Properties*
|`NavigationInfo`       |`+OrientationInterpol'r+`    |`Color` 
|`Viewpoint`            |`PositionInterpolator`     |`Coordinate` 
|                         |`ScalarInterpolator`       |`Normal`
|*Light Sources*          |                             |`TextureCoordinate`
|`DirectionalLight`     |*Property Nodes*             |
|`PointLight`           |`Appearance`               |*Appearance Properties*
|`SpotLight`            |`AudioClip`                |`ImageTexture` 
|                         |`FontStyle`                |`Material`
|*Child Nodes*            |`MovieTexture`             |`Script`
|`PixelTexture`         |`Shape`                    |`TextureTransform`
|`Sound`                |                             |
|`WorldInfo`            |                             |
|=======

Keywords: `DEF`, `EXTERNPROTO`, `IS`, `PROTO`, `ROUTE`, `USE`


[[heading78]]
==== 3.9 Online Tutorials

These web sites provide an excellent introduction to the VRML language:

*VRML 2.0 Interactive Tutorial* by the Interactive Systems and
Multimedia Group at the University of Minho, Portugal. +
_http://sim.di.uminho.pt/vrml/_ +
_http://www-venus.cern.ch/vrmltut/_ (mirror site)

*Floppy's VRML Guide* by James Smith, University of Surrey.
_http://www.ee.surrey.ac.uk/Personal/ee41rs/vrmlguide/index.html_

Advanced VRML 2.0 features are discussed in detail at these sites:

*Interfacing Java and VRML* by Alligator Descartes +
_http://www.hermetica.com/technologia/java/jvrml/_

*VRML Audio Tutorial* by DForm +
_http://www.dform.com/inquiry/tutorials/vrmlaudio/_

*Authoring Compelling, Efficient VRML 2.0 Worlds*, a SIGGRAPH 97 course
by David Story, Delle Maxwell and David Marsland. +
_http://cosmosoftware.com/developer/siggraph97/courses/compel/_

*Texture Mapping in VRML* by Cindy Reed-Ballreich +
_http://www.ywd.com/cindy/vrml_tex.html_ +

Useful information and links for VRML world developers can be found at:

*VR Universe* (Marcus Roskothen) +
_http://www.vruniverse.com/_

*Resources for VRML* *Developers* by Cosmo Software +
_http://cosmosoftware.com/developer_

[[guide4]]
=== 4. Creating VRML Worlds

* <<heading80, 4.1 Modelling Tools>>
* <<heading81, 4.2 File Translators>>
* <<heading87, 4.3 Validators>>
* <<heading91, 4.4 Object Libraries>>
* <<heading92, 4.5 Other Utilities>>

'''''

The previous section explained how the VRML language can be used to
define interactive 3D worlds. This section will describe some of the
many software tools and resources now available, that can be used to
help create these worlds. For the latest information on what is
available the reader should refer to the VRML Repository
(_http://www.sdsc.edu/vrml[www.sdsc.edu/vrml]_).

[[heading80]]
==== 4.1 Modelling Tools

Although its perfectly possible to create VRML world entirely using a
text editor, in practice some sort of modelling application is often
required, especially when more complex shapes are desired. Part Two of
this report has a detailed evaluation of some of the modelling tools
currently available, so they won't be discussed in detail here. However,
its worth noting that many standard modelling and CAD packages now
include a VRML export function, such as Alias|Wavefront Power Animator,
Kinetix 3D Studio Max (see <<heading48, 4.2>>), 3D/Eye TriSpectives.
and Bentley MicroStation. Although, some only support VRML 1.0.

[[heading81]]
==== 4.2 File Translators

A VRML file translator converts geometry data from another 3D file
format into VRML. All the VRML 2.0 translator programs listed below can
be obtained for free via the internet.

[[heading82]]
===== VRML 1.0 to VRML 2.0 Translators

There are currently two freeware command-line programs for converting
VRML 1.0 files into VRML 2.0. They are available for a variety of
operating systems.

[[heading83]]
===== vrml1to2 (Sony Corporation)

platforms: (SunOS 5.5), NEWS (NEWS-OS 6.0), SGI (Irix 5.3) and Windows
95/NT +
_http://vs.spiw.com/vs/vrml1to2E.html_

[[heading84]]
===== Vrml1ToVrml2 (Silicon Graphics)

platforms: SGI (Irix 5.3 & 6.2) and Windows 95/NT. +
_http://cosmosoftware.com/developer/utilities.html_

[[heading85]]
===== Crossroads 3D (Keith Rule)

Crossroads is a freeware Windows 95/NT application by Keth Rule, that
can translate between several different 3D file formats. It also allows
the user to view and maniplate a wireframe rendering of the model.
Version 1.0 Beta can generate a VRML 2.0 file from the following
formats: 3D Studio (.3ds), AutoCAD (.dxf), POVRay, RAW Triangle,
TrueSpace (.cob), VRML 1.0, Wavefront (.obj) and WorldToolkit (.nff).
However, it cannot currently read a VRML 2.0 file. Crossroads is
available from _http://www.europa.com/~keithr_.

image:http://www.agocg.ac.uk/train/vrml2rep/part1/fig41s.jpg[fig41s,width=200,height=150]

*Figure 4-1* Crossroads 3D

[[heading86]]
===== VRML Exporter Plug-in for 3D Studio MAX (Kinetix)

A free plug-in for 3D Studio MAX, that allows a model to be saved in
VRML 2.0 format, is available from the Kinetix Web site. The exporter
supports animations, sensors, sound, URL anchors, inlines, and LOD,
Billboard, Background and NavigationInfo nodes. The export dialogue also
includes various options to optimise the output, such as removal of
normals, indentation and numerical precison.

image:http://www.agocg.ac.uk/train/vrml2rep/part1/fig42s.jpg[fig42s,width=200,height=174]

*Figure 4-2* 3D Studio MAX VRML Exporter Dialogue Window

The plug-in is available from
_http://www.ktx.com/3dsmax/html/vrml_exporter.html_ (Windows NT). A
tutorial on how to use 3DS MAX for creating VRML 2.0 worlds can be found
at _http://www.dform.com/inquiry/tutorials/3dsmax/_.

[[heading87]]
==== 4.3 Validators

If you want other people to view and interact with your VRML world, it
is good idea to check that it conforms correctly to the VRML 2.0
specification. VRML browsers handle files containing invalid VRML in
different ways, some may show only part of the scene, while others may
display nothing at all. Even .wrl files generated by modelling tools or
translators can sometimes contain errors. A validator program reads a
VRML file, checks its syntax and reports any errors it may find.

[[heading88]]
===== Vorlon (Trapezium Development Company)

Vorlon is a free, command-line VRML 2.0 validator produced by the
Trapezium Development Company. It is a Java application, so can run on
any operating system (Windows, Macintosh, UNIX) that has a Java Virtual
Machine installed. Vorlon can check inline files, report the inclusion
of unnecessary nodes or fields and also handle gzipped compressed files.
It can check both local and remote VRML files, the latter by specifying
the file's URL. The software can be obtained from:

_http://www.trapezium.com/vorlon.html_

[[heading89]]
===== Shadows (Hans Holten-Lund)

Shadows is another freeware command-line validator, currently only
available for Windows 95 and NT. There is an option to print out the
parsed scene graph.

_http://www.it.dtu.dk/~hahl/shadows.html_

[[heading90]]
===== Using VRML Browsers to Check VRML

Some VRML browsers, such as Cosmo Player will create a log of any errors
they encounter when loading a VRML file. Inspecting this log file can
also help the user identify any problems with the VRML source.

[[heading91]]
==== 4.4 Object Libraries

Silicon Graphics has a large collection of VRML 2.0 objects, ranging
from household items to spacecraft, which are free for people to use in
their own worlds. The objects can be obtained from
_http://cosmosoftware.com/developer/models.html_. There is also a
selection of textures.

[[heading92]]
==== 4.5 Other Utilities

*Rotor* by David Harvey is a Java applet that allows the user to create
custom VRML extrusion shapes, by defining polygon cross-sections with
the mouse pointer. The VRML 2.0 source is then displayed. The applet can
be viewed at _http://web.ftech.net/~honeyg/rotor.htm_.

*SitePad* from Modelworks Software is a shareware VRML file editor, that
includes colour coding and various other useful utilities which make
writing a VRML file easier, see _http://www.modelworks.com_ for more
details.

*VRMLPlot.m* by Dr. Craig Sayers, is a Matlab function for generating
interactive 3D VRML 2.0 graphs and animations from statistical data,
using the Matlab software. The source code is available from
_http://www.dsl.whoi.edu/~sayers/VRMLplot_.

image:http://www.agocg.ac.uk/train/vrml2rep/part1/fig43s.jpg[fig43s,width=200,height=138]

*Figure 4-3* SitePad Pro


[[guide5]]
=== 5. Publishing Worlds on the Web

* <<heading94, 5.1 MIME type>>
* <<heading95, 5.2 Integrating VRML with HTML>>
* <<heading97, 5.3 File Optimization>>

'''''

[[heading94]]
==== 5.1 MIME type

When a Web server sends a document to a browser, it includes the data's
MIME (Multipurpose Internet Mail Extensions) type in the header
information. This enables the browser to determine how to display the
document correctly, for example, by starting a helper application or
plug-in.

The official MIME type for VRML files is:

*`+model/vrml+`*

Where *`model`* is the MIME major type for 3D data descriptions and
*`vrml`* is the minor type for VRML documents.

For compatibility with the earlier VRML 1.0 specification, most browsers
also accept the experimental VRML MIME type, *`+x-world/x-vrml+`*.

A web server needs to be able to recognize VRML files, in order for it
to send the correct MIME type. It does this by associating the .wrl file
extension with the VRML MIME type. This may require the Web Master to
add the MIME type to one of the server's configuration files. However,
more recent versions of the server software are likely to be already
configured with the VRML MIME type and no changes will be necessary. For
example, the Apache 1.2.5 recognises the VRML MIME type.

The Web browser must be also configured so that it associates the
installed VRML browser or plugin with the VRML MIME type. Normally, this
is done automatically when a VRML browser application is installed,
however sometimes it may be necessary to configure the browser manually.
This will usually be described in VRML browser installation
instructions.

[[heading95]]
==== 5.2 Integrating VRML with HTML

The availability of plug-in type VRML browsers enables content creators
to combine both HTML and VRML elements into a multimedia document, by
the use of frames and the *`EMBED`* tag. As demonstrated in Figures
1-1 and 2-3.

The *`EMBED`* element is a Netscape extension to HTML, that allows
objects, such as video, sound, as well as VRML, to be incorporated
within a HTML page and displayed by the appropiate plug-in application.
For example, the following markup in a HTML document will cause a VRML
world called model.wrl to be embedded within the document, with the size
of the plug-in set at 400 by 300 pixels.

[source]
....
<EMBED src="model.wrl" WIDTH=400 HEIGHT=300>
....

Sometimes it is useful to have the VRML browser plug-in display the 3D
scene without also showing its navigation user interface. This can be
achieved with the Cosmo Player 2.1 or WorldView plug-in, by adding the
attribute *`+VRML-DASHBOARD="FALSE"+`* to the *`EMBED`* element. A
similar affect can also be produced by adding the
*`+NavigationInfo {type "NONE"}+`* node to the original VRML file.

The *`EMBED`* element is supported by the latest versions of both
Netscape Navigator and Microsoft Internet Explorer. The new HTML 4.0
standard introduces a more sophisticated *`OBJECT`* element as a
replacement for the *`EMBED`* element.

There is a practical, as well as aesthetic, reason for embedding a VRML
world within a HTML document. The size of the VRML browser window is a
factor in the time it takes for it to render the 3D scene. Therefore,
making the VRML window smaller may make movement through the scene
noticeably smoother, especially for more complex worlds.

[[heading96]]
===== Frames

Another Netscape extension to HTML, now widely supported and used, are
frames. This enables the main Web browser window to be divided into a
number of smaller windows or frames, each containing its own unique
document. Frames usually contain HTML documents, but they can equally be
used to display VRML worlds.

To create a HTML document containing frames, the *`FRAMESET`* and
*`FRAME`* elements are used. The *`FRAMESET`* element defines the
layout of the frames and the *`FRAME`* element specifies the URL of
the document that each frame should contain. The *`FRAME`* element
also includes a *`NAME`*`+ +`attribute, which uniquely identifies each
frame window. This enables a document to be displayed within a specific
named frame when a HTML hyperlink is activated, by using the
*`TARGET`* attribute:

[source]
....
<A href="doc.html" TARGET="frame_name"> ... </A>
....

 +
VRML hyperlinks can also be targeted to particular frames, using the
*`parameter`* field of the `Anchor` node. The VRML equivalent of the
above would be:

[source]
....
Anchor {
    url     "doc.html"
    parameter   "target=frame_name"
    children [
        # ..hyperlinked objects
    ]
}
....

 +
It is also possible with some VRML browsers, to change or set the
current viewpoint using the *`+<A>+`* HTML element. For example, if a
HTML document contains the following hyperlink:

[source]
....
<A href="model.wrl#view1"> ... </A>
....

 +
And model.wrl contains a *`Viewpoint`* node named *`VIEW1`* using
the *`DEF`* syntax. The VRML browser will display the scene from that
viewpoint, when the HTML hyperlink is activated. If this technique is
used with frames, then the VRML browser may change the viewpoint,
without reloading the whole .wrl file.


[[heading97]]
==== 5.3 File Optimization

If you want your VRML world to be viewed by as many people as possible,
then you need to consider the size and complexity of your .wrl file.
Large VRML files take longer to download, so discouraging users with low
speed internet connections. Complex worlds usually create extra
processing demands on a computer, so users with less powerful machines
may find interacting and moving through these worlds, slow and
unresponsive. However, this does not necessarily mean that user-friendly
worlds must be simple. This section will describe some of the various
techniques that can be used to optimize the VRML file, so allowing the
virtual world to be both engaging and feature-rich, but also efficient
in the use of computer and network resources.

[[heading98]]
===== Reduce Detail

Try to avoid unneccessary detailing when creating objects. More
geometric detailing means more polyons which in turn means slower
rendering by the browser. The skill is deciding exactly what is the
right amount of detail. Using less than a 1000 polygons in the model
will ensure that your world can be viewed by a wide audience, but you
may feel that it inadequately represents your design, so some
experimentation may be necessary.

Problems often arise when using models originally created for other CAD
or rendering packages. These applications generally use models with high
levels of detail, since geometric accuracy is more important than
real-time rendering. Therefore, worlds directly converted from other 3D
file formats can sometimes be quite large and difficult to view
interactively with a VRML browser.

Texture mapping provides a useful and efficient way of indicating
complexity, without explictly modelling the geometry. For example, a
building can be created by applying images of windows and doors to the
faces of a box shape. However, care should be taken to reduce the size
of the image files to a minimum by reducing the resolution, colour depth
or using compression.

The number of polygons can also be further reduced by using the
*`Billboard`* node wherever possible. Since a billboard object always
faces the viewer, only one side needs to be modelled or texture mapped.
Ideal for representing objects such as trees and street signs.

[[heading99]]
===== Level-of-Detail and Inlines

The *`LOD`* node utilizes the fact that objects farther away from the
viewer do not need to be drawn with as much detail as those closer to
the viewer. By defining alternative representations of the same object
with different levels of detail, the world builder can help the VRML
browser display the scene efficiently. The amount of detail displayed at
a time can also be controlled by using the *`VisibilitySensor`* node
or by defining a visibility limit in the *`NavigationInfo`* node.

Many VRML browsers implement delayed loading of VRML files referenced by
*`Inline`* nodes. This means that the user can start viewing and
interacting with the base scene whilst the inlined objects are being
fetched. Once the inlined objects have been retrieved they are added to
the scene. Therefore, using inlines in a VRML file can help reduce the
time taken for the browser to start displaying a world, even though the
scene may be initially incomplete.

[[heading100]]
===== Optimization of VRML Code

A VRML file can be made more efficient by ensuring that:

* Primitive shape nodes, such as *`Cube`* and *`Sphere`*, are used
  instead of explicitly defining the face coordinates with a
*`IndexedFaceSet`* node. As well as requiring fewer lines of code, a
  VRML browser may be able to display a primitive shape more efficiently,
  for example the number of faces on a sphere may be changed dynamically
  according to its proximity to the viewer.
* Node fields that do not change their default values are not included.
* Nodes are reused using the *`+DEF/USE+`* syntax. Careful structuring
  of the scene graph hierarchy can sometimes reduce the number of nodes
  required. Also consider using *`PROTO`* or *`EXTERNPROTO`* for
  declaring similar types of objects.
* Wherever possible, turn off rendering of back faces using
*`+solid TRUE+`*, in *`ElevationGrid`*, *`Extrusion`* and
*`IndexedFaceSet`* nodes.

VRML worlds generated from other 3D file formats, using geometry
translator utilities, can often produce inefficiently written VRML
files. So in addition:

* Numerical values are written with unnecessarily high levels of
  precision. For example, 10 is written as 10.000000.
* Face normals are included, when the values can be left to the browser
  to calculate.
* Nodes are included that don't do anything useful, such as empty
  transformations.

*Chisel* is a free command-line java application for optimizing VRML 2.0
files, from the Trapezium Development Company. They claim it can reduce
the size of VRML files generated using 3D Studio MAX by upto 40%, by
removing unnecessary digits, white spaces, polygons, redundant fields
and extraneous nesting levels. The program was not yet available at the
time of writing. See _http://www.trapezium.com/chisel.html_ for more
details.

[[heading101]]
===== File Compression

The easiest way to significantly reduce the size of a VRML file
(although not its complexity) is to compress it. Gzip compression is the
technique most widely used for VRML files. When compressed, the VRML
files are given the extension .wrl.gz or .wrz. Most VRML browsers will
automatically uncompress these files when they are loaded. The GNU gzip
command-line compression program is freely available for most computer
platforms from _http://www.gzip.org_ and many other sites such as HENSA
(_http://www.hensa.ac.uk[www.hensa.ac.uk]_).

A binary version of VRML will also reduce the size of the file. But this
format is still under development (see
<<heading103, section 6.1>>) and is not widely supported.


[[guide6]]
=== 6. Future Development

* <<heading103, 6.1 Development Process>>
* <<heading104, 6.2 Mailing List>>
* <<heading105, 6.3 Conclusion>>

'''''

[[heading103]]
==== 6.1 Development Process

VRML 2.0 is now an International Standard (VRML97). Changes to this
specification are unlikely in the immediate future. This is because new
functionality can be added to VRML through the use of scripts and
prototypes, without requiring any changes to the core language itself.

Any changes to the specification need to be approved by the VRML
Consortium, which is advised by the VRML Review Board. Proposals for new
standards or recommended practices are developed by the Review Board
with the help of Working Groups. Currently there are working groups
investigating the follow areas:

* *External Authoring Interface (EAI)* +
Developing standard techniques for allowing Java applets in a HTML
document to control a VRML world. +
 +
* **Compressed Binary Format +
**Developing a binary format for VRML files will significantly reduce
file size and so allow quicker downloading and parsing.* +
* +
* **Streaming +
**To enable the streaming of geometry and data such as video and audio
into a VRML world, so that it can be continuously updated. +
 +
* **Multi-User Worlds +
**To define a framework and interface for supporting the virtual
presence and interaction of many users in a single scene at the same
time.* +
* +
* **Compatibility +
**Encourage compatibility between different VRML browsers, so that they
show and implement worlds in the same way.* +
* +
* **Representation +
**Develop standard methods for representing humanoid and organic
forms.* +
* +
* **Reusable Elements +
**For example, user interface widgets and universal media libraries than
can be held locally.

More information about the Working Groups can be found at: +
_http://www.vrml.org/WorkingGroups_

*Java3D* is an application programming interface (API) being developed
by Sun Microsystems for displaying 3D graphics using their Java
Language. It will support the VRML 2.0 specification and include a VRML
geometry loader and browser tool in the API utility library. See
_http://java.sun.com/products/java-media/3D/_ for more details.

[[heading104]]
==== 6.2 Mailing List

The main discussion forum and source of the latest information about the
development of VRML is the *www-vrml* mailing list. To join the list,
send an email to _Majordomo@vrml.org_. In the message body write:

`+ +`*`+subscribe www-vrml+`*

This is a heavily used mailing list, so you may prefer to receive the
digest version instead. Details about this and how to unsubscribe are
sent to you when you join.

Archives of the mailing list are available at
_http://www.vrml.org/www-vrml/hypermail/_

[[heading105]]
==== 6.3 Conclusion

VRML has now been firmly established as the standard format for 3D
content on the Web. This has been achieved both through its wide spread
use and its formal adoption as an international standard by the ISO. It
is supported by many of the major companies in the computer industry,
through the VRML Consortium. But the involvement of many individuals
just commited to the idea of VRML, has played an important part in its
development.

The current and future success of VRML depends on two factors, the
availability of software and the availability of worlds. This report has
shown that many different VRML browsers are now available for viewing
VRML 2.0 worlds. They can be obtained for Microsoft Windows (usually
32-bit only), Apple Mac and UNIX systems, and probably just as
important, are usually free. The fact that VRML plug-ins are now
standard components of the major Web browser distributions is another
important factor in making VRML popular. However, there are still a
significant problem with incompatibilty, a VRML world may look or behave
differently depending on the browser used.

A wide variety of VRML modelling applications and tools are now also
available, making world creation easier and not just limited to people
with a text editor and an in-depth knowledge of the VRML specification
(although that always helps). They range from simple file translators to
sophisticated modelling packages, some of which are evaluated in Part
Two of this report. Its also worth noting that many widely-used CAD and
modelling packages now include VRML export facilities (although often
VRML 1.0 only). So, VRML could also become the main file format for
exchanging 3D models. However, many of these programs still produce
unncessary large or sometimes even incorrect VRML files. Hopefully,
these problems will be sorted out in the near future, although its
always worth remembering that CAD models don't always make good
interactive worlds since they contain too much detail.

The success of a VRML world depends not only on its content, which of
course is very important, but also on the time it takes for it to
download and whether the user's computer has the power to show it
satisfactorily. Large, over complex VRML worlds make interaction slow,
unresponsive and ultimately disappointing to everyone except those with
the most poweful workstations. <<guide5, Section 5>> discussed
various techniques for making VRML worlds more efficient. However,
computers are becoming more powerful, Intel's MMX technology and the
availability of affordable 3D accelerator cards for PCs, creates a large
potential audience for VRML.

Will VRML be overtaken by another technology? That is of course
possible, but VRML is already well established and can easily be
extended to include new features. New technologies, such as Java3D or
for creating multi-user 3D worlds will work with VRML rather than
attempt to replace it.

Already there are many excellent examples on the Web, that show the
power and the exciting potential of the VRML 2.0 language. However, more
worlds are still required. Hopefully, this report has provided you with
the information necessary to start viewing and creating these worlds.

[[guide7]]
=== 7. Further Information

* <<heading107, 7.1 Internet Resources>>
* <<heading108, 7.2 Books>>
* <<heading109, 7.3 SIMA Publications>>

'''''

[[heading107]]
==== 7.1 Internet Resources

The *VRML Repository* at the San Diego Supercomputer Centre maintains an
up-to-date list of most VRML software and resources. +
_http://www.sdsc.edu/vrml_

*VRMLworks* provides useful information on a wide range of VRML topics. +
_http://hiwaay.net/~crispen/vrmlworks/_

The principal USENET newsgroup for discussions on VRML is
*link:news:comp.lang.vrml[comp.lang.vrml]*. +
The *Frequently-Asked-Questions (FAQ)* list for this newsgroup is located at +
_http://hiwaay.net/~crispen/vrmlworks/faq/_

The *VRML Consortium*'s site contains official VRML information
_http://www.vrml.org_

Latest VRML news can be found at the *Mining Co*. VRML site +
_http://vrml.miningco.com_

*Networked Virtual Reality Resource Centre for Arts and Design* at the
University of Teesside, _http://vr3.tees.ac.uk/rachael/_

[[heading108]]
==== 7.2 Books

*VRML 2.0 Sourcebook* (2nd Edition) +
Andrea Ames, David Nadeau, John Moreland +
John Wiley & Sons, 1997 +
ISBN 0-471-16507-7

*The Annotated VRML 2.0 Reference Manual* +
Rikk Carey, Gavin Bell +
Addison-Wesley Developers Press, 1997 +
ISBN 0-201-41974-2 +
This book can also be viewed online at _http://www.best.com/~rikk/Book/_

*VRML 2.0 Handbook: Building Moving Worlds on the Web* +
Jed Hartman, Josie Wernecke +
Addison-Wesley, 1996 +
ISBN 0-201-47944-3

*Java for 3D and VRML Worlds* +
Roger Lea, Kouichi Matsuda, Ken Miyashita +
New Riders Publishing, 1996 +
ISBN 1562056891

*Teach Yourself VRML 2 in 21 days* +
Chris Marrin, Bruce Campbell +
Sams.net Publishing, 1997 +
ISBN 1-57521-193-9

*Late Night VRML 2.0 with Java* +
Bernie Roehl, Justin Couch et al. +
Macmillan Computer Publishing, 1997 +
ISBN 1562765043

*VRML Clearly Explained* (2nd Edition) +
John Vacca +
AP Professional, 1998 +
ISBN 0-12-710008-3

[[heading109]]
==== 7.3 SIMA Publications

*The Virtual Reality Modelling Language in Art and Design Higher Education*, +
Neil Ashdown, SIMA Report No 17, Feb 1996 +
_http://www.man.ac.uk/MVC/SIMA/vrart/title.html_

*The DESIGN of Virtual Environments with Particular Reference to VRML*, +
Stephen Boyd Davis, SIMA Report No 27, June 1996 +
_http://www.man.ac.uk/MVC/SIMA/vrml_design/title.html_

*Briefing Report: An Introduction to VRML*, +
Sue Cunningham, May 1997 +
_http://www.man.ac.uk/MVC/SIMA/articles/VRML.html_


=== _Part Two: *An Evaluation of VRML Modelling Tools*_

*Part Two* +
*An Evaluation of +
VRML Modelling Tools* +

'''''

* <<eval1, 1. Introduction>>
* <<eval2, 2. Software Selection>>
* <<eval3, 3. Evaluation Methodology>>
* <<eval4, 4. Software Overview>>
* <<eval5, 5. Comparison of Features>>
* <<eval6, 6. Conclusions>>


[[eval1]]
=== 1. Introduction

* <<headinge2, 1.1 VRML Modelling Tools>>
* <<headinge3, 1.2 Evaluation Objectives>>
* <<headinge4, 1.3 Report Structure>>

'''''

[[headinge2]]
==== 1.1 VRML Modelling Tools

A VRML modelling tool is a software application that enables the user to
create a 3D object or world, without having to directly edit a VRML file
in a text editor. The availability of these applications is essential
for the development of VRML, since creating VRML worlds "by hand"
becomes much more difficult as the scene complexity increases, and also
more importantly, it enables more people to become world builders by
making the whole modelling process easier.

A wide range of applications could be described as VRML modelling tools,
including file translators which convert model data from other 3D file
formats into VRML, or specialised geometry generators, for example,
terrain generators. However, for the purposes of this evaluation we will
define a VRML modelling tool as:

_A stand-alone application with an advanced graphical user interface
(GUI), that allows the user to interactively create and view custom 3D
shapes and worlds, and which can save and reload the scene data in a
VRML file format._

This report will only be evaluating VRML 2.0 modelling tools. So,
applications that can only save in the VRML 1.0 format will not be
considered.


[[headinge3]]
==== 1.2 Evaluation Objectives

The number of VRML 2.0 modelling tools becoming available is steadily
increasing, however they can also vary considerably in functionality and
price. This report will evaluate a sample of these applications. It will
compare their features, test how easy they are to use and identify how
much of the VRML 2.0 specification they support.

Of course, the features of each product will change as newer versions
are released and there will be other applications not included in this
evaluation. Nevertheless, this report should still provide a useful
indication of the range and type of applications available and the sort
of features to look for.

[[headinge4]]
==== 1.3 Report Structure

The report is organised as follows:

Section 2, _<<eval2, Software Selection>>_ describes the testing
platform and the process for selecting the modelling tools for
evaluation.

Section 3, _<<eval3, Evaluation Methodology>>_ outlines the testing
procedure and the criteria used in the evaluation.

Section 4, _<<eval4, Software Overview>>_ discusses the main
features of each application.

Section 5, _<<eval5, Comparison of Features>>_, presents the data in a table.

Section 6, _<<eval6, Conclusions>>,_ summarizes the results of this evaluation.


[[eval2]]
=== 2. Software Selection

* <<headinge6, 2.1 Testing Platform>>
* <<headinge7, 2.2 Selection Procedure>>

'''''

[[headinge6]]
==== 2.1 Testing Platform

All the modelling applications were tested on a single platform:

* Opus PC with Intel Pentium 166 MHz MMX processor
* 64 MB RAM
* 1024x768 resolution display with 24-bit colour
* Microsoft Windows95 operating system

This single platform was chosen as it was considered to be a fairly
typical (or a least not unusual) configuration in the Higher Education
sector.

[[headinge7]]
==== 2.2 Selection Procedure

The first step was to identify the software to be evaluated. To be
selected, an application had to satisfy four main requirements:

* It conformed to our definition of a _VRML modelling tool_ (see Section 1.1).
* The generated output could be saved in a VRML 2.0 format.
* The software was available for downloading via the internet, as either
freeware, shareware or in trial form
* It had to operate on the testing platform, i.e. a Windows95 version
had to be available.

Seventeen VRML modelling applications were initially identified, using
information provided at SDSC VRML Repository (_www.sdsc.edu/vrml_).
However, not all of these packages satisfied the conditions described
above. For example, some were only VRML 1.0 compliant, while others just
had VRML output capabilities. So, following further investigation, this
list was reduced to six. The packages chosen for evaluation were:

Caligari *TrueSpace 3.0* +
Ligos *V-Realm Builder 2.0* +
Paragraph *Internet Space Builder 2.1* +
Platinum *VRCreator 2.0* +
Rendersoft *VRML Editor 1.3* +
Sculptware LLC *Site Sculptor 1.0*

==== _Part Two: *An Evaluation of VRML Modelling Tools*_


[[eval3]]
=== 3. Evaluation Methodology

* <<headinge9, 3.1 Evaluation Criteria>>
* <<headinge10, 3.2 Testing Procedure>>

'''''

[[headinge9]]
==== 3.1 Evaluation Criteria

When evaluating each modelling application, the following features were
considered:

*File import and export capabilities.* +
Can the software load or save worlds in VRML 1.0, VRML 2.0, gzip
compressed VRML (.wrl.gz .wrz) or in any other commonly used 3D formats,
such as 3D Studio (.3ds) or AutoCAD (.dxf)?

*The user interface.* +
Does the application supports multiple viewport windows, real-time
shading or context-sensitive help? How easy is it to create a scene or
assign properties to it? Can the GUI be customised?

*Modelling features.* +
Can the user create objects from primitive shapes such as spheres, cones
or boxes? Is it possible to define and edit individual polygons or
surfaces? Does the modeller include Boolean operations?

*File formats.* +
Which file formats does the application support for texture images,
movies and audio?

*Supported VRML 2.0 nodes.* +
Can the user add URL anchors to objects, use inlines or create objects
using Billboard, ElevationGrid and Text nodes? Can worlds contain
multiple light sources and viewpoints? Does the application also support
Background, Fog, LOD (level-of-detail) and Sound nodes?

*Animation.* +
Does the modeller support VRML 2.0 animation and object behaviours? Can
they be created with a key-frame editor or with JavaScript?

*Libraries.* +
Does the software package include example objects, materials or
textures?

*System Requirements.* +
Which computer platforms is the software available for? Does it utilize
any hardware accelerated graphics capabilities.

*Cost.* +
Note that the report uses the price of the software quoted on the
company's web site in U.S. dollars, at the time of the evaluation. This
value will be subject to change and/or discounts and is included simply
to illustrate the relative prices of the different products.

[[headinge10]]
==== 3.2 Testing Procedure

Once the selected modelling package had been downloaded from its
internet site (the URLs are included in each product summary in Section
4), and installed on the Windows95 testing platform, it was evaluated
against the criteria set out above. To test the accuracy of any
generated VRML files, the CosmoPlayer browser plugin was used to view
the files.


[[eval4]]
=== 4. Software Overview

* <<headinge12, 4.1 Caligari TrueSpace 3.0>>
* <<headinge13, 4.2 Ligos V-Realm Builder 2.0>>
* <<headinge14, 4.3 Paragraph Internet Space Builder 2.1>>
* <<headinge15, 4.4 Platinum VRCreator 2.0>>
* <<headinge16, 4.5 Rendersoft VRML Editor 1.3>>
* <<headinge17, 4.6 Sculptware LLC SiteSculptor 1.0>>

'''''

[[headinge12]]
==== 4.1 Caligari TrueSpace 3.0

web site: _http://www.caligari.com_ +
platforms: Windows 95, Windows NT 4 +
cost: $495 +
download: _http://www.caligari.com/products/_ +

image:http://www.agocg.ac.uk/train/vrml2rep/part2/fig1s.jpg[fig1s]

*Figure 1* TrueSpace

Caligari TrueSpace is not designed primarily as a dedicated VRML world
builder, but rather as a complete modelling and rendering package. So,
despite comprehensive modelling capabilities, it lacks many of the
features one would require in creating a fully featured VRML world.

The package has extensive import capabilities. It can import model or
scene data in the VRML 1.0 format (.wrl), including gzipped files (.gz,
.wrz), 3D Studio (.3ds, .prj, .asc), AutoCAD (.dxf), Lightwave (.lwb),
Videoscape (.geo), Wavefront (.obj) and Imagine (.iob), as well as its
own Caligari scene format (.scn). It can save data in Calgari (.scn),
VRML 1.0 and 2.0 formats. However, note that TrueSpace 3.0 cannot load
VRML 2.0 files, giving the user no opportunity to amend existing scenes.

The graphical user interface is distinctive, with intensive use of
icons, which can take a while to get used to. However, the online help
is extensive and the animated task tutorials are particularly good. The
software supports multiple floating viewport windows, with a choice of
wirefame or real-time shaded rendering by either Direct3D or 3DR.

The modelling tools are extensive, including Boolean operations, spline
based modelling, collision detection for positioning, meta-balls and
surface particles. TrueSpace supports texture image files in BMP, TGA,
TXR and JPEG formats and AVI for video. The package also includes an
object library.

Animations are created with key-frame, dynamics and inverse kinematics
tools, However, it was not possible to ascertain whether animations were
exported to the VRML 2.0 file, since the save command was disabled on
the trial version of the software that was tested.

TrueSpace does support URL anchors, inlining and LOD and includes a
publish command for transferring the output VRML files to the web
server. It can also optimise VRML files when saving by removing extra
spaces and unnecessary numerical precision.

[[headinge13]]
==== 4.2 Ligos V-Realm Builder 2.0

web site:

_http://www.ligos.com_

platforms:

Windows 95, Windows NT

cost:

$495

download:

_http://www.ligos.com/vrml/vproduct.htm_

image:http://www.agocg.ac.uk/train/vrml2rep/part2/fig2s.jpg[fig2s]

*Figure 2* V-Realm Builder

V-Realm Builder is a dedicated VRML authoring tool, that allows the user
to take full advantage of all the features of the VRML 2.0 language. The
user interface includes up to four viewport windows with real-time
rendering (utilising OpenGL), and a graphical representation of the VRML
scene graph. The user can directly edit this scene graph, inserting,
moving, deleting nodes or changing node properties.

The application can import VRML 1.0, VRML 2.0, 3D Studio (.3ds),
TrueSpace (.cob), Wavefront (.obj) and RAW (.raw) files, though the lack
of an AutoCAD (.dxf) import facility is surprising. A world can only be
saved in VRML 2.0 format (although this was disabled on the
demonstration version tested), with optional gzip compression.

V-Realm's object manipulation interface is well designed and easy to
use. An object is selected by either clicking on it or its name in the
node tree. Scaling, moving or rotating is achieved by manipulating the
easily identifiable tags around the selected object. A similar
manipulator tool exists for defining light sources.

The package includes a large customisable library of objects, material
properties and textures, which the user can drag-and-drop into the
scene. It accepts textures in GIF, JPEG and SGI RGB file formats, movies
in MPEG and AVI formats, and audio as WAV and MIDI files.

To create custom shapes and surfaces, there are interactive editors for
defining indexed face sets, extrusions and elevation grids. Although no
Boolean operations are available. Object behaviours and animations are
added to a world via a key-frame editor. The user can also insert
sensors, ROUTES and even JavaScript nodes directly into the scene graph.
V-Realm also supports URL anchors, LOD and PROTO nodes.

[[headinge14]]
==== 4.3 Paragraph Internet Space Builder 2.1

web site:

_http://www.paragraph.com_

platforms:

Windows 95, Windows NT

cost:

$89.95

download:

_http://cosmosoftware.com/products/designer/brief.html*_

image:http://www.agocg.ac.uk/train/vrml2rep/part2/fig3s.jpg[fig3s]

*Figure 3* Internet Space Builder

Internet Space Builder (ISB) will import VRML 2.0 (.wrl), D96 (.d96) and
Paragraph's Virtual Home Space Builder files (.mus) as complete scenes
and 3D Studio files (.3ds) as objects to be placed within an existing
scene. Worlds can only be saved in the VRML 2.0 format, with the option
of gzip compression.

ISB has a very easy to use interface, which takes full use of the
drag-and-drop method for adding objects and properties to the scene.
Particularly useful are the WYSIWYG shapes, objects and texture
galleries which allow the user to view and rotate it prior to insertion
into the scene.

The scene can be viewed from multiple viewport windows, including plan
and perspective views. The world is also represented as a hierarchical
scene tree, although it does not seem to be based on VRML scene graph.
The demonstration version of ISB, which was tested, was restricted to
scenes containing a maximum of 1400 faces, 7 textures, 2 pictures, 2
movies, and 2 URLs.

For object creation and manipulation, ISB allows the extrusion of 2D
shapes to form 3D objects, basic Boolean operations (union and
subtraction) and standard transformations and grouping.

ISB has the most advanced texture mapping features of all the packages
evaluated, but also the easiest to use. The user can interactively edit,
apply transformations, vary transparency and even animate a texture. ISB
accepts BMP, GIF and JPEG files for textures.

However, interactivity in ISB is limited to animated textures and URL
anchors. Version 2.1 does not support any behaviours or script nodes,
although Paragraph intend to include these in a later release of the
software. There also seems no obvious facility for adding or moving
light sources.

Other features include support for WAV and MIDI sound, backgrounds and a
publish tool for transferring files to a web server.

In conclusion, Internet Space Builder is a reasonably well featured
package, backed up by good downloadable help pages from the Paragraph
web site. There is a room creation emphasis, for example floor plans and
household objects. It does lack some advanced VRML 2.0 features, which
should be rectified with later releases. Its drag and drop interface
makes many awkward tasks simple and multiple floating viewports make
visualisation easy.

*Note that Silicon Graphics, Inc. has recently acquired Paragraph
International and Internet Space Builder has been renamed HomeSpace
Designer.

[[headinge15]]
==== 4.4 Platinum VRCreator 2.0

web site:

_http://www.platinum.com_

platforms:

Windows 95

cost:

Learning Edition FREE, Personal Edition $115

download:

_http://www.platinum.com/products/appdev/vream/vrc_ps.htm_

image:http://www.agocg.ac.uk/train/vrml2rep/part2/fig4s.jpg[fig4s]

*Figure 4* VRCreator

VRCreator is available in two versions, Learning Edition and Personal
Edition. Platinum have made the Learning Edition freely available on
their web site, and can be used without any time limit. The main
differences in functionality between the two versions are:

* The Learning Edition restricts the user to a maximum of 300 objects in
a scene and the number of animations.
* The Learning Edition can read VRML 1.0 and 2.0 files and save only in
the VRML 2.0 format. Whereas the Personal Edition can also import
AutoCAD (.dxf), 3D Studio (.3ds), TrueSpace (.cob), Wavefront (.obj) and
Lightwave (.lw, .lwo, .lws) files.
* The embedded 3D Modeller application, for creating custom objects, is
only available in the Personal Edition.

VRCreator 2.0 is certainly one of the most fully featured of the
packages tested, allowing the user to use the full extent of the VRML
2.0 specification. Although it should be noted that the author
encountered difficulties importing some VRML 2.0 files created with
other world building software.

An excellent user interface provides the user with a wealth of scene
information and commands, without being cluttered. The scene can be
viewed from multiple viewport windows, with real-time rendering (by the
use of Microsoft's Direct3D rendering engine). Navigating the scene is
made easy via a floating navigation bar and a small cubic icon provides
a simple and quick reference to one's orientation.

An extensive library of hundreds of 3D objects, colours, textures,
sounds and behaviours (such as sensors, movements and even explosions)
are accessed via the "VR Binder". Adding an object to the scene, or
applying a particular property is achieved through a simple
drag-and-drop action.

The VR Binder also contains a representation of the VRML scene graph,
through which the user can directly edit the nodes and their values,
tools for creating objects, applying transformations, material
properties and animations (via a key-frame editor), and a embedded VRML
2.0 browser for navigating and viewing the animated scenes.

With VRCreator, the user can define multiple viewpoints, lighting and
URL anchors, use level-of-detail and switch nodes, apply BMP and JPEG
textures, and insert ROUTES and edit JavaScript scripts. Other useful
features include a polygon counter, debug window for checking VRML
syntax, tutorial wizards, a facility for uploading files to a web
server, and file optimisation by removing extra white space.

VR Creator 2.0 is the most fully featured and easy to use of the systems
tested. The behaviour library makes the often complex task of adding
time and motion characteristics to scene simple. The large object
library provides an excellent resource to a potential world builder,
however problems loading VRML worlds from other software packages will
need to be fixed.

[[headinge16]]
==== 4.5 Rendersoft VRML Editor 1.3

web site:

_http://homer.pacific.net.sg/~jupboo_

platforms:

Windows 95, Windows NT 4.0

cost:

$15

download:

as above

image:http://www.agocg.ac.uk/train/vrml2rep/part2/fig5s.jpg[fig5s]

*Figure 5* VRML Editor

VRML Editor is a shareware product. The registered version includes
extra texturing capabilities and scaling functions. Initial positive
thoughts were of the package's low price and tiny file size (the
downloaded zip file which included software, help and examples was less
than 700KB). The user interface and package reflect this though in its
limited sophistication.

Object and view manipulations are awkward, for example there is no
facility for multiple viewports on screen at one time (a near necessity
in a 3D modelling context), although icons do allow a certain degree of
user-friendliness. However, worlds are rendered on-screen in real-time,
using the OpenGL API.

Import and export facilities are limited, reflecting VRML Editor's low
end nature. One can only import files generated with VRML 1.0
specification, though saving to both VRML 1.0 and 2.0 (.wrl) is
available. In addition, it is possible to save a 2D screen dump as a
JPEG (.jpg) in 8, 16 or 24-bit colour.

With VRML Editor, you can quickly create scenes composed of cones,
spheres, cubes, discs, and cylinders. Translations, scaling, colour
properties and texture maps (GIF and JPEG formats) are easily applied,
with the results viewed instantaneously. One useful feature is the
ability to create three-dimensional ASCII text within the scene.
However, for more complex shapes, the co-ordinates must be read from an
external file.

Whilst it is possible to create multiple light sources, it is impossible
to delete any once created. Also there seems to be no facility to define
viewpoints.

URL anchors can be added to these objects and a background colour
defined. But none of the more advanced features of VRML 2.0
specification, such as level-of-detail or animations can be used with
this program. Essentially, it seems to be a VRML 1.0 modeller with a
VRML 2.0 export feature.

In conclusion, this package is excellent for a VRML novice. Its file
size and cost working greatly to its advantage. What it lacks in
sophistication is made up in ease of use, one is able to begin on first
use to create simple VRML scenes in a matter of a few minutes. That
said, it is a package that will be "outgrown" very quickly as so many of
the VRML 2.0 specification nodes are unavailable.

[[headinge17]]
==== 4.6 Sculptware LLC SiteSculptor 1.0

web site: _http://www.sculptware.com_ +
platforms: Windows 95, Windows NT 4.0 +
cost: $375 +
download: _http://www.sculptware.com/main/download.htm_

image:http://www.agocg.ac.uk/train/vrml2rep/part2/fig6s.jpg[fig6s]

*Figure 6* SiteSculptor

Sculptware describe SiteSculptor as an "industrial strength" VRML
authoring tool. As such, it contains many of the advanced modelling
features one would expect to see in a CAD application, such as Boolean
operations, surface trimming and NURB surfaces. But the VRML-specific
features are limited to URL anchors, automatic inlining of objects and
determination of levels-of-detail.

One of the main strengths of SiteSculptor is its intuitive user
interface. The command structure is easy to follow and can be quickly
learnt. The inclusion of a "previous commands" icons list, is a
particularly useful. The user can also customise menus and accelerator
keys.

However, the package does not perform real-time rendering of the scene,
whilst working on it. This is a fundamental "minus" against it for a
VRML modelling tool. In order to view a VRML scene properly, it is
necessary to download and install a separate VRML browser plugin
(Live3D).

Since the import facility was disabled on the trial version evaluated,
it was not possible to ascertain whether the package was able to import
VRML 1.0 or VRML 2.0 (or both). It does accept Spider (.spd) and CadKey
(.prt) file types and is able to export successfully to VRML 2.0 (.wrl).
However, its inability to import other commonly used 3D file formats,
such as AutoCAD (.dxf) or 3D Studio (.3ds) is disappointing.


[[eval5]]
=== 5. Comparison of Features

[cols="<16%,<14%,<14%,<14%,<14%,<14%,<14%",opts=autowidth]
|===
| + |Caligari +
*TrueSpace 3.0* |Ligos +
*V-Realm Builder 2.0* |Paragraph +
*Internet Space Builder 2.1* |Platinum +
*VR Creator 2.0* +
Learning Edition |Rendersoft +
*VRML Editor 1.3* |Sculptware LLC +
*Site Sculptor 1.0*

|*File Import* | | | | | |

|VRML 1.0 |*yes* |*yes* |no |*yes* |*yes* | +
|VRML 2.0 |no |*yes* |*yes* |*yes* |no | +
|gzipped VRML |*yes* |*yes* |*yes* |no |no |no
|3DStudio .3ds |*yes* |*yes* |*yes* |no (PE yes) |no |no
|AutoCAD .dxf |*yes* |no |no |no |no |no

|*File Export* | | | | | |

|VRML 1.0 |*yes* | + |no |no |*yes* | +
|VRML 2.0 |*yes* |*yes* |*yes* |*yes* |*yes* |*yes*
|gzipped VRML |no |*yes* |*yes* |no |no |no

|*User Interface* | | | | | |

|multiple view windows |*yes* |*yes* |*yes* |*yes* |no |*yes*
|real-time shading |*yes* |*yes* |*yes* |*yes* |*yes* |no (plugin req.)
|scene graph |no |*yes* |*yes* |*yes* |no |no
|publish tool |*yes* |no |*yes* |*yes* |no |no
|customisable |no |no |*yes* |*yes* |no |*yes*
|context-sensitive help |no |no |*yes* |no |no |*yes*
|tutorials |*yes* |no |*yes* |*yes* |no |no

|*Modelling Features* | | | | | |

|primitive shapes |*yes* |*yes* |*yes* |*yes* |*yes* |*yes*
|polygon editor |*yes* |*yes* |*yes* |no (PE yes) |no |*yes*
|boolean ops |*yes* |no |*yes* |no (PE yes) |no |*yes*

|*File Formats* | | | | | |

|texture image |BMP, JPEG, TGA, TXR |GIF, JPEG, SGI |BMP, GIF, JPEG |BMP, JPEG |BMP, JPEG |JPEG |movie |AVI |MPEG, AVI |animated GIF |none |none |none

|audio | + |WAV, MIDI |WAV, MIDI |WAV |none |none

|*Supported VRML 2.0 Nodes* | | | | | |

|Background    |*yes* |*yes* |*yes* |*yes* |*yes* |no
|URL Anchor    |*yes* |*yes* |*yes* |*yes* |*yes* |*yes*
|Billboard     |no |*yes* |no |no |no |no
|ElevationGrid |no |*yes* |no |no |no |no
|Fog           |*yes* |*yes* |no |no |no |no
|Inline        |*yes* |*yes* |no |*yes* |no |*yes*
|LOD           |*yes* |*yes* |no |*yes* |no |*yes* (auto)
|Sound         |*yes* |*yes* |*yes* |*yes* |no |no
|Text          |*yes* |*yes* |*yes* |*yes* |*yes* |no
|Viewpoint     |*yes* |*yes* |*yes* |*yes* |no |*yes*
|lights        |*yes* |*yes* |no |*yes* |*yes* |no
|PROTO         |no |*yes* |no |no |no |no

|*Animation* | | | | | |

|key-frame editor |*yes* |*yes* |no |*yes* |no |no
|Sensors and ROUTES | + |*yes* |no |*yes* |no |no
|JavaScript |no |*yes* |no |*yes* |no |no

|*Libraries* | | | | | |

|objects |*yes* |*yes* |*yes* |*yes* |*yes* |no
|materials |*yes* |*yes* |no |*yes* |no |no
|textures |*yes* |*yes* |*yes* |*yes* |no |*yes*
|behaviours |no |no |no |*yes* |no |no
|drag-and-drop |no |*yes* |*yes* |*yes* |no |no

|*Miscellaneous* | | | | | |

|graphics API |Direct3D/3DR |OpenGL | + |Direct3D |OpenGL | +

|cost |$495 |$495 |$89.95 |FREE +
(PE $115) |$15 (shareware) |$375
|===


_A blank cell indicates that feature was not determined._ +
_PE: Personal Edition_


[[eval6]]
=== 6. Conclusions

The VRML modelling tools reviewed in this report vary considerably in
the range of features they offer, the proportion of the VRML 2.0
specification they support and in how much they cost. They can be
divided loosely into three different categories, based upon their
functionality.

_A modeller with VRML export_. +
Caligari *TrueSpace* is a very good package for doing what it is
designed for, that is high quality modelling and rendering. However, if
you are only interested in creating animated VRML worlds, this package
offers too few VRML specific features for its relatively high cost.

_Static VRML world builders_. +
Although these applications have been designed primarily for creating
VRML worlds, they do not support the full VRML 2.0 specification, in
particular animations. RenderSoft's *VRML Editor* is inexpensive and
easy to use, but has only a limited number of functions. Sculptware LLC
*Site Sculptor* is orientated towards the CAD user, offering many
sophisticated modelling tools. Paragraph's *Internet Space Builder*
enables the user to quickly build texture-rich static virtual worlds,
with a drag-and-drop interface and large object library.

_Animated VRML world builders_. +
With the final two programs, the user can also add animations and object
behaviours to a VRML world. Ligos *V-Realm Builder* supports the entire
VRML 2.0 specification, however, it does require the user to have some
knowledge of how the VRML scene graph works. The less expensive Platinum
*VRCreator* does not support so many nodes, but does have a more
intuitive interface, with the drag-and-drop behaviours library an
especially useful feature. Another plus point for VRCreator is that its
Learning Edition is freely available.

However, incompatibility is still a major problem with all the VRML
modelling tools, despite VRML 2.0 now being an international standard.
We encountered many difficulties whilst transferring VRML 2.0 files
between applications and found that the appearance of a model would
sometimes be inconsistent between modeller and VRML browser.
"Fine-tuning" with a text editor is still sometimes necessary.

[[appxa]]
=== _Appendix A: *VRML Nodes*_


'''''

* <<appxafie, Field Value Types>>
* <<appxanod, Node Reference>>


[[appxafie]]
=== Field Value Types

The following table summarizes the different VRML field value types.
There are single (SF) and multiple-value (MF) versions of most field
types. A single value may consist of more than one component or element,
such as a vector. These components are separated by white spaces. A
multiple-value field is a list of zero or more single values, enclosed
within square brackets and optionally separated by commas.

*`SFBool`* +
::
  A single boolean value, written as either `TRUE` or `FALSE`. +
*`SFColor`*`+ +`_and_`+ +`*`MFColor`* +
::
  A SFColor field contains 3 floating point values, ranging between 0.0
  and 1.0, representing the red, green and blue colour components. +
  _SFColor example:_ `+1.0 0.0 0.0+` _defines a bright red colour_ +
*`SFFloat`*`+ +`_and_`+ +`*`MFFloat`* +
::
  The SFFloat field has one floating point value. The MFFloat field
  contains zero or more floating point values. +
  _MFFloat example:_ `+[-3.1415926, 12.5e-3, .0001]+` +
*`SFImage`* +
::
  SFImage defines a single uncompressed 2D pixel image, that is used in
  texture mapping. The format is
  `+<width> <height> <num components> <pixels values>.+` +
  _SFImage example:_ `+1 2 2 0xFF 0x00 0xFF 0x00+` +
  _a 2x2 greyscale image forming a black (0x00) and white (0xFF)
  chequer-board pattern._ +
*`SFInt32`* _and_`+ +`*`MFInt32`* +
::
  A SFInt32 field contains a single 32-bit integer value. MFInt32 has
  zero or more integers. +
  _MFInt32 example:_ `+[8, -67, 45]+` +
*`SFNode`* _and_`+ +`*`MFNode`* +
::
  A single VRML node (SFNode) or list of nodes (MFNode). +
  _SFNode example:_ `+Cube { }+` +
*`SFRotation`* _and_`+ +`*`MFRotation`* +
::
  A SFRotation field has four components, the first three floating point
  values define the normalized rotation axis, the final value specifies
  the (right-handed) rotation in radians. +
  _SFRotation example:_`+ 1 0 0 3.1415926 +`_a 180° rotation about the
  x-axis._ +
*`SFString`* _and_`+ +`*`MFString`* +
::
  SFString consists of a list of UTF-8 characters enclosed in double
  quotes. +
  _MFString example:_ `+["Hello World", "Goodbye"]+` +
*`SFTime`* _and_`+ +`*`MFTime`* +
::
  SFTime is a floating point value representing the number of seconds
  that have passed since midnight GMT, 1st January 1970. +
  *`SFVec2f`* _and_`+ +`*`MFVec2f`* +
  A 2D vector described by two floating point values. Usually used to
  specify a 2D position. +
  _MFVec2f example:_ [`+2.5 4.0, 0.1 0.5, 10.0 12.0]+` +
*`SFVec3f`* _and_`+ +`*`MFVec3f`* +
::
  A 3D floating point vector, typically used to define a 3D position. +
  _SFVec2f example:_ `+32.04 50 0.03 +`

[[appxanod]]
=== Node Reference

This section lists all the VRML nodes and their fields. Each definition
contains:

* the node's name (e.g. *`Sphere`*) +
* each field name (e.g. *`radius`*) +
* default value for each field (e.g. *`1`*) +
* the field type (e.g. `field`) +
* field value type (e.g. `+SFFloat)+` +
* the range of acceptable values (e.g.
  `+(0,+`∞`+) +`_indicates that the value must
  be greater than zero but less than infinity, square brackets indicate an
  inclusive boundary, e.g._ `+[0,1]+` _means_ 0 <= _value_ <= 1)

EventIn and eventOut fields cannot be set when writing a VRML node, so
are shown in a light type. The symbols on the left of each node name,
indicate the type and properties of that node.

📁 Grouping node that has a field containing children nodes.
📂 Child node. Typical functions:
💡 light source
🧿 sensor +
🔮 interpolator (for key-frame animation) +
⏱ time dependent node (activate and deactivate at specific times) +
🧷 bindable node (only one node of each type can be bound at a time) +
🧾 Property node, used in particular fields of child nodes. +
🔲 node appears in the *`geometry`* field of the *`Shape`* node +
🔳 node used within one of the fields of the *`Appearance`* node +
🟦 geometric property node, used within geometry nodes.

📁🧿

[source]
....
Anchor { 
  addChildren                 # eventIn       MFNode   
  removeChildren              # eventIn       MFNode   
  children        []          # exposedField  MFNode   
  description     ""          # exposedField  SFString  
  parameter       []          # exposedField  MFString 
  url             []          # exposedField  MFString 
  bboxCenter      0 0 0       # field         SFVec3f   (-,)
  bboxSize        -1 -1 -1    # field         SFVec3f   (0,) or -1,-1,-1
}
....

🧾

[source]
....
Appearance { 
  material         NULL       # exposedField  SFNode 
  texture          NULL       # exposedField  SFNode 
  textureTransform NULL       # exposedField  SFNode 
}
....

⏱

[source]
....
AudioClip { 
  description      ""         # exposedField  SFString 
  loop             FALSE      # exposedField  SFBool   
  pitch            1.0        # exposedField  SFFloat   (0,)
  startTime        0          # exposedField  SFTime    (-,)
  stopTime         0          # exposedField  SFTime    (-,)
  url              []         # exposedField  MFString 
  duration_changed            # eventOut      SFTime   
  isActive                    # eventOut      SFBool   
}
....

📂🧷

[source]
....
Background { 
  set_bind                    # eventIn       SFBool   
  groundAngle     []          # exposedField  MFFloat   [0,/2]
  groundColor     []          # exposedfield  MFColor   [0,1]
  backUrl         []          # exposedField  MFString 
  bottomUrl       []          # exposedField  MFString 
  frontUrl        []          # exposedField  MFString 
  leftUrl         []          # exposedField  MFString 
  rightUrl        []          # exposedField  MFString 
  topUrl          []          # exposedField  MFString 
  skyAngle        []          # exposedField  MFFloat   [0,]
  skyColor        [ 0 0 0 ]   # exposedField  MFColor   [0,1]
  isBound                     # eventOut      SFBool   
}
....

📁

[source]
....
Billboard { 
  addChildren                 # eventIn       MFNode   
  removeChildren              # eventIn       MFNode   
  axisOfRotation   0 1 0      # exposedField  SFVec3f   (-,)
  children         []         # exposedField  MFNode   
  bboxCenter       0 0 0      # field         SFVec3f   (-,)
  bboxSize         -1 -1 -1   # field         SFVec3f   (0,) or -1,-1,-1
}
....

🧾🔲

[source]
....
Box { 
  size            2 2 2       # field         SFVec3f   (0,)
}
....

📁🧿

[source]
....
Collision { 
  addChildren                 # eventIn       MFNode   
  removeChildren              # eventIn       MFNode   
  children        []          # exposedField  MFNode   
  collide         TRUE        # exposedField  SFBool   
  bboxCenter      0 0 0       # field         SFVec3f   (-,)
  bboxSize        -1 -1 -1    # field         SFVec3f   (0,) or -1,-1,-1
  proxy           NULL        # field         SFNode   
  collideTime                 # eventOut      SFTime   
}
....

🧾🟦

[source]
....
Color { 
  color           []          # exposedField  MFColor   [0,1]
}
....

📂🔮

[source]
....
ColorInterpolator { 
  set_fraction                # eventIn       SFFloat   (-,)
  key             []          # exposedField  MFFloat   (-,)
  keyValue        []          # exposedField  MFColor   [0,1]
  value_changed               # eventOut      SFColor 
}
....

🧾🔲

[source]
....
Cone { 
  bottomRadius    1           # field         SFFloat   (0,)
  height          2           # field         SFFloat   (0,)
  side            TRUE        # field         SFBool    
  bottom          TRUE        # field         SFBool    
}
....

🧾🟦

[source]
....
Coordinate { 
  point           []          # exposedField  MFVec3f   (-,)
}
....

📂🔮

[source]
....
CoordinateInterpolator { 
  set_fraction                # eventIn       SFFloat   (-,)
  key             []          # exposedField  MFFloat   (-,)
  keyValue        []          # exposedField  MFVec3f   (-,)
  value_changed               # eventOut      MFVec3f 
}
....

🧾🔲

[source]
....
Cylinder { 
  bottom          TRUE        # field         SFBool    
  height          2           # field         SFFloat   (0,)
  radius          1           # field         SFFloat   (0,)
  side            TRUE        # field         SFBool    
  top             TRUE        # field         SFBool    
}
....

📂🧿

[source]
....
CylinderSensor { 
  autoOffset      TRUE        # exposedField  SFBool     
  diskAngle       0.262       # exposedField  SFFloat   (0,/2)
  enabled         TRUE        # exposedField  SFBool     
  maxAngle        -1          # exposedField  SFFloat   [-2,2]
  minAngle        0           # exposedField  SFFloat   [-2,2]
  offset          0           # exposedField  SFFloat   (-,)
  isActive                    # eventOut      SFBool     
  rotation_changed            # eventOut      SFRotation 
  trackPoint_changed          # eventOut      SFVec3f    
}
....

📂💡

[source]
....
DirectionalLight { 
  ambientIntensity  0         # exposedField  SFFloat   [0,1]
  color             1 1 1     # exposedField  SFColor   [0,1]
  direction         0 0 -1    # exposedField  SFVec3f   (-,)
  intensity         1         # exposedField  SFFloat   [0,1]
  on                TRUE      # exposedField  SFBool  
}
....

🧾🔲

[source]
....
ElevationGrid { 
  set_height                  # eventIn       MFFloat  
  color             NULL      # exposedField  SFNode   
  normal            NULL      # exposedField  SFNode   
  texCoord          NULL      # exposedField  SFNode   
  height            []        # field         MFFloat   (-,)
  ccw               TRUE      # field         SFBool   
  colorPerVertex    TRUE      # field         SFBool   
  creaseAngle       0         # field         SFFloat   [0,]
  normalPerVertex   TRUE      # field         SFBool   
  solid             TRUE      # field         SFBool   
  xDimension        0         # field         SFInt32   [0,)
  xSpacing          1.0       # field         SFFloat   (0,)
  zDimension        0         # field         SFInt32   [0,)
  zSpacing          1.0       # field         SFFloat   (0,)
}
....

🧾🔲

[source]
....
Extrusion { 
  set_crossSection            # eventIn       MFVec2f    
  set_orientation             # eventIn       MFRotation 
  set_scale                   # eventIn       MFVec2f    
  set_spine                   # eventIn       MFVec3f    
  beginCap        TRUE        # field         SFBool     
  ccw             TRUE        # field         SFBool     
  convex          TRUE        # field         SFBool     
  creaseAngle     0           # field         SFFloat   [0,)
  crossSection    [ 1 1, 1 -1, -1 -1, -1 1, 1 1 ]   
                              # field         MFVec2f   (-,)
  endCap          TRUE        # field         SFBool     
  orientation     0 0 1 0     # field         MFRotation [-1,1],(-,)
  scale           1 1         # field         MFVec2f   (0,)
  solid           TRUE        # field         SFBool     
  spine           [ 0 0 0, 0 1 0 ]  # field   MFVec3f   (-,)
}
....

📂🧷

[source]
....
Fog { 
  color           1 1 1       # exposedField  SFColor   [0,1]
  fogType         "LINEAR"    # exposedField  SFString 
  visibilityRange 0           # exposedField  SFFloat   [0,)
  set_bind                    # eventIn       SFBool   
  isBound                     # eventOut      SFBool   
}
....

🧾

[source]
....
FontStyle { 
  family          ["SERIF"]   # field         MFString 
  horizontal      TRUE        # field         SFBool   
  justify         "BEGIN"     # field         MFString 
  language        ""          # field         SFString 
  leftToRight     TRUE        # field         SFBool   
  size            1.0         # field         SFFloat   (0,)
  spacing         1.0         # field         SFFloat   [0,)
  style           "PLAIN"     # field         SFString 
  topToBottom     TRUE        # field         SFBool   
}
....

📁

[source]
....
Group { 
  addChildren                 # eventIn       MFNode  
  removeChildren              # eventIn       MFNode  
  children        []          # exposedField  MFNode  
  bboxCenter      0 0 0       # field         SFVec3f   (-,)
  bboxSize        -1 -1 -1    # field         SFVec3f   (0,) or -1,-1,-1
}
....

🧾🔳

[source]
....
ImageTexture { 
  url             []          # exposedField  MFString 
  repeatS         TRUE        # field         SFBool   
  repeatT         TRUE        # field         SFBool   
}
....

🧾🔲

[source]
....
IndexedFaceSet { 
  set_colorIndex              # eventIn       MFInt32 
  set_coordIndex              # eventIn       MFInt32 
  set_normalIndex             # eventIn       MFInt32 
  set_texCoordIndex           # eventIn       MFInt32 
  color           NULL        # exposedField  SFNode  
  coord           NULL        # exposedField  SFNode  
  normal          NULL        # exposedField  SFNode  
  texCoord        NULL        # exposedField  SFNode  
  ccw             TRUE        # field         SFBool  
  colorIndex      []          # field         MFInt32   [-1,)
  colorPerVertex  TRUE        # field         SFBool  
  convex          TRUE        # field         SFBool  
  coordIndex      []          # field         MFInt32   [-1,)
  creaseAngle     0           # field         SFFloat   [0,)
  normalIndex     []          # field         MFInt32   [-1,)
  normalPerVertex TRUE        # field         SFBool  
  solid           TRUE        # field         SFBool  
  texCoordIndex   []          # field         MFInt32   [-1,)
}
....

🧾🔲

[source]
....
IndexedLineSet { 
  set_colorIndex              # eventIn       MFInt32 
  set_coordIndex              # eventIn       MFInt32 
  color           NULL        # exposedField  SFNode  
  coord           NULL        # exposedField  SFNode  
  colorIndex      []          # field         MFInt32   [-1,)
  colorPerVertex  TRUE        # field         SFBool  
  coordIndex      []          # field         MFInt32   [-1,)
}
....

📁

[source]
....
Inline { 
  url             []          # exposedField  MFString 
  bboxCenter      0 0 0       # field         SFVec3f   (-,)
  bboxSize        -1 -1 -1    # field         SFVec3f   (0,) or -1,-1,-1
}
....

📁

[source]
....
LOD { 
  level           []          # exposedField  MFNode  
  center          0 0 0       # field         SFVec3f   (-,)
  range           []          # field         MFFloat   (0,)
}
....

🧾🔳

[source]
....
Material { 
  ambientIntensity 0.2        # exposedField  SFFloat   [0,1]
  diffuseColor    0.8 0.8 0.8 # exposedField  SFColor   [0,1]
  emissiveColor   0 0 0       # exposedField  SFColor   [0,1]
  shininess       0.2         # exposedField  SFFloat   [0,1]
  specularColor   0 0 0       # exposedField  SFColor   [0,1]
  transparency    0           # exposedField  SFFloat   [0,1]
}
....

🧾🔳

[source]
....
MovieTexture { 
  loop            FALSE       # exposedField  SFBool   
  speed           1.0         # exposedField  SFFloat   (-,)
  startTime       0           # exposedField  SFTime    (-,)
  stopTime        0           # exposedField  SFTime    (-,)
  url             []          # exposedField  MFString 
  repeatS         TRUE        # field         SFBool   
  repeatT         TRUE        # field         SFBool   
  duration_changed            # eventOut      SFTime   
  isActive                    # eventOut      SFBool   
}
....

📂🧷

[source]
....
NavigationInfo { 
  set_bind                          # eventIn       SFBool   
  avatarSize      [0.25, 1.6, 0.75] # exposedField  MFFloat  [0,)
  headlight       TRUE              # exposedField  SFBool   
  speed           1.0               # exposedField  SFFloat  [0,)
  type            ["WALK", "ANY"]   # exposedField  MFString 
  visibilityLimit 0.0               # exposedField  SFFloat  [0,)
  isBound                           # eventOut      SFBool   
}
....

🧾🟦

[source]
....
Normal { 
  vector          []          # exposedField  MFVec3f   (-,)
}
....

📂🔮

[source]
....
NormalInterpolator { 
  set_fraction                # eventIn       SFFloat   (-,)
  key             []          # exposedField  MFFloat   (-,)
  keyValue        []          # exposedField  MFVec3f   (-,)
  value_changed               # eventOut      MFVec3f 
}
....

📂🔮

[source]
....
OrientationInterpolator { 
  set_fraction                # eventIn       SFFloat   (-,)
  key             []          # exposedField  MFFloat   (-,)
  keyValue        []          # exposedField  MFRotation [-1,1],(-,)
  value_changed               # eventOut      SFRotation 
}
....

🧾

[source]
....
PixelTexture { 
  image           0 0 0       # exposedField  SFImage 
  repeatS         TRUE        # field         SFBool   
  repeatT         TRUE        # field         SFBool   
}
....

📂🧿

[source]
....
PlaneSensor { 
  autoOffset      TRUE        # exposedField  SFBool  
  enabled         TRUE        # exposedField  SFBool  
  maxPosition     -1 -1       # exposedField  SFVec2f   (-,)
  minPosition     0 0         # exposedField  SFVec2f   (-,)
  offset          0 0 0       # exposedField  SFVec3f   (-,)
  isActive                    # eventOut      SFBool  
  trackPoint_changed          # eventOut      SFVec3f 
  translation_changed         # eventOut      SFVec3f 
}
....

📂💡

[source]
....
PointLight { 
  ambientIntensity  0         # exposedField  SFFloat   [0,1]
  attenuation       1 0 0     # exposedField  SFVec3f   [0,)
  color             1 1 1     # exposedField  SFColor   [0,1]
  intensity         1         # exposedField  SFFloat   [0,1]
  location          0 0 0     # exposedField  SFVec3f   (-,)
  on                TRUE      # exposedField  SFBool  
  radius            100       # exposedField  SFFloat   [0,)
}
....

🧾🔲

[source]
....
PointSet { 
  color           NULL        # exposedField  SFNode  
  coord           NULL        # exposedField  SFNode  
}
....

📂🔮

[source]
....
PositionInterpolator { 
  set_fraction                # eventIn       SFFloat   (-,)
  key             []          # exposedField  MFFloat   (-,)
  keyValue        []          # exposedField  MFVec3f   (-,)
  value_changed               # eventOut      SFVec3f 
}
....

📂🧿

[source]
....
ProximitySensor { 
  center          0 0 0       # exposedField  SFVec3f   (-,)
  size            0 0 0       # exposedField  SFVec3f   [0,)
  enabled         TRUE        # exposedField  SFBool  
  isActive                    # eventOut      SFBool     
  position_changed            # eventOut      SFVec3f    
  orientation_changed         # eventOut      SFRotation 
  enterTime                   # eventOut      SFTime     
  exitTime                    # eventOut      SFTime     
}
....

📂🔮

[source]
....
ScalarInterpolator { 
  set_fraction                # eventIn       SFFloat   (-,)
  key             []          # exposedField  MFFloat   (-,)
  keyValue        []          # exposedField  MFFloat   (-,)
  value_changed               # eventOut      SFFloat 
}
....

📂

[source]
....
Script { 
  url             []              # exposedField  MFString 
  directOutput    FALSE           # field         SFBool   
  mustEvaluate    FALSE           # field         SFBool   
  # And any number of:
  eventName                       # eventIn       eventType 
  fieldName       initialValue    # field         fieldType 
  eventName                       # eventOut      eventType 
}
....

📂

[source]
....
Shape { 
  appearance      NULL        # exposedField  SFNode 
  geometry        NULL        # exposedField  SFNode 
}
....

📂

[source]
....
Sound { 
  direction       0 0 1       # exposedField  SFVec3f   (-,)
  intensity       1           # exposedField  SFFloat   [0,1]
  location        0 0 0       # exposedField  SFVec3f   (-,)
  maxBack         10          # exposedField  SFFloat   [0,)
  maxFront        10          # exposedField  SFFloat   [0,)
  minBack         1           # exposedField  SFFloat   [0,)
  minFront        1           # exposedField  SFFloat   [0,)
  priority        0           # exposedField  SFFloat   [0,1]
  source          NULL        # exposedField  SFNode   
  spatialize      TRUE        # field         SFBool   
}
....

🧾🔲

[source]
....
Sphere { 
  radius          1           # field         SFFloat   (0,)
}
....

📂🧿

[source]
....
SphereSensor { 
  autoOffset      TRUE        # exposedField  SFBool     
  enabled         TRUE        # exposedField  SFBool     
  offset          0 1 0 0     # exposedField  SFRotation [-1,1],(-,)
  isActive                    # eventOut      SFBool     
  rotation_changed            # eventOut      SFRotation 
  trackPoint_changed          # eventOut      SFVec3f    
}
....

📂💡

[source]
....
SpotLight { 
  ambientIntensity  0               # exposedField  SFFloat [0,1]
  attenuation       1 0 0           # exposedField  SFVec3f [0,)
  beamWidth         1.570796        # exposedField  SFFloat (0,/2]
  color             1 1 1           # exposedField  SFColor [0,1]
  cutOffAngle       0.785398        # exposedField  SFFloat (0,/2]
  direction         0 0 -1          # exposedField  SFVec3f (-,)
  intensity         1               # exposedField  SFFloat [0,1]
  location          0 0 0           # exposedField  SFVec3f (-,)
  on                TRUE            # exposedField  SFBool  
  radius            100             # exposedField  SFFloat [0,)
}
....

📁

[source]
....
Switch { 
  choice          []          # exposedField  MFNode  
  whichChoice     -1          # exposedField  SFInt32   [-1,)
}
....

🧾🔲

[source]
....
Text { 
  string          []          # exposedField  MFString 
  fontStyle       NULL        # exposedField  SFNode   
  length          []          # exposedField  MFFloat   [0,)
  maxExtent       0.0         # exposedField  SFFloat   [0,)
}
....

🧾🟦

[source]
....
TextureCoordinate { 
  point           []          # exposedField  MFVec2f   (-,)
}
....

🧾🔳

[source]
....
TextureTransform { 
  center          0 0         # exposedField  SFVec2f   (-,)
  rotation        0           # exposedField  SFFloat   (-,)
  scale           1 1         # exposedField  SFVec2f   (-,)
  translation     0 0         # exposedField  SFVec2f   (-,)
}
....

📂⏱

[source]
....
TimeSensor { 
  cycleInterval   1           # exposedField  SFTime    (0,)
  enabled         TRUE        # exposedField  SFBool   
  loop            FALSE       # exposedField  SFBool   
  startTime       0           # exposedField  SFTime    (-,)
  stopTime        0           # exposedField  SFTime    (-,)
  cycleTime                   # eventOut      SFTime   
  fraction_changed            # eventOut      SFFloat  
  isActive                    # eventOut      SFBool   
  time                        # eventOut      SFTime   
}
....

📂🧿

[source]
....
TouchSensor { 
  enabled         TRUE        # exposedField  SFBool  
  hitNormal_changed           # eventOut      SFVec3f 
  hitPoint_changed            # eventOut      SFVec3f 
  hitTexCoord_changed         # eventOut      SFVec2f 
  isActive                    # eventOut      SFBool  
  isOver                      # eventOut      SFBool  
  touchTime                   # eventOut      SFTime  
}
....

📁

[source]
....
Transform { 
  addChildren                 # eventIn       MFNode      
  removeChildren              # eventIn       MFNode      
  center            0 0 0     # exposedField  SFVec3f     (-,)
  children          []        # exposedField  MFNode      
  rotation          0 0 1 0   # exposedField  SFRotation  [-1,1],(-,)
  scale             1 1 1     # exposedField  SFVec3f     (0,)
  scaleOrientation  0 0 1 0   # exposedField  SFRotation  [-1,1],(-,)
  translation       0 0 0     # exposedField  SFVec3f     (-,)
  bboxCenter        0 0 0     # field         SFVec3f     (-,)
  bboxSize          -1 -1 -1  # field         SFVec3f     (0,) or -1,-1,-1
}  
....

📂🧷

[source]
....
Viewpoint { 
  set_bind                    # eventIn       SFBool     
  fieldOfView     0.785398    # exposedField  SFFloat    (0,)
  jump            TRUE        # exposedField  SFBool     
  orientation     0 0 1 0     # exposedField  SFRotation [-1,1],(-,)
  position        0 0 10      # exposedField  SFVec3f    (-,)
  description     ""          # field         SFString   
  bindTime                    # eventOut      SFTime     
  isBound                     # eventOut      SFBool     
}
....

📂🧿

[source]
....
VisibilitySensor { 
  center          0 0 0       # exposedField  SFVec3f   (-,)
  enabled         TRUE        # exposedField  SFBool  
  size            0 0 0       # exposedField  SFVec3f   [0,)
  enterTime                   # eventOut      SFTime  
  exitTime                    # eventOut      SFTime  
  isActive                    # eventOut      SFBool  
}
....

📂

[source]
....
WorldInfo { 
  info            []          # field         MFString 
  title           ""          # field         SFString 
}
....


== VRML Specification v2.0

*The Virtual Reality Modeling Language Specification* +
Version 2.0, ISO/IEC WD 14772 +
August 4, 1996

''''''

:overview_html:  http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/Overview.html
:index_html:  http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/index.html
:credits_html:  http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/credits.html
:changeLog_html:  http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/changeLog.html
:foreword_html: http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/part1/foreword.html
:introduction_html: http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/part1/introduction.html
:scope_html:  http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/part1/scope.html
:references_html: http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/part1/references.html
:glossary_html: http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/part1/glossary.html
:concepts_html: http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/part1/concepts.html
:nodesRef_html: http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/part1/nodesRef.html
:fieldsRef_html:  http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/part1/fieldsRef.html
:conformance_html:  http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/part1/conformance.html
:grammar_html:  http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/part1/grammar.html
:examples_html: http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/part1/examples.html
:java_html: http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/part1/java.html
:javascript_html: http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/part1/javascript.html
:bibliography_html: http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/part1/bibliography.html
:part1_index_html:  http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/part1/part1.index.html

=== What is VRML?

VRML is an acronym for "Virtual Reality Modeling Language". It is a file
format for describing interactive 3D objects and worlds to
be experienced on the world wide web (similar to how HTML is used to
view text). The first release of http://vag.vrml.org/vrml10c.html[The
VRML 1.0 Specification] was created by Silicon Graphics, Inc. and based
on the _http://www.sgi.com/Technology/Inventor/[Open Inventor] file
format. The second release of VRML adds significantly more interactive
capabilities. It was designed by the Silicon Graphics' VRML 1.0 team
with contributions from Sony Research and Mitra. VRML 2.0 was reviewed
by the VRML moderated email discussion group
(http://vrml.wired.com/[www-vrml@wired.com]), and later adopted and
endorsed by a plethora of companies and individuals. See the
_http://sdsc.edu/vrml/[San Diego Supercomputer Center's VRML Repository]
or _http://vrml.sgi.com/[Silicon Graphics' VRML site] for more
information.___

==== What is _Moving Worlds_?

_Moving Worlds_ is the name of Silicon Graphics' submission to the
http://vag.vrml.org/rfp.html[Request-for-Proposals] for VRML 2.0. It was
chosen by the VRML community as the working document for VRML 2.0. It
was created by Silicon Graphics, Inc. in collaboration with Sony and
Mitra. Many people in the VRML community were actively involved with
_Moving Worlds_ and contributed numerous ideas, reviews, and
improvements.

==== What is the VRML Specification?

The VRML Specification is the technical document that precisely
describes the VRML file format. It is primarily intended for
implementors writing VRML browsers and authoring systems. It is also
intended for readers interested in learning the details about VRML. Note
however that many people (especially non-programmers) find the VRML
Specification inadequate as a starting point or primer. There are a
variety of excellent introductory books on VRML in bookstores.

==== How was _Moving Worlds_ chosen as the VRML 2.0 Specification?

The http://vag.vrml.org./[VRML Architecture Group] (VAG) put out a
http://vag.vrml.org/rfp.html[Request-for-Proposals] (RFP) in January
1996 for VRML 2.0. Six proposals were received and then
http://vag.vrml.org./vrml20info.html[debated for about 2 months].
_Moving Worlds_ developed a http://vag.vrml.org./polling/Results.html[strong consensus] 
and was eventually selected by the VRML community in a poll. The VAG made it
official on March 27th.

==== How can I start using VRML 2.0?

You must install a VRML 2.0 browser. The following VRML 2.0 *Draft*
browsers or toolkits are available: +

*   DimensionX's _http://www.dimensionx.com/products/lr/[Liquid Reality] toolkit_
*   Silicon Graphics' _http://vrml.sgi.com/[Cosmo Player for Windows95] browser_
*   Sony's _http://vs.sony.co.jp/VS-E/vstop.html[CyberPassage] browser_

See http://sdsc.edu/SDSC/Partners/vrml/software/browsers.html[San Diego
Supercomputer Center's] VRML Repository for more details on available
VRML browsers and tools.

''''''

Official VRML 2.0 Specification

Compressed *PostScript* (xxxk)
Compressed *tar HTML Directory* (xxxk)

*Draft 3* VRML 2.0

link:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/draft2b_to_3.html[Changes from Draft #2b --> #3] !Compressed (gzip) *Postscript* (880K)

link:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/VRML2.DRAFT3.html.gz[Compressed (gzip) *HTML*] (140K)
link:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/VRML2.DRAFT3.html[Uncompressed *HTML*] (536K)

link:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/VRML2.DRAFT3.tar.gz[Compressed (gzip) *tar HTML dir*] (952K)
!http://www.nist.gov/itl/div878/ovrt/VRML2.DRAFT3.pdf[PDF format] (thanks to Sandy Ressler)

Draft 2 VRML 2.0

link:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/draft2b.ps.gz[Compressed (gzip) Postscript] (404K)
link:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/draft2b.html.gz[Compressed (gzip) HTML] (84K)

''''''

The Virtual Reality Modeling Language specification was originally
developed by http://www.sgi.com/[Silicon Graphics, Inc.] in
collaboration with http://vs.sony.co.jp/VS-E/vstop.html[Sony] and
mailto:mitra@earth.path.net[Mitra]. Many people in the VRML community
have been involved in the review and evolution of the specification (see
<<credits_html, Credits>>). Moving Worlds VRML 2.0 is a tribute to
the successful collaboration of all of the members of the VRML
community. mailto:gavin@acm.org[Gavin Bell], mailto:rikk@best.com[Rikk
Carey], and mailto:cmarrin@sgi.com[Chris Marrin] have headed the effort
to produce the final specification.

Please send errors or suggestions to rikk@best.com, cmarrin@sgi.com,
and/or gavin@acm.org.

''''''

==== Related Documents

* http://vrml.wired.com/vrml.tech/vrml10-3.html[VRML 1.0 Specification]
* http://vag.vrml.org/rfp.html[VRML 2.0: Request-for-Proposal from the VAG]
* http://vag.vrml.org/vrml20info.html[VRML 2.0: Process from the VAG]
* http://vag.vrml.org/polling/Results.html[VRML 2.0: Polling Results]

==== Related Sites

* http://vag.vrml.org/[VRML Architecture Group (VAG)]
* http://www.wired.com/vrml/[`+www-vrml@wired.com email list information+`]`+ +`
* http://www.oki.com/vrml/VRML_FAQ.html[VRML FAQ]
* http://sdsc.edu/vrml/[San Diego Supercomputer Center VRML Repository]
* http://vrml.sgi.com/[Silicon Graphics VRML/Cosmo site]
* http://vs.sony.co.jp/VS-E/vstop.html[SONY's Virtual Society site]
* http://www.eit.com/www.lists/www.lists.2.html[www-vrml email list archive]

''''''

Contact rikk@best.com, mailto:cmarrin@sgi.com[cmarrin@sgi.com,] or gavin@acm.org with questions or comments.

This URL: http://vrml.sgi.com/moving-worlds/index.html


[[overview_html]]
=== Overview

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/pix/vrmllogo2.0.gif[pix/vrmllogo2.0,width=470,height=84]

*An Overview of the* +
*Virtual Reality Modeling Language* +
*Version 2.0* +
*August 4, 1996*

* <<Introduction, Introduction>>
* <<Summary, Summary of VRML 2.0 Features>>
* <<NodeChanges, Changes from VRML 1.0>>


[[introduction]]
==== Introduction

This overview provides a brief high-level summary of the VRML 2.0
specification. The purposes of the overview are to give you the general
idea of the major features, and to provide a summary of the differences
between VRML 1.0 and VRML 2.0. The overview consists of two sections:

* <<Summary, Summary of VRML 2.0 Features>>
* <<NodeChanges, Changes from VRML 1.0>>

This overview assumes that readers are at least vaguely familiar with
VRML 1.0. If you're not, read the introduction to the official
http://vrml.wired.com/vrml.tech/vrml10-3.html[VRML 1.0 specification].
Note that VRML 2.0 includes some changes to VRML 1.0 concepts and names,
so although you should understand the basic idea of what VRML is about,
you shouldn't hold on too strongly to details and definitions from 1.0
as you read the specification.

The VRML 2.0 specification is available at:
http://vrml.sgi.com/moving-worlds/spec/[http://vrml.sgi.com/moving-worlds/spec/.]


[[Summary]]
==== Summary of VRML 2.0 Features

VRML 1.0 provided a means of creating and viewing static 3D worlds; VRML
2.0 provides much more. The overarching goal of VRML 2.0 is to provide a
richer, more exciting, more interactive user experience than is possible
within the static boundaries of VRML 1.0. The secondary goal is to
provide a solid foundation for future VRML expansion to grow from, and
to keep things as simple and as fast as possible -- for everyone from
browser developers to world designers to end users.

VRML 2.0 provides these extensions and enhancements to VRML 1.0:

* Enhanced static worlds
* Interaction
* Animation
* Scripting
* Prototyping

Each section of this summary contains links to relevant portions of the
official specification.

===== Enhanced Static Worlds

You can add realism to the static geometry of your world using new
features of VRML 2.0:

New nodes allow you to create ground-and-sky <<Background, backdrops>> 
to scenes, add distant mountains and clouds, and dim distant objects with
<<Fog, fog>>. Another new node lets you easily create irregular
<<ElevationGrid, terrain>> instead of using
flat planes for ground surfaces.

VRML 2.0 provides 3D spatial <<Sound, sound>>-generating nodes to further
enhance realism -- you can put crickets, breaking glass, ringing
telephones, or any other sound into a scene.

If you're writing a browser, you'll be happy to see that optimizing and
parsing files are easier than in VRML 1.0, thanks to a new simplified
<<StructuringtheSceneGraph, scene graph structure>>.

===== Interaction

No more moving like a ghost through cold, dead worlds: now you can
directly interact with objects and creatures you encounter. New
<<Sensors, sensor>> nodes set off events when you move in certain 
areas of a world and when you click certain objects. They even let 
you drag objects or controls from one place to another. 
<<TimeSensor, Another kind of sensor>> keeps track of the passage 
of time, providing a basis for everything from alarm clocks to 
repetitive animations.

And no more walking through walls. <<Collision, Collision detection>> 
ensures that solid objects react like solid objects; you bounce off them 
(or simply stop moving) when you run into them. Terrain following allows 
you to travel up and down steps or ramps.

===== Animation

VRML2.0 includes a variety of animation objects called
<<InterpolatorNodes, Interpolators>>. This
allow you to create pre-defined animations of a many aspects of the
world and then play it at some opportune time. With animation
interpolators you can create moving objects such as flying birds,
automatically opening doors, or walking robots, objects that change
color as they move, such as the sun, objects that morph their geometry
from one shape to another, and you can create guided tours that
automatically move the user along a predefined path.

===== Scripting

VRML 2.0 wouldn't be able to move without the new
<<Scripting, Script>> nodes. Using Scripts,
you can not only animate creatures and objects in a world, but give them
a semblance of intelligence. Animated dogs can fetch newspapers or
frisbees; clock hands can move; birds can fly; robots can juggle.

These effects are achieved by means of events; a script takes input from
sensors and generates events based on that input which can change other
nodes in the world. Events are passed around among nodes by way of
special statements called
<<Routes, routes>>.

===== Prototyping

Have an idea for a new kind of geometry node that you want everyone to
be able to use? Got a nifty script that you want to turn into part of
the next version of VRML? In VRML 2.0, you can encapsulate a group of
nodes together as a new node type, a <<Prototypes, prototype>>, and 
then make that node type available to anyone who wants to use it. You can 
then create instances of the new type, each with different field values --
for instance, you could create a Robot prototype with a robotColor
field, and then create as many individual different-colored Robot nodes
as you like.

===== Example

So how does all this fit together? Here's a look at possibilities for
implementing a fully-interactive demo world called Gone Fishing.

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/pix/fish.start.gif[(floating worldlet),width=119,height=112]

In Gone Fishing, you start out hanging in space near a floating
worldlet. If you wanted a more earthbound starting situation, you could
(for instance) make the worldlet an island in the sea, using a
Background node to show shaded water and sky meeting at the horizon as
well as distant unmoving geometry like mountains. You could also add a 
haze in the distance using the fog parameters in a Fog node.

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/pix/neon1.gif[(first neon sign),width=170,height=102]

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/pix/neon2.gif[(second neon sign),width=142,height=109]

As you approach the little world, you can see two neon signs blinking on
and off to attract you to a building. Each of those signs consists of
two pieces of geometry under a Switch node. A TimeSensor generates time events which 
a Script node picks up and processes; the Script then sends other events to 
the Switch node telling it which of its children should be active. All events 
are sent from node to node by way of ROUTE statements.

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/pix/iris.gif[(door opening),width=155,height=108]

As you approach the building -- a domed aquarium on a raised platform --
you notice that the entry portals are closed. There appears to be no way
in, until you click the front portal; it immediately slides open with a
motion like a camera's iris. That portal is attached to a TouchSensor
that detects your click; the sensor tells a Script node that you've
clicked, and the Script animates the opening portal, moving the geometry
for each piece of the portal a certain amount at a time. The script
writer only had to specify certain key frames of the animation;
interpolator nodes generate intermediate values to provide smooth
animation between the key frames. The door, by the way, is set up
for collision detection using a Collision node, so that without clicking
to open it you'd never be able to get in.

You enter the aquarium and a light turns on. A ProximitySensor node
inside the room noticed you coming in and sent an event to, yes, another
Script node, which told the light to turn on. The sensor, script, and
light can also easily be set up to darken the room when you leave.

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/pix/notouch.gif[(fish + sign),width=266,height=111]

Inside the aquarium, you can see and hear bubbles drifting up from the
floor. The bubbles are moved by another Script; the bubbling sound is
created by a PointSound node. As you move further into the building and
closer to the bubbles, the bubbling sound gets louder.

Besides the bubbles, which always move predictably upward, three fish
swim through the space inside the building. The fish could all be based
on a single Fish node type, defined in this file by a PROTO statement as
a collection of geometry, appearance, and behavior; to create new kinds
of fish, the world builder could just plug in new geometry or behavior.

Proximity sensors aren't just for turning lights on and off; they can be
used by moving creatures as well. For example, the fish could be
programmed (using a similar ProximitySensor/Script/ROUTE combination to
the one described above) to avoid you by swimming away whenever you got
too close. Even that behavior wouldn't save them from users who don't
follow directions, though:

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/pix/bellyup.gif[(dead fish),width=105,height=77]

Despite (or maybe because of) the warning sign on the wall, most users
"touch" one or more of the swimming fish by clicking them. Each fish behaves
differently when touched; one of them swims for the door, one goes
belly-up. These behaviors are yet again controlled by Script nodes.

To further expand Gone Fishing, a world designer might allow users to
"pick up" the fish and move them from place to place. This could be
accomplished with a PlaneSensor node, which translates a user's
click-and-drag motion into translations within the scene. Other
additions -- sharks that eat fish, tunnels for the fish to swim through,
a kitchen to cook fish dinners in, and so on -- are limited only by the
designer's imagination.

Gone Fishing is just one example of the sort of rich, interactive world
you can build with VRML 2.0. For details of the new nodes and file
structure, see the "Concepts" section of the
<<concpets_html, VRML 2.0 Specification>>.


[[NodeChanges]]
==== Changes from VRML 1.0

This section provides a very brief list of the changes to the set of
predefined node types for VRML 2.0. It briefly describes all the
<<new_nodes, newly added nodes>>, summarizes the
<<changed_nodes, changes to VRML 1.0 nodes>>, and lists the
<<deleted_nodes, VRML 1.0 nodes that have been deleted>> in VRML 2.0.
(For fuller descriptions of each node type, click the type name to link
to the relevant portion of the VRML 2.0 specification proposal.)
Finally, this document briefly describes the <<new_fields, new field types>> 
in VRML 2.0.

[[new_nodes]]
===== New Node Types

The new node types are listed by category:

* <<grouping_nodes, Grouping Nodes>>
* <<browser_info, Browser Information>>
* <<lighting, Lights and Lighting>>
* <<sound, Sound>>
* <<shapes, Shapes>>
* <<geometry, Geometry>>
* <<appearance, Appearance>>
* <<geom_sensors, Geometric Sensors>>
* <<special, Special Nodes>>

[[grouping_nodes]]
===== Grouping Nodes

<<Collision, Collision>>::
  Tells the browser whether or not given pieces of geometry can be
  navigated through.
<<Transform, Transform>>::
  Groups nodes together under a single coordinate system, or "frame of
  reference"; incorporates the fields of the VRML 1.0 Separator and
  Transform nodes.

[[browser_info]]
===== Browser Information

In place of the old Info node type, VRML 2.0 provides several new node
types to give specific information about the scene to the browser:

<<Background, Background>>::
  Provides a shaded plane and/or distant geometry to be used as a
  backdrop, drawn behind the displayed scene.
<<NavigationInfo, NavigationInfo>>::
  Provides hints to the browser about what kind of viewer to use (walk,
  examiner, fly, etc.), suggested average speed of travel, a radius
  around the camera for use by collision detection, and an indication of
  whether the browser should turn on a headlight.
<<Viewpoint, Viewpoint>>::
  Specifies an interesting location in a local coordinate system from
  which a user might wish to view the scene. Replaces the former
  PerspectiveCamera node.
<<WorldInfo, WorldInfo>>::
  Provides the scene's title and other information about the scene (such
  as author and copyright information), in a slightly more structured
  manner than a VRML 1.0 Info node.

[[lighting]]
===== Lights and Lighting

<<Fog, Fog>>::
  Describes a variety of atmospheric effects such as fog, haze, and
  smoke.

[[sound]]
===== Sound

<<Sound, Sound>>::
  Defines a sound source that emits sound primarily in a 3D space.

[[shapes]]
===== Shapes

<<Shape, Shape>>::
  A node whose fields specify a set of geometry nodes and a set of
  property nodes to apply to the geometry.

[[geometry]]
===== Geometry

<<ElevationGrid, ElevationGrid>>::
  Provides a compact method of specifying an irregular "ground" surface.
<<Extrusion, Extrusion>>::
  A compact representation of extruded shapes and solids of rotation.
<<Text, Text>>::
  Replaces VRML 1.0's AsciiText node; has many more options, to allow
  easy use of non-English text.

===== Geometric Properties

<<Color, Color>>::
  Defines a set of RGB colors to be used in the `color` fields of
  various geometry nodes.

[[appearance]]
===== Appearance

<<Appearance, Appearance>>::
  Gathers together all the appearance properties for a given Shape node.

[[geom_sensors]]
===== Sensors

<<ProximitySensor, ProximitySensor>>::
  Generates events when the camera moves within a bounding box of a
  specified size around a specified point.
<<TouchSensor, TouchSensor>>::
  Generates events when the user moves the pointing device across an
  associated piece of geometry, and when the user clicks on said
  geometry.
<<CylinderSensor, CylinderSensor>>::
  Generates events that interpret a user's click-and-drag on a virtual
  cylinder.
<<PlaneSensor, PlaneSensor>>::
  Generates events that interpret a user's click-and-drag as translation
  in two dimensions.
<<SphereSensor, SphereSensor>>::
  Generates events that interpret a user's click-and-drag on a virtual
  sphere.
<<VisibilitySensor, VisibilitySensor>>::
  Generates events as a regions enters and exits rendered view.
<<TimeSensor, TimeSensor>>::
  Generates events at a given time or at given intervals.

[[scripting]]
===== Scripting

<<Script, Script>>::
  Contains a program which can process incoming events and generating
  outgoing ones.

[[interpolator]]
===== Interpolator Nodes

<<ColorInterpolator, ColorInterpolator>>::
  Interpolates intermediate values from a given list of color values.
<<CoordinateInterpolator, CoordinateInterpolator>>::
  Interpolates intermediate values from a given list of 3D vectors.
<<NormalInterpolator, NormalInterpolator>>::
  Interpolates intermediate normalized vectors from a given list of 3D
  vectors.
<<OrientationInterpolator, OrientationInterpolator>>::
  Interpolates intermediate absolute rotations from a given list of
  rotation amounts.
<<PositionInterpolator, PositionInterpolator>>::
  Interpolates intermediate values from a given list of 3D vectors,
  suitable for a series of translations.
<<ScalarInterpolator, ScalarInterpolator>>::
  Interpolates intermediate values from a given list of floating-point
  numbers.

[[changed_nodes]]
===== Changed Node Types

Almost all node types have been changed in one way or another -- if
nothing else, most can now send and receive simple events. The most
far-reaching changes, however, are in the new approaches to grouping
nodes: in particular, Separators have been replaced by Transforms, which
incorporate the fields of the now-defunct Transform node, and Groups no
longer allow state to leak. The other extensive changes are in the
structure of geometry-related nodes (which now occur only as fields in a
Shape node). See the section of the spec titled
"<<StructuringtheSceneGraph, Structuring the Scene Graph>>" for details.

[[deleted_nodes]]
===== Deleted Node Types

The following VRML 1.0 node types have been removed from VRML 2.0:

* AsciiText: replaced with <<Text, Text>>
* Info: replaced with <<WorldInfo, WorldInfo>>
* OrthographicCamera: shifted to browser UI responsibility (that is,
  browsers may provide an orthographic view of a world as an option)
* PerspectiveCamera: replaced with <<Viewpoint, Viewpoint>>.
* Separator: use <<Transform, Transform>> instead
* transformation nodes: incorporated into Transform
** MatrixTransform
** Transform
** Translation
** Rotation
** Scale

[[new_fields]]
===== New Field Types

In addition to all of the other changes, VRML 2.0 introduces a couple of
new field types:

* An <<SFInt32, SFInt32>> field (formerly SFLong) contains a 32-bit integer. 
  An MFInt32 field contains a list of 32-bit integers.
* An <<SFNode, SFNode>> field contains a node (or rather, a pointer to a node). 
  An MFNode field contains a list of pointers to nodes.
* An <<SFTime, SFTime>> field contains a double-precision floating point value 
  indicating a number of seconds since 00:00:00 Jan 1, 1970 GMT.


[[index_html]]
=== Contents

This document is the official and complete specification ** of the
*Virtual Reality Modeling Language*, (VRML), Version 2.0.

* <<foreword_html, Foreword>>
* <<introduction_html, Introduction>>
* <<scope_html, 1 Scope>>
* <<references_html, 2 References>>
* <<glossary_html, 3 Definitions>>
* <<concepts_html, 4 Concepts>>
* <<nodesRef_html, 5 Nodes>>
* <<fieldsRef_html, 6 Fields and Events>>
* <<conformance_html, 7 Conformance>>
* <<grammar_html, A Grammar>>
* <<examples_html, B Examples>>
* <<java_html, C Java>>
* <<javascript_html, D JavaScript>>
* <<bibliography_html, E Bibliography>>
* <<part1_index_html, F Index >>

The *_Foreword_* provides background on the standards process for VRML,
and *_Introduction_* describes the conventions used in the
specification. The following annexes define the specifications for VRML:

_1. *Scope*_ defines the problems that VRML addresses. _2. *References*_ lists the normative standards referenced in the specification.
_3. *Definitions*_ contains the glossary of terminology used in the specification. _4. *Concepts*_ describes various fundamentals of VRML.
_5. *Nodes*_ defines the syntax and semantics of VRML. _6. *Fields*_ specifies the datatype primitives used by nodes.
_7. *Conformance*_ describes the minimum support requirements for a VRML implementation.
There are several appendices included in the specification:

_A. *Grammar*_ presents the BNF for the VRML file format. _B. *Examples*_ includes a variety of VRML example files.
_C. *Java*_ describes how VRML scripting integrates with Java. _D. *JavaScript*_ describes how VRML scripting integrates with JavaScript.
_E. *Bibliography*_ lists the informative, non-standard topics referenced in the specification. _F. *The *Index_* lists the concepts, nodes, and fields in alphabetical order.

The *_Document Change Log_* summarizes significant changes to this
document and *_Credits_* lists the major contributors to this document:

* <<changeLog_html, Document change log>>
* <<credits_html, Credits>>


[[credits_html]]
=== Credits

There are many people that have contributed to the VRML 2.0
Specification. We have listed the major contributors below.

==== Authors

Gavin Bell, gavin@acm.org +
Rikk Carey, rikk@best .com +
Chris Marrin, cmarrin@sgi.com +


==== Contributors

Ed Allard, eda@sgi.com +
Curtis Beeson, curtisb@sgi.com +
Geoff Brown, gb@sgi.com +
Sam T. Denton, denton@maryville.com +
Christopher Fouts, fouts@atlanta.sgi.com +
Rich Gossweiler, dr_rich@sgi.com +
Jan Hardenbergh, jch@jch.com +
Jed Hartman, jed@sgi.com +
Jim Helman, jimh@sgi.com +
Yasuaki Honda, honda@arch.sony.co.jp +
Jim Kent, jkent@sgi.com +
Chris Laurel, laurel@dimensionx.com +
Rodger Lea, rodger@csl.sony.co.jp +
Jeremy Leader, jeremy@worlds.net +
Kouichi Matsuda, matsuda@arch.sony.co.jp +
Mitra, mitra@earth.path.net +
David Mott, mott@best.com +
Chet Murphy, cmurphy@modelworks.com +
Michael Natkin, mjn@sgi.com +
Rick Pasetto, rsp@sgi.com +
Bernie Roehl, broehl@sunee.uwaterloo.ca +
John Rohlf, jrohlf@sgi.com +
Ajay Sreekanth, ajay@cs.berkeley.edu +
Paul Strauss, pss@sgi.com +
Josie Wernecke, josie@sgi.com +
Ben Wing, wing@dimensionx.com +
Daniel Woods, woods@sgi.com  +


==== Reviewers

Yukio Andoh, andoh@dst.nk-exa.co.jp +
Gad Barnea, barnea@easynet.fr +
Philippe F. Bertrand, philippe@vizbiz.com +
Don Brutzman, brutzman@cs.nps.navy.mil +
Sam Chen, sambo@sgi.com +
Mik Clarke, RAZ89@DIAL.PIPEX.COM +
Justin Couch, jtc@hq.adied.oz.au +
Ross Finlayson, raf@tomco.net +
Clay Graham, clay@sgi.com +
John Gwinner, 75162.514@compuserve.com +
Jeremy Leader, jeremy@worlds.net +
Braden McDaniel, braden@shadow.net +
Tom Meyer, tom@tom.com +
Stephanus Mueller, steffel@blacksun.de +
Rob Myers, rob@sgi.com +
Alan Norton, norton@sgi.com +
Tony Parisi, tparisi@intervista.com +
Mark Pesce, mpesce@netcom.com +
Scott S. Ross, ssross@fedex.com +
Hugh Steele, hughs@virtuality.com +
Dave Story, story@sgi.com +
Helga Thorvaldsdottir, helga@sgi.com +
Harrison Ulrich, hrulrich@conline.com +
Chee Yu, chee@netgravity.com +
The entire VRML community, www-vrml@wired.com +

Contact rikk@best.com, cmarrin@sgi.com, or mailto:gavin@acm.org[gavin@acm,org] 
with questions or comments.


[[foreword_html]]
==== Foreword

ISO (the International Organization for Standardization) and IEC (the
International Electrotechnical Commission) form the specialized system
for worldwide standardization. National bodies that are members of ISO
or IEC participate in the development of International Standards through
technical committees established by the respective organization to deal
with particular fields of technical activity. ISO and IEC technical
committees collaborate in fields of mutual interest. Other international
organizations, governmental and non-governmental, in liaison with ISO
and IEC, also take part in the work.

In the field of information technology, ISO and IEC have established a
joint technical committee, ISO/IEC JTC 1. Draft International Standards
adopted by the joint technical committee are circulated to national
bodies for voting. Publication as an International Standard requires
approval by at least 75% of the national bodies casting a vote.

International Standard ISO/IEC 14772 was prepared by Joint Technical
Committee ISO/IEC JTC 1, Information Technology Sub-Committee 24,
Computer Graphics and Image Processing in collaboration with the VRML
Architecture Group (VAG, http://vag.vrml.org) and the VRML moderated
email list (http://www.wired.com/vrml/[www-vrml@wired.com]).

ISO/IEC 14772 is a single part standard, under the general title of
Information Technology - Computer Graphics and Image Processing -
Virtual Reality Modeling Language (VRML).


Contact rikk@best.com, cmarrin@sgi.com, or gavin@acm.org with questions or comments.

[[introduction_html]]
=== Introduction

==== Purpose

The Virtual Reality Modeling Language (VRML) is a file format for
describing 3D interactive worlds and objects. It may be used in
conjunction with the World Wide Web. It may be used to create
three-dimensional representations of complex scenes such as
illustrations, product definition and virtual reality presentations.

==== Design Criteria

VRML has been designed to fulfill the following requirements:

*_Authorability_* ::
  Make it possible to develop application generators and editors, as
  well as to import data from other industrial formats.
*_Completeness_* ::
  Provide all information necessary for implementation and address a
  complete feature set for wide industry acceptance.
*_Composability_* ::
  The ability to use elements of VRML in combination and thus allow
  re-usability.
*_Extensibility_* ::
  The ability to add new elements.
*_Implementability_* ::
  Capable of implementation on a wide range of systems.
*_Multi-user potential_* ::
  Should not preclude the implementation of multi-user environments.
*_Orthogonality_* ::
  The elements of VRML should be independent of each other, or any
  dependencies should be structured and well defined.
*_Performance_* ::
  The elements should be designed with the emphasis on interactive
  performance on a variety of computing platforms.
*_Scalability_* ::
  The elements of VRML should be designed for infinitely large
  compositions.
*_Standard practice_* ::
  Only those elements that reflect existing practice, that are necessary
  to support existing practice, or that are necessary to support
  proposed standards should be standardized.
*_Well-structured_* ::
  An element should have a well-defined interface and a simply stated
  unconditional purpose. Multipurpose elements and side effects should
  be avoided.

==== Characteristics of VRML

VRML is capable of representing static and animated objects and it can
have hyperlinks to other media such as sound, movies, and image.
Interpreters (browsers) for VRML are widely available for many different
platforms as well as authoring tools for the creation VRML files.

VRML supports an extensibility model that allows new objects to be
defined and a registration process to allow application communities to
develop interoperable extensions to the base standard. There is a
mapping between VRML elements and commonly used 3D application
programmer interface (API) features.

==== *Conventions used in the specification*

_Field names_ are in _italics_. File format and api are
in`+ +`*`+bold, fixed-spacing+`*.

New terms are in _italics_.


Contact rikk@best.com , cmarrin@sgi.com, or gavin@acm.org with questions or comments.

[[scope_html]]
=== 1. Scope and Field of Application

The scope of the standard incorporates the following:

* a mechanism for storing and transporting two-dimensional and
  three-dimensional data
* elements for representing two-dimensional and three-dimensional
  primitive information
* elements for defining characteristics of such primitives
* elements for viewing and modeling two-dimensional and
  three-dimensional information
* a container mechanism for incorporating data from other metafile formats
* mechanisms for defining new elements which extend the capabilities of
  the metafile to support additional types and forms of information


Contact rikk@best.com, cmarrin@sgi.com, or gavin@acm.org with questions or comments.

[[references_html]]
=== 2. References

This annex contains the list of published standards referenced by the
specification. See "_<<bibliography_html, Appendix E. Bibliography>>"_
for a list of informative documents and technology.

JPEG::

  "Joint Photographic Experts Group", International Organization for
  Standardization, "Digital Compression and Coding of Continuous-tone
  Still Images, Part 1: Requirements and guidelines" ISO/IEC IS 10918-1,
  1991,
  [ http://www.iso.ch/isob/switch-engine-cate.pl?KEYWORDS=10918&searchtype=refnumber, ftp://rtfm.mit.edu/pub/usenet/news.answers/jpeg-faq/part1 ].

MIDI::

  "Musical Instrument Digital Interface", International MIDI Association,
  23634 Emelita Street, Woodland Hills, California 91367 USA, 1983,
  [ ftp://rtfm.mit.edu/pub/usenet/news.answers/music/midi/bibliography ].
MPEG::

  "Motion Picture Experts Group", International Organization
  for Standardization, ISO/IEC IS 11172-1:1993,
  [ http://www.iso.ch/isob/switch-engine-cate.pl?searchtype=refnumber&KEYWORDS=11172 ].
PNG::

  "PNG (Portable Network Graphics), Specification Version 0.96", W3C
  Working Draft 11-Mar-1996, [http://www.w3.org/pub/WWW/TR/WD-png ,
  http://www.boutell.com/boutell/png/ ].

RURL::

  "Relative Uniform Resource Locator", IETF RFC 1808,
  [http://ds.internic.net/rfc/rfc1808.txt ].

URL::

  "Uniform Resource Locator", IETF RFC 1738,
  [http://ds.internic.net/rfc/rfc1738.txt ]

UTF8::

  "Information Technology Universal Multiple-Octet Coded Character Set
  (UCS)", Part 1: Architecture and Basic Multi-Lingual Plane, ISO/IEC
  10646-1:1993, [http://www.iso.ch/cate/d18741.html ,
  http://www.dkuug.dk/JTC1/SC2/WG2/docs/n1335 ].

WAV::

  Waveform Audio File Format, "Multimedia Programming Interface and Data
  Specification v1.0", Issued by IBM & Microsoft, 1991,
  [ftp://ftp.cwi.nl/pub/audio/RIFF-format].


Contact rikk@best.com, cmarrin@sgi.com, or gavin@acm.org with questions or comments.

[[glossary_html]]
=== 3. Definitions

[[AppearanceNode]]
==== appearance node

A node of type Appearance, FontStyle, ImageTexture, Material,
MovieTexture, PixelTexture, or TextureTransform. Appearance nodes
control the rendered appearance of the geometry nodes with which they
are associated.

[[BindableLeafNode]]
==== bindable leaf node

A node of type Background, Fog, NavigationInfo, or Viewpoint. These
nodes may have many instances in a scene graph, but only one instance
may be active at any instant of time.

[[ChildrenNodes]]
==== children nodes

Nodes which are parented by grouping nodes and thus are affected by the
transformations of all ancestors. 
See "<<GroupingNodes, Concepts - Grouping and Children Nodes>>" 
for list of allowable children nodes.

[[ColourModel]]
==== colour model

Characterization of a colour space in terms of explicit parameters.VRML
allows colors to be defined only with the RGB color model.

[[DisplayDevice]]
==== display device

A graphics device on which VRML scenes can be represented.

[[DragSensor]]
==== drag sensor

Drag sensors (CylinderSensor, PlaneSensor, SphereSensor) cause events to
be generated in response to pointer motions which are sensor-dependent.
For example, the SphereSensor generates spherical rotation events. 
See "<<DragSensors, Concepts - Drag Sensors>>" for details.

[[Event]]
==== event

Messages sent from one node to another as defined by a Route. Events
signal changes to field values, external stimuli, interactions between
nodes, etc.

[[ExposedField]]
==== exposed field

A field which can receive events to change its value(s) and generates
events when its value(s) change.

[[def_ExecutionModel]]
==== execution model

The characterization of the way in which scripts execute within the
context of VRML.

[[ExternalPrototype]]
==== external prototype

Prototypes defined in external files and referenced by a URL.

[[Field]]
==== field

The parameters that distinguish one node from another of the same type.
Fields can contain various kind of data and one or many values.

[[GeometryNode]]
==== geometry node

Nodes of type Box, Cone, Cylinder, ElevationGrid, Extrusion,
IndexedFaceSet, IndexedLineSet, PointSet, Sphere, and Text which contain
mathematical descriptions of three-dimensional points, lines, surfaces,
text strings and solid objects.

[[GeometricPropertyNode]]
==== geometric property node

A node of type Color, Coordinate, Normal, or TextureCoordinate. These
nodes define the properties of specific geometry nodes.

[[GeometricSensorNode]]
==== geometric sensor node

A node of type ProximitySensor, VisibilitySensor, TouchSensor,
CylinderSensor, PlaneSensor, or SphereSensor. These nodes generate
events based on user actions, such as a mouse click or navigating close
to a particular object.

[[GroupingNode]]
==== grouping node

A node of type Anchor, Billboard, Collision, Group, or Transform. These
nodes group child nodes and other grouping nodes together and cause the
group to exhibit special behavior which is dependent on the node type.

[[IETF]]
==== IETF

Internet Engineering Task Force. The organization which develops
Internet standards.

[[Instance]]
==== instance

An instantiation of a previously defined node created by the USE syntax.

[[InterpolatorNode]]
==== interpolator node

A node of type ColorInterpolator, CoordinateInterpolator,
NormalInterpolator, OrientationInterpolator, PositionInterpolator, or
ScalarInterpolator.These nodes define a piece-wise linear interpolation
of a particular type of value at specified times.

[[JPEG]]
==== JPEG

Joint Photographic Experts Group.

[[MIDI]]
==== MIDI

Musical Instrument Digital Interface - a standard for digital music
representation.

[[MIME]]
==== MIME

Multipurpose Internet Mail Extension used to specify filetyping rules
for browsers. 
See "<<Filetype, Concepts - File Extension and MIME Types>>"
for details.

[[Node]]
==== node

The fundamental component of a scene graph in VRML. Nodes are
abstractions of various real-world objects and concepts. Examples
include spheres, lights, and material descriptions. Nodes contain
fields, and events. Messages are sent between nodes via routes.

[[NodeType]]
==== node type

A required parameter for each node that describes, in general, its
particular semantics. For example, Box, Group, Sound, and SpotLight. See
"<<NFE, Concepts - Nodes, Fields, and Events>>" 
and "<<nodesRef_html, Nodes Reference>>" for details.

[[Prototype]]
==== prototype

The definition of a new node type in terms of the nodes defined in this
standard.

[[RGB]]
==== RGB

The VRML colour model. Each colour is represented as a combination of
the three primary colours red, green, and blue.

[[Route]]
==== route

The connection between a node generating an event and a node receiving
an event.

[[SceneGraph]]
==== scene graph

An ordered collection of grouping nodes and leaf nodes. Grouping nodes,
such as Transform, LOD, and Switch nodes, can have child nodes. These
children can be other grouping nodes or leaf nodes, such as shapes,
browser information nodes, lights, viewpoints, and sounds.

[[SensorNode]]
==== sensor node

A node of type Anchor, CylinderSensor, PlaneSensor, ProximitySensor,
SphereSensor, TimeSensor, TouchSensor, or VisibilitySensor. These nodes
detect changes and generate events. Geometric sensor nodes generate
events based on user actions, such as a mouse click or navigating close
to a particular object. TimeSensor nodes generate events at regular
intervals in time.

[[SpecialGroupNode]]
==== special group node

A node of type LOD (level of detail), InLine, or Switch. These nodes are
grouping nodes which exhibit special behavior, such as selecting one of
many children to be rendered based on a dynamically changing parameter
value or dynamically loading its children from an external file.

[[TextureCoordinates]]
==== texture coordinates

The set of 2D coordinates used by vertex-based geometry nodes (e.g.
IndexedFaceSet and ElevationGrid) and specified in the TextureCoordinate
node to map textures to the vertices of some geometry nodes. Texture
coordinates range from 0 to 1 across the texture image.

[[TextureTransform_]]
==== texture transform

A node which defines a 2D transformation that is applied to texture
coordinates.

[[URL]]
==== URL

Uniform Resource Locator as defined in IETF RFC 1738.

[[URN]]
==== URN

Uniform Resource Name

[[VRMLDocumentServer]]
==== VRML document server

An application that locates and transmits VRML files and supporting
files to VRML client applications (browsers).

[[VRMLFile]]
==== VRML file

A file containing information encoded according to this standard.


Contact rikk@best.com link:rikk@best.com[] , cmarrin@sgi.com, or gavin@acm.org with questions or comments.

[[concepts_html]]
=== 4. Concepts

This section describes key concepts relating to the definition and use
of the VRML specification. This includes how nodes are combined into
scene graphs, how nodes receive and generate events, how to create node
types using prototypes, how to add node types to VRML and export them
for use by others, how to incorporate programmatic scripts into a VRML
file, and various general topics on nodes.

4.1 *<<FileSyntaxandStructure, File Syntax and Structure>>* +
4.1.1 <<SyntaxBasics, Syntax Basics>> +
4.1.2 <<FileSyntaxVsPublicInterface, File Syntax vs. Public Interface>> +
4.1.3 <<URLsAndURNs, URLs and URNs>> +
4.1.4 <<DataProtocol, Data Protocol>> +
4.1.5 <<CustomProtocol, Scripting Language Protocols>> +
4.1.6 <<Filetype, File Extension and MIME Types>> +
4.1.7 <<URNs, URNs>> +
4.2 *<<NFE, Node, Field, and Events>>* +
4.2.1 <<NodesIntroduction, Introduction>> +
4.2.2 <<GeneralNodeCharacteristics, General Node Characteristics>> +
4.3 *<<StructuringtheSceneGraph, The Structure of the Scene Grap**h**>>* +
4.3.1 <<GroupingNodes, Grouping and Children Nodes>> +
4.3.2 <<Instancing, Instancing>> +
4.3.3 <<StandardUnits, Standard Units>> +
4.3.4 <<CoordinateSystems, Coordinate Systems and Transformations>> +
4.3.5 <<ViewingModel, Viewing Model>> +
4.3.6 <<BoundingBoxes, Bounding Boxes>> +
4.4 *<<Events, Events>>* +
4.4.1 <<Routes, Routes>> +
4.4.2 <<Sensors, Sensors>> +
4.4.3 <<ExecutionModel, Execution Model>> +
4.4.4 <<Loops, Loops>> +
4.4.5 <<Fan-in, Fan-in and Fan-out>> +
4.5 *<<Time, Time>>* +
4.5.1 <<TimeIntroduction, Introduction>> +
4.5.1 <<DiscreteAndContinuousChanges, Discrete and Continuous Changes>> +
4.6 *<<Prototypes, Prototypes>>* +
4.6.1 <<IntroductionToPrototypes, Introduction to Prototypes>> +
4.6.2 <<ISStatement, IS Statement>> +
4.6.3 <<PrototypeScopingRules, Prototype Scoping Rules>> +
4.6.4 <<EXTERNPROTO, Defining Prototypes in External files>> +
4.7 *<<Scripting, Scripting>>* +
4.7.1 <<ScriptingIntroduction, Introduction>> +
4.7.2 <<ScriptExecution, Script Execution>> +
4.7.3 <<initialize, Initialize and _Shutdown_>> +
4.7.4 <<eventsProcessed, eventsProcessed>> +
4.7.5 <<ScriptsWithDirectOutputs, Scripts with Direct Outputs>> +
4.7.6 <<AsynchronousScripts, Asynchronous Scripts>> +
4.7.7 <<ScriptLanguages, Script Languages>> +
4.7.8 <<EventInHandling, EventIn Handling>> +
4.7.9 <<AccessingFieldsAndEvents, Accessing Fields and Events>> +
4.7.10 <<BrowserInterface, Browser Script Interface>> +
4.8 *<<BrowserExtensions, Browser Extensions>>* +
4.8.1 <<CreatingExtensions, Creating Extensions>> +
4.8.2 <<ReadingExtensions, Reading Extensions>> +
4.9 *<<NodeConcepts, Node Concepts>>* +
4.9.1 <<BindableLeafNodes, Bindable Child Nodes>> +
4.9.2 <<GeometryNodes, Geometry>> +
4.9.3 <<InterpolatorNodes, Interpolators>> +
4.9.4 <<Lights, Light Sources>> +
4.9.5 <<Lighting, Lighting Model>> +
4.9.6 <<SensorNodes, Sensor Nodes>> +
4.9.7 <<TimeDependentNodes, Time Dependent Nodes>> +

[[FileSyntaxandStructure]]
==== 4.1 File Syntax and Structure

[[SyntaxBasics]]
===== 4.1.1 Syntax Basics

For easy identification of VRML files, every VRML 2.0 file must begin
with the characters:

[source]
....
        #VRML V2.0 utf8
....

The identifier utf8 allows for international characters to be displayed
in VRML using the UTF-8 encoding of the ISO 10646 standard. Unicode is
an alternate encoding of ISO 10646. UTF-8 is explained under the
<<Text, Text>> node.

Any characters after these on the same line are ignored. The line is
terminated by either the ASCII newline or carriage-return characters.

The # character begins a comment; all characters until the next newline
or carriage return are ignored. The only exception to this is within
double-quoted SFString and MFString fields, where the # character will
be part of the string.

Note: Comments and whitespace may not be preserved; in particular, a
VRML document server may strip comments and extra whitespace from a VRML
file before transmitting it. <<WorldInfo, WorldInfo>>
nodes should be used for persistent information such as copyrights or
author information. To extend the set of existing nodes in VRML 2.0, use
prototypes or external prototypes rather than named information nodes.

Commas, blanks, tabs, newlines and carriage returns are whitespace
characters wherever they appear outside of string fields. One or more
whitespace characters separate the syntactical entities in VRML files,
where necessary.

After the required header, a VRML file can contain any combination of
the following:

* Any number of prototypes (see "_<<Prototypes, Prototypes>>")_
* Any number of _children_ nodes (see "_<<GroupingNodes, Grouping and Children Nodes>>")_
* Any number of <<Routes, ROUTE>> statements ( see "<<Routes, Routes>>")

See the "<<grammar_html, Grammar Reference>>" annex for precise grammar rules.

Field, event, prototype, and node names must not begin with a digit
(0x30-0x39) but may otherwise contain any characters except for
non-printable ASCII characters (0x0-0x20), double or single quotes
(0x22: ", 0x27: '), sharp (0x23: #), plus (0x2b: +), comma (0x2c: ,),
minus (0x2d: -), period (0x2e: .), square brackets (0x5b, 0x5d: `[]`),
backslash (0x5c: `)` or curly braces (0x7b, 0x7d: `{}`). Characters in
names are as specified in ISO 10646, and are encoded using UTF-8. VRML
is case-sensitive; "Sphere" is different from "sphere" and "BEGIN" is
different from "begin."

The following reserved keywords shall not be used for node, PROTO,
EXTERNPROTO, or DEF names:

[opts="header,autowidth",frame=ends,grid=rows]
|===
|*`DEF`* |*`EXTERNPROTO`* |*`FALSE`* |*`IS`* |*`NULL`* |*`PROTO`* |*`ROUTE`*

|*`TO`* |*`TRUE`* |*`USE`* |*`eventIn`* |*`eventOut`* |*`exposedField`* |*`field`*
|===

[[FileSyntaxVsPublicInterface]]
===== 4.1.2 File Syntax vs. Public Interface

In this document, the first item in a link:nodesRef_html[node specification]
is the public interface for the node. The syntax for the
public interface is the same as that for that node's prototype. This
interface is the definitive specification of the fields, events, names,
types, and default values for a given node. Note that this syntax is
not the actual file format syntax. However, the parts of
the interface that are identical to the file syntax are in *bold*. For
example, the following defines the `Collision` node's public interface
and file format:

[source]
....
    Collision { 
      eventIn      MFNode   addChildren
      eventIn      MFNode   removeChildren
      exposedField MFNode   children        []
      exposedField SFBool   collide         TRUE
      field        SFVec3f  bboxCenter      0 0 0
      field        SFVec3f  bboxSize        -1 -1 -1
      field        SFNode   proxy           NULL
      eventOut     SFTime   collideTime
    }
....

Fields that have associated implicit _set__ and __changed_ events are
labeled *exposedField*. For example, the _on_ field has an implicit
_set_on_ input event and an _on_changed_ output event. Exposed fields
may be connected using *ROUTE* statements, and may be read and/or
written by Script nodes. Also, any exposedField or EventOut name can be
prefixed with _get__ to indicate a read of the current value of the
eventOut. This is used only in Script nodes or when accessing the VRML
world from an external API.

Note that this information is arranged in a slightly different manner in
the actual file syntax. The keywords "`field`" or "`exposedField`"
and the types of the fields (e.g. `SFColor`) are not
specified when expressing a node in the file format. An example of the
file format for the `Collision` node is:

[source]
....
Collision {
  children        []
  collide         TRUE
  bboxCenter      0 0 0
  bboxSize        -1 -1 -1
  proxy           NULL
}
....

The rules for naming fields, exposedFields, eventOuts and eventIns for
the built-in nodes are as follows:

* All names containing multiple words start with a lower case letter and
  the first letter of all subsequent words are capitalized (e.g. _bboxCenter_), 
  with the exception of _get__ and __changed_ described below.
* All eventIns have the prefix "_set__" - with the exception of the
  _addChildren_ and _removeChildren_ eventIns.
* All eventOuts have the suffix "__changed_" appended - with the
  exception of eventOuts of type SFBool. Boolean eventOuts begin with the
  word "_is_" (e.g. _isFoo_) for better readability.
* All eventIns and eventOuts of type SFTime do not use the "_set__"
  prefix or "__changed_" suffix.

User defined field names found in Script and PROTO nodes are recommended
to follow these naming conventions, but are not required.

[[URLsAndURNs]]
===== 4.1.3 URLs and URNs

A _URL_ (Uniform Resource Locator) <<references_html, URL>>
specifies a file located on a particular server and accessed through a
specified protocol (e.g. http). A _URN_ (Uniform Resource Name)
<<bibliography_html, URN>> provides a more abstract way to
refer to data than is provided by a URL.

All URL/URN fields are of type MFString. The strings in the field
indicate multiple locations to look for data, in decreasing order of
preference. If the browser cannot locate the first URL/URN or doesn't
support the protocol type, then it may try the second location, and so
on. Note that the URL and URN field entries are delimited by " ", and due to the "<<DataProtocol, Data Protocol>> "and the
"<<CustomProtocol, Scripting Language Protocols>>" are a superset of
the standard URL syntax (IETF RFC 1738). Browsers may skip to the next
URL/URN by searching for the closing, un-escaped ". 
See "<<MFString, Field and Event Reference - SFString and MFString>>" 
for details on the string field.

URLs are described in "Uniform Resource Locator", IETF
RFC 1738, http://ds.internic.net/rfc/rfc1738.txt.

Relative URLs are handled as described in "Relative Uniform Resource Locator", 
IETF RFC 1808, http://ds.internic.net/rfc/rfc1808.txt.

VRML 2.0 browsers are not required to support URNs. If they do not
support URNs, they should ignore any URNs that appear in MFString fields
along with URLs.


See "<<URNs, URN's>>" for more details on URNs.

[[DataProtocol]]
===== 4.1.4 Data Protocol

The IETF is in the process of standardizing a "Data:" URL to be used for
inline inclusion of base64 encoded data, such as JPEG images. This
capability should be supported as specified in: "_Data: URL scheme_",
http://www.internic.net/internet-drafts/draft-masinter-url-data-01.txt.,
<<references_html, DATA>>. Note that this is an Internet
Draft, and the specification may (but is unlikely to) change.

[[CustomProtocol]]
===== 4.1.5 Scripting Language Protocols

The Script node's URL field may also support a custom protocol for the
various scripting languages. For example, a script URL prefixed with
_javascript:_ shall contain JavaScript source, with newline characters
allowed in the string. A script prefixed with _javabc:_ shall contain
Java bytecodes using a base64 encoding. The details of each language
protocol are defined in the appendix for each language. Browsers are not
required to support any specific scripting language, but if they do then
they shall adhere to the protocol for that particular scripting
language. The following example, illustrates the use of mixing custom
protocols and standard protocols in a single url (order of precedence
determines priority):


[source]
....
#VRML V2.0 utf8
Script {
    url [ "javascript: ...",             # custom protocol JavaScript
          "http://bar.com/foo.js",       # std protocol JavaScript
          "http://bar.com/foo.class" ]   # std protocol Java byte
}
....

[[Filetype]]
===== 4.1.6 File Extension and Mime Type

The file extension for VRML files is `.wrl` (for _world_).

The official MIME type for VRML files is defined as:

[source]
....
       model/vrml
....

where the MIME major type for 3D data descriptions is `model`, and
the minor type for VRML documents is `vrml`.

For historical reasons (VRML 1.0) the following MIME type must also be
supported:

[source]
....
       x-world/x-vrml
....

where the MIME major type is `+x-world,+` and the minor type for VRML
documents is `+x-vrml+`.

IETF work-in-progress on this subject can be found in "

ftp://ds.internic.net/internet-drafts/draft-nelson-model-mail-ext-01.txt[The
Model Primary Content Type for Multipurpose Internet Mail Extensions]",
(ftp://ds.internic.net/internet-drafts/draft-nelson-model-mail-ext-01.txt).

[[URNs]]
===== 4.1.7 URN's

URN's are location independent pointers to a file, or to different
representations of the same content. In most ways they can be used like
URL's except that when fetched a smart browser should fetch them from
the closest source. While URN resolution over the net has not been
standardized yet, they may be used now as persistent unique identifiers
for files, prototypes, textures etc. For more information on the
standardization effort see: http://services.bunyip.com:8000/research/ietf/urn-ietf/ . 
VRML 2.0 browsers are not required to support URN's however they are 
required to ignore them if they do not support them.

URN's may be assigned by anyone with a domain name, for example if the
company Foo owns foo.com then it may allocate URN's that begin with
"urn:inet:foo.com:" such as, for example "urn:inet:foo.com:texture/wood01". 
No special semantics are required of the string following the prefix, except 
that they should be lower case, and characters should be "URL" encoded as 
specified in RFC1738.

To reference a texture, proto or other file by URN it should be included
in the url field of another node, for example:

[source]
....
    ImageTexture {
        url [ "http://www.foo.com/textures/woodblock_floor.gif",
              "urn:inet:foo.com:textures/wood001" ]
    } 
....

specifies a URL file as the first choice and URN as the second choice.
Note that until URN resolution is widely deployed, it is advisable to
include a URL alternative whenever a URN is used. See
http://earth.path.net/mitra/papers/vrml-urn.html for more details and
recommendations.


[[NFE]]
==== 4.2 Nodes, Fields, and Events

[[NodesIntroduction]]
===== 4.2.1 Introduction

At the highest level of abstraction, VRML is simply a file format for
describing objects. Theoretically, the objects can contain anything --
3D geometry, MIDI data, JPEG images, and so on. VRML defines a set of
objects useful for doing 3D graphics, multi-media, and interactive
object/world building. These objects are called _nodes_, and contain
elemental data which is stored in _fields_ and _events._

[[GeneralNodeCharacteristics]]
===== 4.2.2 General Node Characteristics

A node has the following characteristics:

* *A type name -* This is a name like Box, Color, Group, Sphere, Sound,
  SpotLight, and so on.
* *The parameters that distinguish a node from other nodes of the same
  type -* For example, each Sphere node might have a different radius, and
  different spotlights have different intensities, colors, and locations.
  These parameters are called <<fieldsRef_html, fields>>. A node can have
  0 or more fields. Each node specification defines the type, name, and
  default value for each of its fields. The default value for the field is
  used if a value for the field is not specified in the VRML file. The
  order in which the fields of a node are read does not matter. For
  example, "Cone \{ bottomRadius 1 height 6 }" and "Cone \{ height 6
  bottomRadius 1}" are equivalent. There are two kinds of fields:
*_field_* and *_exposedField_*. _Fields_ define the initial values for
  the node's state, but cannot be changed and are considered private.
  _ExposedFields_ also define the initial value for the node's state, but
  are public and may be modified by other nodes.
* *A set of associated events that nodes can receive and send -* Nodes
  can receive a number of incoming _set__ events, denoted as *_eventIn_*,
  (such as _set_position_, _set_color_, and _set_on_), which typically
  change the node. Nodes can also send out a number of __changed_ events,
  denoted as *_eventOut_*, which indicate that something in the node has
  changed (for example, _position_changed_, _color_changed_,
  _on_changed_). The _exposedField_ keyword may be used as a short-hand
  for specifying that a given field has a _set__ eventIn that is directly
  wired to a field value and a __changed_ eventOut. For example, the
  declaration:
+
....
    exposedField foo
....
+
is equivalent to the declaration:
+
....
    eventIn set_foo
    field foo
    eventOut foo_changed
....
+
where _set_foo,_ if written to, automatically sets the value of the
field _foo_ and generates a _foo_changed_ eventOut.

The file syntax for representing nodes is as follows:

[source]
....
      nodetype { fields }
....

Only the node type and braces are required; nodes may or may not have
field values specified. Unspecified field values are set to the default
values in the specification.


[[StructuringtheSceneGraph]]
==== 4.3 The Structure of the Scene Graph

This section describes the general scene graph hierarchy, how to reuse
nodes within a file, coordinate systems and transformations in VRML
files, and the general model for viewing and interaction within a VRML
world.

[[GroupingNodes]]
===== 4.3.1 Grouping and Children Nodes

Grouping nodes are used to create hierarchical transformation graphs.
Grouping nodes have a _children_ field that contains a list of nodes
which are the transformation descendants of the group. Each grouping
node defines a coordinate space for its children. This coordinate space
is relative to the parent node's coordinate space--that is,
transformations accumulate down the scene graph hierarchy. Children
nodes are restricted to the following node types:

[%autowidth,cols=",,",]
|===
|<<Anchor, Anchor>> |<<LOD, LOD>>
|<<Sound, Sound>>

|<<Background, Background>>
|<<NavigationInfo, NavigationInfo>>
|<<SpotLight, SpotLight>>

|<<Billboard, Billboard>>
|<<NormalInterpolator, NormalInterpolator>>
|<<SphereSensor, SphereSensor>>

|<<Collision, Collision>>
|<<OrientationInterpolator, OrientationInterpolator>>
|<<Switch, Switch>>

|<<ColorInterpolator, ColorInterpolator>>
|<<PlaneSensor, PlaneSensor>>
|<<TimeSensor, TimeSensor>>

|<<CoordinateInterpolator, CoordinateInterpolator>>
|<<PointLight, PointLight>>
|<<TouchSensor, TouchSensor>>

|<<CylinderSensor, CylinderSensor>>
|<<PositionInterpolator, PositionInterpolator>>
|<<Transform, Transform>>

|<<DirectionalLight, DirectionalLight>>
|<<ProximitySensor, ProximitySensor>>
|<<Viewpoint, Viewpoint>>

|<<Fog, Fog>>
|<<ScalarInterpolator, ScalarInterpolator>>
|<<VisibilitySensor, VisibilitySensor>>

|<<Group, Group>> |<<Script, Script>>
|<<WorldInfo, WorldInfo>>

|<<Inline, Inline>> |<<Shape, Shape>>
|<<Prototypes, PROTO'd child nodes>>
|===

All grouping nodes also have _addChildren_ and _removeChildren_ eventIn
definitions. The _addChildren_ event adds the node(s) passed in to the
grouping node's _children_ field. Any nodes passed to the _addChildren_
event that are already in the group's children list are ignored. The
_removeChildren_ event removes the node(s) passed in from the grouping
node's _children_ field. Any nodes passed in the _removeChildren_ event
that are not in the grouping nodes's _children_ list are ignored.

The following nodes are grouping nodes:

| <<Anchor, Anchor>>
| <<Billboard, Billboard>>
| <<Collision, Collision>>
| <<Group, Group>>
| <<Transform, Transform>>

[[Instancing]]
===== 4.3.2 Instancing

A node may be referenced in a VRML file multiple times. This is called
_instancing_ (using the same instance of a node multiple times; called
"sharing", "aliasing" or "multiple references" by other systems) and is
accomplished by using the DEF and USE keywords.

The **DEF** keyword defines a node's name and creates a node of that type.
The **USE** keyword indicates that a reference to a previously named node
should be inserted into the scene graph. This has the affect of sharing
a single node in more than one location in the scene. If the node is
modified, then all references to that node are modified. DEF/USE name
scope is limited to a single file. If multiple nodes are given the same
name, then the last DEF encountered during parsing is used
for USE definitions.

Tools that create VRML files may need to modify user-defined node names
to ensure that a multiply instanced node with the same name as some
other node will be read correctly. The recommended way of doing this is
to append an underscore followed by an integer to the user-defined name.
Such tools should automatically remove these automatically generated
suffixes when VRML files are read back into the tool (leaving only the
user-defined names).

Similarly, if an un-named node is multiply instanced, tools will have to
automatically generate a name to correctly write the VRML file. The
recommended form for such names is just an underscore followed by an
integer.

[[StandardUnits]]
===== 4.3.3 Standard Units

VRML provides no capability to define units of measure. All linear
distances are assumed to be in meters and all angles are in radians.
Time units are specified in seconds. Colors are specified in the RGB
(Red-Green-Blue) color space and are restricted to the 0.0 to 1.0 range.

[[CoordinateSystems]]
===== 4.3.4 Coordinate Systems and Transformations

VRML uses a Cartesian, right-handed, 3-dimensional coordinate system. By
default, objects are projected onto a 2-dimensional display device by
projecting them in the direction of the positive Z-axis, with the
positive X-axis to the right and the positive Y-axis up. A modeling
transformation (<<Transform, Transform>> and
<<Billboard, Billboard>>) or viewing transformation
(<<Viewpoint, Viewpoint>>) can be used to alter this
default projection.

Scenes may contain an arbitrary number of _local_ (or _object-space_)
_coordinate systems_, defined by the transformation fields of the
Transform and Billboard nodes.

Conceptually, VRML also has a _world coordinate system_. The various
local coordinate transformations map objects into the world coordinate
system, which is where the scene is assembled. Transformations
accumulate downward through the scene graph hierarchy, with each
Transform and Billboard inheriting transformations of their parents.
(Note however, that this series of transformations _takes effect_ from
the leaf nodes _up_ through the hierarchy. The local transformations
closest to the Shape object take effect first, followed in turn by each
successive transformation upward in the hierarchy.)

[[ViewingModel]]
===== 4.3.5 Viewing Model

This specification assumes that there is a real person viewing and
interacting with the VRML world. The VRML author may place any number of
_viewpoints_ in the world -- interesting places from which the user
might wish to view the world. Each viewpoint is described by a
<<Viewpoint, Viewpoint>> node. Viewpoints exist in a specific 
coordinate system, and both the viewpoint and the coordinate system 
may be animated. Only one Viewpoint may be active at a time. See the 
description of "<<BindableLeafNodes, Bindable Children Nodes>>" for details. 
When a viewpoint is activated, the browser parents its view (or camera) into
the scene graph under the currently active viewpoint. Any changes to the
coordinate system of the viewpoint have effect on the browser view.
Therefore, if a user teleports to a viewpoint that is moving (one of its
parent coordinate systems is being animated), then the user should move
along with that viewpoint. It is intended, but not required, that
browsers support a user-interface by which users may "teleport"
themselves from one viewpoint to another.

[[BoundingBoxes]]
===== 4.3.6 Bounding Boxes

Several of the nodes in this specification include a bounding box field.
This is typically used by grouping nodes to provide a hint to the
browser on the group's approximate size for culling optimizations. The
default size for bounding boxes (-1, -1, -1) implies that the user did
not specify the bounding box and the browser must compute it or assume
the most conservative case. A _bboxSize_ value of (0, 0, 0) is valid and
represents a point in space (i.e. infinitely small box). Note that the
bounding box of may change as a result of changing children. The
_bboxSize_ field values must be >= 0.0. Otherwise, results are
undefined. The _bboxCenter_ fields specify a translation offset from the
local coordinate system and may be in the range: _-infinity_ to
_+infinity_.

The _bboxCenter_ and _bboxSize_ fields may be used to specify a maximum
possible bounding box for the objects inside a grouping node (e.g.
Transform). These are used as hints to optimize certain operations such
as determining whether or not the group needs to be drawn. If the
specified bounding box is smaller than the true bounding box of the
group, results are undefined. The bounding box should be large enough to
completely contain the effects of all sounds, lights and fog nodes that
are children of this group. If the size of this group may change over
time due to animating children, then the bounding box must also be large
enough to contain all possible animations (movements). The bounding box
should typically be the union of the group's children bounding boxes; it
should not include any transformations performed by the group itself
(i.e. the bounding box is defined in the local coordinate system of the
group).


[[Events]]
==== 4.4 Events

Most nodes have at least one eventIn definition and thus can receive
_events._ Incoming events are data messages sent by other nodes to
change some state within the receiving node. Some nodes also have
eventOut definitions. These are used to send data messages to
destination nodes that some state has changed within the source node.

If an eventOut is read before it has sent any events (e.g.
_get_foo_changed_), the _initial_ _value_ as specified in 
"<<fieldsRef_html, Field and Event Reference>>" for each field/event
type is returned.

[[Routes]]
===== 4.4.1 Routes

The connection between the node generating the event and the node
receiving the event is called a _route_. A node that produces events of
given type can be routed to a node that receives events of the same type
using the following syntax:

[source]
....
  ROUTE NodeName.eventOutName_changed TO NodeName.set_eventInName
....

The prefix _set__ and the suffix __changed_ are
recommended conventions, not strict rules. Thus, when
creating prototypes or scripts, the names of the eventIns and the
eventOuts may be any legal identifier name. Note however, that
exposedField's implicitly define _set_xxx_ as an eventIn, _xxx_changed_
as an eventOut, and _xxx_ as a field for a given exposedField named
_xxx_. It is strongly recommended that developers follow these
guidelines when creating new types. There are three exceptions in the
VRML Specification to this recommendation: Boolean events, Time events,
and children events. All SF/MFBool eventIns and eventOuts are named
_isFoo_ (e.g. _isActive_). All SF/MFTime eventIns and eventOuts are
named _fooTime_ (e.g. _enterTime_). The eventIns on groups for adding
and removing children are named: _addChildren_ and _removeChildren_.
These exceptions were made to improve readability.

Routes are not nodes; ROUTE is merely a syntactic construct for
establishing event paths between nodes. ROUTE statements may appear at
either the top-level of a .wrl file or prototype implementation, or may
appear inside a node wherever fields may appear.

The types of the eventIn and the eventOut must match exactly. For
example, it is illegal to route from an SFFloat to an SFInt32 or from an
SFFloat to an MFFloat.

Routes may be established only from eventOuts to eventIns. Since
exposedField's implicitly define a field, an eventIn, and an eventOut,
it is legal to use the exposedField's defined name when routing to and
from it, (rather than specifying the _set__ prefix and __changed_
suffix). For example, the following TouchSensor's _enabled_ exposedField
is routed to the DirectionalLight's _on_ exposed field. Note that each
of the four routing examples below are legal syntax:

[source]
....
    DEF CLICKER TouchSensor { enabled TRUE }
    DEF LIGHT DirectionalLight { on  FALSE }
    ROUTE CLICKER.enabled TO LIGHT.on
or
    ROUTE CLICKER.enabled_changed TO LIGHT.on
or
    ROUTE CLICKER.enabled TO LIGHT.set_on
or
    ROUTE CLICKER.enabled_changed TO LIGHT.set_on
....

Redundant routing is ignored. If a file repeats a routing path, the
second (and all subsequent identical routes) are ignored. Likewise for
dynamically created routes via a scripting language supported by the
browser.

[[Sensors]]
===== 4.4.2 Sensors

Sensor nodes generate events. Geometric sensor nodes (ProximitySensor,
VisibilitySensor, TouchSensor, CylinderSensor, PlaneSensor, SphereSensor
and the Collision group) generate events based on user actions, such as
a mouse click or navigating close to a particular object. TimeSensor
nodes generate events as time passes. 
See "<<SensorNodes, Sensor Nodes>>" for more details on the specifics of
sensor nodes.

Each type of sensor defines when an event is generated. The state of the
scene graph after several sensors have generated events must be as if
each event is processed separately, in order. If sensors generate events
at the same time, the state of the scene graph will be undefined if the
results depends on the ordering of the events (world creators must be
careful to avoid such situations).

It is possible to create dependencies between various types of sensors.
For example, a TouchSensor may result in a change to a
VisibilitySensor's transformation, which may cause it's visibility
status to change. World authors must be careful to avoid creating
indeterministic or paradoxical situations (such as a TouchSensor that is
active if a VisibilitySensor is visible, and a VisibilitySensor that is
NOT visible if a TouchSensor is active).

[[ExecutionModel]]
===== 4.4.3 Execution Model

Once a Sensor or Script has generated an *initial event*, the event is
propagated along any ROUTES to other nodes. These other nodes may
respond by generating additional events, and so on. This process is
called an *event cascade*. All events generated during a given event
cascade are given the same timestamp as the initial event (they are all
considered to happen instantaneously).

Some sensors generate multiple events simultaneously; in these cases,
each event generated initiates a different event cascade.

[[Loops]]
===== 4.4.4 Loops

Event cascades may contain *loops*, where an event 'E' is routed to a
node that generated an event that eventually resulted in 'E' being
generated. Loops are broken as follows: implementations must not
generate two events from the same eventOut that have identical
timestamps. Note that this rule also breaks loops created by setting up
cyclic dependencies between different Sensor nodes.

[[Fan-in]]
===== 4.4.5 Fan-in and Fan-out

Fan-in occurs when two or more routes write to the same eventIn. If two
events with different values but the same timestamp are received at an
eventIn, then the results are undefined. World creators must be careful
to avoid such situations.

Fan-out occurs when one eventOut routes to two or more eventIns. This
case is perfectly legal and results in multiple events sent with the
same values and the same timestamp.


[[Time]]
==== 4.5 Time

[[TimeIntroduction]]
===== 4.5.1 Introduction

The browser controls the passage of time in a world by causing
TimeSensors to generate events as time passes. Specialized browsers or
authoring applications may cause time to pass more quickly or slowly
than in the real world, but typically the times generated by TimeSensors
will roughly correspond to "real" time. A world's creator must make no
assumptions about how often a TimeSensor will generate events but can
safely assume that each time event generated will be greater than any
previous time event.

Time (0.0) starts at 00:00:00 GMT January 1, 1970.

Events that are "in the past" cannot be generated; processing an event
with timestamp 't' may only result in generating events with timestamps
greater than or equal to `t'.

[[DiscreteAndContinuousChanges]]
===== 4.5.2 Discrete and Continuous Changes

VRML does not distinguish between discrete events (like those generated
by a TouchSensor) and events that are the result of sampling a
conceptually continuous set of changes (like the fraction events
generated by a TimeSensor). An ideal VRML implementation would generate
an infinite number of samples for continuous changes, each of which
would be processed infinitely quickly.

Before processing a discrete event, all continuous changes that are
occurring at the discrete event's timestamp should behave as if they
generate events at that same timestamp.

Beyond the requirements that continuous changes be up-to-date during the
processing of discrete changes, implementations are free to otherwise
sample continuous changes as often or as infrequently as they choose.
Typically, a TimeSensor affecting a visible (or otherwise perceptible)
portion of the world will generate events once per "frame," where a
"frame" is a single rendering of the world or one time-step in a
simulation.


[[Prototypes]]
==== 4.6 Prototypes

[[IntroductionToPrototypes]]
===== 4.6.1 Introduction to Prototypes

Prototyping is a mechanism that allows the set of node types to be
extended from within a VRML file. It allows the encapsulation and
parameterization of geometry, attributes, behaviors, or some combination
thereof.

A prototype definition consists of the following:

* the **PROTO** keyword,
* the _name_ of the new node type,
* the _prototype declaration_ which contains:
** a list of public _eventIns_ and _eventOuts_ that can send and receive events
** a list of public _exposedFields_ and _fields_, with default values,
* the _prototype definition_ which contains a list of one or more nodes,
  and zero or more routes and prototypes. The nodes in this list may also
  contain the *IS* syntax associates field and event names contained
  within the prototype definition with the events and fields names in the
  prototype declaration.

Square brackets enclose the list of events and fields, and braces
enclose the definition itself:

[source]
....
PROTO prototypename [ eventIn      eventtypename name
                      eventOut     eventtypename name
                      exposedField fieldtypename name defaultValue
                      field        fieldtypename name defaultValue
                      ... ] {
  Zero or more routes and prototypes
  First node (defines the node type of this prototype)  
  Zero or more nodes (of any type), routes, and prototypes
}
....

The names of the fields, exposedFields, eventIns, and eventOuts must be
unique for a single prototype (or built-in node). Therefore, the
following prototype is illegal:

[source]
....
PROTO badNames [ field        SFBool   foo
                 eventOut     SFColor  foo
                 eventIn      SFVec3f  foo
                 exposedField SFString foo ] {...}
....

because the name foo is overloaded. Prototype and built-in node field
and event name spaces do not overlap. Therefore, it is legal to use the
same names in different prototypes, as follows:

[source]
....
PROTO foo  [ field    SFBool   foo
             eventOut SFColor  foo2
             eventIn  SFVec3f  foo3 ] {...}

PROTO bar  [ field    SFBool   foo
             eventOut SFColor  foo2
             eventIn  SFVec3f  foo3 ] {...}
....

A prototype statement does *not* define an actual instance of node in
the scene. Rather, it creates a new node type (named _prototypename_)
that can be created later in the same file as if it were a built-in
node. It is thus necessary to define a node of the type of the prototype
to actually create an object. For example, the following file is an
empty scene with a _fooSphere_ prototype that serves no purpose:


[source]
....
#VRML V2.0 utf8
PROTO fooSphere [ field SFFloat fooRadius 3.0 ] {
    Sphere {
        radius 3             # default radius value for fooSphere
        radius IS fooRadius  # associates radius with fooRadius
    }
}
....

In the following example, a _fooSphere_ is created and thus produces a
visible result:


[source]
....
#VRML V2.0 utf8
PROTO fooSphere [ field SFFloat fooRadius 3.0 ] {
    Sphere {
        radius 3             # default radius value for fooSphere
        radius IS fooRadius  # associates radius with fooRadius
    }
}
fooSphere { fooRadius 42.0 }
....

The first node found in the prototype definition is used
to define the node type of this prototype. This first node type
determines how instantiations of the prototype can be used in a VRML
file. An instantiation is created by filling in the parameters of the
prototype declaration and inserting the first node (and its scene graph)
wherever the prototype instantiation occurs. For example, if the first
node in the prototype definition is a Material node, then instantiations
of the prototype can be used wherever a Material can be used. Any other
nodes and accompanying scene graphs are not rendered, but may be
referenced via routes or scripts (and thus cannot be ignored). The
following example defines a _RampMaterial_ prototype which animates a
Material's _diffuseColor_ continuously and that must be used wherever a
Material can be used in the file (i.e. within an Appearance node):


[source]
....
#VRML V2.0 utf8
PROTO RampMaterial [ field MFColor colors 0 0 0 field SFTime cycle 1 ] {
    DEF M Material {}
    DEF C ColorInterpolator { keyValue IS colors  key ... }
    DEF T TimeSensor { enabled TRUE loop TRUE cycleInterval IS cycle }
    ROUTE T.fraction_changed TO C.set_fraction
    ROUTE C.value_changed TO M.diffuseColor
}

Transform {
    children Shape {
        geometry Sphere {}
        appearance Appearance {
            material RampMaterial {
                colors [ 1 0 0, 0 0 1, 1 0 0 ] # red to green to red
                cycle 3.0                      # 3 second cycle
            }
        } 
    }
}
....

The next example defines a _SphereCone_ (fused Sphere and Cone) and
illustrates how the first node in the prototype definition may contain a
complex scene graph:


[source]
....
#VRML V2.0 utf8
PROTO SphereCone [ field SFFloat radius    2.0
                   field SFFloat height    5.0
                   field SFNode  sphereApp NULL
                   field SFNode  coneApp   NULL   ] {
    Transform {
        children [
            Shape {
                appearance IS sphereApp
                geometry Sphere { radius IS radius }
            }  
            Shape {
                appearance IS coneApp
                geometry Cone { height IS height }
            }   
        ]
    }
}

Transform {
    translation 15 0 0
    children SphereCone {
        radius 5.0
        height 20.0
        sphereApp Appearance { material Material { ... } }
        coneApp Appearance { texture ImageTexture { ... } }
    }
}
Transform {
    translation -10 0 0
    children SphereCone {          # default proto's radius and height
        sphereApp Appearance { texture ImageTexture { ... } }
        coneApp Appearance {  material Material { ... } }
    }
}
....

PROTO and EXTERNPROTO statements may appear anywhere ROUTE statements
may appear-- either at the top-level of a file or a prototype
definition, or wherever fields may appear.

[[ISStatement]]
===== 4.6.2 IS Statement

The _eventIn_ and _eventOut_ prototype declarations receive and send
events to and from the prototype's definition. Each eventIn in the
prototype declaration is associated with an eventIn or exposedField
defined in the prototype's node definition via the _IS_ syntax. The
eventIn declarations define the events that the prototype can receive.
Each eventOut in the prototype declaration is associated with an
eventOut or exposedField defined in the prototype's node definition via
the IS syntax. The eventOut declarations define the events that the
prototype can send. For example, the following statement exposes a
Transform node's _set_translation_ event by giving it a new name
(_set_position_) in the prototype interface:

[source]
....
PROTO FooTransform [ eventIn SFVec3f set_position ] {
    Transform { set_translation IS set_position }
}
....

Fields, (exposedField and field), specify the initial state of nodes.
Defining fields in a prototype's declaration allows the initial state of
associated fields in the prototype definition to be specified when an
instance of the prototype is created. The fields of the prototype are
associated with fields in the node definition using the *IS* keyword.
Field default values must be specified in the prototype declaration. For
example:

[source]
....
PROTO BarTransform [ exposedField SFVec3f position 42 42 42 ] {
    Transform {
        translation IS position
        translation  100 100 100
    }
}
....

defines a prototype, BarTransform, that specifies the initial values
(42, 42, 42) of the _position_ exposed field . The _position_ field is associated with the _translation_ field of the Tranform node in the
prototype definition using the IS syntax. Note that the field values in
the prototype definition for _translation_ (100, 100, 100) are legal, but overridden by the prototype declaration defaults.

Note that in some cases, it is necessary to specify the field defaults
inside the prototype definition. For example, the following prototype
associates the prototype definition's Material node _diffuseColor_
(exposedField) to the prototype declaration's eventIn _myColor_ and also
defines the default _diffuseColor_ values:

[source]
....
PROTO foo [ eventIn myColor ] {
    Material {
        diffuseColor  1 0 0
        diffuseColor  IS myColor   # or set_diffuseColor IS myColor
    }
}
....

IS statements may appear inside the prototype definition wherever fields
may appear. IS statements must refer to fields or events defined in the
prototype declaration. Inversely, it is an error for an IS statement to
refer to a non-existent declaration. It is an error if the type of the
field or event being associated does not match the type declared in the
prototype's interface declaration. For example, it is illegal to
associate an SFColor with an SFVec3f, and it is also illegal to
associate an SFColor with an MFColor, and vice versa. The following
table defines the rules for mapping between the prototype declarations
and the primary scene graph's nodes (yes denotes a legal mapping, no
denotes an error):

.*Prototype declaration*
[%autowidth,cols="20%,16%,16%,16%,16%,16%",]
|===
2.1+a|   |exposedField    |field    |eventIn    |eventOut

|*N*    |exposedField |*yes* |*yes* |*yes* |*yes*
|*o*    |field        |no    |*yes* |no    |no
|*d*    |eventIn      |no    |no    |*yes* |no
|*e*    |eventOut     |no    |no    |no    |*yes*
|===

Specifying the field and event types both in the prototype declaration
and in the node definition is intended to prevent user errors and to
provide consistency with "<<EXTERNPROTO, External Prototypes>>".

[[PrototypeScopingRules]]
===== 4.6.3 Prototype Scoping Rules

A prototype is instantiated as if _prototypename_ were a built-in node.
The prototype name must be unique within the scope of the file, and
cannot rename a built-in node or prototype.

Prototype instances may be named using DEF and may be multiply instanced
using USE as any built-in node. A prototype instance can be used in the
scene graph wherever the first node of the primary scene graph can be
used. For example, a prototype defined as:

[source]
....
    PROTO MyObject [ ... ] {
      Box { ... }
      ROUTE ...
      Script { ... }
      ...
    }
....

may be instantiated wherever a Box may be used (e.g. Shape node's
_geometry_ field), since the first node of the prototype definition is a
Box.

A prototype's scene graph defines a DEF/USE name scope separate from the
rest of the scene; nodes DEF'd inside the prototype may not be USE'd
outside of the prototype's scope, and nodes DEF'ed outside the prototype
scope may not be USE'ed inside the prototype scope.

Prototype definitions appearing inside a prototype implementation (i.e.
nested) are local to the enclosing prototype. For example, given the
following:

[source]
....
    PROTO one [...] {
        PROTO two [...] { ... }
        ...
        two { } # Instantiation inside "one":  OK
    }
    two { } # ERROR: "two" may only be instantiated inside "one".
....

The second instantiation of "two" is illegal. IS statements inside a
nested prototype's implementation may refer to the prototype
declarations of the innermost prototype. Therefore, IS statements in
"two" cannot refer to declarations in "one".

A prototype may be instantiated in a file anywhere after the completion
of the prototype definition. A prototype may not be instantiated inside
its own implementation (i.e. recursive prototypes are illegal). The
following example produces an error:

[source]
....
    PROTO Foo [] {
        Foo {}
    }
....

[[EXTERNPROTO]]
===== 4.6.4 Defining Prototypes in External Files

The syntax for defining prototypes in external files is as follows:

[source]
....
EXTERNPROTO extern prototypename [ eventIn eventtypename name
                                   eventOut eventtypename name
                                   field fieldtypename name
                                   exposedField fieldtypename name ]                         ... ]
  "URL/URN" or [ "URL/URN", "URL/URN", ... ]
....

The external prototype is then given the name _externprototypename_ in
this file's scope. It is an error if the eventIn/eventOut declaration in
the EXTERNPROTO is not a subset of the eventIn/eventOut
declarations specified in the PROTO referred to by the URL. If multiple
URLs or URNs are specified, the browser searches in the order of
preference (see "<<URLsAndURNs, URLs and URNs>>").

Unlike a prototype, an external prototype does not contain an inline
implementation of the node type. Instead, the prototype implementation
is fetched from a URL or URN. The other difference between a prototype
and an external prototype is that external prototypes do not contain
default values for fields. The external prototype references a file that
contains the prototype implementation, and this file contains the field
default values.

The URL/URNs refer to legal VRML files in which the first
prototype found in the file is used to define the external prototype's
definition. Note that the _prototypename_ does not need to
match the _externprotoname_. The following example illustrates how an
external prototype's declaration may be a subset of the prototype's
declaration (_diff_ vs. _diffuse_ and _shiny_) and how the external
prototype's typename may differ from the prototype's typename (e.g.
_FooBar_ != _SimpleMaterial_):

[source]
....
foo.wrl:
-------
#VRML V2.0 utf8
EXTERNPROTO FooBar [ eventIn SFColor diff ] "http://foo.com/coolNode.wrl
...


http://foo.com/coolNode.wrl:
---------------------------
#VRML V2.0 utf8
PROTO SimpleMaterial [ exposedField SFColor diffuse 1 0 0
                       eventIn      SFFloat shiny   0.5   ]
{
    Material { ... }
}
....

To allow the creation of libraries of small, reusable PROTO definitions,
browsers shall recognize EXTERNPROTO URLs that end with "*#*_name_" to
mean the prototype definition of "name" in the given file. For example,
a library of standard materials might be stored in a file called
"materials.wrl" that looks like:


[source]
....
#VRML V2.0 utf8
PROTO Gold   [] { Material { ... } }
PROTO Silver [] { Material { ... } }
...etc.
....

A material from this library could be used as follows:


[source]
....
#VRML V2.0 utf8
EXTERNPROTO Gold [] "http://.../materials.wrl#Gold"
...
    Shape {
        appearance Appearance { material Gold {} }
        geometry   ...
    }
....

The advantage is that only one http fetch needs to be done if several
things are used from the library; the disadvantage is that the entire
library will be transmitted across the network even if only one
prototype is used in the file.


[[Scripting]]
==== 4.7 Scripting

[[ScriptingIntroduction]]
===== 4.7.1 Introduction

Decision logic and state management is often needed to decide what
effect an event should have on the scene -- "if the vault is currently
closed AND the correct combination is entered, then open
the vault." These kinds of decisions are expressed as Script nodes (see
"<<Script, Nodes Reference - Script>>") that receive
events from other nodes, process them, and send events to other nodes. A
Script node can also keep track of information between execution, (i.e.
managing internal state over time). This section describes the general
mechanisms and semantics that all scripting languages must support. See
the specific scripting language appendix for the syntax and details of
any language (see "<<java_html, Appendix C. Java Reference>>" and 
"<<javascript_html, Appendix D. JavaScript Reference>>").

Event processing is done by a program or script contained in (or
referenced by) the Script node's _url_ field. This program or script can
be written in any programming language that the browser supports.
Browsers are not required to implement any specific
scripting languages in VRML 2.0.

A Script node is activated when it receives an event. At that point the
browser executes the program in the Script node's _url_ field (passing
the program to an external interpreter if necessary). The program can
perform a wide variety of actions: sending out events (and thereby
changing the scene), performing calculations, communicating with servers
elsewhere on the Internet, and so on. 
See "<<ExecutionModel, Execution Model>>" for a detailed description 
of the ordering of event processing.

[[ScriptExecution]]
===== 4.7.2 Script Execution

Scripts nodes allow the world author to insert logic into the middle of
an event cascade. Scripts also allow the world author to generate an
event cascade when a Script node is created or, in some scripting
languages, at arbitrary times.

Script nodes receive events in timestamp order. Any events generated as
a result of processing an event are given timestamps corresponding to
the event that generated them. Conceptually, it takes no time for a
Script node to receive and process an event, even though in practice it
does take some amount of time to execute a Script.

[[initialize]]
4.7.3 Initialize and Shutdown

The scripting language binding may define an _initialize_ method (or
constructor). This method is called before any events are generated.
Events generated by the initialize method must have timestamps less than
any other events that are generated by the Script node.

Likewise, the scripting language binding may define a _shutdown_ method
(or destructor). This method is called when the corresponding Script
node is deleted or the world containing the Script node is unloaded or
replaced by another world. This can be used as a clean up operation,
such as informing external mechanisms to remove temporary files.

[[eventsProcessed]]
===== 4.7.4 _EventsProcessed_

The scripting language binding may also define an _eventsProcessed_
routine that is called after one or more events are received. It allows
Scripts that do not rely on the order of events received to generate
fewer events than an equivalent Script that generates events whenever
events are received. If it is used in some other way, eventsProcessed
can be non-deterministic, since different implementations may call
eventsProcessed at different times.

For a single event cascade, a given Script node's eventsProcessed
routine must be called at most once. Events generated from an
eventsProcessed routine are given the timestamp of the last event
processed.

[[ScriptsWithDirectOutputs]]
===== 4.7.5 Scripts with Direct Outputs

Scripts that have access to other nodes (via SFNode or MFNode fields or
eventIns) and that have their "directOutputs" field set to TRUE may
directly post eventIns to those nodes. They may also read the last value
sent from any of the node's eventOuts.

When setting a value in another node, implementations are free to either
immediately set the value or to defer setting the value until the Script
is finished. When getting a value from another node, the value returned
must be up-to-date; that is, it must be the value immediately before the
time of the current timestamp (the current timestamp is the timestamp of
the event that caused the Script node to execute).

The order of execution of Script nodes that do not have ROUTES between
them is undefined. If multiple directOutputs Scripts all read and/or
write the same node, the results may be undefined. Just as with ROUTE
fan-in, these cases are inherently non-deterministic and it is up to the
world creator to ensure that these cases do not happen.

[[AsynchronousScripts]]
===== 4.7.6 Asynchronous Scripts

Some languages supported by VRML browser may allows Script nodes to
spontaneously generate events, allowing users to create Script nodes
that function like new Sensor nodes. In these cases, the Script is
generating the initial event that cause the event cascade, and the
scripting language and/or the browser will determine an appropriate
timestamp for that initial event. Such events are then sorted into the
event stream and processed like any other event, following all of the
same rules for looping, etc.

[[ScriptLanguages]]
===== 4.7.7 Script Languages

The Script node's _url_ field may specify a URL which refers to a file
(e.g. http:) or directly inlines (e.g. javabc:) scripting language code.
The mime-type of the returned data defines the language type.
Additionally instructions can be included inline using either the
<<DataProtocol, data: protocol>> (which allows a mime-type specification) 
or a "<<CustomProtocol, Scripting Language Protocol>>" defined for the
specific language (in which the language type is inferred).

[[EventInHandling]]
===== 4.7.8 EventIn Handling

Events received by the Script node are passed to the appropriate
scripting language function in the script. The function's name depends
on the language type used--in some cases it is identical to name of the
eventIn, while in others it is a general callback function for all
eventIns (see the language appendices for details). The function is
passed two arguments, the event value and the event timestamp.

For example, the following Script node has one eventIn field named
_start_ and three different URL values specified in the _url_ field:
JavaScript, Java, and inline JavaScript:

[source]
....
    Script { 
        eventIn SFBool start
        url [ "http://foo.com/fooBar.class",
              "http://foo.com/fooBar.js",
              "javascript:function start(value, timestamp) { ... }"
        ]
    }
....

In the above example when a _start_ eventIn is received by the Script
node, one of the scripts found in the url field is executed. The Java
code is the first choice, the JavaScript code is the second choice, and
the inline JavaScript code the third choice - 
see "<<URLsAndURNs, URLs and URNs>>" for a description of order of
preference for multiple valued URL fields. In the above example, .

[[AccessingFieldsAndEvents]]
===== 4.7.9 Accessing Fields and Events

The fields, eventIns and eventOuts of a Script node are accessible from
scripting language functions. The Script's eventIns can be routed to and
its eventOuts can be routed from. Another Script node with a pointer to
this node can access its eventIns and eventOuts just like any other
node.

====== Accessing Fields and EventOuts of the Script

Fields defined in the Script node are available to the script through a
language specific mechanism (e.g. a member variable is automatically
defined for each field and event of the Script node). The field values
can be read or written and are persistent across function calls.
EventOuts defined in the script node can also be read - the value is the
last value sent.

====== Accessing Fields and EventOuts of Other Nodes

The script can access any exposedField, eventIn or eventOut of any node
to which it has a pointer. The syntax of this mechanism is language
dependent. The following example illustrates how a Script node accesses
and modifies an exposed field of another node (i.e. sends a
_set_translation_ eventIn to the Transform node) using a fictitious
scripting language:

[source]
....
    DEF SomeNode Transform { }
    Script {
        field   SFNode  tnode USE SomeNode
        eventIn SFVec3f pos
        directOutput TRUE
        url "... 
            function pos(value, timestamp) { 
                tnode.set_translation = value; 
            }"
    }
....

====== Sending EventOuts

Each scripting language provides a mechanism for allowing scripts to
send a value through an eventOut defined by the Script node. For
example, one scripting language may define an explicit function for
sending each eventOut, while another language may use assignment
statements to automatically defined eventOut variables to implicitly
send the eventOut. The results of sending multiple values through an
eventOut during a single script execution are undefined - it may result
in multiple eventOuts with the same timestamp or a single event out with
the value of the last assigned value.

[[BrowserInterface]]
===== 4.7.10 Browser Script Interface

The browser interface provides a mechanism for scripts contained by
Script nodes to get and set browser state, such as the URL of the
current world. This section describes the *semantics* that
functions/methods that the browser interface supports. A C-like syntax
is used to define the type of parameters and returned values, but is
hypothetical. See the specific appendix for a language for the actual
syntax required. In this hypothetical syntax, types are given as VRML
field types. Mapping of these types into those of the underlying
language (as well as any type conversion needed) is described in the
appropriate language reference.

  SFString getName( );
  SFString getVersion( );

The *getName()* and *getVersion()* methods get the "name" and "version"
of the browser currently in use. These values are defined by the browser
writer, and identify the browser in some (unspecified) way. They are not
guaranteed to be unique or to adhere to any particular format, and are
for information only. If the information is unavailable these methods
return empty strings.

    SFFloat getCurrentSpeed( );

The *getCurrentSpeed()* method returns the speed at which the viewpoint
is currently moving, in meters per second. If speed of motion is not
meaningful in the current navigation type, or if the speed cannot be
determined for some other reason, 0.0 is returned.

    SFFloat getCurrentFrameRate( );

The *getCurrentFrameRate()* method returns the current frame rate in
frames per second. The way in which this is measured and whether or not
it is supported at all is browser dependent. If frame rate is not
supported, or can't be determined, 0.0 is returned.

    SFString getWorldURL( );

The *getWorldURL()* method returns the URL for the root of the currently
loaded world.

    void replaceWorld( MFNode nodes );

The *replaceWorld()* method replaces the current world with the world
represented by the passed nodes. This will usually not return, since the
world containing the running script is being replaced.

    void loadURL( MFString url, MFString parameter );

The *loadURL* method loads _url_ with the passed parameters. _Parameter_
is as described in the Anchor node. This method returns immediately but
if the URL is loaded into this browser window (e.g. - there is no TARGET
parameter to redirect it to another frame) the current world will be
terminated and replaced with the data from the new URL at some time in
the future.

    void setDescription( SFString description );

The *setDescription* method sets the passed string as the current
description. This message is displayed in a browser dependent manner. To
clear the current description, send an empty string.

    MFNode createVrmlFromString( SFString vrmlSyntax );

The *createVrmlFromString()* method takes a string consisting of a VRML
scene description, parses the nodes contained therein and returns the
root nodes of the corresponding VRML scene.

    void createVrmlFromURL( MFString url, SFNode node, SFString event );

The *createVrmlFromURL()* instructs the browser to load a VRML scene
description from the given URL or URLs. After the scene is loaded,
_event_ is sent to the passed _node_ returning the root nodes of the
corresponding VRML scene. The event parameter contains a string naming
an MFNode eventIn on the passed node.

    void addRoute( SFNode fromNode, SFString fromEventOut, 
                   SFNode toNode, toEventIn );

    void deleteRoute( SFNode fromNode, SFString fromEventOut,
                      SFNode toNode, SFString toEventIn );

These methods respectively add and delete a route between the given
event names for the given nodes.


[[BrowserExtensions]]
==== 4.8 Browser Extensions

[[CreatingExtensions]]
===== 4.8.1 Creating Extensions

Browsers that wish to add functionality beyond the capabilities in the
specification should do so by creating <<Prototypes, prototypes>> or
<<EXTERNPROTO, external prototypes>>. If the new node cannot be
expressed using the prototyping mechanism (i.e. it cannot be expressed
as VRML scene graph), then it should be defined as an external prototype
with a unique URN specification. Authors who use the extended
functionality may provide multiple, alternative URLs or URNs to
represent the content to ensure that it is viewable on all browsers.

For example, suppose a browser wants to create a native Torus geometry
node implementation:

[source]
....
EXTERNPROTO Torus [ field SFFloat bigR, field SFFloat smallR ]
    ["urn:inet:library:Torus", "http://.../proto_torus.wrl" ]
....

This browser will recognize the URN and use its own private
implementation of the Torus node. Other browsers may not recognize the
URN, and skip to the next entry in the URL list and search for the
specified prototype file. If no URLs or URNs are found, the Torus is
assumed to be a an empty node.

Note that the prototype name, "Torus", in the above example has no
meaning whatsoever. The URN/URL uniquely and precisely defines the
name/location of the node implementation. The prototype name is strictly
a convention chosen by the author and shall not be interpreted in any
semantic manner. The following example uses both "Ring" and "Donut" to
name the torus node, but that the URN/URL, "urn:library:Torus,
http://.../proto_torus.wrl", specify the actual definition of the Torus
node:


[source]
....
#VRML V2.0 utf8

EXTERNPROTO Ring [field SFFloat bigR, field SFFloat smallR ]
    ["urn:library:Torus", "http://.../proto_torus.wrl" ]

EXTERNPROTO Donut [field SFFloat bigR, field SFFloat smallR ]
    ["urn:library:Torus", "http://.../proto_torus.wrl" ]

Transform { ... children Shape { geometry Ring } }
Transform { ... children Shape { geometry Donut } }
....

[[ReadingExtensions]]
===== 4.8.2 Reading Extensions

VRML-compliant browsers must recognize and implement the PROTO,
EXTERNPROTO, and URN specifications. Note that the prototype names (e.g.
Torus) has no semantic meaning whatsoever. Rather, the URL and the URN
uniquely determine the location and semantics of the node. Browsers
shall not use the PROTO or EXTERNPROTO name to imply anything about the
implementation of the node.


[[NodeConcepts]]
==== 4.9 Node Concepts

[[BindableLeafNodes]]
===== 4.9.1 Bindable Children Nodes

The <<Background, Background>>, <<Fog, Fog>>,
<<NavigationInfo, NavigationInfo>>, and
<<Viewpoint, Viewpoint>> nodes have the unique behavior
that only one of each type can be active (i.e. affecting the user's
experience) at any point in time. 
See "<<GroupingNodes, Grouping and Children Nodes>>" for a description of
legal children nodes. The browser shall maintain a stack for each type
of binding node. Each of these nodes includes a _set_bind_ eventIn and
an _isBound_ eventOut. The _set_bind_ eventIn is used to moves a given
node to and from its respective top of stack. A TRUE value sent to
_set_bind_ eventIn, moves the node to the top of the stack, and a FALSE
value removes it from the stack. The _isBound_ event is output when a
given node is moved to the top of the stack, removed from the stack, or
is pushed down in the stack by another node being placed on top. That
is, the _isBound_ event is sent when a given node ceases to be the
active node. The node at the top of stack, (the most recently bound
node), is the active node for its type and is used by the browser to set
world state. If the stack is empty (i.e. either the file has no binding
nodes for a given type or the stack has been popped until empty), then
the default field values for that node type are used to set world state.
The results are undefined if a multiply instanced (DEF/USE) bindable
node is bound.

====== Bind Stack Behavior

. During read:
  * the first encountered _<binding node>_ is bound by pushing it to the
  top of the <__binding node__> binding stack:
  ** nodes contained within <<Inline, Inlines>> are not
  candidates for the first encountered binding node,
  ** the first node within a prototype is valid as the a first encountered
  binding node,
  * the first encountered node sends an _isBound_ TRUE __ event.
. When a _set_bind_ TRUE eventIn is received by a _<binding node>:_
  * if it is not on the top of the stack:
  ** the existing top of stack node sends an _isBound_ eventOut FALSE,
  ** the new node is moved to the top of the stack (i.e.
  there is only one entry in the stack for any node at any time) and
  becomes the currently bound _<binding node>,_
  ** the new top of stack node sends an _isBound_ TRUE __ eventOut;
  * else if the node is already at the top of the stack, then this event
  has no affect.
. When a _set_bind_ FALSE eventIn is received by a _<binding node>:_
  * it is removed from the stack,
  * if it is on the top of the stack:
  ** it sends an _isBound_ eventOut FALSE,
  ** the next node in the stack becomes the currently bound _<binding
  node>_ (i.e. pop) __ and issues an _isBound_ TRUE __ eventOut.
. If a _set_bind_ FALSE eventIn is received by a node not in the stack,
  the event is ignored and _isBound_ events are not sent.
. When a node replaces another node at the top of the stack, the
  _isBound_ TRUE and FALSE eventOuts from the two nodes are sent
  simultaneously (i.e. identical timestamps).
. If a bound node is deleted then it behaves as if it received a
  _set_bind_ FALSE event (see #3).

[[GeometryNodes]]
===== 4.9.2 Geometry

Geometry nodes must be contained by a <<Shape, Shape>>
node in order to be visible to the user. The Shape node contains exactly
one geometry node in its _geometry_ field. This node must be one of the
following node types:

| <<Box, Box>>
| <<Cone, Cone>>
| <<Cylinder, Cylinder>>
| <<ElevationGrid, ElevationGrid>>
| <<Extrusion, Extrusion>>
| <<IndexedFaceSet, IndexedFaceSet>>
| <<IndexedLineSet, IndexedLineSet>>
| <<PointSet, PointSet>>
| <<Sphere, Sphere>>
| <<Text, Text>>

Several geometry nodes also contain <<Coordinate3, Coordinate>>,
<<Color, Color>>, <<Normal, Normal>>, and
<<TextureCoordinate, TextureCoordinate>> as geometric property nodes.
These property nodes are separated out as individual nodes so that
instancing and sharing is possible between different geometry nodes. All
geometry nodes are specified in a local coordinate system and are
affected by parent transformations.

_Application of material, texture, and colors:_::
  See "_<<Lighting, Lighting Model>>" for details on how material,
  texture, and color specifications interact._
_Shape Hints Fields:_::
  The ElevationGrid, Extrusion, and IndexedFaceSet nodes each have three
  SFBool fields that provide hints about the shape--whether it contains
  ordered vertices, whether the shape is solid, and whether it contains
  convex faces. These fields are _ccw_, _solid_, and _convex_.
  +
  The _ccw_ field indicates whether the vertices are ordered in a
  counter-clockwise direction when the shape is viewed from the outside
  (TRUE). If the order is clockwise, this field value is FALSE and the
  vertices are ordered in a clockwise direction when the shape is viewed
  from the outside. The _solid_ field indicates whether the shape
  encloses a volume (TRUE), and can be used as a hint to perform
  backface culling. If nothing is known about the shape, this field
  value is FALSE (and implies that backface culling cannot be performed
  and that the polygons are two-sided). If solid is TRUE, the ccw field
  has no affect. The _convex_ field indicates whether all faces in the
  shape are convex (TRUE). If nothing is known about the faces, this
  field value is FALSE.
  +
  These hints allow VRML implementations to optimize certain rendering
  features. Optimizations that may be performed include enabling
  backface culling and disabling two-sided lighting. For example, if an
  object is solid and has ordered vertices, an implementation may turn
  on backface culling and turn off two-sided lighting. If the object is
  not solid but has ordered vertices, it may turn off backface culling
  and turn on two-sided lighting.
_Crease Angle Field_:::
  The _creaseAngle_ field, used by the ElevationGrid, Extrusion, and
  IndexedFaceSet nodes, affects how default normals are generated. For
  example, when an IndexedFaceSet has to generate default normals, it
  uses the _creaseAngle_ field to determine which edges should be
  smoothly shaded and which ones should have a sharp crease. The crease
  angle is the positive angle between surface normals on adjacent
  polygons. For example, a crease angle of .5 radians means that an edge
  between two adjacent polygonal faces will be smooth shaded if the
  normals to the two faces form an angle that is less than .5 radians
  (about 30 degrees). Otherwise, it will be faceted. Crease angles must
  be greater than or equal to 0.0.

[[InterpolatorNodes]]
===== 4.9.3 Interpolators

Interpolators nodes are designed for linear keyframed animation. That
is, an interpolator node defines a piecewise linear function, _f(t)_, on
the interval (_-infinity, infinity)._ The piecewise linear function is
defined by _n_ values of _t,_ called _`+key,+`_ and the _n_
corresponding values of _f(t)_, called _`keyValue`_. The keys must be
monotonic non-decreasing and are not restricted to any interval. An
interpolator node evaluates _f(t)_ given any value of _t_ (via the
_set_fraction_`+ +`eventIn).

Let the _n_ keys _k0, k1, k2, ..., k(n-1)_ partition the domain
(_-infinity, infinity_) into the _n+1_ subintervals given by
(-_infinity_, _k0), [k0, k1), [k1, k2), ... , [k(n-1), infinity)_. Also,
let the n values _v0, v1, v2, ..., v(n-1)_ be the values of an unknown
function, _F(t),_ at the associated key values. That is, _vj = F(kj)._
The piecewise linear interpolating function, _f(t)_, is defined to be

[source]
....
     f(t) = v0,     if t < k0,
          = v(n-1), if t > k(n-1),
          = vi,     if t = ki for some value of i, where -1<i<n,
          = linterp(t, vj, v(j+1)), if kj < t < k(j+1),
....

where _linterp(t,x,y)_ is the linear interpolant, and _-1< j < n-1_. The
third conditional value of _f(t)_ allows the defining of multiple values
for a single key, i.e. limits from both the left and right at a
discontinuity in _f(t)_.The first specified value will be used as the
limit of _f(t)_ from the left, and the last specified value will be used
as the limit of _f(t)_ from the right. The value of _f(t)_ at a multiply
defined key is indeterminate, but should be one of the associated limit
values.

There are six different types of interpolator nodes, each based on the
type of value that is interpolated:

| <<ColorInterpolator, ColorInterpolator>>
| <<CoordinateInterpolator, CoordinateInterpolator>>
| <<NormalInterpolator, NormalInterpolator>>
| <<OrientationInterpolator, OrientationInterpolator>>
| <<PositionInterpolator, PositionInterpolator>>
| <<ScalarInterpolator, ScalarInterpolator>>


All interpolator nodes share a common set of fields and semantics:

[source]
....
      exposedField MFFloat      key           [...]
      exposedField MF<type>     keyValue      [...]
      eventIn      SFFloat      set_fraction
      eventOut     [S|M]F<type> value_changed
....

The type of the _keyValue_ field is dependent on the type of the
interpolator (e.g. the ColorInterpolator's _keyValue_ field is of type
MFColor). Each value in the _keyValue_ field corresponds in order to a
parameterized time in the _key_ field. Therefore, there exists exactly
the same number of values in the _keyValue_ field as key values in the
_key_ field.

The set_fraction eventIn receives a float event and causes the
interpolator function to evaluate. The results of the linear
interpolation are sent to _value_changed_ eventOut.

Four of the six interpolators output a single-valued field to
_value_changed_. The exceptions, CoordinateInterpolator and
NormalInterpolator, send multiple-value results to _value_changed_. In
this case, the _keyValue_ field is an __n__x__m__ array of values, where
_n_ is the number of keys and _m_ is the number of values per key. It is
an error if _m_ is not a positive integer value.

The following example illustrates a simple ScalarInterpolator which
contains a list of float values (11.0, 99.0, and 33.0), the keyframe
times (0.0, 5.0, and 10.0), and outputs a single float value for any
given time:

[source]
....
    ScalarInterpolator {
       key      [ 0.0,  5.0,  10.0]
       value    [11.0, 99.0, 33.0]
    }
....

For an input of 2.5 (via `set_fraction`), this `ScalarInterpolator`
would send an output value of:

[source]
....
    eventOut SFFloat value_changed 55.0
                         # = 11.0 + ((99.0-11.0)/(5.0-0.0)) * 2.5
....

Whereas the CoordinateInterpolator below defines an array of coordinates
for each keyframe value and sends an array of coordinates as output:

[source]
....
    CoordinateInterpolator {
       key   [ 0.0,  0.5,  1.0]
       value [ 0  0  0,    10 10 30,   # 2 keyValue(s) at key 0.0
                10 20 10,   40 50 50,  # 2 keyValue(s) at key 0.5
                33 55 66,   44 55 65 ] # 2 keyValue(s) at key 1.0

    }
....

In this case, there are two coordinates for every keyframe. The first
two coordinates (0, 0, 0) and (10, 10, 30) represent the value at
keyframe 0.0, the second two coordinates (10, 20, 10) and (40, 50, 50)
represent that value at keyframe 0.5, and so on. If a `set_fraction`
value of 0.25 (meaning 25% of the animation) was sent to this
CoordinateInterpolator, the resulting output value would be:

[source]
....
     eventOut MFVec3f value_changed [ 5 10 5,  25 30 40 ]
....

If an interpolator node's _value_ eventOut is read (e.g. _get_value_)
before it receives any inputs, then _keyValue[0]_ is returned.

The location of an interpolator node in the scene graph has no affect on
its operation. For example, if a parent of an interpolator node is a
Switch node with _whichChoice_ set to -1 (i.e. ignore its children), the
interpolator continues to operate as specified (receives and sends
events).

[[Lights]]
===== 4.9.4 Light Sources

In general, shape nodes are illuminated by the sum of all of the lights
in the world that affect them. This includes the contribution of both
the direct and ambient illumination from light sources. Ambient
illumination results from the scattering and reflection of light
originally emitted directly by light sources. The amount of ambient
light is associated with the individual lights in the scene. This is a
gross approximation to how ambient reflection actually occurs in nature.

There are three types of light source nodes:

| <<DirectionalLight, DirectionalLight>>
| <<PointLight, PointLight>>
| <<SpotLight, SpotLight>>

All light source node contain an _intensity_, a _color_, and an
_ambientIntensity_ field. The _intensity_ field specifies the brightness
of the direct emission from the light, and the _ambientIntensity_
specifies the intensity of the ambient emission from the light. Light
intensity may range from 0.0, no light emission, to 1.0, full intensity.
The color field specifies the spectral color properties of the light
emission, as an RGB value in the 0.0 to 1.0 range.

PointLight and SpotLight illuminate all objects in the world that fall
within their volume of lighting influence regardless of location within
the file. PointLight defines this volume of influence as a sphere
centered at the light (defined by a radius). SpotLight defines the
volume of influence a solid angle defined by a radius and a cutoff
angle. DirectionalLights illuminate only the objects descended from the
light's parent grouping node (including any descendant children of the
parent group node).

[[Lighting]]
===== 4.9.5 Lighting Model

====== Lighting `off'

A Shape node is unlit if any of the following are true:

* The shape's _appearance_ field is NULL (default)
* The _material_ field in the Appearance node is NULL (default)

If the shape is unlit, then the color (I~rgb~) and alpha (A,
1-transparency) of the shape at each point on the shape's geometry is
given by the following table:

[%autowidth,cols="34%,33%,33%",]
|===
|*Unlit Geometry* |Color per-vertex or per-face |Color NULL
|No texture |I~rgb~= I~Crgb + ~A = 1 
            |I~rgb~= (1, 1, 1)~ + ~A = 1
|Intensity (one-component) texture 
            |I~rgb~= I~T~ × I~Crgb + ~A = 1 
            |I~rgb~= (I~T~,I~T~,I~T~ ) + A = 1
|Intensity+Alpha (two-component) texture 
            |I~rgb~= I~T~ × I~Crgb + ~A = A~T~ 
            |I~rgb~= (I~T~,I~T~,I~T~ ) + A = A~T~
|RGB (three-component) texture 
            |I~rgb~= I~Trgb + ~A = 1 
            |I~rgb~= I~Trgb + ~A = 1
|RGBA (four-component) texture 
            |I~rgb~= I~Trgb + ~A = A~T~ 
            |I~rgb~= I~Trgb + ~A = A~T~
|===

where:

A~T~    = normalized (0-1) alpha value from 2 or 4 component texture image +
I~Crgb~ = interpolated per-vertex color, or per-face color, from Color node +
I~T~    = normalized (0-1) intensity from 1-2 component texture image +
I~Trgb~ = color from 3-4 component texture image

====== Lighting `on'

If the shape is lit (a Material and an Appearance node are
specified for the Shape), then the Color and Texture nodes determine the
diffuse color for the lighting equation, as specified in the following
table:

[%autowidth,cols="34%,33%,33%",]
|===
|*Lit Geometry* |Color per-vertex or per-face |Color NULL
|No texture     |O~drgb~ = I~Crgb + ~A = 1-T~M~ 
                |O~drgb~ = I~Mrgb + ~A = 1-T~M~
|Intensity texture (one-component) 
                |O~drgb~ = I~T~ × I~Crgb + ~A = 1-T~M~ 
                |O~drgb~ = I~T~ × I~Mrgb~ + A = 1-T~M~
|Intensity+Alpha texture (two-component) 
                |O~drgb~ = I~T~ × I~Crgb + ~A = A~T~ 
                |O~drgb~ = I~T~ × I~Mrgb~ + A = A~T~
|RGB texture (three-component) 
                |O~drgb~ = I~Trgb + ~A = 1-T~M~ 
                |O~drgb~ = I~Trgb + ~A = 1-T~M~
|RGBA texture (four-component) 
                |O~drgb~ = I~Trg + ~A = A~T~ 
                |O~drgb~ = I~Trgb + ~A = A~T~
|===

where:

I~Mrgb~ = material diffuseColor +
O~drgb~ = diffuse factor, used in lighting equations below +
T~M~    = material transparency

\... and all other terms are as above.

====== Lighting equations

An ideal VRML 2.0 implementation will evaluate the following lighting
equation at each point on a surface. RGB intensities at each point on a
geometry (I~rgb~) are given by:

I~rgb~= I~frgb~ × (1 - s~0~) + s~0~ × ( O~ergb~ + SUM( on~i~ ×
attenuation~i~ × spot~i~ × I~iprgb~ × ( ambient~i~ + diffuse~i~ +
specular~i~ )))

where:

[cols=">,<",frame=sides,opts="autowidth"]
|===
|·           |= modified vector dot product: 0 if dot product < 0, dot product otherwise.
|I~frgb~     |= currently bound fog's color
|I~iprgb~    |= light i color
|I~ia~       |= light i ambientIntensity
|*`L`*     |= (Point/SpotLight) normalized vector from point on geometry to light source i position
|*`L`*     |= (DirectionalLight) -direction of light source i
|*`N`*     |= normalized normal vector at this point on geometry
|O~a~        |= material ambientIntensity
|O~drgb~     |= diffuse color, from material node, Color node, and/or Texture node
|O~ergb~     |= material emissiveColor
|O~srgb~     |= material specularColor
|*`V`*     |= normalized vector from point on geometry to viewer's position
|attenuation~i~ |= max ( 1/(c~1~ + c~2~×d~L~ + c~3~×d~L~^²^ ), 1 )
|ambient~i~  |= I~ia~ × I~iprgb~ × O~drgb~ × O~a
|~ c~1~,c~2~,c~3~ |= light i attenuation
|d~V~        |= distance from point on geometry to viewer's position, in world space
|d~L~        |= distance from light to point on geometry, in light's coordinate system
|diffuse~i~  |= k~d~ × O~drgb~ × ( *`N`* · *`L`* )
|k~d~        |= k~s~ = light i intensity
|on~i~       |= 1 if light source i affects this point on the geometry
|on~i~       |= 0 if light source does not affect this geometry (if farther away
                than radius for Point or SpotLights, outside of enclosing
                Group/Transform for a DirectionalLight, or on field is FALSE).
|shininess   |= material shininess
|specular~i~ |= k~s~ × O~srgb~ × (*N* · ((*L* + *V*) / \|*L* + *V*\| ) ^shininess×128^
|===

[cols=">,<",frame=sides,opts="autowidth"]
|===
|spot~i~ = 1 |spotCutoff~i~ >= pi/2 or light i is PointLight or DirectionalLight
|spot~i~ = 0 |spotCutoff~i~ < pi/2 and *L* · *spotDir*~i~ < cos(spotCutoff~i~)
|spot~i~ = ( *L* · *spotDir*~i~ ) ^spotExponent*128^ 
             |spotCutoff < pi/2 and *L* · *spotDir*~i~ > cos(spotCutoff~i~)
|===

[cols=">,<",frame=sides,opts="autowidth"]
|===
|spotCutoff~i~ |= SpotLight i cutoff angle
|*spotDir*~i~  |= normalized SpotLight i direction
|spotExponent  |= SpotLight i exponent + SUM: sum over all light sources i
|===

[cols=">,<",frame=sides,opts="autowidth"]
|===
|s~0~ = 1 |no fog

|s~0~ = (fogVisibility-d~V~) / fogVisibility 
|fogType "LINEAR", d~V~ < fogVisibility

|s~0~ = 0 |fogType "LINEAR", d~V~ > fogVisibility

|s~0~ = exp(-d~V~ / (fogVisibility-d~V~) ) 
|fogType "EXPONENTIAL", d~V~ < fogVisibility

|s~0~ = 0 |fogType "EXPONENTIAL", d~V~ > fogVisibility
|===

====== References

The VRML lighting equations are based on the simple illumination
equations given in "Computer Graphics: Principles and Practice", Foley,
van Dam, Feiner and Hughes, section 16.1, "Illumination and Shading",
<<bibliography_html, FOLE>>, and in the OpenGL 1.1 specification 
(http://www.sgi.com/Technology/openGL/spec.html) section 2.13 (Lighting) 
and 3.9 (Fog), <<bibliography_html, OPEN>>.

[[SensorNodes]]
===== 4.9.6 Sensor Nodes

There are several different kinds of sensor nodes:
<<ProximitySensor, ProximitySensor>>,
<<TimeSensor, TimeSensor>>,
<<VisibilitySensor, VisibilitySensor>>, and a variety of
_pointing device sensors_ (<<Anchor, Anchor>>,
<<CylinderSensor, CylinderSensor>>,
<<PlaneSensor, PlaneSensor>>,
<<SphereSensor, SphereSensor>>,
<<TouchSensor, TouchSensor>>). Sensors are children nodes
in the hierarchy and therefore may be parented by grouping nodes, 
see "<<GroupingNodes, Grouping and Children Nodes>>".

The <<ProximitySensor, ProximitySensor>> detects when the
user navigates into a specified invisible region in the world. The
<<TimeSensor, TimeSensor>> is a clock that has no
geometry or location associated with it - it is used to start and stop
time-based nodes, such as interpolators. The
<<VisibilitySensor, VisibilitySensor>> detects when a
specific part of the world becomes visible to the user. Pointing device
sensors detect user pointing events, such as the user activating on a
piece of geometry (i.e. TouchSensor). Proximity, time, and visibility
sensors are additive. Each one is processed independently of whether
others exist or overlap.

[[PointerDeviceSensors]]
====== Pointing Device Sensors

The following nodes are considered to be pointing device sensors:

* <<Anchor, Anchor>>
* <<CylinderSensor, CylinderSensor>>
* <<PlaneSensor, PlaneSensor>>
* <<SphereSensor, SphereSensor>>
* <<TouchSensor, TouchSensor>>

Pointing device sensors are activated when the user points to geometry
that is influenced by a specific pointing device sensor. These sensors
have influence over all geometry that is descendant from the sensor's
parent group. [In the case of the Anchor node, the Anchor itself is
considered to be the parent group.] Typically, the pointing device
sensor is a sibling to the geometry that it influences. In other cases,
the sensor is a sibling to groups which contain geometry (that is
influenced by the pointing device sensor).

For a given user activation, the _lowest_, enabled pointing device
sensor in the hierarchy is activated - all other pointing device sensors
_above_ it are ignored. The hierarchy is defined by the geometry node
which is activated and the entire hierarchy upward. If there are
multiple pointing device sensors tied for lowest, then each of these is
activated simultaneously and independently, possibly resulting in
multiple sensors activated and outputting simultaneously. This feature
allows useful combinations of pointing device sensors (e.g. TouchSensor
and PlaneSensor). If a pointing device sensor is instanced (DEF/USE),
then the any geometry associated with any of its parents must be tested
for intersection and activated hit.

The <<Anchor, Anchor>> node is considered to be a
pointing device sensor when trying to determine which sensor (or Anchor)
to activate. For example, in the following file a click on _Shape3_ is
handled by _SensorD_, a click on _Shape2_ is handled by _SensorC_ and
the _AnchorA_, and a click on _Shape1_ is handled by _SensorA_ and
_SensorB_:

[source]
....
Group {
    children [
        DEF Shape1  Shape       { ... }
        DEF SensorA TouchSensor { ... }
        DEF SensorB PlaneSensor { ... }
        DEF AnchorA Anchor {
            url "..."
            children [
                DEF Shape2  Shape { ... }
                DEF SensorC TouchSensor { ... }
                Group {
                    children [
                        DEF Shape3  Shape { ... }
                        DEF SensorD TouchSensor { ... }
                    ]
                }

            ]

        }
    ]
}
....

[[DragSensors]]
====== Drag Sensors

Drag sensors are a subset of pointing device sensors. There are three
drag sensors (<<CylinderSensor, CylinderSensor>>,
<<PlaneSensor, PlaneSensor>>,
<<SphereSensor, SphereSensor>>) in which pointer motions
cause events to be generated according to the "virtual shape" of the
sensor. For instance the output of the SphereSensor is an SFRotation,
_rotation_changed_, which can be connected to a Transform node's
_set_rotation_ field to rotate an object. The effect is the user grabs
an object and spins it about the center point of the SphereSensor.

To simplify the application of these sensors, each node has an _offset_
and an _autoOffset_ exposed field. Whenever the sensor generates output,
(as a response to pointer motion), the output value (e.g. SphereSensor's
_rotation_changed_) is added to the _offset_. If _autoOffset_ is TRUE
(default), this offset is set to the last output value when the pointing
device button is released (_isActive_ FALSE). This allows subsequent
grabbing operations to generate output relative to the last release
point. A simple dragger can be constructed by sending the output of the
sensor to a Transform whose child is the object being grabbed. For
example:

[source]
....
    Group {
        children [
            DEF S SphereSensor { autoOffset TRUE }
            DEF T Transform {
                children Shape { geometry Box {} }
            }
        ]
        ROUTE S.rotation_changed TO T.set_rotation
    }
....

The box will spin when it is grabbed and moved via the pointer.

When the pointing device button is released, _offset_ is set to the last
output value and an _offset_changed_ event is sent out. This behavior
can be disabled by setting the _autoOffset_ field to FALSE.

[[TimeDependentNodes]]
===== 4.9.7 Time Dependent Nodes

<<AudioClip, AudioClip>>,
<<MovieTexture, MovieTexture>>, and
<<TimeSensor, TimeSensor>> are time dependent nodes that
should activate and deactivate themselves at specified times. Each of
these nodes contains the exposedFields: _startTime_, _stopTime_, and
_loop,_ and the eventOut: _isActive_. The exposedField values are used
to determine when the container node becomes active or inactive. Also,
under certain conditions, these nodes ignore events to some of their
exposedFields. A node ignores an eventIn by not accepting the new value
and not generating an eventOut___changed__ event. In this section we
refer to an abstract *_TimeDep_* node which can be any one of AudioClip,
MovieTexture, or TimeSensor.

TimeDep nodes can execute for 0 or more cycles. A cycle is defined by
field data within the node. If, at the end of a cycle, the value of
_loop_ is FALSE, then execution is terminated (see below for events at
termination). Conversely, if _loop_ is TRUE at the end of a cycle, then
a TimeDep node continues execution into the next cycle. A TimeDep node
with loop TRUE at the end of every cycle continues cycling forever if
_startTime >= stopTime_, or until _stopTime_ if _stopTime > startTime_.

A TimeDep node will generate an _isActive_ TRUE event when it becomes
active and will generate an _isActive_ FALSE event when it becomes
inactive. These are the only times at which an _isActive_ event is
generated, i.e., they are not sent at each tick of a simulation.

A TimeDep node is inactive until its _startTime_ is reached. When time
`now` is equal to _startTime_ an _isActive_ TRUE event is generated
and the TimeDep node becomes active. When a TimeDep node is read from a
file, and the ROUTEs specified within the file have been established,
the node should determine if it is active and, if so, generate an
_isActive_ TRUE event and begin generating any other necessary events.
However, if a node would have become inactive at any time before the
reading of the file, then no events are generated upon the completion of
the read.

An active TimeDep node will become inactive at time `now` for `now`
_= stopTime > startTime._ The value of _stopTime_ is ignored if
_stopTime <= startTime_. Also, an active TimeDep node will become
inactive at the end of the current cycle if _loop_ = FALSE. If an active
TimeDep node receives a _set_loop_ = FALSE event, then execution
continues until the end of the current cycle or until _stopTime_ (if
_stopTime > startTime_), whichever occurs first. The termination at the
end of cycle can be overridden by a subsequent _set_loop_ = TRUE event.

_set_startTime_ events to an active TimeDep node are ignored.
_set_stopTime_ events, where _set_stopTime <= startTime,_ to an active
TimeDep node are also ignored. A _set_stopTime_ event to an active
TimeDep node, where _startTime < set_stopTime <=_ `+now,+` result in
events being generated as if _stopTime_ = `+now. +`That is, final
events, including an _isActive_ FALSE, are generated and the node
becomes inactive. The _stopTime_changed_ event will have the
_set_stopTime_ value. Other final events are node dependent (c.f.,
TimeSensor).

A TimeDep node may be re-started while it is active by sending it a
_set_stopTime_ = `+now +`event (which will cause the node to become
inactive) and a _set_startTime_ event (setting it to `now` or any time
in the future). Browser authors should note that these events will have
the same time stamp and should be processed as _set_stopTime,_ then
_set_startTime_ to produce the correct behavior.

The default values for each of the TimeDep nodes have been specified
such that a node with default values became inactive in the past (and,
therefore, will generate no events upon reading). A TimeDep node can be
made active upon reading by specifying _loop_ TRUE. This use of a
nonterminating TimeDep node should be used with caution since it incurs
continuous overhead on the simulation.


Contact rikk@best.com , cmarrin@sgi.com, or gavin@acm.org with questions or comments.

[[nodesRef_html]]
=== 5. Node Reference


This section provides a detailed definition of the syntax and semantics
of each node in the specification.


[width="100%",cols="34%,33%,33%",]
|===
a|
<<GroupingNodes, Grouping nodes>>

• <<Anchor, Anchor>>
• <<Billboard, Billboard>>
• <<Collision, Collision>>
• <<Group, Group>>
• <<Transform, Transform>>

Special Groups

• <<Inline, Inline>>
• <<LOD, LOD>>
• <<Switch, Switch>>

Common Nodes

• <<AudioClip, AudioClip>>
• <<DirectionalLight, DirectionalLight>>
• <<PointLight, PointLight>>
• <<Script, Script>>
• <<Shape, Shape>>
• <<Sound, Sound>>
• <<SpotLight, SpotLight>>
• <<WorldInfo, WorldInfo>>

a|
<<Sensors, Sensors>>

• <<CylinderSensor, CylinderSensor>>
• <<PlaneSensor, PlaneSensor>>
• <<ProximitySensor, ProximitySensor>>
• <<SphereSensor, SphereSensor>>
• <<TimeSensor, TimeSensor>>
• <<TouchSensor, TouchSensor>>
• <<VisibilitySensor, VisibilitySensor>>

<<GeometryNodes, Geometry>>

• <<Box, Box>>
• <<Cone, Cone>>
• <<Cylinder, Cylinder>>
• <<ElevationGrid, ElevationGrid>>
• <<Extrusion, Extrusion>>
• <<IndexedFaceSet, IndexedFaceSet>>
• <<IndexedLineSet, IndexedLineSet>>
• <<PointSet, PointSet>>
• <<Sphere, Sphere>>
• <<Text, Text>>

*Geometric Properties*

• <<Color, Color>>
• <<Coordinate, Coordinate>>
• <<Normal, Normal>>
• <<TextureCoordinate, TextureCoordinate>>

a|
*Appearance*

• <<Appearance, Appearance>>
• <<FontStyle, FontStyle>>
• <<ImageTexture, ImageTexture>>
• <<Material, Material>>
• <<MovieTexture, MovieTexture>>
• <<PixelTexture, PixelTexture>>
• <<TextureTransform, TextureTransform>>

<<InterpolatorNodes, Interpolators>>

• <<ColorInterpolator, ColorInterpolator>>
• <<CoordinateInterpolator, CoordinateInterpolator>>
• <<NormalInterpolator, NormalInterpolator>>
• <<OrientationInterpolator, OrientationInterpolator>>
• <<PositionInterpolator, PositionInterpolator>>
• <<ScalarInterpolator, ScalarInterpolator>>

<<BindableLeafNodes, Bindable Nodes>>

• <<Background, Background>>
• <<Fog, Fog>>
• <<NavigationInfo, NavigationInfo>>
• <<Viewpoint, Viewpoint>>

|===


[[Anchor]]
==== Anchor

[source]
....
Anchor {
  eventIn      MFNode   addChildren
  eventIn      MFNode   removeChildren
  exposedField MFNode   children        []
  exposedField SFString description     "" 
  exposedField MFString parameter       []
  exposedField MFString url             []
  field        SFVec3f  bboxCenter      0 0 0
  field        SFVec3f  bboxSize        -1 -1 -1
}
....

The Anchor grouping node causes a URL to be fetched over the network
when the viewer activates (e.g. clicks) some geometry contained within
the Anchor's children. If the URL pointed to is a legal VRML world, then
that world replaces the world which the Anchor is a part of. If non-VRML
data type is fetched, it is up to the browser to determine how to handle
that data; typically, it will be passed to an appropriate general
viewer.

Exactly how a user activates a child of the Anchor depends on the
pointing device and is determined by the VRML browser. Typically,
clicking with the pointing device will result in the new scene replacing
the current scene. An Anchor with an empty ("") _url_ does nothing when
its children are chosen. 
See "<<Sensors, Concepts - Sensors and Pointing Device Sensors>>" 
for a description of how multiple Anchors and pointing device
sensors are resolved on activation.

See the "<<GroupingNodes, Concepts - Grouping and Children Nodes>>" 
section for a description of _children_, _addChildren_, and
_removeChildren_ fields and eventIns.

The _description_ field in the Anchor allows for a prompt to be
displayed as an alternative to the URL in the _url_ field. Ideally,
browsers will allow the user to choose the description, the URL, or both
to be displayed for a candidate Anchor.

The _parameter_ exposed field may be used to supply any additional
information to be interpreted by the VRML or HTML browser. Each string
should consist of "keyword=value" pairs. For example, some browsers
allow the specification of a 'target' for a link, to display a link in
another part of the HTML document; the _parameter_ field is then:

[source]
....
Anchor {
  parameter [ "target=name_of_frame" ]
  ...
}
....

An Anchor may be used to bind the initial Viewpoint in a world by
specifying a URL ending with "#ViewpointName", where "ViewpointName" is
the name of a viewpoint defined in the file. For example:

[source]
....
Anchor {
  url "http://www.school.edu/vrml/someScene.wrl#OverView"
  children  Shape { geometry Box {} }
}
....

specifies an anchor that loads the file "someScene.wrl", and binds the
initial user view to the Viewpoint named "OverView" (when the Box is
activated). If the named Viewpoint is not found in the file, then ignore
it and load the file with the default Viewpoint. If no world is
specified, then this means that the Viewpoint specified should be bound
(_set_bind_ TRUE). For example:

[source]
....
Anchor {
  url "#Doorway"
  children Shape { geometry Sphere {} }
}
....

binds viewer to the viewpoint defined by the "Doorway" viewpoint in the
current world when the sphere is activated. In this case, if the
Viewpoint is not found, then do nothing on activation.


See "<<URLsAndURNs, Concepts - URLS and URNs>>" for more
details on the _url_ field.

The _bboxCenter_ and _bboxSize_ fields specify a bounding box that
encloses the Anchor's children. This is a hint that may be used for
optimization purposes. If the specified bounding box is smaller than the
actual bounding box of the children at any time, then the results are
undefined. A default _bboxSize_ value, (-1 -1 -1), implies that the
bounding box is not specified and if needed must be calculated by the
browser. 
See "<<BoundingBoxes, Concepts - Bounding Boxes>>" for a
description of _bboxCenter_ and _bboxSize_ fields.


[[Appearance]]
==== Appearance

[source]
....
Appearance {
  exposedField SFNode material          NULL
  exposedField SFNode texture           NULL
  exposedField SFNode textureTransform  NULL
}
....

The Appearance node specifies the visual properties of geometry by
defining the material and texture nodes. The value for each of the
fields in this node can be NULL. However, if the field is non-NULL, it
must contain one node of the appropriate type.

The _material_ field, if specified, must contain a
<<Material, Material>> node. If the _material_ field is NULL or
unspecified, lighting is off (all lights are ignored during rendering of
the object that references this Appearance) and the unlit object color
is (0, 0, 0) - 
see "<<LightsAndLighting, Concepts - Lighting Model>>" for
details of the VRML lighting model.

The _texture_ field, if specified, must contain one of the various types
of texture nodes (<<ImageTexture, ImageTexture>>,
<<MovieTexture, MovieTexture>>, or <<PixelTexture, PixelTexture>>).
If the texture node is NULL or unspecified, the object that references
this Appearance is not textured.

The _textureTransform_ field, if specified, must contain a
<<TextureTransform, TextureTransform>> node. If the _texture_ field is
NULL or unspecified, or if the _textureTransform_ is NULL or
unspecified, the _textureTransform_ field has no effect.


[[AudioClip]]
==== AudioClip

[source]
....
AudioClip {
  exposedField   SFString description      ""
  exposedField   SFBool   loop             FALSE
  exposedField   SFFloat  pitch            1.0
  exposedField   SFTime   startTime        0
  exposedField   SFTime   stopTime         0
  exposedField   MFString url              []
  eventOut       SFTime   duration_changed
  eventOut       SFBool   isActive
}
....

An AudioClip node specifies audio data that can be referenced by other
nodes that require an audio source.

The _description_ field is a textual description of the audio source. A
browser is not required to display the _description_ field but may
choose to do so in addition to or in place of playing the sound.

The _url_ field specifies the URL from which the sound is loaded.
Browsers shall support at least the _wavefile_ format in uncompressed
PCM format <<bibliography_html, WAVE>>. It is recommended
that browsers also support the MIDI file type 1 sound format
<<bibliography_html, MIDI>>. MIDI files are presumed to
use the General MIDI patch set. See the section on URLs and URNs in 
"<<URLsAndURNs, Concepts - URLs and URNs>>" for details on
the _url_ field. Results are not defined when the URL references
unsupported data types.

The _loop, startTime,_ and _stopTime_ exposedFields and the _isActive_
eventOut, and their affects on the AudioClip node, are discussed in
detail in the "<<TimeDep, Concepts - Time Dependent Nodes>>" section.
The "_cycle"_ of an AudioClip is the length of time in seconds for one
playing of the audio at the specified _pitch_.

The _pitch_ field specifies a multiplier for the rate at which sampled
sound is played. Only positive values are valid for _pitch_ (a value of
zero or less will produce undefined results). Changing the _pitch_ field
affects both the pitch and playback speed of a sound. A _set_pitch_
event to an active AudioClip is ignored (and no _pitch_changed_ eventOut
is generated). If _pitch_ is set to 2.0, the sound should be played one
octave higher than normal and played twice as fast. For a sampled sound,
the _pitch_ field alters the sampling rate at which the sound is played.
The proper implementation of the _pitch_ control for MIDI (or other note
sequence sound clip) is to multiply the tempo of the playback by the
_pitch_ value and adjust the MIDI Coarse Tune and Fine Tune controls to
achieve the proper pitch change. The _pitch_ field must be > 0.0.

A _duration_changed_ event is sent whenever there is a new value for the
"normal" duration of the clip. Typically this will only occur when the
current _url_ in use changes and the sound data has been loaded,
indicating that the clip is playing a different sound source. The
duration is the length of time in seconds for one cycle of the audio for
a _pitch_ set to 1.0. Changing the _pitch_ field will not trigger a
_duration_changed_ event. A duration value of -1 implies the sound data
has not yet loaded or the value is unavailable for some reason.

The _isActive_ eventOut can be used by other nodes to determine if the
clip is currently active. If an AudioClip is active, then it should be
playing the sound corresponding to the sound time (i.e., in the sound's
local time system with sample 0 at time 0):

[source]
....
      fmod (now - startTime, duration / pitch).
....


[[Background]]
==== Background

[source]
....
Background {
  eventIn      SFBool   set_bind
  exposedField MFFloat  groundAngle  []
  exposedfield MFColor  groundColor  []
  exposedField MFString backUrl      []
  exposedField MFString bottomUrl    []
  exposedField MFString frontUrl     []
  exposedField MFString leftUrl      []
  exposedField MFString rightUrl     []
  exposedField MFString topUrl       []
  exposedField MFFloat  skyAngle     []
  exposedField MFColor  skyColor     [ 0 0 0 ]
  eventOut     SFBool   isBound
}
....

The Background node is used to specify a color backdrop that simulates
ground and sky, as well as a background texture, or _panorama_, that is
placed behind all geometry in the scene and in front of the ground and
sky. Background nodes are specified in the local coordinate system and
are affected by the accumulated rotation of their parents (see below).

Background nodes are bindable nodes (
see "<<BindableLeafNodes, Concepts - Bindable Children Nodes>>"). 
There exists a Background stack, in which the top-most
Background on the stack is the currently active Background and thus
applied to the view. To move a Background to the top of the stack, a
TRUE value is sent to the _set_bind_ eventIn. Once active, the
Background is then bound to the browsers view. A FALSE value of
_set_bind_, removes the Background from the stack and unbinds it from
the browser viewer. 
See "<<BindableLeafNodes, Concepts - Bindable Children Nodes>>" 
for more details on the the bind stack.

The ground and sky backdrop is conceptually a partial sphere (i.e.
ground) enclosed inside of a full sphere (i.e. sky) in the local
coordinate system, with the viewer placed at the center of the spheres.
Both spheres have infinite radius (epsilon apart), and each is painted
with concentric circles of interpolated color perpendicular to the local
Y axis of the sphere. The Background node is subject to the accumulated
rotations of its parent transformations - scaling and translation
transformations are ignored. The sky sphere is always slightly farther
away from the viewer than the ground sphere - the ground appears in
front of the sky in cases where they overlap.

The _skyColor_ field specifies the color of the sky at the various
angles on the sky sphere. The first value of the _skyColor_ field
specifies the color of the sky at 0.0 degrees, the north pole (i.e.
straight up from the viewer). The _skyAngle_ field specifies the angles
from the north pole in which concentric circles of color appear - the
north pole of the sphere is implicitly defined to be 0.0 degrees, the
natural horizon at _pi_/2 radians, and the south pole is _pi_ radians.
_skyAngle_ is restricted to increasing values in the range 0.0 to _pi_.
There must be one more _skyColor_ value than there are _skyAngle_ values
- the first color value is the color at the north pole, which is not
specified in the _skyAngle_ field. If the last _skyAngle_ is less than
_pi_, then the color band between the last _skyAngle_ and the south pole
is clamped to the last _skyColor_. The sky color is linearly
interpolated between the specified _skyColor_ values.

The _groundColor_ field specifies the color of the ground at the various
angles on the ground sphere. The first value of the _groundColor_ field
specifies the color of the ground at 0.0 degrees, the south pole (i.e.
straight down). The _groundAngle_ field specifies the angles from the
south pole that the concentric circles of color appear - the south pole
of the sphere is implicitly defined at 0.0 degrees. _groundAngle_ is
restricted to increasing values in the range 0.0 to _pi_. There must be
one more _groundColor_ values than there are _groundAngle_ values - the
first color value is for the south pole which is not specified in the
_groundAngle_ field. If the last _groundAngle_ is less than _pi_ (it
usually is), then the region between the last _groundAngle_ and the
north pole is invisible. The ground color is linearly interpolated
between the specified _groundColor_ values.

The _backUrl_, _bottomUrl_, _frontUrl_, _leftUrl_, _rightUrl_, and
_topUrl_ fields specify a set of images that define a background
panorama, between the ground/sky backdrop and the world's geometry. The
panorama consists of six images, each of which is mapped onto the faces
of an infinitely large cube centered in the local coordinate system. The
images are applied individually to each face of the cube; the entire
image goes on each face. On the front, back, right, and left faces of
the cube, when viewed from the inside with the Y-axis up, the texture is
mapped onto each face with the same orientation as the if image was
displayed normally in 2D. On the top face of the cube, when viewed from
the inside looking up along the +Y axis with the +Z axis as the view up
direction, the texture is mapped onto the face with the same orientation
as the if image was displayed normally in 2D. On the bottom face of the
box, when viewed from the inside down the -Y axis with the -Z axis as
the view up direction, the texture is mapped onto the face with the same
orientation as the if image was displayed normally in 2D.

Alpha values in the panorama images (i.e. two or four component images)
specify that the panorama is semi-transparent or transparent in regions,
allowing the _groundColor_ and _skyColor_ to be visible. One component
images are displayed in greyscale; two component images are displayed in
greyscale with alpha transparency; three component images are displayed
in full RGB color; four component images are displayed in full RGB color
with alpha transparency. Often, the _bottomUrl_ and _topUrl_ images will
not be specified, to allow sky and ground to show. The other four images
may depict surrounding mountains or other distant scenery. Browsers are
required to support the JPEG <<references_html, JPEG>> and
<<references_html, PNG>> image file formats, and in addition, may 
support any other image formats. Support for the <<bibliography_html, GIF>>
format (including transparent backgrounds) is recommended. See the
section "<<URLsAndURNs, Concepts - URLS and URNs>>" for details on
the _url_ fields.

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/background.gif[]

Panorama images may be one component (greyscale), two component
(greyscale plus alpha), three component (full RGB color), or
four-component (full RGB color plus alpha).

Ground colors, sky colors, and panoramic images do not translate with
respect to the viewer, though they do rotate with respect to the viewer.
That is, the viewer can never get any closer to the background, but can
turn to examine all sides of the panorama cube, and can look up and down
to see the concentric rings of ground and sky (if visible).

Background is not affected by <<Fog, Fog>>. Therefore, if a Background
is active (i.e bound) while a Fog is active, then the Background will be
displayed with no fogging effects. It is the author's responsibility to
set the Background values to match the Fog (e.g. ground colors fade to
fog color with distance and panorama images tinted with fog color).

The first Background node found during reading of the world is
automatically bound (receives _set_bind_ TRUE) and is used as the
initial background when the world is loaded.


[[Billboard]]
==== Billboard

[source]
....
Billboard {
  eventIn      MFNode   addChildren
  eventIn      MFNode   removeChildren
  exposedField SFVec3f  axisOfRotation  0 1 0
  exposedField MFNode   children        []
  field        SFVec3f  bboxCenter      0 0 0
  field        SFVec3f  bboxSize        -1 -1 -1
}
....

The Billboard node is a grouping node which modifies its coordinate
system so that the billboard node's local Z-axis turns to point at the
viewer. The Billboard node has children which may be other grouping or
leaf nodes.

The _axisOfRotation_ field specifies which axis to use to perform the
rotation. This axis is defined in the local coordinates of the Billboard
node. The default (0,1,0) is useful for objects such as images of trees
and lamps positioned on a ground plane. But when an object is oriented
at an angle, for example, on the incline of a mountain, then the
_axisOfRotation_ may also need to be oriented at a similar angle.

A special case of billboarding is _screen-alignment_ -- the object
rotates to always stay aligned with the viewer even when the viewer
elevates, pitches and rolls. This special case is distinguished by
setting the _axisOfRotation_ to (0, 0, 0).

To rotate the Billboard to face the viewer, determine the line between
the Billboard's origin and the viewer's position; call this the
_billboard-to-viewer_ line. The _axisOfRotation_ and the
billboard-to-viewer line define a plane. The local z-axis of the
Billboard is then rotated into that plane, pivoting around the
_axisOfRotation_.

If the _axisOfRotation_ and the billboard-to-viewer line are coincident
(the same line), then the plane cannot be established, and the rotation
results of the Billboard are undefined. For example, if the
_axisOfRotation_ is set to (0,1,0) (Y-axis) and the viewer flies over
the Billboard and peers directly down the Y-axis the results are
undefined**.**

Multiple instances of Billboards (DEF/USE) operate as expected - each
instance rotates in its unique coordinate system to face the viewer.

See the "<<GroupingNodes, Concepts - Grouping and Children Nodes>>" 
section for a description the _children_, _addChildren_, and
_removeChildren_ fields and eventIns.

The _bboxCenter_ and _bboxSize_ fields specify a bounding box that
encloses the Billboard's children. This is a hint that may be used for
optimization purposes. If the specified bounding box is smaller than the
actual bounding box of the children at any time, then the results are
undefined. A default _bboxSize_ value, (-1 -1 -1), implies that the
bounding box is not specified and if needed must be calculated by the
browser. 
See "<<BoundingBoxes, Concepts - Bounding Boxes>>" for a
description of _bboxCenter_ and _bboxSize_ fields.


[[Box]]
==== Box

[source]
....
Box {
  field    SFVec3f size  2 2 2 
}
....

The Box node specifies a rectangular parallelepiped box in the local
coordinate system centered at (0,0,0) in the local coordinate system and
aligned with the coordinate axes. By default, the box measures 2 units
in each dimension, from -1 to +1. The Box's _size_ field specifies the
extents of the the box along the X, Y, and Z axes respectively and must
be greater than 0.0.

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/box.gif[]

Textures are applied individually to each face of the box; the entire
untransformed texture goes on each face. On the front, back, right, and
left faces of the box, when viewed from the outside with the Y-axis up,
the texture is mapped onto each face with the same orientation as the if
image was displayed in normally 2D. On the top face of the box, when
viewed from the outside along the +Y axis looking down with the -Z axis
as the view up direction, the texture is mapped onto the face with the
same orientation as if the image were displayed normally in 2D. On the
bottom face of the box, when viewed from the outside along the -Y axis
looking up with the +Z axis as the view up direction, the texture is
mapped onto the face with the same orientation as if the image were
displayed normally in 2D. <<TextureTransform, TextureTransform>>
affects the texture coordinates of the Box.

The Box geometry is considered to be solid and thus requires outside
faces only. When viewed from the inside the results are undefined.


[[Collision]]
==== Collision

[source]
....
Collision { 
  eventIn      MFNode   addChildren
  eventIn      MFNode   removeChildren
  exposedField MFNode   children        []
  exposedField SFBool   collide         TRUE
  field        SFVec3f  bboxCenter      0 0 0
  field        SFVec3f  bboxSize        -1 -1 -1
  field        SFNode   proxy           NULL
  eventOut     SFTime   collideTime
}
....

By default, *all objects in the scene are collidable*. Browser shall
detect geometric collisions between the user's avatar (see
<<NavigationInfo, NavigationInfo>>) and the scene's geometry, and
prevent the avatar from `entering' the geometry. The Collision node is
grouping node that may turn off collision detection for its descendants,
specify alternative objects to use for collision detection, and send
events signaling that a collision has occurred between the user's avatar
and the Collision group's geometry or alternate. If there are no
Collision nodes specified in a scene, browsers shall detect collision
with all objects during navigation.

See the "<<GroupingNodes, Concepts - Grouping and Children Nodes>>" 
section for a description the _children_, _addChildren_, and
_removeChildren_ fields and eventIns.

The Collision node's _collide_ field enables and disables collision
detection. If _collide_ is set to FALSE, the children and all
descendants of the Collision node will not be checked for
collision, even though they are drawn. This includes any descendant
Collision nodes that have _collide_ set to TRUE - (i.e. setting
_collide_ to FALSE turns it off for every node below it).

Collision nodes with the _collide_ field set to TRUE detect the nearest
collision with their descendant geometry (or proxies). Note that not all
geometry is collidable - see each geometry node's sections for details.
When the nearest collision is detected, the collided Collision node
sends the time of the collision through its _collideTime_ eventOut. This
behavior is recursive - if a Collision node contains a child,
descendant, or proxy (see below) that is a Collision node, and both
Collisions detect that a collision has occurred, then both send a
_collideTime_ event out at the same time, and so on.

The _bboxCenter_ and _bboxSize_ fields specify a bounding box that
encloses the Collision's children. This is a hint that may be used for
optimization purposes. If the specified bounding box is smaller than the
actual bounding box of the children at any time, then the results are
undefined. A default _bboxSize_ value, (-1 -1 -1), implies that the
bounding box is not specified and if needed must be calculated by the
browser. 
See "<<BoundingBoxes, Concepts - Bounding Boxes>>" for a
description of the _bboxCenter_ and _bboxSize_ fields.

The collision proxy, defined in the _proxy_ field, is a legal child
node, (
see "<<GroupingNodes, Concepts - Grouping and Children Nodes>>"), 
that is used as a substitute for the Collision's children
during collision detection. The proxy is used strictly for
collision detection - it is not drawn.

If the value of the _collide_ field is FALSE, then collision detection
is not performed with the children or proxy descendant
nodes. If the root node of a scene is a Collision node with the
_collide_ field set to FALSE, then collision detection is disabled for
the entire scene, regardless of whether descendent Collision nodes have
set _collide_ TRUE.

If the value of the _collide_ field is TRUE and the _proxy_ field is
non-NULL, then the _proxy_ field defines the scene which collision
detection is performed. If the _proxy_ value is NULL, the _children_ of
the collision node are collided against.

If _proxy_ is specified, then any descendant children of the Collision
node are ignored during collision detection. If _children_ is empty,
_collide_ is TRUE and _proxy_ is specified, then collision detection is
done against the proxy but nothing is displayed (i.e. invisible
collision objects).

The _collideTime_ eventOut generates an event specifying the time when
the user's avatar (see <<NavigationInfo, NavigationInfo>>) intersects
the collidable children or proxy of the Collision node. An ideal
implementation computes the exact time of intersection. Implementations
may approximate the ideal by sampling the positions of collidable
objects and the user. Refer to the <<NavigationInfo, NavigationInfo>>
node for parameters that control the user's size.

Browsers are responsible for defining the navigation behavior when
collisions occur. For example, when the user comes sufficiently close to
an object to trigger a collision, the browser may have the user bounce
off the object, come to a stop, or glide along the surface.


[[Color]]
==== Color

[source]
....
Color {
  exposedField MFColor color  []
}
....

This node defines a set of RGB colors to be used in the fields of
another node.

Color nodes are only used to specify multiple colors for a
single piece of geometry, such as a different color for each face or
vertex of an IndexedFaceSet. A Material node is used to specify the
overall material parameters of a lighted geometry. If both a Material
and a Color node are specified for a geometry, the colors should ideally
replace the diffuse component of the material.

Textures take precedence over colors; specifying both a Texture and a
Color node for a geometry will result in the Color node being ignored.

See "<<LightsAndLighting, Concepts - Lighting Model>>" for
details on lighting equations.


[[ColorInterpolator]]
==== ColorInterpolator

[source]
....
ColorInterpolator {
  eventIn      SFFloat set_fraction
  exposedField MFFloat key           []
  exposedField MFColor keyValue      []
  eventOut     SFColor value_changed
}
....

This node interpolates among a set of MFColor key values, to produce an
SFColor (RGB) _value_changed_ event. The number of colors in the
_keyValue_ field must be equal to the number of keyframes in the _key_
field. The _keyValue_ field and _value_changed_ events are defined in
RGB color space. A linear interpolation, using the value of
_set_fraction_ as input, is performed in HSV space.

Refer to "<<InterpolatorNodes, Concepts - Interpolators>>" for a
more detailed discussion of interpolators.


[[Cone]]
==== Cone

[source]
....
Cone {
  field     SFFloat   bottomRadius 1
  field     SFFloat   height       2
  field     SFBool    side         TRUE
  field     SFBool    bottom       TRUE
}
....

The Cone node specifies a cone which is centered in the local coordinate
system and whose central axis is aligned with the local Y-axis. The
_bottonRadius_ field specifies the radius of the cone's base, and the
_height_ field specifies the height of the cone from the center of the
base to the apex. By default, the cone has a radius of 1.0 at the bottom
and a height of 2.0, with its apex at y=1 and its bottom at y=-1. Both
_bottomRadius_ and _height_ must be greater than 0.0.

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/cone.gif[]

The _side_ field specifies whether sides of the cone are created, and
the _bottom_ field specifies whether the bottom cap of the cone is
created. A value of TRUE specifies that this part of the cone exists,
while a value of FALSE specifies that this part does not exist (not
rendered). Parts with field values of FALSE are not collided with during
collision detection.

When a texture is applied to the sides of the cone, the texture wraps
counterclockwise (from above) starting at the back of the cone. The
texture has a vertical seam at the back in the YZ plane, from the apex
(0, _height_/2, 0) to the point (0, 0, -r). For the bottom cap, a circle
is cut out of the unit texture square centered at (0, -_height_/2, 0)
with dimensions (2 * _bottomRadius)_ by (2 * _bottomRadius)_. The bottom
cap texture appears right side up when the top of the cone is rotated
towards the -Z axis. <<TextureTransform, TextureTransform>> affects
the texture coordinates of the Cone.

The Cone geometry is considered to be solid and thus requires outside
faces only. When viewed from the inside the results are undefined.


[[Coordinate]]
==== Coordinate

[source]
....
Coordinate {
  exposedField MFVec3f point  []
}
....

This node defines a set of 3D coordinates to be used in the _coord_
field of vertex-based geometry nodes (such as IndexedFaceSet,
IndexedLineSet, and PointSet).


[[CoordinateInterpolator]]
==== CoordinateInterpolator

[source]
....
CoordinateInterpolator {
  eventIn      SFFloat set_fraction
  exposedField MFFloat key           []
  exposedField MFVec3f keyValue      []
  eventOut     MFVec3f value_changed
}

[source]
....

This node linearly interpolates among a set of MFVec3f value. This would
be appropriate for interpolating <<Coordinate, Coordinate>> positions
for a geometric morph.

The number of coordinates in the _keyValue_ field must be an integer
multiple of the number of keyframes in the _key_ field; that integer
multiple defines how many coordinates will be contained in the
_value_changed_ events.

Refer to "<<InterpolatorNodes, Concepts - Interpolators>>" for a
more detailed discussion of interpolators.


[[Cylinder]]
==== Cylinder

[source]
....
Cylinder {
  field    SFBool    bottom  TRUE
  field    SFFloat   height  2
  field    SFFloat   radius  1
  field    SFBool    side    TRUE
  field    SFBool    top     TRUE
}
....

The Cylinder node specifies a capped cylinder centered at (0,0,0) in the
local coordinate system and with a central axis oriented along the local
Y-axis. By default, the cylinder is sized at -1 to +1 in all three
dimensions. The _radius_ field specifies the cylinder's radius and the
_height_ field specifies the cylinder's height along the central axis.
Both _radius_ and _height_ must be greater than 0.0.

The cylinder has three _parts_: the _side_, the _top_ (Y = +height) and
the _bottom_ (Y = -height). Each part has an associated SFBool field
that indicates whether the part exists (TRUE) or does not exist (FALSE).
If the parts do not exist, the they are not considered during collision
detection.

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/cylinder.gif[]

When a texture is applied to a cylinder, it is applied differently to
the sides, top, and bottom. On the sides, the texture wraps
counterclockwise (from above) starting at the back of the cylinder. The
texture has a vertical seam at the back, intersecting the YZ plane. For
the top and bottom caps, a circle is cut out of the unit texture square
centered at (0, +/- _height_, 0) with dimensions 2*_radius_ by
2*_radius_. The top texture appears right side up when the top of the
cylinder is tilted toward the +Z axis, and the bottom texture appears
right side up when the top of the cylinder is tilted toward the -Z axis.
<<TextureTransform, TextureTransform>> affects the texture coordinates
of the Cylinder.

The Cylinder geometry is considered to be solid and thus requires
outside faces only. When viewed from the inside the results are
undefined.


[[CylinderSensor]]
==== CylinderSensor

[source]
....
CylinderSensor {
  exposedField SFBool     autoOffset TRUE
  exposedField SFFloat    diskAngle  0.262
  exposedField SFBool     enabled    TRUE
  exposedField SFFloat    maxAngle   -1
  exposedField SFFloat    minAngle   0
  exposedField SFFloat    offset     0
  eventOut     SFBool     isActive
  eventOut     SFRotation rotation_changed
  eventOut     SFVec3f    trackPoint_changed
}
....

The CylinderSensor maps pointing device (e.g. mouse or wand) motion into
a rotation on an invisible cylinder that is aligned with the Y axis of
its local space.

The _enabled_ exposed field enables and disables the CylinderSensor - if
TRUE, the sensor reacts appropriately to user events, if FALSE, the
sensor does not track user input or send output events. If _enabled_
receives a FALSE event and _isActive_ is TRUE, the sensor becomes
disabled and deactivated, and outputs an _isActive_ FALSE event. If
_enabled_ receives a TRUE event the sensor is enabled and ready for user
activation.

The CylinderSensor generates events if the pointing device is activated
while over any descendant geometry nodes of its parent group and then
moved while activated. Typically, the pointing device is a 2D device
such as a mouse. The pointing device is considered to be moving within a
plane at a fixed distance from the viewer and perpendicular to the line
of sight; this establishes a set of 3D coordinates for the pointer. If a
3D pointer is in use, then the sensor generates events only when the
pointer is within the user's field of view. In either case, the pointing
device is considered to "pass over" geometry when that geometry is
intersected by a line extending from the viewer and passing through the
pointer's 3D coordinates. If multiple sensors' geometry intersect this
line (hereafter called the bearing), only the nearest will be eligible
to generate events.

Upon activation of the pointing device (e.g. mouse button down) over the
sensor's geometry, an _isActive_ TRUE event is sent. The angle between
the bearing vector and the local Y axis of the CylinderSensor determines
whether the sides of the invisible cylinder or the caps (disks) are used
for manipulation. If the angle is less than the _diskAngle_, then the
geometry is treated as an infinitely large disk and dragging motion is
mapped into a rotation around the local Y axis of the sensor's
coordinate system. The feel of the rotation is as if you were rotating a
dial or crank. Using the right-hand rule, the X axis of the sensor's
local coordinate system, (defined by parents), represents the zero
rotation value around the sensor's local Y axis. For each subsequent
position of the bearing, a _rotation_changed_ event is output which
corresponds to the angle between the local X axis and the vector defined
by the intersection point and the nearest point on the local Y axis,
plus the _offset_ value. _trackPoint_changed_ events reflect the
unclamped drag position on the surface of this disk. When the pointing
device is deactivated and _autoOffset_ is TRUE, _offset_ is set to the
last rotation angle and an _offset_changed_ event is generated. 
See "<<DragSensors, Concepts - Drag Sensors>>" for more
details on _autoOffset_ and _offset_changed_.

If angle between the bearing vector and the local Y axis of the
CylinderSensor is greater than or equal to _diskAngle_, then the sensor
behaves like a cylinder or rolling pin. The shortest distance between
the point of intersection (between the bearing and the sensor's
geometry) and the Y axis of the parent group's local coordinate system
determines the radius of an invisible cylinder used to map pointing
device motion, and mark the zero rotation value. For each subsequent
position of the bearing, a _rotation_changed_ event is output which
corresponds to a relative rotation from the original intersection, plus
the _offset_ value. _trackPoint_changed_ events reflect the unclamped
drag position on the surface of this cylinder. When the pointing device
is deactivated and _autoOffset_ is TRUE, _offset_ is set to the last
rotation angle and an _offset_changed_ event is generated. 
See "<<DragSensors, Concepts - Drag Sensors>>" for more
details.

When the sensor generates an _isActive_ TRUE event, it grabs all further
motion events from the pointing device until it releases and generates
an _isActive_ FALSE event (other pointing device sensors
cannot generate events during this time). Motion of the
pointing device while _isActive_ is TRUE is referred to as a "drag". If
a 2D pointing device is in use, _isActive_ events will typically reflect
the state of the primary button associated with the device (i.e.
_isActive_ is TRUE when the primary button is pressed, and FALSE when
not released). If a 3D pointing device (e.g. wand) is in use, _isActive_
events will typically reflect whether the pointer is within or in
contact with the sensor's geometry.

While the pointing device is activated, _trackPoint_changed_ and
_rotation_changed_ events are output and are interpreted from pointing
device motion based on the sensor's local coordinate system at the time
of activation. _trackPoint_changed_ events represent the unclamped
intersection points on the surface of the invisible cylinder or disk. If
the initial angle results in cylinder rotation (as opposed to disk
behavior) and if the pointing device is dragged off the cylinder while
activated, browsers may interpret this in several ways (e.g. clamp all
values to the cylinder, continue to rotate as the point is dragged away
from the cylinder, etc.). Each movement of the pointing device, while
_isActive_ is TRUE, generates _trackPoint_changed_ and
_rotation_changed_ events.

_minAngle_ and _maxAngle_ may be set to clamp _rotation_changed_ events
to a range of values (measured in radians about the local Z and Y axis
as appropriate). If _minAngle_ is greater than _maxAngle_,
_rotation_changed_ events are not clamped.


See "<<PointingDeviceSensors, Concepts - Pointing Device Sensors and Drag Sensors>>" 
for more details.


[[DirectionalLight]]
==== DirectionalLight

[source]
....
DirectionalLight {
  exposedField SFFloat ambientIntensity  0 
  exposedField SFColor color             1 1 1
  exposedField SFVec3f direction         0 0 -1
  exposedField SFFloat intensity         1 
  exposedField SFBool  on                TRUE 
}
....

The DirectionalLight node defines a directional light source that
illuminates along rays parallel to a given 3-dimensional vector. 
See "<<Lights, Concepts - Lights>>" for a definition of the
_ambientIntensity_, _color_, _intensity_, and _on_ fields.

The _direction_ field specifies the direction vector within the local
coordinate system that the light illuminates in. Light is emitted along
parallel rays from an infinite distance away. A directional light source
illuminates only the objects in its enclosing parent group. The light
may illuminate everything within this coordinate system, including all
children and descendants of its parent group. The accumulated
transformations of the parent nodes affect the light.


See "<<Lighting, Concepts - Lighting Model>>"for a precise
description of VRML's lighting equations.

Some low-end renderers do not support the concept of per-object
lighting. This means that placing DirectionalLights inside local
coordinate systems, which implies lighting only the objects beneath the
Transform with that light, is not supported in all systems. For the
broadest compatibility, lights should be placed at outermost scope.


[[ElevationGrid]]
==== ElevationGrid

[source]
....
ElevationGrid {
  eventIn      MFFloat  set_height
  exposedField SFNode   color             NULL
  exposedField SFNode   normal            NULL
  exposedField SFNode   texCoord          NULL
  field        MFFloat  height            []
  field        SFBool   ccw               TRUE
  field        SFBool   colorPerVertex    TRUE
  field        SFFloat  creaseAngle       0
  field        SFBool   normalPerVertex   TRUE
  field        SFBool   solid             TRUE
  field        SFInt32  xDimension        0
  field        SFFloat  xSpacing          0.0
  field        SFInt32  zDimension        0
  field        SFFloat  zSpacing          0.0
}
....

The ElevationGrid node specifies a uniform rectangular grid of varying
height in the XZ plane of the local coordinate system. The geometry is
described by a scalar array of height values that specify the height of
a rectangular surface above each point of the grid.

The _xDimension_ and _zDimension_ fields indicate the number of
dimensions of the grid _height_ array in the X and Z directions. Both
_xDimension_ and _zDimension_ must be > 1. The vertex locations for the
rectangles are defined by the _height_ field and the _xSpacing_ and
_zSpacing_ fields:

* The _height_ field is an _xDimension_ by _zDimension_ array of scalar
values representing the height above the grid for each vertex the height
values are stored in row major order.
* The _xSpacing_ and _zSpacing_ fields indicates the distance between
vertices in the X and Z directions respectively, and must be >= 0.

Thus, the vertex corresponding to the point, P[_i,_ j], on the grid is
placed at:

[source]
....
    P[i,j].x = xSpacing * i
    P[i,j].y = height[ i + j * zDimension]
    P[i,j].z = zSpacing * j

    where 0<i<xDimension and 0<j<zDimension. 
....

The _set_height_ eventIn allows the height MFFloat field to be changed
to allow animated ElevationGrids.

The default texture coordinates range from [0,0] at the first vertex to
[1,1] at the last vertex. The S texture coordinate will be aligned with
X, and the T texture coordinate with Z.

The _colorPerVertex_ field determines whether colors (if specified in
the color field) should be applied to each vertex or each quadrilateral
of the ElevationGrid. If _colorPerVertex_ is FALSE and the _color_ field
is not NULL, then the _color_ field must contain a Color node containing
at least (_xDimension-1)_*(_zDimension-1)_ colors. If _colorPerVertex_
is TRUE and the _color_ field is not NULL, then the _color_ field must
contain a Color node containing at least _xDimension_*_zDimension_
colors.

See the "<<Geometry, Concepts - Geometry>>" for a description of the _ccw_,
_solid_, and _creaseAngle_ fields.

By default, the rectangles are defined with a counterclockwise ordering,
so the Y component of the normal is positive. Setting the _ccw_ field to
FALSE reverses the normal direction. Backface culling is enabled when
the _ccw_ field and the _solid_ field are both TRUE (the default).


[[Extrusion]]
==== Extrusion

[source]
....
Extrusion {
  eventIn MFVec2f    set_crossSection
  eventIn MFRotation set_orientation
  eventIn MFVec2f    set_scale
  eventIn MFVec3f    set_spine
  field   SFBool     beginCap         TRUE
  field   SFBool     ccw              TRUE
  field   SFBool     convex           TRUE
  field   SFFloat    creaseAngle      0
  field   MFVec2f    crossSection     [ 1 1, 1 -1, -1 -1, -1 1, 1 1 ]
  field   SFBool     endCap           TRUE
  field   MFRotation orientation      0 0 1 0
  field   MFVec2f    scale            1 1
  field   SFBool     solid            TRUE
  field   MFVec3f    spine            [ 0 0 0, 0 1 0 ]
}
....

The Extrusion node specifies geometric shapes based on a two dimensional
cross section extruded along a three dimensional spine. The cross
section can be scaled and rotated at each spine point to produce a wide
variety of shapes.

An Extrusion is defined by a 2D _crossSection_ piecewise linear curve
(described as a series of connected vertices), a 3D _spine_ piecewise
linear curve (also described as a series of connected vertices), a list
of 2D _scale_ parameters, and a list of 3D _orientation_ parameters.
Shapes are constructed as follows: The cross-section curve, which starts
as a curve in the XZ plane, is first scaled about the origin by the
first _scale_ parameter (first value scales in X, second value scales in
Z). It is then rotated about the origin by the first _orientation_
parameter, and translated by the vector given as the first vertex of the
_spine_ curve. It is then extruded through space along the first segment
of the _spine_ curve. Next, it is scaled and rotated by the second
_scale_ and _orientation_ parameters and extruded by the second segment
of the _spine_, and so on. The number of _scale_ and _orientation_
values shall equal the number of spine points, or contain one value that
is applied to all points. The _scale_ values must be > 0.

A transformed cross section is found for each joint (that is, at each
vertex of the _spine_ curve, where segments of the extrusion connect),
and the joints and segments are connected to form the surface. No check
is made for self-penetration. Each transformed cross section is
determined as follows:

. Start with the cross section as specified, in the XZ plane.
. Scale it about (0, 0, 0) by the value for _scale_ given for the
current joint.
. Apply a rotation so that when the cross section is placed at its
proper location on the spine it will be oriented properly. Essentially,
this means that the cross section's Y axis (_up_ vector coming out of
the cross section) is rotated to align with an approximate tangent to
the spine curve.
+
_For all points other than the first or last:_ The tangent for
_spine_[_i_] is found by normalizing the vector defined by
(_spine_[__i__+1] - _spine_[_i_-1]).
+
_If the spine curve is closed:_ The first and last points need to have
the same tangent. This tangent is found as above, but using the points
_spine_[0] for _spine_[_i_], _spine_[1] for _spine_[__i__+1] and
_spine_[_n_-2] for _spine_[_i_-1], where _spine_[_n_-2] is the next to
last point on the curve. The last point in the curve, _spine_[_n_-1], is
the same as the first, _spine_[0].
+
_If the spine curve is not closed:_ The tangent used for the first point
is just the direction from _spine_[0] to _spine_[1], and the tangent
used for the last is the direction from _spine_[_n_-2] to
_spine_[_n_-1].
+
In the simple case where the spine curve is flat in the XY plane, these
rotations are all just rotations about the Z axis. In the more general
case where the spine curve is any 3D curve, you need to find the
destinations for all 3 of the local X, Y, and Z axes so you can
completely specify the rotation. The Z axis is found by taking the cross
product of:
+
(_spine_[_i_-1] - _spine_[_i_]) and (_spine_[__i__+1] - _spine_[_i_]).
+
If the three points are collinear then this value is zero, so take the
value from the previous point. Once you have the Z axis (from the cross
product) and the Y axis (from the approximate tangent), calculate the X
axis as the cross product of the Y and Z axes.
. Given the plane computed in step 3, apply the _orientation_ to the
cross-section relative to this new plane. Rotate it counter-clockwise
about the axis and by the angle specified in the _orientation_ field at
that joint.
. Finally, the cross section is translated to the location of the
_spine_ point.

_Surfaces of revolution:_ If the cross section is an approximation of a
circle and the spine is straight, then the Extrusion is equivalent to a
surface of revolution, where the _scale_ parameters define the size of
the cross section along the spine.

_Cookie-cutter extrusions:_ If the scale is 1, 1 and the spine is
straight, then the cross section acts like a cookie cutter, with the
thickness of the cookie equal to the length of the spine.

_Bend/twist/taper objects:_ These shapes are the result of using all
fields. The spine curve bends the extruded shape defined by the cross
section, the orientation parameters twist it around the spine, and the
scale parameters taper it (by scaling about the spine).

Extrusion has three _parts_: the _sides_, the _beginCap_ (the surface at
the initial end of the spine) and the _endCap_ (the surface at the final
end of the spine). The caps have an associated SFBool field that
indicates whether it exists (TRUE) or doesn't exist (FALSE).

When the _beginCap_ or _endCap_ fields are specified as TRUE, planar cap
surfaces will be generated regardless of whether the _crossSection_ is a
closed curve. (If _crossSection_ isn't a closed curve, the caps are
generated as if it were -- equivalent to adding a final point to
_crossSection_ that's equal to the initial point. Note that an open
surface can still have a cap, resulting (for a simple case) in a shape
something like a soda can sliced in half vertically.) These surfaces are
generated even if _spine_ is also a closed curve. If a field value is
FALSE, the corresponding cap is not generated.

Extrusion automatically generates its own normals. Orientation of the
normals is determined by the vertex ordering of the triangles generated
by Extrusion. The vertex ordering is in turn determined by the
_crossSection_ curve. If the _crossSection_ is counterclockwise when
viewed from the +Y axis, then the polygons will have counterclockwise
ordering when viewed from 'outside' of the shape (and _vice versa_ for
clockwise ordered _crossSection_ curves).

Texture coordinates are automatically generated by extrusions. Textures
are mapped so that the coordinates range in the U direction from 0 to 1
along the _crossSection_ curve (with 0 corresponding to the first point
in _crossSection_ and 1 to the last) and in the V direction from 0 to 1
along the _spine_ curve (again with 0 corresponding to the first listed
_spine_ point and 1 to the last). When _crossSection_ is closed, the
texture has a seam that follows the line traced by the _crossSection_'s
start/end point as it travels along the _spine_. If the _endCap_ and/or
_beginCap_ exist, the _crossSection_ curve is uniformly scaled and
translated so that the largest dimension of the cross-section (X or Z)
produces texture coordinates that range from 0.0 to 1.0. The _beginCap_
and _endCap_ textures' S and T directions correspond to the X and Z
directions in which the _crossSection_ coordinates are defined.


See "<<GeometryNodes, Concepts - Geometry Nodes>>" for a
description of the _ccw_, _solid_, _convex_, and _creaseAngle_ fields.


[[Fog]]
==== Fog

[source]
....
Fog {
  exposedField SFColor  color            1 1 1
  exposedField SFString fogType          "LINEAR"
  exposedField SFFloat  visibilityRange  0
  eventIn      SFBool   set_bind
  eventOut     SFBool   isBound
}
....

The Fog node provides a way to simulate atmospheric effects by blending
objects with the color specified by the _color_ field based on the
objects' distances from the viewer. The distances are calculated in the
coordinate space of the Fog node. The _visibilityRange_ specifies the
distance (in the Fog node's coordinate space) at which objects are
totally obscured by the fog. Objects located _visibilityRange_ meters or
more away from the viewer are drawn with a constant color of _color_.
Objects very close to the viewer are blended very little with the fog
_color_. A _visibilityRange_ of 0.0 or less disables the Fog node. Note
that _visibilityRange_ is affected by the scaling transformations of the
Fog node's parents - translations and rotations have no affect on
_visibilityRange_.

Fog nodes are "<<BindableLeafNodes, Concepts - Bindable Children Nodes>>" 
and thus there exists a Fog stack, in which the top-most Fog
node on the stack is currently active. To push a Fog node onto the top
of the stack, a TRUE value is sent to the _set_bind_ eventIn. Once
active, the Fog is then bound to the browsers view. A FALSE value of
_set_bind_, pops the Fog from the stack and unbinds it from the browser
viewer. 
See "<<BindableLeafNodes, Concepts - Bindable Children Nodes>>" 
for more details on the the Fog stack.

The _fogType_ field controls how much of the fog color is blended with
the object as a function of distance. If _fogType_ is "LINEAR" (the
default), then the amount of blending is a linear function of the
distance, resulting in a depth cuing effect. If _fogType_ is
"EXPONENTIAL" then an exponential increase in blending should be used,
resulting in a more natural fog appearance.

For best visual results, the Background node (which is unaffected by the
Fog node) should be the same color as the fog node. The Fog node can
also be used in conjunction with the _visibilityLimit_ field of
NavigationInfo node to provide a smooth fade out of objects as they
approach the far clipping plane.

See the section "<<Lighting, Concepts - Lighting Model>>" for details on
lighting calculations.


[[FontStyle]]
==== FontStyle

[source]
....
FontStyle {
  field SFString family       "SERIF"
  field SFBool   horizontal   TRUE
  field MFString justify      "BEGIN"
  field SFString language     ""
  field SFBool   leftToRight  TRUE
  field SFFloat  size         1.0
  field SFFloat  spacing      1.0
  field SFString style        "PLAIN"
  field SFBool   topToBottom  TRUE
}
....

The FontStyle node defines the size, font family, and style of text's
font, as well as the direction of the text strings and any specific
language rendering techniques that must be used for non-English text.
See <<Text, Text>> node for application of FontStyle.

The _size_ field specifies the height (in object space units) of glyphs
rendered and determines the spacing of adjacent lines of text. All
subsequent strings advance in either X or Y by -( _size_ * _spacing_).

===== Font Family and Style

Font attributes are defined with the family and style fields. It is up
to the browser to assign specific fonts to the various attribute
combinations.

The _family_ field specifies a case-sensitive SFString value that may be
*"SERIF"* (the default) for a serif font such as Times Roman; *"SANS"*
for a sans-serif font such as Helvetica; or *"TYPEWRITER"* for a
fixed-pitch font such as Courier. A _family_ value of empty quotes,
*""*, is identical to *"SERIF"*.

The _style_ field specifies a case-sensitive SFString value that may be
*"PLAIN"* (the default) for default plain type; *"BOLD"* for boldface
type; *"ITALIC"* for italic type; or *"BOLDITALIC"* for bold and italic
type. A _style_ value of empty quotes, *""*, is identical to *"PLAIN"*.

===== Direction, Justification and Spacing

The _horizontal_, _leftToRight_, and _topToBottom_ fields indicate the
direction of the text. The _horizontal_ field indicates whether the text
advances horizontally in its major direction (_horizontal_ = TRUE, the
default) or vertically in its major direction (_horizontal_ = FALSE).
The _leftToRight_ and _topToBottom_ fields indicate direction of text
advance in the major (characters within a single string) and minor
(successive strings) axes of layout. Which field is used for the major
direction and which is used for the minor direction is determined by the
_horizontal_ field.

For horizontal text (_horizontal_ = TRUE), characters on each line of
text advance in the positive X direction if _leftToRight_ is TRUE or in
the negative X direction if _leftToRight_ is FALSE. Characters are
advanced according to their natural advance width. Then each line of
characters is advanced in the negative Y direction if _topToBottom_ is
TRUE or in the positive Y direction if _topToBottom_ is FALSE. Lines are
advanced by the amount of _size_ * _spacing_.

For vertical text (_horizontal_ = FALSE), characters on each line of
text advance in the negative Y direction if _topToBottom_ is TRUE or in
the positive Y direction if _topToBottom_ is FALSE. Characters are
advanced according to their natural advance height. Then each line of
characters is advanced in the positive X direction if _leftToRight_ is
TRUE or in the negative X direction if _leftToRight_ is FALSE. Lines are
advanced by the amount of _size_ * _spacing_.

The _justify_ field determines alignment of the above text layout
relative to the origin of the object coordinate system. It is an
MFString which can contain 2 values. The first value specifies alignment
along the major axis and the second value specifies alignment along the
minor axis, as determined by the _horizontal_ field. A _justify_ value
of *""* is equivalent to the default value. If the second string, minor
alignment, is not specified then it defaults to the value *"FIRST"*.
Thus, _justify_ values of *""*, *"BEGIN"*, and *["BEGIN" "FIRST"]* are
equivalent.

The major alignment is along the X axis when _horizontal_ is TRUE and
along the Y axis when _horizontal is_ FALSE. The minor alignment is
along the Y axis when _horizontal_ is TRUE and along the X axis when
_horizontal is_ FALSE. The possible values for each enumerant of the
_justify_ field are *"FIRST"*, *"BEGIN"*, *"MIDDLE"*, and *"END"*. For
major alignment, each line of text is positioned individually according
to the major alignment enumerant. For minor alignment, the block of text
representing all lines together is positioned according to the minor
alignment enumerant. The following table describes the behavior in terms
of which portion of the text is at the origin:

===== Major Alignment, _horizontal_ = TRUE:

[cols=",,",]
|===
|*Enumerant* |*_leftToRight_ = TRUE* |*_leftToRight_ = FALSE*
| FIRST | Left edge of each line | Right edge of each line | BEGIN | Left edge of each line | Right edge of each line
| MIDDLE | Centered about X-axis | Centered about X-axis | END | Right edge of each line | Left edge of each line
|===

===== Major Alignment, _horizontal_ = FALSE:

[cols=",,",]
|===
|*Enumerant* |*_topToBottom_ = TRUE* |*_topToBottom_ = FALSE*
| FIRST | Top edge of each line | Bottom edge of each line | BEGIN | Top edge of each line | Bottom edge of each line
| MIDDLE | Centered about Y-axis | Center about Y-axis | END | Bottom edge of each line | Top edge of each line
|===

===== Minor Alignment, _horizontal_ = TRUE:

[cols=",,",]
|===
|*Enumerant* |*_topToBottom_ = TRUE* |*_topToBottom_ = FALSE*
| FIRST | Baseline of first line | Baseline of first line | BEGIN | Top edge of first line | Bottom edge of first line
| MIDDLE | Centered about Y-axis | Centered about Y-axis | END | Bottom edge of last line  | Top edge of last line
|===

===== Minor Alignment, _horizontal_ = FALSE:

[cols=",,",]
|===
|*Enumerant* |*_leftToRight_ = TRUE* |*_leftToRight_ = FALSE*
| FIRST | Left edge of first line | Right edge of first line | BEGIN | Left edge of first line | Right edge of first line
| MIDDLE | Centered about X-axis | Centered about X-axis | END | Right edge of last line | Left edge of last line
|===

The default minor alignment is *"FIRST"*. This is a special case of
minor alignment when _horizontal_ is TRUE. Text starts at the baseline
at the Y-axis. In all other cases, :*"FIRST"* is identical to *"BEGIN"*.
In the following tables, each color-coded cross-hair indicates where the
X and Y axes should be in relation to the text:

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/key.gif[]

*_horizontal_ [.underline]#= TRUE:#*

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/horizontal.gif[]


*_[.underline]#horizontal = FALSE:#_*

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/vertical.gif[]

The _language_ field specifies the context of the language for the text
string. Due to the multilingual nature of the ISO 10646-1:1993, the
_language_ field is needed to provide a proper language attribute of the
text string. The format is based on the POSIX locale specification as
well as the RFC 1766: language[_territory]. The values for the language
tag is based on the ISO 639, i.e. zh for Chinese, jp for Japanese, sc
for Swedish. The territory tag is based on the ISO 3166 country code,
i.e. TW is for Taiwan and CN for China for the "zh" Chinese language
tag. If the _language_ field is set to empty "", then local language
bindings are used.

Please refer to these sites for more details:

[source]
....
    http://www.chemie.fu-berlin.de/diverse/doc/ISO_639.html
    http://www.chemie.fu-berlin.de/diverse/doc/ISO_3166.html
....


[[Group]]
==== Group

[source]
....
Group {
  eventIn      MFNode  addChildren
  eventIn      MFNode  removeChildren
  exposedField MFNode  children       []
  field        SFVec3f bboxCenter     0 0 0
  field        SFVec3f bboxSize       -1 -1 -1
}
....

A Group node is equivalent to a Transform node, without the
transformation fields.

See the "<<GroupingNodes, Concepts - Grouping and Children Nodes>>" 
section for a description of the _children_, _addChildren_, and
_removeChildren_ fields and eventIns.

The _bboxCenter_ and _bboxSize_ fields specify a bounding box that
encloses the Group's children. This is a hint that may be used for
optimization purposes. If the specified bounding box is smaller than the
actual bounding box of the children at any time, then the results are
undefined. A default _bboxSize_ value, (-1 -1 -1), implies that the
bounding box is not specified and if needed must be calculated by the
browser. 
See "<<BoundingBoxes, Concepts - Bounding Boxes>>" for a
description of the _bboxCenter_ and _bboxSize_ fields.


[[ImageTexture]]
==== ImageTexture

[source]
....
ImageTexture {
  exposedField MFString url     []
  field        SFBool   repeatS TRUE
  field        SFBool   repeatT TRUE
}
....

The ImageTexture node defines a texture map by specifying an image file
and general parameters for mapping to geometry. Texture maps are defined
in a 2D coordinate system, (s, t), that ranges from 0.0 to 1.0 in both directions. The bottom edge of the image corresponds to the S-axis of
the texture map, and left edge of the image corresponds to the T-axis of
the texture map. The lower-left pixel of the image corresponds to s=0,
t=0, and the top-right pixel of the image corresponds to s=1, t=1.

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/ImageTexture.gif[]

The texture is read from the URL specified by the _url_ field. To turn
off texturing, set the _url_ field to have no values ([]). Browsers are
required to support the JPEG <<references_html, JPEG>> and
PNG
<<references_html, PNG>>
image file formats, and in addition, may support any other image
formats. Support for the GIF format
<<bibliography_html, GIF>> including transparent
backgrounds is also recommended. See the section ""

<<URLsAndURNs, Concepts - URLS and URNs>>" for details on
the _url_ field.

Texture images may be one component (greyscale), two component
(greyscale plus transparency), three component (full RGB color), or
four-component (full RGB color plus transparency). An ideal VRML
implementation will use the texture image to modify the diffuse color
and transparency of an object's material (specified in a
<<Material, Material>> node), then perform any lighting calculations
using the rest of the object's material properties with the modified
diffuse color to produce the final image. The texture image modifies the
diffuse color and transparency depending on how many components are in
the image, as follows:

. Diffuse color is multiplied by the greyscale values in the texture
image.
. Diffuse color is multiplied by the greyscale values in the texture
image; material transparency is multiplied by transparency values in
texture image.
. RGB colors in the texture image replace the material's diffuse color.
. RGB colors in the texture image replace the material's diffuse color;
transparency values in the texture image replace the material's
transparency.


See "<<Lighting, Concepts - Lighting Model>>" for details on
lighting equations and the interaction between textures, materials, and
geometries.

Browsers may approximate this ideal behavior to increase performance.
One common optimization is to calculate lighting only at each vertex and
combining the texture image with the color computed from lighting
(performing the texturing after lighting). Another common optimization
is to perform no lighting calculations at all when texturing is enabled,
displaying only the colors of the texture image.

The _repeatS_ and _repeatT_ fields specify how the texture wraps in the
S and T directions. If _repeatS_ is TRUE (the default), the texture map
is repeated outside the 0-to-1 texture coordinate range in the S
direction so that it fills the shape. If _repeatS_ is FALSE, the texture
coordinates are clamped in the S direction to lie within the 0-to-1
range. The _repeatT_ field is analogous to the _repeatS_ field.


[[IndexedFaceSet]]
==== IndexedFaceSet

[source]
....
IndexedFaceSet {
  eventIn       MFInt32 set_colorIndex
  eventIn       MFInt32 set_coordIndex
  eventIn       MFInt32 set_normalIndex
  eventIn       MFInt32 set_texCoordIndex
  exposedField  SFNode  color             NULL
  exposedField  SFNode  coord             NULL
  exposedField  SFNode  normal            NULL
  exposedField  SFNode  texCoord          NULL
  field         SFBool  ccw               TRUE
  field         MFInt32 colorIndex        []
  field         SFBool  colorPerVertex    TRUE
  field         SFBool  convex            TRUE
  field         MFInt32 coordIndex        []
  field         SFFloat creaseAngle       0
  field         MFInt32 normalIndex       []
  field         SFBool  normalPerVertex   TRUE
  field         SFBool  solid             TRUE
  field         MFInt32 texCoordIndex     []
}
....

The IndexedFaceSet node represents a 3D shape formed by constructing
faces (polygons) from vertices listed in the _coord_ field. The _coord_
field must contain a Coordinate node. IndexedFaceSet uses the indices in
its _coordIndex_ field to specify the polygonal faces. An index of -1
indicates that the current face has ended and the next one begins. The
last face may (but does not have to be) followed by a -1. If the
greatest index in the _coordIndex_ field is N, then the Coordinate node
must contain N+1 coordinates (indexed as 0-N). IndexedFaceSet is
specified in the local coordinate system and is affected by parent
transformations.

For descriptions of the _coord_, _normal_, and _texCoord_ fields, see
the <<Coordinate, Coordinate>>, <<Normal, Normal>>, and
<<TextureCoordinate, TextureCoordinate>> nodes.


See "<<LightsAndLighting, Concepts - Lighting Model>>" for
details on lighting equations and the interaction between textures,
materials, and geometries.

If the color field is not NULL then it must contain a Color node, whose
colors are applied to the vertices or faces of the IndexedFaceSet as
follows:

* If _colorPerVertex_ is FALSE, colors are applied to each face, as
follows:
** If the _colorIndex_ field is not empty, then they are used to choose
one color for each face of the IndexedFaceSet. There must be at least as
many indices in the _colorIndex_ field as there are faces in the
IndexedFaceSet. If the greatest index in the _colorIndex_ field is N,
then there must be N+1 colors in the Color node. The _colorIndex_ field
must not contain any negative entries.
** If the _colorIndex_ field is empty, then the colors are applied to
each face of the IndexedFaceSet in order. There must be at least as many
colors in the Color node as there are faces.
* If _colorPerVertex_ is TRUE, colors are applied to each vertex, as
follows:
** If the _colorIndex_ field is not empty, then it is used to choose
colors for each vertex of the IndexedFaceSet in exactly the same manner
that the _coordIndex_ field is used to choose coordinates for each
vertex from the Coordinate node. The _colorIndex_ field must contain at
least as many indices as the _coordIndex_ field, and must contain
end-of-face markers (-1) in exactly the same places as the _coordIndex_
field. If the greatest index in the _colorIndex_ field is N, then there
must be N+1 colors in the Color node.
** If the _colorIndex_ field is empty, then the _coordIndex_ field is
used to choose colors from the Color node. If the greatest index in the
_coordIndex_ field is N, then there must be N+1 colors in the Color
node.

If the _normal_ field is NULL, then the browser should automatically
generate normals, using _creaseAngle_ to determine if and how normals
are smoothed across shared vertices.

If the _normal_ field is not NULL, then it must contain a Normal node,
whose normals are applied to the vertices or faces of the IndexedFaceSet
in a manner exactly equivalent to that described above for applying
colors to vertices/faces.

If the _texCoord_ field is not NULL, then it must contain a
<<TextureCoordinate, TextureCoordinate>> node. The texture coordinates
in that node are applied to the vertices of the IndexedFaceSet as
follows:

* If the _texCoordIndex_ field is not empty, then it is used to choose
texture coordinates for each vertex of the IndexedFaceSet in exactly the
same manner that the _coordIndex_ field is used to choose coordinates
for each vertex from the Coordinate node. The _texCoordIndex_ field must
contain at least as many indices as the _coordIndex_ field, and must
contain end-of-face markers (-1) in exactly the same places as the
_coordIndex_ field. If the greatest index in the _texCoordIndex_ field
is N, then there must be N+1 texture coordinates in the
TextureCoordinate node.
* If the _texCoordIndex_ field is empty, then the _coordIndex_ array is
used to choose texture coordinates from the TextureCoordinate node. If
the greatest index in the _coordIndex_ field is N, then there must be
N+1 texture coordinates in the TextureCoordinate node.

If the _texCoord_ field is NULL, a default texture coordinate mapping is
calculated using the bounding box of the shape. The longest dimension of
the bounding box defines the S coordinates, and the next longest defines
the T coordinates. If two or all three dimensions of the bounding box
are equal, then ties should be broken by choosing the X, Y, or Z
dimension in that order of preference. The value of the S coordinate
ranges from 0 to 1, from one end of the bounding box to the other. The T
coordinate ranges between 0 and the ratio of the second greatest
dimension of the bounding box to the greatest dimension. See the figure
below for an illustration of default texture coordinates for a simple
box shaped IndexedFaceSet with a bounding box with X dimension twice as
large as the Z dimension which is twice as large as the Y dimension:

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/IFStexture.gif[]

See the introductory "<<GeometryNodes, Concepts - Geometry>>" section for a
description of the _ccw_, _solid_, _convex_, and _creaseAngle_ fields.


[[IndexedLineSet]]
==== IndexedLineSet

[source]
....
IndexedLineSet {
  eventIn       MFInt32 set_colorIndex
  eventIn       MFInt32 set_coordIndex
  exposedField  SFNode  color             NULL
  exposedField  SFNode  coord             NULL
  field         MFInt32 colorIndex        []
  field         SFBool  colorPerVertex    TRUE
  field         MFInt32 coordIndex        []
}
....

The IndexedLineSet node represents a 3D geometry formed by constructing
polylines from 3D points specified in the _coord_ field. IndexedLineSet
uses the indices in its _coordIndex_ field to specify the polylines by
connecting together points from the _coord_ field. An index of -1
indicates that the current polyline has ended and the next one begins.
The last polyline may (but does not have to be) followed by a -1.
IndexedLineSet is specified in the local coordinate system and is
affected by parent transformations.

The _coord_ field specifies the 3D vertices of the line set and is
specified by a <<Coordinate, Coordinate>> node.

Lines are not lit, not texture-mapped, or not collided with during
collision detection.

If the _color_ field is not NULL, it must contain a Color node, and the
colors are applied to the line(s) as follows:

* If _colorPerVertex_ is FALSE:
** If the _colorIndex_ field is not empty, then one color is used for
each polyline of the IndexedLineSet. There must be at least as many
indices in the _colorIndex_ field as there are polylines in the
IndexedLineSet. If the greatest index in the _colorIndex_ field is N,
then there must be N+1 colors in the Color node. The _colorIndex_ field
must not contain any negative entries.
** If the _colorIndex_ field is empty, then the colors are applied to
each polyline of the IndexedLineSet in order. There must be at least as
many colors in the Color node as there are polylines.
* If _colorPerVertex_ is TRUE:
** If the _colorIndex_ field is not empty, then colors are applied to
each vertex of the IndexedLineSet in exactly the same manner that the
_coordIndex_ field is used to supply coordinates for each vertex from
the Coordinate node. The _colorIndex_ field must contain at least as
many indices as the _coordIndex_ field and must contain end-of-polyline
markers (-1) in exactly the same places as the _coordIndex_ field. If
the greatest index in the _colorIndex_ field is N, then there must be
N+1 colors in the Color node.
** If the _colorIndex_ field is empty, then the _coordIndex_ field is
used to choose colors from the Color node. If the greatest index in the
_coordIndex_ field is N, then there must be N+1 colors in the Color
node.

If the _color_ field is NULL and there is a Material defined for the
Appearance affecting this IndexedLineSet, then use the _emissiveColor_
of the Material to draw the lines. 
See "<<Lighting, Concepts - Lighting Model, Lighting Off>>" 
for details on lighting equations.


[[Inline]]
==== Inline

[source]
....
Inline {
  exposedField MFString url        []
  field        SFVec3f  bboxCenter 0 0 0
  field        SFVec3f  bboxSize   -1 -1 -1
}
....

The Inline node is a grouping node that reads its children data from a
location in the World Wide Web. Exactly when its children are read and
displayed is not defined; reading the children may be delayed until the
Inline is actually visible to the viewer. The _url_ field specifies the
URL containing the children. An Inline with an empty URL does nothing.

An Inline's URLs shall refer to a valid VRML file that contains a list
of children nodes at the top level. 
See "<<GroupingNodes, Concepts - Grouping and Children Nodes>>". 
The results are undefined if the URL refers to a file that is
not VRML or if the file contains non-children nodes at the top level.

If multiple URLs are specified, the browser may display a URL of a lower
preference file while it is obtaining, or if it is unable to obtain the
higher preference file. 
See "<<URLsAndURNs, Concepts - URLS and URNs>>" for details on
the _url_ field and preference order.

The _bboxCenter_ and _bboxSize_ fields specify a bounding box that
encloses the Inlines's children. This is a hint that may be used for
optimization purposes. If the specified bounding box is smaller than the
actual bounding box of the children at any time, then the results are
undefined. A default _bboxSize_ value, (-1 -1 -1), implies that the
bounding box is not specified and if needed must be calculated by the
browser. 
See "<<BoundingBoxes, Concepts - Bounding Boxes>>" for a
description of the _bboxCenter_ and _bboxSize_ fields.


[[LOD]]
==== LOD

[source]
....
LOD {
  exposedField MFNode  level    [] 
  field        SFVec3f center   0 0 0
  field        MFFloat range    [] 
}
....

The LOD node specifies various levels of detail or complexity for a
given object, and provides hints for browsers to automatically choose
the appropriate version of the object based on the distance from the
user. The _level_ field contains a list of nodes that represent the same
object or objects at varying levels of detail, from highest to the
lowest level of detail, and the _range_ field specifies the ideal
distances at which to switch between the levels. See the "<<GroupingNodes, Concepts - Grouping and Children Nodes>>" 
section for a details on the types of nodes that are legal
values for _level_.

The _center_ field is a translation offset in the local coordinate
system that specifies the center of the LOD object for distance
calculations. In order to calculate which level to display, first the
distance is calculated from the viewpoint, transformed into the local
coordinate space of the LOD node, (including any scaling
transformations), to the _center_ point of the LOD. If the distance is
less than the first value in the _range_ field, then the first level of
the LOD is drawn. If between the first and second values in the _range_
field, the second level is drawn, and so on.

If there are N values in the _range_ field, the LOD shall have N+1 nodes
in its _level_ field. Specifying too few levels will result in the last
level being used repeatedly for the lowest levels of detail; if more
levels than ranges are specified, the extra levels will be ignored. The
exception to this rule is to leave the range field empty, which is a
hint to the browser that it should choose a level automatically to
maintain a constant display rate. Each value in the _range_ field should
be greater than the previous value; otherwise results are undefined.

Authors should set LOD ranges so that the transitions from one level of
detail to the next are smooth. Browsers may adjust which level of detail
is displayed to maintain interactive frame rates, to display an
already-fetched level of detail while a higher level of detail
(contained in an Inline node) is fetched, or might disregard the
author-specified ranges for any other implementation-dependent reason.
For best results, specify ranges only where necessary, and nest LOD
nodes with and without ranges. Browsers should try to honor the hints
given by authors, and authors should try to give browsers as much
freedom as they can to choose levels of detail based on performance.

LOD nodes are evaluated top-down in the scene graph. Only the
descendants of the currently selected level are rendered. Note that all
nodes under an LOD node continue to receive and send events (i.e.
routes) regardless of which LOD _level_ is active. For example, if an
active TimeSensor is contained within an inactive level of an LOD, the
TimeSensor sends events regardless of the LOD's state.


[[Material]]
==== Material

[source]
....
Material {
  exposedField SFFloat ambientIntensity  0.2
  exposedField SFColor diffuseColor      0.8 0.8 0.8
  exposedField SFColor emissiveColor     0 0 0
  exposedField SFFloat shininess         0.2
  exposedField SFColor specularColor     0 0 0
  exposedField SFFloat transparency      0
}
....

The Material node specifies surface material properties for associated
geometry nodes and are used by the VRML lighting equations during
rendering. 
See "<<Lighting, Concepts - Lighting Model>>" for a detailed
description of the VRML lighting model equations.

All of the fields in the Material node range from 0.0 to 1.0.

The fields in the Material node determine the way light reflects off an
object to create color:

* The _diffuseColor_ reflects all VRML light sources depending on the
angle of the surface with respect to the light source. The more directly
the surface faces the light, the more diffuse light reflects.
* The _ambientIntensity_ field specifies how much ambient light from
light sources this surface should reflect. Ambient light is
omni-directional and depends only on the number of light sources, not
their positions with respect to the surface. Ambient color is calculated
as _ambientIntensity_ * _diffuseColor_.
* The _specularColor_ and _shininess_ determine the specular
highlights--for example, the shiny spots on an apple. When the angle
from the light to the surface is close to the angle from the surface to
the viewer, the _specularColor_ is added to the diffuse and ambient
color calculations. Lower shininess values produce soft glows, while
higher values result in sharper, smaller highlights.
* Emissive color models "glowing" objects. This can be useful for
displaying radiosity-based models (where the light energy of the room is
computed explicitly), or for displaying scientific data.
* Transparency is how "clear" the object is, with 1.0 being completely
transparent, and 0.0 completely opaque.

'''''

[This section belong in the Conformance annex.]

For rendering systems that do not support the full OpenGL lighting
model, the following simpler lighting model is recommended:

A transparency value of 0 is completely opaque, a value of 1 is
completely transparent. Browsers need not support partial transparency,
but should support at least fully transparent and fully opaque surfaces,
treating transparency values >= 0.5 as fully transparent.

_Issues for Low-End Rendering Systems._ Many low-end PC rendering
systems are not able to support the full range of the VRML material
specification. For example, many systems do not render individual red,
green and blue reflected values as specified in the _specularColor_
field. The following table describes which Material fields are typically
supported in popular low-end systems and suggests actions for browser
implementors to take when a field is not supported.

[source]
....
Field           Supported?    Suggested Action

ambientIntensity No           Ignore
diffuseColor     Yes          Use
specularColor    No           Ignore
emissiveColor    No           If diffuse == 0.8 0.8 0.8, use emissive
shininess        Yes          Use
transparency     Yes          if < 0.5 then opaque else transparent
....

The emissive color field is used when all other colors are black 
(0 0 0). Rendering systems which do not support specular color may
nevertheless support a specular intensity. This should be derived by
taking the dot product of the specified RGB specular value with the
vector [.32 .57 .11]. This adjusts the color value to compensate for the
variable sensitivity of the eye to colors.

Likewise, if a system supports ambient intensity but not color, the same
thing should be done with the ambient color values to generate the
ambient intensity. If a rendering system does not support per-object
ambient values, it should set the ambient value for the entire scene at
the average ambient value of all objects.

It is also expected that simpler rendering systems may be unable to
support both diffuse and emissive objects in the same world. Also, many
renderers will not support _ambientIntensity_ with per-vertex colors
specified with the Color node.


[[MovieTexture]]
==== MovieTexture

[source]
....
MovieTexture {
  exposedField SFBool   loop             FALSE
  exposedField SFFloat  speed            1
  exposedField SFTime   startTime        0
  exposedField SFTime   stopTime         0
  exposedField MFString url              []
  field        SFBool   repeatS          TRUE
  field        SFBool   repeatT          TRUE
  eventOut     SFFloat  duration_changed
  eventOut     SFBool   isActive
}
....

The MovieTexture node defines a time dependent texture map (contained in
a movie file) and parameters for controlling the movie and the texture
mapping. A MovieTexture can also be used as the source of sound data for
a <<Sound, Sound>> node, but in this special case are not used for
rendering.

Texture maps are defined in a 2D coordinate system, (s, t), that ranges from 0.0 to 1.0 in both directions. The bottom edge of the image
corresponds to the S-axis of the texture map, and left edge of the image
corresponds to the T-axis of the texture map. The lower-left pixel of
the image corresponds to s=0, t=0, and the top-right pixel of the image
corresponds to s=1, t=1.

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/ImageTexture.gif[]

The _url_ field that defines the movie data must support MPEG1-Systems
(audio and video) or MPEG1-Video (video-only) movie file formats
<<references_html, MPEG>>. 
See "<<URLsAndURNs, Concepts - URLS and URNs>>" for details on
the _url_ field. It is recommended that implementations support
greyscale or alpha transparency rendering if the specific movie format
being used supports these features.


See "<<Lighting, Concepts - Lighting Model>>" for details on
lighting equations and the interaction between textures, materials, and
geometries.

As soon as the movie is loaded, a _duration_changed_ eventOut is sent.
This indicates the duration of the movie, in seconds. This eventOut
value can be read (for instance, by a Script) to determine the duration
of a movie. A value of -1 implies the movie has not yet loaded or the
value is unavailable for some reason.

The _loop, startTime,_ and _stopTime_ exposedFields and the _isActive_
eventOut, and their affects on the MovieTexture node, are discussed in
detail in the "<<TimeDep, Concepts - Time Dependent Nodes>>" section.
The "_cycle"_ of a MovieTexture is the length of time in seconds for one
playing of the movie at the specified _speed_.

If a MovieTexture is inactive when the movie is first loaded, then frame
0 is shown in the texture if _speed_ is non-negative, or the last frame
of the movie if _speed_ is negative. A MovieTexture will always display
frame 0 if _speed_ = 0. For positive values of _speed_, the frame an
active MovieTexture will display at time `now` corresponds to the
frame at movie time (i.e., in the movie's local time system with frame 0
at time 0, at _speed_ = 1):

[source]
....
        fmod (now - startTime, duration/speed)
....

If _speed_ is negative, then the frame to display is the frame at movie
time:

[source]
....
        duration + fmod(now - startTime, duration/speed).
....

When a MovieTexture becomes inactive, the frame corresponding to the
time at which the MovieTexture became inactive will remain as the
texture.

The _speed_ exposedField indicates how fast the movie should be played.
A _speed_ of 2 indicates the movie plays twice as fast. Note that the
_duration_changed_ output is not affected by the _speed_ exposedField.
_set_speed_ events are ignored while the movie is playing. A negative
_speed_ implies that the movie will play backwards. However, content
creators should note that this may not work for streaming movies or very
large movie files.

MovieTextures can be referenced by an Appearance node's texture field
(as a movie texture) and by a Sound node's source field (as an audio
source only). A legal implementation of the MovieTexture node is not
required to play audio if _speed_ is not equal to 1.


=== NavigationInfo

[source]
....
NavigationInfo {
  eventIn      SFBool   set_bind
  exposedField MFFloat  avatarSize       [ 0.25, 1.6, 0.75 ]
  exposedField SFBool   headlight        TRUE
  exposedField SFFloat  speed            1.0 
  exposedField MFString type             "WALK" 
  exposedField SFFloat  visibilityLimit  0.0 
  eventOut     SFBool   isBound
}
....

The NavigationInfo node contains information describing the physical
characteristics of the viewer and viewing model. NavigationInfo is a
bindable node (see "_link:#BindableLeafNodes[Concepts - Bindable
Children Nodes]")_ and thus there exists a NavigationInfo stack in the
browser in which the top-most NavigationInfo on the stack is the
currently active NavigationInfo. The current NavigationInfo is
considered to be a child of the current Viewpoint - regardless of where
it is initially located in the file. Whenever the current Viewpoint
changes, the current NavigationInfo must be re-parented to it. Whenever
the current NavigationInfo changes, the new NavigationInfo must be
re-parented to the current Viewpoint.

If a TRUE value is sent to the _set_bind_ eventIn of a NavigationInfo,
it is pushed onto the NavigationInfo stack and activated. When a
NavigationInfo is bound, the browser uses the fields of the
NavigationInfo to set the navigation controls of its user interface and
the NavigationInfo is conceptually re-parented under the currently bound
Viewpoint. All subsequent scaling changes to the current
<<Viewpoint, Viewpoint>>'s coordinate system automatically change
aspects (see below) of the NavigationInfo values used in the browser
(e.g. scale changes to any parent transformation). A FALSE value of
_set_bind_, pops the NavigationInfo from the stack, results in an
_isBound_ FALSE event, and pops to the next entry in the stack which
must be re-parented to the current Viewpoint. 
See "<<BindableLeafNodes, Concepts - Bindable Children Nodes>>" 
for more details on the the binding stacks.

The _type_ field specifies a navigation paradigm to use. Minimally,
browsers shall support the following navigation types: "WALK",
"EXAMINE", "FLY", and "NONE". Walk navigation is used for exploring a
virtual world. It is recommended that the browser should support a
notion of gravity in walk mode. Fly navigation is similar to walk except
that no notion of gravity should be enforced. There should still be some
notion of "up" however. Examine navigation is typically used to view
individual objects and often includes (but does not require) the ability
to spin the object and move it closer or further away. The "none" choice
removes all navigation controls - the user navigates using only controls
provided in the scene, such as guided tours. Also allowed are browser
specific navigation types. These should include a unique suffix (e.g.
_sgi.com) to prevent conflicts. The _type_ field is multi-valued so that
authors can specify fallbacks in case a browser does not understand a
given type. If none of the types are recognized by the browser, then the
default "WALK" is used. These strings values are case sensitive ("walk"
is not equal to "WALK").

The _speed_ is the rate at which the viewer travels through a scene in
meters per second. Since viewers may provide mechanisms to travel faster
or slower, this should be the default or average speed of the viewer. If
the NavigationInfo _type_ is EXAMINE, _speed_ should affect panning and
dollying--it should have no effect on the rotation speed. The
transformation hierarchy of the currently bound
<<Viewpoint, Viewpoint>> (see above) scales the _speed_ - translations
and rotations have no effect on _speed_. Speed must be >= 0.0 - where
0.0 specifies a stationary avatar.

The _avatarSize_ field specifies the user's physical dimensions in the
world for the purpose of collision detection and terrain following. It
is a multi-value field to allow several dimensions to be specified. The
first value should be the allowable distance between the user's position
and any collision geometry (as specified by
<<CollideStyle, Collision>>) before a collision is detected. The
second should be the height above the terrain the viewer should be
maintained. The third should be the height of the tallest object over
which the viewer can "step". This allows staircases to be built with
dimensions that can be ascended by all browsers. Additional values are
browser dependent and all values may be ignored, but if a browser
interprets these values the first 3 should be interpreted as described
above. The transformation hierarchy of the currently bound
<<Viewpoint, Viewpoint>> scales the _avatarSize_ - translations and
rotations have no effect on _avatarSize_.

For purposes of terrain following the browser needs a notion of the
_down_ direction (down vector), since gravity is applied in the
direction of the down vector. This down vector should be along the
negative Y-axis in the local coordinate system of the currently bound
Viewpoint (i.e., the accumulation of the Viewpoint's parent
transformations, not including the Viewpoint's orientation field).

The _visibilityLimit_ field sets the furthest distance the user is able
to see. The browser may clip all objects beyond this limit, fade them
into the background or ignore this field. A value of 0.0 (the default)
indicates an infinite visibility limit. _VisibilityLimit_ is restricted
to be >= 0.0.

The _speed_, _avatarSize_ and _visibilityLimit_ values are all scaled by
the transformation being applied to currently bound
<<Viewpoint, Viewpoint>>. If there is no currently bound Viewpoint,
they are interpreted in the world coordinate system. This allows these
values to be automatically adjusted when binding to a Viewpoint that has
a scaling transformation applied to it without requiring a new
NavigationInfo node to be bound as well. If the scale applied to the
Viewpoint is non-uniform the behavior is undefined.

The _headlight_ field specifies whether a browser should turn a
headlight on. A headlight is a directional light that always points in
the direction the user is looking. Setting this field to TRUE allows the
browser to provide a headlight, possibly with user interface controls to
turn it on and off. Scenes that enlist pre-computed lighting (e.g.
radiosity solutions) can turn the headlight off. The headlight shall
have _intensity_ = 1, _color_ = 1 1 1, _ambientIntensity_ = 0.0, and
_direction_ = 0 0 -1.

It is recommended that the near clipping plane should be set to one-half
of the collision radius as specified in the _avatarSize_ field. This
recommendation may be ignored by the browser, but setting the near plane
to this value prevents excessive clipping of objects just above the
collision volume and provides a region inside the collision volume for
content authors to include geometry that should remain fixed relative to
the viewer, such as icons or a heads-up display, but that should not be
occluded by geometry outside of the collision volume.

The first NavigationInfo node found during reading of the world is
automatically bound (receives a _set_bind_ TRUE event) and supplies the
initial navigation parameters.


[[Normal]]
==== Normal

[source]
....
Normal {
  exposedField MFVec3f vector  []
}
....

This node defines a set of 3D surface normal vectors to be used in the
_vector_ field of some geometry nodes (IndexedFaceSet, ElevationGrid).
This node contains one multiple-valued field that contains the normal
vectors. Normals should be unit-length or results are undefined.

To save network bandwidth, it is expected that implementations will be
able to automatically generate appropriate normals if none are given.
However, the results will vary from implementation to implementation.


[[NormalInterpolator]]
==== NormalInterpolator

[source]
....
NormalInterpolator {
  eventIn      SFFloat set_fraction
  exposedField MFFloat key           []
  exposedField MFVec3f keyValue      []
  eventOut     MFVec3f value_changed
}
....

This node interpolates among a set of multi-valued Vec3f values,
suitable for transforming normal vectors. All output vectors will have
been normalized by the interpolator.

The number of normals in the _keyValue_ field must be an integer
multiple of the number of keyframes in the _key_ field; that integer
multiple defines how many normals will be contained in the
_value_changed_ events.

Normal interpolation is to be performed on the surface of the unit
sphere. That is, the output values for a linear interpolation from a
point P on the unit sphere to a point Q also on unit sphere should lie
along the shortest arc (on the unit sphere) connecting points P and Q.
Also, equally spaced input fractions will result in arcs of equal
length. Cases where P and Q are diagonally opposing allow an infinite
number of arcs. The interpolation for this case can be along any one of
these arcs.

Refer to "<<InterpolatorNodes, Concepts - Interpolators>>" for a
more detailed discussion of interpolators.


[[OrientationInterpolator]]
==== OrientationInterpolator

[source]
....
OrientationInterpolator {
  eventIn      SFFloat    set_fraction
  exposedField MFFloat    key           []
  exposedField MFRotation keyValue         []
  eventOut     SFRotation value_changed
}
....

This node interpolates among a set of SFRotation values. The rotations
are absolute in object space and are, therefore, not cumulative. The
_keyValue_ field must contain exactly as many rotations as there are
keyframes in the _key_ field, or an error will be generated and results
will be undefined.

An orientation represents the final position of an object after a
rotation has been applied. An OrientationInterpolator will interpolate
between two orientations by computing the shortest path on the unit
sphere between the two orientations. The interpolation will be linear in
arc length along this path. The path between two diagonally opposed
orientations will be any one of the infinite possible paths with arc
length PI.

If two consecutive keyValue values exist such that the arc length
between them is greater than PI, then the interpolation will take place
on the arc complement. For example, the interpolation between the
orientations:

[source]
....
    0 1 0 0 --> 0 1 0 5.0
....

is equivalent to the rotation between the two orientations:

[source]
....
    0 1 0 2*PI --> 0 1 0 5.0 
....

Refer to "<<InterpolatorNodes, Concepts - Interpolators>>" for a
more detailed discussion of interpolators.


[[PixelTexture]]
==== PixelTexture

[source]
....
PixelTexture {
  exposedField SFImage  image      0 0 0
  field        SFBool   repeatS    TRUE
  field        SFBool   repeatT    TRUE
}
....

The PixelTexture node defines a 2D image-based texture map as an
explicit array of pixel values and parameters controlling tiling
repetition of the texture onto geometry.

Texture maps are defined in a 2D coordinate system, (s, t), that ranges from 0.0 to 1.0 in both directions. The bottom edge of the pixel image
corresponds to the S-axis of the texture map, and left edge of the pixel
image corresponds to the T-axis of the texture map. The lower-left pixel
of the pixel image corresponds to s=0, t=0, and the top-right pixel of
the image corresponds to s=1, t=1.

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/ImageTexture.gif[]

Images may be one component (greyscale), two component (greyscale plus
alpha opacity), three component (full RGB color), or four-component
(full RGB color plus alpha opacity). An ideal VRML implementation will
use the texture image to modify the diffuse color and transparency ( = 1
- alpha opacity) of an object's material (specified in a
<<Material, Material>> node), then perform any lighting calculations
using the rest of the object's material properties with the modified
diffuse color to produce the final image. The texture image modifies the
diffuse color and transparency depending on how many components are in
the image, as follows:

. Diffuse color is multiplied by the greyscale values in the texture
image.
. Diffuse color is multiplied by the greyscale values in the texture
image; material transparency is multiplied by transparency values in
texture image.
. RGB colors in the texture image replace the material's diffuse color.
. RGB colors in the texture image replace the material's diffuse color;
transparency values in the texture image replace the material's
transparency.

Browsers may approximate this ideal behavior to increase performance.
One common optimization is to calculate lighting only at each vertex and
combining the texture image with the color computed from lighting
(performing the texturing after lighting). Another common optimization
is to perform no lighting calculations at all when texturing is enabled,
displaying only the colors of the texture image.


See "<<Lighting, Concepts - Lighting Model>>" for details on
the VRML lighting equations.

See the "<<SFImage, Field Reference - SFImage>>" specification
for details on how to specify an image.

The _repeatS_ and _repeatT_ fields specify how the texture wraps in the
S and T directions. If _repeatS_ is TRUE (the default), the texture map
is repeated outside the 0-to-1 texture coordinate range in the S
direction so that it fills the shape. If _repeatS_ is FALSE, the texture
coordinates are clamped in the S direction to lie within the 0-to-1
range. The _repeatT_ field is analogous to the _repeatS_ field.


[[PlaneSensor]]
==== PlaneSensor

[source]
....
PlaneSensor {
  exposedField SFBool  autoOffset          TRUE
  exposedField SFBool  enabled             TRUE
  exposedField SFVec2f maxPosition         -1 -1
  exposedField SFVec2f minPosition         0 0
  exposedField SFVec3f offset              0 0 0
  eventOut     SFBool  isActive
  eventOut     SFVec3f trackPoint_changed
  eventOut     SFVec3f translation_changed
}
....

The PlaneSensor maps pointing device (e.g. mouse or wand) motion into
translation in two dimensions, in the XY plane of its local space.
PlaneSensor uses the descendant geometry of its parent node to determine
if a hit occurs.

The _enabled_ exposed field enables and disables the PlaneSensor - if
TRUE, the sensor reacts appropriately to user events, if FALSE, the
sensor does not track user input or send output events. If _enabled_
receives a FALSE event and _isActive_ is TRUE, the sensor becomes
disabled and deactivated, and outputs an _isActive_ FALSE event. If
_enabled_ receives a TRUE event the sensor is enabled and ready for user
activation.

The PlaneSensor generates events if the pointing device is activated
while over any descendant geometry nodes of its parent group and then
moved while activated. Typically, the pointing device is a 2D device
such as a mouse. The pointing device is considered to be moving within a
plane at a fixed distance from the viewer and perpendicular to the line
of sight; this establishes a set of 3D coordinates for the pointer. If a
3D pointer is in use, then the sensor generates events only when the
pointer is within the user's field of view. In either case, the pointing
device is considered to "pass over" geometry when that geometry is
intersected by a line extending from the viewer and passing through the
pointer's 3D coordinates. If multiple sensors' geometry intersect this
line (hereafter called the bearing), only the nearest will be eligible
to generate events.

Upon activation of the pointing device (e.g. mouse button down) over the
sensor's geometry, an _isActive_ TRUE event is sent. Dragging motion is
mapped into a relative translation in the XY plane of the sensor's local
coordinate system as it was defined at the time of activation. For each
subsequent position of the bearing, a _translation_changed_ event is
output which corresponds to a relative translation from the original
intersection point projected onto the XY plane, plus the _offset_ value.
The sign of the translation is defined by the XY plane of the sensor's
coordinate system. _trackPoint_changed_ events reflect the unclamped
drag position on the surface of this plane. When the pointing device is
deactivated and _autoOffset_ is TRUE, _offset_ is set to the last
translation value and an _offset_changed_ event is generated. 
See "<<DragSensors, Concepts - Drag Sensors>>" for more
details.

When the sensor generates an _isActive_ TRUE event, it grabs all further
motion events from the pointing device until it releases and generates
an _isActive_ FALSE event (other pointing device sensors
cannot generate events during this time). Motion of the
pointing device while _isActive_ is TRUE is referred to as a "drag". If
a 2D pointing device is in use, _isActive_ events will typically reflect
the state of the primary button associated with the device (i.e.
_isActive_ is TRUE when the primary button is pressed, and FALSE when
not released). If a 3D pointing device (e.g. wand) is in use, _isActive_
events will typically reflect whether the pointer is within or in
contact with the sensor's geometry.

_minPosition_ and _maxPosition_ may be set to clamp _translation_ events
to a range of values as measured from the origin of the XY plane. If the
X or Y component of _minPosition_ is greater than the corresponding
component of _maxPosition_, _translation_changed_ events are not clamped
in that dimension. If the X or Y component of _minPosition_ is equal to
the corresponding component of _maxPosition_, that component is
constrained to the given value; this technique provides a way to
implement a line sensor that maps dragging motion into a translation in
one dimension.

While the pointing device is activated, _trackPoint_changed_ and
_translation_changed_ events are output. _trackPoint_changed_ events
represent the unclamped intersection points on the surface of the local
XY plane. If the pointing device is dragged off of the XY plane while
activated (e.g. above horizon line), browsers may interpret this in
several ways (e.g. clamp all values to the horizon). Each movement of
the pointing device, while _isActive_ is TRUE, generates
_trackPoint_changed_ and _translation_changed_ events.


See "<<PointingDeviceSensors, Concepts - Pointing Device Sensors and Drag Sensors>>" 
for more details.


[[PointLight]]
==== PointLight

[source]
....
PointLight {
  exposedField SFFloat ambientIntensity  0 
  exposedField SFVec3f attenuation       1 0 0
  exposedField SFColor color             1 1 1 
  exposedField SFFloat intensity         1
  exposedField SFVec3f location          0 0 0
  exposedField SFBool  on                TRUE 
  exposedField SFFloat radius            100
}
....

The PointLight node specifies a point light source at 3D location in the
local coordinate system. A point source emits light equally in all
directions; that is, it is omni-directional. PointLights are specified
in their local coordinate system and are affected by parent
transformations.


See "<<Lights, Concepts - Light Sources>>" for a detailed
description of the _ambientIntensity_, _color_, and _intensity_ fields.

A PointLight may illuminate geometry within _radius_ (>= 0.0) meters of
its _location_. Both radius and location are affected by parent
transformations (scale _radius_ and transform _location_).

A PointLight's illumination falls off with distance as specified by
three _attenuation_ coefficients. The attenuation factor is
_1/(attenuation[0] + attenuation[1]*r + attenuation[2]*r^2)_, where _r_
is the distance of the light to the surface being illuminated. The
default is no attenuation. An _attenuation_ value of *0 0 0* is
identical to *1 0 0*. Attenuation values must be >= 0.0. Renderers that
do not support a full attenuation model may approximate as necessary.

See "<<Lighting, Concepts - Lighting Model>>" for a detailed
description of VRML's lighting equations.


[[PointSet]]
==== PointSet

[source]
....
PointSet {
  exposedField  SFNode  color      NULL
  exposedField  SFNode  coord      NULL
}
....

The PointSet node specifies a set of 3D points in the local coordinate
system with associated colors at each point. The _coord_ field specifies
a <<Coordinate, Coordinate>> node (or instance of a Coordinate node) -
results are undefined if the _coord_ field specifies any other type of
node. PointSet uses the coordinates in order. If the _coord_ field is
NULL, then the PointSet is empty.

PointSets are not lit, not texture-mapped, or collided with during
collision detection.

If the _color_ field is not NULL, it must specify a <<Color, Color>>
node that contains at least the number of points contained in the
_coord_ node - results are undefined if the _color_ field specifies any
other type of node. Colors shall be applied to each point in order. The
results are undefined if the number of values in the Color node is less
than the number of values specified in the Coordinate node

If the _color_ field is NULL and there is a Material defined for the
Appearance affecting this PointSet, then use the _emissiveColor_ of the
Material to draw the points. 
See "<<Lighting, Concepts - Lighting Model, Lighting Off>>" 
for details on lighting equations.


[[PositionInterpolator]]
==== PositionInterpolator

[source]
....
PositionInterpolator {
  eventIn      SFFloat set_fraction
  exposedField MFFloat key           []
  exposedField MFVec3f keyValue      []
  eventOut     SFVec3f value_changed
}
....

This node linearly interpolates among a set of SFVec3f values. This is
appropriate for interpolating a translation. The vectors are interpreted
as absolute positions in object space. The _keyValue_ field must contain
exactly as many values as in the _key_ field.

Refer to "<<InterpolatorNodes, Concepts - Interpolators>>" for a
more detailed discussion of interpolators.


[[ProximitySensor]]
==== ProximitySensor

[source]
....
ProximitySensor {
  exposedField SFVec3f    center      0 0 0
  exposedField SFVec3f    size        0 0 0
  exposedField SFBool     enabled     TRUE
  eventOut     SFBool     isActive
  eventOut     SFVec3f    position_changed
  eventOut     SFRotation orientation_changed
  eventOut     SFTime     enterTime
  eventOut     SFTime     exitTime
}
....

The ProximitySensor generate events when the user enters, exits, and
moves within a region in space (defined by a box). A proximity sensor
can be enabled or disabled by sending it an _enabled_ event with a value
of TRUE or FALSE - a disabled sensor does not send output events.

A ProximitySensor generates _isActive_ TRUE/FALSE events as the viewer
enters and exits the rectangular box defined by its _center_ and _size_
fields. Browsers shall interpolate user positions and timestamp the
_isActive_ events with the exact time the user first intersected the
proximity region. The _center_ field defines the center point of the
proximity region in object space, and the _size_ field specifies a
vector which defines the width (x), height (y), and depth (z) of the box
bounding the region. ProximitySensor nodes are affected by the
hierarchical transformations of its parents.

The _enterTime_ event is generated whenever the _isActive_ TRUE event is
generated (user enters the box), and _exitTime_ events are generated
whenever _isActive_ FALSE event is generated (user exits the box).

The _position_changed_ and _orientation_changed_ events send events
whenever the position and orientation of the viewer changes with respect
to the ProximitySensor's coordinate system - this includes enter and
exit times. Note that the user movement may be as a result of a variety
of circumstances (e.g. browser navigation, proximity sensor's coordinate
system changes, bound Viewpoint's position or orientation changes, or
the ProximitySensor's coordinate system changes).

Each ProximitySensor behaves independently of all other ProximitySensors
- every enabled ProximitySensor that is effected by the user's movement
receives and sends events, possibly resulting in multiple
ProximitySensors receiving and sending events simultaneously. Unlike
TouchSensors, there is no notion of a ProximitySensor lower in the scene
graph "grabbing" events.

Instanced (DEF/USE) ProximitySensors use the union of all
the boxes to check for enter and exit - an instanced ProximitySensor
will detect enter and exit for all instances of the box and send output
events appropriately.

A ProximitySensor that surrounds the entire world will have an enterTime
equal to the time that the world was entered and can be used to start up
animations or behaviors as soon as a world is loaded. A ProximitySensor
with a (0 0 0) _size_ field cannot generate events - this is equivalent
to setting the _enabled_ field to FALSE.


[[ScalarInterpolator]]
==== ScalarInterpolator

[source]
....
ScalarInterpolator {
  eventIn      SFFloat set_fraction
  exposedField MFFloat key           []
  exposedField MFFloat keyValue      []
  eventOut     SFFloat value_changed
}
....

This node linearly interpolates among a set of SFFloat values. This
interpolator is appropriate for any parameter defined using a single
floating point value, e.g., width, radius, intensity, etc. The
_keyValue_ field must contain exactly as many numbers as there are
keyframes in the _key_ field.

Refer to "<<InterpolatorNodes, Concepts - Interpolators>>" for a
more detailed discussion of interpolators.


[[Script]]
==== Script

[source]
....
Script { 
  exposedField MFString url           [] 
  field        SFBool   directOutput  FALSE
  field        SFBool   mustEvaluate  FALSE
  # And any number of:
  eventIn      eventTypeName eventName
  field        fieldTypeName fieldName initialValue
  eventOut     eventTypeName eventName
}
....

The Script node is used to program behavior in a scene. Script nodes
typically receive events that signify a change or user action, contain a
program module that performs some computation, and effect change
somewhere else in the scene by sending output events. Each Script node
has associated programming language code, referenced by the _url_ field,
that is executed to carry out the Script node's function. That code will
be referred to as "the script" in the rest of this description.

Browsers are not required to support any specific language. See the
section in "<<Scripting, Concepts - Scripting>>" for detailed
information on scripting languages. Browsers are required to adhere to
the language bindings of languages specified in annexes of the
specification. See the section "<<URLsAndURNs, Concepts - URLS and URNs>>" for details on
the _url_ field.

When the script is created, any language-dependent or user-defined
initialization is performed. The script is able to receive and process
events that are sent to it. Each event that can be received must be
declared in the Script node using the same syntax as is used in a
prototype definition:

[source]
....
    eventIn type name
....

The _type_ can be any of the standard VRML fields (see
"<<fieldsRef_html, Field Reference>>"), and _name_ must be an
identifier that is unique for this Script node.

The Script node should be able to generate events in response to the
incoming events. Each event that can be generated must be declared in
the Script node using the following syntax:

[source]
....
    eventOut type name
....

Script nodes cannot have exposedFields. The implementation
ramifications of exposedFields is far too complex and thus not allowed.

If the Script node's _mustEvaluate_ field is FALSE, the browser can
delay sending input events to the script until its outputs are needed by
the browser. If the _mustEvaluate_ field is TRUE, the browser should
send input events to the script as soon as possible, regardless of
whether the outputs are needed. The _mustEvaluate_ field should be set
to TRUE only if the Script has effects that are not known to the browser
(such as sending information across the network); otherwise, poor
performance may result.

Once the script has access to a VRML node (via an SFNode or MFNode value
either in one of the Script node's fields or passed in as an eventIn),
the script should be able to read the contents of that node's exposed
field. If the Script node's _directOutput_ field is TRUE, the script may
also send events directly to any node to which it has access, and may
dynamically establish or break routes. If _directOutput_ is FALSE (the
default), then the script may only affect the rest of the world via
events sent through its eventOuts.

A script is able to communicate directly with the VRML browser to get
the current time, the current world URL, and so on. This is strictly
defined by the API for the specific language being used.

It is expected that all other functionality (such as networking
capabilities, multi-threading capabilities, and so on) will be provided
by the scripting language.

The location of the Script node in the scene graph has no affect on its
operation. For example, if a parent of a Script node is a Switch node
with _whichChoice_ set to -1 (i.e. ignore its children), the Script
continues to operate as specified (receives and sends events).


[[Shape]]
==== Shape

[source]
....
Shape {
  exposedField SFNode appearance NULL
  exposedField SFNode geometry   NULL
}
....

The Shape node has two fields: _appearance_ and _geometry_ which are
used to create rendered objects in the world. The _appearance_ field
specifies an <<Appearance, Appearance>> node that specifies the visual
attributes (e.g. material and texture) to be applied to the geometry .
The _geometry_ field specifies a <<Geometry, geometry node>>. The
specified geometry node is rendered with the specified appearance nodes
applied.


See "<<LightsAndLighting, Concepts - Lighting Model>>" for
details of the VRML lighting model and the interaction between
Appearance and <<GeometryNodes, geometry nodes>>.

If the _geometry_ field is NULL the object is not drawn.


[[Sound]]
==== Sound

[source]
....
Sound {
  exposedField SFVec3f  direction     0 0 1
  exposedField SFFloat  intensity     1
  exposedField SFVec3f  location      0 0 0
  exposedField SFFloat  maxBack       10
  exposedField SFFloat  maxFront      10
  exposedField SFFloat  minBack       1
  exposedField SFFloat  minFront      1
  exposedField SFFloat  priority      0
  exposedField SFNode   source        NULL
  field        SFBool   spatialize    TRUE
}
....

The Sound node describes the positioning and spatial presentation of a
sound in a VRML scene. The sound may be located at a point and emit
sound in a spherical or ellipsoid pattern, in the local coordinate
system. The ellipsoid is pointed in a particular direction and may be
shaped to provide more or less directional focus from the location of
the sound. The sound node may also be used to describe an ambient sound
which tapers off at a specified distance from the sound node.

The Sound node also enables ambient background sound to be created by
setting of the maxFront and maxBack to the radius of the area for the
ambient noise. If ambient noise is required for the whole scene then
these values should be set to at least cover the distance from the
location to the farthest point in scene from that point (including
effects of transforms).

The _source_ field specifies the sound source for the sound node. If
there is no source specified the Sound will emit no audio. The source
field shall specify either an AudioClip or a MovieTexture node.
Furthermore, the MovieTexture node must refer to a movie format that
supports sound (e.g. MPEG1-Systems
<<references_html, MPEG>>).

The _intensity_ field adjusts the volume of each sound source; The
_intensity_ is an SFFloat that ranges from 0.0 to 1.0. An _intensity_ of
0 is silence, and an _intensity_ of 1 is the full volume of the sound in
the sample or the full volume of the MIDI clip.

The _priority_ field gives the author some control over which sounds the
browser will choose to play when there are more sounds active than sound
channels available. The _priority_ varies between 0.0 and 1.0, with 1.0
being the highest priority. For most applications priority 0.0 should be
used for a normal sound and 1.0 should be used only for special event or
cue sounds (usually of short duration) that the author wants the user to
hear even if they are farther away and perhaps of lower intensity than
some other ongoing sounds. Browsers should make as many sound channels
available to the scene as is efficiently possible.

If the browser does not have enough sound channels to play all of the
currently active sounds, it is recommended that the browser sort the
active sounds into an ordered list using the following sort keys:

. decreasing _priority_;
. for sounds with _priority_ > 0.5, increasing (now-_startTime_)
. decreasing _intensity_ at viewer location ((_intensity_/distance)**2);

where now represents the current time, and _startTime_ is the
_startTime_ field of the audio source node specified in the _source_
field.

It is important that sort key #2 be used for the high priority (event
and cue) sounds so that new cues will be heard even when the channels
are "full" of currently active high priority sounds. Sort key #2 should
not be used for normal priority sounds so selection among them will be
based on sort key #3 - intensity and distance from the viewer.

The browser should play as many sounds from the beginning of this sorted
list as it has available channels. On most systems the number of
concurrent sound channels is distinct from the number of concurrent MIDI
streams. On these systems the browser may maintain separate ordered
lists for sampled sounds and MIDI streams.

A sound's _location_ in the scene graph determines its spatial location
(the sound's location is transformed by the current transformation) and
whether or not it can be heard. A sound can only be heard while it is
part of the traversed scene; sound nodes that are descended from LOD,
Switch, or any grouping or prototype node that disables traversal (i.e.
drawing) of its children will not be audible unless they are traversed.
If a sound is silenced for a time under a Switch or LOD node, and later
it becomes part of the traversal again, the sound picks up where it
would have been had it been playing continuously.

Around the _location_ of the emitter, _minFront_ and _minBack_ determine
the extent of the full intensity region in front of and behind the
sound. If the location of the sound is taken as a focus of an ellipsoid,
the _minBack_ and _minFront_ values, in combination with the _direction_
vector determine the two foci of an ellipsoid bounding the ambient
region of the sound. Similarly, _maxFront_ and _maxBack_ determine the
limits of audibility in front of and behind the sound; they describe a
second, outer ellipsoid. If _minFront_ equals _minBack_ and _maxFront_
equals _maxBack_, the sound is omni-directional, the direction vector is
ignored, and the min and max ellipsoids become spheres centered around
the sound node. The fields _minFront_, _maxFront_, _minBack_, and
_maxBack_ are scaled by the parent transformations - these values must
be >= 0.0.

The inner ellipsoid defines a space of full intensity for the sound.
Within that space the sound will play at the intensity specified in the
sound node. The outer ellipsoid determines the maximum extent of the
sound. Outside that space, the sound cannot be heard at all. In between
the two ellipsoids, the intensity drops off proportionally with inverse
square of the distance. With this model, a Sound usually will have
smooth changes in intensity over the entire extent is which it can be
heard. However, if at any point the maximum is the same as or inside the
minimum, the sound is cut off immediately at the edge of the minimum
ellipsoid.

The ideal implementation of the sound attenuation between the inner and
outer ellipsoids is an inverse power dropoff. A reasonable approximation
to this ideal model is a linear dropoff in decibel value. Since an
inverse power dropoff never actually reaches zero, it is necessary to
select an appropriate cutoff value for the outer ellipsoid so that the
outer ellipsoid contains the space in which the sound is truly audible
and excludes space where it would be negligible. Keeping the outer
ellipsoid as small as possible will help limit resources used by nearly
inaudible sounds. Experimentation suggests that a 20dB dropoff from the
maximum intensity is a reasonable cutoff value that makes the bounding
volume (the outer ellipsoid) contain the truly audible range of the
sound. Since actual physical sound dropoff in an anechoic environment
follows the inverse square law, using this algorithm it is possible to
mimic real-world sound attenuation by making the maximum ellipsoid ten
times larger than the minimum ellipsoid. This will yield inverse square
dropoff between them.

Browsers should support spatial localization of sound as well as their
underlying sound libraries will allow. The _spatialize_ field is used to
indicate to browsers that they should try to locate this sound. If the
_spatialize_ field is TRUE, the sound should be treated as a monaural
sound coming from a single point. A simple spatialization mechanism just
places the sound properly in the pan of the stereo (or multichannel)
sound output. Sounds are faded out over distance as described above.
Browsers may use more elaborate sound spatialization algorithms if they
wish.

Authors can create ambient sounds by setting the _spatialize_ field to
FALSE. In that case, stereo and multichannel sounds should be played
using their normal separate channels. The distance to the sound and the
minimum and maximum ellipsoids (discussed above) should affect the
intensity in the normal way. Authors can create ambient sound over the
entire scene by setting the _minFront_ and _minBack_ to the maximum
extents of the scene.


[[Sphere]]
==== Sphere

[source]
....
Sphere {
  field SFFloat radius  1
}
....

The Sphere node specifies a sphere centered at (0, 0, 0) in the local
coordinate system. The _radius_ field specifies the radius of the sphere
and must be >= 0.0.

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/sphere.gif[]

When a texture is applied to a sphere, the texture covers the entire
surface, wrapping counterclockwise from the back of the sphere. The
texture has a seam at the back where the YZ plane intersects the sphere.
<<TextureTransform, TextureTransform>> affects the texture coordinates
of the Sphere.

The Sphere geometry is considered to be solid and thus requires outside
faces only. When viewed from the inside the results are undefined.


[[SphereSensor]]
==== SphereSensor

[source]
....
SphereSensor {
  exposedField SFBool     autoOffset        TRUE
  exposedField SFBool     enabled           TRUE
  exposedField SFRotation offset            0 1 0 0
  eventOut     SFBool     isActive
  eventOut     SFRotation rotation_changed
  eventOut     SFVec3f    trackPoint_changed
}
....

The SphereSensor maps pointing device (e.g. mouse or wand) motion into
spherical rotation about the center of its local space. SphereSensor
uses the descendant geometry of its parent node to determine if a hit
occurs. The feel of the rotation is as if you were rolling a ball.

The _enabled_ exposed field enables and disables the SphereSensor - if
TRUE, the sensor reacts appropriately to user events, if FALSE, the
sensor does not track user input or send output events. If _enabled_
receives a FALSE event and _isActive_ is TRUE, the sensor becomes
disabled and deactivated, and outputs an _isActive_ FALSE event. If
_enabled_ receives a TRUE event the sensor is enabled and ready for user
activation.

The SphereSensor generates events if the pointing device is activated
while over any descendant geometry nodes of its parent group and then
moved while activated. Typically, the pointing device is a 2D device
such as a mouse. The pointing device is considered to be moving within a
plane at a fixed distance from the viewer and perpendicular to the line
of sight; this establishes a set of 3D coordinates for the pointer. If a
3D pointer is in use, then the sensor generates events only when the
pointer is within the user's field of view. In either case, the pointing
device is considered to "pass over" geometry when that geometry is
intersected by a line extending from the viewer and passing through the
pointer's 3D coordinates. If multiple sensors' geometry intersect this
line (hereafter called the bearing), only the nearest will be eligible
to generate events.

Upon activation of the pointing device (e.g. mouse button down) over the
sensor's geometry an _isActive_ TRUE event is sent. The vector defined
by the initial point of intersection on the SphereSensor's geometry and
the local origin determines the radius of the sphere used to map
subsequent pointing device motion while dragging. The virtual sphere
defined by this radius and the local origin at the time of activation
are used to interpret subsequent pointing device motion and is not
affected by any changes to the sensor's coordinate system while the
sensor is active. For each position of the bearing, a _rotation_changed_
event is output which corresponds to a relative rotation from the
original intersection, plus the _offset_ value. The sign of the rotation
is defined by the local coordinate system of the sensor.
_trackPoint_changed_ events reflect the unclamped drag position on the
surface of this sphere. When the pointing device is deactivated and
_autoOffset_ is TRUE, _offset_ is set to the last rotation value and an
_offset_changed_ event is generated. 
See "<<DragSensors, Concepts - Drag Sensors">> for more
details.

When the sensor generates an _isActive_ TRUE event, it grabs all further
motion events from the pointing device until it releases and generates
an _isActive_ FALSE event (other pointing device sensors
cannot generate events during this time). Motion of the
pointing device while _isActive_ is TRUE is referred to as a "drag". If
a 2D pointing device is in use, _isActive_ events will typically reflect
the state of the primary button associated with the device (i.e.
_isActive_ is TRUE when the primary button is pressed and FALSE when
released). If a 3D pointing device (e.g. wand) is in use, _isActive_
events will typically reflect whether the pointer is within or in
contact with the sensor's geometry.

While the pointing device is activated, _trackPoint_changed_ and
_rotation_changed_ events are output. _trackPoint_changed_ events
represent the unclamped intersection points on the surface of the
invisible sphere. If the pointing device is dragged off the sphere while
activated, browsers may interpret this in several ways (e.g. clamp all
values to the sphere, continue to rotate as the point is dragged away
from the sphere, etc.). Each movement of the pointing device, while
_isActive_ is TRUE, generates _trackPoint_changed_ and
_rotation_changed_ events.


See "<<PointingDeviceSensors, Concepts - Pointing Device Sensors and Drag Sensors>>" 
for more details.


[[SpotLight]]
==== SpotLight

[source]
....
SpotLight {
  exposedField SFFloat ambientIntensity  0 
  exposedField SFVec3f attenuation       1 0 0
  exposedField SFFloat beamWidth         1.570796
  exposedField SFColor color             1 1 1 
  exposedField SFFloat cutOffAngle       0.785398
  exposedField SFVec3f direction         0 0 -1
  exposedField SFFloat intensity         1  
  exposedField SFVec3f location          0 0 0  
  exposedField SFBool  on                TRUE
  exposedField SFFloat radius            100 
}
....

The SpotLight node defines a light source that emits light from a
specific point along a specific direction vector and constrained within
a solid angle. Spotlights may illuminate geometry nodes that respond to
light sources and intersect the solid angle. Spotlights are specified in
their local coordinate system and are affected by parent
transformations.


See "<<Lights, Concepts - Light Sources>>" for a detailed
description of _ambientIntensity,_ _color_, _intensity_, and VRML's
lighting equations. 
See "<<Lighting, Concepts - Lighting Model>>" for a detailed
description of the VRML lighting equations.

The _location_ field specifies a translation offset of the center point
of the light source from the light's local coordinate system origin.
This point is the apex of the solid angle which bounds light emission
from the given light source. The _direction_ field specifies the
direction vector of the light's central axis defined in its own local
coordinate system. The _on_ field specifies whether the light source
emits light--if TRUE, then the light source is emitting light and may
illuminate geometry in the scene, if FALSE it does not emit light and
does not illuminate any geometry. The _radius_ field specifies the
radial extent of the solid angle and the maximum distance from
_location_ than may be illuminated by the light source - the light
source does not emit light outside this radius. The _radius_ must be >=
0.0.

The _cutOffAngle_ field specifies the outer bound of the solid angle.
The light source does not emit light outside of this solid angle. The
_beamWidth_ field specifies an inner solid angle in which the light
source emits light at uniform full intensity. The light source's
emission intensity drops off from the inner solid angle (_beamWidth_) to
the outer solid angle (_cutOffAngle_). The drop off function from the
inner angle to the outer angle is a cosine raised to a power function:

[source]
....
    intensity(angle) = intensity * (cosine(angle) ** exponent)

    where exponent = 0.5*log(0.5)/log(cos(beamWidth)),
          intensity is the SpotLight's field value,
          intensity(angle) is the light intensity at an arbitrary
              angle from the direction vector,
          and angle ranges from 0.0 at central axis to cutOffAngle.
....

If _beamWidth_ > _cutOffAngle_, then _beamWidth_ is assumed to be equal
to _cutOffAngle_ and the light source emits full intensity within the
entire solid angle defined by _cutOffAngle_. Both _beamWidth_ and
_cutOffAngle_ must be greater than 0.0 and less than or equal to PI/2.
See figure below for an illustration of the SpotLight's field semantics
(note: this example uses the default attenuation).

The light's illumination falls off with distance as specified by three
_attenuation_ coefficients. The attenuation factor is _1/(attenuation[0]
+ attenuation[1]*r + attenuation[2]*r^2)_, where _r_ is the distance of
the light to the surface being illuminated. The default is no
attenuation. An _attenuation_ value of *0 0 0* is identical to *1 0 0*.
Attenuation values must be >= 0.0.

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/spotlight.gif[]


[[Switch]]
==== Switch

[source]
....
Switch {
  exposedField    MFNode  choice      []
  exposedField    SFInt32 whichChoice -1
}
....

The Switch grouping node traverses zero or one of the nodes specified in
the _choice_ field.

See the "<<GroupingNodes, Concepts - Grouping and Children Nodes>>" 
section which describes "__children node__s" for a details on
the types of nodes that are legal values for _choice_.

The _whichChoice_ field specifies the index of the child to traverse,
where the first child has index 0. If _whichChoice_ is less than zero or
greater than the number of nodes in the _choice_ field then nothing is
chosen.

Note that all nodes under a Switch continue to receive and send events
(i.e.routes) regardless of the value of _whichChoice_. For example, if
an active TimeSensor is contained within an inactive choice of an
Switch, the TimeSensor sends events regardless of the Switch's state.


[[Text]]
==== Text

[source]
....
Text {
  exposedField  MFString string    []
  exposedField  SFNode   fontStyle NULL
  exposedField  MFFloat  length    []
  exposedField  SFFloat  maxExtent 0.0
}
....

The Text node specifies a two-sided, flat text string object positioned
in the X-Y plane of the local coordinate system based on values defined
in the fontStyle field (see <<FontStyle, FontStyle>> node). Text nodes
may contain multiple text strings specified using the UTF-8 encoding as
specified by the http://www.iso.ch/cate/d18741.html[ISO 10646-1:1993]
standard (http://www.iso.ch/cate/d18741.html). Due to the drastic
changes in Korean Jamo language, the character set of the UTF-8 will be
based on ISO 10646-1:1993 plus pDAM 1 - 5 (including the Korean
changes). The text strings are stored in visual order.

The text strings are contained in the _string_ field. The _fontStyle_
field contains one <<FontStyle, FontStyle>> node that specifies the
font size, font family and style, direction of the text strings, and any
specific language rendering techniques that must be used for the text.

The _maxExtent_ field limits and scales all of the text strings if the
length of the maximum string is longer than the maximum extent, as
measured in the local coordinate space. If the text string with the
maximum length is shorter than the _maxExtent_, then there is no
scaling. The maximum extent is measured horizontally for horizontal text
(<<FontStyle, FontStyle>> node: __horizontal__=TRUE) and vertically
for vertical text (<<FontStyle, FontStyle>> node:
__horizontal__=FALSE). The _maxExtent_ field must be >= 0.0.

The _length_ field contains an MFFloat value that specifies the length
of each text string in the local coordinate space. If the string is too
short, it is stretched (either by scaling the text or by adding space
between the characters). If the string is too long, it is compressed
(either by scaling the text or by subtracting space between the
characters). If a length value is missing--for example, if there are
four strings but only three length values--the missing values are
considered to be 0.

For both the _maxExtent_ and _length_ fields, specifying a value of 0
indicates to allow the string to be any length.

Textures are applied to text as follows. The texture origin is at the
origin of the first string, as determined by the justification. The
texture is scaled equally in both S and T dimensions, with the font
height representing 1 unit. S increases to the right, and T increases
up.

===== ISO 10646-1:1993 Character Encodings

Characters in http://www.iso.ch/welcome.html[ISO 10646] are encoded in
multiple octets. Code space is divided into four units, as follows:

[source]
....
+-------------+-------------+-----------+------------+
| Group-octet | Plane-octet | Row-octet | Cell-octet |
+-------------+-------------+-----------+------------+
....

The ISO 10646-1:1993 allows two basic forms for characters:

. UCS-2 (Universal Coded Character Set-2). Also known as the Basic
Multilingual Plane (BMP). Characters are encoded in the lower two octets
(row and cell). Predictions are that this will be the most commonly used
form of 10646.
. UCS-4 (Universal Coded Character Set-4). Characters are encoded in the
full four octets.

In addition, three transformation formats (UCS Transformation Format
(UTF) are accepted: UTF-7, UTF-8, and UTF-16. Each represents the nature
of the transformation - 7-bit, 8-bit, and 16-bit. The UTF-7 and UTF-16
can be referenced in the Unicode Standard 2.0 book.

The UTF-8 maintains transparency for all of the ASCII code values
(0...127). It allows ASCII text (0x0..0x7F) to appear without any
changes and encodes all characters from 0x80.. 0x7FFFFFFF into a series
of six or fewer bytes.

If the most significant bit of the first character is 0, then the
remaining seven bits are interpreted as an ASCII character. Otherwise,
the number of leading 1 bits will indicate the number of bytes
following. There is always a o bit between the count bits and any data.

First byte could be one of the following. The X indicates bits available
to encode the character.

[source]
....
 0XXXXXXX only one byte   0..0x7F (ASCII)
 110XXXXX two bytes       Maximum character value is 0x7FF
 1110XXXX three bytes     Maximum character value is 0xFFFF
 11110XXX four bytes      Maximum character value is 0x1FFFFF
 111110XX five bytes      Maximum character value is 0x3FFFFFF
 1111110X six bytes       Maximum character value is 0x7FFFFFFF
....

All following bytes have this format: 10XXXXXX

A two byte example. The symbol for a register trade mark is "circled R
registered sign" or 174 in ISO/Latin-1 (8859/1). It is encoded as 0x00AE
in UCS-2 of the ISO 10646. In UTF-8 it is has the following two byte
encoding 0xC2, 0xAE.


See "<<Lighting, Concepts - Lighting Model>>" for details on
VRML lighting equations and how Appearance, Material and textures
interact with lighting.

The Text node does not perform collision detection.


[[TextureCoordinate]]
==== TextureCoordinate

[source]
....
TextureCoordinate {
  exposedField MFVec2f point  []
}
....

The TextureCoordinate node specifies a set of 2D texture coordinates
used by vertex-based geometry nodes (e.g.
<<IndexedFaceSet, IndexedFaceSet>> and
<<ElevationGrid, ElevationGrid>>) to map from textures to the
vertices. Textures are two dimensional color functions that given an S
and T pair return a color value. Texture maps parameter values range
from 0.0 to 1.0 in S and T. However, TextureCoordinate values, specified
by the _point_ field, can range from -_infinity_ to +__infinity__.
Texture coordinates identify a location (and thus a color value) in the
texture map. The horizontal coordinate, S, is specified first, followed
by the vertical coordinate, T.

If the texture map is repeated in a given direction (S or T), then a
texture coordinate C is mapped into a texture map that has N pixels in
the given direction as follows:

[source]
....
        Location = (C - floor(C)) * 
....

If the texture is not repeated:

[source]
....
        Location = (C > 1.0 ? 1.0 : (C < 0.0 ? 0.0 : C)) * N
....

See texture nodes for details on repeating textures
(<<ImageTexture, ImageTexture>>, <<MovieTexture, MovieTexture>>,
<<PixelTexture, PixelTexture>>).


[[TextureTransform]]
==== TextureTransform

[source]
....
TextureTransform {
  exposedField SFVec2f center      0 0
  exposedField SFFloat rotation    0
  exposedField SFVec2f scale       1 1
  exposedField SFVec2f translation 0 0
}
....

The TextureTransform node defines a 2D transformation that is applied to
texture coordinates (see <<TextureCoordinate, TextureCoordinate>>).
This node affects the way textures are applied to the surface of
geometry. The transformation consists of (in order) a non-uniform scale
about an arbitrary center point, a rotation about the center point, and
a translation. This allows for changes to the size, orientation, and
position of textures on shapes. Note that these changes appear reversed
when when viewed in the surface of geometry. For example, a _scale_
value of *2 2* will scale the texture coordinates and have the net
effect of shrinking the texture size by a factor of 2 (texture
coordinates are twice as large and thus cause the texture to repeat). A
translation of *0.5 0.0* translates the texture coordinates +.5 units
along the S-axis and has the net effect of translating the texture -0.5
along the S-axis on the geometry's surface. A rotation of PI/2 of the
texture coordinates results in a -PI/2 rotation of the texture on the
geometry.

The _center_ field specifies a translation offset in texture coordinate
space about which the _rotation_ and _scale_ fields are applied. The
_scale_ field specifies a scaling factor in S and T of the texture
coordinates about the _center_ point - _scale_ values must be >= 0.0.
The _rotation_ field specifies a rotation in radians of the texture
coordinates about the _center_ point after the scale has taken place.
The _translation_ field specifies a translation of the texture
coordinates.

Given a 2-dimensional texture coordinate *T* and a TextureTransform
node, *T* is transformed into point *`+T'+`* by a series of intermediate
transformations. In matrix-transformation notation, where C (center), T
(translation), R (rotation), and S (scale) are the equivalent
transformation matrices,

[source]
....
  T' = TT x C x R x S x -TC x T  (where T is a column vector)
....

Note that TextureTransforms cannot combine or accumulate.


[[TimeSensor]]
==== TimeSensor

[source]
....
TimeSensor {
  exposedField SFTime   cycleInterval 1
  exposedField SFBool   enabled       TRUE
  exposedField SFBool   loop          FALSE
  exposedField SFTime   startTime     0
  exposedField SFTime   stopTime      0
  eventOut     SFTime   cycleTime
  eventOut     SFFloat  fraction_changed
  eventOut     SFBool   isActive
  eventOut     SFTime   time
}
....

TimeSensors generate events as time passes. TimeSensors can be used to
drive continuous simulations and animations, periodic activities (e.g.,
one per minute), and/or single occurrence events such as an alarm clock.
TimeSensor discrete eventOuts include: _isActive_, which becomes TRUE
when the TimeSensor begins running, and FALSE when it stops running, and
_cycleTime_, a time event at _startTime_ and at the beginning of each
new cycle (useful for synchronization with other time-based objects).
The remaining outputs generate continuous events and consist of
_fraction_changed_, which is an SFFloat in the closed interval [0,1]
representing the completed fraction of the current cycle, and _time_, an
SFTime event specifying the absolute time for a given simulation tick.

If the _enabled_ exposedField is TRUE, the TimeSensor is enabled and may
be running. If a _set_enabled_ FALSE event is received while the
TimeSensor is running, then the sensor should evaluate and send all
relevant outputs, send a FALSE value for _isActive,_ and disable itself.
However, events on the exposedFields of the TimeSensor (such as
_set_startTime)_ are processed and their corresponding eventOuts
(_startTime_changed)_ are sent regardless of the state of _enabled_. The
remaining discussion assumes _enabled_ is TRUE.

The _loop, startTime,_ and _stopTime_ exposedFields, and the _isActive_
eventOut and their affects on the TimeSensor node, are discussed in
detail in the "<<TimeDep, Concepts - Time Dependent Nodes>>" section.
The "_cycle"_ of an TimeSensor lasts for _cycleInterval_ seconds. The
value of _cycleInterval_ must be greater than 0 (a value less than or
equal to 0 produces undefined results). Because the TimeSensor is more
complex than the abstract TimeDep node and generates continuous
eventOuts, some of the information in the "Time Dependent Nodes" section
is repeated here.

A _cycleTime_ eventOut can be used for synchronization purposes, e.g.,
sound with animation. The value of a _cycleTime_ eventOut will be equal
to the time at the beginning of the current cycle. A _cycleTime_
eventOut is generated at the beginning of every cycle, including the
cycle starting at _startTime_. The first _cycleTime_ eventOut for a
TimeSensor node can be used as an alarm (single pulse at a specified
time).

When a TimeSensor becomes active it will generate an _isActive =_ TRUE
event and __ begin generating _time,_ _fraction_changed,_ and
_cycleTime_ events, which may be routed to other nodes to drive
animation or simulated behaviors - (see below for behavior at read
time). The _time_ event outputs the absolute time for a given tick of
the TimeSensor (<<SFTime, time>> fields and events
represent the number of seconds since midnight GMT January 1, 1970).
_fraction_changed_ events output a floating point value in the closed
interval [0, 1], where 0 corresponds to _startTime_ and 1 corresponds to
_startTime +_ N__*cycleInterval,__ where N = 1, 2, ... . That is, the
_time_ and _fraction_changed_ eventOuts __ can be computed as:

[source]
....
    time = now
    f = fmod(now - startTime, cycleInterval)
    if (f == 0.0 && now > startTime) 
        fraction_changed = 1.0
    else
        fraction_changed = f / cycleInterval
....

A TimeSensor can be set up to be active at read time by specifying
_loop_ TRUE (not the default) and _stopTime <= startTime_ (satisfied by
the default values). The _time_ events output absolute times for each
tick of the TimeSensor -- times must start at _startTime_ and end with
either __startTime__+__cycleInterval__, _stopTime_, or loop forever
depending on the values of the other fields. An active TimeSensor must
stop at the first simulation tick when time `now` >= _stopTime >
startTime_.

No guarantees are made with respect to how often a TimeSensor will
generate time events, but a TimeSensor should generate events at least
at every simulation tick. TimeSensors are guaranteed to generate final
_time_ and _fraction_changed_ events. If loop is FALSE, the final _time_
event will be generated with a value of (_startTime+cycleInterval_) or
__stopTime (__if _stopTime_ > _startTime),_ whichever value is less. If
_loop_ is TRUE at the completion of every cycle, then the final event
will be generated as evaluated at _stopTime_ (if _stopTime_ >
_startTime)_ or never__.__

An active TimeSensor ignores _set_cycleInterval_, and _set_startTime_
events. An active TimeSensor also ignores _set_stopTime_ events for
_set_stopTime < startTime_. For example, if a _set_startTime_ event is
received while a TimeSensor is active, then that _set_startTime_ event
is ignored (the _startTime_ field is not changed, and a
_startTime_changed_ eventOut is not generated). If an active TimeSensor
receives a _set_stopTime_ event that is less than `now` and greater
than or equal to _startTime_, it behaves as if the _stopTime_ requested
is `now` and sends the final events based on `now` (note that
_stopTime_ is set as specified in the eventIn).


[[TouchSensor]]
==== TouchSensor

[source]
....
TouchSensor {
  exposedField SFBool  enabled TRUE
  eventOut     SFVec3f hitNormal_changed
  eventOut     SFVec3f hitPoint_changed
  eventOut     SFVec2f hitTexCoord_changed
  eventOut     SFBool  isActive
  eventOut     SFBool  isOver
  eventOut     SFTime  touchTime
}
....

A TouchSensor tracks the location and state of the pointing device and
detects when the user points at geometry contained by the TouchSensor's
parent group. This sensor can be enabled or disabled by sending it an
_enabled_ event with a value of TRUE or FALSE. If the TouchSensor is
disabled, it does not track user input or send output events.

The TouchSensor generates events as the pointing device "passes over"
any geometry nodes that are descendants of the TouchSensor's parent
group. Typically, the pointing device is a 2D device such as a mouse. In
this case, the pointing device is considered to be moving within a plane
a fixed distance from the viewer and perpendicular to the line of sight;
this establishes a set of 3D coordinates for the pointer. If a 3D
pointer is in use, then the TouchSensor generates events only when the
pointer is within the user's field of view. In either case, the pointing
device is considered to "pass over" geometry when that geometry is
intersected by a line extending from the viewer and passing through the
pointer's 3D coordinates. If multiple surfaces intersect this line
(hereafter called the bearing), only the nearest will be eligible to
generate events.

The _isOver_ eventOut reflects the state of the pointing device with
regard to whether it is over the TouchSensor's geometry or not. When the
pointing device changes state from a position such that its bearing
does not intersect any of the TouchSensor's
geometry to one in which it does intersect geometry, an _isOver_ TRUE
event is generated. When the pointing device moves from a position such
that its bearing intersects geometry to one in which it no longer
intersects the geometry, or some other geometry is obstructing the
TouchSensor's geometry, an _isOver_ FALSE event is generated. These
events are generated only when the pointing device has moved and changed
`over state; events are not generated if the geometry itself is
animating and moving underneath the pointing device.

As the user moves the bearing over the TouchSensor's geometry, the point
of intersection (if any) between the bearing and the geometry is
determined. Each movement of the pointing device, while _isOver_ is
TRUE, generates _hitPoint_changed_, _hitNormal_changed_, and
_hitTexCoord_changed_ events. _hitPoint_changed_ events contain the 3D
point on the surface of the underlying geometry, given in the
TouchSensor's coordinate system. _hitNormal_changed_ events contain the
surface normal vector at the hitPoint. _hitTexCoord_changed_ events
contain the texture coordinates of that surface at the hitPoint, which
can be used to support the 3D equivalent of an image map.

If _isOver_ is TRUE, the user may activate the pointing device to cause
the TouchSensor to generate _isActive_ events (e.g. press the primary
mouse button). When the TouchSensor generates an _isActive_ TRUE event,
it grabs all further motion events from the pointing device until it
releases and generates an _isActive_ FALSE event (other pointing device
sensors will not generate events during this time). Motion
of the pointing device while _isActive_ is TRUE is referred to as a
"drag". If a 2D pointing device is in use, _isActive_ events will
typically reflect the state of the primary button associated with the
device (i.e. _isActive_ is TRUE when the primary button is pressed, and
FALSE when not released). If a 3D pointing device is in use, _isActive_
events will typically reflect whether the pointer is within or in
contact with the TouchSensor's geometry.

The eventOut field _touchTime_ is generated when all three of the
following conditions are true:

* the pointing device was over the geometry when it was
initially activated (_isActive_ is TRUE),
* the pointing device is currently over the
geometry (_isOver_ is TRUE),
* and, the pointing device is deactivated (_isActive_
FALSE event is also generated).


See "<<PointingDeviceSensors, Concepts - Pointing Device Sensors>>" 
for more details.


[[Transform]]
==== Transform

[source]
....
Transform {
  eventIn      MFNode      addChildren
  eventIn      MFNode      removeChildren
  exposedField SFVec3f     center           0 0 0
  exposedField MFNode      children         []
  exposedField SFRotation  rotation         0 0 1  0
  exposedField SFVec3f     scale            1 1 1
  exposedField SFRotation  scaleOrientation 0 0 1  0
  exposedField SFVec3f     translation      0 0 0
  field        SFVec3f     bboxCenter       0 0 0
  field        SFVec3f     bboxSize         -1 -1 -1
}  
....

A Transform is a grouping node that defines a coordinate system for its
children that is relative to the coordinate systems of its parents. See
also "<<CoordinateSystems, Concepts - Coordinate Systems and Transformations>>." 

See the "<<GroupingNodes, Concepts - Grouping and Children Nodes>>" 
section for a description of the _children_, _addChildren_, and
_removeChildren_ fields and eventIns.

The _bboxCenter_ and _bboxSize_ fields specify a bounding box that
encloses the Transform's children. This is a hint that may be used for
optimization purposes. If the specified bounding box is smaller than the
actual bounding box of the children at any time, then the results are
undefined. A default _bboxSize_ value, (-1 -1 -1), implies that the
bounding box is not specified and if needed must be calculated by the
browser. 
See "<<BoundingBoxes, Concepts - Bounding Boxes>>" for a
description of the _bboxCenter_ and _bboxSize_ fields.

The _translation_, _rotation_, _scale_, _scaleOrientation_ and _center_
fields define a geometric 3D transformation consisting of (in order) a
(possibly) non-uniform scale about an arbitrary point, a rotation about
an arbitrary point and axis, and a translation. The _center_ field
specifies a translation offset from the local coordinate system's
origin, (0,0,0). The _rotation_ field specifies a rotation of the
coordinate system. The _scale_ field specifies a non-uniform scale of
the coordinate system - _scale_ values must be >= 0.0. The
_scaleOrientation_ specifies a rotation of the coordinate system before
the scale (to specify scales in arbitrary orientations). The
_scaleOrientation_ applies only to the scale operation. The
_translation_ field specifies a translation to the coordinate system.

Given a 3-dimensional point *P* and Transform node, *P* is transformed
into point *`+P'+`* in its parent's coordinate system by a series of
intermediate transformations. In matrix-transformation notation, where C
(center), SR (scaleOrientation), T (translation), R (rotation), and S
(scale) are the equivalent transformation matrices,

[source]
....
  P' = T x C x R x SR x S x -SR x -TC x P  (where P is a column vector)
....

The Transform node:

[source]
....
Transform {
    center           C
    rotation         R
    scale            S
    scaleOrientation SR
    translation      T
    children         [...]
}
....

is equivalent to the nested sequence of:

[source]
....
Transform { translation T
 Transform { translation C 
  Transform { rotation R
   Transform { rotation SR 
    Transform { scale S 
     Transform { rotation -SR 
      Transform { translation -C
              ... 
}}}}}}}
 
....


[[Viewpoint]]
==== Viewpoint

[source]
....
Viewpoint {
  eventIn      SFBool     set_bind
  exposedField SFFloat    fieldOfView    0.785398
  exposedField SFBool     jump           TRUE
  exposedField SFRotation orientation    0 0 1  0
  exposedField SFVec3f    position       0 0 10
  field        SFString   description    ""
  eventOut     SFTime     bindTime
  eventOut     SFBool     isBound
}
....

The Viewpoint node defines a specific location in a local coordinate
system from which the user might view the scene. Viewpoints are "<<BindableLeafNodes, Concepts - Bindable Children Nodes>>" 
and thus there exists a Viewpoint stack in the browser in which
the top-most Viewpoint on the stack is the currently active Viewpoint.
If a TRUE value is sent to the _set_bind_ eventIn of a Viewpoint, it is
moved to the top of the Viewpoint stack and thus activated. When a
Viewpoint is at the top of the stack, the user's view is conceptually
re-parented as a child of the Viewpoint. All subsequent changes to the
Viewpoint's coordinate system change the user's view (e.g. changes to
any parent transformation nodes or to the Viewpoint's position or
orientation fields). Sending a _set_bind_ FALSE event removes the
Viewpoint from the stack and results in _isBound_ FALSE and _bindTime_
events. If the popped Viewpoint is at the top of the viewpoint stack the
user's view is re-parented to the next entry in the stack. 
See "<<BindableLeafNodes, Concepts - Bindable Children Nodes>>" 
for more details on the the binding stacks. When a Viewpoint is
moved to the top of the stack, the existing top of stack Viewpoint sends
an _isBound_ FALSE event and is pushed onto the stack.

Viewpoints have the additional requirement from other binding nodes in
that they store the relative transformation from the user view to the
current Viewpoint when they are moved to the top of stack. This is
needed by the _jump_ field, described below.

An author can automatically move the user's view through the world by
binding the user to a Viewpoint and then animating either the Viewpoint
or the transformations above it. Browsers shall allow the user view to
be navigated relative to the coordinate system defined by the Viewpoint
(and the transformations above it), even if the Viewpoint or its parent
transformations are being animated.

The _bindTime_ eventOut sends the time at which the Viewpoint is bound
or unbound. This can happen during loading, when a _set_bind_ event is
sent to the Viewpoint, or when the browser binds to the Viewpoint via
its user interface (see below).

The _position_ and _orientation_ fields of the Viewpoint node specify
relative locations in the local coordinate system. _Position_ is
relative to the coordinate system's origin (0,0,0), while _orientation_
specifies a rotation relative to the default orientation; the default
orientation has the user looking down the -Z axis with +X to the right
and +Y straight up. Viewpoints are affected by the transformation
hierarchy.

Navigation types (see <<NavigationInfo, NavigationInfo>>) that require
a definition of a _down_ vector (e.g. terrain following) shall use the
negative Y-axis of the coordinate system of the currently bound
Viewpoint. Likewise navigation types (see
<<NavigationInfo, NavigationInfo>>) that require a definition of an
_up_ vector shall use the positive Y-axis of the coordinate system of
the currently bound Viewpoint. Note that the _orientation_ field of the
Viewpoint does not affect the definition of the down or up vectors. This
allows the author to separate the viewing direction from the gravity
direction.

The _jump_ field specifies whether the user's view `jumps' (or animates)
to the position and orientation of a bound Viewpoint. Regardless of the
value of _jump_ at bind time, the relative viewing transformation
between the user's view and the current Viewpoint shall be stored with
the current Viewpoint for later use when _un-jumping_. The following is
a re-write of the general bind stack rules described in "<<BindableLeafNodes, Concepts - Bindable Child Nodes, Bind Stack Behavior>>" 
with additional rules regarding
Viewpoints (in *bold*):

. During read:
* the first encountered Viewpoint is bound by pushing it to the top of
the Viewpoint stack,
** nodes contained within <<Inline, Inlines>> are not
candidates for the first encountered Viewpoint,
** the first node within a prototype is a valid candidate for the first
encountered Viewpoint;
* the first encountered Viewpoint sends an _isBound_ TRUE __ event.
. When a _set_bind_ TRUE eventIn is received by a Viewpoint__:__
* if it is not on the top of the stack:
** *the relative transformation from the current top of stack Viewpoint
to the user's view is stored with the current top of stack Viewpoint,*
** the current top of stack node sends an _isBound_ eventOut FALSE,
** the new node is moved to the top of the stack and
becomes the currently bound Viewpoint__,__
** the new Viewpoint (top of stack) sends an _isBound_ TRUE __ eventOut,
** *if _jump_ is TRUE for the new Viewpoint, then the user's view is
`jumped' (or animated) to match the values in the _position_ and
_orientation_ fields of the new Viewpoint;*
* else if the node is already at the top of the stack, then this event
has no affect.
. When a _set_bind_ FALSE eventIn is received by a Viewpoint:
* it is removed from the stack,
* if it is on the top of the stack:
** it sends an _isBound_ eventOut FALSE,
** the next node in the stack becomes the currently bound Viewpoint __
(i.e. pop) __ and issues an _isBound_ TRUE __ eventOut,
** *if its _jump_ is TRUE the user's view is `jumped' (or animated) to
the _position_ and _orientation_ of the next Viewpoint in the stack
with the stored relative transformation for with this next
Viewpoint applied,*
. If a _set_bind_ FALSE eventIn is received by a node not in the stack,
the event is ignored and _isBound_ events are not sent.
. When a node replaces another node at the top of the stack, the
_isBound_ TRUE and FALSE eventOuts from the two nodes are sent
simultaneously (i.e. identical timestamps).
. If a bound node is deleted then it behaves as if it received a
_set_bind_ FALSE event (see #3).

Note that the _jump_ field may change after a Viewpoint is bound - the
rules described above still apply. If _jump_ was TRUE when the Viewpoint
is bound, but changed to FALSE before the _set_bind_ FALSE is sent, then
the Viewpoint does not _un-jump_ during unbind. If _jump_ was FALSE when
the Viewpoint is bound, but changed to TRUE before the _set_bind_ FALSE
is sent, then the Viewpoint does perform the _un-jump_ during unbind.

The _fieldOfView_ field specifies a preferred field of view from this
viewpoint, in radians. A small field of view roughly corresponds to a
telephoto lens; a large field of view roughly corresponds to a
wide-angle lens. The field of view should be greater than zero and
smaller than PI; the default value corresponds to a 45 degree field of
view. The value of _fieldOfView_ represents the maximum viewing angle in
any direction axis of the view. For example, a browser with a
rectangular viewing projection shall use an angle of _fieldOfView_ for
the larger direction (depending on aspect ratio) and _fieldOfView_
times _aspect ratio_ in the smaller direction. If the aspect ratio is 2x1 (i.e. horizontal twice the vertical) and the fieldOfView is 1.0,
then the horizontal viewing angle would be 1.0 and the vertical viewing
angle would be 0.5. _fieldOfView_ is a hint to the browser and may be
ignored.

The _description_ field identifies Viewpoints that are recommended to be
publicly accessible through the browser's user interface (e.g.
Viewpoints menu). The string in the _description_ field should be
displayed if this functionality is implemented. If _description_ is
empty, then the Viewpoint should not appear in any public user
interface. It is recommended that the browser bind and move to a
Viewpoint when its _description_ is selected, either animating to the
new position or jumping directly there. Once the new position is reached
both the _isBound_ and _bindTime_ eventOuts are sent.

The URL syntax ".../scene.wrl#ViewpointName" specifies the user's
initial view when entering "scene.wrl" to be the first Viewpoint in file
"scene.wrl" that appears as "DEF ViewpointName Viewpoint \{ ... }" -
this overrides the first Viewpoint in the file as the initial user view
and receives a _set_bind_ TRUE message. If the Viewpoint "ViewpointName"
is not found, then assume that no Viewpoint was specified and use the
first Viewpoint in the file. The URL syntax "#ViewpointName" specifies a
view within the existing file. If this is loaded, then receives a
_set_bind_ TRUE message.

If a Viewpoint is bound (_set_bind_) and is the child of an LOD, Switch,
or any node or prototype that disables its children, then the result is
undefined. If a Viewpoint is bound that results in collision with
geometry, then the browser performs its self-defined navigation
adjustments as if the user navigated to this point (see
<<Collision, Collision>>).


[[VisibilitySensor]]
==== VisibilitySensor

[source]
....
VisibilitySensor {
  exposedField SFVec3f center   0 0 0
  exposedField SFBool  enabled  TRUE
  exposedField SFVec3f size     0 0 0
  eventOut     SFTime  enterTime
  eventOut     SFTime  exitTime
  eventOut     SFBool  isActive
}
....

The VisibilitySensor detects visibility changes of a rectangular box as
the user navigates the world. VisibilitySensor is typically used to
detect when the user can see a specific object or region in the scene,
and to activate or deactivate some behavior or animation in order to
attract the user or improve performance.

The _enabled_ field enables and disables the VisibilitySensor. If
_enabled_ is set to FALSE, the VisibilitySensor does not send output
events. If _enabled_ is TRUE, then the VisibilitySensor detects changes
to the visibility status of the box specified and sends events through
the _isActive_ eventOut. A TRUE event is output to _isActive_ when any
portion of the box impacts the rendered view, and a FALSE event is sent
when the box has no effect on the view. Browsers shall guarantee that if
_isActive_ is FALSE that the box has absolutely no effect on the
rendered view - browsers may error liberally when _isActive_ is TRUE
(e.g. maybe it does affect the rendering).

The exposed fields _center_ and _size_ specify the object space location
of the box center and the extents of the box (i.e. width, height, and
depth). The VisibilitySensor's box is effected by hierarchical
transformations of its parents.

The _enterTime_ event is generated whenever the _isActive_ TRUE event is
generated, and _exitTime_ events are generated whenever _isActive_ FALSE
events are generated.

Each VisibilitySensor behaves independently of all other
VisibilitySensors - every enabled VisibilitySensor that is affected by
the user's movement receives and sends events, possibly resulting in
multiple VisibilitySensors receiving and sending events simultaneously.
Unlike TouchSensors, there is no notion of a Visibility Sensor lower in
the scene graph "grabbing" events. Instanced (DEF/USE) VisibilitySensors
use the union of all the boxes defined by their instances
to check for enter and exit - an instanced VisibilitySensor will detect
enter, motion, and exit for all instances of the box and send output
events appropriately.


[[WorldInfo]]
==== WorldInfo

[source]
....
WorldInfo {
  field MFString info  []
  field SFString title ""
}
....

The WorldInfo node contains information about the world. This node has
no effect on the visual appearance or behavior of the world - it is
strictly for documentation purposes. The _title_ field is intended to
store the name or title of the world so that browsers can present this
to the user - for instance, in their window border. Any other
information about the world can be stored in the _info_ field - for
instance, the scene author, copyright information, and public domain
information.

Contact rikk@best.com link:rikk@best.com[] , cmarrin@sgi.com, or gavin@acm.org with questions or comments.


[[conformance_html]]
=== 6. Field and Event Reference

6.1 <<Introduction, Introduction>> +
6.2 <<SFBool, SFBool>> +
6.3 <<SFColor, SFColor and MF Color>> +
6.4 <<SFFloat, SFFloat and MFFloat>> +
6.5 <<SFImage, SFImage>> +
6.6 <<SFInt32, SFInt32 and MFInt32>> +
6.7 <<SFNode, SFNode and MFNode>> +
6.8 <<SFRotation, SFRotation and SFRotation>> +
6.9 <<SFString, SFString and MFString>> +
6.10 <<SFTime, SFTime>> +
6.11 <<SFVec2f, SFVec2f and MFVec2f>> +
6.12 <<SFVec3f, SFVec3f and MFVec3f>> +


[[6introduction]]
==== 6.1 Introduction

This annex describes the syntax and general semantics of _fields_ and
_events,_ the elemental data types used by VRML nodes to define objects
(see "<<nodesRef_html, Node Reference>>"). Nodes are composed of fields 
and events ( see "<<NFE, Concepts - Nodes, Fields, and Events>>"). 
The types defined in this annex are used by
both fields and events.

There are two general classes of fields and events; fields/events that
contain a single value (where a value may be a single number, a vector,
or even an image), and fields/events that contain multiple values.
Single-valued fields/events have names that begin with *`+SF.+`*
Multiple-valued fields/events have names that begin with *`MF`*.

Multiple-valued fields/events are written as a series of values enclosed
in square brackets, and separated by whitespace (e.g. commas). If the
field or event has zero values then only the square brackets ("[ ]") are
written. The last value may optionally be followed by whitespace (e.g.
comma). If the field has exactly one value, the brackets may be omitted
and just the value written. For example, all of the following are valid
for a multiple-valued MFInt32 field named _foo_ containing the single
integer value 1:

[source]
....
   foo 1
   foo [1,]
   foo [ 1 ]
....


[[SFBool]]
==== 6.2 SFBool

A field or event containing a single boolean value. SFBools are written
as *TRUE* or *FALSE*. For example,

[source]
....
    fooBool FALSE
....

is an SFBool field, _fooBool_, defining a FALSE value.

The initial value of an SFBool eventOut is FALSE.


[[SFColor]]
[[MFColor]]
==== 6.3 SFColor/MFColor

SFColor specifies one RGB (red-green-blue) color triple, and MFColor
specifies zero or more RGB triples. Each color is written to file as an
RGB triple of floating point numbers in ANSI C floating point format, in
the range 0.0 to 1.0. For example:

[source]
....
   fooColor [ 1.0 0. 0.0, 0 1 0, 0 0 1 ]
....

is an MFColor field, _fooColor_, containing the three primary colors
red, green, and blue.

The initial value of an SFColor eventOut is (0 0 0). The initial value
of an MFColor eventOut is [ ].

[[SFFloat]]
[[MFFloat]]
==== 6.4 SFFloat/MFFloat

SFFloat specifies one single-precision floating point number, and
MFFloat specifies zero or more single-precision floating point numbers.
SFFloats and MFFloats are written to file in ANSI C floating point
format. For example:

[source]
....
    fooFloat [ 3.1415926, 12.5e-3, .0001 ]
....

is an MFFloat field, _fooFloat_, containing three floating point values
values.

The initial value of an SFFloat eventOut is 0.0. The initial value of an
MFFloat eventOut is [ ].

[[SFImage]]
==== 6.5 SFImage

The SFImage field or event defines a single uncompressed 2-dimensional
pixel image. SFImage fields and events are written to file as three
integers representing the width, height and number of components in the
image, followed by width*height hexadecimal values representing the
pixels in the image, separated by whitespace:

[source]
....
     fooImage <width> <height> <num components> <pixels values>
....

A one-component image specifies one-byte hexadecimal values representing
the intensity of the image. For example, `0xFF` is full intensity,
`0x00` is no intensity. A two-component image puts the intensity in
the first (high) byte and the alpha (opacity) in the second (low) byte.
Pixels in a three-component image have the red component in the first
(high) byte, followed by the green and blue components (`0xFF0000` is
red). Four-component images put the alpha byte after red/green/blue
(`0x0000FF80` is semi-transparent blue). A value of `0x00` is
completely transparent, 0xFF is completely opaque.

Each pixel is read as a single unsigned number. For example, a
3-component pixel with value `0x0000FF` may also be written as
`0xFF` or `+255 +`(decimal). Pixels are specified from left to right,
bottom to top. The first hexadecimal value is the lower left pixel and
the last value is the upper right pixel.

For example,

[source]
....
    fooImage 1 2 1 0xFF 0x00
....

is a 1 pixel wide by 2 pixel high one-component (i.e. greyscale) image,
with the bottom pixel white and the top pixel black. And:

[source]
....
   fooImage 2 4 3 0xFF0000 0xFF00 0 0 0 0 0xFFFFFF 0xFFFF00
                  # red    green  black.. white    yellow
....

is a 2 pixel wide by 4 pixel high RGB image, with the bottom left pixel
red, the bottom right pixel green, the two middle rows of pixels black,
the top left pixel white, and the top right pixel yellow.

The initial value of an SFImage eventOut is (0 0 0).


[[SFInt32]]
[[MFInt32]]
==== 6.6 SFInt32/MFInt32

The SFInt32 field and event specifies one 32-bit integer, and the
MFInt32 field and event specifies zero or more 32-bit integers. SFInt32
and MFInt32 fields and events are written to file as an integer in
decimal or hexadecimal (beginning with '0x') format. For example:

[source]
....
    fooInt32 [ 17, -0xE20, -518820 ]
....

is an MFInt32 field containing three values.

The initial value of an SFInt32 eventOut is 0. The initial value of an
MFInt32 eventOut is [ ].

[[SFNode]]
[[MFNode]]
==== 6.7 SFNode/MFNode

The SFNode field and event specifies a VRML node, and the MFNode field
and event specifies zero or more nodes. The following example
illustrates valid syntax for an MFNode field, _fooNode_, defining four
nodes:

[source]
....
    fooNode [ Transform { translation 1 0 0 }
              DEF CUBE Box { }
              USE CUBE
              USE SOME_OTHER_NODE ]
....

The SFNode and MFNode fields and events may contain the keyword NULL to
indicate that it is empty.

The initial value of an SFNode eventOut is NULL. The initial value of an
MFNode eventOut is [ ].

[[SFRotation]]
[[MFRotation]]
==== 6.8 SFRotation/MFRotation

The SFRotation field and event specifies one arbitrary rotation, and the
MFRotation field and event specifies zero or more arbitrary rotations.
S/MFRotations are written to file as four floating point values
separated by whitespace. The first three values specify a normalized
rotation axis vector about which the rotation takes place. The fourth
value specifies the amount of right-handed rotation about that axis, in
radians. For example, an SFRotation containing a 180 degree rotation
about the Y axis is:

[source]
....
    fooRot 0.0 1.0 0.0  3.14159265
....

The initial value of an SFRotation eventOut is (0 0 1 0). The initial
value of an MFRotation eventOut is [ ].

[[SFString]]
[[MFString]]
==== 6.9 SFString/MFString

The SFString and MFString fields and events contain strings formatted
with the UTF-8 universal character set
(http://www.iso.ch/cate/d18741.html[ISO/IEC 10646-1:1993],
http://www.iso.ch/cate/d18741.html). SFString specifies a single string,
and the MFString specifies zero or more strings. Strings are written to
file as a sequence of UTF-8 octets enclosed in double quotes (e.g.
`+"string"+`).

Due to the drastic changes in Korean Jamo language, the character set of
the UTF-8 will be based on ISO 10646-1:1993 plus pDAM 1 - 5 (including
the Korean changes). The text strings are stored in visual order.

Any characters (including newlines and '#') may appear within the
quotes. To include a double quote character within the string, precede
it with a backslash. To include a backslash character within the string,
type two backslashes. For example:

[source]
....
    fooString [ "One, Two, Three", "He said, \"Immel did it!\"" ]
....

is a MFString field, _fooString_, with two valid strings.

The initial value of an SFString eventOut is "". The initial value of an
MFRotation eventOut is [ ].

[[SFTime]]
[[MFTime]]
==== 6.10 SFTime/MFTime

The SFTIme field and event specifies a single time value, and the MFTime
field and event specifies zero or more time values. Time values are
written to file as a double-precision floating point number in ANSI C
floating point format. Time values are specified as the number of
seconds from a specific time origin. Typically, SFTime fields and events
represent the number of seconds since Jan 1, 1970, 00:00:00 GMT.

The initial value of an SFTime eventOut is -1. The initial value of an
MFTime eventOut is [ ].

[[SFVec2f]]
[[MFVec2f]]
==== 6.11 SFVec2f/MFVec2f

An SFVec2f field or event specifies a two-dimensional vector. An MFVec2f
field or event specifies zero or more two-dimensional vectors. SFVec2fs
and MFVec2fs are written to file as a pair of floating point values
separated by whitespace. For example:

[source]
....
    fooVec2f [ 42 666, 7, 94 ]
....

is a MFVec2f field, _fooVec2f_, with two valid vectors.

The initial value of an SFVec2f eventOut is (0 0). The initial value of
an MFVec2f eventOut is [ ].

[[SFVec3f]]
[[MFVec3f]]
==== 6.12 SFVec3f/MFVec3f

An SFVec3f field or event specifies a three-dimensional vector. An
MFVec3f field or event specifies zero or more three-dimensional vectors.
SFVec3fs and MFVec3fs are written to file as three floating point values
separated by whitespace. For example:

[source]
....
    fooVec3f [ 1 42 666, 7, 94, 0 ]
....

is a MFVec3f field, _fooVec3f_, with two valid vectors.

The initial value of an SFVec3f eventOut is (0 0 0). The initial value
of an MFVec3f eventOut is [ ].

Contact rikk@best.com , cmarrin@sgi.com, or gavin@acm.org with questions or comments.


[[fieldsRef_html]]
=== 7. Conformance and Minimum Support Requirements

• 7.1 <<Introduction, Introduction>>
• 7.2 <<Conformance, Conformance>>
• 7.3 <<MinimumSupportRequirements, Minimum support requirements>>


[[Introduction]]
==== 7.1 Introduction

===== 7.1.1 Objectives

This clause provides rules for identifying conforming generators and
interpreters of ISO/IEC 14772 along with specifications as to the
minimum level of complexity which must be supported.

The primary objectives of these rules are:

. to promote interoperability by eliminating arbitrary subsets of, or
extensions to, ISO/IEC 14772;
. to promote uniformity in the development of conformance tests;
. to facilitate automated test generation.

===== 7.1.2 Scope

This clause provides conformance criteria for metafiles, metafile
generators, and metafile interpreters.

This clause addresses the VRML data stream and implementation
requirements. Implementation requirements address the latitude allowed
by VRML generators and interpreters. This clause does not directly
address the environmental, performance, or resource requirements of the
generator or interpreter.

This clause does not define the application requirements or dictate
application functional content within a VRML file.

The scope of this clause is limited to rules for the open interchange of
VRML content.


[[Conformance]]
==== 7.2 Conformance

===== 7.2.1 Conformance of metafiles

Conformance of metafiles to ISO/IEC 14772 is defined in terms of the
functionality and form specified in Part 1. In order to conform to
ISO/IEC 14772, a metafile shall be a syntactically correct metafile.

A metafile is a syntactically correct version of ISO/IEC 14772 if the
following conditions are met:

. The metafile contains as its first element a VRML header comment node;
. All nodes contained therein match the functional specification of the
corresponding nodes of ISO/IEC 14772-1. The metafile shall obey the
relationships defined in the formal grammar and all other syntactic
requirements.
. The sequence of nodes in the metafile obeys the relationships
specified in ISO/IEC 14772-1 producing the structure specified in
ISO/IEC 14772-1. For example, ...
. No nodes appear in the metafile other than those specified in ISO/IEC
14772-1 unless required for the encoding technique. All nodes not
defined in ISO/IEC 14772-1 are encoded using the PROTO or EXTERNPROTO
nodes.
. The metafile is encoded according to the rules in the standard clear
text encoding in ISO/IEC 14772-1 or such other encodings that are
standardized.

===== 7.2.2 Conformance of metafile generators

Conformance of metafile generators is defined in terms of conformance to
the functionality defined in ISO/IEC 14772-1. A metafile generator which
conforms to ISO/IEC 14772 shall:

. generate no syntax in violation of ISO/IEC 14772;
. generate metafiles which conform to ISO/IEC 14772;
. map the graphical characteristics of application pictures onto a set
of VRML nodes which define those pictures within the latitude allowed in
ISO/IEC 14772.

===== 7.2.3 Conformance of metafile interpreters

Conformance of metafile interpreters is defined in terms of the
functionality in ISO/IEC 14772. A metafile interpreter which conforms to
ISO/IEC 14772 shall:

. be able to read any metafile which conforms to ISO/IEC 14772;
. render the graphical characteristics of the VRML nodes in any such
metafile into a graphical image or picture within the latitude defined
in ISO/IEC 14772.


[[MinimumSupportRequirements]]
==== 7.3 Minimum support requirements

===== 7.3.1 Minimum support requirements for generators

There is no minimum complexity which must be supported by a conforming
VRML generator except that the file must contain the required VRML
header. Any compliant set of nodes may be generated of arbitrary
complexity.

===== 7.3.2 Minimum support requirements for interpreters

This subclause defines the minimum complexity which must be supported by
a VRML interpreter. Interpreter implementations may choose to support
greater limits but may not reduce the limits described in Table 7-1.
When the metafile being interpreted contains nodes which exceed the
latitude implemented by the interpreter, the interpreter will attempt to
skip that node and continue at the next node. Where latitude is
specified in this table for a particular node, full support is required
for other aspects of that node.

.Table 7-1: Minimum support criteria for VRML interpreters
[width="100%",cols="50%,50%",]
|===
|*Node*     |*Minimum support*

|All groups              |At least 512 children. Ignore bboxCenter and bboxSize
|All interpolators       |At least first 256 key-value pairs interpreted
|All lights              |At least 8 simultaneous lights.
|All strings             |At least 255 characters per string
|All URL fields          |At least 16 URL's per field
|Transformation stack    |At least 32 levels in the transformation stack
|Anchor                  |Ignore parameters. Ignore description
|Appearance              |Full support
|AudioClip               |Ignore description. At least 30 seconds duration.
                          Wavefile in uncompressed PCM format
|Background              |Full support
|Billboard               |Full support except as for all groups
|Box                     |Full support
|Collision               |Full support except as for all groups
|Color                   |Full support
|ColorInterpolator       |Full support except as for all interpolators
|Cone                    |Full support
|Coordinate              |At least first 16384 coordinates per coordinate node
                          supported with indices to others ignored
|CoordinateInterpolator  |Full support except as for all interpolators
|Cylinder                |Full support
|CylinderSensor          |Full support except as for all interpolators
|DirectionalLight        |Global application of light source
|ElevationGrid           |At least 16384 heights per grid
|Extrusion               |At least 64 joints per extrusion. At least 1024 vertices in cross-section.
|Fog                     |Full support
|FontStyle               |If non-Latin characters, family can be ignored.
|Group                   |Full support except as for all groups
|ImageTexture            |Point sampling. At least JPEG and PNG formats
|IndexedFaceSet          |At least 1024 vertices per face. At least 1024 faces. Ignore ccw. Ignore convex. Ignore solid.
|IndexedLineSet          |At least 1024 vertices per polyline At least 1024 polylines per set
|Inline                  |Full support except as for all groups
|LOD                     |At least first 4 level/range combinations shall be interpreted
|Material                |Ignore ambient intensity/specular colour/emissive colour.
                          At least transparent and opaque with values less than 0.5 opaque
|MovieTexture            |At least one simultaneously active movie texture.
                          At least MPEG1-Systems and MPEG1-Video.
|NavigationInfo          |Ignore avatarSize.
                          Ignore types other than "WALK", "FLY", "EXAMINE", and "NONE".
                          Ignore visibilityLimit
|Normal                  |At least first 16384 normals per normal node supported with
                          indices to others ignored
|NormalInterpolator      |Full support except as for all interpolators
|OrientationInterpolator |Full support except as for all interpolators
|PixelTexture            |At least 256256 image size
|PlaneSensor             |Full support
|PointLight              |Full support
|PointSet                |At least 4096 points per point set
|PositionInterpolator    |Full support except as for all interpolators
|ProximitySensor         |Full support
|ScalarInterpolator      |Full support except as for all interpolators
|Script                  |At least 32 eventIns. At least 32 fields. At least 32 eventOuts.
|Shape                   |Full support
|Sound                   |At least 3 simultaneously active sounds.
                          At least linear sound attenuation between inner and outer ellipsoids.
                          At least spatialization across the panorama being viewed.
                          At least 2 priorities
|Sphere                  |Full support
|SphereSensor            |Full support
|SpotLight               |Beam width can be ignored
|Switch                  |Full support except as for all groups
|Text                    |At least UTF-8 character encoding transformation format
|TextureCoordinate       |At least first 16384 texture coordinates per texture
                          coordinate node supported with indices to others ignored
|TextureTransform        |Full support
|TimeSensor              |Full support
|TouchSensor             |Full support
|Transform               |Full support except as for all groups
|Viewpoint               |Ignore fieldOfView. Ignore description
|VisibilitySensor        |Full support
|WorldInfo               |Full support
|===


Contact rikk@best.com, cmarrin@sgi.com, or gavin@acm.org with questions or comments.


[[grammar_html]]
=== Appendix A. Grammar Definition

This section provides a detailed description of the grammar for each
node in VRML 2.0. There are four sections:
<<Introduction, Introduction>>, <<General, General>>,
<<Nodes, Nodes>>, and <<Fields, Fields>>.


[[a1_Introduction]]
==== A.1 Introduction

VRML grammar is ambiguous; semantic knowledge of the names and types of
fields, eventIns, and eventOuts for each node type (either builtIn or
user-defined using *PROTO* or *EXTERNROTO*) must be used during parsing
so that the parser knows which field type is being parsed.

The '#' (0x23) character begins a comment wherever it appears outside of
quoted SFString or MFString fields. The '#' character and all characters
until the next carriage-return or newline make up the comment and are
treated as whitespace.

The carriage return (0x0d), newline (0x0a), space (0x20), tab (0x09),
and comma (0x2c) characters are whitespace characters wherever they
appear outside of quoted SFString or MFString fields. Any number of
whitespace characters and comments may be used to separate the syntactic
entities of a VRML file.

Please see the <<nodesRef_html, Nodes Reference>> section of the Moving
Worlds specification for a description of the allowed fields, eventIns
and eventOuts for all pre-defined node types. Also note that some of the
basic types that will typically be handled by a lexical analyzer
(_sffloatValue_, _sftimeValue_, _sfint32Value_, and _sfstringValue_)
have not been formally specified; please see the
<<fieldsRef_html, Fields Reference>> section of the spec for a more
complete description of their syntax.


[[General]]
==== A.2 General

_vrmlScene:_::
  _declarations_

_declarations:_::
  _declaration_ +
  _declaration declarations_

_declaration:_::
  _nodeDeclaration_ +
  _protoDeclaration_ +
  _routeDeclaration_ +
  *NULL*

_nodeDeclaration:_::
  _node_ +
  *DEF* _nodeNameId node_ +
  *USE* _nodeNameId_

_protoDeclaration:_::
  _proto_ +
  _externproto_

_proto:_::
  *PROTO* _nodeTypeId_ *[* _interface_declarations_ *] \{* _vrmlScene_
  *}*

_interfaceDeclarations:_::
  _interfaceDeclaration_ +
  _interfaceDeclaration interfaceDeclarations_

_restrictedInterfaceDeclaration:_::
  *eventIn* _fieldType_ _eventInId_ +
  *eventOut* _fieldType eventOutId_ +
  *field* _fieldType fieldId fieldValue_

_interfaceDeclaration:_::
  _restrictedInterfaceDeclaration_ +
  *exposedField* _fieldType fieldId fieldValue_

_externproto:_::
  *EXTERNPROTO* _nodeTypeId_ *[* _externInterfaceDeclarations_ *]*
  _mfstringValue_

_externInterfaceDeclarations:_::
  _externInterfaceDeclaration_ +
  _externInterfaceDeclaration externInterfaceDeclarations_

_externInterfaceDeclaration:_::
  *eventIn* _fieldType_ _eventInId_ +
  *eventOut* _fieldType eventOutId_ +
  *field* _fieldType fieldId_ +
  *exposedField* _fieldType fieldId_

_routeDeclaration:_::
  *ROUTE* _nodeNameId_ *.* _eventOutId_ *TO* _nodeNameId_ *.*
  _eventInId_


[[Nodes]]
==== A.3 Nodes

_node:_::
  _nodeTypeId_ *\{* _nodeGuts_ *}* +
  *Script \{* _scriptGuts_ *}*

_nodeGuts:_::
  _nodeGut_ +
  _nodeGut nodeGuts_

_scriptGuts:_::
  _scriptGut_ +
  _scriptGut scriptGuts_

_scriptGut:_::
  _nodeGut_ +
  _restrictedInterfaceDeclaration_ +
  *eventIn* _fieldType_ _eventInId_ *IS* _eventInId_ +
  *eventOut* _fieldType eventOutId_ *IS* _eventOutId_ +
  *field* _fieldType fieldId_ *IS* _fieldId_

_nodeGut:_::
  _fieldId fieldValue_ +
  _fieldId_ *IS* _fieldId_ +
  _eventInId_ *IS* _eventInId_ +
  _eventOutId_ *IS* _eventOutId_ +
  _routeDeclaration_ +
  _protoDeclaration_

_nodeNameId:_::
  _Id_

_nodeTypeId:_::
  _Id_

_fieldId:_::
  _Id_

_eventInId:_::
  _Id_

_eventOutId:_::
  _Id_

_Id:_::
  _IdFirstChar_ +
  _IdFirstChar IdRestChars_

_IdFirstChar:_::
  Any ISO-10646 character encoded using UTF-8 except: 0x30-0x39,
  0x0-0x20, 0x22, 0x23, 0x27, 0x2c, 0x2e, 0x5b, 0x5c, 0x5d, 0x7b, 0x7d.

_IdRestChars:_::
  Any number of ISO-10646 characters except: 0x0-0x20, 0x22, 0x23, 0x27,
  0x2c, 0x2e, 0x5b, 0x5c, 0x5d, 0x7b, 0x7d.


[[Fields]]
==== A.4 Fields

_fieldType:_::
  *MFColor* +
  *MFFloat* +
  *MFInt32* +
  *MFNode* +
  *MFRotation* +
  *MFString* +
  *MFVec2f* +
  *MFVec3f* +
  *SFBool* +
  *SFColor* +
  *SFFloat* +
  *SFImage* +
  *SFInt32* +
  *SFNode* +
  *SFRotation* +
  *SFString* +
  *SFTime* +
  *SFVec2f* +
  *SFVec3f*

_fieldValue:_::
  _sfboolValue_ +
  _sfcolorValue_ +
  _sffloatValue_ +
  _sfimageValue_ +
  _sfint32Value_ +
  _sfnodeValue_ +
  _sfrotationValue_ +
  _sfstringValue_ +
  _sftimeValue_ +
  _sfvec2fValue_ +
  _sfvec3fValue_ +
  _mfcolorValue_ +
  _mffloatValue_ +
  _mfint32Value_ +
  _mfnodeValue_ +
  _mfrotationValue_ +
  _mfstringValue_ +
  _mfvec2fValue_ +
  _mfvec3fValue_

_sfboolValue:_::
  *TRUE* +
  *FALSE*

_sfcolorValue:_::
  _float float float_

_sffloatValue:_::
  ... floating point number in ANSI C floating point format...

_sfimageValue:_::
  _int32 int32 int32 int32s..._

_sfint32Value:_::
  *_[0-9]+_* +
  *0x__[0-9A-F]+__*

_sfnodeValue:_::
  _nodeDeclaration_ +
  *NULL*

_sfrotationValue:_::
  _float float float float_

_sfstringValue:_::
  *"_.*_"* ... double-quotes must be \", backslashes must be \\...

_sftimeValue:_::
  ... double-precision number in ANSI C floating point format...

_sfvec2fValue:_::
  _float float_

_sfvec3fValue:_::
  _float float float_

_mfcolorValue:_::
  _sfcolorValue_ +
  *[ ]* +
  *[* _sfcolorValues_ *]*

_sfcolorValues:_::
  _sfcolorValue_ +
  _sfcolorValue sfcolorValues_

_mffloatValue:_::
  _sffloatValue_ +
  *[ ]* +
  *[* _sffloatValues_ *]*

_sffloatValues:_::
  _sffloatValue_ +
  _sffloatValue_ ** _sffloatValues_

_mfint32Value:_::
  _sfint32Value_ +
  *[ ]* +
  *[* _sfint32Values_ *]*

_sfint32Values:_::
  _sfint32Value_ +
  _sfint32Value sfint32Values_

_mfnodeValue:_::
  _nodeDeclaration_ +
  *[ ]* +
  *[* _nodeDeclarations_ *]*

_nodeDeclarations:_::
  _nodeDeclaration_ +
  _nodeDeclaration nodeDeclarations_

_mfrotationValue:_::
  _sfrotationValue_ +
  *[ ]* +
  *[* _sfrotationValues_ *]*

_sfrotationValues:_::
  _sfrotationValue_ +
  _sfrotationValue sfrotationValues_

_mfstringValue:_::
  _sfstringValue_ +
  *[ ]* +
  *[* _sfstringValues_ *]*

_sfstringValues:_::
  _sfstringValue_ +
  _sfstringValue sfstringValues_

_mfvec2fValue:_::
  _sfvec2fValue_ +
  *[ ]* +
  *[* _sfvec2fValues_*]*

_sfvec2fValues:_::
  _sfvec2fValue_ +
  _sfvec2fValue sfvec2fValues_

_mfvec3fValue:_::
  _sfvec3fValue_ +
  *[ ]* +
  *[* _sfvec3fValues_ *]*

_sfvec3fValues:_::
  _sfvec3fValue_ +
  _sfvec3fValue_  _sfvec3fValues_


Contact rikk@best.com, cmarrin@sgi.com, or gavin@acm.org with questions or comments.

[[examples_html]]
=== Appendix B. Examples

This appendix provides a variety of examples of VRML 2.0.


==== Simple example: "red sphere meets blue box"

This file contains a simple scene defining a view of a red sphere and a
blue box, lit by a directional light:

[source]
....
#VRML V2.0 utf8
Transform {
  children [
    NavigationInfo { headlight FALSE } # We'll add our own light

    DirectionalLight {        # First child
        direction 0 0 -1      # Light illuminating the scene
    }

    Transform {               # Second child - a red sphere
      translation 3 0 1
      children [
        Shape {
          geometry Sphere { radius 2.3 }
          appearance Appearance {
            material Material { diffuseColor 1 0 0 }   # Red
         }
        }
      ]
    }

    Transform {               # Third child - a blue box 
      translation -2.4 .2 1
      rotation     0 1 1  .9
      children [
        Shape {
          geometry Box {}
          appearance Appearance {
            material Material { diffuseColor 0 0 1 }  # Blue
         }
        }
      ]
    }

  ] # end of children for world
}
....


==== Instancing (Sharing)

Reading the following file results in three spheres being drawn. The
first sphere defines a unit sphere at the original named "Joe", the
second sphere defines a smaller sphere translated along the +x axis, the
third sphere is a reference to the second sphere and is translated along
the -x axis. If any changes occur to the second sphere (e.g. radius
changes), then the third sphere, (which is not really a reference to the
second) will change too:

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/instancing.jpg[]


[source]
....
#VRML V2.0 utf8
Transform {
  children [
    DEF Joe Shape { geometry Sphere {} }
    Transform {
      translation 2 0 0
      children    DEF Joe Shape { geometry Sphere { radius .2 } }
    }
    Transform {
      translation -2 0 0
      children    USE Joe 
    }

  ]
}
....

(Note that the spheres are unlit because no appearance was specified.)


==== Prototype example

A simple chair with variable colors for the leg and seat might be
prototyped as:

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/proto.jpg[]


[source]
....
#VRML V2.0 utf8

PROTO TwoColorStool [ field SFColor legColor  .8 .4 .7
                      field SFColor seatColor .6 .6 .1 ]
{
  Transform {
    children [
      Transform {   # stool seat
       translation 0 0.6 0
        children
          Shape {
            appearance Appearance {
              material Material { diffuseColor IS seatColor }
            }
            geometry Box { size 1.2 0.2 1.2 }
          }
      }

      Transform {   # first stool leg
       translation -.5 0 -.5
        children
          DEF Leg Shape {
            appearance Appearance {
              material Material { diffuseColor IS legColor }
            }
            geometry Cylinder { height 1 radius .1 }
          }
      }
      Transform {   # another stool leg
       translation .5 0 -.5
        children USE Leg
      }
      Transform {   # another stool leg
       translation -.5 0 .5
        children USE Leg
      }
      Transform {   # another stool leg
       translation .5 0 .5
        children USE Leg
      }
    ] # End of root Transform's children
  } # End of root Transform
} # End of prototype

# The prototype is now defined. Although it contains a number of nodes,
# only the legColor and seatColor fields are public. Instead of using the
# default legColor and seatColor, this instance of the stool has red legs
# and a green seat:


TwoColorStool {
  legColor 1 0 0 seatColor 0 1 0
}
NavigationInfo { type "EXAMINE" }      # Use the Examine viewer
....


==== Scripting Example

This Script node decides whether or not to open a bank vault given
openVault and combinationEntered messages To do this it remembers
whether or not the correct combination has been entered:

[source]
....
DEF OpenVault Script {
    # Declarations of what's in this Script node:
    eventIn SFTime  openVault
    eventIn SFBool  combinationEntered
    eventOut SFTime vaultUnlocked
    field SFBool    unlocked FALSE

    # Implementation of the logic:
    url "javascript:
        function combinationEntered(value) { unlocked = value; }
        function openVault(value) {
            if (unlocked) vaultUnlocked = value;
        }"
}
....

Note that the _openVault_ eventIn and the _vaultUnlocked_ eventOut are
or type SFTime. This is so they can be wired directly to a TouchSensor
and TimeSensor, respectively. The TimeSensor can output into an
interpolator which performs an opening door animation.


==== Geometric Properties

For example, the following IndexedFaceSet (contained in a Shape node)
uses all four of the geometric property nodes to specify vertex
coordinates, colors per vertex, normals per vertex, and texture
coordinates per vertex (note that the material sets the overall
transparency):

[source]
....
Shape {
  geometry IndexedFaceSet {
     coordIndex  [ 0, 1, 3, -1, 0, 2, 5, -1, ...]
     coord       Coordinate        { point [0.0 5.0 3.0, ...] }
     color       Color             { rgb [ 0.2 0.7 0.8, ...] }
     normal      Normal            { vector [0.0 1.0 0.0, ...] }
     texCoord    TextureCoordinate { point [0 1.0, ...] }
  }
  appearance Appearance { material Material { transparency 0.5 } }
}
....


==== Transforms and Leaves

This example has 2 parts. First is an example of a simple VRML 1.0
scene. It contains a red cone, a blue sphere, and a green cylinder with
a hierarchical transformation structure. Next is the same example using
the Moving Worlds Transforms and leaves syntax.

image:http://www.graphics.stanford.edu/courses/cs248-98-fall/Assignments/Assignment3/VRML2_Specification/spec/Images/leaves.jpg[]

===== VRML 1.0


[source]
....
#VRML V1.0 ascii
Separator {
    Transform {
        translation 0 2 0
    }
    Material {
        diffuseColor 1 0 0
    }
    Cone { }

    Separator {
        Transform {
            scaleFactor 2 2 2
        }
        Material {
            diffuseColor 0 0 1
        }
        Sphere { }

        Transform {
            translation 2 0 0
        }
        Material {
            diffuseColor 0 1 0
        }
        Cylinder { }
    }
}
....

===== VRML 2.0


[source]
....
#VRML V2.0 utf8
Transform {
    translation 0 2 0
    children [
        Shape {
            appearance Appearance {
                material Material { 
                    diffuseColor 1 0 0 
                }
            }
            geometry Cone { }
        },

        Transform {
            scale 2 2 2
            children [
                Shape {
                    appearance Appearance {
                        material Material { 
                            diffuseColor 0 0 1 
                        }
                    }
                    geometry Sphere { }
                },

                Transform {
                    translation 2 0 0
                    children [
                        Shape {
                            appearance Appearance {
                                material Material { 
                                    diffuseColor 0 1 0
                                }
                            }
                            geometry Cylinder { }
                        }
                    ]
                }
            ]
        }
    ]
}
....

Note that the default Viewpoint will not have the objects centered in
the view as shown above.


==== Transform: VRML 1.0 vs. VRML 2.0

Here is an example that illustrates the order in which the elements of a
Transform are applied:

[source]
....
Transform {
    translation T1
    rotation R1
    scale S
    scaleOrientation R2
    center T2
    ...
}
....

is equivalent to the nested sequence of:

[source]
....
Transform { translation T1 
 children [ Transform { translation T2 
  children [ Transform { rotation R1
   children [ Transform { rotation R2 
    children [ Transform { scale S 
     children [ Transform { rotation -R2 
      children [ Transform { translation -T2
              ... 
       }
      ]}
     ]}
    ]}
   ]}
  ]}
 ]
}
....


==== Prototypes and Alternate Representations

Moving Worlds has the capability to define new nodes. VRML 1.0 had the
ability to add nodes using the _fields_ field and _isA_ keyword. The
prototype feature can duplicate all the features of the 1.0 node
definition capabilities, as well as the alternate representation feature
proposed in the VRML 1.1 draft spec. Take the example of a
RefractiveMaterial. This is just like a Material node but adds an
indexOfRefraction field. This field can be ignored if the browser cannot
render refraction. In VRML 1.0 this would be written like this:

[source]
....
...
RefractiveMaterial {
    fields [ SFColor ambientColor,  MFColor diffuseColor, 
             SFColor specularColor, MFColor emissiveColor,
             SFFloat shininess,     MFFloat transparency,
             SFFloat indexOfRefraction, MFString isA ]

    isA "Material"
}
....

If the browser had been hardcoded to understand a RefractiveMaterial the
indexOfRefraction would be used, otherwise it would be ignored and
RefractiveMaterial would behave just like a Material node.

In VRML 2.0 this is written like this:

[source]
....
...
PROTO RefractiveMaterial [ 
            field SFFloat ambientIntensity  0 0 0
            field MFColor diffuseColor      0.5 0.5 0.5
            field SFColor specularColor     0 0 0
            field MFColor emissiveColor     0 0 0
            field SFFloat shininess         0
            field MFFloat transparency      0 0 0
            field SFFloat indexOfRefraction 0.1 ]
{
    Material {
            ambientIntensity  IS ambientIntensity
            diffuseColor      IS diffuseColor
            specularColor     IS specularColor
            emissiveColor     IS emissiveColor
            shininess         IS shininess
            transparency      IS transparency
    }
}
....

While this is more wordy, notice that the default values were given in
the prototype. These are different than the defaults for the standard
Material. So this allows you to change defaults on a standard node. The
EXTERNPROTO capability allows the use of alternative implementations of
a node:

[source]
....
...
EXTERNPROTO RefractiveMaterial [
            field SFFloat ambientIntensity
            field MFColor diffuseColor
            field SFColor specularColor
            field MFColor emissiveColor
            field SFFloat shininess
            field MFFloat transparency
            field SFFloat indexOfRefraction ]

    http://www.myCompany.com/vrmlNodes/RefractiveMaterial.wrl,
    http://somewhere.else/MyRefractiveMaterial.wrl
....

This will choose from one of three possible sources of
RefractiveMaterial. If the browser has this node hardcoded, it will be
used. Otherwise the first URL will be requested and a prototype of the
node will used from there. If that fails, the second will be tried.


==== Anchor

The _target_ parameter can be used by the anchor node to send a request
to load a URL into another frame:

[source]
....
Anchor {
  url "http://somehost/somefile.html"
  parameters [ "target=name_of_frame" ]
  ...
}
....

An Anchor may be used to bind the viewer to a particular _viewpoint_ in
a virtual world by specifying a URL ending with "#viewpointName", where
"viewpointName" is the DEF name of a viewpoint defined in the world. For
example:

[source]
....
Anchor {
  url "http://www.school.edu/vrml/someScene.wrl#OverView"
  children Shape { geometry Box {} }
}
....

specifies an anchor that puts the viewer in the "someScene" world bound
to the viewpoint named "OverView" when the Box is chosen (note that
"OverView" is the name of the viewpoint, not the value of the
viewpoint's description field). If no world is specified, then the
current scene is implied; for example:

[source]
....
Anchor {
  url "#Doorway"
  children Shape { Sphere {} }
}
....

binds you to the Viewpoint with the DEF name "Doorway" in the current
scene.


==== Directional Light

A directional light source illuminates only the objects in its enclosing
grouping node. The light illuminates everything within this coordinate
system, including the objects that precede it in the scene graph--for
example:

[source]
....
Transform {
  children [
    DEF UnlitShapeOne Shape { ... }
    DEF LitParent Transform {
      children [
        DEF LitShapeOne Shape { ... }
        DirectionalLight { .... } # lights the shapes under LitParent
        DEF LitShapeTwo Shape { ... }
      ]
    }
    DEF UnlitShapeTwo Shape { ... }
  ]
}
....


==== PointSet

This simple example defines a PointSet composed of 3 points. The first
point is red (1 0 0), the second point is green (0 1 0), and the third
point is blue (0 0 1). The second PointSet instances the Coordinate node
defined in the first PointSet, but defines different colors:

[source]
....
Shape {
  geometry PointSet {
    coord DEF mypts Coordinate { point [ 0 0 0, 2 2 2, 3 3 3 ] }
    color Color { color [ 1 0 0, 0 1 0, 0 0 1 ] }
  }
}
Shape {
  geometry PointSet {
    coord USE mypts
    color Color { color [ .5 .5 0, 0 .5 .5, 1 1 1 ] }
  }
}
....

This simple example defines a PointSet composed of 3 points. The first
point is red (1 0 0), the second point is green (0 1 0), and the third
point is blue (0 0 1). The second PointSet instances the Coordinate node
defined in the first PointSet, but defines different colors:


==== Level of Detail

The LOD node is typically used for switching between different versions
of geometry at specified distances from the viewer. But if the range
field is left at its default value the browser selects the most
appropriate child from the list given. It can make this selection based
on performance or perceived importance of the object. Children should be
listed with most detailed version first just as for the normal case.
This "performance LOD" feature can be combined with the normal LOD
function to give the browser a selection of children from which to
choose at each distance.

In this example, the browser is free to choose either a detailed or a
less-detailed version of the object when the viewer is closer than 100
meters (as measured in the coordinate space of the LOD). The browser
should display the less-detailed version of the object if the viewer is
between 100 and 1,000 meters and should display nothing at all if the
viewer is farther than 1,000 meters. Browsers should try to honor the
hints given by authors, and authors should try to give browsers as much
freedom as they can to choose levels of detail based on performance.

[source]
....
LOD {
  range [100, 1000]
  levels [
    LOD {
      levels [
        Transform { ... detailed version...  }
        DEF LoRes Transform { ... less detailed version... }
      ]
    }
    USE LoRes,
    Shape { } # Display nothing
  ]
}
....

For best results, specify ranges only where necessary, and nest LOD
nodes with and without ranges.


==== Color Interpolator

This example interpolates from red to green to blue in a 10 second
cycle:

[source]
....
DEF myColor ColorInterpolator {
  key        [   0.0,    0.5,    1.0 ]
  keyValue   [ 1 0 0,  0 1 0,  0 0 1 ] # red, green, blue
}

DEF myClock TimeSensor {
  cycleInterval 10.0      # 10 second animation
  loop          TRUE      # infinitely cycling animation
}

ROUTE myClock.fraction_changed TO myColor.set_fraction
....


==== TimeSensor

The TimeSensor is very flexible. Here are some of the many ways in which
it can be used:

* a TimeSensor can be triggered to run continuously by setting
_cycleInterval_ > 0, and _loop_ = TRUE, and then routing a time output
from another node that triggers the loop (e.g. the _touchTime_ eventOut
of a TouchSensor can then be routed to the TimeSensor's _startTime_ to
start the TimeSensor running).
* a TimeSensor can be made to run continuously upon reading by setting
_cycleInterval_ > 0, _startTime_ > 0, _stopTime_ = 0, and _loop_ = TRUE.
(This use is not recommended.)

{empty}1. Animate a box when the user clicks on it:

[source]
....
DEF XForm Transform { children [
  Shape { geometry Box {} }
  DEF Clicker TouchSensor {}
  DEF TimeSource TimeSensor { cycleInterval 2.0 } # Run once for 2 sec.
  # Animate one full turn about Y axis:
  DEF Animation OrientationInterpolator {
       key      [ 0,      .33,       .66,        1.0 ]
       keyValue [ 0 1 0 0, 0 1 0 2.1, 0 1 0 4.2, 0 1 0 0 ]
  }
]}
ROUTE Clicker.touchTime TO TimeSource.startTime
ROUTE TimeSource.fraction_changed TO Animation.set_fraction
ROUTE Animation.value_changed TO XForm.rotation
....

{empty}2. Play Westminster Chimes once an hour:


[source]
....
#VRML V2.0 utf8

Group { children [
  DEF Hour TimeSensor {
    loop          TRUE
    cycleInterval 3600.0         # 60*60 seconds == 1 hour
  }
  Sound {
    source DEF Sounder AudioClip {
      url "http://...../westminster.mid" }
    }
  }
]}
ROUTE Hour.cycleTime TO Sounder.startTime
....

{empty}3. Make a grunting noise when the user runs into a wall:

[source]
....
DEF Walls Collision { children [
  Transform {
    #... geometry of walls...
  }
  Sound {
    source DEF Grunt AudioClip {
      url "http://...../grunt.wav"
    }
  }
]}
ROUTE Walls.collision TO Grunt.startTime
....


==== Shuttles and Pendulums

Shuttles and pendulums are great building blocks for composing
interesting animations. This shuttle translates its children back and
forth along the X axis, from -1 to 1. The pendulum rotates its children
about the Y axis, from 0 to 3.14159 radians and back again.

[source]
....
PROTO Shuttle [
    exposedField SFBool enabled TRUE
    field SFFloat rate 1
    eventIn SFBool moveRight
    eventOut SFBool isAtLeft
    field MFNode children ]
{
    DEF F Transform { children IS children }
    DEF T TimeSensor { 
        cycleInterval IS rate 
        enabled IS enabled
    }
    DEF S Script {
        eventIn  SFBool  enabled IS set_enabled
        field    SFFloat rate IS rate
        eventIn  SFBool  moveRight IS moveRight
        eventIn  SFBool  isActive
        eventOut SFBool  isAtLeft IS isAtLeft
        eventOut SFTime  start
        eventOut SFTime  stop
        field    SFNode  timeSensor USE T

        url "javascript:
            // constructor: send initial isAtLeft eventOut
            function initialize() {
                isAtLeft = true;
            }

            function moveRight(move, ts) {
                if (move) {
                    // want to start move right
                    start = ts;
                    stop = ts + rate / 2;
                }
                else {
                    // want to start move left
                    start = ts - rate / 2;
                    stop = ts + rate / 2;
                }
            }

            function isActive(active) {
                if (!active) isAtLeft = !moveRight;
            }

            function set_enabled(value, ts) {
                if (value) {
                    // continue from where we left off
                    start = ts - (timeSensor.time - start);
                    stop  = ts - (timeSensor.time - stop);
                }
            }"
    }

    DEF I PositionInterpolator {
        keys [ 0, 0.5, 1 ]
        values [ -1 0 0, 1 0 0, -1 0 0 ]
    }

    ROUTE T.fraction_changed TO I.set_fraction
    ROUTE T.isActive TO S.isActive
    ROUTE I.value_changed TO F.set_translation
    ROUTE S.start TO T.set_startTime
    ROUTE S.stop TO T.set_stopTime
}


PROTO Pendulum [
    exposedField SFBool enabled TRUE
    field SFFloat rate 1
    field SFFloat maxAngle
    eventIn SFBool moveCCW
    eventOut SFBool isAtCW
    field MFNode children ]
{
    DEF F Transform { children IS children }
    DEF T TimeSensor { 
        cycleInterval IS rate 
        enabled IS enabled
    }
    DEF S Script {
        eventIn  SFBool     enabled IS set_enabled
        field    SFFloat    rate IS rate
        field    SFFloat    maxAngle IS maxAngle
        eventIn  SFBool     moveCCW IS moveCCW
        eventIn  SFBool     isActive
        eventOut SFBool     isAtCW IS isAtCW
        eventOut SFTime     start
        eventOut SFTime     stop
        eventOut MFRotation rotation
        field    SFNode     timeSensor USE T

        url "javascript:
            function initialize() {
                // constructor:setup interpolator,
                // send initial isAtCW eventOut
                isAtCW = true;

                rot[0] = 0; rot[1] = 1; rot[2] = 0; 
                rot[3] = 0;
                rotation[0] = rot;
                rotation[2] = rot;

                rot[3] = maxAngle;
                rotation[1] = rot;
            }

            function moveCCW(move, ts) {
                if (move) {
                    // want to start CCW half (0.0 - 0.5) of move
                    start = ts;
                    stop = start + rate / 2;
                }
                else {
                    // want to start CW half (0.5 - 1.0) of move
                    start = ts - rate / 2;
                    stop = ts + rate / 2;
                }
            }

            function isActive(active) {
                if (!active) isAtCW = !moveCCW;
            }

            function set_enabled(value, ts) {
                if (value) {
                    // continue from where we left off
                    start = ts - (timeSensor.time - start);
                    stop  = ts - (timeSensor.time - stop);
                }
            }"
    }
    DEF I OrientationInterpolator {
        keys [ 0, 0.5, 1 ]
    }
    ROUTE T.fraction_changed TO I.set_fraction
    ROUTE I.value_changed TO F.set_rotation
    ROUTE T.isActive TO S.isActive
    ROUTE S.start TO T.set_startTime
    ROUTE S.stop TO T.set_stopTime
    ROUTE S.rotation TO I.set_values
}
....

In use, the Shuttle can have its isAtRight output wired to its moveLeft
input to give a continuous shuttle. The Pendulum can have its isAtCCW
output wired to its moveCW input to give a continuous Pendulum effect.


==== Robot

Robots are very popular in in VRML discussion groups. Here's a simple
implementation of one. This robot has very simple body parts: a cube for
his head, a sphere for his body and cylinders for arms (he hovers so he
has no feet!). He is something of a sentry - he walks forward, turns
around, and walks back. He does this whenever you are near. This makes
use of the Shuttle and Pendulum above.

[source]
....
DEF Walk Shuttle { 
    enabled FALSE
    rate 10
    children [
        DEF Near ProximitySensor { size 10 10 10 } 
        DEF Turn Pendulum {
            enabled FALSE

            children [
                # The Robot
                Shape {
                    geometry Box { } # head
                }
                Transform {
                    scale 1 5 1
                    translation 0 -5 0
                    children [ Shape { geometry Sphere { } } ] # body
                }
                DEF Arm Pendulum {
                    maxAngle 0.52 # 30 degrees
                    enabled FALSE

                    children [ 
                        Transform {
                            scale 1 7 1
                            translation 1 -5 0
                            rotation 1 0 0 4.45 # rotate so swing
                                                # centers on Y axis
                            center 0 3.5 0

                            children [ 
                                Shape { geometry Cylinder { } } 
                            ]
                        }
                    ]
                }

                # duplicate arm on other side and flip so it swings
                # in opposition
                Transform {
                    rotation 0 1 0 3.14159
                    translation 10 0 0
                    children [ USE Arm ]
                }
            ]
        }
    ]
}

# hook up the sentry.  The arms will swing infinitely.  He walks
# along the shuttle path, then turns, then walks back, etc.
ROUTE Near.isActive TO Arm.enabled
ROUTE Near.isActive TO Walk.enabled
ROUTE Arm.isAtCW TO Arm.moveCCW
ROUTE Walk.isAtLeft TO Turn.moveCCW
ROUTE Turn.isAtCW TO Walk.moveRight
....


==== Chopper

Here is a simple example of how to do simple animation triggered by a
touchsensor. It uses an EXTERNPROTO to include a Rotor node from the net
which will do the actual animation.

[source]
....
EXTERNPROTO Rotor [ 
    eventIn MFFloat Spin 
    field MFNode children ]
 "http://somewhere/Rotor.wrl" # Where to look for implementation


PROTO Chopper [ 
    field SFFloat maxAltitude 30
       field SFFloat rotorSpeed 1 ] 
{
    Group {
        children [
            DEF Touch TouchSensor { }, # Gotta get touch events
            Shape { ... body... },
            DEF Top Rotor { ... geometry ... },
            DEF Back Rotor { ... geometry ... }
        ]
    }

    DEF SCRIPT Script {
        eventIn SFBool startOrStopEngines
        field maxAltitude IS maxAltitude
        field rotorSpeed IS rotorSpeed
        field SFNode topRotor USE Top
        field SFNode backRotor USE Back
        field SFBool bEngineStarted FALSE

        url "chopper.vs"
    }

    ROUTE Touch.isActive -> SCRIPT.startOrStopEngines
}


DEF MyScene Group {
    DEF MikesChopper Chopper { maxAltitude 40 }
}


chopper.vs:
-------------
    function startOrStopEngines(value, ts) {
        // Don't do anything on mouse-down:
        if (value) return;

        // Otherwise, start or stop engines:
        if (!bEngineStarted) {
            StartEngine();
        }
        else {
            StopEngine();
        }
    }

    function SpinRotors(fInRotorSpeed, fSeconds) {
        rp[0] = 0;
        rp[1] = fInRotorSpeed;
        rp[2] = 0;
        rp[3] = fSeconds;
        TopRotor.Spin = rp;

        rp[0] = fInRotorSpeed;
        rp[1] = 0;
        rp[2] = 0;
        rp[3] = fSeconds;
        BackRotor.Spin = rp;
    }

    function StartEngine() {
        // Sound could be done either by controlling a PointSound node
        // (put into another SFNode field) OR by adding/removing a
        // PointSound from the Separator (in which case the Separator
        // would need to be passed in an SFNode field).

        SpinRotors(fRotorSpeed, 3);
        bEngineStarted = TRUE;
    }

    function StopEngine() {
        SpinRotors(0, 6);
        bEngineStarted = FALSE;
    }
}
....


==== Guided Tour

Moving Worlds has great facilities to put the viewer's camera under
control of a script. This is useful for things such as guided tours,
merry-go-round rides, and transportation devices such as busses and
elevators. These next 2 examples show a couple of ways to use this
feature.

The first example is a simple guided tour through the world. Upon entry,
a guide orb hovers in front of you. Click on this and your tour through
the world begins. The orb follows you around on your tour. Perhaps a
PointSound node can be embedded inside to point out the sights. A
ProximitySensor ensures that the tour is started only if the user is
close to the initial starting point. Note that this is done without
scripts thanks to the touchTime output of the TouchSensor.

[source]
....
Group {
    children [
        <geometry for the world>,

        DEF GuideTransform Transform {
            children [
                DEF TourGuide Viewpoint { jump FALSE },
                DEF ProxSensor ProximitySensor { size 10 10 10 }
                DEF StartTour TouchSensor { },
                Shape { geometry Sphere { } }, # the guide orb
            ]
        }
    ]
}

DEF GuidePI PositionInterpolator {
    keys [ ... ]
    values [ ... ]
}

DEF GuideRI RotationInterpolator {
    keys [ ... ]
    values [ ... ]
}

DEF TS TimeSensor { cycleInterval 60 } # 60 second tour

ROUTE ProxSensor.isActive TO StartTour.enabled
ROUTE StartTour.touchTime TO TS.startTime
ROUTE TS.isActive TO TourGuide.bind
ROUTE TS.fraction TO GuidePI.set_fraction
ROUTE TS.fraction TO GuideRI.set_fraction
ROUTE GuidePI.outValue TO GuideTransform.set_translation
ROUTE GuideRI.outValue TO GuideTransform.set_rotation
....


==== Elevator

Here's another example of animating the camera. This time it's an
elevator to ease access to a multistory building. For this example I'll
just show a 2 story building and I'll assume that the elevator is
already at the ground floor. To go up you just step inside. A
ProximitySensor fires and starts the elevator up automatically. I'll
leave call buttons for outside the elevator, elevator doors and floor
selector buttons as an exercise for the reader!

[source]
....
Group {
    children [

        DEF ETransform Transform {
            children [
                DEF EViewpoint Viewpoint { }
                DEF EProximity ProximitySensor { size 2 2 2 }
                <geometry for the elevator, 
                 a unit cube about the origin with a doorway>
            ]
        }
    ]
}
DEF ElevatorPI PositionInterpolator {
    keys [ 0, 1 ]
    values [ 0 0 0, 0 4 0 ] # a floor is 4 meters high
}
DEF TS TimeSensor { cycleInterval 10 } # 10 second travel time

DEF S Script {
    field SFNode viewpoint USE EViewpoint
    eventIn SFBool active
    eventIn SFBool done
    eventOut SFTime start
    behavior "Elevator.java"
}

ROUTE EProximity.enterTime TO TS.startTime
ROUTE TS.isActive TO EViewpoint.bind
ROUTE TS.fraction_changed TO ElevatorPI.set_fraction
ROUTE ElevatorPI.value_changed TO ETransform.set_translation
....


Contact rikk@best.com, mailto:cmarrin@sgi.com[cmarrin@sgi.com,] or gavin@acm.org with questions or comments.

[[java_html]]

<199607310711.QAA18942@yoritomo.soft.arch.sony.co.jp>
<199607300150.SAA03932@homey.engr.sgi.com>
<199607300330.TAA14414@mole.dimensionx.com> <31FE4D45.3359@sgi.com>


=== Appendix C. Java Scripting Reference

 +

This annex describes the Java classes and methods that allow Script
nodes ( see "<<Script, Nodes Reference - Script>>") to interact with
associated scenes. 
See "<<Scripting, Concepts - Scripting>>" for a general
description of scripting languages in VRML.

• <<Language, C.1 Language>>
• <<Protocol, C.2 Supported Protocol in the Script Node's _url_ Field>>
• <<FileExtension, C.2.1 File Extension>>
• <<MIMEtype, C.2.2 MIME Type>>
• <<EventIn, C.3 EventIn Handling>>
• <<ParameterPassing, C.3.1 Parameter Passing and the EventIn Field/Method>>
• <<ProcessEvents, C.3.2 _processEvents()_ and _processEvent()_ Methods>>
• <<EventsProcessed, C.3.3 _eventsProcessed()_ Method>>
• <<Shutdown, C.3.4 _shutdown()_ Method>>
• <<Initialize, C.3.5 _initialize()_ Method>>
• <<AccessingFields, C.4 Accessing Fields and Events>>
• <<AccessingScript, C.4.1 Accessing Fields and EventOuts of the Script>>
• <<AccessingOtherNodes, C.4.2 Accessing Fields and EventOuts of Other Nodes>>
• <<SendingEventOuts, C.4.3 Sending EventOuts>>
• <<ExposedClasses, C.5 Exposed Classes and Methods for Nodes and Fields>>
• <<FieldClass, C.5.1 _Field_ Class and _ConstField_ Class>>
• <<NodeClass, C.5.2 _Node_ Class>>
• <<WorldClass, C.5.3 _Browser_ Class>>
• <<UserDefined, C.5.4 User-defined Classes and Packages>>
• <<StandardJavaPackage, C.5.5 Standard Java Packages>>
• <<Exceptions, C.6 Exceptions>>
• <<Example, C.7 Example>>
• <<ClassDefinitions, C.8 Class Definitions>>
• <<ClassHierarchy, C.8.1 Class Hierarchy>>
• <<VrmlPackages, C.8.2 VRML Packages>>
• <<VrmlPackage, C.8.2.1 _vrml_ Package>>
• <<VrmlFieldPackage, C.8.2.2 _vrml.field_ Package>>
• <<VrmlNodePackage, C.8.2.3 _vrml.node_ Package>>
• <<ExampleOfException, C.9 Example of Exception Class>>


[[Language]]
==== C.1 Language

http://java.sun.com[Java](TM) is an object-oriented,
platform-independent, multi-threaded, general-purpose programming
environment developed at http://www.sun.com/[Sun Microsystems, Inc].
See the Java web site for a full description of the Java programming
language (http://java.sun.com/). This appendix describes the Java
bindings of VRML to the Script node.


[[Protocol]]
==== C.2 Supported Protocol in the Script Node's _url_ Field

The _url_ field of the Script node contains the URL of a file containing
the Java byte code, for example: **

[source]
....
     Script { 
         url "http://foo.co.jp/Example.class"
         eventIn SFBool start
     }
....

[[FileExtension]]
===== C.2.1 File Extension

The file extension for Java byte code is *.class*.

[[MIMEtype]]
===== C.2.2 MIME Type

The MIME type for Java byte code is defined as follows:

[source]
....
        application/x-java
....


[[Eventin]]
==== C.3 EventIn Handling

Events to the Script node are passed to the corresponding Java method
(processEvent or processEvents) in the script. It is necessary to
specify the script in the url field of the Script node.

If a Java byte code file is specified in the url field, the following
two conditions must hold:

* it must contain the class definition whose name is exactly the same as
the body of the file name, and
* it must be a subclass of 'Script' class in "<<VrmlNodePackage, vrml.node Package>>".

For example, the following Script node has one eventIn field whose name
is 'start'.

[source]
....
    Script { 
           url "http://foo.co.jp/Example.class"
           eventIn SFBool start
    }
....

This node points to the script file 'Example.class' - its source
('Example.java') looks like this:

[source]
....
    import vrml.*;
    import vrml.field.*;
    import vrml.node.*;

    class Example extends Script {
        ...
        // This method is called when any event is received
        public void processEvent (Event e) {
           // ... perform some operation ...
        }
    }
....

In the above example, when the start eventIn is sent the processEvent()
method is executed and receives the eventIn.

[[ParameterPassing]]
===== C.3.1 Parameter Passing and the EventIn Field/Method

When a Script node receives an eventIn, a processEvent() or
processEvents() method in the file specified in url field of the Script
node is called, which receives the eventIn as Java objects (Event
object). 
See "<<ProcessEvents, processEvent() or processEvents() Method>>".

The Event object has three information associated with it: name, value
and timestamp of the eventIn. These can be retrieved using the
corresponding method on the Event object.

[source]
....
   class Event {
      public String getName();
      public ConstField getValue();
      public double getTimeStamp();
   }
....

Suppose that the eventIn type is *SFXXX* and eventIn name is
*eventInYYY*, then

* the getName() should return "*eventInYYY*"
* the getValue() should return ConstField.
* getTimeStamp() should return the timestamp when the eventIn was
occurred

In the above example, the eventIn name would be "start" and eventIn
value could be casted to be ConstSFBool. Also, the timestamp for the
time when the eventIn was occurred is available as a double. These are
passed as an Event object to processEvent() method:

[source]
....
        public void processEvent (Event e) {
            if(e.getName().equals("start")){
                  ConstSFBool v = (ConstSFBool)e.getValue();
                  if(v.getValue()==true){
                       // ... perform some operation ...
                  }
             }
        }
....

[[ProcessEvents]]
===== C.3.2 _processEvents()_ and _processEvent()_ Method

Authors can define a processEvents method within a class that is called
when the script receives some set of events. The prototype of
processEvents method is

[source]
....
    public void processEvents(int count, Event events[]);
....

_count_ indicates the number of events delivered. _events_ is the array
of events delivered. Its default behavior is to iterate over each event,
calling processEvent() on each one as follows:

[source]
....
    public void processEvents(int count, Event events[])
    {
        for (int i = 0; i < count; i++) {
            processEvent( events[i] );
        }
    }
....

Although authors could change this operation by giving a user-defined
processEvents() method, in most cases, they would only change
processEvent() method and eventsProcessed() method described below.

When multiple eventIns are routed from single node to single script node
and then the eventIns which have the same timestamp are occurred, the
processEvents() receives multiple events as the event array. Otherwise,
each coming event invokes separate processEvents().

For example, processEvents() method receives two events in the following
case :

[source]
....
    Transform {
        children [
            DEF TS TouchSensor {}
            Shape { geometry Cone {} }
        ]
    }
    DEF SC Script {
        url     "Example.class"
        eventIn SFBool isActive
        eventIn SFTime touchTime
    }
    ROUTE TS.isActive  TO SC.isActive
    ROUTE TS.touchTime TO SC.touchTime
....

Authors can define a processEvent method within a class. The prototype
of processEvent is

[source]
....
    public void processEvent(Event event);
....

Its default behavior is no operation.

[[EventsProcessed]]
===== C.3.3 _eventsProcessed()_ Method

Authors can define an _eventsProcessed_ method within a class that is
called after some set of events has been received. It allows Scripts
that do not rely on the ordering of events received to generate fewer
events than an equivalent Script that generates events whenever events
are received. It is called after every invocation of processEvents().

Events generated from an eventsProcessed routine are given the timestamp
of the last event processed.

The prototype of eventsProcessed method is

[source]
....
    public void eventsProcessed();
....

Its default behavior is no operation.

[[Shutdown]]
===== C.3.4 _shutdown()_ Method

Authors can define a _shutdown_ method within a class that is called
when the corresponding Script node is deleted.

The prototype of shutdown method is

[source]
....
    public void shutdown();
....

Its default behavior is no operation.

[[Initialize]]
===== C.3.5 _initialize()_ Method

Authors can define an initialize method within a class that is called
before any event is generated. The various methods on Script such as
getEventIn, getEventOut, getExposedField, and getField are not
guaranteed to return correct values before the call to initialize (i.e.
in the constructor). initialize() is called once during the life of
Script object.

The prototype of initialize method is

[source]
....
    public void initialize();
....

Its default behavior is no operation.


[[AccessingFields]]
==== C.4 Accessing Fields and Events

The fields, eventIns and eventOuts of a Script node are accessible from
their corresponding Java classes.

[[AccessingScript]]
===== C.4.1 Accessing Fields, EventIns and EventOuts of the Script

Each field defined in the Script node is available to the script by
using its name. Its value can be read or written. This value is
persistent across function calls. EventOuts defined in the script node
can also be read.

Accessing fields of Script node can be done by using Script class
methods. Script class has several methods to do that: getField(),
getEventOut(), getEventIn() and getExposedField().

* Field *getField*(String _fieldName_) +
is the method to get the reference to the Script node's 'field' whose
name is _fieldName_. The return value can be converted to an appropriate
Java "_<<FieldClass, Field Class>>"._
* Field *getEventOut*(String _eventName_) +
is the method to get the reference to the Script node's 'eventOut' whose
name is _eventName_. The return value can be converted to an appropriate
Java "_<<FieldClass, Field Class>>"._
* Field *getEventIn*(String _eventName_) +
is the method to get the reference to the Script node's 'eventIn' whose
name is _eventName_. The return value can be converted to an appropriate
Java "
+
<<FieldClass, Field Class>>". When you call getValue() method on a
'field' object obtained by getEventIn() method, the return value is
unspecified. Therefore, you can not rely on it. 'EventIn' is a
write-only field.

When you call setValue(), set1Value(), addValue() or insertValue()
method on a 'field' object obtained by *getField()* method, the value
specified as an argument is stored in the corresponding VRML node's
field.

When you call setValue(), set1Value(), addValue() or insertValue()
method on a 'field' object obtained by *getEventOut()* method, the value
specified as an argument generates an event in VRML scene. The effect of
this event is specified by the associated Route in the VRML scene.

When you call setValue(), set1Value(), addValue() or insertValue()
methods on a 'field' object obtained by *getEventIn()* method, the value
specified an argument generates an event to the Script node. For
example, the following Script node defines an eventIn, _start_, a field,
_state_, and an eventOut, _on_. The method _initialize()_ is invoked
before any events are received, and the method _processEvent()_ is
invoked when _start_ receives an event:

[source]
....
    Script {
        url       "Example.class"
        eventIn   SFBool start
        eventOut  SFBool on
        field     SFBool state TRUE
    }


  Example.class:

    import vrml.*;
    import vrml.field.*;
    import vrml.node.*;

    class Example extends Script {
       private SFBool state;
       private SFBool on;

       public void initialize(){
            state = (SFBool) getField("state");
            on = (SFBool) getEventOut("on");
        }

        public void processEvent(Event e) {
            if(state.getValue()==true){
                on.setValue(true); // set true to eventOut 'on'
                state.setValue(false);
            }
            else {
                on.setValue(false); // set false to eventOut 'on'
                state.setValue(true);
            }
        }
    }
....

[[AccessingOtherNodes]]
===== C.4.2 Accessing Fields, EventIns and EventOuts of Other Nodes

If a script program has an access to any node, any eventIn, eventOut or
exposedField of that node is accessible by using the getEventIn(),
getEventOut() method or getExposedField() method defined on the node's
class (
see "<<ExposedClasses, Exposed Classes and Methods for Nodes and Fields>>"
).

The typical way for a script program to have an access to another VRML
node is to have an SFNode field which provides a reference to the other
node. The following example shows how this is done:

[source]
....
    DEF SomeNode Transform { }
    Script {
         field SFNode node USE SomeNode
         eventIn SFVec3f pos
         url "Example.class"
    }


Example.class:

    import vrml.*;
    import vrml.field.*;
    import vrml.node.*;

    class Example extends Script {
        private SFNode node;
        private SFVec3 trans;

       public void initialize(){
            private SFNode node = (SFNode) getField("node");
       }

        public void processEvent(Event e) {
             // gets the ref to 'translation' field of Transform node

             trans = (SFVec3f)(node.getValue())
                                       .getExposedField("translation");
             trans.setValue((ConstSFVec3f)e.getValue());
        }
    }
....

[[SendingEventOuts]]
===== C.4.3 Sending EventIns or EventOuts

Sending eventOuts from script is done by setting value to the reference
to the 'eventOut' of the script by setValue(), set1Value(), addValue()
or insertValue() method. Sending eventIns from script is done by setting
value to the reference to the 'eventIn' by setValue(), set1Value(),
addValue() or insertValue() method.


[[ExposedClasses]]
==== C.5 Exposed Classes and Methods for Nodes and Fields

Java classes for VRML are defined in the packages: _vrml, vrml.node_ and
_vrml.field_.

The Field class extends
http://java.sun.com/books/Series/Tutorial/java/javaOO/objectclass.html%20[Java's
Object class] by default; thus, Field has the full functionality of the
Object class, including the

http://java.sun.com/books/Series/Tutorial/java/javaOO/objectclass.html%20[getClass()]
method. The rest of the package defines a "Const" read-only class for
each VRML <<fieldsRef_html, field type>>, with a *getValue()* method
for each class; and another read/write class for each VRML field type,
with both *getValue()* and *setValue()* methods for each class. A
getValue() method converts a VRML-type value into a Java-type value. A
setValue() method converts a Java-type value into a VRML-type value and
sets it to the VRML field.

Most of the *setValue()* methods and *set1Value()* methods are listed as
"http://java.sun.com/tutorial/java/exceptions/index.html[throws
exception]," meaning that errors are possible -- you may need to write
exception handlers (using Java's *catch()* method) when you use those
methods. Any method not listed as "throws exception" is guaranteed to
generate no exceptions. Each method that throws an exception includes a
prototype showing which exception(s) can be thrown.

[[FieldClass]]
===== C.5.1 Field Class and ConstField Class

All VRML data types have an equivalent classes in Java.

[source]
....
    class Field {
    }
....

Field class is the root of each field types. This class has two
subclasses : read-only class and writable class

* *Read-only class* +
This class supports *getValue()* method. In addition, some classes
support some convenient methods to get value from the object. +
 +
ConstSFBool, ConstSFColor, ConstMFColor, ConstSFFloat, ConstMFFloat,
ConstSFImage, ConstSFInt32, ConstMFInt32, ConstSFNode, ConstMFNode,
ConstSFRotation, ConstMFRotation, ConstSFString, ConstMFString,
ConstSFVec2f, ConstMFVec2f, ConstSFVec3f, ConstMFVec3f, ConstSFTime,
ConstMFTime +
 +
* *Writable class* +
This type of classes support both *getValue()* and *setValue()* methods.
If the class name is prefixed with *MF* meaning that it is a multiple
valued field class, the class also supports the *set1Value(), addValue()
and insertValue()* method. +
In addition, some classes support some convenient methods to get and set
value from the object.
+
SFBool, SFColor, MFColor, SFFloat, MFFloat, SFImage, SFInt32, MFInt32,
SFNode, MFNode, SFRotation, MFRotation, SFString, MFString, SFVec2f,
MFVec2f, SFVec3f, MFVec3f, SFTime, MFTime

The Java _Field_ class and its subclasses have several methods to get
and set values: getSize(), getValue(), get1Value(), setValue(),
set1Value(), addValue() or insertValue().

* *getSize*() +
is the method to return the number of elements of each multiple value
field class(MF class).
* *getValue*() +
is the method to convert a VRML-type value into a Java-type value and
returns it.
* *get1Value*(int _index_) +
is the method to convert a VRML-type value (_index_-th element) into a
Java-type value and returns it. The index of the first element is 0.
Getting the element beyond the existing elements throws exception.
* *setValue*(_value_) +
is the method to convert a Java-type _value_ into a VRML-type value and
sets it to the VRML field.
* *set1Value*(int _index_, _value_) +
is the method to convert from a Java-type _value_ to a VRML-type value
and set it to the _index_-th element.
* *addValue*(_value_) +
is the method to convert from a Java-type _value_ to a VRML-type value
and add it to the last element.
* *insertValue*(int _index_, _value_) +
is the method to convert from a Java-type _value_ to a VRML-type value
and insert it to the _index_-th element. The index of the first element
is 0. Setting the element beyond the existing elements throws exception.

In these methods, getSize(), get1Value(), set1Value(), addValue() and
insertValue() are only available for multiple value field classes(MF
classes). 
See "<<vrmlpackage, vrml _Package_>>" for each classes' methods definition.


[[NodeClass]]
==== C.5.2 Node Class

_Node_ class has several methods: getType(), getEventOut(),
getEventIn(), getExposedField(), getBrowser()

* String *getType*() +
is the method to returns the type of the node.
* ConstField *getEventOut*(String _eventName_) +
is the method to get the reference to the node's 'eventOut' whose name
is _eventName_. The return value can be converted to an appropriate Java
"_<<FieldClass, Field Class>>"._
* Field *getEventIn*(String _eventName_) +
is the method to get the reference to the node's 'eventIn' whose name is
_eventName_. The return value can be converted to an appropriate Java
"_<<FieldClass, Field Class>>". When you call getValue() method on a
'field' object obtained by getEventIn() method, the return value is
unspecified. Therefore, you can not rely on it. 'EventIn' is a
write-only field._
* Field *getExposedField*(String _eventName_) +
is the method to get the reference to the node's 'exposedField' whose
name is _eventName_. The return value can be converted to an appropriate
Java "_<<FieldClass, Field Class>>"._
* Browser *getBrowser*() +
is method to get the browser that this node is contained in. 
See "<<SceneClass, Browser Class>>".

When you call setValue(), set1Value(), addValue() or insertValue()
method on a 'field' object obtained by *getEventIn()* method, the value
specified as an argument generates an event to the node.

When you call setValue(), set1Value(), addValue() or insertValue()
method on a 'field' object obtained by *getExposedField()* method, the
value specified as an argument generates an event in VRML scene. The
effect of this event is specified by the associated Route in the VRML
scene.


[[BrowserClass]]
===== C.5.3 Browser Class

This section lists the public Java interfaces to the _Browser_ class,
which allows scripts to get and set browser information. For
descriptions of the following methods see the "<<BrowserInterface, Concepts - Scripting - Browser Interface>>". 

[width="100%",cols="50%,50%",]
|===
a|
===== *Return value*
a|
===== *Method name*

|_String_ |*_getName_*_()_

|_String_ |*_getVersion_*_()_

|_float_ |*_getCurrentSpeed_*_()_

|_float_ |*_getCurrentFrameRate_*_()_

|_String_ |*_getWorldURL_*_()_

|_void_ |*_replaceWorld_*_(Node[] nodes)_

|_Node[]_ |*_createVrmlFromString_*_(String vrmlSyntax)_

|_void_ |*_createVrmlFromURL_*_(String[] url, Node node, String event)_

|_void_ |*_addRoute_*_(Node fromNode, String fromEventOut, Node toNode,
                  String toEventIn)_
|_void_ |*_deleteRoute_*_(Node fromNode, String fromEventOut, Node toNode,                      String toEventIn)_

|_void_ |*_loadURL_*_(String[] url, String[] parameter)_

|_void_ |*_setDescription_*_(String description)_
|===


See "<<VrmlPackage, vrml _Package_>>"for each method's definition.

Conversion table from the types used in Browser class to Java type.

[cols=",",]
|===
|*VRML type* |*Java type*
|_SFString_ |_String_
|_SFFloat_ |_float_
|_MFString_ |_String[]_
|_MFNode_ |_Node[]_
|===

[[UserDefined]]
===== C.5.4 User-defined Classes and Packages

The Java classes defined by a user can be used in the Java program. They
are searched from the directory where the Java program is placed.

If the Java class is in a package, this package is searched from the
directory where the Java program is placed.

[[StandardJavaPackage]]
===== C.5.5 Standard Java Packages

Java programs have access to the full set of classes available in java.*
The handling of these classes - especially _AWT_, and the security model
of networking - will be browser specific. Threads are required to work
as normal for Java.


[[Exceptions]]
==== C.6 Exceptions

Java methods may throw the following exceptions:

* *InvalidFieldException* +
is thrown at the time getField() is executed and the field name is
invalid.
* *InvalidEventInException* +
is thrown at the time getEventIn() is executed and the eventIn name is
invalid.
* *InvalidEventOutException* +
is thrown at the time getEventOut() or getEventOut() is executed and the
eventOut name is invalid.
* *InvalidExposedFieldException* +
is thrown at the time getExposedField() is executed and the exposedField
name is invalid.
* *InvalidVRMLSyntaxException* +
is thrown at the time createVrmlFromString(), createVrmlFromURL() or
loadURL() is executed and the vrml string is invalid.
* *InvalidRouteException* +
is thrown at the time addRoute() or deleteRoute() is executed and one or
more the arguments is invalid.
* *InvalidFieldChangeException* +
may be thrown as a result of all sorts of illegal field changes, for
example:
** Adding a node from one World as the child of a node in another World.
** Creating a circularity in a scene graph.
** Setting an invalid string on enumerated fields, such as the fogType
field of the Fog node.
+
It is not guaranteed that such exceptions will be thrown, but a browser
should do the best job it can.
* *InvalidNavigationTypeException* +
is thrown at the time setNavigationType() is executed and the argument
is invalid.
* *ArrayIndexOutOfBoundsException* +
is generated at the time setValue(), set1Value(), addValue() or
insertValue() is executed and the index is out of bound. This is the
standard exception defined in the Java Array class.

If exceptions are not redefined by authors, a browser's behavior is
unspecified - 
see "<<ExampleOfException, Example of Exception Class>>".


[[Example]]
==== C.7 Example

Here's an example of a <<Script, Script>> node which
determines whether a given color contains a lot of red. The Script node
exposes a color field, an eventIn, and an eventOut:

[source]
....
Script {
    field    SFColor currentColor 0 0 0
    eventIn  SFColor colorIn
    eventOut SFBool  isRed
    url "ExampleScript.class"
}
....

And here's the source code for the "ExampleScript.java" file that gets
called every time an eventIn is routed to the above Script node:

[source]
....
import vrml.*;
import vrml.field.*;
import vrml.node.*;

class ExampleScript extends Script {
    // Declare field(s)
    private SFColor currentColor;
  
    // Declare eventOut field(s)
    private SFBool isRed;
  
    public void initialize(){
       currentColor = (SFColor) getField("currentColor");
       isRed = (SFBool) getEventOut("isRed");
    }
  
    public void processEvent(Event e){
        // This method is called when a colorIn event is received
        currentColor.setValue((ConstSFColor)e.getValue());
    }
  
    public void eventsProcessed() {
        if (currentColor.getValue()[0] >= 0.5) // if red is at or above 50%
            isRed.setValue(TRUE);
    }
}
....

For details on when the methods defined in ExampleScript are called -

see "<<ExecutionModel, Concepts -Execution Model>>".

===== Browser class examples:

* <<BrowserClass, createVrmlFromUrl method>> +
+
....
DEF Example Script {
    url "Example.class"
    field   MFString target_url "foo.wrl"
    eventIn MFNode   nodesLoaded
    eventIn SFBool   trigger_event
  }

Example.class:
  import vrml.*;
  import vrml.field.*;
  import vrml.node.*;
  
  class Example extends Script {
      private MFString target_url;
      private Browser browser;

      public void initialize(){
          target_url = (MFString)getField("target_url");
          browser = this.getBrowser();
      }

      public void processEvent(Event e){
          if(e.getName().equals("trigger_event")){
              // do something and then fetch values
              browser.createVRMLFromURL(target_url.getValue(), this, "nodesLoaded");
         }
          if(e.getName().equals("nodesLoaded")){
              // do something
          }
      }
  }
....
* <<BrowserClass, addRoute method>> +
+
....
  DEF Sensor TouchSensor {}
  DEF Example Script {
    url "Example.class"
    field   SFNode fromNode USE Sensor
    eventIn SFBool clicked
    eventIn SFBool trigger_event
  }

Example.class:
  import vrml.*;
  import vrml.field.*;
  import vrml.node.*;
  
  class Example extends Script {
      private SFNode fromNode;
      private Browser browser;
     
      public void initialize(){
          fromNode = (SFNode) getField("fromNode");
          browser = this.getBrowser();
      }

      public void processEvent(Event e){
         if(e.getName().equals("trigger_event")){
              // do something and then add routing
              browser.addRoute(fromNode.getValue(), "isActive", this, "clicked");
         }
         if(e.getName().equals("clicked")){
              // do something
          }
      }
  }
....


[[ClassDefinitions]]
==== C.8 Class Definitions

[[ClassHierarchy]]
===== C.8.1 Class Hierarchy

The classes are divided into three packages: _vrml_, _vrml.field_ and
_vrml.node_.

[source]
....
java.lang.Object
     |
     +- vrml.Event
     +- vrml.Browser
     +- vrml.Field
     |       +- vrml.field.SFBool
     |       +- vrml.field.SFColor
     |       +- vrml.field.SFFloat
     |       +- vrml.field.SFImage
     |       +- vrml.field.SFInt32
     |       +- vrml.field.SFNode
     |       +- vrml.field.SFRotation
     |       +- vrml.field.SFString
     |       +- vrml.field.SFTime
     |       +- vrml.field.SFVec2f
     |       +- vrml.field.SFVec3f
     |       |
     |       +- vrml.MField
     |       |       +- vrml.field.MFColor
     |       |       +- vrml.field.MFFloat
     |       |       +- vrml.field.MFInt32
     |       |       +- vrml.field.MFNode
     |       |       +- vrml.field.MFRotation
     |       |       +- vrml.field.MFString
     |       |       +- vrml.field.MFTime
     |       |       +- vrml.field.MFVec2f
     |       |       +- vrml.field.MFVec3f
     |       |
     |       +- vrml.ConstField
     |               +- vrml.field.ConstSFBool
     |               +- vrml.field.ConstSFColor
     |               +- vrml.field.ConstSFFloat
     |               +- vrml.field.ConstSFImage
     |               +- vrml.field.ConstSFInt32
     |               +- vrml.field.ConstSFNode
     |               +- vrml.field.ConstSFRotation
     |               +- vrml.field.ConstSFString
     |               +- vrml.field.ConstSFTime
     |               +- vrml.field.ConstSFVec2f
     |               +- vrml.field.ConstSFVec3f
     |               |
     |               +- vrml.ConstMField
     |                       +- vrml.field.ConstMFColor
     |                       +- vrml.field.ConstMFFloat
     |                       +- vrml.field.ConstMFInt32
     |                       +- vrml.field.ConstMFNode
     |                       +- vrml.field.ConstMFRotation
     |                       +- vrml.field.ConstMFString
     |                       +- vrml.field.ConstMFTime
     |                       +- vrml.field.ConstMFVec2f
     |                       +- vrml.field.ConstMFVec3f
     |
     +- vrml.BaseNode
             +- vrml.node.Node
             +- vrml.node.Script

java.lang.Exception
        java.lang.RuntimeException
                vrml.InvalidRouteException
                vrml.InvalidFieldException
                vrml.InvalidEventInException
                vrml.InvalidEventOutException
                vrml.InvalidExposedFieldException
                vrml.InvalidNavigationTypeException
                vrml.InvalidFieldChangeException
        vrml.InvalidVRMLSyntaxException
....

[[VrmlPackages]]
===== C.8.2 vrml Packages

C.8.2.1

[#VrmlPackage]#vrml Package#

[source]
....
package vrml;

public abstract class Field implements Cloneable
{
   public Object clone();
}

public abstract class ConstField extends Field
{
}

public class ConstMField extends ConstField
{
   public int getSize();
}

public class MField extends Field
{
   public int getSize();
   public void clear();
   public void delete(int index);
}

public class Event implements Cloneable {
  public String getName();
  public double getTimeStamp();
  public ConstField getValue();
  public Object clone();
}

public class Browser {
  // Browser interface
  public String getName();    
  public String getVersion();    

  public float getCurrentSpeed();

  public float getCurrentFrameRate();

  public String getWorldURL();
  public void replaceWorld(Node[] nodes);

  public Node[] createVrmlFromString(String vrmlSyntax)
    throws InvalidVRMLSyntaxException;

  public void createVrmlFromURL(String[] url, Node node, String event)
    throws InvalidVRMLSyntaxException;

  public void addRoute(Node fromNode, String fromEventOut,
    Node toNode, String toEventIn);
  public void deleteRoute(Node fromNode, String fromEventOut,
    Node toNode, String toEventIn);

  public void loadURL(String[] url, String[] parameter)
    throws InvalidVRMLSyntaxException;
  public void setDescription(String description);
}

//
// This is the general BaseNode class
// 
public abstract class BaseNode 
{
  // Returns the type of the node.  If the node is a prototype
  //   it returns the name of the prototype.
  public String getType();

  // Get the Browser that this node is contained in.
  public Browser getBrowser();
}
....

C.8.2.2

[#VrmlFieldPackage]#vrml.field Package#

[source]
....
package vrml.field;

public class ConstSFBool extends ConstField
{
   public boolean getValue();
}

public class ConstSFColor extends ConstField
{
   public void getValue(float color[]);
   public float getRed();
   public float getGreen();
   public float getBlue();
}

public class ConstSFFloat extends ConstField
{
   public float getValue();
}

public class ConstSFImage extends ConstField
{
   public int getWidth();
   public int getHeight();
   public int getComponents();
   public void getPixels(byte pixels[]);
}

public class ConstSFInt32 extends ConstField
{
   public int getValue();
}

public class ConstSFNode extends ConstField
{
  /* *****************************************
   * Return value of getValue() must extend Node class.
   * The concrete class is implementation dependent 
   * and up to browser implementation. 
   ****************************************** */
   public Node getValue();
}

public class ConstSFRotation extends ConstField
{
   public void getValue(float[] rotation);
}

public class ConstSFString extends ConstField
{
   public String getValue();
}

public class ConstSFTime extends ConstField
{
   public double getValue();
}

public class ConstSFVec2f extends ConstField
{
   public void getValue(float vec2[]);
   public float getX();
   public float getY();
}

public class ConstSFVec3f extends ConstField
{
   public void getValue(float vec3[]);
   public float getX();
   public float getY();
   public float getZ();
}

public class ConstMFColor extends ConstMField
{
   public void getValue(float colors[][]);
   public void getValue(float colors[]);
   public void get1Value(int index, float color[]);
   public void get1Value(int index, SFColor color);
}

public class ConstMFFloat extends ConstMField
{
   public void getValue(float values[]);
   public float get1Value(int index);
}

public class ConstMFInt32 extends ConstMField
{
   public void getValue(int values[]);
   public int get1Value(int index);
}

public class ConstMFNode extends ConstMField
{
  /******************************************
   * Return value of getValue() must extend Node class.
   * The concrete class is implementation dependent 
   * and up to browser implementation. 
   *******************************************/ 
   public void getValue(Node values[]);
   public Node get1Value(int index);
}

public class ConstMFRotation extends ConstMField
{
   public void getValue(float rotations[][]);
   public void getValue(float rotations[]);
   public void get1Value(int index, float rotation[]);
   public void get1Value(int index, SFRotation rotation);
}

public class ConstMFString extends ConstMField
{
   public void getValue(String values[]);
   public String get1Value(int index);
}

public class ConstMFTime extends ConstMField
{
   public void getValue(double times[]);
   public double get1Value(int index);
}

public class ConstMFVec2f extends ConstMField
{
   public void getValue(float vecs[][]);
   public void getValue(float vecs[]);
   public void get1Value(int index, float vec[]);
   public void get1Value(int index, SFVec2f vec);
}

public class ConstMFVec3f extends ConstMField
{
   public void getValue(float vecs[][]);
   public void getValue(float vecs[]);
   public void get1Value(int index, float vec[]);
   public void get1Value(int index, SFVec3f vec);
}

public class SFBool extends Field
{
   public SFBool(boolean value);
   public boolean getValue();
   public void setValue(boolean b);
   public void setValue(ConstSFBool b);
   public void setValue(SFBool b);
}

public class SFColor extends Field
{
   public SFColor(float red, float green, float blue);
   public void getValue(float color[]);
   public float getRed();
   public float getGreen();
   public float getBlue();
   public void setValue(float color[]);
   public void setValue(float red, float green, float blue);
   public void setValue(ConstSFColor color);
   public void setValue(SFColor color);
}

public class SFFloat extends Field
{
   public SFFloat(float f);
   public float getValue();
   public void setValue(float f);
   public void setValue(ConstSFFloat f);
   public void setValue(SFFloat f);
}

public class SFImage extends Field
{
   public SFImage(int width, int height, int components, byte pixels[]);
   public int getWidth();
   public int getHeight();
   public int getComponents();
   public void getPixels(byte pixels[]);
   public void setValue(int width, int height, int components,
                        byte pixels[]);
   public void setValue(ConstSFImage image);
   public void setValue(SFImage image);
}

public class SFInt32 extends Field
{
   public SFInt32(int value);
   public int getValue();
   public void setValue(int i);
   public void setValue(ConstSFInt32 i);
   public void setValue(SFInt32 i);
}

public class SFNode extends Field
{
   public SFNode(Node node);

  /******************************************
   * Return value of getValue() must extend Node class.
   * The concrete class is implementation dependent 
   * and up to browser implementation. 
   *******************************************/ 
  public Node getValue();
   public void setValue(Node node);
   public void setValue(ConstSFNode node);
   public void setValue(SFNode node);
}

public class SFRotation extends Field
{
   public SFRotation(float axisX, float axisY, float axisZ, float rotation);
   public void getValue(float[] rotation);
   public void setValue(float[] rotation);
   public void setValue(float axisX, float axisY, float axisZ, float rotation);
   public void setValue(ConstSFRotation rotation);
   public void setValue(SFRotation rotation);
}

public class SFString extends Field
{
   public SFString(String s);
   public String getValue();
   public void setValue(String s);
   public void setValue(ConstSFString s);
   public void setValue(SFString s);
}

public class SFTime extends Field
{
   public SFTime(double time);
   public double getValue();
   public void setValue(double time);
   public void setValue(ConstSFTime time);
   public void setValue(SFTime time);
}

public class SFVec2f extends Field
{
   public SFVec2f(float x, float y);
   public void getValue(float vec[]);
   public float getX();
   public float getY();
   public void setValue(float vec[]);
   public void setValue(float x, float y);
   public void setValue(ConstSFVec2f vec);
   public void setValue(SFVec2f vec);
}

public class SFVec3f extends Field
{
   public SFVec3f(float x, float y, float z);
   public void getValue(float vec[]);
   public float getX();
   public float getY();
   public float getZ();
   public void setValue(float vec[]);
   public void setValue(float x, float y, float z);
   public void setValue(ConstSFVec3f vec);
   public void setValue(SFVec3f vec);
}

public class MFColor extends MField
{
   public MFColor(float value[][]);
   public MFColor(float value[]);
   public MFColor(int size, float value[]);

   public void getValue(float colors[][]);
   public void getValue(float colors[]);

   public void setValue(float colors[][]);
   public void setValue(int size, float colors[]);
   /****************************************************
    color[0] ... color[size - 1] are used as color data
    in the way that color[0], color[1], and color[2] 
    represent the first color. The number of colors
    is defined as "size / 3".
    ***************************************************/

   public void setValue(ConstMFColor colors);

   public void get1Value(int index, float color[]);
   public void get1Value(int index, SFColor color);

   public void set1Value(int index, ConstSFColor color);
   public void set1Value(int index, SFColor color);
   public void set1Value(int index, float red, float green, float blue);

   public void addValue(ConstSFColor color);
   public void addValue(SFColor color);
   public void addValue(float red, float green, float blue);

   public void insertValue(int index, ConstSFColor color);
   public void insertValue(int index, SFColor color);
   public void insertValue(int index, float red, float green, float blue);
}

public class MFFloat extends MField
{
   public MFFloat(float values[]);

   public void getValue(float values[]);

   public void setValue(float values[]);
   public void setValue(int size, float values[]);
   public void setValue(ConstMFFloat value);

   public float get1Value(int index);

   public void set1Value(int index, float f);
   public void set1Value(int index, ConstSFFloat f);
   public void set1Value(int index, SFFloat f);

   public void addValue(float f);
   public void addValue(ConstSFFloat f);
   public void addValue(SFFloat f);

   public void insertValue(int index, float f);
   public void insertValue(int index, ConstSFFloat f);
   public void insertValue(int index, SFFloat f);
}

public class MFInt32 extends MField
{
   public MFInt32(int values[]);

   public void getValue(int values[]);

   public void setValue(int values[]);
   public void setValue(int size, int values[]);
   public void setValue(ConstMFInt32 value);

   public int get1Value(int index);

   public void set1Value(int index, int i);
   public void set1Value(int index, ConstSFInt32 i);
   public void set1Value(int index, SFInt32 i);

   public void addValue(int i);
   public void addValue(ConstSFInt32 i);
   public void addValue(SFInt32 i);

   public void insertValue(int index, int i);
   public void insertValue(int index, ConstSFInt32 i);
   public void insertValue(int index, SFInt32 i);
}

public class MFNode extends MField
{
   public MFNode(Node node[]);

  /******************************************
   * Return value of getValue() must extend Node class.
   * The concrete class is implementation dependent 
   * and up to browser implementation. 
   *******************************************/ 
   public void getValue(Node node[]);

   public void setValue(Node node[]);
   public void setValue(int size, Node node[]);
   public void setValue(ConstMFNode node);

   public Node get1Value(int index);

   public void set1Value(int index, Node node);
   public void set1Value(int index, ConstSFNode node);
   public void set1Value(int index, SFNode node);

   public void addValue(Node node);
   public void addValue(ConstSFNode node);
   public void addValue(SFNode node);

   public void insertValue(int index, Node node);
   public void insertValue(int index, ConstSFNode node);
   public void insertValue(int index, SFNode node);
}

public class MFRotation extends MField
{
   public MFRotation(float rotations[][]);
   public MFRotation(float rotations[]);
   public MFRotation(int size, float rotations[]);

   public void getValue(float rotations[][]);
   public void getValue(float rotations[]);

   public void setValue(float rotations[][])
   public void setValue(int size, float rotations[]);
   public void setValue(ConstMFRotation rotations);

   public void get1Value(int index, float rotation[]);
   public void get1Value(int index, SFRotation rotation);

   public void set1Value(int index, ConstSFRotation rotation);
   public void set1Value(int index, SFRotation rotation);
   public void set1Value(int index, float ax, float ay, float az, float angle);

   public void addValue(ConstSFRotation rotation);
   public void addValue(SFRotation rotation);
   public void addValue(float ax, float ay, float az, float angle);

   public void insertValue(int index, ConstSFRotation rotation);
   public void insertValue(int index, SFRotation rotation);
   public void insertValue(int index, float ax, float ay, float az, float angle);
}

public class MFString extends MField
{
   public MFString(String s[]);

   public void getValue(String s[]);

   public void setValue(String s[]);
   public void setValue(int size, String s[]);
   public void setValue(ConstMFString s);

   public String get1Value(int index);

   public void set1Value(int index, String s);
   public void set1Value(int index, ConstSFString s);
   public void set1Value(int index, SFString s);

   public void addValue(String s);
   public void addValue(ConstSFString s);
   public void addValue(SFString s);

   public void insertValue(int index, String s);
   public void insertValue(int index, ConstSFString s);
   public void insertValue(int index, SFString s);
}

public class MFTime extends MField
{
   public MFTime(double times[]);

   public void getValue(double times[]);

   public void setValue(double times[]);
   public void setValue(int size, double times[]);
   public void setValue(ConstMFTime times);

   public double get1Value(int index);

   public void set1Value(int index, double time);
   public void set1Value(int index, ConstSFTime time);
   public void set1Value(int index, SFTime time);

   public void addValue(double time);
   public void addValue(ConstSFTime time);
   public void addValue(SFTime time);

   public void insertValue(int index, double time);
   public void insertValue(int index, ConstSFTime time);
   public void insertValue(int index, SFTime time);
}

public class MFVec2f extends MField
{
   public MFVec2f(float vecs[][]);
   public MFVec2f(float vecs[]);
   public MFVec2f(int size, float vecs[]);

   public void getValue(float vecs[][]);
   public void getValue(float vecs[]);

   public void setValue(float vecs[][]);
   public void setValue(int size, vecs[]);
   public void setValue(ConstMFVec2f vecs);

   public void get1Value(int index, float vec[]);
   public void get1Value(int index, SFVec2f vec);

   public void set1Value(int index, float x, float y);
   public void set1Value(int index, ConstSFVec2f vec);
   public void set1Value(int index, SFVec2f vec);

   public void addValue(float x, float y);
   public void addValue(ConstSFVec2f vec);
   public void addValue(SFVec2f vec);

   public void insertValue(int index, float x, float y);
   public void insertValue(int index, ConstSFVec2f vec);
   public void insertValue(int index, SFVec2f vec);
}

public class MFVec3f extends MField
{
   public MFVec3f(float vecs[][]);
   public MFVec3f(float vecs[]);
   public MFVec3f(int size, float vecs[]);

   public void getValue(float vecs[][]);
   public void getValue(float vecs[]);

   public void setValue(float vecs[][]);
   public void setValue(int size, float vecs[]);
   public void setValue(ConstMFVec3f vecs);

   public void get1Value(int index, float vec[]);
   public void get1Value(int index, SFVec3f vec);

   public void set1Value(int index, float x, float y, float z);
   public void set1Value(int index, ConstSFVec3f vec);
   public void set1Value(int index, SFVec3f vec);

   public void addValue(float x, float y, float z);
   public void addValue(ConstSFVec3f vec);
   public void addValue(SFVec3f vec);

   public void insertValue(int index, float x, float y, float z);
   public void insertValue(int index, ConstSFVec3f vec);
   public void insertValue(int index, SFVec3f vec);
}
....

C.8.2.3

[#VrmlNodePackage]#vrml.node Package#

package *vrml.node*;

[source]
....
//
// This is the general Node class
// 
public abstract class Node extends BaseNode { 
  
  // Get an EventIn by name. Return value is write-only.
  //   Throws an InvalidEventInException if eventInName isn't a valid
  //   event in name for a node of this type.
  public final Field getEventIn(String fieldName);

  // Get an EventOut by name. Return value is read-only.
  //   Throws an InvalidEventOutException if eventOutName isn't a valid
  //   event out name for a node of this type.
  public final ConstField getEventOut(String fieldName);

  // Get an exposed field by name. 
  //   Throws an InvalidExposedFieldException if fieldName isn't a valid
  //   exposed field name for a node of this type.
  public final Field getExposedField(String fieldName);
}

//
// This is the general Script class, to be subclassed by all scripts.
// Note that the provided methods allow the script author to explicitly
// throw tailored exceptions in case something goes wrong in the
// script.
//
public abstract class Script extends BaseNode { 
 
  // This method is called before any event is generated
  public void initialize();

  // Get a Field by name.
  //   Throws an InvalidFieldException if fieldName isn't a valid
  //   event in name for a node of this type.
  protected final Field getField(String fieldName);

  // Get an EventOut by name.
  //   Throws an InvalidEventOutException if eventOutName isn't a valid
  //   event out name for a node of this type.
  protected final Field getEventOut(String fieldName);

  // processEvents() is called automatically when the script receives 
  //   some set of events. It should not be called directly except by its subclass.
  //   count indicates the number of events delivered.
  public void processEvents(int count, Event events[]);

  // processEvent() is called automatically when the script receives 
  // an event. 
  public void processEvent(Event event);

  // eventsProcessed() is called after every invocation of processEvents().
  public void eventsProcessed()

  // shutdown() is called when this Script node is deleted.
  public void shutdown(); 
}
....


[[ExampleOfException]]
==== C.9 Example of Exception Class

[source]
....

public class InvalidEventInException extends IllegalArgumentException
{
   /**
    * Constructs an InvalidEventInException with no detail message.
    */
   public InvalidEventInException(){
      super();
   }
   /**
    * Constructs an InvalidEventInException with the specified detail message.
    * A detail message is a String that describes this particular exception.
    * @param s the detail message
    */
   public InvalidEventInException(String s){
      super(s);
   }
}

public class InvalidEventOutException extends IllegalArgumentException
{
   public InvalidEventOutException(){
      super();
   }
   public InvalidEventOutException(String s){
      super(s);
   }
}

public class InvalidFieldException extends IllegalArgumentException
{
   public InvalidFieldException(){
      super();
   }
   public InvalidFieldException(String s){
      super(s);
   }
}

public class InvalidExposedFieldException extends IllegalArgumentException
{
   public InvalidExposedFieldException(){
      super();
   }
   public InvalidExposedFieldException(String s){
      super(s);
   }
}

public class InvalidVRMLSyntaxException extends Exception
{
   public InvalidVRMLSyntaxException(){
      super();
   }
   public InvalidVRMLSyntaxException(String s){
      super(s);
   }
}

public class InvalidRouteException extends IllegalArgumentException
{
   public InvalidRouteException(){
      super();
   }
   public InvalidRouteException(String s){
      super(s);
   }
}

public class InvalidNavigationTypeException extends IllegalArgumentException
{
   public InvalidNavigationTypeException(){
      super();
   }
   public InvalidNavigationTypeException(String s){
      super(s);
   }
}

public class InvalidFieldChangeException extends IllegalArgumentException
{
   public InvalidFieldChangeException(){
      super();
   }
   public InvalidFieldChangeException(String s){
      super(s);
   }
}
....


Contact matsuda@arch.sony.co.jp(Kou1 Ma2da), sugino@ssd.sony.co.jp, or honda@arch.sony.co.jp with questions or comments.


[[javascript_html]]
=== Appendix D. JavaScript Scripting Reference

This appendix describes the use of JavaScript with the <<Script, Script>> node. 
See "<<Scripting, Concepts - Scripting>>" for a general overview of scripting 
in VRML, and see "<<Script, Nodes Reference - Script>>" for a description
of the Script node.

• <<Language, D.1 Language>>
• <<Protocol, D.2 Supported Protocol in the Script node's _url_ field>>
• <<FileExtension, D.2.1 File Extension>>
• <<MIMEtype, D.2.2 MIME Type>>
• <<EventIn, D.3 EventIn Handling>>
• <<ParameterPassing, D.3.1 Parameter passing and the EventIn Function>>
• <<EventsProcessed, D.3.2 eventsProcessed() Method>>
• <<Initialize, D.3.3 initialize() Method>>
• <<Shutdown, D.3.3 shutdown() Method>>
• <<AccessingFields, D.4 Accessing Fields and Events>>
• <<AccessingScript, D.4.1 Accessing Fields and EventOuts of the Script>>
• <<AccessingOtherNodes, D.4.2 Accessing Fields and EventOuts of Other Nodes>>
• <<SendingEventOuts, D.4.3 Sending EventOuts>>
• <<ExposedClasses, D.5 JavaScript Objects>>
• <<BrowserClass, D.5.1 Browser Object>>
• <<Mapping, D.5.2 Mapping between JavaScript Types and VRML Types>>
• <<Example, D.6 Example>>

[[d1_Language]]
==== D.1 Language

http://home.netscape.com/comprod/products/navigator/version_2.0/script/index.html[Netscape
JavaScript] was created by Netscape Communications Corporation
(http://home.netscape.com). JavaScript is a programmable API that allows
cross-platform scripting of events, objects, and actions. A full
description of JavaScript can be found at:
http://home.netscape.com/comprod/products/navigator/version_2.0/script/script_info/.
This appendix describes the use of JavaScript as the scripting language
of a Script node.


[[d2_Protocol]]
==== D.2 Supported Protocol in the Script Node's _url_ Field

The url field of the Script node may contain a URL that references
JavaScript code:

[source]
....
 Script {  url "http://foo.com/myScript.js"  }
....

The <<CustomProtocol, javascript: protocol>> allows the
script to be placed inline as follows:

[source]
....
    Script {  url "javascript: function foo() { ... }"   }
....

The _url_ field may contain multiple URLs and thus reference a remote
file or in-line code:

[source]
....
    Script { 
        url [ "http://foo.com/myScript.js",
              "javascript: function foo() { ... }" ]
    }
....

[[d2_1_FileExtension]]
===== D.2.1 File Extension

The file extension for JavaScript source code is *.js*.

[[d2_2_MIMEtype]]
===== D.2.2 MIME Type

The MIME type for JavaScript source code is defined as follows:

[source]
....
        application/x-javascript
....


[[d3_Eventin]]
==== D.3 EventIn Handling

Events sent to the Script node are passed to the corresponding JavaScript 
function in the script. It is necessary to specify the script in the _url_ 
field of the Script node. The function's name is the same as the eventIn and 
is passed two arguments, the event value and its timestamp ( See "<<ParameterPassing, Parameter passing and the EventIn function>>"). If 
there isn't a corresponding JavaScript function in the script, the browser's 
behavior is undefined.

For example, the following Script node has one eventIn field whose name
is _start_:

[source]
....
    Script { 
        eventIn SFBool start
        url "javascript: function start(value, timestamp) { ... }"
    }
....

In the above example, when the _start_ eventIn is sent the start()
function is executed.

[[d3_1_ParameterPassing]]
===== D.3.1 Parameter Passing and the EventIn Function

When a Script node receives an eventIn, a corresponding method in the
file specified in _url_ field of the Script node is called, which has
two arguments. The value of the eventIn is passed as the first argument
and timestamp of the eventIn is passed as the second argument. The type
of the value is the same as the type of the EventIn and the type of the
timestamp is *SFTime*. 
See "<<Mapping, Mapping between JavaScript types and VRML types>>" for a
description of how VRML types appear in JavaScript.

[[d3_2_EventsProcessed]]
===== D.3.2 eventsProcessed() Method

Authors may define a function named _eventsProcessed_ which will be
called after some set of events has been received. Some implementations
will call this function after the return from each EventIn function,
while others will call it only after processing a number of EventIn
functions. In the latter case an author can improve performance by
placing lengthy processing algorithms which do not need to execute for
every event received into the _eventsProcessed_ function.

*Example:*::
  The author needs to compute a complex inverse kinematics operation at
  each time step of an animation sequence. The sequence is
  single-stepped using a TouchSensor and button geometry. Normally the
  author would have an EventIn function execute whenever the button is
  pressed. This function would increment the time step then run the
  inverse kinematics algorithm. But this would execute the complex
  algorithm at every button press and the user could easily get ahead of
  the algorithm by clicking on the button rapidly. To solve this the
  EventIn function can be changed to simply increment the time step and
  the IK algorithm can be moved to an eventsProcessed function. In an
  efficient implementation the clicks would be queued. When the user
  clicks quickly the time step would be incremented once for each button
  click but the complex algorithm will be executed only once. This way
  the animation sequence will keep up with the user.

The _eventsProcessed_ function takes no parameters. Events generated
from it are given the timestamp of the last event processed.

[[d3_3_Initialize]]
===== D.3.3 initialize() Method

Authors may define a function named _initialize_ which is called when
the corresponding Script node has been loaded and before any events are
processed. This can be used to prepare for processing before events are
received, such as construct geometry or initialize external mechanisms.

The _initialize_ function takes no parameters. Events generated from it
are given the timestamp of when the Script node was loaded.

[[d3_3_Shutdown]]
===== D.3.3 shutdown() Method

Authors may define a function named _shutdown_ which is called when the
corresponding Script node is deleted or the world containing the Script
node is unloaded or replaced by another world. This can be used to send
events informing external mechanisms that the Script node is being
deleted so they can clean up files, etc.

The _shutdown_ function takes no parameters. Events generated from it
are given the timestamp of when the Script node was deleted.


[[d4_AccessingFields]]
==== D.4 Accessing Fields

The fields, eventIns and eventOuts of a Script node are accessible from
its JavaScript functions. As in all other nodes the fields are
accessible only within the Script. The Script's eventIns can be routed
to and its eventOuts can be routed from. Another Script node with a
pointer to this node can access its eventIns and eventOuts just like any
other node.

[[d4_1_AccessingScript]]
===== D.4.1 Accessing Fields and EventOuts of the Script

Fields defined in the Script node are available to the script by using
its name. It's value can be read or written. This value is persistent
across function calls. EventOuts defined in the script node can also be
read. The value is the last value sent.

[[d4_2_AccessingOtherNodes]]
===== D.4.2 Accessing Fields and EventOuts of Other Nodes

The script can access any exposedField, eventIn or eventOut of any node
to which it has a pointer:

[source]
....
    DEF SomeNode Transform { }
    Script {
        field SFNode node USE SomeNode
        eventIn SFVec3f pos
        directOutput TRUE
        url "... 
            function pos(value) { 
                node.set_translation = value; 
            }"
    }
....

This sends a set_translation eventIn to the Transform node. An eventIn
on a passed node can appear only on the left side of the assignment. An
eventOut in the passed node can appear only on the right side, which
reads the last value sent out. Fields in the passed node cannot be
accessed, but exposedFields can either send an event to the "set_..."
eventIn, or read the current value of the "..._changed" eventOut. This
follows the routing model of the rest of VRML.

[[d4_3_SendingEventOuts]]
===== D.4.3 Sending EventOuts

Assigning to an eventOut sends that event at the completion of the
currently executing function. This implies that assigning to the
eventOut multiple times during one execution of the function still only
sends one event and that event is the last value assigned.


[[d5_ExposedClasses]]
==== D.5 JavaScript Object

[[d5_1_BrowserClass]]
===== D.5.1 Browser Object

This section lists the functions available in the _browser_ object,
which allows scripts to get and set browser information. Return values
and parameters are shown typed using VRML data types for clarity. For
descriptions of the methods, see the
<<BrowserInterface, Browser Interface>> topic of the
<<Scripting, Scripting>> section of the spec.

[opts=autowidth,frame=ends,grid=rows]
|===
|*Return value* |*Method Name*
|SFString |*getName*()
|SFString |*getVersion*()
|SFFloat  |*getCurrentSpeed()*
|SFFloat  |*getCurrentFrameRate*()
|SFString |*getWorldURL*()
|void     |*replaceWorld*(MFNode nodes)
|SFNode   |*createVrmlFromString*(SFString vrmlSyntax)
|SFNode   |*createVrmlFromURL*(MFString url, Node node, SFString event)
|void     |*addRoute*(SFNode fromNode, SFString fromEventOut, toNode, SFString toEventIn)
|void     |*deleteRoute*(SFNode fromNode, SFString fromEventOut, toNode, SFString toEventIn)
|void     |*loadURL*(MFString url, MFString parameter)
|void     |*setDescription*(SFString description)
|===

[[d5_2_Mapping]]
===== D.5.2 Mapping between JavaScript types and VRML types

JavaScript has few native types. It has strings, booleans, a numeric
type and objects. Objects have members which can be any of the three
simple types, a function, or another object. VRML types are mapped into
JavaScript by considering MF field types as objects containing one
member for each value in the MF field. These are accessed using array
dereferencing operations. For instance getting the third member of an
MFFloat field named _foo_ in JavaScript is done like this:

[source]
....
    bar = foo[3];
....

After this operation _bar_ contains a single numeric value. Note that
array indexing in JavaScript starts at index 1.

Simple SF field type map directly into JavaScript. SFString becomes a
JavaScript string, SFBool becomes a boolean, and SFInt32 and SFFloat
become the numeric type. SF fields with more than one numeric value are
considered as objects containing the numeric values of the field. For
instance an SFVec3f is an object containing 3 numeric values, accessed
using array dereferencing. To access the y component of an SFVec3f named
_foo_ do this:

[source]
....
    bar = foo[2];
....

After this operation _bar_ contains the y component of vector _foo_.

Accessing an MF field containing a vector is done using double array
dereferencing. If foo is now an MFVec3f, accessing the y component of
the third value is done like this:

[source]
....
    bar = foo[3][1];
....

Assigning a JavaScript value to a VRML type (such as when sending an
eventOut), performs the appropriate type conversion. Assigning a one
dimensional array to an SFField with vector contents (SFVec2f, SFVec3f,
SFRotation or SFColor) assigns one element to each component of the
vector. If too many elements is passed the trailing values are ignored.
If too few are passed the vector is padded with 0's. Assigning a numeric
value to an SFInt32 truncates the value.

Assigning a simple value to an MFField converts the single value to a
multi-value field with one entry. Assigning an array to an SFField
places the first array element into the field. Assigning a one
dimensional array to an MFField with vector quantities first translates
the array into the the vector quantity then assigns this as a single
value to the MFField. For instance if foo is a 4 element array and it is
assigned to an MFVec2f, the first 2 elements are converted to an
SFVec2f, the last 2 elements are discarded, then the SFVec2f is
converted to an MFVec2f with one entry.

Assigning a string value to any numeric type (anything but
SFString/MFString) attempts to convert the number to a float then does
the assignment. If it does not convert a 0 is assigned. Assigning to an
SFTime interprets the value as a double.

Assigning to an SFImage interprets the value as a numeric vector with at
least 3 values. The first 2 are the x,y dimensions of the image in
pixels, the third value is the number of components in the image (1 for
monochrome, 3 for rgb, etc.) and the remaining values are pixel colors
as described in "<<SFImage, Fields and Events - SFImage>>".

[[d6_Example]]
==== D.6 Example

Here's an example of a <<Script, Script>> node which
determines whether a given color contains a lot of red. The Script node
exposes a color field, an eventIn, and an eventOut:

[source]
....
Script {
    field    SFColor currentColor 0 0 0
    eventIn  SFColor colorIn
    eventOut SFBool  isRed

    url "javascript: function colorIn(newColor, ts) {
            // This method is called when a colorIn event is received
            currentColor = newColor;
        }

        function eventsProcessed() {
            if (currentColor[0] >= 0.5)
                // if red is at or above 50%
                isRed = true;
        }"
}
....

For details on when the methods defined in ExampleScript are called -
see the "<<ExecutionModel, Concepts - Execution Model>>".

===== Browser class example

* <<BrowserClass, createVrmlFromString method>>

[source]
....
DEF Example Script {
    field   SFNode myself USE Example
    field   MFString url "foo.wrl"
    eventIn MFNode   nodesLoaded
    eventIn SFBool   trigger_event
    url "javascript: function trigger_event(value, ts){
            // do something and then fetch values
            browser.createVRMLFromURL(url, myself, "nodesLoaded");
        }

        function nodesLoaded(value, timestamp){
            // do something
        }"
}
....

* <<BrowserClass, addRoute method>>

[source]
....
DEF Sensor TouchSensor {}
DEF Baa Script {
    field   SFNode myself USE Baa
    field   SFNode fromNode USE Sensor
    eventIn SFBool clicked
    eventIn SFBool trigger_event
    url "javascript: function trigger_event(eventIn_value){
            // do something and then add routing
            browser.addRoute(fromNode, "isActive", myself, "clicked");
        }

        function clicked(value){
        // do something
   }
}
....


Contact rikk@best.com , cmarrin@sgi.com, or gavin@acm.org with questions or comments.


[[bibliography_html]]
=== Appendix E. Bibliography


This appendix contains the informative references in the VRML
specification. These are references to unofficial standards or
documents. All official standards are referenced in "<<references_html, 2. Normative References>>".
[[DATA]]
`[DATA]`::
  "_Data: URL scheme_", IETF work-in-progress, http://www.internic.net/internet-drafts/draft-masinter-url-data-01.txt].

[[GIF]]
`[GIF]`::
  "GRAPHICS INTERCHANGE FORMAT (sm)", Version 89a, which appears in many unofficial places on the WWW, [http://www.radzone.org/tutorials/gif89a.txt , http://www.w3.org/pub/WWW/Graphics/GIF/spec-gif87.txt , "CompuServe at: GO CIS:GRAPHSUP, library 16, "Standards and Specs", GIF89M.TXT, 1/12/95"].

[[FOLE]]
`[FOLE]`::
  "Computer Graphics: Principles and Practice", Foley, van Dam, Feiner and Hughes.

[[OPEN]]
`[OPEN]`::
  OpenGL 1.1 specification, Silicon Graphics, Inc., [http://www.sgi.com/Technology/openGL/spec.html].

[[URN_]]
`[URN]`::
  Universal Resource Name, IETF work-in-progress, [http://services.bunyip.com:8000/research/ietf/urn-ietf/, http://earth.path.net/mitra/papers/vrml-urn.html ].


Contact rikk@best.com, cmarrin@sgi.com, or gavin@acm.org with questions or comments.

[[part1_index_html]]
=== Appendix F. Index

[%autowidth,cols=4,grid=rows]
|====
|<<Anchor, Anchor>>
|<<Appearance, Appearance>>
|<<AudioClip, AudioClip>>
|<<Background, Background>>
|<<Billboard, Billboard>>
|<<Box, Box>>
|<<BrowserExtensions, Browser Extensions>>
|<<CollideStyle, Collision>>
|<<Color, Color>>
|<<ColorInterpolator, ColorInterpolator>>
|<<Cone, Cone>>
|<<Coordinate, Coordinate>>
|<<CoordinateInterpolator, CoordinateInterpolator>>
|<<Cylinder, Cylinder>>
|<<CylinderSensor, CylinderSensor>>
|<<Instancing, DEF>>
|<<DirectionalLight, DirectionalLight>>
|<<ElevationGrid, ElevationGrid>>
|<<Extrusion, Extrusion>>
|<<Fog, Fog>>
|<<FontStyle, FontStyle>>
|<<NFE, field>>
|<<Events, Events>>
|<<Extrusion, Extrusion>>
|<<NFE, eventIn>>
|<<NFE, eventOut>>
|<<NFE, exposedField>>
|<<SyntaxBasics, FALSE>>
|<<FileSyntaxandStructure, File Syntax and Structure>>
|<<Group, Group>>
|<<ISStatement, IS>>
|<<ImageTexture, ImageTexture>>
|<<IndexedFaceSet, IndexedFaceSet>>
|<<IndexedLineSet, IndexedLineSet>>
|<<Inline, Inline>>
|<<LightsAndLighting, Lights and Lighting>>
|<<LOD, LOD>>
|<<Material, Material>>
|<<MFColor, MFColor>>
|<<MFFloat, MFFloat>>
|<<MFInt32, MFInt32>>
|<<MFNode, MFNode>>
|<<MFRotation, MFRotation>>
|<<MFString, MFString>>
|<<MFTime, MFTime>>
|<<MFVec2f, MFVec2f>>
|<<MFVec3f, MFVec3f>>
|<<MovieTexture, MovieTexture>>
|<<SyntaxBasics, NULL>>
|<<NavigationInfo, NavigationInfo>>
|<<NodeConcepts, Node Concepts>>
|<<NFE, Nodes, Fields, and Events>> 
|<<Normal, Normal>>
|<<NormalInterpolator, NormalInterpolator>>
|<<OrientationInterpolator, OrientationInterpolator>>
|<<Prototypes, PROTO>>
|<<PixelTexture, PixelTexture>>
|<<PlaneSensor, PlaneSensor>>
|<<PointLight, PointLight>>
|<<PointSet, PointSet>>
|<<PositionInterpolator, PositionInterpolator>>
|<<Prototypes, Prototypes>>
|<<ProximitySensor, ProximitySensor>>
|<<Routes, ROUTE>>
|<<ScalarInterpolator, ScalarInterpolator>>
|<<Script, Script>>
|<<Scripting, Scripting>>
|<<SFBool, SFBool>>
|<<SFColor, SFColor>>
|<<SFFloat, SFFloat>>
|<<SFImage, SFImage>>
|<<SFInt32, SFInt32>>
|<<SFNode, SFNode>>
|<<SFRotation, SFRotation>>
|<<SFString, SFString>>
|<<SFTime, SFTime>>
|<<SFVec2f, SFVec2f>>
|<<SFVec3f, SFVec3f>>
|<<Shape, Shape>>
|<<Sound, Sound>>
|<<Sphere, Sphere>>
|<<SphereSensor, SphereSensor>>
|<<SpotLight, SpotLight>>
|<<StructuringtheSceneGraph, Structure of the Scene Graph>>
|<<Switch, Switch>>
|<<SyntaxBasics, Syntax Basics>>
|<<SyntaxBasics, TO>>
|<<SyntaxBasics, TRUE>>
|<<Text, Text>>
|<<TextureTransform, TextureTransform>>
|<<TextureCoordinate, TextureCoordinate>>
|<<Time, Time>>
|<<TimeSensor, TimeSensor>>
|<<TouchSensor, TouchSensor>>
|<<Transform, Transform>>
|<<URN, URLs and URNs>>
|<<Instancing, USE>>
|<<Viewpoint, Viewpoint>>
|<<VisibilitySensor, VisibilitySensor>>
|<<WorldInfo, WorldInfo>>
|====

Contact rikk@best.com , cmarrin@sgi.com, or gavin@acm.org with questions or comments.

[[changeLog_html]]
=== Document Change Log

*8/4/96 (rc):* ::

* *Released VRML 2.0 Specification.*

*8/1/96 (rc):* ::

* Many minor fixes (spelling, links, grammar, etc.)

*7/30-31/96 (rc):* ::

* Many minor fixes.

*7/29/96 (rc):* ::

* *Changed* CylinderSensor's _offset_ type from SFRotation --> SFFloat.
* *Changed* default Viewpoint position to 0 0 10 - this was an agreed
  upon change from a while ago that I forgot to integrate. Helps novices
  with world building.
* Moved "_Concepts - Grouping and Children Nodes"_ from 4.9.3 to 4.3.1.

*7/27/96 (rc):* ::

* Updated _Java Reference_ from Sony.
* *Changed* *Fog*'s default _visibilityRange_ from 1000 to 0 (off by default!).
* Clarifed *Extrusion* node - either 1 or N _scale_ and _orientation_
  values are required. If 1, then apply to all spine pts - if N then one
  for each spine pt. This changed the _scale_ defaults
  *BACK* to the original *1 1* (rather than the *[1 1, 1 1]*
  change made after Draft 3).
* *Added* PLAIN _style_ to *FontStyle* (was "").
* Many, many small clarifications.

*7/24/96 (rc):* ::

* *Added* new Script functions to "_Concepts - Scripting_":
  *loadURL(...)* and *setDescription(...)*.
* *Changed* *FontStyle* _justify_ field from SFString to MFString as per
  the _Puk-Marrin-Jan_ referendum.
* New section in Concepts "_4.9.7 Time Dependent Nodes_" describing
  shared info on *TimeSensor*, *MovieTexture*, and *AudioClip*. Several
  important time-based behavior issues were clarified.
* *TimeSensor*, *MovieTexture*, and *AudioClip* have been re-written and
  clarified. Some important clarifications have been made.
* *TimeSensor* ignores _stopTime_ if _stopTime <= startTime_ (this used
  to be a strict inequality) This change provides consistency with
  *AudioClip* and *MovieTexture* and enables a "restart" of time dependent
  nodes via events at a single point in time.

*7/23/96 (rc):* ::

* Moved External API to http://reality.sgi.com/cmarrin/vrml/.
* Clarified that VRML files and *Inline* files must contain children
  nodes at the top level.

*7/22/96 (rc):* ::

* Assorted small fixes.
* Clarified Viewpoint binding and jump behavior.

*7/21/96 (rc):* ::

* Added Box/Cone/Cylinder figures.
* Many clarifications and small fixes.
* Clarified "Concepts - Sensors" section (added Anchor ref).
* concepts: MIME types *model/vrml* and *x-world/x-vrml.*

*7/19-20/96 (rc):* ::

* Many small corrections and clarifications.
* Clarified PROTOs; especially the IS rules.
* Clarified *Background* and *AudioClip* (thanks to Daniel Woods and Chris Fouts).

*7/18/96 (rc):* ::

* *TimeSensor* clarifications (thans to Dan Woods and Chris Fouts).
  Especially w.r.t. _fraction_changed_ outputting 1.0 at end of cycle.
* Changed *AudioClip's* _duration_changed_ eventOut type changed from
  SFFloat to SFTime. This was a mistake.
* Updated latest java.html from Sony.
* Clarified *PROTO:*
** first node is the type of the proto (not first node with an IS)
** DEF's within a PROTO are not exported
* Changed *Inline* to support generic VRML file syntax (not restricted
  to child nodes). This was done to be symmetric/consistent with VRML
  files and prototype rules.
* Many typos and grammar fixes.
* Changed *Exrusion* _scale_ default -> *`+scale [ 1 1, 1 1 ]+`*.

*7/17/96 (rc):* ::

* Clarified *SpotLight* text and added figure.
* Clarified the Binding Node behavior in concepts.html. Bind means top-of-stack, etc.
* Clarified that instanced binding nodes have undefined results.
* Changed *Viewpoint* _bindTime_changed_ to _bindTime_

*7/15/96 (rc):* ::

* *Publish official Draft #3.*

*7/12-14/96 (rc):* ::

* Fixed lots of links.
* Lots of minor error correctsions and clarifications.

*7/11/96 (rc):* ::

* Released unofficial draft of Draft #3.
* Combined all examples into the spec/part1/examples.html section.
* Changed *Switch*'s _whichChild_ to _whichChoice_ to be consistent with
  _choice_ field naming.
* Added "+" and "-" to illegal first characters for names.
* Clarified *PlaneSensor* to default to the local XY plane (the text had
  refs to both XY and XZ).
* Fixed lots of typos and minor errors.
* Added links/ref from nodesRef.html to concepts.html on Grouping nodes
  (_addChildren_ et al fields) and Bounding boxes (_bbox_* fields), and
  removed redundant explanations.
* Changed **Interpolator*'s _value_ exposedField to _keyValue_. This
  avoid the confusion that the _value_changed_ eventOut is tied directly
  to the _value_ exposedField.

*7/10/96 (rc):* ::

* Added Gavin Bell's execution model section to concepts.html. This
  reordered the section and impacted Scripting and Time sections.

*7/9/96 (rc):* ::

* Added/updated missing/old sections (java, conformance, etc.).
* Clarified that DEF, USE, PROTO, EXTERNPROTO, TRUE, FALSE, NULL,
  eventIn, eventOut, field, exposedField, IS, and TO are illegal names
  (concepts.html).
* Sony: Java.html: Remove base node class which implements Node.

*7/8/96 (Sony) (to Java Reference):* ::

* Add base node classes which implement Node.

*7/7/96 (rc):* ::

* *Numerous* style and structure changes as per ISO meeting.
* Changed SFImage (and thus *PixelTexture*) to use RGBA (A = alpha),
  rather than RGBT (T=transparency). This change was motivated by the fact
  that most image formats and rendering libraries specify in terms of
  alpha or opacity, not transparency.
* Created new sections as per ISO (empty for now).
* Added section numbers to Concepts section.
* Moved long examples out of Concepts into examples.html.

*7/3/96 (rc):* ::

(note: many of these changes were part of cm's pre-nuptial spree, the rest were 
undo's of cm's changes...rc)

* *Viewpoint* _description_ field was restored to non-exposed as per
  Draft 2b. Since this field is a hint to the browser, it may not have any
  effect and thus should not be encouraged to be changed.
* *TouchSensor* _hit*_ eventOuts where changed to be _hit*_changed_ as
  per naming convention.
* *Shape* _appearance_ and _geometry_ fields were exposed.
* *PointLight* and *SpotLight* default _radius_ changed to 100 meters (from 1m).
* *NavigationInfo*'s _avatarSize_ field defaults were changed to a more
  typical avatar size (from a radius of 1 meter to an avatar with a radius
  of 0.25m, eye height of 1.6m, and maximum step height of 0.75m). Also,
  the _bind_changed_ event out was changed to _isBound_ as per the field
  naming exceptions.
* _isActive_ eventOut was added to *MovieTexture* to be consistent with
  the *AudioClip* node.
* Added _set_*index_ eventIns to *IndexedFaceSet* and *IndexedLineSet*.
  This enables the geometry to be changed from a script.
* *FontStyle* fields were restored to not exposed to keep
  the implementation easier and since multiple FontStyles can give most of
  the effect needed.
* *Fog* was changed as follows: the _size_ field was removed, *Fog* is
  now a bindable node (thus only one is active at a time and no need for
  _size_), _fogType_ was added for more fog effects, and _set_bind_ and
  _isBound_ events were added to allow binding. This change was made
  mostly to make the implementation easier (multiple Fog nodes with limits
  was extremely difficult to implement). Default _visibilityRange_ changed
  from 1 to 1000. The two values of fogType are: LINEAR and EXPONENTIAL
  (originally the second type was named "FOG" and thus not very clear).
* *CylinderSensor*, *PlaneSensor*, and *SphereSensor* had _offset_ and
  _autoOffset_ exposed fields added - this fixes a major problem with
  these sensors. See concepts.html#DragSensors for for a summary and see
  nodesRef.html for specifics per node.
* *Background*'s _bind_changed_ changed to _isBound_ as per style
  exception in Concepts.
* Clarified field naming exceptions in Concepts (see <<FileSyntaxandStructure>>).
* All events with the name _*Time_changed_ that represent actual
  timestamp had the __changed_ suffix removed, as per the style exception
  defined in Draft 2. This was done for readability and comprehension.
* All _bbox_size_ fields defaults changed from (0,0,0) to (-1,-1,-1).
  Where (-1,-1,-1) means unspecified by the user (and thus up to the
  browser to compute), and (0,0,0) defines an infinitely small, (i.e.
  point), bbox. Added section to Key Concepts on bboxes (<<BoundingBoxes>>).
* Removed "_" from add_children and remove_children fields of group nodes.
* _duration_ eventOut on *AudioClip* was changed to _duration_changed._
* Decided not to pluralize the MF fields - made all fields singular (w/
  exception of children).

*7/2/96 (rc):* ::

* Updated to Working Draft #3 titles and date. Most of these are
described in the 7/3 log.

*7/5/96 (Sony) (to Java Reference):* ::

* The return value of createVrmlFromURL() changes from Node to void.
* The return value of createVrmlFromString() changes from Node to Node[].
* Add 'createVrmlFromURL' and 'addRoute' example(thanks to Justin Couch
  (jtc@hq.adied.oz.au))

*7/3/96 (Sony) (to Java Reference):* ::

* Change from 'interface Node' to 'class Node'.
* Inline script part is deleted.(thanks to Mik Clark(raz89@dial.pipex.com))

*7/1/96 (Sony) (to Java Reference):* ::

* Add 'File extension' and 'MIME type' section.
* Add 'Exception' classes example(thanks to Gad Barnea(barnea@easynet.fr)).

*6/27/96 (Sony) (to Java Reference):* ::

* The return value's type of *Node*'s *getValue()* method changed from
  'ConstField' to '*Field*'.
* Add '*shutdown()*' method to '*Script*' class.
* Add '*getWorldTitle()*' and '*setWorldTitle()*' methods to '*Browser*' class.
* Add the description on how to resolve user-defined classes(thanks to
  Mik Clarke (RAZ89@DIAL.PIPEX.COM)).

*6/14-24/96 (cfm):* ::

* Lots of undocumented changes. 
  **NOTE: I have attempted to document then in later log messages above. (7/2/96...rc).**

*6/14/96 (cfm):* ::

* Updated External API to reflect discussions.
* Offer HTML, compressed HTML and compressed Postscript versions of spec.

*6/5/96 (cfm):* ::

* Added first draft of External API and externalBindings (for review).
* Added Script voting page

*6/4/96 (rc):* ::

* Released Draft #2b.
* Reinstated the _Java Reference_ section (added link to front page).
* Moved the _VRMLScript Reference_ section to an equal level to the _Java Reference_.
* Removed any statements REQUIRING a scripting language (until further notice).
* Added link to the External API section (TBD).

*5/30/96 (rc):* ::

* *Released Draft #2.*

*5/29-30/96 (rc):* ::

* Renamed *ElevationGrid* fields: _verticesPerColumn/Row_ to
  _x/yDimension_, renamed _gridSize_ to _x/ySpacing_ - due to many
  requests and inquiries.
* Various document fixes and final cleanup, spell-check, and details.
* _scriptType_ was removed from the *Script* node - use the data: or
  vrmlscript: syntax.

*5/28/96 (rc):* ::

* Added _bboxCenter, bboxSize, add_children_ and _remove_children_
  eventIns to *Anchor, Billboard,* and *Collision.*
* *Background*'s _sky/groundRange_ field renamed to _sky/groundAngle_ for clarity.
* Daniel Woods re-wrote the *Interpolators* sections in *concepts.html*
  and *nodesRef.html (all Interpolators*) - mostly clarifications and
  improvements.

*5/21-27/96 (rc):* ::

* *Background* URL fields changed from _pos/neg/xyz_ to _backUrl,
  bottonUrl, frontUrl, leftUrl, rightUrl_, and _topUrl_ to remain
  consistent with keeping _url_ in all URL field names.
* Browser camera is child of the current *Viewpoint* (at all times).
  Current *NavigationInfo* is automatically parented to current
  *Viewpoint* too.
* Added _jump_ exposedField to *Viewpoint* node - if TRUE, then when
  binding to *Viewpoint*, browser must move current view to this
  *Viewpoint*.
* Binding rules for *Background*, *NavigationInfo*, and *Viewpoint* (see
  concepts) were clarified - multiple _set_bind_ of TRUE simply move the
  node to top of stack (not added twice), while _set_bind_ FALSE of an
  unbound node have no effect.
* For nested hierarchies of *ProxitySensors* or *VisibilitySensors*, all
  sensors output. If instanced, the user the union of the boxes to test
  against.
* Various clarifications and consistency improvements to *TouchSensor,
  CylinderSensor, PlaneSensor*, and *SphereSensor.*
* Merged *DiskSensor* into *CylinderSensor* - one sensor can do both.
* For nested hierarchies of
  *TouchSensor*/*CylinderSensor*/*PlaneSensor*/*SphereSensor,* the lowest
  node outputs. If siblings, then each one outputs. If instanced, then
  check each instance every test.
* For nested hierarchies of *Collisions,* the nearest collision and
  lowest node outputs. If instanced, then check each instance every test.
* Changed all eventIns and eventOuts to use the recommended naming syntax: 
  set_foo and foo_changed (exceptions for Bool and Time eventOuts - use 
  isFoo for Bool and fooTime for Time).
* Spec does not comment on whether Examine view mode (or any mode for
  that matter) performs collision detection - this left up to the
  browsers.
* *ProximitySensors* do send pos/orient outputs when viewer exits.
* *LineSet* and *PointSet* nodes do not perform collisions (since they have no area).
* Clarified sensors edge conditions - browser needs to handle this.
* Significant changes to *TimeSensor* - defaults have changed and
  evaluation has been clarified and changed.
* Changed _bind_ and _isBound_ to _set_bind_ and _bind_changed_ in
  *Background*, *NavigationInfo*, and *Viewpoint* nodes to be consistent
  with naming recommendations.
* Clarified *TouchSensor* to output _hit*_ events when pointer moves
  regardless if isActive is TRUE -- this enables drag-n-drop actions.
* Moved file syntax defs to top of node sections in Node Reference.
* *NavigationInfo* - recommend that near plane = 1/4 avatar size.
* Re-wrote *VisibilitySensor* section - clarifications only.
* Changed *VisibilitySensor's* bboxC__enter__ and bboxS__ize__
  exposedFields to _center_ and _size_ in order to clarify that these
  boxes are not necessarily bounding boxes and to make more consistent
  with *ProximitySensor*'s box specs.
* Added the missing _isActive_ field to *PlaneSensor*.
* Replaced **TimeSenso**r's _discrete_ field with _cycleTime_ eventOut - much simpler.
* Re-wrote *TimeSensor* and *AudioClip* for clarity.
* Removed *VisibilitySensor*'s _isActive_ field and added the field _enabled._
* Spell check on NodeRef.html.
* Moved sections at the top of _Node Reference_ (e.g. Lights and Lighting) to _Key Concepts_.
* A variety of minor style changes and clarifications to _Node Reference_ and _Key Concepts_.

*5/9/96 (rc):* ::

* Improved *TimeSensor* section
* Miscellaneous clarifications

*4/19/96 (rc):* ::

* Clarified ideal spatial *Sound* implementation
* *ProximitySensor* bbox size of (0 0 0) no longer uses the parent's
  bbox - it disables the sensor.
* Changed all url fields to be exposedFields (*Anchor*, *AudioClip*,
  *Inline*, **Texture*, *Script*)
* Removed _numPoints_ from *PointSet.*

*4/18/96 (rc):* ::

* *Published Draft #1*:

http://vag.vrml.org/VRML2.0/DRAFT1/spec.main.html

*4/17/96 (rc):* ::

* Changed comma to be a whitespace character
* Added _parameters_ field to *Anchor*
* Changed *Anchor* fields to be exposed

*4/12/96 (cm):* ::

* Fixed lots of links and caught some instances of old node names
* Fixed error in *Anchor* example caught by mott
* Got rid of duplicate *FontStyle* description caught by jkent
* Got rid of image field in *ImageTexture* node
* Fixed error in *NavigationInfo* of misnamed and duplicated avatarSize
field

*4/2/96 (rc):* ::

* Changed *ClickSensor* to *TouchSensor* (as per Denton, almost)
* Major update/rewrite *Text* and *FontStyle* nodes and description (thanx to Jan H.).
* Redesigned texture nodes: renamed *Texture2* to *ImageTexture*,
  created *MovieTexture*, and created *PixelTexture* (no URL, just pixels)
* Changed *Fog*'s maximumVisibility to visibilityRange
* Changed *BoxProximitySensor* to *ProximitySensor* (since there's only one now)
* Removed hit* fields and enabled field from *PlaneSensor*
* Changed all URL fields (e.g. filename) to `url' - let's face it these
  are URLs, not files (e.g. data:...").
* Removed *WWW* prefix from all nodes . Too many nodes now have URLs in
  them - prefer to keep the names simple and clear.
* Made the following changes to simplify: *Coordinate3* -> *Coordinate*,
  *Texture2Transform* -> *TextureTransform*, *TextureCoordinate2* ->
  *TextureCoordinate.*
* Changed *Cube* to *Box* ( to be more geometrically correct), and
  merged fields into a single size Vec3f. (Note that Sphere is a sphere
  since it is defined by a radius, not 3 radii.)
* Minor *Fog* re-write and field name change
* Added bind and isBound fields to *NavInfo*, *Viewpoint*, *Background*,
  and wrote the section *Bindable Leaf Nodes*.
* Added clickTime to *TimeSensor* and re-defined time as absolute (not relative)

*3/24/96 (rc):* ::

* Re-organized and alphabetized nodes in Node Ref
* Added *Billboard*
* Replaced *PointSound* and *DirectedSound* with *Sound* and *AudioClip*
* Changed/updated *GeneralCylinder* to *Extrusion*

*3/23/96 (rc):* ::

* Texture2Transform fields exposed
* Disallowed exposed fields in Script nodes
* ColorInterpolate's "eventOut outValue" changed from MF to SFColor to match the Material node
* Re-wrote the Interpolator's section - added more detail
* Lots of minor re-org to improve readability
* Added WWWMovieTexture and ImageTexture
* Changed Texture2 to WWWTexture2 - removed image field

*3/5/96(cm):* Removed voting booth. Added BNF syntax section to spec. Misc. edits to spec.

*1/24/96(cm)*: Added questions to the voting booth.

*1/24/96(cm):* More spec refinements, more logos and a Sample Software section.

*1/23/96(cm):* Added API and major spec update.

*1/16/96(cm):* First public version.

*1/8/96 (gb):* Created.


Contact rikk@best.com, cmarrin@sgi.com, or mailto:gavin@acm.org[gavin@acm,org] with questions or comments.

