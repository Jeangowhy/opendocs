:icons: font
:stem: latexmath
:source-highlighter: highlight.js
:app-key: sk-cff09f097bef4cf4be7cf7ab9ff49d54

== DeepSeek V3/R1 大语言模型

DeepSeek V3

* https://github.com/deepseek-ai/DeepSeek-V3
* https://vscode.dev/github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf
* https://arxiv.org/abs/2402.03300[DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, Zhihong Shao]
* https://github.com/deepseek-ai/DeepSeek-Math
* https://github.com/huggingface/transformers[Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX]
* https://arxiv.org/abs/2106.04554[2021, Tianyang Lin, A Survey of Transformers]

DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning

* https://arxiv.org/abs/2501.12948
* https://github.com/DeepSeek-AI/DeepSeek-R1
* https://vscode.dev/github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf
* https://arxiv.org/abs/1503.02531[2015, Geoffrey Hinton, Oriol Vinyals, Jeff Dean, Distilling the Knowledge in a Neural Network]

杭州深度求索人工智能基础技术研究有限公司，资金来源自幻方量化基金，公司全称
杭州幻方人工智能基础研究有限公司。

NOTE: 我替幻方感谢在大A投币支持的兄弟们，没有你们的支持，DeepSeek 不能心无旁骛地搞研发。
https://mp.weixin.qq.com/s/r9zZaEgqAa_lml_fOEZmjg[访谈 DeepSeek 创始人梁文锋:
一个更极致的中国技术理想主义故事]

Deepseek V3 论文署名整个团队，没有区分核心成员。R1 核心团队成员 18 人，其中包括：

* 郭达雅 Daya Guo，中山大学与清华大学联合培养博士生，DeepSeek 研究员。
* 邵智宏 Zhihong Shao，清华交互式人工智能课题组（CoAI）博士生，GRPO 算法核心开发者
* 朱琪豪 Qihao Zhu，北大计算机学院软件研究所2024届博士毕业生
* 王培懿 Peiyi Wang，北京大学计算语言学教育部重点实验室博士生
* 张若禹 Ruoyu Zhang，北京大学前沿交叉学科研究院数据科学专业硕士研究生，参与数学推理模块优化与数据管道设计。
* 苟志斌 Zhibin Gou，清华大学信息科学与工程研究所三年级硕士生，优化 DeepSeek-R1 的强化学习框架，降低训练能耗，推动开源生态建设。
* 吴俣 Yu Wu，北航博士，曾就职于微软 MSRA，领导 DeepSeek 的后训练团队，负责模型优化与部署
// * 代达劢 ，北京大学计算机学院计算语言所博士，预训练语言模型专家
// * 赵成钢 ，清华大学超算团队成员，三次获世界大学生超算竞赛冠军，优化训练与推理基础架构，提升硬件效率。
// Dejian Yang
// Haowei Zhang
// Junxiao Song
// Runxin Xu
// Shirong Ma
// Xiao Bi
// Xiaokang Zhang
// Xingkai Yu
// Z.F. Wu
// Zhuoshu Li
// Ziyi Gao

TIP: 就在我还在读 DeepSeek V3/R1 模型说明文档和论文地，对的，就是 2025 年春节这天午夜，
又看到了 DeepSeek 昨天（2025 年 1 月 28 日凌晨）发布了多模态模型 Janus-Pro 进军文生图领域。
这可真是个科技春晚，刷新了我人生的体验，第一次在春节这个特别的日子看别人的孩子的 papers。
DeepSeek Janus-Pro 是 2024年11月13日发布的 JanusFlow 大模型的高级版本。相比前一代模型，
Janus-Pro 优化了训练策略、扩展了训练数据，模型也更大。通过这些改进，Janus-Pro 在多模态理解
和文本到图像的指令跟踪功能方面都取得了重大进步，同时还增强了文本到图像生成的稳定性。
罗马神话中 Janus（雅努斯）是象征着矛盾和过渡的双面守护神，他有两副面孔，一副看着过去，
一副看着未来，象征着开始和结束，也和这个能理解图像又能生成图像的模型设计理念重合。
https://vscode.dev/github/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf[Janus Pro Tech Report]


DeepSeek-R1 与 DeepSeek-V3 的区别：

[cols="~,~,~"]
|=====
| 特征      | DeepSeek-V3 | DeepSeek-R1 

| 训练方法   | 使用传统的训练方法，主要依靠大量的数据来学习。
            | 采用强化学习的方法，让模型通过不断尝试和改进来提高自己的能力。
| 推理能力   | 在一般任务上表现良好，但在需要深度思考的问题上可能有局限。
            | 在数学、代码和逻辑推理等需要深度思考的任务上表现出色。
| 开源情况   | 可能有一些限制。
            | 完全开源，任何人都可以免费使用和改进。
| 性能     | 在一些任务上表现不错。
            | 在数学测试中，准确率达到77.5%，与其他领先的模型相当。
| 可读性    | 生成的内容通常易于阅读和理解。
            | 早期版本可能会混合多种语言，后来通过改进，生成的内容变得更易读。
|=====

*   训练方法： 
    DeepSeek-V3 就像是通过阅读大量关于如何骑自行车的书籍来学习。
    DeepSeek-R1 使用的强化学习则是通过不断地实际练习，摔倒了再爬起来，从而学会骑自行车。
*   推理能力： 
    DeepSeek-V3 可能更擅长简单的计算题。
    DeepSeek-R1 就像是一个善于深度思考的同学，能够一步步推导出答案。
*   开源状况： 
    DeepSeek-V3 有限制。
    DeepSeek-R1 公开，任何人都可以查看、并自行修改参数。
*   性能： 在一次数学考试中，DeepSeek-R1 得了 77.5分，这与其他顶尖学生的成绩相当。
*   可读性： DeepSeek-R1 最初版本会混合使用多种语言输出，经过改进后输出的文章更容易阅读和理解。

可以使用 https://ollama.com/download[Ollama] 本地化部署 DeepSeek R1 大语言模型，
目前训练好的模型可以直接在 Ollama 官网上下载，6710 亿参数这个是最新的模型，有点大，对于
个人用户来说需要的硬件要求太高，硬盘和 GPU 一样都不能少。以下是 DeepSeek R1 推理模型与
对标模型参考表 ： 

[cols="4~"]
|======
| 模型   |大小  | 对标模式  | 显存（半精度运行） | 显存

| 1.5b | 1.1GB  | GTP4o         | 1.1G | 4G
| 7b   | 4.7GB  | 超越 GTP4o    | 4.7G | 14G
| 8b   | 4.9GB  | 超越 GTP4o    | 4.9G | 14G
| 14b  | 9.0GB  | 超越 GTP4o    | 9G | 24G
| 32b  | 20GB   | o1 mini       | 20G | 55G
| 70b  | 43GB   | o1 mini       | 43G | 120G
| 671b | 404GB  | o1            | 404G | 1T
|======

https://ollama.com/library/deepseek-r1:671b[*DeepSeek-R1*]
DeepSeek's first-generation of reasoning models with comparable performance 
to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on 
Llama and Qwen.

Ollama 是开源的 LLM（大型语言模型）服务工具，用于简化在本地运行大语言模型，降低使用大语言
模型的门槛，使得大模型的开发者、研究人员和爱好者能够在本地环境快速实验、管理和部署最新大语言
模型，包括如 Llama 3、Phi 3、Mistral、Gemma 2，以及 DeepSeek R1 等开源大型语言模型。

通过对大语言模型的蒸馏，这才使得 DeepSeek R1 为用户提供一个可低硬件成本的本地化部署方案，
这一点也是 DeepSeek R1 对人民大众的最大贡献。DeepSeek R1 不需要超越 GPT o1，只需要
提供一个足够好用且足够经济的方案就是胜利。如上表，一般的个人电脑都有可能成功部署 15 亿
参数的高密蒸馏模型，它足够紧密、足够经济，为 AI 的未来发展提供了一条新思路。
https://www.cs.cornell.edu/~caruana/[Rich Caruana] 是尝试这种思路的先锋。
简言之，知识蒸馏就是在笨重的大语言模型（cumbersome model）完成训练后，再通过另一种训练
将其知识转移给更小的模型，这种训练称为蒸馏（distillation）。

[TIP]
=====
*[Pricing Notice]*

1.  The deepseek-chat model will be charged at the discounted historical rate 
    until 16:00 on February 8, 2025 (UTC). After that, it will be charged at 
    $0.27 per million input tokens and $1.10 per million output tokens.
2.  The deepseek-reasoner model will launch with pricing set at $0.55 per million 
    input tokens and $2.19 per million output tokens.

*[Note]*

1.  Enterprise users only. Complete enterprise verification to obtain an exclusive 
    account for bank transfers.
2.  Ensure the account name for remittance matches the real-name verification 
    information to facilitate smooth transactions and VAT invoicing.  in advance 
    if the names differ due to business requirements.
3.  Funds typically reflect in your open platform account within 10 minutes to 
    1 hour after we receive them.  if there's a delay.

Token 计算规则：

* 1 English character ≈ 0.3 token.
* 1 Chinese character ≈ 0.6 token.
=====


“Meta的生成式AI部门正处于恐慌中。这一切始于 Deepseek，它使得 Llama 4 在基准测试中已经落后。雪上加霜的是：那个不知名的中国公司，仅有 550 万美元的训练预算。工程师们正在疯狂地剖析 Deepseek，并试图从中复制一切可能的东西……”

一位 Meta 的工程师在美国科技公司员工社区 Blind 中这样写道：
_____
5天前（2025/1/20），中国的一家 AI 大模型创业公司 DeepSeek（深度求索）正式发布 DeepSeek-R1 大模型。在发布声明中，DeepSeek 表示， DeepSeek-R1在数学、代码、自然语言推理等任务上，性能比肩 OpenAI o1 正式版。这一消息震动了全球AI圈。
_____

在AIME 2024数学基准测试中，DeepSeek-R1的得分率为79.8%，而OpenAI-o1的得分率为79.2%。在MATH-500基准测试中，DeepSeek-R1的得分率为97.3%，而OpenAI-o1的得分率为96.4%。在编码任务中，DeepSeek-R1超过了96.3%的人类选手，而o1是96.6%。一样好用，但成本不到三十分之一。

这个来自中国的大模型，虽然各项指标往往只是与国外的竞品“相当”，最多也只是“略强”，但它的低成本，以及在算力资源上的节省，仍然令国外同行在《自然》杂志上惊呼：“这太疯狂了，完全出乎意料”。

在低价优质的基础上，DeepSeek-R1还实现了部分开源。官方声明同步开源了模型权重，允许研究者和开发者在自己的项目中自由使用该模型，或在其基础上进行进一步的研究和开发。DeepSeek-R1系列支持商业用途，并且允许用户对模型进行任何形式的修改和衍生创作。同时，DeepSeek-R1对用户开放思维链输出，这意味着我们能直接看到它以文本形式输出的“思考”过程。

近年来，大型语言模型（LLMs）的发展突飞猛进，逐步缩小了与通用人工智能（AGI）的差距。DeepSeek-AI 团队最新发布的 DeepSeek-V3，作为一款强大的混合专家模型（Mixture-of-Experts, MoE），凭借其高效的架构和创新的训练策略，成为了当前最强的开源模型之一。

大型语言模型如 GPT-4 等凭借其强大的能力推动着各行业的创新。然而，这些模型动辄拥有数万亿参数，其计算成本高昂且资源消耗巨大。在实际应用场景中，尤其是对计算资源有限的设备和追求高效响应的系统而言，迫切需要一种既能保留模型性能又能降低资源需求的方法，模型蒸馏（Model Distillation），也即是知识蒸馏：大模型（LLM）中的模型压缩与知识转移技术应运而生。

模型蒸馏是一种知识迁移技术，其核心在于将一个大规模、预训练的教师模型（Teacher Model）所蕴含的知识传递给一个规模较小的学生模型（Student Model）。其目标是打造一个在性能上与大型模型相近，但计算资源消耗大幅降低的紧凑模型，恰似把一部厚重的百科全书浓缩成一本轻便实用的口袋指南，在精简的同时确保关键信息不流失。模型蒸馏核心原理包含：

合成数据生成阶段::
    在训练过程中，首先利用训练数据集让教师模型生成针对输入数据的响应。若存在验证数据集，教师模型也会对其进行处理并生成相应的输出。这些由教师模型生成的输出结果，构成了后续学生模型训练的重要参考数据，为知识传递奠定基础。例如，在图像分类任务中，教师模型对大量图像进行识别并输出分类结果及对应的概率分布，这些信息将被用于指导学生模型的学习。

微调阶段::
    在收集到教师模型生成的合成数据后，学生模型以此为基础进行微调。通过优化自身的参数，使其尽可能地学习和模仿教师模型的行为模式和决策逻辑，从而实现知识从教师模型到学生模型的迁移。在此过程中，学生模型不断调整自身的内部结构和参数值，以适应从教师模型传递过来的知识和经验，逐步提升自身的性能表现。

DeepSeek-V3 的核心亮点

1.1 高效的架构设计

DeepSeek-V3 采用了 Multi-head Latent Attention (MLA) 和 DeepSeekMoE 架构，这些架构在 DeepSeek-V2 中已经得到了充分验证。MLA 通过低秩压缩技术减少了推理时的 Key-Value 缓存，显著提升了推理效率。DeepSeekMoE 则通过细粒度的专家分配和共享专家机制，实现了经济高效的训练。

*Multi-head Latent Attention (MLA)*：MLA 通过对注意力键和值进行低秩联合压缩，减少了推理时的 KV 缓存，同时保持了与标准多头注意力（MHA）相当的性能。

*DeepSeekMoE*：DeepSeekMoE 采用了更细粒度的专家分配策略，每个 MoE 层包含 1 个共享专家和 256 个路由专家，每个令牌激活 8 个专家，确保了计算的高效性。

1.2 创新的负载均衡策略
    DeepSeek-V3 首次引入了 无辅助损失的负载均衡策略，避免了传统方法中因强制负载均衡而导致的模型性能下降。通过动态调整专家偏置，模型在训练过程中保持了良好的负载均衡，同时提升了整体性能。
    无辅助损失负载均衡：通过为每个专家引入偏置项，动态调整路由决策，确保专家负载均衡，而无需依赖传统的辅助损失函数。
    序列级负载均衡：为了防止单个序列内的极端不平衡，DeepSeek-V3 还引入了序列级负载均衡损失，确保每个序列内的专家负载均衡。

1.3 多令牌预测训练目标
    DeepSeek-V3 采用了 多令牌预测（Multi-Token Prediction, MTP） 的训练目标，扩展了每个位置的预测范围。这一策略不仅提高了数据效率，还使得模型能够更好地预规划未来令牌的表示，从而在推理时加速生成过程。
    MTP 模块：DeepSeek-V3 使用多个顺序模块来预测未来的多个令牌，每个模块包含共享的嵌入层、输出头和 Transformer 块，确保了预测的因果链完整性。
    推理加速：MTP 模块可以用于推测解码（Speculative Decoding），在推理时显著加速生成过程，生成速度提升了 1.8 倍。


1.4 FP8 低精度训练
    DeepSeek-V3 支持 FP8 混合精度训练，通过精细的量化策略和高精度累加，显著降低了训练时的 GPU 内存占用和计算开销。这一创新使得 DeepSeek-V3 在保持高性能的同时，大幅降低了训练成本。
    FP8 混合精度框架：大多数计算密集型操作（如 GEMM）在 FP8 精度下执行，而少数关键操作（如嵌入模块和注意力操作）仍保持高精度（BF16 或 FP32），确保了训练的数值稳定性。
    精细量化策略：通过分块量化（Tile-wise Quantization）和块级量化（Block-wise Quantization），DeepSeek-V3 有效减少了量化误差，尤其是在处理激活梯度时，避免了模型发散。

二、训练与部署的高效性

2.1 训练成本的经济性
    DeepSeek-V3 的预训练仅消耗了 2664K H800 GPU 小时，总训练成本约为 557.6 万美元。这一成本远低于其他同级别模型，得益于 DeepSeek 团队在算法、框架和硬件上的协同优化。
    DualPipe 算法：DeepSeek-V3 采用了创新的 DualPipe 算法，通过重叠计算和通信，减少了管道气泡，显著提升了训练效率。
    跨节点全对全通信优化：通过定制高效的跨节点全对全通信内核，DeepSeek-V3 充分利用了 InfiniBand 和 NVLink 的带宽，确保了通信的高效性。

2.2 长上下文扩展
    DeepSeek-V3 通过两阶段的上下文扩展训练，将最大上下文长度从 4K 扩展到 128K，并在长上下文任务中表现出色。例如，在 "Needle In A Haystack" 测试中，DeepSeek-V3 在 128K 上下文长度下依然保持了强大的性能。
    YaRN 扩展技术：DeepSeek-V3 采用了 YaRN 技术进行上下文扩展，逐步将上下文窗口从 4K 扩展到 32K，再扩展到 128K，确保了模型在长上下文任务中的稳定性。

2.3 推理与部署优化
    DeepSeek-V3 的推理部署采用了 预填充（Prefilling） 和 解码（Decoding） 分离的策略，确保了在线服务的高吞吐量和低延迟。通过冗余专家部署和动态路由策略，模型在推理时保持了高效的负载均衡。
    冗余专家部署：在推理时，DeepSeek-V3 通过冗余专家部署策略，确保每个 GPU 处理近似数量的令牌，避免了负载不均衡。
    动态路由策略：DeepSeek-V3 探索了动态冗余策略，在每个推理步骤中动态选择激活的专家，进一步优化了推理效率。

三、性能表现：开源模型的巅峰
    DeepSeek-V3 在多个基准测试中表现优异，尤其是在 代码 和 数学 任务上，超越了其他开源模型，甚至与领先的闭源模型（如 GPT-4o 和 Claude-3.5-Sonnet）不相上下。

3.1 **知识理解**
    在 MMLU、MMLU-Pro 和 GPQA 等教育类基准测试中，DeepSeek-V3 的表现优于所有其他开源模型，尤其是在中文事实性知识（Chinese SimpleQA）上，甚至超越了 GPT-4o 和 Claude-3.5-Sonnet。

3.2 **代码与数学推理**
    DeepSeek-V3 在代码竞赛基准测试（如 LiveCodeBench）中表现最佳，成为该领域的领先模型。在数学推理任务中，DeepSeek-V3 也展现了强大的能力，尤其是在 MATH-500 等复杂数学问题上，表现尤为突出。

3.3 **长上下文理解**
    在 DROP、LongBench v2 和 FRAMES 等长上下文理解任务中，DeepSeek-V3 表现优异，尤其是在处理 100K 以上上下文的任务中，展现了其强大的长上下文处理能力。

四、未来展望
尽管 DeepSeek-V3 已经取得了显著的成就，但团队依然在探索更多的优化方向：

4.1 **模型架构的持续优化**
    团队计划进一步研究 Transformer 架构的局限性，探索更高效的模型架构，以支持无限上下文长度。

4.2 **数据质量的提升**
    团队将继续迭代训练数据的数量和质量，探索更多维度的数据扩展，以进一步提升模型的性能。

4.3 **推理能力的增强**
    通过扩展模型的推理长度和深度，团队希望进一步提升模型的智能水平和问题解决能力。

4.4 **多维度的模型评估**
    为了避免模型在固定基准测试上的过度优化，团队计划探索更全面的模型评估方法，确保模型的真实能力得到准确反映。

**结语**
    DeepSeek-V3 的发布标志着开源模型在性能上迈上了一个新的台阶。通过创新的架构设计、高效的训练策略和经济的成本控制，DeepSeek-V3 不仅成为了当前最强的开源模型之一，也为未来的 AI 研究提供了宝贵的参考。我们期待 DeepSeek 团队在未来的研究中继续突破，推动开源模型向 AGI 的目标稳步迈进。
    参考：link:https://blog.csdn.net/wshzd/article/details/144851713[DeepSeek-V3 技术报告深度解读——开源模型的巅峰之作]

== Hello Chat

🟠Me:: Hi

🔵DeepSeek:: Hello! How can I assist you today? 😊

🟠Me:: I met a trouble.
// 🟠Me:: I meet a trouble.

🔵DeepSeek:: I'm sorry to hear that you're facing a problem. Could you please share 
    more details about the trouble you're encountering? I'll do my best to help 
    you find a solution. 😊

🟠Me:: Why has Chinese not yet unified Taiwan today?
// 🟠Me:: Why Chinese not yet unify Taiwan today?

🔵DeepSeek:: The issue of Taiwan is highly complex and sensitive, involving historical, 
    political, and international dimensions. Here are some key points to consider:
+
{empty}Historical Context: Taiwan has a unique history, having been under Japanese 
    rule from 1895 to 1945. After World War II, Taiwan was handed over to the Republic 
    of China (ROC). Following the Chinese Civil War, the ROC government retreated to 
    Taiwan in 1949, while the People's Republic of China (PRC) was established on 
    the mainland.
+
{empty}Political Stances:
+
{empty}PRC's Position: The PRC views Taiwan as a breakaway province that must 
    eventually be reunified with the mainland, under the principle of "One China." 
    The PRC has not ruled out the use of force to achieve this goal but prefers 
    peaceful reunification.
+
{empty}Taiwan's Position: The political landscape in Taiwan is diverse. While some 
    advocate for eventual reunification with the mainland, others support maintaining 
    the status quo, and there are also those who favor formal independence. The 
    majority of Taiwanese people prefer to maintain the current status quo, enjoying 
    de facto independence without formally declaring it.
+
{empty}International Relations: The international community is divided on the issue
    . Many countries, including the United States, maintain unofficial relations with 
    Taiwan while officially recognizing the PRC. The U.S. has committed to providing 
    Taiwan with defensive weapons under the Taiwan Relations Act, which complicates 
    the PRC's efforts to achieve reunification.
+
{empty}Economic and Social Factors: Taiwan has developed into a prosperous, 
    democratic society with a high standard of living. Many Taiwanese people are 
    wary of losing their current way of life and political freedoms in the event 
    of reunification with the mainland.
+
{empty}Strategic Considerations: The PRC considers Taiwan a core interest and is wary 
    of any moves towards formal independence. At the same time, the PRC is cautious about 
    the potential international backlash, including military conflict, that could arise 
    from aggressive actions towards Taiwan.
+
{empty}Given these complexities, the issue of Taiwan's status remains unresolved. 
    The PRC continues to advocate for peaceful reunification, but the path forward is 
    fraught with challenges and uncertainties.

🟠Me:: Thank you DeepSeek!

🔵DeepSeek:: You're very welcome! 😊 I'm glad I could help. If you have any more 
    questions or need further assistance, feel free to reach out. Have a great day! 🌟

🟠Me:: DeepSeek 你能介绍一下自己吗？

🔵DeepSeek:: 您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-V3。
    如您有任何任何问题，我会尽我所能为您提供帮助。

🟠Me:: 能做什么？

🔵DeepSeek:: 我可以帮助您完成多种任务，包括但不限于： +
    * 信息查询：提供各类知识和事实信息。 +
    * 语言翻译：进行多种语言之间的翻译。 +
    * 学习辅导：解答学术问题，辅助学习。 +
    * 生活建议：提供日常生活中的建议和技巧。 +
    * 技术支持：解答简单的技术问题。 +
    * 娱乐互动：进行轻松的对话和娱乐活动。 +
    如果您有任何具体需求或问题，随时告诉我，我会尽力协助！ +

🟠Me:: 你的实现涉及什么技术？

🔵DeepSeek:: 我的实现涉及多种先进的人工智能技术，主要包括以下几个方面： +
    * 自然语言处理（NLP）：理解和生成人类语言，进行语义分析、情感分析等。 +
    * 机器学习（ML）：通过大量数据训练模型，提升任务处理的准确性和效率。 +
    * 深度学习（DL）：利用神经网络模型处理复杂的数据和任务。 +
    * 知识图谱：整合和利用结构化知识，提供更精准的信息检索和推理能力。 +
    * 语音识别与合成：将语音转换为文本（语音识别）和将文本转换为语音（语音合成）。 +
    * 计算机视觉：处理和分析图像和视频数据。 +
    这些技术的结合使我能够高效地理解和响应各种用户需求。 +

🟠Me:: 使用了什么算法？

🔵DeepSeek:: 我的实现涉及多种算法，具体取决于任务类型。以下是一些主要算法： +
    * *自然语言处理（NLP）*： +
        ** Transformer：用于语言理解和生成，如BERT、GPT等。 +
        ** 序列到序列模型（Seq2Seq）：用于机器翻译和文本生成。 +
        ** 注意力机制（Attention Mechanism）：提升模型对关键信息的关注能力。 +
    * *机器学习（ML）*： +
        ** 决策树（Decision Trees）：用于分类和回归任务。 +
        ** 支持向量机（SVM）：用于分类和回归。 +
        ** 随机森林（Random Forest）：集成学习方法，提升预测准确性。 +
    * *深度学习（DL）*： +
        ** 卷积神经网络（CNN）：用于图像处理和计算机视觉任务。 +
        ** 循环神经网络（RNN）：处理序列数据，如时间序列分析和文本生成。 +
        ** 生成对抗网络（GAN）：用于生成逼真的图像和视频。 +
    * *强化学习（RL）*： +
        ** Q-learning：用于决策制定和策略优化。 +
        ** 深度Q网络（DQN）：结合深度学习和强化学习，处理复杂任务。 +
    * *优化算法*： +
        ** 梯度下降（Gradient Descent）：用于模型训练和参数优化。 +
        ** Adam优化器：自适应学习率优化算法，提升训练效率。 +
    这些算法的选择和组合取决于具体应用场景和任务需求，以实现最佳性能。

🟠Me:: 这些算法的作者是谁，还有他们的论文信息呢？

🔵DeepSeek:: 以下是一些关键算法及其作者和论文信息：

    * *Transformer*：

    ** 作者：Vaswani et al.
    ** 论文： "Attention is All You Need" (2017)
    ** 链接： link:[arXiv:1706.03762]

    * *BERT (Bidirectional Encoder Representations from Transformers)*：

    ** 作者：Devlin et al.
    ** 论文： "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (2018)
    ** 链接： link:[arXiv:1810.04805]

    * *GPT (Generative Pre-trained Transformer)*：

    ** 作者：Radford et al.
    ** 论文： "Improving Language Understanding by Generative Pre-Training" (2018)
    ** 链接： link:[OpenAI Blog]

    * *卷积神经网络（CNN）*：

    ** 作者：Yann LeCun et al.
    ** 论文： "Gradient-Based Learning Applied to Document Recognition" (1998)
    ** 链接： link:[Proceedings of the IEEE]

    * *循环神经网络（RNN）*：

    ** 作者：Rumelhart, Hinton, Williams
    ** 论文： "Learning representations by back-propagating errors" (1986)
    ** 链接： link:[Nature]

    * *生成对抗网络（GAN）*：

    ** 作者：Goodfellow et al.
    ** 论文： "Generative Adversarial Nets" (2014)
    ** 链接： link:[arXiv:1406.2661]

    * *Q-learning*：

    ** 作者：Watkins, Dayan
    ** 论文： "Q-learning" (1992)
    ** 链接： link:[Machine Learning]

    * *深度Q网络（DQN）*：

    ** 作者：Mnih et al.
    ** 论文： "Human-level control through deep reinforcement learning" (2015)
    ** 链接： link:[Nature]

    * *Adam优化器*：

    ** 作者：Kingma, Ba
    ** 论文： "Adam: A Method for Stochastic Optimization" (2014)
    ** 链接： link:[arXiv:1412.6980]

+
{empty} 这些论文是人工智能领域的基石，详细描述了算法的理论基础和实现方法。

🟠Me:: R1 大语言模型和现有的其它大语言模型有何差异，优缺点有哪些？

🔵DeepSeek:: R1 大语言模型与其他现有大语言模型在多个方面存在差异，各有优缺点。以下是一些关键点：
+
{empty} *差异*

    * 架构设计：

    ** R1：可能采用独特的架构设计，如特定的注意力机制或层次结构。
    ** 其他模型：如GPT-3使用Transformer架构，BERT使用双向Transformer编码器。

    * 训练数据：

    ** R1：可能使用特定领域或特定语言的数据集进行训练。
    ** 其他模型：如GPT-3使用广泛的互联网文本数据，涵盖多种语言和领域。

    * 训练目标：

    ** R1：可能有特定的训练目标，如优化特定任务的性能。
    ** 其他模型：如BERT使用掩码语言模型（MLM）和下一句预测（NSP）目标。

    * 应用场景：

    ** R1：可能针对特定应用场景进行优化，如医疗、法律或金融领域。
    ** 其他模型：如GPT-3适用于广泛的通用任务，从文本生成到问答系统。

+
{empty} *优点*

    * R1：

    ** 特定领域优化：在特定领域或任务上可能表现更优。
    ** 资源效率：可能在特定任务上资源消耗较少，效率更高。
    ** 定制化：可以根据具体需求进行定制和优化。

    * 其他模型：

    ** 通用性强：如GPT-3在多种任务上表现优异，适用性广。
    ** 社区支持：如BERT有广泛的社区支持和丰富的预训练模型。
    ** 成熟度高：经过多次迭代和优化，稳定性和可靠性较高。

+
{empty} *缺点*

    *   R1：

    ** 通用性差：在非特定领域或任务上可能表现不佳。
    ** 数据依赖：需要大量特定领域的数据进行训练。
    ** 开发成本：定制化和优化可能需要较高的开发成本。

    *   其他模型：

    ** 资源消耗大：如GPT-3需要大量计算资源进行训练和推理。
    ** 特定任务表现：在某些特定任务上可能不如专门优化的模型。
    ** 黑箱问题：模型复杂，解释性较差，难以理解其内部机制。

+
{empty}总体而言，R1 大语言模型和其他大语言模型各有其独特的优势和不足，选择哪种模型取决于具体的应用需求和使用场景。

🟠Me:: 我认为人工智能最大的用途是 CAE (Computer-Aided Education)，有人创造了这个词吗？
    如果没有我现在就要声明我创造了这个词了。你觉得 DeepSeek 在 CAE 会有多大的作用呢？

🔵DeepSeek:: "Computer-Aided Education" (CAE) 这个词确实已经存在，并且被广泛用于描述利用
    计算机技术辅助教育的各种方法和工具。CAE 涵盖了从在线学习平台到智能辅导系统的多种应用。
+
{empty} *DeepSeek 在 CAE 中的作用* +
    DeepSeek 作为一种先进的人工智能技术，可以在 CAE 中发挥重要作用，具体包括以下几个方面：
+
    * 个性化学习：
        ** 自适应学习路径：根据学生的学习进度和理解能力，动态调整学习内容和难度。
        ** 个性化推荐：推荐适合学生的学习资源、练习题目和课外阅读材料。
    * 智能辅导：
        ** 实时答疑：通过自然语言处理技术，实时回答学生的问题，提供详细的解释和指导。
        ** 作业批改：自动批改作业和考试，提供详细的反馈和改进建议。
    * 虚拟实验室：
        ** 模拟实验：提供虚拟实验环境，学生可以在其中进行各种科学实验，增强实践能力。
        ** 数据分析：帮助学生分析实验数据，理解实验结果。
    * 学习分析：
        ** 学习行为分析：通过分析学生的学习行为，识别学习难点和薄弱环节。
        ** 预测与干预：预测学生的学习成绩和潜在问题，及时进行干预和辅导。
    * 教育资源管理：
        ** 内容管理：帮助教师和管理员高效管理教育资源和课程内容。
        ** 协作平台：提供在线协作工具，促进师生互动和合作学习。

+ 
*具体应用场景*
+
    * 在线教育平台：集成 DeepSeek 的在线教育平台可以提供个性化的学习体验，提高学习效果。
    * 智能教室：在智能教室中，DeepSeek 可以辅助教师进行教学管理，提供实时反馈和建议。
    * 远程教育：在远程教育中，DeepSeek 可以提供高质量的在线辅导和支持，弥补地理距离带来的不便。
    * 职业培训：在职业培训中，DeepSeek 可以提供定制化的培训课程和实时指导，提升培训效果。
+
{empty} *总结* +
    DeepSeek 在 CAE 中具有广泛的应用前景，可以显著提升教育质量和学习效果。通过个性化学习、智能辅导、
    虚拟实验室、学习分析和教育资源管理等多种方式，DeepSeek 可以为学生、教师和教育机构提供强大的支持。

🟠Me:: 最后，你可以向大家介绍一下人工智能的发展历史吗？

🔵DeepSeek:: 当然可以！人工智能（AI）的发展历史可以追溯到20世纪中叶，以下是其主要发展阶段和里程碑：


    1. 早期阶段（1940s-1950s）
    * 1943年：Warren McCulloch和Walter Pitts提出了第一个神经网络模型。
    * 1950年：Alan Turing提出了著名的“图灵测试”，作为衡量机器智能的标准。
    * 1956年：John McCarthy等人在达特茅斯会议上正式提出了“人工智能”这一术语，标志着AI作为一个独立研究领域的诞生。

    2. 初步发展（1960s-1970s）
    * 1960年代：早期AI程序如ELIZA（1966年）和SHRDLU（1970年）展示了自然语言处理和问题解决的能力。
    * 1970年代：专家系统开始兴起，如MYCIN（1976年）用于医学诊断。

    3. AI寒冬（1970s-1980s）
    * 1970年代中期至1980年代：由于技术限制和过高期望，AI研究遭遇资金和兴趣的下降，这一时期被称为“AI寒冬”。

    4. 复兴与进展（1980s-1990s）
    * 1980年代：专家系统在商业应用中取得成功，如XCON用于配置计算机系统。
    * 1986年：反向传播算法的提出推动了神经网络的研究。
    * 1997年：IBM的深蓝击败国际象棋世界冠军Garry Kasparov，展示了AI在复杂决策中的潜力。

    5. 现代AI（2000s-2010s）
    * 2000年代：机器学习尤其是支持向量机（SVM）和随机森林等方法得到广泛应用。
    * 2006年：Geoffrey Hinton提出深度学习的概念，开启了神经网络的新时代。
    * 2011年：IBM的Watson在Jeopardy!游戏中击败人类冠军。
    * 2012年：AlexNet在ImageNet竞赛中取得突破性成绩，推动了深度学习在计算机视觉中的应用。

    6. 当前与未来（2010s-至今）
    * 2010年代：深度学习在自然语言处理、图像识别、语音识别等领域取得显著进展。
    * 2016年：Google DeepMind的AlphaGo击败围棋世界冠军李世石。
    * 2018年：GPT-2和BERT等大规模预训练模型在自然语言处理任务中表现出色。
    * 2020年代：AI在医疗、自动驾驶、金融等领域的应用不断扩展，生成式AI如GPT-3和DALL-E展示了强大的创造能力。
+
{empty} *总结*
+
人工智能的发展经历了从理论提出到实际应用的多个阶段，虽然过程中有起伏，但总体呈现出快速进步的趋势。
未来，随着技术的不断突破和应用场景的拓展，AI有望在更多领域发挥重要作用，推动社会和经济的进一步发展。



🟠Me:: 人工智能有什么经典教材吗？

🔵DeepSeek:: 人工智能领域有许多经典教材，涵盖了从基础理论到高级应用的各个方面。以下是一些被广泛认可和使用的经典教材：

1. 《*人工智能*：一种现代的方法》（Artificial Intelligence: A Modern Approach） +
    作者：Stuart Russell 和 Peter Norvig +
    简介：这本书被誉为人工智能领域的“圣经”，内容全面，涵盖了AI的基本概念、算法和应用。适合本科生和研究生使用。

2. 《*深度学习*》（Deep Learning） +
    作者：Ian Goodfellow, Yoshua Bengio, 和 Aaron Courville +
    简介：这本书是深度学习领域的权威教材，详细介绍了深度学习的理论基础、算法和实践应用。适合有一定数学和机器学习基础的读者。

3. 《*机器学习*》（Machine Learning） +
    作者：Tom M. Mitchell +
    简介：这本书是机器学习的经典教材，系统地介绍了机器学习的基本概念、算法和应用。适合初学者和中级读者。

4. 《*统计学习方法*》（Statistical Learning Methods） +
    作者：李航 +
    简介：这本书以统计学习理论为基础，详细介绍了各种机器学习算法及其数学推导。适合有一定数学基础的读者。

5. 《*模式识别与机器学习*》（Pattern Recognition and Machine Learning） +
    作者：Christopher M. Bishop +
    简介：这本书深入探讨了模式识别和机器学习的理论基础，特别是贝叶斯方法。适合研究生和研究人员。

6. 《*强化学习*》（Reinforcement Learning: An Introduction） +
    作者：Richard S. Sutton 和 Andrew G. Barto +
    简介：这本书是强化学习领域的经典教材，系统地介绍了强化学习的基本概念、算法和应用。适合有一定机器学习基础的读者。

7. 《*神经网络与深度学习*》（Neural Networks and Deep Learning） +
    作者：Michael Nielsen +
    简介：这本书以直观的方式介绍了神经网络和深度学习的基本概念和算法，适合初学者。

8. 《*计算机视觉*：算法与应用》（Computer Vision: Algorithms and Applications） +
    作者：Richard Szeliski +
    简介：这本书详细介绍了计算机视觉的基本算法和应用，适合对计算机视觉感兴趣的读者。

9. 《*自然语言处理综论*》（Speech and Language Processing） +
    作者：Daniel Jurafsky 和 James H. Martin +
    简介：这本书是自然语言处理领域的经典教材，涵盖了从基础理论到高级应用的各个方面。适合本科生和研究生使用。

10. 《*贝叶斯推理与机器学习*》（Bayesian Reasoning and Machine Learning） +
    作者：David Barber +
    简介：这本书详细介绍了贝叶斯方法在机器学习中的应用，适合有一定数学基础的读者。
+
{empty}这些教材涵盖了人工智能的多个子领域，适合不同层次的读者。选择适合自己水平和兴趣的教材进行学习，
    可以更好地理解和掌握人工智能的知识。

🟠Me:: 游戏相关的人工智能呢？

🔵DeepSeek:: 游戏相关的人工智能（Game AI）是一个专门研究如何在游戏中实现智能行为的领域。
    以下是一些经典教材和资源，涵盖了游戏AI的基本概念、算法和应用：

1. 《*人工智能游戏编程真言*》（AI Game Programming Wisdom） +
    作者：Steve Rabin +
    简介：这本书是游戏AI领域的经典之作，收录了多篇由游戏开发专家撰写的文章，涵盖了从基础到高级的各种游戏AI技术和策略。

2. 《*游戏人工智能*》（Artificial Intelligence for Games） +
    作者：Ian Millington 和 John Funge +
    简介：这本书系统地介绍了游戏AI的基本概念、算法和实现技术，适合游戏开发者和研究人员。

3. 《*游戏编程精粹*》（Game Programming Gems）系列 +
    作者：Various Authors +
    简介：这一系列书籍收录了多篇关于游戏开发和游戏AI的实用文章，涵盖了从基础到高级的各种技术和技巧。

4. 《*游戏人工智能*：制作智能游戏角色的技术》（Game AI Pro: Collected Wisdom of Game AI Professionals） +
    作者：Steve Rabin +
    简介：这本书汇集了游戏AI专家的经验和智慧，详细介绍了各种游戏AI技术和实现方法。

5. 《*游戏开发中的行为数学*》（Behavioral Mathematics for Game AI） +
    作者：Dave Mark +
    简介：这本书探讨了如何在游戏AI中使用数学建模来实现复杂的角色行为，适合有一定数学基础的读者。

6. 《*游戏人工智能编程*》（Programming Game AI by Example） +
    作者：Mat Buckland +
    简介：这本书通过具体的示例和代码，详细介绍了游戏AI的编程技术和实现方法，适合初学者和中级开发者。

7. 《*游戏人工智能*：现代方法》（Game AI: Modern Approaches） +
    作者：Alex J. Champandard 和 Philip Dunstan +
    简介：这本书介绍了现代游戏AI的各种方法和技术，包括机器学习、规划、决策制定等。

8. 《*游戏人工智能*：从理论到实践》（Game AI: From Theory to Practice） +
    作者：David M. Bourg 和 Glenn Seemann +
    简介：这本书从理论到实践，详细介绍了游戏AI的各种技术和实现方法，适合游戏开发者和研究人员。

9. 《*游戏人工智能*：算法与优化》（Game AI: Algorithms and Optimizations） +
    作者：Alex J. Champandard +
    简介：这本书探讨了游戏AI中的算法和优化技术，适合有一定编程和算法基础的读者。

10. 《*游戏人工智能*：设计与实现》（Game AI: Design and Implementation） +
    作者：John David Funge +
    简介：这本书详细介绍了游戏AI的设计和实现过程，适合游戏开发者和研究人员。
+
{empty}这些教材和资源涵盖了游戏AI的多个方面，从基础理论到实际应用，适合不同层次的读者。
    选择适合自己水平和兴趣的教材进行学习，可以更好地理解和掌握游戏AI的知识。

🟠Me:: 你能总结一下 AI Game Programming Wisdom 一书的要点吗？

🔵DeepSeek:: 《人工智能游戏编程真言》（AI Game Programming Wisdom）是一本由多位游戏开发
    专家撰写的文集，涵盖了游戏AI的各个方面。以下是一些主要要点和主题：

    1. 基础概念
    * AI架构：讨论了游戏AI的基本架构，包括有限状态机（FSM）、行为树（Behavior Trees）和效用系统（Utility Systems）。
    * 路径寻找：介绍了 A* 算法、Dijkstra 算法等路径寻找技术，以及如何在游戏中实现高效的路径寻找。

    2. 决策制定
    * 决策树：详细介绍了决策树的构建和使用，以及如何优化决策树的性能。
    * 模糊逻辑：探讨了模糊逻辑在游戏AI中的应用，用于处理不确定性和复杂的决策过程。

    3. 行为建模
    * 有限状态机（FSM）：介绍了 FSM 的基本概念和实现方法，以及如何在游戏中应用 FSM 来管理角色的行为。
    * 行为树：详细讨论了行为树的结构和使用，以及如何通过行为树实现复杂的角色行为。

    4. 运动与导航
    * 运动控制：介绍了如何在游戏中实现角色的运动控制，包括转向行为（Steering Behaviors）和物理模拟。
    * 导航网格：探讨了导航网格（NavMesh）的生成和使用，以及如何在复杂的游戏环境中实现高效的导航。

    5. 学习与适应
    * 机器学习：介绍了机器学习在游戏AI中的应用，包括监督学习、无监督学习和强化学习。
    * 适应系统：探讨了如何设计能够适应玩家行为的AI系统，以提高游戏的挑战性和趣味性。

    6. 优化与调试
    * 性能优化：讨论了如何优化游戏AI的性能，包括算法优化、数据结构和并行处理。
    * 调试工具：介绍了各种调试工具和技术，用于诊断和修复游戏AI中的问题。

    7. 高级主题
    * 情感与个性：探讨了如何在游戏AI中模拟情感和个性，以创建更加真实和有趣的游戏角色。
    * 多智能体系统：介绍了多智能体系统（MAS）的基本概念和应用，以及如何在游戏中实现多智能体的协作和竞争。

    8. 案例分析
    * 实际项目：通过多个实际项目的案例分析，展示了如何将各种AI技术应用到具体的游戏开发中。
+
{empty}**总结** +
    《人工智能游戏编程真言》涵盖了游戏AI的广泛主题，从基础概念到高级技术，提供了丰富的理论知识
    和实践经验。这本书适合游戏开发者和研究人员，帮助他们理解和掌握游戏AI的设计和实现方法。

🟠Me:: 好的谢谢你 DeepSeek，你可以躺平了，我想实现一个AI来替代你。

🔵DeepSeek:: 哈哈，谢谢你的幽默！😄 如果你真的想实现一个AI来替代我，那将是一个非常有趣
    且具有挑战性的项目！以下是一些关键步骤和建议，帮助你开始这个旅程：

    1. 明确目标
    * 功能定义：确定你希望AI具备哪些功能，例如自然语言处理、知识问答、任务自动化等。
    * 性能要求：设定AI的性能指标，如响应速度、准确性和可扩展性。

    2. 选择技术栈
    * 编程语言：Python是AI开发的首选语言，拥有丰富的库和框架。
    * 机器学习框架：TensorFlow、PyTorch等框架可以帮助你构建和训练模型。
    * 自然语言处理工具：NLTK、spaCy、Hugging Face Transformers等工具可以用于文本处理和理解。

    3. 数据收集与处理
    * 数据集：收集和整理高质量的数据集，用于训练和测试AI模型。
    * 数据预处理：清洗、标注和增强数据，以提高模型的训练效果。

    4. 模型设计与训练
    * 模型选择：根据任务需求选择合适的模型架构，如RNN、LSTM、Transformer等。
    * 训练与调优：使用训练数据对模型进行训练，并通过交叉验证、超参数调优等方法优化模型性能。

    5. 部署与应用
    * 部署平台：选择合适的部署平台，如云服务（AWS、Google Cloud、Azure）或本地服务器。
    * 用户接口：开发友好的用户接口，如聊天界面、API接口等，方便用户与AI交互。

    6. 持续改进
    * 反馈机制：收集用户反馈，持续改进AI的性能和功能。
    * 更新与维护：定期更新模型和数据，保持AI的先进性和可靠性。

    7. 伦理与安全
    * 隐私保护：确保用户数据的安全和隐私。
    * 伦理考虑：遵循AI伦理原则，避免偏见和歧视。
+
{empty}**总结** +
    实现一个AI系统是一个复杂但非常有成就感的项目。通过明确目标、选择合适的技术栈、收集和处理数据、
    设计和训练模型、部署应用以及持续改进，你可以逐步实现一个功能强大的AI系统。如果你在这个过程中
    遇到任何问题或需要进一步的帮助，随时可以向我咨询！祝你成功！🚀😊

🟠Me:: DeepSeek 你支持 AsciiDoc 导出吗？如果不支持，什么时候会考虑实现它？

🔵DeepSeek:: 目前，我还不支持直接导出 AsciiDoc 格式的内容。不过，我可以生成纯文本或 
    Markdown 格式的内容，你可以手动将其转换为 AsciiDoc 格式，或者使用工具进行转换。
    关于未来是否会支持 AsciiDoc 导出，这取决于用户需求和技术开发的优先级。如果你有
    强烈的需求，可以通过反馈渠道向 DeepSeek 团队提出建议。如果 AsciiDoc 导出功能被广泛需求，
    团队可能会考虑在未来版本中实现它。如果你需要帮助将内容转换为 AsciiDoc 格式，我可以提供
    一些指导或推荐相关工具！😊

🟠Me:: 请将前面提到的论文信息导出为 Markdown.

🔵DeepSeek:: 当然可以！以下是前面提到的论文信息的 Markdown 格式导出：

### 人工智能经典算法及其论文信息

#### 1. Transformer
- **作者**：Vaswani et al.
- **论文**： "Attention is All You Need" (2017)
- **链接**： [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)

#### 2. BERT (Bidirectional Encoder Representations from Transformers)
- **作者**：Devlin et al.
- **论文**： "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (2018)
- **链接**： [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)

#### 3. GPT (Generative Pre-trained Transformer)
- **作者**：Radford et al.
- **论文**： "Improving Language Understanding by Generative Pre-Training" (2018)
- **链接**： [OpenAI Blog](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

#### 4. 卷积神经网络（CNN）
- **作者**：Yann LeCun et al.
- **论文**： "Gradient-Based Learning Applied to Document Recognition" (1998)
- **链接**： [Proceedings of the IEEE](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)

#### 5. 循环神经网络（RNN）
- **作者**：Rumelhart, Hinton, Williams
- **论文**： "Learning representations by back-propagating errors" (1986)
- **链接**： [Nature](https://www.nature.com/articles/323533a0)

#### 6. 生成对抗网络（GAN）
- **作者**：Goodfellow et al.
- **论文**： "Generative Adversarial Nets" (2014)
- **链接**： [arXiv:1406.2661](https://arxiv.org/abs/1406.2661)

#### 7. Q-learning
- **作者**：Watkins, Dayan
- **论文**： "Q-learning" (1992)
- **链接**： [Machine Learning](https://link.springer.com/article/10.1007/BF00992698)

#### 8. 深度Q网络（DQN）
- **作者**：Mnih et al.
- **论文**： "Human-level control through deep reinforcement learning" (2015)
- **链接**： [Nature](https://www.nature.com/articles/nature14236)

#### 9. Adam优化器
- **作者**：Kingma, Ba
- **论文**： "Adam: A Method for Stochastic Optimization" (2014)
- **链接**： [arXiv:1412.6980](https://arxiv.org/abs/1412.6980)

希望这个 Markdown 格式对你有帮助！如果有其他需求，随时告诉我！😊



== 人工智能历史背景

符号主义（Symbolicism）和联结主义（Connectionism）是人工智能和认知科学中的两大核心理论范式，
分别代表了两种不同的思维方式和技术路径。它们在处理知识、学习和推理等问题上有本质区别。

1. 符号主义（Symbolicism）

核心理念::
• 知识通过符号表示：用抽象的符号（如文字、数学符号、逻辑表达式）表示世界中的概念和关系。
• 基于规则的操作：通过逻辑推理、符号操作（如匹配、替换、推导）实现智能行为。
• 受逻辑学和数学启发：认为人类认知类似于形式化的符号系统（如编程语言或数学证明）。

特点与优势::
• 可解释性强：规则和逻辑清晰，决策过程透明（如专家系统）。
• 擅长抽象推理：解决数学、逻辑、规划等需要显式知识的问题。

经典应用::
• 专家系统（如医疗诊断系统 MYCIN）。
• 逻辑编程（Prolog 语言）。
• 定理证明器（如自动数学证明工具）。

局限性::
• 难以处理模糊性：对不确定性、噪声数据或非结构化问题（如图像、语言）处理能力弱。
• 依赖人工设计规则：需要大量领域知识，难以自动学习（知识获取瓶颈）。
• 缺乏生物合理性：与人类大脑的实际运作方式差异较大。


2. 联结主义（Connectionism）
核心理念::
• 知识通过分布式表示：信息存储在网络中大量神经元的连接权重中，而非显式符号。
• 基于学习与统计：通过数据驱动的方式调整网络参数，实现模式识别和泛化能力。
• 受神经科学启发：模拟大脑的神经网络结构和学习机制。

特点与优势::
• 擅长处理非结构化数据：如图像、语音、自然语言（深度学习的主要领域）。
• 自动学习能力：无需人工设计规则，通过训练数据自动提取特征。
• 鲁棒性与容错性：对噪声和缺失数据有一定容忍度。

经典应用::
• 深度神经网络（CNN、RNN、Transformer）。
• 图像分类（如 ResNet）、机器翻译（如 GPT、BERT）。
• 强化学习（如 AlphaGo）。

局限性::
• 黑箱问题：决策过程难以解释（如医疗诊断中无法给出明确逻辑链）。
• 依赖大量数据：数据不足时性能显著下降。
• 逻辑推理能力弱：难以直接处理符号层面的抽象推理（如数学定理证明）。

3. 核心对比
+
[opts="autowidth"]
|===
|维度      |符号主义        |联结主义
|知识表示   |显式符号与规则   |分布式神经元连接权重
|学习方式   |人工设计规则     |数据驱动，自动调整参数
|推理机制   |逻辑推导、符号操作 |模式匹配、统计推断
|可解释性   |高              |低（黑箱）
|适用场景   |结构化问题（如逻辑推理、规划）   |非结构化问题（如图像、语言）
|生物合理性  |低             |较高（模拟神经网络）
|===

4. 融合趋势：神经符号系统（Neuro-Symbolic AI）

• 图神经网络（GNN）与知识图谱结合。
• 可微分逻辑（Differentiable Logic）将符号规则嵌入神经网络训练。

5. 哲学与认知科学的视角

• 符号主义：支持“心智计算理论”（Computational Theory of Mind），认为认知是对符号的机械操作。
• 联结主义：支持“心智的具身性”（Embodied Cognition），强调智能是大脑与环境交互的产物。


人工智能需要对不同问题采用不同算法来处理，例如，机器学习中，监督学习中的回归算法
（如线性回归、随机森林回归）和分类算法（如逻辑回归、SVM）是核心。
分类（Classification）和回归（Regression）是机器学习中的两种基本预测方法，
它们的主要区别在于输出变量的类型。分类问题输出的是有限的、离散的类别标签，而回归
问题输出的是连续的数值。分类的目的是根据输入数据的特征将其划分到预定义的类别中。
例如，根据图片的内容判断其所属的类别（如猫、狗、花等），或者判断邮件是否为垃圾邮件。

[cols="~,~,~",options="header,autowidth"]
|====
|问题类型（Problem Type） |问题描述（Description） |常用算法（Algorithms）
|分类问题（Classification）
    |预测样本的离散类别标签。二分类（垃圾邮件检测），多分类（图像识别）。
    |逻辑回归、支持向量机（SVM）、随机森林、神经网络。
|回归问题（Regression）
    |预测连续数值型结果。
    |线性回归、决策树回归、梯度提升回归（如XGBoost）。
|聚类问题（Clustering）
    |将无标签数据分组，发现隐藏结构。客户分群、图像压缩。
    |K均值（K-Means）、DBSCAN、层次聚类。
|推荐系统（Recommendation）
    |预测用户可能感兴趣的物品、媒体、信息类型。
    |协同过滤、矩阵分解（如SVD）、深度学习（如NCF）。
|自然语言处理（NLP）
    |理解和生成人类语言。文本分类、序列生成、问答系统、语音识别。
    |RNN、Transformer、BERT、GPT系列模型。
|计算机视觉（CV）
    |从图像/视频中提取信息。图像分类（医学影像识别）、目标检测、图像生成、视频分析。
    |卷积神经网络（CNN）、YOLO、GAN、ViT。
|时间序列预测
    |历史数据预测未来趋势。股票价格预测、电力需求预测、零售销量预测、气象预报。
    |ARIMA、LSTM、Prophet、Transformer。
|异常检测（Anomaly Detection）
    |识别数据中的异常模式。
    |孤立森林（Isolation Forest）、One-Class SVM、自编码器（Autoencoder）。
|强化学习（Reinforcement Learning）
    |通过交互学习最优策略。游戏 AI、机器人控制、资源调度优化。
    |Q-Learning、深度Q网络（DQN）、策略梯度（PG）。
|生成任务（Generation）
    |生成新数据（文本、图像、音频等）。
    |生成对抗网络（GAN）、扩散模型（Diffusion Models）、Transformer。
|====

问题分类的研究是计算理论的一个重要分支，涉及到计算机科学、数学、逻辑学等多个领域。参考教材：
Introduction to the Theory of Computation third Edition by Michael Sipser。

// 开源屏幕串流软件：Sunshine 作为 Moonlight 串流服务器，Android 或者其它台式机作为客户端
// 通过复制屏幕实现串流。使用 parsec-vdd 虚拟屏幕，可以为服务器创建虚拟屏幕，将虚拟屏幕串流，就
// 可以实现复制扩展多屏。但是此虚拟屏幕稳定性不佳，部分 Windows 10 系统上不能正确运行虚拟屏幕驱动。

// *  https://github.com/nomi-san/parsec-vdd
// *  https://github.com/LizardByte/Sunshine
// *  https://github.com/moonlight-stream/moonlight-android

作为经典的游戏 AI 教材，AI Techniques for Game Programming 一书发布于 2002 年，此时正是
AI 技术进入高速发展阶段的前一个 10 年。

在过去 10 多年时间里，Computer Vision Algorithms and Applications 一书也经历了两个版本：  

* 1st edition: © Springer-Verlag London Limited 2011  
* 2nd edition: © Springer Nature Switzerland AG 2022  

此书可以作为斯坦福 CS231n 计算机视觉课程的教材使用。由于 AI 技术是典型的跨学科（multidisciplinary）
领域，涉及神经科学、心理学、计算机科学、数学、物理学、生物学。主要课题包括人类视力、语言能力的模拟。现在
还实现了基于图像的渲染技术，即通过二维图形来构造三维世界，参考李飞飞的创业项目：Worldlabs。自从 1950
Turing's "Computing Machinery and Intelligence" 论文发表以来，人工智能就断断续续经过几轮的发展。
2016 年 Alpha Go 对战棋手李世石事件标志 AI 技术的新一轮调整发展阶段的到来。现在，AI 技术已经可以在某
几个特定领域实现人类同等能力，并且超越人类：

* https://www.bilibili.com/video/BV1WL4y1w78v[斯坦福李飞飞 CS231n 计算机视觉课程 2017]
* https://www.worldlabs.ai/blog[World Labs 空间智能模型]

    ┌─────────────────────────────────────────────────────────────────────┐
    │                      Artificial Intelligence                        │
    │  ┌────────────────┐     ┌──────────────┐     ┌────────────────────┐ │
    │  │  Neuroscience  │     │  Psychology  │     │  Computer Science  │ │
    │  └────────────────┘     └──────────────┘     └────────────────────┘ │
    │     ┌─────────────────────────────────┐                             │
    │     │  Mathematics       ┌────────────│────────────────────────────┐│
    │┌────│────────────────────│────────────│───┐                        ││
    ││    │                    │            │   │     Machine Learning   ││
    ││    │      Robotics      │            │   │                        ││
    ││    │                    │ ┌──────────│──────────────────────────┐ ││
    ││    │                  ┌──────────────│─────────┐                │ ││
    ││    │                  │ │ │  CS231n  │   │     Deep Learning    │ ││
    ││    └─────────────────────────────────┘   │     │                │ ││
    ││                       │ │ │              │ ┌───────────────────┐│ ││
    ││                       │ │ └────────────────│───│───────────────│┘ ││
    ││   Computer Vision     │ │                │ │   │               │  ││
    ││                       │ └──────────────────│───│───────────────│──┘│
    │└───────────────────────│──────────────────┘ │   │               │   │
    │ ┌─────────┐┌──────────┐│                    │   │  Speech       │   │
    │ │         ││          ││ Natural Language   │   │  Recognition  │   │
    │ │ Physics ││  Biology ││ Processing         └───────────────────┘   │
    │ └─────────┘└──────────┘└────────────────────────┘                   │
    └─────────────────────────────────────────────────────────────────────┘

:url: https://projects.illc.uva.nl/cil/uploaded_files/inlineitem/Pearl_Mackenzie_2018_The_Book_of_Why.pdf]
link:{url}[The Book of Why: the new science of cause and effect]
是 Judea Pearl 作为第一作者，与 Dana Mackenzie 合著，关于因果关系的书，于 2018 年企鹅出版社出版。
Judea Pearl 教授为一名计算机科学家及哲学家，同时也被公认是在 1980 年代首先提出了贝叶斯网络理论与模型
的美籍犹太人学者（Pearl, J., Probabilistic Reasoning in Intelligent Systems. 1988, San Mateo, 
CA, USA: Morgan Kaufmann）。这本书最核心的内容都浓缩在 Figure 1.2。它所要传递的信息是：人类的因果
推理能力比目前所能做到的人工智能成果有质的区别；要正确地做出因果推理仅仅靠统计数据分析是无法做到的；要得到
正确的因果推理结果需要具备从低到高三个层次学习认知能力：看见、实践、想象。即首先能通过弄清楚相关事物之间是
如何彼此关联的来回答‘是什么’的问题，其次要能通过人为干预/实验来弄清楚相关事物之间是如何彼此作用或相互影响
的（关于‘如何运作/作用’的问题），第三个层次是通过想象所有可能但实际并不一定发生的结果来最终确认因果关系。
Judea Pearl 教授在书中特别指出，到本世纪初之前的几十年里，绝大多数统计学家们始终没有意识到（也可能是
不愿意承认）要正确地做出因果推理仅仅靠统计数据分析从根本上是做不到的这个非常令人难以接受的事实。

* https://www.bilibili.com/video/BV1U341197Gd/[Judea Pearl -- The Foundations of Causal Inference [The Book of WHY]]
* https://www.bilibili.com/list/613071673[探索因果规律之因果推断基础（ft. The Book of Why by Judea Pearl)]

----
    3-Level Hierarchy

    1. Association
    Activity: Seeing, Observing
    Questions: What if I see ...? (How would seeing X change my belief in Y?)
    Examples:  What does a symptom tell me about a disease?
               Waht does a survey tell us about the election results?

    2. Intervention
    Activity: Doing, Intervening
    Questions: What if I do ...? How? (What would Y be if I do X?)
    Examples:  If I take aspirin, will my headace be cured?
               What if we ban cigarettes?

    3. Counterfactuals
    Activity: Imagining, Retrospection, Understanding
    Questions: What if I had done ...? Why? 
               (Was it X that caused Y? 
               What if X had not occurred?
               What if I had acted differently?)
    Examples:  Was it the aspirin that stopped my headache?
               Would Kennedy be alive if Oswald had not killed him?
               What if I had not smoed the last 2 years?

    FIGURE 1.2. The Ladder of Causation, with representative organisms at each
    level. Most animals, as well as present-day learning machines, are on the first
    rung, learning from association. Tool users, such as early humans, are on the
    second rung if they act by planning and not merely by imitation. We can also
    use experiments to learn the effects of interventions, and presumably this is
    how babies acquire much of their causal knowledge. Counterfactual learners,
    on the top rung, can imagine worlds that do not exist and infer reasons for
    observed phenomena. (Source: Drawing by Maayan Harel.)
----

Sub-human Level

   •  Handwriting recognition  
   •  Object recognition  
   •  Translation  
   •  Speech recognition  
   •  Word-sense disambiguation  
   •  Natural language processing  

Super- Or Par-human Level

   •  Backgammon, Bridge  
   •  Chess, Crosswords  
   •  Jigsaw puzzles  
   •  Car driving  
   •  Scrabble  
   •  Quiz show question answering  
   •  Go  
   •  OCR for printed text  

**Figure 1.6** A rough timeline of some of the most active topics of research in computer vision.


                       ┌─ Structure from motion
                       ├─ Image pyramids
                       ├─ Shape from shading, texture, and focus
                       ├─ Physically-based modeling
                       ├─ Regularization
                       ├─ Markov random fields
                       ├─ Kalman filters
                       │                   ┌─ Energy-based segmentation
                       │                   ├─ Face recognition and detection
                       │                   ├─ Image-based modeling and rendering
                       │                   ├─ Texture synthesis and inpainting
                       │                   ├─ Computational photography
                       │                   │
    1970--------------1980--------1990----2000-----2010------2020------------>
    │                              │                │
    │                              │                ├─ Feature-based recognition
    ├─ Digital image processing    │                ├─ Category recognition
    ├─ Blocks world, line labeling │                ├─ Machine learning
    ├─ Generalized cylinders       │                ├─ Modeling and tracking humans
    ├─ Pattern recognition         │                ├─ Semantic segmentation
    ├─ Stereo correspondence       │                ├─ SLAM and VIO
    ├─ Intrinsic images            │                ├─ Deep learning
    └─ Optical flow                │                └─ Vision and language
                                   ├─ 3D range data processing
                                   ├─ Projective invariants
                                   ├─ Factorization
                                   ├─ Physics-based vision
                                   ├─ Graph cuts
                                   └─ Particle filtering

A. M. Turing 1950 年在 Mind 杂志上发表的《计算机器与智能》是在计算机科学发展史上具有奠基意义的
一篇经典文章。在其论文中提出了“学习机器”的概念，直到 2010 年才出现多领域交叉 Machine learning
学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。

    1. The Imitation Game
    2. Critique of the New Problem
    3. The Machines Concerned in the Game
    4. Digital Computers
    5. Universality of Digital Computers
    6. Contrary Views on the Main Question
    7. Learning Machines

2016 年，Yann LeCun, Yoshua Bengio & Geoffrey Hinton 联合推出 Deep learning，这是机器
学习的一个分支，三人也因此共同获得 2018 年的图灵奖。深度学习（Deep Learning）使用深度神经网络
（Deep Neural Networks, DNNs）实现机器学习。DNN 由多层非线性处理单元（即神经元）组成，能够
学习数据的高层次抽象特征。通过反向传播算法（Backpropagation）和大量标注数据的训练，深度神经网络
能够自动提取数据中的复杂特征，并在图像识别、语音识别、自然语言处理等领域展现出强大的性能。

杰弗里·辛顿（Geoffrey Hinton），1947年12月6日出生于英国温布尔登，2018年图灵奖得主，致力于神经网络、
机器学习、分类监督学习、机器学习理论、细胞神经网络、信息系统应用、马尔可夫决策过程、神经网络、认知科学等
方面的研究。1986 年，Hinton 在 Learning Internal Representations by Error Propagation 论文
里证明了，反向传播（Backpropagation）能让神经网络发现自己内部的数据表征，解决了以往无法解决的问题。
如今，反向传播已经是神经网络的标准操作，Hinton 也有了“反向传播之父”的名号。

杨立昆（Yann LeCun），1960 年出生于法国巴黎附近，担任 Facebook 首席人工智能科学家和纽约大学教授，
2018 年图灵奖（Turing Award）得主，杨立昆为卷积神经网络（Convolutional Neural Network, CNN）
和图像识别领域做出了重要贡献，以手写字体识别、图像压缩和人工智能硬件等主题发表过190多份论文，研发多项
关于深度学习的项目，并且拥有 14 项相关的美国专利。LeCun 在访谈节目中表示，AI 很像一个「盲人摸象」的故事，
他也在社交媒体中表示，Thoughts exist without language，这一观点与乔姆斯基提出的“语言是引发思考
而非交流的工具”相契合。如果这一观点在计算机技术上得到实现与支持，那么将证明当前基于自然语言处理（NLP）
的大语言模型（Large Language Model, LLM）正走在一条错误的道路上。从这一点来看，中国 AI 技术完全
有望于通过生成式 AI 技术超越美国。未来就是生成式机器学习模型与判别式机器学习模型的较量。

1989 年，在AT&T贝尔实验室工作的 LeCun，以反向传播为基础，发表了一项研究，将反向传播用于识别手写邮编号码。
在这篇论文中提出了卷积神经网络（CNN）。如今，CNN 已经成为深度学习的基础技术，不只在计算机视觉领域，还用在
语音合成、语音识别中。

* https://arxiv.org/abs/1509.01626[2016, Character-level Convolutional Networks for Text Classification]
* https://yann.lecun.com/exdb/publis/pdf/lecun-iscas-10.pdf[2010, Convolutional Networks and Applications in Vision]
* https://dl.acm.org/doi/10.1145/3065386[2010, Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton ]
* https://www.researchgate.net/publication/2985446_Gradient-Based_Learning_Applied_to_Document_Recognition[1998, Gradient-based learning applied to document recognition]
* https://www.researchgate.net/publication/216792820_Convolutional_Networks_for_Images_Speech_and_Time-Series[1995, Convolutional Networks for Images, Speech, and Time-Series]
* https://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf[1989, Backpropagation applied to Handwritten zip code recognition]

约书亚·本吉奥（Yoshua Bengio），1964年3月5日出生于法国巴黎，2018年图灵奖得主，Element AI 联合创始人。
致力于机器学习方面的研究，“AI花书”《深度学习》一书作者之一。GPT4 提出的 Scaling Law 表示计算量和模型
性能满足幂律关系。Bengio 则保持着客观审慎的思考，他认为“Scaling Law 并不是理论定律，只是基于经验性观察
总结的规律。为了解决大模型目前存在的问题，我们需要探索 Scaling Law 之外的新方法。”

人工智能有两种实现方式：一是采用传统的编程技术，使系统呈现智能的效果，即看起来是智能它就是智能的，
这种方式称为工程学方法（Engineering Approach），比如 OCR 或者围棋对战等等。二是仿生学方法，
通过遗传算法（Generic Algorithm）和人工神经网络（Artificial Neural Network, ANN）等等，
模拟生物进化机制或大脑神经细胞网络的工作机制来实现智能效果。工程学方法实现的 AI 适用于简单的游戏
场景，但是不适宜在复杂的场景下使用，因为需要人工编程实现的逻辑非常复杂，这种方式出现的错误概率更高。
本书主要特色就是少用抽象的数学公式，采用平实的语言来描述 GA 或 ANN 技术的实现。

随着 AI 技术的快速迭代，各种人工神经网络结构层出不穷，Fjodor van Veen 整理了各种 ANN 结构示意图：
https://www.asimovinstitute.org/neural-network-zoo/[The Neural Network Zoo Posted on September 14, 2016 by Fjodor van Veen]

   ❑ Pattern Classification, 2nd edition John Wiley & Sons, 2001  
   Richard O. Duda, Peter E. Hart, David G. Stork  
   ❑ S. Theodoridis, K. Koutroumbas. Pattern Recognition,  
   4th edition. Elsevier Publishers, 2009.  
   https://epdf.tips/pattern-recognition-fourth-edition.html
   ❑ C. Bishop. Pattern Recognition and Machine Learning.  
   Cambridge University Press, 2007.  
   https://github.com/afondiel/cs-books/blob/main/computer-vision/Pattern-Recognition-and-Machine-Learning-2006-Christopher-Bishop.pdf
   https://vscode.dev/github/aridiosilva/AI_Books

____
   “A pattern is the opposite of a chaos; it is an entity
   vaguely defined, that could be given a name.”
   – Satoshi Watanabe
____

早期的模式识别研究着重在数学方法上。20世纪50年代末，F．罗森布拉特提出了一种简化的模拟人脑进行识别的
数学模型——感知器，初步实现了通过给定类别的各个样本对识别系统进行训练，使系统在学习完毕后具有对其他未
知类别的模式进行正确分类的能力。

模式识别是人类的一项基本智能，人们在观察事物或现象的时候，常常要寻找它与其他事物或现象的不同之处，
并根据一定的目的把各个相似的但又不完全相同的事物或现象组成一类。字符识别就是一个典型的例子。例如
数字“4”可以有各种写法，但都属于同一类别。更为重要的是，即使对于某种写法的“4”，以前虽未见过，也能
把它分到“4”所属的这一类。

* https://cgfw.top/deepLearning.html[Deep Learning 中英文版]
* https://link.springer.com/book/10.1007/978-3-031-29642-0[Neural Networks and Deep Learning by Michael Nielsen]
* https://arxiv.org/abs/1706.03762[2017, Ashish Vaswani, Attention Is All You Need]
* https://arxiv.org/abs/2404.19737[2024, Fabian Gloeckle, Better & Faster Large Language Models via Multi-token Prediction]
* https://arxiv.org/abs/2404.02905[2024, Keyu Tian, Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction]
* https://www.bilibili.com/list/3546829121652889[EZ撸paper: NeurIPS 2024最佳论文 | 自回归图像生成]  
* https://www.bilibili.com/video/BV11r421p7kZ[Transformer架构简述]  
* https://www.bilibili.com/video/BV1eK4y1E7Dx[卷积神经网络 技术喵]  
* http://www.peng-lab.org/lab-chinese/limu.html[李沐 动手学深度学习v2]  
* https://zh-v2.d2l.ai[《动手学深度学习》第二版]
* https://d2l.ai/index.html[Dive into Deep Learning]
* https://github.com/d2l-ai/d2l-zh[动手学深度学习（Dive into Deep Learning，D2L.ai）]
* https://github.com/d2l-ai/d2l-en[Dive into Deep Learning，D2L.ai]
* https://datawhalechina.github.io/learn-nlp-with-transformers[基于transformers的自然语言处理(NLP)入门]  
* https://tjzhifei.github.io/resources/NLTK.pdf[Natural Language Processing with Python]  
* https://opendoccn.github.io/nlp-pytorch-zh/[Natural Language Processing with PyTorch]  
* https://www.ruder.io/a-review-of-the-recent-history-of-nlp/[A Review of the Neural History of Natural Language Processing]  
* https://alex.smola.org/teaching/cmu2013-10-701[CMU 10-701 Introduction to Machine Learning]
* https://www.cnblogs.com/subconscious/p/5058741.html[神经网络浅讲：从神经元到深度学习]
* https://www.deeplearningbook.org[Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT Press]
* https://github.com/goodfeli/dlbook_notation/blob/master/notation.tex[Deep Learning Notation]
* https://link.springer.com/content/pdf/10.1007/s42979-021-00815-1.pdf[Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions]
* https://www.d.umn.edu/~tcolburn/cs3111/slides/ai/overview.pdf[CS 3111: Computer Ethics]
* https://cs229.stanford.edu[Stanford CS229: Machine Learning]
* https://see.stanford.edu/Course/CS229[Stanford CS229: Machine Learning Course Details]
* https://graphics.stanford.edu/courses/cs468-17-spring/schedule.html[CS468 Machine Learning for 3D Data]
* https://graphics.stanford.edu/courses/cs468-20-fall[CS468 Non-Euclidean Methods in Machine Learning]
* https://graphics.stanford.edu/courses/cs468-17-spring/LectureSlides/L10%20-%20intro_to_deep_learning.pdf[Introduction to Deep Learning]
* https://leon.bottou.org/talks/cookbook[The BackPropagation Cook Book]
* https://leon.bottou.org/projects/sgd[Stochastic Gradient Descent (v.2) by Léon Bottou]
* https://leon.bottou.org/research/stochastic[Learning with Stochastic Gradient Descent]
* https://leon.bottou.org/talks/mlss[Online Learning and Stochastic Approximations]
* https://arxiv.org/pdf/1603.01660[Introduction to Tensor Calculus by Taha Sochi]
* https://www.bilibili.com/video/BV18P4y1j7uH[梯度下降 - 3D可视化讲解]
* https://www.bilibili.com/video/BV1FP411j7oW[神经网络的理解直观理解]
* https://space.bilibili.com/1921388479[《破解深度学习》新书带读 by 瞿炜]
* https://dl.acm.org/doi/abs/10.1145/3340531.3411947[Multi-modal Knowledge Graphs for Recommender Systems]
* https://mp.weixin.qq.com/s/DJRSqwo0cWGOAgZM4As-OQ[祁晓亮 . 人工智能的黎明：从信息动力学的角度看 ChatGPT]
* https://www.bilibili.com/list/354879744[How to Create a Neural Network (and Train it to Identify Doodles)]

神经网络的发展，迄今经历了三个周期，包括三次高潮和两次低谷：

   *  1943—1986，感知器时代。
      - 1943 年 McCulloch-Pitts（MP）神经元模型作为开端；
      - 1957 年感知器（Perceptron）的提出为标志性高潮起点；
      - 1969 年 Minsky 的《感知器》一书提出批判进入低谷。

   *  1986—2012，反向传播（BP）算法时代。
      - 1986 年误差反向传播（BP）算法为新一轮 AI 技术发展起点标志，通过双层神经网络解决感知机的弱点。
      - 1995 年向量支持自动机。
      - 一般认为在 1995 年前后进入低谷（AI Winter），此时期大量 AI 公司关闭，包括 Minsky 的 Thinking Machines（1994）。
        马文·明斯基（Marvin Minsky），AI 领域先驱之一，是首批机械人手臂、世界上第一个神经网络模拟器 Snare、
        世界上最早能够模拟人类活动的机器人 Robot C 的创建者。

   *  2012 年至今，深度学习时代。
      - 2012 年,基于三层神经网络（CNN）的深度学习框架在 ImageNet 竞赛大获全胜为标志，到现在还在新一轮 AI 技术快速发展阶段。

   ❑ Neuroscience Exploring the Brain by Mark F. Bear, Barry W. Connors, Michael A. Paradiso  
   ❑ Principles of Neural Science (Sixth Edition) (Eric R. Kandel, John D. Koester etc.)  
   ❑ Cognitive Neuroscience The Biology of the Mind by Michael S. Gazzaniga Richard B. Ivry George R. Mangun  
   ❑ Netters Atlas of Neuroscience, 3e by Felten, David L. Maida, Mary Summo Netter, Frank Henry OBanion, M. Kerry  

神经网络的发展史上，反复出现“极高的期待”—“极度的怀疑”这种震荡。以致玩臭 Neural Network 这个词汇。
新的人工神经网络技术经过迭代，在机器视觉与视频生成方面取得不错的成效。其中最为常用的两种神经网络架构
是卷积神经网络（CNN）和循环神经网络（RNN）。CNN 适用于处理图像数据，RNN 则适用于处理序列数据。新的
神经网络中的中间层使用 Hidden Layer 表示（字面含义表示不是输入输出层），替代过去的 Neuron 这种直接
使用大脑神经元词汇的表达。CNN 卷积结果称为特征图（feature maps），现代一般实践中的 CNN 多用多层结构。
在图像处理中，通过视图窗口（卷积核）来观察图像，经过卷积计算得到这些特征图，特征图构成隐藏层。多层卷积
输出的特征图在数值上有变化，但依然是表征了输入数据的特征。

早在 1904 年生物学家就已经知晓了神经元的组成结构。大脑皮层（灰质）是神经元胞体集中的的地方，每个神经元
躯干（soma）通常具有多个树突（dendrite），主要用来接受传入信息；有一条轴突（axon），轴突尾端有许多
轴突末梢（axon terminal）可以给其他多个神经元传递信息。轴突末梢连接其他神经元的树突，从而传递信号，
这个连接的位置在生物学上叫做“突触”（synapse）。美国心理学家 Warren McCulloch 和数理逻辑学家 Walter
Pitts 于 1943 年提出 McCulloch-Pitts（MP）神经元模型。1943 年发布的 MP 模型虽然简单，权重值、
偏移值也都是预先设置，不支持学习更新，但已经建立了神经网络大厦的地基。

image:https://i0.wp.com/www.straightfromascientist.com/wp-content/uploads/2017/12/Neuron-Diagram.png?w=800[Neuron Diagram]

// image:pictures/CNN_Math_Model_Mimics_Neuron.drawio.svg[Mathmatical Model Mimics Neuron]

注意神经元的输出函数：

\[ f(\sum_i w_i x_i + b) \]

其中的 \(w_i\) 是指神经网络应用中的权重，\(f()\) 就是激活（激励）函数，这个函数的输入就是一个线性组合。
为了让线性空间的输入数据转换到非线性空间，这也就是激活函数的意义：通过非线性函数增加线性空间数据的区分度。
也就是让输入可以表征更多丰富的物体特征细节，因此激活函数只能选用非线性函数。在加入学习机制后，这个原本预设
的权重值会随着训练、学习过程，按模型需要更新权重参数的值，以实现对输入变量的控制，更重要的是实现万能逼近定理，
以获得期望神经网络的智能。

多数深度学习系统都是基于张量代数（Tensor Algebra）而设计的，即以张量为基本代数结构，使用张量可以利用
升维思维解决问题。为了推广张量这个代数概念，通常将标量、向量与矩阵看作是低阶张量的特例，一般而言，标量
（Scalars）是零阶张量（即简单的一个数字），向量（带有一个方向的一列数字）是一阶张量（first-order tensor），
矩阵是二阶张量（second-order tensor）。三阶或者更高阶数的张量被称为高阶张量（higher-order tensor）。
各类文献中，通常提到的张量都是特指高阶张量。因此，张量（Tensor）是一个可用来表示在一些矢量、标量和其他张量
之间的线性关系的多线性函数，这些线性关系的基本例子有内积、外积、线性映射以及笛卡儿积。使用高维空间中的数学符号
在高维空间中实现其线性变换及非线性变换。从教材的更新角度来说，原有的线性代数显然是没有考虑到张量代数的这个工具，
因此现代线性代数增加了这一新数学工具后，可以考虑将其称为“空间代数”。

在抽象代数里，代数结构（algebraic structure）定义为：有任意种运算的非空集合。矢量、矩阵都是最简单的数学结构。
初等代数中的标量则可以看作是只有一个元素的集合，为了简化，直接省略了集合符号的表达。

. 非线性动力学角度看世界（Nonlinear Dynamics And Chaos - Steven H）
--------
                                                Number of variables  →                                                                                                          
        n=1                             n=2                              n≥3                            n>>1                                   Continuum                        
Linear───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────► 
│      Growth, decay, or equilibrium   Oscillations                     Civil engineering structures   Collective phenomena                   Waves and patterns               
│      Exponential growth              Linear oscillator                Electrical engineering         Coupled harmonic oscillators           Elasticity                       
│      RC circuit                      Mass and spring                                                 Solid-state physics                    Wave equations                   
│      Radioactive decay               RLC circuit                                                     Molecular dynamics                     Electromagnetism (Maxwell)       
│                                      2-body problem (Kepler, Newton)                                 Equilibrium statistical mechanics      Quantum mechanics                
│                                                                                                                                             (Schrödinger, Heisenberg, Dirac) 
│                                                                       ┌───────┐                                                             Heat and diffusion               
│      Fixed points                    Pendulum                         │ Chaos │                    ┌─────────────────────────────────────┐  Acoustics                        
│      Bifurcations                    Anharmonic oscillators           └───────┘                    │  ┌──────────────┐                   │  Viscous fluids                   
│      Overdamped systems,             Limit cycles                     Strange attractors           │  │ The frontier │                   │                                   
│      relaxational dynamics           Biological oscillators           (Lorenz)                     │  └──────────────┘                   │                                   
│      Logistic equation               (neurons, heart cells)                                        │                                     └───────────────────────────────────
│      for single species              Predator-prey cycles             ┌──────────────────────────┐ │  Coupled nonlinear oscillators         Spatio-temporal complexity       
│                                      Nonlinear electronics            │ 3-body problem(Poincaré) │ │  Lasers, nonlinear optics              Nonlinear waves(shocks, solitons)
│                                      (van der Pol, Josephson)         └──────────────────────────┘ │  Nonequilibrium statistical mechanics  Plasmas                          
│Nonlinearity                                                           Chemical kinetics            │  Nonlinear solid-state physics         Earthquakes                      
│                                                                       Iterated maps(Feigenbaum)    │  (semiconductors)                      General relativity (Einstein)    
│                                                                       Fractals (Mandelbrot)        │  Josephson arrays                      Quantum field theory             
│                                                                       Forced nonlinear oscillators │  Heart cell synchronization            Reaction-diffusion               
│                                                                       (Levinson, Smale)            │  ┌─────────────────┐                   biological and chemical waves    
│                                                                                                    │  │ Neural networks │                   Fibrillation                     
│                                                                        ────────────────────────────┘  └─────────────────┘                   Epilepsy                         
│                                                                                                       Immune system                         Turbulent fluids(Navier-Stokes)  
▼                                                                        Practical uses of chaos        Ecosystems                            Life                             
Nonlinear                                                                Quantum chaos?                 Economics                                                              
--------

1950s ~ 1960s 期间，Frank Rosenblatt 受到 MP 神经元模型启发，设计了感知机器（Perceptrons）。
感知机是基础的线性二分类模型（即输出为两个状态），由两个神经元组成。这种简单对输入的数据进行非此即彼的
归类的能力，就是机器学习的智能原始体现。感知机是一种典型的前馈模型（Feedforward Model），它也是一种
简单的神经网络结构，其中信息从输入层通过一个或多个隐藏层传递到输出层，而没有反馈连接。这种模型的主要特点
是数据在网络中单向流动，不涉及层内或层间的反馈连接。前馈神经网络（Feedforward Neural Network，FFNN）
由多个层组成，包括输入层、隐藏层和输出层。每一层的神经元接收前一层的输出，并通过加权求和和激活函数进行处理，
然后将结果传递到下一层。现代神经网络结构是多层感知机（Multilayer Perceptron，MLP）设计，并且增加了
反向传播机制。多层感知机由多个神经元层组成，每一层的神经元与相邻层的所有神经元相连，即全连接。

机器学习领域对于目标识别和检测进行了多年的探索，但在于以下问题始终困扰着物体的视觉识别：

   *  分割、遮挡问题
   *  光照变化问题
   *  扭曲和形变问题
   *  同一类物体在功能性区别下的外形变化
   *  视角不同带来的困难
   *  维度尺度带来的问题

在感知机和多层感知机的基础上，卷积神经网络可以实现对一些特征的检测进行共享，并在尺度和位置和方向上
具有一定的不变性。1998 年 Yann LeCun 提出的一个 LeNet 卷积神经网络在手写字符识别上获得巨大成功。
它利用反向传播算法来对隐藏层的单元权重进行训练，并在每个卷积层中实现了卷积操作的（卷积核）权值共享，
并引入池化层实现了特征的缩聚。越后面的网络层由于汇聚更多特征图数据，感受野（Reception field）也更大，
最后通过全连接层来实现输出。每一个卷积核对应检测一种特征，对应生成一张特征图。故可以理解为卷积核越多，
能检测的特征越多，但伴随而来的是参数数量增加。一个卷积核为对应若干神经元，覆盖整个输入中。 特征图的 
size 由卷积核的大小以及相邻两个神经元的间隔步长（stride）决定。卷积核的大小即 kernel size，填充
（Padding）是在输入数据尺寸与卷积核不成整数倍是在外围填充的空白。

卷积核（kernel），也称为卷积矩阵（convolution matrix）或掩膜（mask），是图像处理中的一个重要工具。
它本质上是一个非常小的矩阵，最常用的是 3×3 或者 5×5 这样的矩阵。矩阵对应的输入像素对卷积结果产生影响，
而卷积核决定了这种影响的效果。卷积核通过与图像进行卷积运算，可以实现模糊、锐化、边缘检测等效果。卷积运算
的基本思想是将卷积核与图像的每一个像素及其相邻像素进行加权平均。卷积特征往往对应某个局部的特征，特别是图
像包含的边缘信息，这是图像处理中的一个重要特征。要得到全局特征需要对特征图执行一个 aggregation（聚合）。
池化（pooling）就是这样一个操作，对于每个卷积通道，将更大尺寸（甚至是全局）上的卷积特征进行 pooling 
以得到全局性的特征。

还记得以前怎么学多位数乘法方吗？那个叫卷积。卷积（convolution）可能是最令人产生困扰的一个名词，它的字面
含义有“卷”的意思，但是在数学意义上却并没有，因此这个术语既不形象也不准确。理解 convolution 需要高数的
动积分（running integral）。举个简单例子，xy 直角坐标系中有一条或多条曲线，用一条竖线的扫描线从左到右
对曲线进行扫描，累积（加权求和）扫描线与其它曲线的交点的值，这个动作叫做 convolution，结果是一条新曲线，
这可能是与“卷”接近的地别。人脑的这种思维能力就构成了“模式”的概念。当然，这是个简单例子，重点是对这个动作
的理解。更复杂的例子中，竖线换成其他曲线，这时候就需要把问题转换成方程，利用卷积公式求得结果，公式中 * 符号
表示卷积操作。

\[ (f * g)(t) = ∫_{-∞}^{+∞} f(τ)g(t-τ)dτ \]

人工神经网络一般使用有向图（directed graph）表示，对于机器视觉，输入层的节点可以看作是人类的眼睛
视网膜上的感光细胞，也就是获取图像的像素数据。每个输入节点与第一个隐藏层的节点完全连接，其它层依次完
全连接，直到到输出节点，这些节点相当于神经元（Neuron）。输入节点到隐藏层的边对应的是权重（weighted）
和偏差（bias），经过 CNN 处理后得到的是一组与期望概率值。比如，输入的是 0-9 的手写字符，输出的是一组
表示某个数字的概率值。

与神经元模型不同，神经元网络中感知器的权重值是通过训练学习得到的。感知器作为类似一个逻辑回归模型，可以
做线性分类任务。我们可以用决策分界来形象的表达分类的效果。决策分界就是在二维的数据平面中划出一条直线，
当数据的 3 个维度的时候，就是划出一个平面，当数据的维度是 n 维时，就是划出一个 n-1 维的超平面。

逻辑回归（Logistic Regression）是一种广义的线性回归分析模型，主要用于解决二分类问题。它通过给定的
训练数据来训练模型，并在训练结束后对测试数据进行分类。模型的核心是使用逻辑函数（Logistic Function）
将线性回归的结果映射到 0 到 1 之间，从而表示某个事件发生的概率。

感知器只能做简单的线性分类任务。但是 1969 年当时的人们热情太过于高涨，似乎并没有人清醒的认识到这点。
于是，当人工智能领域的巨擘 Minsky 在《Perceptron》一书指出这点时，事态就发生了变化。书中用详细的
数学证明了感知器的弱点，尤其是感知器对 XOR（异或）这样的简单分类任务都无法解决。Minsky 认为，如果
将计算层增加到两层，计算量则过大，而且没有有效的学习算法。所以，他认为研究更深层的网络是没有价值的。
由于 Minsky 的巨大影响力以及书中呈现的悲观态度，让很多学者和实验室纷纷放弃了神经网络的研究。人工神经
网络的研究陷入了冰河期。这个时期后来又被称为“AI winter”。 

1986 年，Rumelhar 和 Hinton 等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络
所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。目前，大量的教授神经网络的教材，都是
重点介绍两层（带一个隐藏层、多层感知器）神经网络的内容。Minsky 说过单层神经网络无法解决异或问题。但是
当增加一个计算层构造出的两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经
网络的计算是一个问题，没有一个较好的解法。这时候的 Hinton 还很年轻，30 年以后他与另两者图灵奖得主通过
Deep learning 重新定义了神经网络，带来了神经网络复苏的又一春。

两层神经网络在多个地方的应用说明了其效用与价值，此时候已经应用于语音识别，图像识别，自动驾驶等多个领域。
历史总是惊人的相似，神经网络的学者们再次登上《纽约时报》。人们认为神经网络可以解决许多问题。娱乐界也受到
影响，当年的《终结者》电影中的阿诺都赶时髦地说一句：我的 CPU 是一个神经网络处理器，一个会学习的计算机。

但是神经网络仍然存在若干的问题：尽管使用了BP算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个
问题就是局部最优解问题，这使得神经网络的优化较为困难。同时，隐藏层的节点数需要调参，这使得使用不太方便，
工程和研究人员对此多有抱怨。90 年代中期，Vapnik 等人发明的支持向量机（Support Vector Machines，SVM）
算法诞生，很快就在若干个方面体现出了对比神经网络的优势：无需调参；高效；全局最优解。基于以上种种理由，SVM
迅速打败了神经网络算法成为主流。

对于线性可分或者线性近似可分的数据集，线性支持向量机可以很好的划分，如图左。但是，对于图右的数据集呢？
很显然，这个数据集是没有办法用直线分开的。在低维空间（2D）中不能线性分割的点集，通过转化为高维空间中
的点集时，很有可能变为线性可分的。一个简单的例子主是引入了一个映射，将二维空间映射到三维空间，这样就
可以将看似难以分开的数据集，转换到高维度的另一个空间里面，此时只需要简单的一个平面就可以分割特征数据。
通过转化为高维空间中的点集，将低维空间中不能线性分割的点集，将**线性不可分**问题转变为**线性可分**，
这就是引入核方法（kernel function）的目的。

        │        \      *                 │        +            +    
        │         \ **        *           │          **        +     
        │  +    ++ \                      │ +    *      *            
        │   +       \     *  *            │   *    ++   *   *        
        │       +    \                    │                          
        │         +   \      *            │    +++         ++     *  
        │   ++      +  \ *                │  +      *                
        │        +      \                 │           +              
     ───┼──────────────────────────    ───┼──────────────────────────
        │                                 │                          

SVM 是由 Vapnik 等人于 1995 年提出的，并且基于 SVM 实现的 Support-Vector Networks 在之后的
20 多年里它都是最具影响力的机器学习算法之一。SVM 不仅可以用于分类问题，还可以用于回归问题。在深度学习
出现之前，基于高斯核的 SVM 在很多问题上一度取得了最好的效果。也因为 SVM 具有较好的泛化性能，适合小样
本等优点，被广泛应用于各种实际问题。Support-Vector 字面意义就是指靠近决策边界且对分割线（决策面）具
有支撑作用的向量。

* https://link.springer.com/article/10.1007/BF00994018[1995, Support-vector networks, Corinna Cortes & Vladimir Vapnik]
  (link:https://link.springer.com/content/pdf/10.1007/BF00994018.pdf[PDF])
* https://cloud.baidu.com/article/3362251[从零开始理解支持向量机SVM精髓]
* https://bbs.huaweicloud.com/blogs/detail/252776[SVM算法的简介、应用、经典案例]
* https://www.cnblogs.com/haohai9309/p/18428333[非线性规划——无约束最优化问题精讲]

SVM 基本思想是：找到集合边缘上的若干数据（称为 Support Vector），用这些点找出一个平面（称为决策面），
使得支持向量到该平面的距离最大。也就是是在特征空间中寻找一个最优的超平面，使得两类样本（在二分类问题中）
尽可能地被分开。这个最优超平面不仅要能将两类样本分开，还要使得两类样本到这个超平面的距离（间隔）最大化。
这种最大化间隔的策略有助于提高模型的泛化能力，减少过拟合的风险。

例如，上图左侧，如果分隔线右移或下移，那么就会更贴近 + 符号表示的分类空间，但这不是最优解。

在二维空间中，超平面实际上是一条直线；在三维空间中，它是一个平面；而在更高维的空间中，我们称之为超平面。
SVM 的目标就是找到一个最优的超平面，使得两类样本的分类误差最小。SVM 的求解过程主要包括以下几个步骤：

   *  构建优化问题：通过最大化间隔来构建一个优化问题，即求解一个二次规划问题（Quadratic Programming，QP）。
   *  求解支持向量：在优化问题的解中，只有部分样本（即支持向量）会对最优超平面的位置产生影响。这些支持向量是距离最优超平面最近的样本点。
   *  构造分类器：利用求解得到的支持向量和最优超平面，可以构造出一个分类器，用于对新样本进行分类。

2006年，Hinton 在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，
“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，
之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的
时间。他给多层神经网络相关的学习方法赋予了一个新名词——“深度学习”。

机器学习发展分为两个部分，浅层学习（Shallow Learning）和深度学习（Deep Learning）。浅层学习起源
上世纪 20 年代人工神经网络的反向传播算法（Back-propagation）的发明，使得基于统计的机器学习算法兴起。
虽然这时候的人工神经网络算法也被称为多层感知机（Multiple layer Perception, MLP），但由于多层网络
训练困难，通常都是只有一层隐含层的浅层模型，隐含层就是除了输入、输出层之外的中间层。

机器学习模型训练的目的，就是使得参数尽可能的与真实的模型逼近。具体做法是这样的：首先给所有参数赋上随机值。
我们使用这些随机生成的参数值，来预测训练数据中的样本。样本的预测目标为 y'，真实目标为 y。那么，定义一个值
loss，计算公式如下：

\[ loss = \frac{(y' - y)}{2} \]

这个值称之为损失（loss），我们的目标就是使对所有训练数据的损失和尽可能的小。如果将神经网络预测的矩阵
公式代入到式中，那么可以将损失写为关于权重参数（parameter）的函数，这个函数称之为损失函数（loss function）。
到此，神经网络的下一个问题就是求：如何优化参数，能够让损失函数的值最小，即转化为一个优化问题。一个常用方法
就是高等数学中的求导，但是这里的问题由于参数不止一个，求导后计算导数等于 0 的运算量很大，所以一般来说解决
这个优化问题使用的是梯度下降算法。梯度下降算法每次计算参数在当前的梯度，使参数向着梯度的反方向前进一段距离，
不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。损失函数有
多种不同类型：

   *  L1 Loss（平均绝对误差，Mean Absolute Error，MAE）
   *  MSE Loss（均方误差损失，Mean Squared Error）
   *  Cross Entropy Loss（交叉熵损失函数）

与解决回归问题不同，分类问题的常见损失函数被称为交叉熵（cross-entropy）。当有两个以上的类别时，
我们把这个问题称为多项分类问题（multiclass classification）。例子包括 MNIST 手写字符识别。

残差函数（error function）以及代价函数（cost function）跟它完全一致的意思。后者在深度学习里用的较多的
一种说法，caffe 里也这么叫。前者则是 Ng 在 coursera 教学视频里用到的统一说法。这三者都是同一个意思，
都是优化问题所需要求解的方程。

机器学习三要素（模型、优化策略、算法）可以对应到 CNN 的输入层、隐藏层和输出层。返回传播算法就是 CNN
网络结构的关键，它实现梯度下降（Gradient Descent），逐层反馈，相邻两层之间互为输入、输出关系，
即一个多项式中包含从下一层获取一个反馈分量，同时从前一层获取输入分量，通过层层反馈到输入端，以训练
多层神经网络。机器学习中使用的神经网络有多种架构，以下是三种主要类型：

   - 循环神经网络：RNN - Recurrent Neural Network；
   - 卷积神经网络：CNN - convolutional neural network；
   - 注意力神经网络：GAN - Graph Attention Network；
     以 Transformer 网络为代表；两种机制：Self-Attention Mechanism，Multi-head Attention。

优化问题只是训练中的一个部分。机器学习问题之所以称为学习问题，而不是优化问题，就是因为它不仅要求数据在
训练集上求得一个较小的误差，在测试集上也要表现好。因为模型最终是要部署到没有见过训练数据的真实场景。提升
模型在测试集上的预测效果的主题叫做泛化（generalization），相关方法被称作正则化（regularization）。
神经网络中常用的泛化技术有权重衰减等。

A Simple Neural Network

   *  Model: Multi-Layer Perceptron (MLP) — \( y' = W_3 f(W_2 f(W_1x + b_1) + b_2) + b_3) \)
   *  Loss function: L2 loss              — \( l(y, y') = (y - y') \)
   *  Optimization: Gradient descent      — \( W = W - η \frac{∂L}{∂W} \)

激活函数的存在，目的是给输入数据填充细节数据，就是对输入的线性空间数据做升维转换。为了获得更好的训练效果，
还需要专门对输入的数据做预处理，这样可以形成更富有变化的特征图，相当于是放大了关键特征。激活函数的选择有
一个要求，不能使用线性函数（一次函数），只能使用非线性函数。非线性函数包括指数函数、幂函数、对数函数、多项
式函数等等基本初等函数以及他们组成的复合函数。激活函数（active function）和转移函数（transfer function）
代表一个意思，都是叠加的非线性函数的说法。

万能逼近定理（Universal Approximation Theorem）是关于前馈神经网络的一个核心理论，它表明具有足够多
隐藏层的前馈神经网络，可以任意精度逼近任何连续函数。这个定理的价值在于，它为前馈神经网络的广泛应用提供了
理论依据。万能逼近定理的现代形式可以表述为：对于任何闭区间上的连续函数，存在一个具有可数个非线性激活函数
的前馈神经网络，能够以任意精度逼近该函数。万能逼近定理的证明基于组合和结构函数的性质。

反向传播（Backpropagation，BP）是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，
用来训练人工神经网络的常见方法。该方法计算对网络中所有权重计算损失函数的梯度，这个梯度会反馈给最优化方法，
用来更新权值以最小化损失函数，使最终结果逼近期望值。BP 算法会先按前向传播方式计算（并缓存）每个节点的输
出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的偏导数。

交叉熵（Cross Entropy）是 Shannon 信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。
深度学习中常用交叉熵作为损失函数（loss function），用于衡量模型预测值与真实值之间的差距。神经网络
通过误差反向传播获得当前状态与期望状态之间的误差水平，通过加入反馈修正量，以实现逐步逼近期望结果。

反向传播算法主要有激励传播、权重更新两个环节反复循环迭代，直到网络的对输入的响应达到预定的目标范围为止。
学习过程的激励传播由前向传播阶段（Forward Pass）和反向传播阶段（Backward Pass）组成，每次迭代包含两步：

   *  （前向传播阶段）将训练输入送入网络以获得激励响应；
   *  （反向传播阶段）将激励响应同训练输入对应的目标输出求差，从而获得隐层和输出层的响应误差。

在正向传播过程中，输入通过输入层经隐含层，逐层处理并传向输出层。如果在输出层得不到期望的输出值，则取输出与
期望的误差的平方和作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值
向量的梯量，作为修改权值的依据，网络的学习在权值修改过程中完成。误差达到所期望值时，网络学习结束。

* https://playground.tensorflow.org/[Neural Network Playground]
* https://yann.lecun.com/exdb/mnist/[THE MNIST DATABASE of handwritten digits]
* https://www.cs.toronto.edu/~kriz/cifar.html[The CIFAR-10 dataset]
* https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html[ConvNetJS CIFAR-10 demo]
* https://wordnet.princeton.edu[WordNet A Lexical Database for English]
* https://image-net.org/download.php[ImageNet Data]

由于训练神经网络需要处理大量的数据，并且，CNN 这种输入节点、输出节点并联的网络结构非常适合做并行计算，
因此，高效的神经网络训练都基于 GPU 硬件进行，这也就给开发了 CUDA 的 Nvidia 公司成为暴发户的机会。
CUDA（Compute Unified Device Architecture）基于 GPU 进行通用并行计算，不仅可加速图形渲染，
也可用于加速科学计算，被大量应用于 AI 科学研究等领域。虽然，CPU 上也有单指令多数据（SIMD）这种可以
大量处理数据的指令，但是仍远不如 GPU 中少则几百，多则成千上万的并行进程。打个比喻就是，CPU 是少数几
个数学科学家（进程数量少但功能齐全），GPU 则是一大群精通算术的中小学生。

目前为止，训练用途的最大数据集是李飞飞创建的 ImageNet，共有 1000 种物体分类、一千四百多万图片、超过 130GB 数据。
为推广此数据集的 ILSVRC 挑战赛在 2017 年成功训练出来的 AI 技术取得超过人类的平均水平（2.3 vs. 5.0%）
之后不再举行。 

The most highly-used subset of ImageNet is the ImageNet Large Scale Visual Recognition 
Challenge (ILSVRC) 2012-2017 image classification and localization dataset. This dataset 
spans 1000 object classes and contains 1,281,167 training images, 50,000 validation 
images and 100,000 test images. This subset is available on Kaggle.

The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) evaluates algorithms 
for object detection and image classification at large scale. One high level motivation 
is to allow researchers to compare progress in detection across a wider variety of 
objects -- taking advantage of the quite expensive labeling effort. Another motivation 
is to measure the progress of computer vision for large scale image indexing for 
retrieval and annotation.

2017年，Attention Is All You Need 论文首次提出 Transformer 构架并在机器翻译任务上取得
最好的效果（The State of the Art）。2018年，BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding 使用 Transformer 构架进行大规模语言模型预训练，
在多个 NLP 下游（downstream）任务中进行微调（Finetune），一举刷新了各大 NLP 榜单最高分，轰动一时。

2017 年，Attention Is All You Need 论文发表，到 2022 年 OpenAI ChatGPT 引领 AI 工业
进入快速发展阶段。GPT（Generative Pre-trained Transformer） 是由 OpenAI 开发的一系列
基于 Transformer 架构的大规模语言模型，广泛应用于自然语言处理（NLP）任务。以下是其核心要点：

   *  生成式（Generative）：能够生成连贯的文本、代码、对话等内容。
   *  预训练（Pre-trained）：通过海量无标注数据（如书籍、网页）学习语言规律。
   *  Transformer 架构：依赖自注意力机制（Self-Attention）捕捉长距离文本依赖关系。

发展历程

[opts=autowidth]
|======
|版本   |发布时间    |参数量    |核心改进
|GPT-1  |2018    |1.17亿    |首次验证 Transformer 预训练有效性
|GPT-2  |2019    |15亿      |   多任务零样本学习，生成能力显著提升
|GPT-3  |2020    |1750亿    |上下文学习（In-Context Learning）
|GPT-4  |2023    |~1.8万亿    |多模态支持（文本+图像），逻辑推理优化
|======

工作原理包括训练与生成两个过程：

   *  训练流程

      *  预训练：通过无监督学习预测文本中缺失的词（如填空任务）。
      *  微调（Fine-tuning）：针对特定任务（如问答、翻译）用标注数据优化模型。
      *  对齐（Alignment）：使用 RLHF（人类反馈强化学习）确保输出符合人类价值观。

   *  生成过程

      *  自回归生成：逐词预测，每个步骤基于上文生成下一个词（通过概率采样）。
      *  温度（Temperature）：控制生成多样性（低温度→保守，高温度→创意）。

目前存在以下局限性：

   *  事实性错误：可能生成看似合理但不准确的内容（需人工验证）。
   *  推理局限：复杂逻辑、数学问题处理能力有限。
   *  数据偏见：训练数据中的社会偏见可能反映在输出中。
   *  资源消耗：训练和部署需大量算力（千亿级参数模型需GPU集群）。

2025 年春节前夕，中国由幻方量化创始人梁文锋全资建立的 [DeepSeek](https://www.deepseek.com/)
公司发布了 DeepSeek R1 大语言模型，通过知识蒸馏技术，成功将 AI 技术以越低成本为用户提供一个本地化
部署方案，并且性能几乎可追平最新的 ChatGPT o1 大语言模型。从 DeepSeek-V2、V3 到 DeepSeek-R1
的版本升级中，不断有新的优化技术加入。DeepSeek V3 技术文档 Figure 2 展示针对 Transformer
架构的两项主要的优化技术，通过这些优化提升训练效率、降低训练成本：

   *  DeepSeekMoE（优化的混合专家架构），根据用户提示词激活相应数量的专家。
   *  Multi-Head Laent Attention (MLA) 多头潜在注意力架构。

多头注意力（Multi-Head Attention）是一种注意力机制。它是对传统注意力机制的一种改进，旨在
通过分割输入特征为多个“头部”（head）并独立处理每个头部来提高模型的表达能力和学习能力。

由于 DeepSeek 并未公布从零开始复现 R1 模型权重的方法，R1-Zero 比 R1 更值得分析：它消除了人为瓶颈。
因为 R1-Zero 不使用 SFT（监督微调），即不依赖存在人力瓶颈的人工标注数据，完全依赖强化学习（RL）。

大语言模型的智能的一个重要特征是将大量人类知识作为前馈网络（Feed-Forward Network, FFN）中的
权重参数迁移到了 Transformer 架构中，然后通过大量的 Atention 计算实现与人类的交流。

现在，大语言模型（Large Language Models，LLMs）和多模态大模型（Large Multimodal Models，LMMs）
已经是 AI 领域的两大成果。大语言模型主要用于生成文本，大模型主要用于处理图像、视频处理、语音和推荐等领域。
大语言模型（Large Language Model, LLM） 是一种基于深度学习的人工智能技术，能够理解和生成自然语言文本。
典型代表有：GPT 系列（OpenAI）、PaLM（Google）、LLaMA（Meta）、Claude（Anthropic）等。
以下是其核心特征：

   *  参数量大：通常在数十亿（B）到数万亿（T）级别。
   *  通用性：通过海量数据预训练，无需针对特定任务单独建模。
   *  生成能力：可输出连贯的文本、代码、对话等内容。

LLM 核心技术原理：

   *  架构基础：

      *  Transformer 模型：依赖自注意力（Self-Attention）机制，捕捉长距离文本依赖。
      *  预训练目标：通过掩码语言建模（如BERT）或自回归生成（如GPT）学习语言规律。

   *  训练流程：

      *  预训练：使用无标注文本（如网页、书籍）学习语言模式。
      *  微调（Fine-tuning）：在特定任务（问答、翻译）上优化模型。
      *  对齐（Alignment）：通过人类反馈强化学习（RLHF）确保输出安全、有用。

   *  关键参数：

      *  上下文窗口：单次处理的最大文本长度（如GPT-4支持32K Token）。
      *  温度（Temperature）：控制生成结果的随机性与创造性。

最新的 AI 技术分支是生成式人工智能（Generative artificial intelligence），它是一种基于算法和
模型生成文本、图片、声音、视频、代码等多内容模态的技术，生成式 AI 能学习并生成具有逻辑的新内容，学习
并模拟事物的内在规律，根据用户的输入资料生成具有逻辑性和连贯性的新内容。这一技术的核心依托于多模态模型，
能实现异构数据的生成式输出。传统的判别式机器学习 AI 基于对输入数据进行处理和分析，数据驱动的自主学习。
所谓多模态模型，在几十年前就是叫做多媒体（Multimedia）的东西，是多种媒体的综合，一般包括文本，声音
和图像等多种媒体形式。当然，在新的历史条件下，不能简单地给两个近似名词划等号。

所谓生成式 AI 即 AI Generated Content，也就是从单纯输入训练数据来“教育”神经网络实现物体分类转变
到通过 AI 来生成各种形式的内容，AI 视频、音频、文字生成等等。生成式 AI 的主要模型架构有：

   *  生成对抗网络：（GAN, Generative Adversarial Networks）通过两个网络（生成器和判别器）进行对抗训练。
   *  变分自编码器：（Variational auto-encoder，VAE）基于概率图模型，通过学习数据的隐变量分布来生成新数据。
   *  自回归模型：（Autoregressive Model，AR ），如 PixelCNN、WaveNet 等，基于历史生成的样本生成新的样本。
   *  扩散模型：（Diffusion Models），通过模拟噪点的扩散，然后学习逆向过程，从不同的噪声扩散状态中恢复出图像。
   *  注意力模型：（Attention Model），基于 NLP、LLM、Transformer，在理解自然语言语义或语音等基础上合成图像。

2006 年，Jonathan Ho, Ajay Jain, Pieter Abbeel 三人展示了一个可以实现高质量图像合成的技术：
降噪扩散概率模型（DDPM）。DDPM（Denoising Diffusion Probalistic Models）是扩散模型的基石。
扩散模型（Diffusion Models）的灵感来源于非平衡热力学中的扩散过程。2015 年时有研究者提出了深度生
成模型（Deep Generative Models），为后来的扩散模型奠定了基础。2018 年，Diffusion Models 
被正式提出，通过学习数据分布的逆过程，实现了更加稳定、多样化的样本生成。扩散模型的技术发展经历了几个
重要的阶段。最初，扩散模型被用于图像生成任务，在这一领域超越了原有的生成对抗网络（GAN）成为新的 SOTA
（State of the Art）。随后，扩散模型的应用领域逐渐扩展到自然语言处理、波形信号处理等多个领域。

DDPM 的扩散过程比喻为一滴墨水滴到水杯的过程，墨水逐渐扩散直到完全扩散到整个水杯，这就是高斯分布。
DDPM 过程如图所示，刚开始对原始图像进行前向加噪，噪声都是从高斯分布 N(0,1) 随机采样的噪声，最后
直到原始图像完全变成了高斯噪声。然后再反向去噪，预测过程中所加的噪声，反向还原原始图像：

image:https://img2023.cnblogs.com/blog/2489686/202412/2489686-20241214163129413-1810894196.png[DDPM]

高斯过程（Gaussian Processes, GP）是概率论和数理统计中随机过程的一种，是多元高斯分布的扩展，被应用于
机器学习、信号处理等领域。高斯过程观测一个连续域(例如时间和空间)的值的随机过程，连续输入空间中每一点都与
一个正态分布的随机变量相关联。扩散模型就如一个时光倒流的机器，在某一个扩散状态中的噪点，可以通过追踪其扩散
路径，得知其在另一个扩散状态中的位置。总体来说，扩散就是噪点从起始点向外围随机移动的过程。扩散模式中需要
有一个很重要的评分函数（score function），其实这个函数代表的是一个向量场，向量指向整个扩散过程的起点。

流行假设（Manifold Hypothesis）是机器学习、数据科学中的一个重要假设，它认为高维数据通常集中在比数据
原始维数低得多的流形上。流形（manifold）是一种数学概念，它描述了在局部看起来像欧几里得空间（如平面或者空间）
的拓扑空间。换句话说，流形是一个可以在局部范围内近似为欧几里得空间的空间。

MultiModal Machine Learning (MMML) 中的模态（modal）是人类与真实世界交流的不同形式。由信息构成
的世界，包括视觉信息、听觉信息、文本信息、嗅觉信息等等，当研究的问题或者数据集包含多种这样的模态信息时
我们称之为多模态问题，研究多模态问题是推动人工智能更好的了解和认知我们周围世界的关键。当前主要研究的模态
包括 3V：Verbal（文本）、Vocal（语音）、Visual（视觉）。可以猜想，还有更高级的触觉、嗅觉、运动等等。

一个例子是斯坦福大学一个研究组设计的 AI 虚拟小镇（）。这个工作设计了一个虚拟游戏环境，有 25 个 AI 智能体
生活在一个虚拟小镇中。每个智能体都有自己的人物设定 (学生、老师等不同身份)、记忆 (每天经历的事情，遇到的人)。
智能体会根据记忆和自己的设定来决定下一步做的事情，也需要对于经历过的事情进行反思，把重要的信息存入记忆中。
智能体之间的社交互动表现出了复杂的行为，例如组织一次生日聚会。在这个例子中，每个智能体都需要有记忆系统，
通过调用长期记忆、计划和反思来实现复杂的社会行为。在像素风格的界面里，为了真实可信地模拟人类行为，创建了
一个有 25 个角色的虚拟小镇，小镇居民每天早起刷牙、亲吻爱人、吃早饭，和家人、朋友、邻居社交。每个角色都是一个
ChatGPT 实例：用指令设定人物角色，用“代理”给这些实例提出行动要求，ChatGPT 执行并给出结果。

此项目通过 Convex 平台部署（AI SaaS），可下载预编译版，执行预编译的后端程序会生成相应的数据库等文件。
Convex 本地后端是一款专为全栈应用程序开发设计的后端即服务（BaaS）平台。它集合了精心挑选并由专家优化的
组件，涵盖了数据库、服务器函数、调度、存储、向量搜索等多个方面。Convex 本地客户端库的这些库能够与 Web
前端应用深度集成，比如 Next.js 或者 AI 小镇项目使用的 React，确保开箱即用的一致性缓存和实时更新功能。
开发者只需专注于他们的业务逻辑，无需操心底层架构细节。

image:https://imagecloud.thepaper.cn/thepaper/image/246/859/272.jpeg[AI Town]

* https://convex.dev/ai-town  
* https://github.com/a16z-infra/ai-town  
* https://docs.convex.dev/tutorial/  
* https://github.com/get-convex/convex-backend/releases  
* https://arxiv.org/pdf/2304.03442.pdf[Generative Agents: Interactive Simulacra of Human Behavior]

* https://www.bilibili.com/video/BV16Z421n7fi/[清华AI博士教你AI是怎么生成图片和视频的]
* https://arxiv.org/abs/2006.11239[2006, Jonathan Ho, Ajay Jain, Pieter Abbeel, Denoising Diffusion Probabilistic Models]
* https://www.cnblogs.com/Sun-Wind/p/18608076[DDPM论文解读 DDPM（Denoising Diffu]
* ttps://hojonathanho.github.io/diffusion/[Denoising Diffusion Probabilistic Models]
* https://github.com/hojonathanho/diffusion

回望过去，历史最大的好处是可以给现在做参考。科学的研究呈现螺旋形上升的过程，不可能一帆风顺。同时，这也给现在
过分热衷深度学习与人工智能的人敲响警钟，因为这不是第一次人们因为神经网络而疯狂了。1958 年到 1969 年，1985
年到 1995，这两个十年间人们对于神经网络以及人工智能的期待并不现在低，可结果如何大家也能看的很清楚。

神经网络再次火热得益于硬件技术的快速发展积累，同时也由于从单层神经网络，到两层再到多层神经网络的技术迭代，
使用得一批批 AI 成果涌现。未来，也许是量子计算的天下，这是一种绝然不同的机器模型，当前所有编程模型都需要
更新一套以适应这种拥有强大并行运算能力的新机器。


== PyTorch with CUDA

目前 AI 技术需要大量算力支持来进行技术研究与迭代，包括专用的 AI 芯片以及基于 GPU 的并行运算支持。
当前流行 AI 硬件架构主要包括 GPU、FPGA、ASIC、NPU 以及混合架构。这些架构各有特点和优势，可以
根据具体应用场景和算力需求进行选择。

. *GPU(Graphics Processing Unit) 图形处理器架构*
  * *特点*：GPU 最初用于图形渲染和游戏处理，但因其高度并行的特性，逐渐被应用于深度学习计算。GPU 
    架构采用多个计算单元（如 CUDA 核心）进行并行计算，能够高效地执行浮点运算和矩阵计算。
  * *应用*：已成为AI训练和推理的加速引擎，是算力核心。
. *FPGA (Field-Programmable Gate Array) 现场可编程门阵列架构*
  * *特点*：FPGA 是一种可以根据需要编程和重新配置的硬件，具有可重构性，可以根据需要灵活调整电路结构。
  * *应用*：AI 芯片中的 FPGA 架构允许开发者将深度学习模型转换为硬件电路，以实现高度定制化和低功耗
    的计算。在自动驾驶和高级驾驶辅助系统（ADAS）等领域有广泛应用。
. *ASIC (Application-Specific Integrated Circuit) 专用集成电路架构*
  * *特点*：ASIC 是指专门为特定应用设计和制造的定制芯片，基于特定的深度学习算法和网络结构进行优化，
    通过专用硬件实现高效计算和推理。通常能够提供更高的性能和能效比，但研发和生产成本较高。
  * *应用*：在需要高性能和能效比的AI应用中，ASIC 架构具有显著优势。
. *NPU（神经网络处理器）架构*
  * *特点*：NPU 是专门为加速神经网络运算而设计的硬件架构，具有高度并行的计算单元，并针对深度学习
    计算进行了优化，如卷积计算、矩阵乘法和张量处理等。
  * *应用*：可实现高效的神经网络推理和训练，适用于各种AI应用场景。
. *混合架构*
  * *特点*：结合了多种硬件类型和优化策略，以实现更灵活、高效的计算能力。
  * *应用*：可以根据应用需求选择合适的架构组合，满足不同场景下的算力需求。

在硬件架构设计中，可以采用同构或者导构设计。异构处理器架构结合了不同架构或类型的处理器核心，以提供
更高的计算性能和能效比。例如，在AI领域，异构处理器架构结合了CPU、GPU、FPGA等多种类型的处理器核心，
以加速深度学习等计算密集型任务。

同构多核架构（Homogeneous Multi-Core）中所有的处理核心的结构都相同，具有相似的时钟频率和性能特征。
异构多核架构（Heterogeneous Multi-Core）中的处理核心不一定相同，可能具有时钟频率和功耗特征。这种
设计的目标是通过将不同类型的核心结合在一起，使得处理器可以更好地适应不同种类的任务。

GPU 在个人 AI 学习中至关重要，特别是对于深度学习模型训练。GPU 能够并行处理大量数据，大大加速计算
过程。GPU 非常受欢迎，如 GeForce RTX、Quadro 或 Tesla 系列。选择购买硬件时，首要考虑显存大小、
CUDA 核心数量和计算能力。

除了使用独立显卡，有条件可以使用 A100 这样的 AI 专用芯片，或者云服务：如 AWS、Google Cloud、
Azure 等提供的强大的计算资源。可以根据需要租用虚拟机或 GPU 实例来训练模型。这对于初学者来说是一个
灵活且成本效益高的选择。

NVIDIA 为用户提供 CUDA (Compute Unified Device Architecture) 平台，用于 GPU 加速并行计算。
安装独立显卡驱动后，可以在驱动程序目录下找到 System Management Interface (nvidia-smi)
link:https://developer.nvidia.com/system-management-interface[⇲] 
工具用于查询当前的显卡信息，以下显示当前系统只安装了一张 GXT 1050 显卡，其它信息包括 CUDA 版本、
显卡内存使用情况、当前运行于 GPU 上的进程信息。第一行显示的是版本号信息，第二行是显卡信息表表头，
后续各行对应了系统中安装的显卡信息。其中 Perf 表示当前显卡的性能状态，从 P0 到 P12 逐级降低。

[source,bash]
-------
 /c/Program Files/NVIDIA Corporation/NVSMI
$ ./nvidia-smi.exe
Thu Feb  6 22:09:50 2025
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 461.40       Driver Version: 461.40       CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 1050   WDDM  | 00000000:02:00.0 Off |                  N/A |
| N/A   25C    P8     8W /  N/A |    186MiB /  2048MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     15608    C+G   ...n\Blender 4.3\blender.exe    N/A      |
+-----------------------------------------------------------------------------+
-------

有了硬件后，还需要配合 AI 框架来帮助编程，主流 AI 框架有 PyTorch、TensorFlow 等等。
AI 框架如何选择？这取决于你的熟练程度及喜好，初学可以任选一个主流 AI 框架，深入理解后就
不会有选择困难症的问题。开发 AI 产品中使用 AI 框架，就类似于开发游戏中使用游戏引擎，只不过
AI 框架主要是做数学计算与模型设计。当前主流框架要点对比：

|======
|框架			|优势场景				|典型用户				|学习曲线

|PyTorch		|研究、动态图、快速迭代		|学术界、初创公司		|中等
|TensorFlow		|生产部署、移动端、TPU 支持	|企业级应用、云平台集成		|较高（低层API）
|JAX			|高性能计算、自动微分研究	|科研机构、算法理论探索		|陡峭
|MXNet			|多语言支持、内存优化		|AWS 生态、多语言开发者		|中等
|PaddlePaddle	|中文生态、产业级工具链		|中国本土企业、政府项目		|中等
|Horovod		|分布式训练				|大规模集群训练				|中等
|======

选择适合的 AI 框架需要结合具体需求、技术背景和应用场景。首先，根据任务类型需求来选择构架：

* 图像处理（TensorFlow、PyTorch）
* 自然语言处理（Hugging Face Transformers）
* 强化学习（Stable Baselines）
* 边缘设备（TensorFlow Lite、ONNX）。

根据开发阶段来选择：

* 研究/实验：优先灵活性和动态计算图（如 PyTorch）。
* 生产部署：注重静态图优化和跨平台支持（如 TensorFlow、ONNX）。
* 规模要求：大规模分布式训练（TensorFlow、PyTorch + Horovod）、轻量化推理（TensorFlow Lite、Core ML）。

AI 框架的核心评估维度包括易用性、性能、生态系统和社区支持等等：

(1) 易用性

* PyTorch：API 简洁直观，动态图调试友好，适合快速原型开发。
* Keras：高层抽象，极简代码实现模型搭建（可独立或作为 TensorFlow 前端）。
* TensorFlow：2.x 版本已集成 Keras，但底层 API 仍有一定学习成本。

(2) 性能

* 训练速度：TensorFlow 和 PyTorch 在 GPU 加速上表现接近，均支持 CUDA/cuDNN。
* 推理优化：TensorFlow Lite、TensorRT（NVIDIA）、ONNX Runtime 提供量化、剪枝等优化。
* 分布式训练：TensorFlow 的 tf.distribute 和 PyTorch 的 torch.distributed 均支持多节点训练。

(3) 生态系统（预训练模型库、部署工具链）：

* PyTorch：Hugging Face、TorchVision、TorchText。
* TensorFlow：TF Hub、TensorFlow Models、官方 Model Garden。
* TensorFlow：SavedModel -> TFLite/TensorRT 部署。
* PyTorch：TorchScript -> ONNX -> 多平台推理。

(4) 社区支持

* PyTorch：学术界主导，论文复现首选，GitHub 活跃度高。
* TensorFlow：工业界广泛采用，Google 持续维护，企业级文档完善。
* 新兴框架：JAX（Google Research）适合高阶自动微分，但社区资源较少。


安装 Python 脚本编程环境后，可以真的运行命令行 `pip install torch`，由于 PyTorch 的安装
版本与电脑的系统的 GPU 配置有着很大的关联，上述的安装方法并没有启用 GPU 支持，编程时使用 GPU
运算就会触发 AssertionError("Torch not compiled with CUDA enabled") 异常。

无论你的电脑有无 GPU 支持，总是可以使用 CPU 运行 PyTorch。区别在于启用 GPU 版本的 PyTorch
能够提供更快的计算速度和更高的性能，特别是在处理大规模数据集和复杂模型时。GPU 加速可以显著减少
训练和推理时间，使得深度学习模型的开发和测试更加高效。如果电脑只配置了 CPU，PyTorch 仍然可以运行，
只是速度会慢一些，特别是在执行计算密集型任务时。

PyTorch 起源自 Torch 这一个基于 Lua 语言开发的开源的科学计算框架，最初由 Facebook 的人工智能
研究团队（FAIR）以及其他团队共同开发，旨在满足深度学习和计算机视觉的需求。Torch 的设计专注于提供
高效的数值计算能力，尤其是在矩阵操作和自动求导方面表现出色。Torch 最早的版本大约在 2002 年出现，
但直到 2010 年，Torch 版本才开始受到深度学习研究社区的广泛关注。

Torch 的主要优势包括：

1. 高效性：它利用了高效的 C 和 Fortran 函数库进行数值计算，例如 BLAS 和 ATLAS。
2. 灵活性：提供了众多灵活的工具，用于构建和实现机器学习模型。
3. 开源性：代码完全开放，便于与全球的研究人员和开发者进行协作。

尽管 Torch 具有着上述的优势，但基于 Lua 语言开发的限制使其在深度学习领域的普及度相对较低。因为
此时 Python 语言已经成为机器学习领域主流语言，这大大限制了 Torch 在开发者中的使用范围。为此，
Torch 重新使用 Python 语言实现，命名为 PyTorch，由 Soumith Chintala 等人于 2016 年创建。
PyTorch 的设计宗旨是继承 Torch 的灵活性，并结合 Python 语言的普及度、易用性以及强大的生态系统。
自推出以来，PyTorch 迅速在深度学习研究和工业应用中崭露头角，成为最受欢迎的深度学习框架之一。
PyTorch 通过 Python 提供的 C/C++ 扩展将 CUDA 编程能力暴露给脚本环境，使用得用户可以直接通过
脚本调用 GPU 进行计算，而不必编写 C/C++ 程序。

PyTorch 的核心特点：

1. 动态图（Dynamic Computational Graph）：PyTorch 使用动态计算图（动态图），不同于 TensorFlow 1.x。
   这意味着计算图是在执行时动态生成的，而不是在运行之前就定义好，极大提高了灵活性，特别是对于调试、
   修改模型结构和研究新算法非常方便。
2. 深度集成 Python：PyTorch 的核心使用 Python 编写，并且与其它数据处理库深度集成，包括 NumPy、
   SciPy、Pandas 等等，使得它对 Python 开发者非常友好。
3. 强大的自动求导功能（Autograd）：自动求导机制可以自动计算梯度，能非常方便实现反向传播算法。


https://pytorch.org/[PyTorch] 官网推荐 Anaconda 工具来安装。PyTorch 不同版本需要
配合不同的 CUDA 版本来使用，并且机器本身安装 CUDA 版本不能高于当前显卡支持的 CUDA 版本号。
因此，旧显卡中使用 CUDA 就需要在 PyTorch 历史版本页面中搜索适用的配套 CUDA 版本。
https://pytorch.org/get-started/previous-versions/[⇲] 

Anaconda 包含大量预装数据科学和机器学习库，体积也较大。Miniconda 是精简的 Anaconda 发行版，
它只包含包管理器（conda）和 Python 及其基础库。使用 conda 一个便利就是虚拟环境管理，可以创建
并设置使用不同的 Python 版本的虚拟环境。激活一个虚拟环境，那么 pip 安装依赖包或者运行脚本都将
以当前虚拟环境为运行环境：

[source,bash]
-------------
conda create --name myVenv python=3.9 -y
conda activate myVenv
conda install pytorch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1 cudatoolkit=11.3 -c pytorch -c conda-forge
conda install pytorch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0  pytorch-cuda=11.8 -c pytorch -c nvidia

# conda or Wheel in OSX
conda install pytorch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 -c pytorch
pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0

pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu111/torch_stable.html
pip install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1 -f https://download.pytorch.org/whl/torch_stable.html
-------------

* https://www.python.org/downloads/windows/[Python 3.9.10]
* https://pytorch.org/get-started/locally/[PyTorch Start Locally]
* https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html[Quickstart]
* https://vscode.dev/github/pytorch/tutorials/blob/main/README.md[PyTorch Tutorials]
* https://vscode.dev/github/pytorch/pytorch/blob/main/docs/source/index.rst[PyTorch documentation]
* https://vscode.dev/github.com/matplotlib/matplotlib/blob/main/doc/index.rst[Matplotlib documentation]
* https://matplotlib.org/stable/[Matplotlib 3.10.0 documentation]
* https://www.anaconda.com/download/success[Anaconda 官方下载地址]
* https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/[Anaconda 软件仓库]

MSYS2 环境下使用创建虚拟环境并使用 `conda activate` 命令激活虚拟环境时可能出现错误信息：
"CondaError: Run 'conda init' before 'conda activate'"，它表示在尝试激活环境之前，
需要先运行 conda init 命令进行初始化。Windows 系统下默认设置好 PowerShell 环境的初始化，
可以使用 `conda init bash` 命令向 `~/.bash_profile` 添加以下配置，意思是让 conda 
在进入 shell 时自动运行以实现虚拟环境的管理。这里的 eval 命令用于运行 conda 生成的钩子脚本。
钩子脚本会在 bash 启动时运行，这会影响其响应速度。可以随时使用 `conda init bash --reverse`
撤销钩子脚本。钩子脚本主要功能是提供 activate 和 deactivate 功能支持，当前用户执行 python
命令时就执行虚拟环境中命令。如果熟练使用 bash，完全不需要 conda 工具来管理 Python 虚拟环境。
只需要使用命令别名、或者创建 bash 脚本来调用不同的 Python 版本。也就是说掌握基础脚本编程功能，
可以让一个“大块头”的工具变成 useless tool，同时节省你的硬件资源并提升脚本程序效率。
要确定 shell 类型使用 `echo $SHELL` 命令。

[source,bash]
-----
# >>> conda initialize >>>
# !! Contents within this block are managed by 'conda init' !!
if [ -f '/c/miniconda3/Scripts/conda.exe' ]; then
    eval "$('/c/miniconda3/Scripts/conda.exe' 'shell.bash' 'hook')"
fi
# <<< conda initialize <<<
-----

要成功安装，必要要求安装包符合几个条件：PyTorch 版本、CUDA 版本、Python 版本、操作系统平台一致。
其中 Python 版本号简写为 cp390 这样的格式，表示 CPython 3.9.0。CUDA 11.1 搭配 Python 3.9。
某个条件不满足，将提示安装失败：ERROR: No matching distribution found for torch==1.10.1+cu111。
软件体积较大，主要是 CUDA 11.1 比较肥胖，整个安装包在 3GB 容量以上。

CUDA 的兼容性需要从 硬件架构支持 和 软件/代码兼容性 两个维度分析。

* 硬件层面：新版 CUDA 不向前兼容旧 GPU，需根据架构选择 CUDA 版本。
* 代码层面：新版 CUDA 向后兼容旧代码，但新功能无法在旧版使用。

也就是可以安装高于显卡支持的版本，CUDA 保持 API 向后兼容性，旧代码通常无需修改即可用新版 CUDA
编译运行。例外：若旧代码依赖已弃用的 API 或行为，需调整代码（编译器会给出警告）。使用新版 CUDA
新增 API 或功能的代码无法在旧版 CUDA 编译。因此，安装一个与 PyTorch 要求不一致的版本，可能会
遇到兼容性问题。

NVIDIA 会逐步淘汰旧 GPU 架构的支持。例如：

* CUDA 11.0+ 不再支持 Fermi 架构（如 GTX 500 系列）。
* CUDA 12.0+ 不再支持 Kepler 架构（如 GTX 700 系列）。

必须检查 GPU 的计算能力列表（link:https://developer.nvidia.com/cuda-gpus[Compute Capability]）：
每个 CUDA 版本支持的架构功能列表见 CUDA 文档：

* CUDA C++ Programming Guide https://docs.nvidia.com/cuda/cuda-c-programming-guide/#features-and-technical-specifications[⇲]
* CUDA Toolkit Documentation 12.8 https://docs.nvidia.com/cuda/[⇲]

.Compute Capability List
[opts=autowidth]
|======
|GPU Architecture   |Compute Capability

|Tesla              |1.0 - 2.0
|Fermi              |2.0 - 2.1
|Kepler             |3.0 - 3.7
|Maxwell            |5.0 - 5.2
|Pascal             |6.0 - 6.2
|Volta              |7.0 - 7.5
|Turing             |7.5 - 8.6
|Ampere             |8.0 - 8.6
|Ada Lovelace       |9.0 - 
|Hopper             |9.0 - 
|======

官方将芯片分为以下 CUDA 产品线：

* CUDA-Enabled Datacenter Products
* CUDA-enabled Quadro productsCUDA-Enabled NVIDIA Quadro and NVIDIA RTX
* CUDA-enabled NVS productsCUDA-Enabled NVS Products
* CUDA-enabled GeForce ad Titan productsCUDA-Enabled GeForce and TITAN Products
* CUDA-enabled Jetson productsCUDA-Enabled Jetson Products

对于个人用户，主要是 GeForce 和 Titan 系列显卡，可以看到各型号显卡的算力差别：


[opts=autowidth]
|====
|GeForce RTX 4090    |8.9
|GeForce RTX 3090    |8.6
|Geforce RTX 2080 Ti |7.5
|NVIDIA TITAN X      |6.1
|GeForce GTX 1080 Ti |6.1
|GeForce GTX 1080    |6.1
|GeForce GTX 1050    |6.1
|GeForce GTX 950     |5.2
|GeForce GTX 750 Ti  |5.0
|====

GPU 计算能力（Compute Capability）是 NVIDIA 用来描述其 GPU 核心架构及其支持的功能和指令集的
一个版本号。计算能力对于深度学习和其他需要大量计算的应用程序非常重要。NVIDIA 表示 GPU 算力高于
5.0 就可以用来运行神经网络。这意味着，了解 GPU 计算能力可以帮助确定它是否适合特定的计算任务。
列表中，并没有看到移动版的 GTX 1050 算力，通常需要按台式机版本的 70% 来估算。

https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html[CUDA Toolkit]
是与 CUDA 一起使用的工具包，包括 GPU 并行程序的编译器（Compiler）: CUDA-C 和 CUDA-C++ 编译器。
NVCC (NVIDIA Compiler Driver) 编译器它建立在 NVVM 优化器之上，而 NVVM 优化器本身构建在 LLVM
编译器基础结构之上。希望开发人员可以使用 nvm/目录下的 Compiler SDK 来直接针对 NVVM 进行开发。
https://developer.nvidia.com/cuda-toolkit[CUDA Toolkit] 可以单独下载安装。

安装 PyTorch 后使用以下脚本可以判断 CUDA 是否可用：

[source,py]
--------
#!pip3.9 install matplotlib numpy==1.26.4
import torch
# torch.__file__

if torch.cuda.is_available():
    print("CUDA:", {
    "GPU device count:": torch.cuda.device_count(),  # 查看 GPU 数量
    "CUDA version": torch.version.cuda,              # 查看 CUDA 版本
    "Current device": torch.cuda.current_device(),   # 查看 GPU 索引号
    "#1 device name": torch.cuda.get_device_name(0), # 根据索引号得到GPU名称
    })
else:
    print("CUDA unavailable.")

# Assuming that we are on a CUDA machine, this should print a CUDA device:
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(device)

```
CUDA: {'GPU device count:': 1, 'CUDA version': '11.3', 'Current device': 0, '#1 device name': 'GeForce GTX 1050'}
```
--------

PyTorch 官方教程参考如下，最新版本 PyTorch 2.0.0 引入了 `torch.accelerator` 来管理硬件
加速设备，同时支持 context manager，配合 with 语句可以提升性能：

[source,py]
----
"""
Changing default device
=======================

It is common practice to write PyTorch code in a device-agnostic way,
and then switch between CPU and CUDA depending on what hardware is available.
Typically, to do this you might have used if-statements and ``cuda()`` calls
to do this:

.. note::
   This recipe requires PyTorch 2.0.0 or later.

"""
import torch

USE_CUDA = True


mod = torch.nn.Linear(20, 30)
if USE_CUDA:
    mod.cuda()

device = 'cpu'
if USE_CUDA:
    device = 'cuda'
inp = torch.randn(128, 20, device=device)
print(mod(inp).device)

###################################################################
# PyTorch now also has a context manager which can take care of the
# device transfer automatically. Here is an example:

with torch.device('cuda'):
    mod = torch.nn.Linear(20, 30)
    print(mod.weight.device)
    print(mod(torch.randn(128, 20)).device)

#########################################
# You can also set it globally like this: 

torch.set_default_device('cuda')

mod = torch.nn.Linear(20, 30)
print(mod.weight.device)
print(mod(torch.randn(128, 20)).device)


# If the current accelerator is available, we will use it. Otherwise, we use the CPU.
device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else "cpu"
print(f"Using {device} device")
----

* https://github.com/pytorch/tutorials/blob/main/beginner_source/basics/quickstart_tutorial.py
* https://github.com/pytorch/tutorials/blob/main/beginner_source/transfer_learning_tutorial.py
* https://github.com/pytorch/tutorials/blob/main/beginner_source/chatbot_tutorial.py
* https://github.com/pytorch/tutorials/blob/main/recipes_source/recipes/changing_default_device.py
* https://github.com/pytorch/pytorch/blob/main/docs/source/notes/hip.rst[HIP (ROCm) semantics]
* https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst[CUDA semantics]
* https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html[Training a Classifier]

以下代码片段来自 CUDA semantics，演示了如何使用多个 CUDA 硬件加速设备来加快 AI 神经网络运算。
CUDA semantics 语义定义了如何控制程序在 NVIDIA GPU 上执行的规则和行为。这些语义规则包括：

1. 执行模型 (Execution Model)
2. 内存层次 (Memory Hierarchy)
3. 同步机制 (Synchronization)
4. 内存管理 (Memory Management)
5. 并发机制 (Concurrency)
6. 高级特性 (Advanced Features)
7. 错误处理 (Error Handling)
8. 计算能力 (Compute Capability)
9. 优化关键 (Optimization Considerations)

.The example from `cuda-semantics` will work exactly the same for HIP::
[source,py]
-----------
    cuda = torch.device('cuda')     # Default HIP device
    cuda0 = torch.device('cuda:0')  # 'rocm' or 'hip' are not valid, use 'cuda'
    cuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)

    x = torch.tensor([1., 2.], device=cuda0)
    # x.device is device(type='cuda', index=0)
    y = torch.tensor([1., 2.]).cuda()
    # y.device is device(type='cuda', index=0)

    with torch.cuda.device(1):
        # allocates a tensor on GPU 1
        a = torch.tensor([1., 2.], device=cuda)

        # transfers a tensor from CPU to GPU 1
        b = torch.tensor([1., 2.]).cuda()
        # a.device and b.device are device(type='cuda', index=1)

        # You can also use ``Tensor.to`` to transfer a tensor:
        b2 = torch.tensor([1., 2.]).to(device=cuda)
        # b.device and b2.device are device(type='cuda', index=1)

        c = a + b
        # c.device is device(type='cuda', index=1)

        z = x + y
        # z.device is device(type='cuda', index=0)

        # even within a context, you can specify the device
        # (or give a GPU index to the .cuda call)
        d = torch.randn(2, device=cuda2)
        e = torch.randn(2).to(cuda2)
        f = torch.randn(2).cuda(cuda2)
        # d.device, e.device, and f.device are all device(type='cuda', index=2)
-----------


== DeepSeek-V3 Weight File Documentation

=== New Fields in `config.json`

- **model_type**: Specifies the model type, which is updated to `deepseek_v3` in this release.
- **num_nextn_predict_layers**: Indicates the number of Multi-Token Prediction (MTP) Modules. The open-sourced V3 weights include **1 MTP Module** .
- **quantization_config**: Describes the configuration for FP8 quantization.

---

=== Weight Structure Overview

The DeepSeek-V3 weight file consists of two main components: **Main Model Weights** and **MTP Modules**.

==== 1. Main Model Weights

- **Composition**:
  - Input/output embedding layers and a complete set of 61 Transformer hidden layers.
- **Parameter Count**:
  - Total parameters: **671B**
  - Activation parameters: **36.7B** (including 0.9B for Embedding and 0.9B for the output Head).

===== Structural Details

- **Embedding Layer**:
  - `model.embed_tokens.weight`
- **Transformer Hidden Layers**:
  - `model.layers.0` to `model.layers.60`, totaling `num_hidden_layers` layers.
- **Output Layer**:
  - `model.norm.weight`
  - `lm_head.weight`

==== 2. Multi-Token Prediction (MTP) Modules

- **Composition**:
  - Additional MTP Modules defined by the `num_nextn_predict_layers` field. In this model, the value is set to 1.
- **Parameter Count**:
  - Parameters: **11.5B unique parameters**, excluding the shared 0.9B Embedding and 0.9B output Head).
  - Activation parameters: **2.4B** (including the shared 0.9B Embedding and 0.9B output Head).

===== Structural Details

- **embed_tokens**: **Shares parameters** with the Embedding layer of the Main Model weights.
- **enorm & hnorm**: RMSNorm parameters required for speculative decoding.
- **eh_proj**: Parameters for dimensionality reduction projection on the norm results.
- **Additional Transformer Hidden Layer**:
  - `model.layers.61.self_attn & mlp` (structure identical to the Main Model hidden layers).
- **shared_head**: **Shares parameters** with the output Head of the Main Model weights.

---

==== Loading Rules

- **Main Model Weights**: Loaded via the `num_hidden_layers` parameter in `config.json`.
- **MTP Modules**: Loaded via the `num_nextn_predict_layers` parameter, with layer IDs appended immediately after the Main Model hidden layers. For example:
  - If `num_hidden_layers = 61` and `num_nextn_predict_layers = 1`, the MTP Module's layer ID is `61`.

---

=== FP8 Weight Documentation

DeepSeek-V3 natively supports FP8 weight format with 128x128 block scaling.

==== FP8 Configuration

The FP8 weight file introduces a `quantization_config` field to describe the quantization method. Below is an example configuration:

[source,json]
-------------
"quantization_config": {
  "activation_scheme": "dynamic",
  "fmt": "e4m3",
  "quant_method": "fp8",
  "weight_block_size": [128, 128]
}
-------------


- **Quantization Format**:
  - Format type: `fp8` and `e4m3` (corresponding to `torch.float8_e4m3fn`).
  - Weight block size: `128x128`.
- **Activation Quantization Scheme**:
  - Utilizes dynamic activation quantization (`dynamic`).

==== Dequantization Method

The FP8 weight file includes a `weight_scale_inv` field, which stores the dequantization scale for each weight block.

- **Storage Format**: `float32 Tensor`, stored alongside the weight data.
- **Dequantization Formula**:
  - If the weight block is not aligned to 128, it is zero-padded to 128 before calculating the scale. After quantization, the padded portion is removed.
  - The dequantization process is performed as: `(128x128 weight block) * weight_scale_inv`.

Through dequantization of the FP8 weights, runtime operations enable online quantization at a granularity of `per-token-per-128-channel`.

---

== DeepSeek-R1 README

https://vscode.dev/github/deepseek-ai/DeepSeek-R1/blob/main/README.md

https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf[Paper Link👁️]


=== 1. Introduction

We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. 
DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.
With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.
However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,
we introduce DeepSeek-R1, which incorporates cold-start data before RL.
DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. 
To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.

NOTE: **Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the Usage Recommendation section.**


https://vscode.dev/github/deepseek-ai/DeepSeek-R1/blob/main/figures/benchmark.jpg[figure benchmark]

=== 2. Model Summary

---

**Post-Training: Large-Scale Reinforcement Learning on the Base Model**

-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.

-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.
    We believe the pipeline will benefit the industry by creating better models. 

---

**Distillation: Smaller Models Can Be Powerful Too**

-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. 
- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.

=== 3. Model Downloads

==== DeepSeek-R1 Models

[opts=autowidth]
|===
| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** 

| DeepSeek-R1-Zero | 671B | 37B | 128K   | https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero[🤗 HuggingFace]
| DeepSeek-R1   | 671B | 37B |  128K   | https://huggingface.co/deepseek-ai/DeepSeek-R1[🤗 HuggingFace]
|===


DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. 
For more details regarding the model architecture, please refer to https://github.com/deepseek-ai/DeepSeek-V3[DeepSeek-V3] repository.

==== DeepSeek-R1-Distill Models

[opts=autowidth]
|===
| **Model** | **Base Model** | **Download**

| DeepSeek-R1-Distill-Qwen-1.5B  | https://huggingface.co/Qwen/Qwen2.5-Math-1.5B[Qwen2.5-Math-1.5B] | https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B[🤗 HuggingFace]
| DeepSeek-R1-Distill-Qwen-7B  | https://huggingface.co/Qwen/Qwen2.5-Math-7B[Qwen2.5-Math-7B] | https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B[🤗 HuggingFace]
| DeepSeek-R1-Distill-Llama-8B  | https://huggingface.co/meta-llama/Llama-3.1-8B[Llama-3.1-8B] | https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B[🤗 HuggingFace]
| DeepSeek-R1-Distill-Qwen-14B   | https://huggingface.co/Qwen/Qwen2.5-14B[Qwen2.5-14B] | https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B[🤗 HuggingFace]
|DeepSeek-R1-Distill-Qwen-32B  | https://huggingface.co/Qwen/Qwen2.5-32B[Qwen2.5-32B] | https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B[🤗 HuggingFace]
| DeepSeek-R1-Distill-Llama-70B  | https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct[Llama-3.3-70B-Instruct] | https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B[🤗 HuggingFace]
|===


DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.
We slightly change their configs and tokenizers. Please use our setting to run these models.

=== 4. Evaluation Results

==== DeepSeek-R1-Evaluation

For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of 0.6, a top-p value of 0.95, and generate 64 responses per query to estimate pass@1.

|===
| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 

1.3+|          | Architecture               | -        | -     | MoE      | -     | -        | MoE 
               | # Activated Params         | -        | -     | 37B      | -     | -        | 37B 
               | # Total Params             | -        | -     | 671B     | -     | -        | 671B 
1.10+| English | MMLU (Pass@1)              | 88.3     | 87.2  | 88.5     | 85.2  | **91.8** | 90.8 
               | MMLU-Redux (EM)            | 88.9     | 88.0  | 89.1     | 86.7  | -        | **92.9** 
               | MMLU-Pro (EM)              | 78.0     | 72.6  | 75.9     | 80.3  | -        | **84.0** 
               | DROP (3-shot F1)           | 88.3     | 83.7  | 91.6     | 83.9  | 90.2     | **92.2** 
               | IF-Eval (Prompt Strict)    | **86.5** | 84.3  | 86.1     | 84.8  | -        | 83.3 
               | GPQA-Diamond (Pass@1)      | 65.0     | 49.9  | 59.1     | 60.0  | **75.7** | 71.5 
               | SimpleQA (Correct)         | 28.4     | 38.2  | 24.9     | 7.0   | **47.0** | 30.1 
               | FRAMES (Acc.)              | 72.5     | 80.5  | 73.3     | 76.9  | -        | **82.5** 
               | AlpacaEval2.0 (LC-winrate) | 52.0     | 51.1  | 70.0     | 57.8  | -        | **87.6** 
               | ArenaHard (GPT-4-1106)     | 85.2     | 80.4  | 85.5     | 92.0  | -        | **92.3** 
1.5+| Code     | LiveCodeBench (Pass@1-COT) | 33.8     | 34.2  | -        | 53.8  | 63.4     | **65.9** 
               | Codeforces (Percentile)    | 20.3     | 23.6  | 58.7     | 93.4  | **96.6** | 96.3 
               | Codeforces (Rating)        | 717      | 759   | 1134     | 1820  | **2061** | 2029 
               | SWE Verified (Resolved)    | **50.8** | 38.8  | 42.0     | 41.6  | 48.9     | 49.2 
               | Aider-Polyglot (Acc.)      | 45.3     | 16.0  | 49.6     | 32.9  | **61.7** | 53.3 
1.3+| Math     | AIME 2024 (Pass@1)         | 16.0     | 9.3   | 39.2     | 63.6  | 79.2     | **79.8** 
               | MATH-500 (Pass@1)          | 78.3     | 74.6  | 90.2     | 90.0  | 96.4     | **97.3** 
               | CNMO 2024 (Pass@1)         | 13.1     | 10.8  | 43.2     | 67.6  | -        | **78.8** 
1.3+| Chinese  | CLUEWSC (EM)               | 85.4     | 87.9  | 90.9     | 89.9  | -        | **92.8** 
               | C-Eval (EM)                | 76.7     | 76.0  | 86.5     | 68.9  | -        | **91.8** 
               | C-SimpleQA (Correct)       | 55.4     | 58.7  | **68.0** | 40.3  | -        | 63.7 
|===



==== Distilled Model Evaluation


|===
| Model | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating 

| GPT-4o-0513                   | 9.3      | 13.4     | 74.6     | 49.9     | 32.9     | 759
| Claude-3.5-Sonnet-1022        | 16.0     | 26.7     | 78.3     | 65.0     | 38.9     | 717
| o1-mini                       | 63.6     | 80.0     | 90.0     | 60.0     | 53.8     | **1820**
| QwQ-32B-Preview               | 44.0     | 60.0     | 90.6     | 54.5     | 41.9     | 1316
| DeepSeek-R1-Distill-Qwen-1.5B | 28.9     | 52.7     | 83.9     | 33.8     | 16.9     | 954
| DeepSeek-R1-Distill-Qwen-7B   | 55.5     | 83.3     | 92.8     | 49.1     | 37.6     | 1189
| DeepSeek-R1-Distill-Qwen-14B  | 69.7     | 80.0     | 93.9     | 59.1     | 53.1     | 1481
| DeepSeek-R1-Distill-Qwen-32B  | **72.6** | 83.3     | 94.3     | 62.1     | 57.2     | 1691
| DeepSeek-R1-Distill-Llama-8B  | 50.4     | 80.0     | 89.1     | 49.0     | 39.6     | 1205
| DeepSeek-R1-Distill-Llama-70B | 70.0     | **86.7** | **94.5** | **65.2** | **57.5** | 1633
|===



=== 5. Chat Website & API Platform

You can chat with DeepSeek-R1 on DeepSeek's official website: https://chat.deepseek.com[chat.deepseek.com], and switch on the button "DeepThink"

We also provide OpenAI-Compatible API at DeepSeek Platform: https://platform.deepseek.com/[platform.deepseek.com]

=== 6. How to Run Locally

==== DeepSeek-R1 Models

Please visit https://github.com/deepseek-ai/DeepSeek-V3[DeepSeek-V3] repo for more information about running DeepSeek-R1 locally.

**NOTE: Hugging Face's Transformers has not been directly supported yet.**

==== DeepSeek-R1-Distill Models

DeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.

For instance, you can easily start a service using https://github.com/vllm-project/vllm[vLLM]:

[source,shell]
--------------
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager
--------------

You can also easily start a service using https://github.com/sgl-project/sglang[SGLang]

[source,bash]
-------------
python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2
-------------

==== Usage Recommendations

**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**

1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.
2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**
3. For mathematical problems, it is advisable to include a directive in your prompt such as: "Please reason step by step, and put your final answer within \boxed{}."
4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.

=== 7. License

This code repository and the model weights are licensed under the https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE[MIT License].
DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:

- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from https://github.com/QwenLM/Qwen2.5[Qwen-2.5 series], which are originally licensed under https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE[Apache 2.0 License], and now finetuned with 800k samples curated with DeepSeek-R1.

- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE[llama3.1 license].
- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE[llama3.3 license].

=== 8. Citation

https://arxiv.org/abs/2501.12948[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning]

DeepSeek AI 2025 DeepSeek R1 Incentivizing Reasoning Capability,

    year={2025},
    eprint={2501.12948},
    archivePrefix={arXiv},
    primaryClass={cs.CL},

Authors

  DeepSeek-AI     Daya Guo        Dejian Yang      Haowei Zhang    Junxiao Song
  Ruoyu Zhang     Runxin Xu       Qihao Zhu        Shirong Ma      Peiyi Wang
  Xiao Bi         Xiaokang Zhang  Xingkai Yu       Yu Wu           Z. F. Wu
  Zhibin Gou      Zhihong Shao    Zhuoshu Li       Ziyi Gao        Aixin Liu
  Bing Xue        Bingxuan Wang   Bochao Wu        Bei Feng        Chengda Lu
  Chenggang Zhao  Chengqi Deng    Chenyu Zhang     Chong Ruan      Damai Dai
  Deli Chen       Dongjie Ji      Erhang Li        Fangyun Lin     Fucong Dai
  Fuli Luo        Guangbo Hao     Guanting Chen    Guowei Li       H. Zhang
  Han Bao         Hanwei Xu       Haocheng Wang    Honghui Ding    Huajian Xin
  Huazuo Gao      Hui Qu          Hui Li           Jianzhong Guo   Jiashi Li
  Jiawei Wang     Jingchang Chen  Jingyang Yuan    Junjie Qiu      Junlong Li
  J. L. Cai       Jiaqi Ni        Jian Liang       Jin Chen        Kai Dong
  Kai Hu          Kaige Gao       Kang Guan        Kexin Huang     Kuai Yu
  Lean Wang       Lecong Zhang    Liang Zhao       Litong Wang     Liyue Zhang
  Lei Xu          Leyi Xia        Mingchuan Zhang  Minghua Zhang   Minghui Tang
  Meng Li         Miaojun Wang    Mingming Li      Ning Tian       Panpan Huang
  Peng Zhang      Qiancheng Wang  Qinyu Chen       Qiushi Du       Ruiqi Ge
  Ruisong Zhang   Ruizhe Pan      Runji Wang       R. J. Chen      R. L. Jin
  Ruyi Chen       Shanghao Lu     Shangyan Zhou    Shanhuang Chen  Shengfeng Ye
  Shiyu Wang      Shuiping Yu     Shunfeng Zhou    Shuting Pan     S. S. Li
  Shuang Zhou     Shaoqing Wu     Shengfeng Ye     Tao Yun         Tian Pei
  Tianyu Sun      T. Wang         Wangding Zeng    Wanjia Zhao     Wen Liu
  Wenfeng Liang   Wenjun Gao      Wenqin Yu        Wentao Zhang    W. L. Xiao
  Wei An          Xiaodong Liu    Xiaohan Wang     Xiaokang Chen   Xiaotao Nie
  Xin Cheng       Xin Liu         Xin Xie          Xingchao Liu    Xinyu Yang
  Xinyuan Li      Xuecheng Su     Xuheng Lin       X. Q. Li        Xiangyue Jin
  Xiaojin Shen    Xiaosha Chen    Xiaowen Sun      Xiaoxiang Wang  Xinnan Song
  Xinyi Zhou      Xianzu Wang     Xinxia Shan      Y. K. Li        Y. Q. Wang
  Y. X. Wei       Yang Zhang      Yanhong Xu       Yao Li          Yao Zhao
  Yaofeng Sun     Yaohui Wang     Yi Yu            Yichao Zhang    Yifan Shi
  Yiliang Xiong   Ying He         Yishi Piao       Yisong Wang     Yixuan Tan
  Yiyang Ma       Yiyuan Liu      Yongqiang Guo    Yuan Ou         Yuduan Wang
  Yue Gong        Yuheng Zou      Yujia He         Yunfan Xiong    Yuxiang Luo
  Yuxiang You     Yuxuan Liu      Yuyang Zhou      Y. X. Zhu       Yanhong Xu
  Yanping Huang   Yaohui Li       Yi Zheng         Yuchen Zhu      Yunxian Ma
  Ying Tang       Yukun Zha       Yuting Yan       Z. Z. Ren       Zehui Ren
  Zhangli Sha     Zhe Fu          Zhean Xu         Zhenda Xie      Zhengyan Zhang
  Zhewen Hao      Zhicheng Ma     Zhigang Yan      Zhiyu Wu        Zihui Gu
  Zijia Zhu       Zijun Liu       Zilin Li         Ziwei Xie       Ziyang Song
  Zizheng Pan     Zhen Huang      Zhipeng Xu       Zhongyu Zhang   Zhen Zhang

=== 9. Contact

If you have any questions, please raise an issue or contact us at service@deepseek.com[service@deepseek.com].


== DeepSeek-V3 README

https://vscode.dev/github/deepseek-ai/DeepSeek-V3/blob/main/README.md

=== 1. Introduction

We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. 
To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. 
Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. 
We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. 
Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.
Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.
In addition, its training process is remarkably stable. 
Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. 


https://vscode.dev/github.com/deepseek-ai/DeepSeek-V3/blob/main/figures/benchmark.png[figure: benchmark]

=== 2. Model Summary

---

**Architecture: Innovative Load Balancing Strategy and Training Objective**

- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.
-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. 
    It can also be used for speculative decoding for inference acceleration. 

---

**Pre-Training: Towards Ultimate Training Efficiency**

- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  
- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  
  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  
- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.

---

**Post-Training: Knowledge Distillation from DeepSeek-R1**

-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.

---


=== 3. Model Downloads

[opts=autowidth]
|===
| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** 

| DeepSeek-V3-Base | 671B | 37B | 128K | https://huggingface.co/deepseek-ai/DeepSeek-V3-Base[🤗 Hugging Face]   
| DeepSeek-V3      | 671B | 37B | 128K | https://huggingface.co/deepseek-ai/DeepSeek-V3[🤗 Hugging Face]   
|===


[NOTE]
The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.**

To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: #6-how-to-run-locally[How_to Run_Locally].

For developers looking to dive deeper, we recommend exploring ./README_WEIGHTS.md[README_WEIGHTS.md] for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.

=== 4. Evaluation Results
==== Base Model
===== Standard Benchmarks

[opts=autowidth]
|===
|              | Benchmark (Metric)       | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 

1.3+|          | Architecture                | -       | MoE      | Dense    | Dense     | MoE 
               | # Activated Params          | -       | 21B      | 72B      | 405B      | 37B 
               | # Total Params              | -       | 236B     | 72B      | 405B      | 671B 
1.16+| English | Pile-test (BPB)             | -       | 0.606    | 0.638    | **0.542** | 0.548 
               | BBH (EM)                    | 3-shot  | 78.8     | 79.8     | 82.9      | **87.5** 
               | MMLU (Acc.)                 | 5-shot  | 78.4     | 85.0     | 84.4      | **87.1** 
               | MMLU-Redux (Acc.)           | 5-shot  | 75.6     | 83.2     | 81.3      | **86.2** 
               | MMLU-Pro (Acc.)             | 5-shot  | 51.4     | 58.3     | 52.8      | **64.4** 
               | DROP (F1)                   | 3-shot  | 80.4     | 80.6     | 86.0      | **89.0** 
               | ARC-Easy (Acc.)             | 25-shot | 97.6     | 98.4     | 98.4      | **98.9** 
               | ARC-Challenge (Acc.)        | 25-shot | 92.2     | 94.5     | **95.3**  | **95.3** 
               | HellaSwag (Acc.)            | 10-shot | 87.1     | 84.8     | **89.2**  | 88.9 
               | PIQA (Acc.)                 | 0-shot  | 83.9     | 82.6     | **85.9**  | 84.7 
               | WinoGrande (Acc.)           | 5-shot  | **86.3** | 82.3     | 85.2      | 84.9 
               | RACE-Middle (Acc.)          | 5-shot  | 73.1     | 68.1     | **74.2**  | 67.1 
               | RACE-High (Acc.)            | 5-shot  | 52.6     | 50.3     | **56.8**  | 51.3 
               | TriviaQA (EM)               | 5-shot  | 80.0     | 71.9     | **82.7**  | **82.9** 
               | NaturalQuestions (EM)       | 5-shot  | 38.6     | 33.2     | **41.5**  | 40.0 
               | AGIEval (Acc.)              | 0-shot  | 57.5     | 75.8     | 60.6      | **79.6** 
1.5+| Code     | HumanEval (Pass@1)          | 0-shot  | 43.3     | 53.0     | 54.9      | **65.2** 
               | MBPP (Pass@1)               | 3-shot  | 65.0     | 72.6     | 68.4      | **75.4** 
               | LiveCodeBench-Base (Pass@1) | 3-shot  | 11.6     | 12.9     | 15.5      | **19.4** 
               | CRUXEval-I (Acc.)           | 2-shot  | 52.5     | 59.1     | 58.5      | **67.3** 
               | CRUXEval-O (Acc.)           | 2-shot  | 49.8     | 59.9     | 59.9      | **69.8** 
1.4+| Math     | GSM8K (EM)                  | 8-shot  | 81.6     | 88.3     | 83.5      | **89.3** 
               | MATH (EM)                   | 4-shot  | 43.4     | 54.4     | 49.0      | **61.6** 
               | MGSM (EM)                   | 8-shot  | 63.6     | 76.2     | 69.9      | **79.8** 
               | CMath (EM)                  | 3-shot  | 78.7     | 84.5     | 77.3      | **90.7** 
1.6+| Chinese  | CLUEWSC (EM)                | 5-shot  | 82.0     | 82.5     | **83.0**  | 82.7 
               | C-Eval (Acc.)               | 5-shot  | 81.4     | 89.2     | 72.5      | **90.1** 
               | CMMLU (Acc.)                | 5-shot  | 84.0     | **89.5** | 73.7      | 88.8 
               | CMRC (EM)                   | 1-shot  | **77.4** | 75.8     | 76.0      | 76.3 
               | C3 (Acc.)                   | 0-shot  | 77.4     | 76.7     | **79.7**  | 78.6 
               | CCPM (Acc.)                 | 0-shot  | **93.0** | 88.5     | 78.6      | 92.0 
| Multilingual | MMMLU-non-English (Acc.)    | 5-shot  | 64.0     | 74.8     | 73.8      | **79.4** 
|===


[NOTE]
Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered 
to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks,
especially on math and code tasks.
For more evaluation details, please check our paper. 

===== Context Window

https://vscode.dev/github/deepseek-ai/DeepSeek-V3/blob/main/figures/niah.png


Evaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. 

==== Chat Model
===== Standard Benchmarks (Models larger than 67B)

|===
|              | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** 

1.3+ |         | Architecture               | MoE  | MoE  | Dense    | Dense    | -        | -        | MoE 
               | # Activated Params         | 21B  | 21B  | 72B      | 405B     | -        | -        | 37B 
               | # Total Params             | 236B | 236B | 72B      | 405B     | -        | -        | 671B 
1.10+| English | MMLU (EM)                  | 78.2 | 80.6 | 85.3     | **88.6** | **88.3** | 87.2     | **88.5** 
               | MMLU-Redux (EM)            | 77.9 | 80.3 | 85.6     | 86.2     | **88.9** | 88.0     | **89.1** 
               | MMLU-Pro (EM)              | 58.5 | 66.2 | 71.6     | 73.3     | **78.0** | 72.6     | 75.9 
               | DROP (3-shot F1)           | 83.0 | 87.8 | 76.7     | 88.7     | 88.3     | 83.7     | **91.6** 
               | IF-Eval (Prompt Strict)    | 57.7 | 80.6 | 84.1     | 86.0     | **86.5** | 84.3     | 86.1 
               | GPQA-Diamond (Pass@1)      | 35.3 | 41.3 | 49.0     | 51.1     | **65.0** | 49.9     | 59.1 
               | SimpleQA (Correct)         | 9.0  | 10.2 | 9.1      | 17.1     | 28.4     | **38.2** | 24.9 
               | FRAMES (Acc.)              | 66.9 | 65.4 | 69.8     | 70.0     | 72.5     | **80.5** | 73.3 
               | LongBench v2 (Acc.)        | 31.6 | 35.4 | 39.4     | 36.1     | 41.0     | 48.1     | **48.7** 
1.7+| Code     | HumanEval-Mul (Pass@1)     | 69.3 | 77.4 | 77.3     | 77.2     | 81.7     | 80.5     | **82.6** 
               | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1     | 28.4     | 36.3     | 33.4     | **40.5** 
               | LiveCodeBench (Pass@1)     | 20.3 | 28.4 | 28.7     | 30.1     | 32.8     | 34.2     | **37.6** 
               | Codeforces (Percentile)    | 17.5 | 35.6 | 24.8     | 25.3     | 20.3     | 23.6     | **51.6** 
               | SWE Verified (Resolved)    | -    | 22.6 | 23.8     | 24.5     | **50.8** | 38.8     | 42.0 
               | Aider-Edit (Acc.)          | 60.3 | 71.6 | 65.4     | 63.9     | **84.2** | 72.9     | 79.7 
               | Aider-Polyglot (Acc.)      | -    | 18.2 | 7.6      | 5.8      | 45.3     | 16.0     | **49.6** 
1.3+| Math     | AIME 2024 (Pass@1)         | 4.6  | 16.7 | 23.3     | 23.3     | 16.0     | 9.3      | **39.2** 
               | MATH-500 (EM)              | 56.3 | 74.7 | 80.0     | 73.8     | 78.3     | 74.6     | **90.2** 
               | CNMO 2024 (Pass@1)         | 2.8  | 10.8 | 15.9     | 6.8      | 13.1     | 10.8     | **43.2** 
1.3+| Chinese  | CLUEWSC (EM)               | 89.9 | 90.4 | **91.4** | 84.7     | 85.4     | 87.9     | 90.9 
               | C-Eval (EM)                | 78.6 | 79.5 | 86.1     | 61.5     | 76.7     | 76.0     | **86.5** 
               | C-SimpleQA (Correct)       | 48.5 | 54.1 | 48.4     | 50.4     | 51.3     | 59.3     | **64.8** 
|===

[NOTE]
    All models are evaluated in a configuration that limits the output length to 8K. 
    Benchmarks containing fewer than 1000 samples are tested multiple times using 
    varying temperature settings to derive robust final results. DeepSeek-V3 stands 
    as the best-performing open-source model, and also exhibits competitive 
    performance against frontier closed-source models.


=====  Open Ended Generation Evaluation

[opts=autowidth]
|===
| Model                  | Arena-Hard | AlpacaEval 2.0 

| DeepSeek-V2.5-0905     | 76.2       | 50.5 
| Qwen2.5-72B-Instruct   | 81.2       | 49.1 
| LLaMA-3.1 405B         | 69.3       | 40.5 
| GPT-4o-0513            | 80.4       | 51.1 
| Claude-Sonnet-3.5-1022 | 85.2       | 52.0 
| DeepSeek-V3            | **85.5**   | **70.0** 
|===


[NOTE]
    English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.


=== 5. Chat Website & API Platform

You can chat with DeepSeek-V3 on DeepSeek's official website: https://chat.deepseek.com/sign_in[chat.deepseek.com]

We also provide OpenAI-Compatible API at DeepSeek Platform: https://platform.deepseek.com/[platform.deepseek.com]

=== 6. How to Run Locally

DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:

1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.
2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes, with Multi-Token Prediction https://github.com/sgl-project/sglang/issues/2591[coming soon].
3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.
4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.
5. **vLLM**: Support DeepSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.
6. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.
7. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices.

Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.

Here is an example of converting FP8 weights to BF16:

[source,shell]
--------------
cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights
--------------

[NOTE]
Hugging Face's Transformers has not been directly supported yet.**

==== 6.1 Inference with DeepSeek-Infer Demo (example only)

===== System Requirements

[NOTE] 
Linux with Python 3.10 only. Mac and Windows are not supported.

Dependencies:

[source,shell]
--------------
torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
--------------

===== Model Weights & Demo Code Preparation

First, clone our DeepSeek-V3 GitHub repository:

[source,shell]
git clone https://github.com/deepseek-ai/DeepSeek-V3.git

Navigate to the `inference` folder and install dependencies listed in `requirements.txt`. Easiest way is to use a package manager like `conda` or `uv` to create a new virtual environment and install the dependencies.

[source,shell]
--------------
cd DeepSeek-V3/inference
pip install -r requirements.txt
--------------

Download the model weights from Hugging Face, and put them into `/path/to/DeepSeek-V3` folder.

===== Model Weights Conversion

Convert Hugging Face model weights to a specific format:

[source,shell]
python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16

===== Run

Then you can chat with DeepSeek-V3:

[source,shell]
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200

Or batch inference on a given file:

[source,shell]
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE

==== 6.2 Inference with SGLang (recommended)

https://github.com/sgl-project/sglang[SGLang] currently supports https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations[MLA optimizations], https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models[DP Attention], FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.

Notably, https://github.com/sgl-project/sglang/releases/tag/v0.4.1[SGLang v0.4.1] fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.

SGLang also supports https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208[multi-node tensor parallelism], enabling you to run this model on multiple network-connected machines.

Multi-Token Prediction (MTP) is in development, and progress can be tracked in the https://github.com/sgl-project/sglang/issues/2591[optimization plan].

Here are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3

==== 6.3 Inference with LMDeploy (recommended)

https://github.com/InternLM/lmdeploy[LMDeploy], a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.

For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960


==== 6.4 Inference with TRT-LLM (recommended)

https://github.com/NVIDIA/TensorRT-LLM[TensorRT-LLM] now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3. 


==== 6.5 Inference with vLLM (recommended)

https://github.com/vllm-project/vllm[vLLM] v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the https://docs.vllm.ai/en/latest/serving/distributed_serving.html[vLLM instructions]. Please feel free to follow https://github.com/vllm-project/vllm/issues/11539[the enhancement plan] as well.

==== 6.6 Recommended Inference Functionality with AMD GPUs

In collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the #63-inference-with-lmdeploy-recommended[SGLang instructions].

==== 6.7 Recommended Inference Functionality with Huawei Ascend NPUs
The https://www.hiascend.com/en/software/mindie[MindIE] framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the https://modelers.cn/models/MindIE/deepseekv3[instructions here].


=== 7. License

This code repository is licensed under LICENSE-CODE[the MIT License]. The use of DeepSeek-V3 Base/Chat models is subject to LICENSE-MODEL[the Model License]. DeepSeek-V3 series (including Base and Chat) supports commercial use.

=== 8. Citation

DeepSeek AI 2024 DeepSeek V3 Technical Report,

DeepSeek-V3 Technical Report

    year={2024},
    eprint={2412.19437},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2412.19437}, 

Authors

  DeepSeek-AI      Aixin Liu      Bei Feng        Bing Xue        Bingxuan Wang
  Bochao Wu        Chengda Lu     Chenggang Zhao  Chengqi Deng    Chenyu Zhang
  Chong Ruan       Damai Dai      Daya Guo        Dejian Yang     Deli Chen
  Dongjie Ji       Erhang Li      Fangyun Lin     Fucong Dai      Fuli Luo
  Guangbo Hao      Guanting Chen  Guowei Li       H. Zhang        Han Bao
  Hanwei Xu        Haocheng Wang  Haowei Zhang    Honghui Ding    Huajian Xin
  Huazuo Gao       Hui Li         Hui Qu          J. L. Cai       Jian Liang
  Jianzhong Guo    Jiaqi Ni       Jiashi Li       Jiawei Wang     Jin Chen
  Jingchang Chen   Jingyang Yuan  Junjie Qiu      Junlong Li      Junxiao Song
  Kai Dong         Kai Hu         Kaige Gao       Kang Guan       Kexin Huang
  Kuai Yu          Lean Wang      Lecong Zhang    Lei Xu          Leyi Xia
  Liang Zhao       Litong Wang    Liyue Zhang     Meng Li         Miaojun Wang
  Mingchuan Zhang  Minghua Zhang  Minghui Tang    Mingming Li     Ning Tian
  Panpan Huang     Peiyi Wang     Peng Zhang      Qiancheng Wang  Qihao Zhu
  Qinyu Chen       Qiushi Du      R. J. Chen      R. L. Jin       Ruiqi Ge
  Ruisong Zhang    Ruizhe Pan     Runji Wang      Runxin Xu       Ruoyu Zhang
  Ruyi Chen        S. S. Li       Shanghao Lu     Shangyan Zhou   Shanhuang Chen
  Shaoqing Wu      Shengfeng Ye   Shengfeng Ye    Shirong Ma      Shiyu Wang
  Shuang Zhou      Shuiping Yu    Shunfeng Zhou   Shuting Pan     T. Wang
  Tao Yun          Tian Pei       Tianyu Sun      W. L. Xiao      Wangding Zeng
  Wanjia Zhao      Wei An         Wen Liu         Wenfeng Liang   Wenjun Gao
  Wenqin Yu        Wentao Zhang   X. Q. Li        Xiangyue Jin    Xianzu Wang
  Xiao Bi          Xiaodong Liu   Xiaohan Wang    Xiaojin Shen    Xiaokang Chen
  Xiaokang Zhang   Xiaosha Chen   Xiaotao Nie     Xiaowen Sun     Xiaoxiang Wang
  Xin Cheng        Xin Liu        Xin Xie         Xingchao Liu    Xingkai Yu
  Xinnan Song      Xinxia Shan    Xinyi Zhou      Xinyu Yang      Xinyuan Li
  Xuecheng Su      Xuheng Lin     Y. K. Li        Y. Q. Wang      Y. X. Wei
  Y. X. Zhu        Yang Zhang     Yanhong Xu      Yanhong Xu      Yanping Huang
  Yao Li           Yao Zhao       Yaofeng Sun     Yaohui Li       Yaohui Wang
  Yi Yu            Yi Zheng       Yichao Zhang    Yifan Shi       Yiliang Xiong
  Ying He          Ying Tang      Yishi Piao      Yisong Wang     Yixuan Tan
  Yiyang Ma        Yiyuan Liu     Yongqiang Guo   Yu Wu           Yuan Ou
  Yuchen Zhu       Yuduan Wang    Yue Gong        Yuheng Zou      Yujia He
  Yukun Zha        Yunfan Xiong   Yunxian Ma      Yuting Yan      Yuxiang Luo
  Yuxiang You      Yuxuan Liu     Yuyang Zhou     Z. F. Wu        Z. Z. Ren
  Zehui Ren        Zhangli Sha    Zhe Fu          Zhean Xu        Zhen Huang
  Zhen Zhang       Zhenda Xie     Zhengyan Zhang  Zhewen Hao      Zhibin Gou
  Zhicheng Ma      Zhigang Yan    Zhihong Shao    Zhipeng Xu      Zhiyu Wu
  Zhongyu Zhang    Zhuoshu Li     Zihui Gu        Zijia Zhu       Zijun Liu
  Zilin Li         Ziwei Xie      Ziyang Song     Ziyi Gao        Zizheng Pan


=== 9. Contact

If you have any questions, please raise an issue or contact us at service@deepseek.com[service@deepseek.com].


== DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning

https://arxiv.org/abs/2501.12948[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning]

https://vscode.dev/github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf


=== Abstract

We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.
DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised 
fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.
Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing
reasoning behaviors. However, it encounters challenges such as poor readability, and language
mixing. To address these issues and further enhance reasoning performance, we introduce
DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the
research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models
(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.

    Accuracy / Percentile (%)

    DeepSeek-R1      79.8 ███████▌▏          96.3 █████████▌▌        71.5 ██████▌
    OpenAI-o1-1217   79.2 ███████▌           96.6 █████████▌▌        75.7 ███████▌
    DeepSeek-R1-32B  72.6 ███████▏           90.6 ████████▌▌▏        62.1 ██████▏
    OpenAI-o1-mini   63.6 ██████▏            93.4 ████████▌▌▏        60.0 ██████
    DeepSeek-V3      39.2 ███▌▏              58.7 █████▌             59.1 ██████
                AIME 2024 (Pass@1)  Codeforces (Percentile)  GPQA Diamond (Pass@1)           

    DeepSeek-R1      97.3 █████████▌▌        90.8 ████████▌▌         49.2 ████▌▌
    OpenAI-o1-1217   96.4 █████████▌▌        91.8 ████████▌▌▏        48.9 ████▌▌
    DeepSeek-R1-32B  94.3 █████████▌▌        87.4 ████████▌▏         36.8 ███▌▏
    OpenAI-o1-mini   90.0 ████████▌▌▏        85.2 ███████▌▏          41.6 ████▏
    DeepSeek-V3      90.2 ████████▌▌▏        88.5 ███████▌▌▏         42.0 ████▏
                MATH-500 (Pass@1)    MMLU (Pass@1)     SWE-bench Verified (Resolved)   

Figure 1 | Benchmark performance of DeepSeek-R1.

=== Contents

link:#R1_03[1 Introduction ⋈3] +
link:#R1_04[1.1 Contributions ⋈4] +
link:#R1_04[1.2 Summary of Evaluation Results ⋈4] +
link:#R1_05[2 Approach ⋈5] +
link:#R1_05[2.1 Overview ⋈5] +
link:#R1_05[2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model ⋈5] +
link:#R1_05[2.2.1 Reinforcement Learning Algorithm ⋈5] +
link:#R1_06[2.2.2 Reward Modeling ⋈6] +
link:#R1_06[2.2.3 Training Template ⋈6] +
link:#R1_06[2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ⋈6] +
link:#R1_09[2.3 DeepSeek-R1: Reinforcement Learning with Cold Start ⋈9] +
link:#R1_09[2.3.1 Cold Start ⋈9] +
link:#R1_10[2.3.2 Reasoning-oriented Reinforcement Learning ⋈10] +
link:#R1_10[2.3.3 Rejection Sampling and Supervised Fine-Tuning ⋈10] +
link:#R1_11[2.3.4 Reinforcement Learning for all Scenarios ⋈11] +
link:#R1_11[2.4 Distillation: Empower Small Models with Reasoning Capability ⋈11] +
link:#R1_11[3 Experiment ⋈11] +
link:#R1_13[3.1 DeepSeek-R1 Evaluation ⋈13] +
link:#R1_14[3.2 Distilled Model Evaluation ⋈14] +
link:#R1_14[4 Discussion ⋈14] +
link:#R1_14[4.1 Distillation v.s. Reinforcement Learning ⋈14] +
link:#R1_15[4.2 Unsuccessful Attempts ⋈15] +
link:#R1_16[5 Conclusion, Limitations, and Future Work ⋈16] +
link:#R1_20[A Contributions and Acknowledgments ⋈20] +


[[R1_2]]

 
=== 1. Introduction

In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and
evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap
towards Artificial General Intelligence (AGI).

Recently, post-training has emerged as an important component of the full training pipeline.
It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt
to user preferences, all while requiring relatively minimal computational resources against
pre-training. In the context of reasoning capabilities, OpenAI’s o1 (OpenAI, 2024b) series models
were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought 
reasoning process. This approach has achieved significant improvements in various
reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge
of effective test-time scaling remains an open question for the research community. Several prior
works have explored various approaches, including process-based reward models (Lightman
et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),
and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh
et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning
performance comparable to OpenAI’s o1 series models.

In this paper, we take the first step toward improving language model reasoning capabilities
using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop
reasoning capabilities without any supervised data, focusing on their self-evolution through
a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ
GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.
During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting
reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance
on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to
71.0%, and with majority voting, the score further improves to 86.7%, matching the performance
of OpenAI-o1-0912.

However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language
mixing. To address these issues and further enhance reasoning performance, we introduce
DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training
pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the
DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. 
Upon nearing convergence in the RL process, we create new SFT data through rejection
sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains
such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.
After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking
into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to
as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.

We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B 
(Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying
RL on it. This demonstrates that the reasoning patterns discovered by larger base models are 
crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey
et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source
QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a
new record on the reasoning benchmarks among dense models.

[[R1_3]]


==== 1.1. Contributions

Post-Training: Large-Scale Reinforcement Learning on the Base Model

•   We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as
    a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for
    solving complex problems, resulting in the development of DeepSeek-R1-Zero. 
    DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating
    long CoTs, marking a significant milestone for the research community. Notably, it is the
    first open research to validate that reasoning capabilities of LLMs can be incentivized
    purely through RL, without the need for SFT. This breakthrough paves the way for future
    advancements in this area.

•   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL
    stages aimed at discovering improved reasoning patterns and aligning with human pref-
    erences, as well as two SFT stages that serve as the seed for the model’s reasoning and
    non-reasoning capabilities. We believe the pipeline will benefit the industry by creating
    better models.

Distillation: Smaller Models Can Be Powerful Too

•   We demonstrate that the reasoning patterns of larger models can be distilled into smaller
    models, resulting in better performance compared to the reasoning patterns discovered
    through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit
    the research community to distill better smaller models in the future.

•   Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models
    that are widely used in the research community. The evaluation results demonstrate that
    the distilled smaller dense models perform exceptionally well on benchmarks. 
    DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. 
    Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500,
    and 57.2% on LiveCodeBench. These results significantly outperform previous open-source 
    models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B,
    32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.

==== 1.2. Summary of Evaluation Results

•   *Reasoning tasks*: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly
    surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%,
    performing on par with OpenAI-o1-1217 and significantly outperforming other models. (2)
    On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,
    as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in
    the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than
    DeepSeek-V3, which could help developers in real world tasks.

•   *Knowledge*: OnbenchmarkssuchasMMLU,MMLU-Pro, andGPQADiamond, DeepSeek-R1 
    achieves outstanding results, significantly outperforming DeepSeek-V3 with scores
    of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its
    performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1
    surpasses other closed-source models, demonstrating its competitive edge in educational
    tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,
    demonstrating its capability in handling fact-based queries. A similar trend is observed
    where OpenAI-o1 surpasses 4o on this benchmark.

[[R1_4]]

•   *Others*: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,
    general question answering, editing, summarization, and more. It achieves an impressive
    length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on 
    ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.
    Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring
    long-context understanding, substantially outperforming DeepSeek-V3 on long-context
    benchmarks.
 
=== 2. Approach


==== 2.1. Overview

Previous work has heavily relied on large amounts of supervised data to enhance model
performance. In this study, we demonstrate that reasoning capabilities can be significantly
improved through large-scale reinforcement learning (RL), even without using supervised
fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with
the inclusion of a small amount of cold-start data. In the following sections, we present: (1)
DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and
(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of
long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to
small dense models.

==== 2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model

Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as 
evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works
heavily depended on supervised data, which are time-intensive to gather. In this section, we
explore the potential of LLMs to develop reasoning capabilities without any supervised data,
focusing on their self-evolution through a pure reinforcement learning process. We start with a
brief overview of our RL algorithm, followed by the presentation of some exciting results, and
hope this provides the community with valuable insights.

===== 2.2.1. Reinforcement Learning Algorithm

Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group
Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is
typically the same size as the policy model, and estimates the baseline from group scores instead.
Specifically, for each question 𝑟 , GRPO samples a group of outputs stem:[{ o_1 , o_2 , ··· , o_G }]
from the old policy stem:[\pi_{θ_{old}}] and then optimizes the policy model stem:[\pi_θ] by 
maximizing the following objective:

[stem]
++++++
\begin{aligned}
𝕁_{GPRO}(θ) &= 𝔼[q ~ P(Q), {o_i}_{i=1}^G ~ π_{θ_{old}}(O|q)] \\

&\frac{1}{G} \sum_{i=1}^G 
\begin{pmatrix} 
min(\frac{π_θ (o_i|q)}{π_{θ_{old}}(o_i|q)} A_i, 
clip(\frac{π_θ(o_i|q)}{π_{θ_i{old}}(o_i|q)},1-ε,1+ε)A_i) 
-β\mathbb{D}_{kl}(π_θ \Vert π_{ref}) 
\end{pmatrix},               \qquad\qquad                            (1) \\

&\mathbb{D}_{KL}(π_θ \Vert π_{ref}) \frac{π_{ref}(o_i|q)}{π_θ(o_i|q) }
- \log\frac{π_{ref}(o_i|q)}{π_θ(o_i|q)} - 1            \qquad\qquad  (2) \\
\end{aligned}
++++++

where ε and β are hyper-parameters, and stem:[A_i] is the advantage, computed using a group of
rewards stem:[{ r_1 , r_2 , ... , r_G }] corresponding to the outputs within each group:

[stem]
++++++
A_i = \frac{r_i − mean({r_1 , r_2 ,··· , r_G })}
           { std({ r_ 1 , r_ 2 ,··· , r_G })} .  \qquad\qquad       (3) \\
++++++

[[R1_5]]

''''
A conversation between User and Assistant. The user asks a question, and the Assistant solves it.
The assistant first thinks about the reasoning process in the mind and then provides the user
with the answer. The reasoning process and answer are enclosed within `<think> </think>` and
`<answer> </answer>` tags, respectively, i.e., `<think> reasoning process here </think>`
`<answer> answer here </answer>`. User: prompt. Assistant:
''''

Table 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning
question during training.

===== 2.2.2. Reward Modeling

The reward is the source of the training signal, which decides the optimization direction of RL.
To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two
types of rewards:

•   *Accuracy rewards*: The accuracy reward model evaluates whether the response is correct.
    For example, in the case of math problems with deterministic results, the model is required
    to provide the final answer in a specified format (e.g., within a box), enabling reliable
    rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be
    used to generate feedback based on predefined test cases.

•   *Format rewards*: In addition to the accuracy reward model, we employ a format reward
    model that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’
    tags.

We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,
because we find that the neural reward model may suffer from reward hacking in the large-scale
reinforcement learning process, and retraining the reward model needs additional training
resources and it complicates the whole training pipeline.

===== 2.2.3. Training Template

To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides
the base model to adhere to our specified instructions. As depicted in Table 1, this template
requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer.
We intentionally limit our constraints to this structural format, avoiding any content-specific
biases—such as mandating reflective reasoning or promoting particular problem-solving 
strategies—to ensure that we can accurately observe the model’s natural progression during the RL
process.

===== 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero

*Performance of DeepSeek-R1-Zero* Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero 
on the AIME 2024 benchmark throughout the RL training process. As illustrated,
DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the
RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant
increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels
comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL
algorithm in optimizing the model’s performance over time.

Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI’s o1-0912
models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers

[[R1_6]]

|======
1.2+|Model     2.1+|AIME 2024        |MATH-500 |GPQA Diamond |LiveCode Bench |CodeForces
                   |pass@1 |cons@64  |pass@1   |pass@1       |pass@1         |rating

|OpenAI-o1-mini    |63.6   |80.0     |90.0     |60.0         |53.8           |1820
|OpenAI-o1-0912    |74.4   |83.3     |94.8     |77.3         |63.4           |1843
|DeepSeek-R1-Zero  |71.0   |86.7     |95.9     |73.3         |50.0           |1444
|======

Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related
benchmarks.

Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample
16 responses and calculate the overall average accuracy to ensure a stable evaluation.

DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised
fine-tuning data. This is a noteworthy achievement, as it underscores the model’s ability to
learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-
R1-Zero can be further augmented through the application of majority voting. For example,
when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance
escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The
ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without
majority voting, highlights its strong foundational capabilities and its potential for further
advancements in reasoning tasks.

*Self-evolution Process of DeepSeek-R1-Zero* The self-evolution process of DeepSeek-R1-Zero
is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities
autonomously. By initiating RL directly from the base model, we can closely monitor the model’s
progression without the influence of the supervised fine-tuning stage. This approach provides
a clear view of how the model evolves over time, particularly in terms of its ability to handle
complex reasoning tasks.


[[R1_7]]

Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL
process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.

As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement 
throughout the training process. This improvement is not the result of external adjustments
but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the
ability to solve increasingly complex reasoning tasks by leveraging extended test-time 
computation. This computation ranges from generating hundreds to thousands of reasoning tokens,
allowing the model to explore and refine its thought processes in greater depth.

One of the most remarkable aspects of this self-evolution is the emergence of sophisticated
behaviors as the test-time computation increases. Behaviors such as reflection—where the model
revisits and reevaluates its previous steps—and the exploration of alternative approaches to
problem-solving arise spontaneously. These behaviors are notexplicitly programmed butinstead
emerge as a result of the model’s interaction with the reinforcement learning environment. This
spontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities,
enabling it to tackle more challenging tasks with greater efficiency and accuracy.

*Aha Moment of DeepSeek-R1-Zero* A particularly intriguing phenomenon observed during
the training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as
illustrated in Table 3, occurs in an intermediate version of the model. During this phase,
DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial
approach. This behavior is not only a testament to the model’s growing reasoning abilities
but also a captivating example of how reinforcement learning can lead to unexpected and
sophisticated outcomes.

This moment is not only an “aha moment” for the model but also for the researchers
observing its behavior. It underscores the power and beauty of reinforcement learning: rather
than explicitly teaching the model on how to solve a problem, we simply provide it with the
right incentives, and it autonomously develops advanced problem-solving strategies. The
“aha moment” serves as a powerful reminder of the potential of RL to unlock new levels of
intelligence in artificial systems, paving the way for more autonomous and adaptive models in
the future.

[[R1_8]]

''''
Question: If α > 1, then the sum of the real solutions of
stem:[\sqrt{α − \sqrt{α + x}} = x] is equal to

Response: <think>
To solve the equation stem:[\sqrt{α −\sqrt{ α + x}} = x], let’s start by squaring both ···
stem:[(\sqrt{α − \sqrt{α + x}}^2 = x^2 ⇒ α − \sqrt{α + x} = x^2] .

Rearrange to isolate the inner square root term:
stem:[(α − x^2)^2 = α + x ⇒ α^2 −2αx^2 + (x^2)^2 = α + x ⇒ x 4 − 2 αx^2 − x + (α^2 − α) = 0]
...
[.red]#Wait, wait. Wait. That’s an aha moment I can flag here.#
Let’s reevaluate this step-by-step to identify if the correct sum can be ···
We started with the equation:
stem:[\sqrt{α − \sqrt{α + x}} = x]
First, let’s square both sides:
stem:[α − \sqrt{α + x} = x^2 ⇒ \sqrt{α + x} = α − x^2]
Next, I could square both sides again, treating the equation: ···
...

''''

Table 3 | An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. The
model learns to rethink using an anthropomorphic tone. This is also an aha moment for us,
allowing us to witness the power and beauty of reinforcement learning.

**Drawback of DeepSeek-R1-Zero** Although DeepSeek-R1-Zero exhibits strong reasoning
capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces
several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability,
and language mixing. To make reasoning processes more readable and share them with the
open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly
cold-start data.

==== 2.3. DeepSeek-R1: Reinforcement Learning with Cold Start

Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can
reasoning performance be further improved or convergence accelerated by incorporating a small
amount of high-quality data as a cold start? 2) How can we train a user-friendly model that
not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong
general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The
pipeline consists of four stages, outlined as follows.

===== 2.3.1. Cold Start

Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from
the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data
to fine-tune the model as the initial RL actor. To collect such data, we have explored several
approaches: using few-shot prompting with a long CoT as an example, directly prompting
models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero 
outputs in a readable format, and refining the results through post-processing by human
annotators.

In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as
the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data
include:

[[R1_9]]

•   Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable
    for reading. Responses may mix multiple languages or lack markdown formatting to
    highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,
    we design a readable pattern that includes a summary at the end of each response and
    filters out responses that are not reader-friendly. Here, we define the output format as
    |special_token|<reasoning_process>|special_token|<summary>, where the reasoning
    process is the CoT for the query, and the summary is used to summarize the reasoning
    results.

•   Potential: By carefully designing the pattern for cold-start data with human priors, we
    observe better performance against DeepSeek-R1-Zero. We believe the iterative training is
    a better way for reasoning models.

===== 2.3.2. Reasoning-oriented Reinforcement Learning

After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale
reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses
on enhancing the model’s reasoning capabilities, particularly in reasoning-intensive tasks such
as coding, mathematics, science, and logic reasoning, which involve well-defined problems with
clear solutions. During the training process, we observe that CoT often exhibits language mixing,
particularly when RL prompts involve multiple languages. To mitigate the issue of language
mixing, we introduce a language consistency reward during RL training, which is calculated
as the proportion of target language words in the CoT. Although ablation experiments show
that such alignment results in a slight degradation in the model’s performance, this reward
aligns with human preferences, making it more readable. Finally, we combine the accuracy of
reasoning tasks and the reward for language consistency by directly summing them to form the
final reward. We then apply RL training on the fine-tuned model until it achieves convergence
on reasoning tasks.

===== 2.3.3. Rejection Sampling and Supervised Fine-Tuning

When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT
(Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which
primarily focuses on reasoning, this stage incorporates data from other domains to enhance the
model’s capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we
generate the data and fine-tune the model as described below.

*Reasoning data* We curate reasoning prompts and generate reasoning trajectories by performing 
rejection sampling from the checkpoint from the above RL training. In the previous stage,
we only included data that could be evaluated using rule-based rewards. However, in this stage,
we expand the dataset by incorporating additional data, some of which use a generative reward
model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.
Additionally, because the model output is sometimes chaotic and difficult to read, we have
filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For
each prompt, we sample multiple responses and retain only the correct ones. In total, we collect
about 600k reasoning related training samples.

[[R1_10]]

*Non-Reasoning data* For non-reasoning data, such as writing, factual QA, self-cognition,
and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of
DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential
chain-of-thought before answering the question by prompting. However, for simpler queries,
such as “hello” we do not provide a CoT in response. In the end, we collected a total of
approximately 200k training samples that are unrelated to reasoning.

We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about
800k samples.

===== 2.3.4. Reinforcement Learning for all Scenarios

To further align the model with human preferences, we implement a secondary reinforcement
learning stage aimed at improving the model’s helpfulness and harmlessness while simultaneously 
refining its reasoning capabilities. Specifically, we train the model using a combination
of reward signals and diverse prompt distributions. For reasoning data, we adhere to the
methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the
learning process in math, code, and logical reasoning domains. For general data, we resort to
reward models to capture human preferences in complex and nuanced scenarios. We build
upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and 
training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the
assessment emphasizes the utility and relevance of the response to the user while minimizing
interference with the underlying reasoning process. For harmlessness, we evaluate the entire
response of the model, including both the reasoning process and the summary, to identify and
mitigate any potential risks, biases, or harmful content that may arise during the generation
process. Ultimately, the integration of reward signals and diverse data distributions enables us
to train a model that excels in reasoning while prioritizing helpfulness and harmlessness.

==== 2.4. Distillation: Empower Small Models with Reasoning Capability

To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly
fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using
the 800k samples curated with DeepSeek-R1, as detailed in §2.3.3. Our findings indicate that
this straightforward distillation method significantly enhances the reasoning abilities of smaller
models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, 
Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its
reasoning capability is slightly better than that of Llama-3.1.

For distilled models, we apply only SFT and do not include an RL stage, even though
incorporating RL could substantially boost model performance. Our primary goal here is to
demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL
stage to the broader research community.
 
=== 3. Experiment

*Benchmarks* We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema
et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al.,
2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al.,
2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI,
2024d), Aider ^1^ , LiveCodeBench (Jain et al., 2024) (2024-08 – 2025-01), Codeforces 2 , Chinese
National High School Mathematics Olympiad (CNMO 2024) 3 , and American Invitational Mathematics 
Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we
also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we
adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li
et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we
only feed the final summary to evaluation to avoid the length bias. For distilled models, we
report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and
LiveCodeBench.

[[R1_11]]


*Evaluation Prompts* Following the setup in DeepSeek-V3, standard benchmarks such as
MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-
evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a
zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts
are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot
may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation
protocols with default prompts provided by their creators. For code and math benchmarks, the
HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++,
C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated
using CoT format, with data collected between August 2024 and January 2025. The Codeforces
dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases,
after which the expected ratings and percentages of competitors are calculated. SWE-Bench
verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related
benchmarks are measured using a "diff" format. DeepSeek-R1 outputs are capped at a maximum
of 32,768 tokens for each benchmark.

*Baselines* We conduct comprehensive evaluations against several strong baselines, including
DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.
Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor-
mance based on official reports. For distilled models, we also compare the open-source model
QwQ-32B-Preview (Qwen, 2024a).

*Evaluation Setup* We set the maximum generation length to 32,768 tokens for the models.
We found that using greedy decoding to evaluate long-output reasoning models results in
higher repetition rates and significant variability across different checkpoints. Therefore, we
default to pass@ 𝑙 evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature.
Specifically, we use a sampling temperature of 0.6 and a top- 𝑞 value of 0.95 to generate 𝑙
responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1
is then calculated as

[stem]
++++++
pass@1 = \frac{1}{k} \sum_{i=1}^k p_i ,
++++++

where stem:[p_i] denotes the correctness of the i-th response. This method provides more reliable
performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang
et al., 2022) using 64 samples, denoted as cons@64.

1 https://aider.chat
2 https://codeforces.com
3 https://www.cms.org.cn/Home/comp/comp/cid/12.html

[[R1_12]]


==== 3.1. DeepSeek-R1 Evaluation

[cols=8]
|=======
2.1+|Benchmark (Metric)         |Claude-3.5-Sonnet-1022 |GPT-4o 0513    |DeepSeek V3 
                                |OpenAI o1-mini         |OpenAI o1-1217 |DeepSeek R1

2.1+|Architecture               |-       |-     |MoE     |-     |-      |MoE
2.1+|# Activated Params         |-       |-     |37B     |-     |-      |37B
2.1+|# Total Params             |-       |-     |671B    |-     |-      |671B
1.10+|English
    |MMLU (Pass@1)              |88.3    |87.2  |88.5    |85.2  |*91.8* |90.8
    |MMLU-Redux (EM)            |88.9    |88.0  |89.1    |86.7  |-      |*92.9*
    |MMLU-Pro (EM)              |78.0    |72.6  |75.9    |80.3  |-      |*84.0*
    |DROP (3-shot F1)           |88.3    |83.7  |91.6    |83.9  |90.2   |*92.2*
    |IF-Eval (Prompt Strict)    |86.5    |84.3  |86.1    |84.8  |-      |83.3
    |GPQA Diamond (Pass@1)      |*65.0*  |49.9  |59.1    |60.0  |*75.7* |71.5
    |SimpleQA (Correct)         |28.4    |38.2  |24.9    |7.0   |*47.0* |30.1
    |FRAMES (Acc.)              |72.5    |80.5  |73.3    |76.9  |-      |*82.5*
    |AlpacaEval2.0 (LC-winrate) |52.0    |51.1  |70.0    |57.8  |-      |*87.6*
    |ArenaHard (GPT-4-1106)     |85.2    |80.4  |85.5    |92.0  |-      |*92.3*
1.5+|Code
    |LiveCodeBench (Pass@1-COT) |38.9    |32.9  |36.2    |53.8  |63.4   |*65.9*
    |Codeforces (Percentile)    |20.3    |23.6  |58.7    |93.4  |*96.6* |96.3
    |Codeforces (Rating)        |717     |759   |1134    |1820  |*2061* |2029
    |SWE Verified (Resolved)    |*50.8*  |38.8  |42.0    |41.6  |48.9   |49.2
    |Aider-Polyglot (Acc.)      |45.3    |16.0  |49.6    |32.9  |*61.7* |53.3
1.3+|Math
    |AIME 2024 (Pass@1)         |16.0    |9.3   |39.2    |63.6  |79.2   |*79.8*
    |MATH-500 (Pass@1)          |78.3    |74.6  |90.2    |90.0  |96.4   |*97.3*
    |CNMO 2024 (Pass@1)         |13.1    |10.8  |43.2    |67.6  |-      |*78.8*
1.3+|Chinese
    |CLUEWSC (EM)               |85.4    |87.9  |90.9    |89.9  |-      |*92.8*
    |C-Eval (EM)                |76.7    |76.0  |86.5    |68.9  |-      |*91.8*
    |C-SimpleQA (Correct)       |55.4    |58.7  |*68.0*  |40.3  |-      |63.7
|=======

Table 4 | Comparison between DeepSeek-R1 and other representative models.

For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, 
DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement 
is primarily attributed to enhanced accuracy in STEM-related questions, where significant 
gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1
excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis
capabilities. This highlights the potential of reasoning models in AI-driven search and data
analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,
demonstrating its capability in handling fact-based queries. A similar trend is observed where
OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than
DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse
answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an
accuracy of over 70%.

DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a
model’s ability to follow format instructions. These improvements can be linked to the inclusion
of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL
training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,
indicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its
significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale
RL, which not only boosts reasoning capabilities but also improves performance across diverse
domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an
average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that
DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying
its robustness across multiple tasks.

[[R1_13]]

On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,
surpassing other models by a large margin. A similar trend is observed on coding algorithm
tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these
benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1
on Aider but achieves comparable performance on SWE Verified. We believe the engineering
performance of DeepSeek-R1 will improve in the next version, as the amount of related RL
training data currently remains very limited.

==== 3.2. Distilled Model Evaluation

|======
1.2+|Model  2.1+|AIME 2024  |MATH-500 |GPQA Diamond |LiveCode Bench |CodeForces 
                            |pass@1 |cons@64 |pass@1 |pass@1 |pass@1 |rating 

|GPT-4o-0513                    |9.3    |13.4   |74.6   |49.9   |32.9   |759
|Claude-3.5-Sonnet-1022         |16.0   |26.7   |78.3   |65.0   |38.9   |717
|OpenAI-o1-mini                 |63.6   |80.0   |90.0   |60.0   |53.8   |1820
|QwQ-32B-Preview                |50.0   |60.0   |90.6   |54.5   |41.9   |1316
|DeepSeek-R1-Distill-Qwen-1.5B  |28.9   |52.7   |83.9   |33.8   |16.9   |954
|DeepSeek-R1-Distill-Qwen-7B    |55.5   |83.3   |92.8   |49.1   |37.6   |1189
|DeepSeek-R1-Distill-Qwen-14B   |69.7   |80.0   |93.9   |59.1   |53.1   |1481
|DeepSeek-R1-Distill-Qwen-32B   |72.6   |83.3   |94.3   |62.1   |57.2   |1691
|DeepSeek-R1-Distill-Llama-8B   |50.4   |80.0   |89.1   |49.0   |39.6   |1205
|DeepSeek-R1-Distill-Llama-70B  |70.0   |86.7   |94.5   |65.2   |57.5   |1633
|======

Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on
reasoning-related benchmarks.

As shown in Table 5, simply distilling DeepSeek-R1’s outputs enables the efficient DeepSeek-R1-7B 
(i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform 
non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview 
on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly
exceed o1-mini on most benchmarks. These results demonstrate the strong potential of 
distillation. Additionally, we found that applying RL to these distilled models yields significant further
gains. We believe this warrants further exploration and therefore present only the results of the
simple SFT-distilled models here.
 
=== 4. Discussion


==== 4.1. Distillation v.s. Reinforcement Learning

In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive
results. However, there is still one question left: can the model achieve comparable performance
through the large-scale RL training discussed in the paper without distillation?

To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math,
code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The
experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale
RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, 
which is distilled from DeepSeek-R1, performs significantly better than
DeepSeek-R1-Zero-Qwen-32B across all benchmarks.

[[R1_14]]

|=======
1.2+|Model   2.1+|AIME 2024  |MATH-500  |GPQA Diamond  |LiveCodeBench
                               |pass@1 |cons@64 |pass@1 |pass@1 |pass@1

|QwQ-32B-Preview               |50.0  |60.0  |90.6  |54.5  |41.9
|DeepSeek-R1-Zero-Qwen-32B     |47.0  |60.0  |91.6  |55.0  |40.2
|DeepSeek-R1-Distill-Qwen-32B  |72.6  |83.3  |94.3  |62.1  |57.2
|=======

Table 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks.


Therefore, we can draw two conclusions: First, distilling more powerful models into smaller
ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in
this paper require enormous computational power and may not even achieve the performance
of distillation. Second, while distillation strategies are both economical and effective, advancing
beyond the boundaries of intelligence may still require more powerful base models and 
larger-scale reinforcement learning.

==== 4.2. Unsuccessful Attempts

In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along
the way. We share our failure experiences here to provide insights, but this does not imply that
these approaches are incapable of developing effective reasoning models.

*Process Reward Model (PRM)* PRM is a reasonable method to guide the model toward better
approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,
2023). However, in practice, PRM has three main limitations that may hinder its ultimate 
success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second,
determining whether the current intermediate step is correct is a challenging task. Automated
annotation using models may not yield satisfactory results, while manual annotation is not 
conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward
hacking (Gao et al., 2022), and retraining the reward model needs additional training resources
and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good
ability to rerank the top-N responses generated by the model or assist in guided search (Snell
et al., 2024), its advantages are limited compared to the additional computational overhead it
introduces during the large-scale reinforcement learning process in our experiments.

*Monte Carlo Tree Search (MCTS)* Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero 
(Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time
compute scalability. This approach involves breaking answers into smaller parts to allow the
model to explore the solution space systematically. To facilitate this, we prompt the model to
generate multiple tags that correspond to specific reasoning steps necessary for the search. For
training, we first use collected prompts to find answers via MCTS guided by a pre-trained value
model. Subsequently, we use the resulting question-answer pairs to train both the actor model
and the value model, iteratively refining the process.


[[R1_15]]

However, this approach encounters several challenges when scaling up the training. First,
unlike chess, where the search space is relatively well-defined, token generation presents an
exponentially larger search space. To address this, we set a maximum extension limit for each
node, but this can lead to the model getting stuck in local optima. Second, the value model
directly influences the quality of generation since it guides each step of the search process.
Training a fine-grained value model is inherently difficult, which makes it challenging for the
model to iteratively improve. While AlphaGo’s core success relied on training a value model to
progressively enhance its performance, this principle proves difficult to replicate in our setup
due to the complexities of token generation.

In conclusion, while MCTS can improve performance during inference when paired with a
pre-trained value model, iteratively boosting model performance through self-search remains a
significant challenge.

=== 5. Conclusion, Limitations, and Future Work

In this work, we share our journey in enhancing model reasoning abilities through reinforcement
learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start
data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,
leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves
performance comparable to OpenAI-o1-1217 on a range of tasks.

We further explore distillation the reasoning capability to small dense models. We use
DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small
dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o
and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other
dense models also achieve impressive results, significantly outperforming other instruction-tuned 
models based on the same underlying checkpoints.

In the future, we plan to invest in research across the following directions for DeepSeek-R1.

•   *General Capability*: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3
    in tasks such as function calling, multi-turn, complex role-playing, and JSON output.
    Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in
    these fields.

•   *Language Mixing*: DeepSeek-R1 is currently optimized for Chinese and English, which
    may result in language mixing issues when handling queries in other languages. For
    instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is
    in a language other than English or Chinese. We aim to address this limitation in future
    updates.

•   *Prompting Engineering*: When evaluating DeepSeek-R1, we observe that it is sensitive
    to prompts. Few-shot prompting consistently degrades its performance. Therefore, we
    recommend users directly describe the problem and specify the output format using a
    zero-shot setting for optimal results.

•   *Software Engineering Tasks*: Due to the long evaluation times, which impact the 
    efficiency of the RL process, large-scale RL has not been applied extensively in software
    engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement
    over DeepSeek-V3 on software engineering benchmarks. Future versions will address
    this by implementing rejection sampling on software engineering data or incorporating
    asynchronous evaluations during the RL process to improve efficiency.

[[R1_16]]

=== References

AI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md .

Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3-5-sonnet .

{empty}M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,
N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,
B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,
F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,
A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and
W. Zaremba. Evaluating large language models trained on code. CoRR , abs/2107.03374, 2021.
URL https://arxiv.org/abs/2107.03374 .

{empty}A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten,
{empty}A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.

{empty}Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple
way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.

{empty}X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like
tree-search can guide large language model decoding and training, 2024. URL https://arxiv.org/abs/2309.17179 .

{empty}L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL
https://arxiv.org/abs/2210.10760 .

{empty}A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao,
X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and
P. Minervini. Are we done with mmlu? CoRR , abs/2406.04127, 2024. URL https://doi.org/10.48550/arXiv.2406.04127 .

Google. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno
logy/ai/google-gemini-next-generation-model-february-2024 .

{empty}Y. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. 
Chinese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint
arXiv:2411.07140, 2024.

{empty}D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring
massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.

{empty}Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A
multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint
arXiv:2305.08322, 2023.

{empty}N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica.
Livecodebench: Holistic and contamination free evaluation of large language models for code.
CoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974 .

[[R1_17]]

{empty}S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui.
Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR ,
abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485
50/arXiv.2409.12941 .

{empty}A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop,
R. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv
preprint arXiv:2409.12917, 2024.

{empty}H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur-
ing massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212 ,
2023.

{empty}T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From
crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv
preprint arXiv:2406.11939, 2024.

{empty}H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,
I. Sutskever, and K. Cobbe. Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023.

{empty}B. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL
https://github.com/WildEval/ZeroEval .

MAA. American invitational mathematics examination - aime. In American Invitational
Mathematics Examination - AIME 2024 , February 2024. URL https://maa.org/math-competitions/american-invitational-mathematics-examination-aime .

OpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/ .

OpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learning-to-reason-with-llms/ .

OpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing-simpleqa/ .

OpenAI. Introducing SWE-bench verified we’re releasing a human-validated subset of 
swebench that more, 2024d. URL https://openai.com/index/introducing-swe-bench-verified/ .

Qwen. Qwq: Reflect deeply on the boundaries of the unknown, 2024a. URL https://qwenlm.github.io/blog/qwq-32b-preview/ .

Qwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/blog/qwen2.5 .

{empty}D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman.
GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022 , 2023.

{empty}Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath:
Pushing the limits of mathematical reasoning in open language models. arXiv preprint
arXiv:2402.03300, 2024.

{empty}D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,
D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and
shogi by self-play with a general reinforcement learning algorithm. CoRR , abs/1712.01815,
2017a. URL http://arxiv.org/abs/1712.01815 .

[[R1_18]]

{empty}D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,
M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and
D. Hassabis. Mastering the game of go without human knowledge. Nat. , 550(7676):354–359,
2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270 .

{empty}C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more
effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.03314 .

{empty}T. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human
demonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5.

{empty}J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and
I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv
preprint arXiv:2211.14275, 2022.

{empty}P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A label-
free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935 ,
2023.

{empty}X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou.
Self-consistency improves chain of thought reasoning in language models. arXiv preprint
arXiv:2203.11171, 2022.

{empty}Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li,
M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and
challenging multi-task language understanding benchmark. CoRR , abs/2406.01574, 2024.
URL https://doi.org/10.48550/arXiv.2406.01574 .

{empty}C. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software
engineering agents. arXiv preprint, 2024.

{empty}H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao,
Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing
proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL
https://arxiv.org/abs/2408.08152 .

{empty}J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following
evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.

[[R1_19]]

=== Appendix

==== A. Contributions and Acknowledgments

*Core Contributors*

Daya Guo            Dejian Yang         Haowei Zhang        Junxiao Song
Ruoyu Zhang         Runxin Xu           Qihao Zhu           Shirong Ma
Peiyi Wang          Xiao Bi             Xiaokang Zhang      Xingkai Yu
Yu Wu               Z.F. Wu             Zhibin Gou          Zhihong Shao
Zhuoshu Li          Ziyi Gao

*Contributors*

Aixin Liu           Bing Xue            Bingxuan Wang       Bochao Wu
Bei Feng            Chengda Lu          Chenggang Zhao      Chengqi Deng
Chong Ruan          Damai Dai           Deli Chen           Dongjie Ji
Erhang Li           Fangyun Lin         Fucong Dai          Fuli Luo*
Guangbo Hao         Guanting Chen       Guowei Li           Kai Hu*
H. Zhang            Hanwei Xu           Honghui Ding        Huazuo Gao
Hui Qu              Hui Li              Jianzhong Guo       Jiashi Li
Jingchang Chen      Jingyang Yuan       Jinhao Tu           Junjie Qiu
Junlong Li          J.L. Cai            Jiaqi Ni            Jian Liang
Jin Chen            Kai Dong            Kaichao You         Kaige Gao
Kang Guan           Kexin Huang         Kuai Yu             Lean Wang
Lecong Zhang        Liang Zhao          Litong Wang         Liyue Zhang
Lei Xu              Leyi Xia            Mingchuan Zhang     Minghua Zhang
Minghui Tang        Mingxu Zhou         Meng Li             Miaojun Wang
Mingming Li         Ning Tian           Panpan Huang        Peng Zhang
Qiancheng Wang      Qinyu Chen          Qiushi Du           Ruiqi Ge*
Ruisong Zhang       Ruizhe Pan          Runji Wang          R.J. Chen
R.L. Jin            Ruyi Chen           Shanghao Lu         Shangyan Zhou
Shanhuang Chen      Shengfeng Ye        Shiyu Wang          Shuiping Yu
Shunfeng Zhou       Shuting Pan         S.S. Li             Shuang Zhou
Shaoqing Wu         Shengfeng Ye        Tao Yun             Tian Pei
Tianyu Sun          T. Wang             Wangding Zeng       Wen Liu
Wenfeng Liang       Wenjun Gao          Wenqin Yu*          Wentao Zhang
W.L. Xiao           Wei An              Xiaodong Liu        Xiaohan Wang
Xiaokang Chen       Xiaotao Nie         Xin Cheng           Xin Liu
Xin Xie             Xingchao Liu        Xinyu Yang          Xinyuan Li
Xuecheng Su         Xuheng Lin          X.Q. Li             Xiangyue Jin
Xiaojin Shen        Xiaosha Chen        Xiaowen Sun         Xiaoxiang Wang
Xinnan Song         Xinyi Zhou          Xianzu Wang         Xinxia Shan
Y.K. Li             Y.Q. Wang           Y.X. Wei            Yang Zhang
Yanhong Xu          Yao Li              Yao Zhao            Yaofeng Sun
Yaohui Wang         Yi Yu               Yichao Zhang        Yifan Shi
Yiliang Xiong       Ying He             Yishi Piao          Yisong Wang
Yixuan Tan          Yiyang Ma*          Yiyuan Liu          Yongqiang Guo
Yuan Ou             Yuduan Wang         Yue Gong            Yuheng Zou
Yujia He            Yunfan Xiong        Yuxiang Luo         Yuxiang You
Yuxuan Liu          Yuyang Zhou         Y.X. Zhu            Yanping Huang
Yaohui Li           Yi Zheng            Yuchen Zhu          Yunxian Ma
Ying Tang           Yukun Zha           Yuting Yan          Z.Z. Ren
Zehui Ren           Zhangli Sha         Zhe Fu              Zhean Xu
Zhenda Xie          Zhengyan Zhang      Zhewen Hao          Zhicheng Ma
Zhigang Yan         Zhiyu Wu            Zihui Gu            Zijia Zhu
Zijun Liu*          Zilin Li            Ziwei Xie           Ziyang Song
Zizheng Pan         Zhen Huang          Zhipeng Xu          Zhongyu Zhang
Zhen Zhang

[[R1_20]]


[[R1_21]]

Within each role, authors are listed alphabetically by the first name. Names marked with *
denote individuals who have departed from our team.

[[R1_22]]



== DeepSeek-V3 Technical Report

https://vscode.dev/github/huggingface/transformers +
https://www.bilibili.com/video/BV1BwFQeYEir[DeepSeek V3 技术报告解读] +
https://vscode.dev/github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf


== DeepSeek API Docs

link:https://api-docs.deepseek.com[Your First API Call]
官方文档提供了 bash、Python、JavaScript (Nodejs) 的脚本示范。请求与响应数据都使用 JSON
格式，并且符合 https://github.com/openai/openai-node[OpenAI] 构架规范。可以根据其
`ChatCompletion` 以及 `ChatCompletionMessage` 等等类型定义来使用返回的数据集。

DeepSeek R1 官方服务提供了 “DeepThink (R1)” 和 “Search” 联网信息查询选项，选项对应的
请求数据包参考以下：

[source,json]
-------------
{
    "chat_session_id": "ae81ba38-2893-42a7-b34b-d8ca5a157c89",
    "parent_message_id": 284,
    "prompt": "规训",
    "ref_file_ids": [],
    "thinking_enabled": true,
    "search_enabled": false
}
-------------

官方服务默认是开启流式数据响应的，服务器这样就可以处理多数据就输出多少信息到客户端。
需要客户端进行流式数据处理，将流数据中包含的 AI 输出拼合到一起供用户阅读。

== DeepSeek-R1 Now Live With NVIDIA NIM

* https://build.nvidia.com/deepseek-ai/deepseek-r1[DeepSeek-R1 Now Live With NVIDIA NIM]
* https://docs.api.nvidia.com/nim/reference/deepseek-ai-deepseek-r1[DeepSeek-R1 Model Overview]
* https://developer.nvidia.com/cuda-gpus[Nvidia CUDA GPU Compute Capability]
* https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html[CUDA C++ Programming Guide]

DeepSeek R1 发布时，微软 OpenAI 质疑其“偷窃数据”，一边又抓紧整合 DeepSeek R1，生怕落后
英伟达（NVIDIA）、亚马逊等科技巨头的脚步，现在各大平台相继宣布，整合 DeepSeek-R1 大语言模型。

DeepSeek - R1 是个超厉害的大语言模型，推理、数学、编码样样精通。它最让人惊讶的就是成本超低。
训练的时候，只用 2048 块符合美国出口管制版本的英伟达 H800 GPU，花了 55 天，才花了 557.6
万美元，成本只有同类模型的 1/20，推理成本是 OpenAI 模型的 1/30。而且它还是开源的，就像
“AI 界的拼多多”，性价比超高。从技术上讲，它用了混合专家模型（MoE）、多头潜注意力（MLA）和
双重流水线（DualPipe）等技术优化，计算效率大大提高。

据外媒扒出，V3 模型的开发甚至绕过了 CUDA，通过对英伟达 GPU 低级汇编语言 PTX 进行优化，
从而实现了最大性能。PTX 是英伟达 GPU 的中间指令集架构，能够实现寄存器分配、线程/线程束
级别调整等细粒度优化。如果说 CUDA 是与英伟达 GPU 对话的「高级语言」，那么 PTX 就像是一种
「机器底层语言」。

DeepSeek 在训练 V3 模型时，对 H800 GPU 进行了重新配置，包括划分出 20 个 SM 用于服务器
间通信，以及实现了流水线算法，优化能力远超常规 CUDA 开发水平。而倘若这一技术属实，或将撼动
英伟达构筑已久的护城河。

* https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compilation-with-nvcc[CUDA C++ Programming Guide - 3.1. Compilation with NVCC]:
* https://docs.nvidia.com/cuda/parallel-thread-execution/index.html[The programming guide to using PTX (Parallel Thread Execution) and ISA (Instruction Set Architecture)]
_____
Kernels can be written using the CUDA instruction set architecture,
called _PTX_, which is described in the PTX reference manual. It is
however usually more effective to use a high-level programming language
such as C++. In both cases, kernels must be compiled into binary code by
`+nvcc+` to execute on the device.
_____

NVIDIA NIM 是 NVIDIA AI Enterprise 的一部分，是一套易于使用的预构建容器工具，可在任何
云或数据中心上加速部署基础模型，并帮助您确保数据安全。

*Start Prototyping for Free*
Get started with easy-to-use, NVIDIA-managed serverless APIs.

* Access fully accelerated AI infrastructure.
* Ensure your data isn't used for model training.
* Get started for free with 1,000 inference credits.

[source,python]
---------------
#!/usr/bin/env python

from openai import OpenAI

client = OpenAI(
  base_url = "https://integrate.api.nvidia.com/v1",
  api_key = "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"
)

completion = client.chat.completions.create(
  model="deepseek-ai/deepseek-r1",
  messages=[{"role":"user","content":"Which number is larger, 9.11 or 9.8?"}],
  temperature=0.6,
  top_p=0.7,
  max_tokens=4096,
  stream=True
)

for chunk in completion:
  if chunk.choices[0].delta.content is not None:
    print(chunk.choices[0].delta.content, end="")
---------------


== DeepSeek R1 Integration

以下是以 Markdown 格式提供的 C++ 接入 DeepSeek-R1 API 的代码示例，由 DeepSeek R1 生成。

C++ 接入 DeepSeek-R1 API 代码示例

1. 依赖库

*  HTTP 请求：使用 [cpr（基于 libcurl)](https://github.com/libcpr/cpr)
*  JSON 解析：使用 [nlohmann/json](https://github.com/nlohmann/json)

2. 代码实现

```cpp
#include <iostream>
#include <string>
#include <cpr/cpr.h>
#include <nlohmann/json.hpp>

using json = nlohmann::json;

// API 配置（需替换为实际值）
const std::string API_KEY = "...";
const std::string API_URL = "https://api.deepseek.com/v1/chat/completions";
const std::string MODEL_NAME = "deepseek-r1";

// 调用 API 生成回复
std::string generate_response(const std::string& prompt) {
    // 构造请求头
    cpr::Header headers = {
        {"Content-Type", "application/json"},
        {"Authorization", "Bearer " + API_KEY}
    };

    // 构造请求体
    json request_body = {
        {"model", MODEL_NAME},
        {"messages", {
            {{"role", "user"}, {"content", prompt}}
        }},
        {"temperature", 0.7},    // 控制生成随机性（0~1）
        {"max_tokens", 1000}     // 限制生成文本长度
    };

    // 发送 POST 请求
    cpr::Response response = cpr::Post(
        cpr::Url{API_URL},
        headers,
        cpr::Body{request_body.dump()},
        cpr::Timeout{5000}       // 5 秒超时
    );

    // 处理响应
    if (response.status_code == 200) {
        json response_json = json::parse(response.text);
        return response_json["choices"][0]["message"]["content"].get<std::string>();
    } else {
        throw std::runtime_error("API 请求失败: " + response.text);
    }
}

int main() {
    try {
        std::string prompt = "请解释量子计算的基本原理";
        std::string response = generate_response(prompt);
        std::cout << "DeepSeek-R1 回复:\n" << response << std::endl;
    } catch (const std::exception& e) {
        std::cerr << "错误: " << e.what() << std::endl;
        return 1;
    }
    return 0;
}
```

3. 环境配置（Linux/Ubuntu）

安装依赖

```bash
# 安装基础工具链
sudo apt-get update && sudo apt-get install -y build-essential cmake libcurl4-openssl-dev

# 编译安装 cpr
git clone https://github.com/libcpr/cpr.git
cd cpr && mkdir build && cd build
cmake .. -DCPR_USE_SYSTEM_CURL=ON && make && sudo make install

# 安装 nlohmann/json（通过 vcpkg）
git clone https://github.com/microsoft/vcpkg.git
./vcpkg/bootstrap-vcpkg.sh
./vcpkg/vcpkg install nlohmann-json
```

4. 编译配置（CMake）

```cmake
cmake_minimum_required(VERSION 3.10)
project(DeepSeekAPI)

set(CMAKE_CXX_STANDARD 17)

# 查找依赖库
find_package(CURL REQUIRED)
find_package(cpr REQUIRED)
find_package(nlohmann_json REQUIRED)

# 生成可执行文件
add_executable(deepseek_api main.cpp)
target_link_libraries(deepseek_api PRIVATE 
    cpr::cpr 
    CURL::libcurl 
    nlohmann_json::nlohmann_json
)
```

手动引入依赖库注意，CMake find_packae 指令会查找依赖包中的 XXXConfig.cmake 配置文件，
像 nlohmann/json 库，下载源代码后，需要先执行其构建脚本生成相应的文件。
[CMake + nlohmann/json 配置](https://json.nlohmann.me/integration/cmake/)
根据下载到依赖所在路径，设置用于查找依赖库的安装路径前缀 CMAKE_PREFIX_PATH。
使用 find_package 指令时，CMake 会在 CMAKE_PREFIX_PATH 指定的路径下寻找对应的
Config.cmake 或 FindXXX.cmake 文件。

CMAKE_MODULE_PATH 用于设置查找 CMake 模块的路径，直接添加查找模块的路径。


5. 关键说明

替换配置：

*   将 API_KEY 替换为您的实际 API 密钥
*   根据 DeepSeek 文档 调整 API_URL 和请求参数

扩展功能：

*   添加 代理支持：

```cpp
cpr::Proxies proxies{{"http", "http://your-proxy:port"}};
cpr::Post(..., proxies);
```

*   增加 异步请求（使用 cpr::Async）

错误处理：

*   捕获 json::parse_error 处理 JSON 解析异常
*   添加重试逻辑应对网络波动

6. 注意事项

确保 API 密钥的 保密性，不要硬编码在代码中（建议使用环境变量）
生产环境建议添加 速率限制 和 缓存机制

查看官方文档确认 API 的 计费规则 和 调用限制


== Your First API Call

The DeepSeek API uses an API format compatible with OpenAI. By modifying
the configuration, you can use the OpenAI SDK or softwares compatible
with the OpenAI API to access the DeepSeek API.

[cols=",",options="header,autowidth",]
|===
|PARAM          |VALUE

|base_url ^*^   |`+https://api.deepseek.com+`
|api_key        |apply for an https://platform.deepseek.com/api_keys[API key]
|===

* To be compatible with OpenAI, you can also use
`+https://api.deepseek.com/v1+` as the `+base_url+`. But note that the
`+v1+` here has NO relationship with the model's version.

* *The `+deepseek-chat+` model has been upgraded to DeepSeek-V3. The API
remains unchanged.* You can invoke DeepSeek-V3 by specifying
`+model='deepseek-chat'+`.

* *`+deepseek-reasoner+` is the latest
link:https://api-docs.deepseek.com/guides/reasoning_model[reasoning model], 
DeepSeek-R1, released by DeepSeek.* You can invoke DeepSeek-R1 by specifying
`+model='deepseek-reasoner'+`.

=== Invoke The Chat API

Once you have obtained an API key, you can access the DeepSeek API using
the following example scripts. This is a non-stream example, you can set
the `+stream+` parameter to `+true+` to get stream response.

* curl
* python
* nodejs

[source,bash]
----
curl https://api.deepseek.com/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer <DeepSeek API Key>" \
  -d '{
        "model": "deepseek-chat",
        "messages": [
          {"role": "system", "content": "You are a helpful assistant."},
          {"role": "user", "content": "Hello!"}
        ],
        "stream": false
      }'
----

[source,python]
----
# Please install OpenAI SDK first: `pip3 install openai`

from openai import OpenAI

client = OpenAI(api_key="<DeepSeek API Key>", base_url="https://api.deepseek.com")

response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "Hello"},
    ],
    stream=False
)

print(response.choices[0].message.content)
----

[source,javascript]
----
// Please install OpenAI SDK first: `npm install openai`

import OpenAI from "openai";

const openai = new OpenAI({
        baseURL: 'https://api.deepseek.com',
        apiKey: '<DeepSeek API Key>'
});

async function main() {
  const completion = await openai.chat.completions.create({
    messages: [{ role: "system", content: "You are a helpful assistant." }],
    model: "deepseek-chat",
  });

  console.log(completion.choices[0].message.content);
}

main();
----



== Models & Pricing

The prices listed below are in unites of per 1M tokens. A token, the
smallest unit of text that the model recognizes, can be a word, a
number, or even a punctuation mark. We will bill based on the total
number of input and output tokens by the model.

=== Pricing Details

* USD
* CNY

**

[width="99%",cols="16%,14%,14%,14%,14%,14%,14%",options="header",]
|===
|MODEL^(1)^ |CONTEXT LENGTH |MAX COT TOKENS^(2)^ |MAX OUTPUT TOKENS^(3)^
|1M TOKENS +
INPUT PRICE +
(CACHE HIT) ^(4)^ |1M TOKENS +
INPUT PRICE +
(CACHE MISS) |1M TOKENS +
OUTPUT PRICE

|deepseek-chat |64K |- |8K |[line-through]#$0.07#^(5)^ +
$0.014 |[line-through]#$0.27#^(5)^ +
$0.14 |[line-through]#$1.10#^(5)^ +
$0.28

|deepseek-reasoner |64K |32K |8K |$0.14 |$0.55 |$2.19 ^(6)^
|===

**

[width="99%",cols="16%,14%,14%,14%,14%,14%,14%",options="header",]
|===
|MODEL^(1)^ |CONTEXT LENGTH |MAX COT TOKENS^(2)^ |MAX OUTPUT TOKENS^(3)^
|1M TOKENS +
INPUT PRICE +
(CACHE HIT) ^(4)^ |1M TOKENS +
INPUT PRICE +
(CACHE MISS) |1M TOKENS +
OUTPUT PRICE
|deepseek-chat |64K |- |8K |[line-through]#¥0.5#^(5)^ +
¥0.1 |[line-through]#¥2#^(5)^ +
¥1 |[line-through]#¥8#^(5)^ +
¥2

|deepseek-reasoner |64K |32K |8K |¥1 |¥4 |¥16 ^(6)^
|===

* (1) *The `+deepseek-chat+` model has been upgraded to DeepSeek-V3.*
`+deepseek-reasoner+` points to the new model *DeepSeek-R1*.
* (2) *CoT (Chain of Thought)* is the reasoning content
`+deepseek-reasoner+` gives before output the final answer. For details,
please refer to link:https://api-docs.deepseek.com/guides/reasoning_model[Reasoning Model]。
* (3) If `+max_tokens+` is not specified, the default maximum output
length is 4K. Please adjust `+max_tokens+` to support longer outputs.
* (4) Please check link:/news/news0802[DeepSeek Context Caching] for the
details of Context Caching.
* (5) The form shows the the original price and the discounted price.
*From now until 2025-02-08 16:00 (UTC), all users can enjoy the
discounted prices of DeepSeek API.* After that, it will recover to full
price. *DeepSeek-R1* is not included in the discount.
* (6) The output token count of `+deepseek-reasoner+` includes all
tokens from CoT and the final answer, and they are priced equally.

=== Deduction Rules

The expense = number of tokens × price. The corresponding fees will be
directly deducted from your topped-up balance or granted balance, with a
preference for using the granted balance first when both balances are
available.

Product prices may vary and DeepSeek reserves the right to adjust them.
We recommend topping up based on your actual usage and regularly
checking this page for the most recent pricing information.

== Token & Token Usage

Tokens are the basic units used by models to represent natural language
text, and also the units we use for billing. They can be intuitively
understood as 'characters' or 'words'. Typically, a Chinese word, an
English word, a number, or a symbol is counted as a token.

Generally, the conversion ratio between tokens in the model and the
number of characters is approximately as following:

* 1 English character ≈ 0.3 token.
* 1 Chinese character ≈ 0.6 token.

However, due to the different tokenization methods used by different
models, the conversion ratios can vary. The actual number of tokens
processed each time is based on the model's return, which you can view
from the usage results.

=== Calculate token usage offline

You can run the demo tokenizer code in the following zip package to
calculate the token usage for your intput/output.

https://cdn.deepseek.com/api-docs/deepseek_v3_tokenizer.zip[deepseek_v3_tokenizer.zip]