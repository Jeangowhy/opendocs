   Formal Languages and Compilation
================================

   ![Formal Languages and Compilation (Texts in Computer Science) 3rd ed. 2019 Edition](https://m.media-amazon.com/images/I/61EldX6zgzL._SL1246_.jpg)

      ┌─────────────────────────────────────────────────┐
      │                                                 │
      │  Texts in Computer Science                      │
      │                                                 │
      │                               001               │
      │                               0010100           │
      │     010100 0001000  01110000  0010001000        │
      │              001000001111101000001001001000     │
      │             000 0  00  000 10 0100101000001101  │
      │        10 0 100 000001000 0 000100010100000     │
      │                   1 010 0 0   0010000010        │
      │                               0010000           │
      │                               010               │
      │                                                 │
      │  Stefano Crespi Reghizzi                        │
      │  Luca Breveglieri                               │
      │  Angelo Morzenti                                │
      │                                                 │
      │                                                 │
      │  Formal                                         │
      │  Languages                                      │
      │  and Compilation                                │
      │                                                 │
      │  Third Edition                                  │
      │                                                 │
      │                              '''''              │
      │                            '''    ''            │
      │                          '''        '           │
      │                         ''     '    ''          │
      │                         '   ''''     '          │
      │                        '''''  '      '          │
      │                              '      '           │
      │                             '       '           │
      │                            ''''''''''  Springer │
      │                            ''''''''''           │
      └─────────────────────────────────────────────────┘


   Texts in Computer Science

   Series Editors

   David Gries, Department of Computer Science, Cornell University, Ithaca, NY, USA

   Orit Hazzan, Faculty of Education in Technology and Science, Technion—Israel
   Institute of Technology, Haifa, Israel

   More information about this series at http://www.springer.com/series/3191

   Stefano Crespi Reghizzi •
   Luca Breveglieri • Angelo Morzenti

   Formal Languages
   and Compilation
   Third Edition

   Springer

      Stefano Crespi Reghizzi
      Dipartimento di Elettronica, Informazione e
      Bioingegneria
      Politecnico di Milano
      Milan, Italy

      Luca Breveglieri
      Dipartimento di Elettronica, Informazione e
      Bioingegneria
      Politecnico di Milano
      Milan, Italy

      Angelo Morzenti
      Dipartimento di Elettronica, Informazione e
      Bioingegneria
      Politecnico di Milano
      Milan, Italy
      ISSN 1868-0941 ISSN 1868-095X (electronic)

   Texts in Computer Science

   ISBN 978-3-030-04878-5 ISBN 978-3-030-04879-2 (eBook)
   https://doi.org/10.1007/978-3-030-04879-2

   Library of Congress Control Number: 2018965427

   1 st & 2 nd editions: © Springer-Verlag London 2009, 2013
   3 rd edition: © Springer Nature Switzerland AG 2019

   This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
   of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations,
   recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission
   or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
   methodology now known or hereafter developed.

   The use of general descriptive names, registered names, trademarks, service marks, etc. in this
   publication does not imply, even in the absence of a specific statement, that such names are exempt from
   the relevant protective laws and regulations and therefore free for general use.

   The publisher, the authors and the editors are safe to assume that the advice and information in this
   book are believed to be true and accurate at the date of publication. Neither the publisher nor the
   authors or the editors give a warranty, express or implied, with respect to the material contained herein or
   for any errors or omissions that may have been made. The publisher remains neutral with regard to
   jurisdictional claims in published maps and institutional affiliations.

   This Springer imprint is published by the registered company Springer Nature Switzerland AG
   The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
=======

   This third edition updates, enriches, and revises the 2013 printing [1], without
   dulling the original structure of the book. The selection of materials and the
   presentation style reflect many years of teaching compiler courses and of doing
   research on formal language theory and formal methods, on compiler and language
   design, and to a lesser extent on natural language processing. In the turmoil of
   information technology developments, the subject of the book has kept the same
   fundamental principles since half a century, yet preserving its conceptual 
   importance and practical relevance.

   This state of affairs of a topic, which is central to computer science and 
   information processing and is based on established principles, might lead some people to
   believe that the corresponding textbooks are by now consolidated, much as the
   classical books on calculus or physics. In reality, this is not the case: there exist fine
   classical books on the mathematical aspects of language and automata theory, but,
   for what concerns application to compiling and data representation, the best books
   are sort of encyclopedias of algorithms, design methods, and practical tricks used in
   compiler design. Indeed, a compiler is a microcosm, and it features many different
   aspects ranging from algorithmic wisdom to computer hardware, system software,
   and network interfaces. As a consequence, the textbooks have grown in size and
   compete with respect to their coverage of the latest developments in programming
   languages, processor and network architectures and clever mappings from the
   former to the latter.

   To put things into order, it is better to separate such complex topics into two
   parts, that we may call basic or fundamental, and advanced or technological, which
   to some extent correspond to the two subsystems that make a compiler: the
   user-language level specific front-end and the machine-level specific back-end. The
   basic part is the subject of this book; it covers the principles and algorithms to be
   used for defining the syntax of languages and for implementing simple translators.
   It does not include: the specialized know-how needed for various classes of
   programming languages (procedural, functional, object oriented, etc.), the computer
   architecture related aspects, and the optimization methods used to improve the
   performance of both the compilation process and the executable code produced.
   In other textbooks on compilers, the bias toward practical aspects has reduced
   the attention to fundamental concepts. This has prevented their authors from taking
   advantage of the improvements and simplifications made possible by decades of
   both extensive use and theoretical polishing, to avoid the irritating variants and
   repetitions that are found in the historical fundamental papers. Moving from these
   premises, we decided to present, in a simple minimalist way, the principles and
   methods currently used in designing syntax-directed applications such as parsing
   and regular expression matching. Thus, we have substantially improved the
   standard presentation of parsing algorithms for both regular expressions and grammars,
   by unifying the concepts and notations used in various approaches, and by
   extending method coverage with a reduced definitional apparatus. An example that
   expert readers should appreciate is the unification of top-down and bottom-up
   parsing algorithms within a new rigorous yet practical framework.

   In this way, the reduced effort and space needed to cover classical concepts has
   made room for new advanced methods typically not present in similar textbooks.
   First, we have introduced in this edition a new efficient algorithm for string
   matching using regular expressions, which are increasingly important in such
   applications as Web browsing and data filtering. Second, our presentation of
   parsing algorithms focuses on the more expressive extended BNF grammars, which
   are the de facto standard in language reference manuals. Third, we present a parallel
   parsing algorithm that takes advantage of the many processing units of modern
   multi-core devices to speed up the syntax analysis of large files. At last, we mention
   the addition in this edition of the recent formal models of input-driven automata and
   languages (also known as visibly pushdown model), which suits the mark-up
   languages (such as XML and JSON) and has been developed also in other domains
   to formally verify critical systems.

   The presentation is illustrated by many small yet realistic examples, to ease the
   understanding of the theory and the transfer to application. Theoretical models of
   automata, transducers, and formal grammars are extensively used, whenever
   practical motivations exist. Algorithms are described in a pseudo-code to avoid the
   disturbing details of a programming language, yet they are straightforward to
   convert into executable procedures, and for some, we have made available their
   code on the Web.

   The book is not restricted to syntax. The study of translations, semantic
   functions (attribute grammars), and static program analysis by data flow equations
   enlarges the understanding of the compilation process and covers the essentials of a
   syntax-directed translator.

   A large collection of problems and solutions is available at the authors’ course
   site.

   This book should be welcome by those willing to teach or learn the essential
   concepts of syntax-directed compilation, without needing to rely on software tools
   and implementations. We believe that learning by doing is not always the best
   approach and that an overcommitment to practical work may sometimes obscure
   the conceptual foundations. In the case of formal languages, the elegance and
   essentiality of the underlying theory allows students to acquire the fundamental
   paradigms of language structures, to avoid pitfalls such as ambiguity, and to
   adequately map structure to meaning. In this field, many if not all relevant algorithms
   are simple enough to be practiced by paper and pencil. Of course, students should
   be encouraged to enroll in a hands-on laboratory and to experiment syntax-directed
   tools (like flex and bison) on realistic cases.

   The authors thank the colleagues Alessandro Barenghi and Marcello Bersani,
   who have taught compilers to computer engineering students by using this book.
   The first author remembers the late Antonio Grasselli, the computer science
   pioneer who first fascinated him with the subject of this book, combining linguistic,
   mathematical, and technological aspects.

   Milan, Italy
   April 2019
   Stefano Crespi Reghizzi
   Luca Breveglieri
   Angelo Morzenti

   Reference
   1. Crespi Reghizzi S, Breveglieri L, Morzenti A (2013) Formal languages and compilation, 2nd
   edn. Springer

<a id="toc"></a>

Contents
========

- [1 Introduction](#P001)
   - [1.1 Intended Scope and Readership](#P001)
   - [1.2 Compiler Parts and Corresponding Concepts](#P002)

- [2 Syntax](#P005)
   - [2.1 Introduction](#P005)
   - [2.1.1 Artificial and Formal Languages](#P005)
   - [2.1.2 Language Types](#P006)
   - [2.1.3 Chapter Outline](#P007)
   - [2.2 Formal Language Theory](#P008)
   - [2.2.1 Alphabet and Language](#P008)
   - [2.2.2 Language Operations](#P011)
   - [2.2.3 Set Operations](#P013)
   - [2.2.4 Star and Cross](#P014)
   - [2.2.5 Language Quotient](#P017)
   - [2.3 Regular Expressions and Languages](#P017)
   - [2.3.1 Definition of Regular Expression](#P018)
   - [2.3.2 Derivation and Language](#P020)
   - [2.3.3 Other Operators](#P024)
   - [2.3.4 Closure Properties of Family REG](#P025)
   - [2.4 Linguistic Abstraction: Abstract and Concrete Lists](#P026)
   - [2.4.1 Lists with Separators and Opening/Closing Marks](#P027)
   - [2.4.2 Language Substitution](#P028)
   - [2.4.3 Hierarchical or Precedence Lists](#P029)
   - [2.5 Context-Free Generative Grammars](#P031)
   - [2.5.1 Limits of Regular Languages](#P031)
   - [2.5.2 Introduction to Context-Free Grammars](#P032)
   - [2.5.3 Conventional Grammar Representations](#P034)
   - [2.5.4 Derivation and Language Generation](#P037)
   - [2.5.5 Erroneous Grammars and Useless Rules](#P039)
   - [2.5.6 Recursion and Language Infinity](#P041)
   - [2.5.7 Syntax Tree and Canonical Derivation](#P043)
   - [2.5.8 Parenthesis Languages](#P047)
   - [2.5.9 Regular Composition of Context-Free Languages](#P051)
   - [2.5.10 Ambiguity in Grammars](#P052)
   - [2.5.11 Catalogue of Ambiguous Forms and Remedies](#P054)
   - [2.5.12 Weak and Structural Equivalence](#P062)
   - [2.5.13 Grammar Transformations and Normal Forms](#P065)
   - [2.6 Grammars of Regular Languages](#P082)
   - [2.6.1 From Regular Expression to Grammar](#P082)
   - [2.6.2 Linear and Unilinear Grammars](#P084)
   - [2.6.3 Linear Language Equations](#P087)
   - [2.7 Comparison of Regular and Context-Free Languages](#P089)
   - [2.7.1 Limits of Context-Free Languages](#P093)
   - [2.7.2 Closure Properties of Families REG and CF](#P095)
   - [2.7.3 Alphabetic Transformations](#P097)
   - [2.7.4 Grammars with Regular Expressions](#P100)
   - [2.8 More General Grammars and Language Families](#P104)
   - [2.8.1 Chomsky Classification of Grammars](#P105)
   - [2.8.2 Further Developments and Final Remarks](#P109)
   - [References](#P112)

- [3 Finite Automata as Regular Language Recognizers](#P115)
   - [3.1 Introduction](#P115)
   - [3.2 Recognition Algorithms and Automata](#P116)
   - [3.2.1 A General Automaton](#P117)
   - [3.3 Introduction to Finite Automata](#P120)
   - [3.4 Deterministic Finite Automata](#P122)
   - [3.4.1 Error State and Total Automaton](#P123)
   - [3.4.2 Clean Automaton](#P124)
   - [3.4.3 Minimal Automaton](#P125)
   - [3.4.4 From Automaton to Grammar](#P129)
   - [3.5 Nondeterministic Automata](#P130)
   - [3.5.1 Motivation of Nondeterminism](#P131)
   - [3.5.2 Nondeterministic Recognizers](#P133)
   - [3.5.3 Automata with Spontaneous Moves](#P135)
   - [3.5.4 Correspondence Between Automata and Grammars](#P136)
   - [3.5.5 Ambiguity of Automata](#P137)
   - [3.5.6 Left-Linear Grammars and Automata](#P138)
   - [3.6 From Automaton to Regular Expression: BMC Method](#P139)
   - [3.7 Elimination of Nondeterminism](#P142)
   - [3.7.1 Elimination of Spontaneous Moves](#P142)
   - [3.7.2 Construction of Accessible Subsets](#P144)
   - [3.8 From Regular Expression to Recognizer](#P147)
   - [3.8.1 Thompson Structural Method](#P148)
   - [3.8.2 Berry–Sethi Method](#P150)
   - [3.9 String Matching of Ambiguous Regular Expressions](#P165)
   - [3.9.1 Trees for Ambiguous r.e and Their Phrases](#P165)
   - [3.9.2 Local Recognizer of Linearized Syntax Trees](#P172)
   - [3.9.3 The Berry–Sethi Parser](#P172)
   - [3.10 Expressions with Complement and Intersection](#P188)
   - [3.10.1 Product of Automata](#P190)
   - [3.11 Summary of the Relations Between Regular Expressions, Grammars and Automata](#P195)
   - [References](#P196)

- [4 Pushdown Automata and Parsing](#P199)
   - [4.1 Introduction](#P199)
   - [4.2 Pushdown Automaton](#P200)
   - [4.2.1 From Grammar to Pushdown Automaton](#P201)
   - [4.2.2 Definition of Pushdown Automaton](#P205)
   - [4.2.3 One Family for Context-Free Languages and Pushdown Automata](#P209)
   - [4.2.4 Intersection of Regular and Context-Free Languages](#P213)
   - [4.3 Deterministic Pushdown Automata and Languages](#P215)
   - [4.3.1 Closure Properties of Deterministic Languages](#P216)
   - [4.3.2 Nondeterministic Languages](#P218)
   - [4.3.3 Determinism and Language Unambiguity](#P220)
   - [4.3.4 Subclasses of Deterministic Pushdown Automata and Languages](#P221)
   - [4.4 Syntax Analysis: Top-Down and Bottom-Up Constructions](#P230)
   - [4.5 Grammar as Network of Finite Automata](#P233)
   - [4.5.1 Syntax Charts](#P237)
   - [4.5.2 Derivation for Machine Nets](#P239)
   - [4.5.3 Initial and Look-Ahead Characters](#P241)
   - [4.6 Bottom-Up Deterministic Analysis](#P244)
   - [4.6.1 From Finite Recognizers to Bottom-Up Parser](#P245)
   - [4.6.2 Construction of ELR(1) Parsers](#P250)
   - [4.6.3 ELR(1) Condition](#P253)
   - [4.6.4 Simplifications for BNF Grammars](#P267)
   - [4.6.5 Parser Implementation Using a Vector Stack](#P271)
   - [4.6.6 Lengthening the Look-Ahead](#P275)
   - [4.7 Deterministic Top-Down Parsing](#P282)
   - [4.7.1 ELL(1) Condition](#P286)
   - [4.7.2 Step-by-Step Derivation of ELL(1) Parsers](#P288)
   - [4.7.3 Direct Construction of Top-Down Predictive Parsers](#P305)
   - [4.7.4 A Graphical Method for Computing Guide Sets](#P314)
   - [4.7.5 Increasing Look-Ahead in Top-Down Parsers](#P322)
   - [4.8 Operator-Precedence Grammars and Parallel Parsing](#P325)
   - [4.8.1 Floyd Operator-Precedence Grammars and Parsers](#P326)
   - [4.8.2 Sequential Operator-Precedence Parser](#P330)
   - [4.8.3 Comparisons and Closure Properties](#P333)
   - [4.8.4 Parallel Parsing Algorithm](#P339)
   - [4.9 Deterministic Language Families: A Comparison](#P346)
   - [4.10 Discussion of Parsing Methods](#P350)
   - [4.11 A General Parsing Algorithm](#P352)
   - [4.11.1 Introductory Presentation](#P353)
   - [4.11.2 Earley Algorithm](#P357)
   - [4.11.3 Syntax Tree Construction](#P366)
   - [4.12 Managing Syntactic Errors and Changes](#P373)
   - [4.12.1 Errors](#P373)
   - [4.12.2 Incremental Parsing](#P379)
   - [References](#P379)

- [5 Translation Semantics and Static Analysis](#P383)
   - [5.1 Introduction](#P383)
   - [5.2 Translation Relation and Function](#P386)
   - [5.3 Transliteration](#P388)
   - [5.4 Purely Syntactic Translation](#P389)
   - [5.4.1 Infix and Polish Notation](#P391)
   - [5.4.2 Ambiguity of Source Grammar and Translation](#P395)
   - [5.4.3 Translation Grammar and Pushdown Transducer](#P397)
   - [5.4.4 Syntax Analysis with Online Translation](#P402)
   - [5.4.5 Top-Down Deterministic Translation by Recursive Procedures](#P403)
   - [5.4.6 Bottom-Up Deterministic Translation](#P406)
   - [5.5 Regular Translation](#P414)
   - [5.5.1 Two-Input Automaton](#P416)
   - [5.5.2 Translation Function and Finite Transducer](#P421)
   - [5.5.3 Closure Properties of Translation](#P426)
   - [5.6 Semantic Translation](#P427)
   - [5.6.1 Attribute Grammars](#P429)
   - [5.6.2 Left and Right Attributes](#P431)
   - [5.6.3 Definition of Attribute Grammar](#P435)
   - [5.6.4 Dependence Graph and Attribute Evaluation](#P437)
   - [5.6.5 One-Sweep Semantic Evaluation](#P442)
   - [5.6.6 Other Evaluation Methods](#P446)
   - [5.6.7 Combined Syntax and Semantic Analysis](#P447)
   - [5.6.8 Typical Applications of Attribute Grammar](#P457)
   - [5.7 Static Program Analysis](#P467)
   - [5.7.1 A Program as an Automaton](#P468)
   - [5.7.2 Liveness Intervals of a Variable](#P471)
   - [5.7.3 Reaching Definition](#P477)
   - [References](#P486)

- [Index](#P487)


Math Symbols Brief
==================

   Markdown 文档中的 LaTeX 数学公式表达式可参考 reStructuredText 用户文档
   [LaTeX syntax for mathematics](https://docutils.sourceforge.io/docs/ref/rst/mathematics.html)
   ![LaTeX syntax for mathematics](pictures/LaTeX_syntax_for_mathematics.drawio.svg)

   书中使用到的逻辑语言符号包括：

   -  “¬” 非（否定）
   -  “∧” 且（并且）
   -  “∨” 或（或者）
   -  “∀” 是全称量词，意为“对于所有”或“对每一个”。
   -  “∃” 是存在量词，意为“存在”或“至少有一个”。

   逻辑符号：∀ 是全称量词，∃ 是存在量词。∀（倒写的A）意思是：“对于所有”或“对每一个”。它用于
   描述一个命题对某一范围内的所有元素都成立。∃（反写的E）意思是：“存在”或“至少有一个”。它用于
   指出在某一范围内至少存在一个元素使得某命题成立。两个符号对应的 LaTeX 语法为 `\forall` 和
   `\exists`。

   英文中角分符号（Apostrophe）称作 prime 源于误读。20 世纪初期，x′ 读作 x prime 并不是因为
   在 x 后跟着一个 prime 符号，而因为它是 x′ (x prime)、x″ (x second) 与 x‴ (x third) 
   中的第一个，这里 prime 是“最初”的意思，并非质数的意思。1950 年代到 1960 年代，术语 prime
   开始用于撇号这一类的符号。现在 x″ 与 x‴ 在英文中通常读作 x double prime 与 x triple prime。

   此外，书中还会使用到部分离散数学、抽象代数概念。群 (group)是一种抽象代数定义的数学结构：如果在
   非空集合 G 上定义的二元运算 * （一个二元运算符）满足以下规律，则称（G，*）是群，简称 G 是群。

   1) 封闭性（Closure）：对于任意 a，b∈G，有 $ a*b∈G $
   2) 结合律（Associativity）：对于任意 a，b，c∈G，有 $ (a*b)*c = a*(b*c) $
   3) 幺元 （Identity）：存在幺元 e，使得对于任意 a∈G，$ e*a = a*e = a $
   4) 逆元：对于任意 a∈G，存在逆元 $ a^{-1} $，使得 $ a^{-1}*a = a*a^{-1} = e $

   如果仅满足封闭性和结合律，则称 G 是一个半群（Semigroup）；如果仅满足封闭性、结合律并且有幺元，
   则称 G 是一个含幺半群（Monoid）。

   数学中 `(G，*)` 这种用于表达群结构的形式符号也称为二元组，这种表达还会在编译器前端的语法扫描、
   词法分析的实现过程中用于表达相应的分析结果。 

   单位元 (Identity Element)，也叫幺元（么元），是集合中的一种特别元素，它与该集合里的其他元素
   运算时，结果依然是其它元素的值。比如，矩阵中的单位矩阵，还有自然数集中的 1。

   封闭性也就是对一个集合中的任意元素进行二元运算后的结果依然属于此集合中的元素，比如，自然数的四则
   运算结果依然是自然数，在形式语言中将此性质称为闭包。参考本书 2.2.4 Star and Cross，它们分别
   是克林闭包 (Kleene Closure) $ ∑^* $ 和正闭包 (Positive Closure) $ ∑^+ $。在编译器设计
   理论中会用到字母表（集合），并且将语言（Language）定义为由字母表元素构成的字符串集合。同时将字
   母表当作一种数据结构，以便定义各种运算，比如串连接、幂运算。当然也包含集合运算，因为字母表本身就是
   集合。

   比如计算机中的二进制串 (binary strings) 可以定义为 $ Σ = \{ 0, 1 \} $，由这个字母表构成
   的所有字串符就组成了一个“二进制语言”。


<a id="P001"></a>

Chapter 1 Introduction
======================

1.1 Intended Scope and Readership
=================================

   The information technology revolution was made possible by the invention of
   electronic digital machines, yet without programming languages their use would have
   been restricted to the few people able to write binary machine code. A programming
   language as a text contains features coming both from human languages and from
   mathematical logic. The translation from a programming language to machine code
   is known as compilation. ➊ Language compilation is a very complex process, which
   would be impossible to master without systematic design methods. Such methods
   and their theoretical foundations are the argument of this book. They make up a
   consistent and largely consolidated body of concepts and algorithms, which are applied
   not just in compilers, but also in other fields. Automata theory is pervasive in all
   the branches of informatics, to model situations or phenomena classifiable as time
   and space discrete systems. Formal grammars, on the other hand, are originated in
   linguistic research, and together with automata, they are widely applied in document
   processing and in data representation and searching, in particular for the Web.

   Coming to the prerequisites, the reader should have a good background in
   programming, though a detailed knowledge of a specific programming language is not
   required, because our presentation of algorithms relies on self-explanatory 
   pseudocode. The reader is expected to be familiar with basic mathematical theories and
   notations, namely from set theory, algebra and logic. The above prerequisites are
   typically met by computer science/engineering or mathematics students with a
   university education of two or more years.

   ➊ This term may sound strange; it originates in the early approach to the compilation of 
   correspondence tables between a command in the language and a series of machine operations.

   © Springer Nature Switzerland AG 2019

   S. Crespi Reghizzi et al., Formal Languages and Compilation,
   Texts in Computer Science, https://doi.org/10.1007/978-3-030-04879-2_1

<a id="P002"></a>

   The selection of topics and the presentation based on rigorous definitions and
   algorithms illustrated by many motivating examples qualifies the book for a university
   course, aiming to expose students to the importance of good theories and of efficient
   algorithms for designing effective systems. In our experience some fifty hours of
   lecture suffice to cover the entire book.

   The authors’ long experience in teaching the subject to different audiences brings
   out the importance of combining theoretical concepts and examples. Moreover, it
   is advisable that the students take advantage of well-known and documented
   software tools (such as the classical Flex and Bison generators of lexical and syntactic
   analyzers), to implement and experiment the main algorithm on realistic case studies.
   With regard to the reach and limits, the book covers the essential concepts and
   methods needed to design simple translators based on the very common syntax directed 
   paradigm. Such systems analyze into syntactical units a source text, written in a 
   formally specified language, and translate it into a different target representation.
   The translation function is nicely formulated as the composition of simpler functions 
   keyed to the syntactical units of the source text.

   It goes without saying that a real compiler for a programming language includes
   other technological aspects and know-how, in particular related to processor and
   computer architecture, which are not covered. Such know-how is essential for
   automatically translating a program into machine instructions and for transforming a
   program in order to make the best use of the computational resources of a
   computer. The study of methods for program transformation and optimization is a more
   advanced topic, which follows the present introduction to compiler methods. The
   next section outlines the contents of the book.

1.2 Compiler Parts and Corresponding Concepts
=============================================

   There are two external interfaces to a compiler: the source language to be analyzed
   and translated and the target language produced by the translator.

   **Chapter 2** describes the so-called syntactic methods that are universally adopted in
   order to provide a rigorous definition of the texts (or character strings) written in the
   source language. The methods to be presented are regular expressions and 
   contextfree (BNF) grammars. To avoid the pitfall of ambiguous definitions, we examine the
   relevant causes of ambiguity in the regular expressions and in the grammars. Useful
   transformations from regular expressions to grammars and between grammars of
   different forms are covered, which are needed in the following chapters. In particular
   we introduce the combination of regular expressions and grammars, which is the
   central model for the parsing algorithms. **Chapter 2** ends with a critical hint to more
   powerful, but rarely applied models: the Chomsky context-sensitive type grammars
   and the recently proposed conjunctive grammars.

<a id="P003"></a>

   The first task of a compiler is to check the correctness of the source text, that is,
   whether the text agrees with the syntactic definition of the source language. In order
   to perform such a check, the algorithm scans the source text character by character,
   and at the end it rejects or accepts the input depending on the result of the analysis.
   By a minimalist abstract approach, such recognition algorithms are conveniently
   described as mathematical machines or automata, in the tradition of the well-known
   Turing machine. Different automaton types correspond to the various families of
   formal languages, and the types practically relevant for compilation are considered
   in the following two chapters.

   **Chapter 3** covers finite automata, which are machines with a finite random access
   memory. They are the recognizers of the languages defined by regular expressions.
   Within compilation, they are used for lexical analysis or scanning, to extract from
   the source text the keywords, numbers, identifiers and in general the pieces of text
   that encode the lexical units of the language, called lexemes or tokens.

   The basic models of finite automata, deterministic and nondeterministic, real-time
   and with spontaneous moves are presented along with the transformations between
   different models. The algorithms for converting a finite automaton into a regular
   expression and conversely are well covered. The other main topics of Chap.3 are: the
   string matching problem for ambiguous regular expressions, and the use of Boolean
   operations, intersection and complement, in the regular expressions.

   **Chapter 4** is devoted to the recognition problem for the languages defined by
   context-free grammars, which is the backbone of syntax-directed translators and
   justifies the salient length of the chapter.

   The chapter starts with the model of nondeterministic pushdown automata and
   its direct correspondence to context-free grammars. The deterministic version of the
   model follows, which defines the leading family of deterministic languages. As a
   special case, the input-driven or visibly pushdown automata have been included,
   because of their relevance for markup languages such as XML and JSON. A rich
   choice of parsing algorithms is presented. First, there is a unified mdern presentation
   of the deterministic parsers, both bottom-up (or LR(1)) and top-down (or LL(1)),
   for context-free grammars extended with regular languages. The third deterministic
   parser is the operator-precedence method, which is here presented in the parallel
   version suitable for multi-core computers. The last parsing method by Earley is the
   most general and accepts any context-free language. **Chapter 4** includes comparisons
   of the different language families suitable for each parsing method, a discussion of
   grammar transformations, and the basic concepts on error handling by parsers.
   The ultimate job of a compiler is to translate a source text into another language.
   The compiler module responsible for finalizing the verification of the source language
   rules and for producing the translation is called the semantic analyzer. It operates on
   the structural representation, the syntax tree, produced by the parser.

   The formal translation models and the methods used to implement semantic
   analyzers are illustrated in Chap.5. Actually, we describe two kinds of transformations.
   First, the pure syntactic translations are modeled by transducers that are either finite
   automata or pushdown automata extended with an output, and that compute a string.
   Second, the semantic translations are performed by functions or methods that
   operate on the syntax tree of the source text and may produce as output not just a string,
   but any sort of data values. Semantic translations are specified by a practical
   semiformal extension to context-free grammars, which is called attribute grammar. This
   approach, by combining the accuracy of formal syntax and the flexibility of
   programming, conveniently expresses the typical analysis and transformation of syntax
   trees that is required by compilers.

<a id="P004"></a>

   To give a concrete idea of compilation, typical simple examples are included:
   the type consistency check between the variables declared and used in a programming 
   language, the translation of high-level statements into machine instructions and
   semantics-directed parsing.

   For sure, compilers do much more than syntax-directed translation. Static program
   analysis is an important example: it consists in examining a program to determine,
   ahead of execution, some properties, or to detect errors not detected by syntactic and
   semantic analysis. The purpose is to improve the robustness, reliability and efficiency
   of the program. An example of error detection is the identification of uninitialized
   variables used in the arithmetic expressions. For code improvement, an example is
   the elimination of useless assignment statements.

   **Chapter 5** terminates with an introduction to the static analysis of programs
   modeled by their control-flow graph, viewed as a finite automaton. Several interesting
   problems can be formalized and statically analyzed by a common approach based
   on flow equations and their solution by iterative approximations converging to the
   least fixed point.

   Last, the book comes with an up-to-date essential bibliography at the end of each
   chapter and is accompanied by a detailed index.

<a id="P005"></a>

Chapter 2 Syntax
================

2.1 Introduction
================

## 2.1.1 Artificial and Formal Languages

   Many centuries after the spontaneous emergence of natural language for human 
   communication, mankind has purposively constructed other communication systems and
   languages, to be called artificial, intended for very specific tasks .A few artificial 
   languages, like the logical propositions of Aristotle or the music sheet notation of
   Guittone d’Arezzo, are very ancient, but their number has exploded with the invention
   of computers. Many artificial languages are intended for man–machine 
   communication, to instruct a programmable machine todo some task: to performa computation,
   to prepare a document, to search a database or the Web, to control a robot, and so on.
   Other languages serve as interfaces between devices or between software 
   applications, e.g., PDF is a language that many document and picture processors read and
   write.

   Any designed language is artificial by definition, but not all artificial languages
   are formalized: thus, a programming language like Java is formalized, but Esperanto,
   although designed by man, is not so.

   But how should we classify the language of genes or DNA, which deals with
   natural biological systems though it has been invented by scientists? The structures
   ofmolecularbiologyarethree-dimensional,yetviewingthemasRNAsequenceshas
   opened the way to the methods of formal languages. ➊

   For a language to be formalized (or formal), the form of its sentences (or syntax)
   and their meaning (or semantics) must be precisely and algorithmically defined. In
   other words, it should be possible for a computer to check that the sentences are
   grammatically correct, and to determine their meaning.

   ➊ For a discussion of the role of linguistic methods in molecular biology, see Searls [1].
   © Springer Nature Switzerland AG 2019

   S. Crespi Reghizzi et al., Formal Languages and Compilation,
   Texts in Computer Science, https://doi.org/10.1007/978-3-030-04879-2_2

<a id="P006"></a>

   Meaning, however, is a difficult and controversial notion. For our purposes, the
   meaningofasentencecanbetakentobethetranslationintoanotherlanguage,which
   is known to the computer or to a well-established software system. For instance,
   the meaning of a Java program is its translation into the machine language of the
   computer executing the program.

   Inthisbook,thetermformallanguageisusedinanarrowersense,whichexcludes
   semantics. In the field of syntax, a formal language is a mathematical structure,
   defined on top of an alphabet, by means of certain axiomatic rules (formal grammar)
   or by using abstract machines such as the famous one due to A. Turing. The notions
   and methods of formal language are analogous to those used in number theory and
   in logic.

   Thus, formal language theory is primarily concerned with the form or syntax of
   sentences, not with meaning, at least not directly. A string (or text) is either valid
   or illegal, that is, it either belongs to the formal language or it does not. Such a
   theory makes a first important step toward the ultimate goal: the study of language
   translation and meaning, which will however require additional methods.


## 2.1.2 Language Types

   In this book, a language is a one-dimensional communication medium, made by
   sequences of symbolic elements of an alphabet, called terminal characters. Actually,
   people often refer to language as other non-textual communication media, which are
   more or less formalized by means of rules. Thus, iconic languages focus on road 
   traffic signs or video display icons. Musical language is concerned with sounds, rhythm,
   and harmony. Architects and industrial designers of buildings and things are interested 
   in their spatial relations, which they describe as the language of design. Early
   child drawings are often considered as sentences of a pictorial language, which can
   be partially formalized in accordance with developmental psychology theories.
   Certainly, the formal approach to syntax to be developed has some interest for nontextual
   languages too, which however are out of scope for this book.

   Within computer science, the term language applies, as said, to a text made by
   a set of characters orderly written from, say, left to right. In addition, the term is
   used to refer to other discrete structures, such as graphs, trees, or arrays of pixels
   describing a digital picture. Formal language theories have been proposed and used
   more or less successfully also for such nontextual languages. ➋

   Reverting to the main stream of textual languages, a frequent request directed to
   the specialist is to define and specify an artificial language. The specification may
   have several uses: as a language reference manual for future users, as an official
   standard definition, or as a contractual document for compiler designers to ensure
   consistency of specification and implementation.

   ➋ Just three examples and their references: graph grammars and languages [2], 
   tree languages [3,4], and picture (or two-dimensional) languages [5,6].

<a id="P007"></a>

   It is not an easy task to write a complete and rigorous definition of a language. Of
   course, the exhaustive approach, to list all the possible sentences or phrases, is unfeasible 
   because the possibilities are infinitely many, since the length of the sentences is
   usually unbounded. As a native language speaker, a programmer is not constrained
   by any strict limit on the length of the phrases he writes. The ensuing problem to
   represent an infinite number of cases by a finite description can be addressed by an
   enumeration procedure, as in logic. When executed, the procedure generates longer
   and longer sentences, in an unending process if the language to be modeled is not
   finite.

   This chapter presents a simple and established manner to express the rules of the
   enumeration procedure in the form of rules of a generative grammar (or syntax).

## 2.1.3 Chapter Outline

   The chapter starts with the basic components of language theory: alphabet, string,
   and operations, such as concatenation and repetition, on strings and sets of strings.
   The first model presented are the regular expressions, which define the family
   of regular languages. Then the lists are introduced as a fundamental and pervasive
   syntax structure in all the kinds of languages. From the exemplification of list variants,
   the idea of linguistic abstraction grows out naturally. This is a powerful reasoning
   tool to reduce the varieties of existing languages to a few paradigms.

   After discussing the limits of regular languages, the chapter moves to context-free
   grammars. Following the basic definitions, the presentation focuses on
   structural properties, namely equivalence, ambiguity, and recursion. Exemplification 
   continues with important linguistic paradigms such as hierarchical lists, 
   parenthesized structures, polish notations, and operator precedence expressions. Their
   combination produces the variety of forms to be found in artificial languages.
   Then the classification of some common forms of ambiguity and corresponding
   remedies is offered as a practical guide for grammar designers.

   Various rule transformations (normal forms) are introduced, which should familiarize 
   the reader with the modifications, to adjust a grammar without affecting the
   language, often needed for the analysis algorithms studied in Chap. 4.

   Returning to regular languages from the grammar perspective, the chapter
   evidences the greater descriptive capacity of context-free grammars. The comparison
   of regular and context-free languages continues by considering the operations that
   may cause a language to exit or remain in one or the other family. Alphabetical
   transformations anticipate the operations studied in Chap. 5 as translations. A
   discussion about the unavoidable regularities found in very long strings completes the
   theoretical picture.

   The last section mentions the Chomsky classification of grammar types and exemplifies 
   context-sensitive grammars, stressing the difficulty of such rarely used model
   and discussing some recent attempts at more expressive grammar models.

<a id="P008"></a>

2.2 Formal Language Theory
==========================

   Formal language theory starts from the elementary notions of alphabet, string
   operations, and aggregate operations on sets of strings. By such operations, complex
   languages can be obtained starting from simpler ones.


## 2.2.1 Alphabet and Language

   An alphabet is a finite set of elements called terminal symbols or characters. 
   Let $ \sum = \{a_1 , a_2 , ..., a_k \} $ be an alphabet with k elements, i.e., its 
   cardinality is $ |\sum| = k $. A string, also called a word, is a sequence, i.e.,
   an ordered set possibly with repetitions, of characters.

   Example 2.1 (binary strings) Let $ Σ = \{ a, b \} $  be an alphabet. Some strings are:
   a a b a, a a a, a b a a, and b.                                               ■

   A language is a set of strings over a specified alphabet.

   Example 2.2 Three sample languages over the same alphabet $ Σ = \{ a, b \} $ :

      L 1 = {aa, aaa}
      L 2 = {aba, aab}
      L 3 = {ab, ba, aabb, abab, ..., aaabbb, ...}
          = set of the strings having as many letters a as letters b             ■

   Notice that a formal language viewed as a set has two layers. At the first level, there
   is an unordered set of nonelementary elements, the strings. At the second level, each
   string is an ordered set of atomic elements, the terminal characters.

   Given a language, a string belonging to it is called a sentence or phrase. Thus,
   $ bbaa \in L_3 $ is a sentence of $ L_3 $ , whereas $ abb ∉ L_3 $ is an incorrect string.

   The cardinality or size of a language is the number of sentences it contains. For
   instance, $ |L_2 | = | {aba, aab} | = 2 $. If the cardinality is finite, the language is
   called finite, too. Otherwise, there is no finite bound on the number of sentences
   and the language is termed infinite. To illustrate, languages $ L_1 $ and $ L_2 $ are finite, but
   language $ L_3 $ is infinite.

   One can observe that a finite language is essentially a collection of words ➌
   sometimes called a vocabulary. A special finite language is the empty set or language ∅,
   which contains no sentence, i.e., |∅| = 0. Usually, when a language contains just
   one element, the set braces are omitted by writing, e.g., abb instead of {abb}.

   ➌ In mathematical writings, the terms word and string are synonymous, while in linguistics a word
   is a string that has a meaning.


<a id="P009"></a>

   It is convenient to introduce the notation $ |x|_b $ for the number of characters b
   present in a string x. For instance,

   $$ |aab|_a = 2    \qquad   |aba|_a = 2    \qquad   |baa|_c = 0 $$

   The length |x| of a string x is the number of characters it contains, e.g., $ |ab| = 2 $
   and |abaa| = 4. Two strings of length h and k, respectively:

   $$ x = a_1 a_2 ... a_h        \qquad         y = b_1 b_2 ... b_k $$

   are equal if $ h = k $ and $ a_i = b_i (1 ≤ i ≤ h) $. In words, by examining the strings from
   left to right, their respective characters coincide. Thus, we obtain:

   $$ aba \neq baa               \qquad         baa \neq ba $$

### 2.2.1.1 String Operations

   In order to manipulate strings, it is convenient to introduce several operations. For
   strings:

   $$ x = a_1 a_2 ... a_h        \qquad         y = b_1 b_2 ... b_k $$

   concatenation ➍ is defined as:

   $$ x · y = a_1 a_2 ... a_h b_1 b_2 ... b_k $$

   The dot ‘·’ may be dropped, writing xy in place of x · y. This operation, essential
   for formal languages, plays the role addition has in number theory.

   Example 2.3 (string concatenation) For strings:

   $$ x = well       \qquad      y = in      \qquad      z = formed $$

   we obtain:

   $$ xy = wellin    \qquad      yx = inwell \neq xy $$

   $$ \begin{aligned} 
      (xy)z &= wellin · formed = x(yz) = well · informed          \qquad\qquad   ■\\
            &= wellinformed 
      \end{aligned} $$

   Concatenation is clearly noncommutative, that is, the identity $$ xy \neq yx $$ does not hold
   in general. The associative property holds:

   $$ (xy)z = x(yz ) $$

   Associativity permits to write without parentheses the concatenation of three or more
   strings. The length of the result is the sum of the lengths of the concatenated strings:

   $$ |xy| = |x| + |y|       \qquad\text{(2.1)} $$

   ➍ Also termed product in mathematical works.


<a id="P010"></a>


### 2.2.1.2 Empty String

   It is useful to introduce the concept of empty (or null) string, denoted by the Greek
   epsilon ε, as the only string satisfying, for every string x, the identity:

   $$ xε = εx = x $$

   From (2.1), it follows that the empty string has length zero: |ε| = 0.

   From an algebraic perspective, the empty string ε is the neutral element with
   respect to concatenation, because any string is unaffected by concatenating ε to the
   left or right.

   The empty string should not be confused with the empty set ∅. In fact, the empty
   set as a language contains no string, whereas the set {ε} contains one, the empty
   string; equivalently, the cardinalities of sets ∅ and {ε} are 0 and 1, respectively. A
   language L is said to be nullable if and only if it includes the empty string, i.e., $ε ∈ L$.

### 2.2.1.3 Substring

   Let $ string x = uyv $ be written as the concatenation of three, possibly empty, strings
   u, y, and v. Then strings u, y, v are substrings of x. Moreover, string u is a prefix
   of x and string v is a suffix of x. A nonempty substring (or prefix or suffix) is called
   proper if it does not coincide with string x.

   Let x be a nonempty string of length at least k, i.e., $ |x| ≥ k ≥ 1 $. The notation
   $ Ini_k (x) $ denotes the prefix u of x having length k, to be termed the initial (of x) of
   length k.

   Example 2.4 The string `x = aabacba` contains the following components:

      prefixes    a, aa, aab, aaba, aabac, aabacb, and aabacba
      suffixes    a, ba, cba, acba, bacba, abacba, and aabacba
      substrings  all the prefixes and suffixes listed above, and the internal 
                  strings such as a, ab, ba, bacb, …

   Notice that the pair bc is not a substring of string x although both letters b and c
   occur in x. The initial of length two is $ Ini_2 (aabacba) = aa $.            ■

   > [!TIP]
   > 单词 initial 意指词首， $ Ini_k (x) $ 表示 x 字符串中长度为 k 的前缀。

### 2.2.1.4 Reversal or Mirror Reflection

   The characters of a string are usually read from left to right, but it is sometimes
   requested to reverse the order. The reversal or reflection of a string $ x = a_1 a_2 ... a_h $
   is the string $ x_R = a_h a_{h−1} ... a_1 $. For instance, it is:

   $$ x = roma          \qquad      x^R = amor $$

<a id="P011"></a>

   The following string identities are immediate:

   $$ (x^R)^R = x       \qquad      (xy)^R = y^R x^R     \qquad      ε^R = ε $$


### 2.2.1.5 Repetition

   When a string contains repetitions, it is handy to have an operator expressing them.
   The m-th power $ x^m $ (for an integer m ≥ 1) of a string x is the concatenation of x with
   itself for m − 1 times:

   $$ x^m = x · x · ··· · x   \\ \qquad \text{m times}$$
   
   By stipulation, the zero power of any string is defined to be the empty string. Thus,
   the complete definition is:

   $$ x m = x m−1 · x for m ≥ 1 x 0 = ε $$

   Here are a few examples:

   $$ \begin{aligned}
      x &= ab        &  x^0 &= ε                   &  x^1 &= x = ab  &  x^2 &= (ab)^2 = abab    \\
      y &= a^2 = aa  &  y^3 &= a^2 a^2 a^2 = a^6   &  ε^0 &= ε       &  ε^2 &= ε
      \end{aligned} $$

   When writing formulas, the string to be repeated must be parenthesized, if it is longer
   than one. Thus, to express the 2-nd power of string ab, i.e., abab, one should write
   $ (ab)^2 $ , not $ ab^2 $, which is the string abb.

   Expressed differently, we assume that the power operation takes precedence over
   concatenation. Similarly, reversal takes precedence over concatenation, e.g., $ ab^R $
   returns ab since $ b^R = b $, while $ (ab)^R = ba $.


## 2.2.2 Language Operations

   It is straightforward to extend an operation, originally defined on a string, to an entire
   language: just apply the operation to each one of the language sentences. By means
   of this general principle, the string operations previously defined can be revisited,
   starting from those that have one argument.

   The reversal $ L^R $ of a language L is the set of the strings that are the reversal of a
   sentence of L:

   $$ L^R = \{ x | x = y^R ∧ y ∈ L \}    \\ \qquad\qquad \ _{\text{characteristic predicate}} $$

   Here the strings x are specified by the property expressed in the so-called 
   characteristic predicate.

   Similarly, the set of the proper prefixes of a language L is:

   $$ prefixes(L) = \{ y| x = yz ∧ x ∈ L ∧ y \neq ε ∧ z \neq ε \} $$


<a id="P012"></a>

   Example 2.5 (prefix-free language) In some applications, the loss of one or more
   final characters of a language sentence is required to produce an incorrect string.
   The motivation is that the compiler is then able to detect the inadvertent truncation
   of a sentence.

   A language is prefix-free if none of the proper prefixes of its sentences is in
   the language, i.e., the set prefixes(L) is disjoint from L. Thus, language $ L_1 =
   \{ x| x = a_n b_n ∧ n ≥ 1 \} $ is prefix-free since every prefix takes the form $ a^n b^m $,
   with n > m ≥ 0, and does not satisfy the characteristic predicate.

   On the other hand, the language $ L_2 = \{ a^m b^n | m > n ≥ 1 \} $ contains $ a^3 b^2 $ as
   well as the proper prefix $ a^3 b $.                                          ■

   Similarly, the operations on two strings can be extended to two languages, by
   letting the former and latter argument span the respective language. For instance, the
   concatenation of languages L′ and L″ is defined as:

   $$ L' · L″ =  \{ x · y| x ∈ L' ∧ y ∈ L″ \} $$

   From this, extending the m-th power operation to a language is straightforward
   (m ≥ 0):

   $$ L^m = L^{m−1} · L    \qquad   \text{for m ≥ 1}  \qquad  L^0 = {ε} $$

   Some special cases follow from the previous definitions:

   $$ ∅^0 = \{ε\}    \qquad   L · ∅ = ∅ · L = ∅       \qquad   L · \{ε\} = \{ε\} · L = L $$

   Example 2.6 (language concatenation) Consider languages $ L_1 $ and $ L_2 $ :

   $$ \begin{aligned}
      L_1 &= \{ a^i   & \text{ | i ≥ 0 ∧ i is even} \} & = \{ε, aa, aaaa, ...\} \\
      L_2 &= \{ b^j a & \text{ | j ≥ 1 ∧ j is odd } \} & = \{ba, bbba, ...\} 
      \end{aligned}$$

   We obtain this concatenation $ L_1 · L_2 $ :

   $$ \begin{aligned}
      L_1 · L_2 & = \{ a^i · b^j a \text{ | (i ≥ 0 ∧ i is even) ∧ (j ≥ 1 ∧ j is odd)} \} \\
                & = εba, a^2 ba, a^4 ba, ..., εb^3 a, a^2 b^3 a, a^4 b^3 a, ...
      \end{aligned}$$

   A common error when computing a power of a base language is to take the same
   string for m times. The result is a different set, included in the power of the base:

   $$ \{ x| x = y^m ∧ y ∈ L \} ⊆ L^m   \qquad  m ≥ 2  \qquad (2.2) $$

   Thus, in (2.2) for $ L = {a, b} $ and m = 2, the set to the left of ‘⊆’ is {aa, bb}, while
   the value of $ L^m $ is {aa, ab, ba, bb}.


<a id="P013"></a>

   Example 2.7 (strings of finite length) The power operation allows a concise definition
   of the strings of length not exceeding some integer $ k ≥ 0 $. Consider the alphabet
   $ Σ = \{ a, b \} $. For $ k = 3 $, the language L:

   $$ L = \{ ε, a, b, aa, ab, ba, bb, aaa, aab, aba, abb, baa, bab, bba, bbb \}
        = Σ^0 ∪ Σ^1 ∪ Σ^2 ∪ Σ^3 $$

   can also be defined as:

   $$ L = \{ ε, a, b \}^3 $$

   Notice that the sentences shorter than 3 are obtained by using the empty string ε of
   the base language.

   By slightly changing the example, the language $ L' = \{ x| 1 ≤ |x| ≤ 3 \} $ is
   defined, with concatenation and power, by the formula:

   $$ L″ = \{ a, b \} · \{ε, a, b\}^2 $$


## 2.2.3 Set Operations

   Since a language is a set, the classical set operations of union ‘∪’, intersection‘∩’, and
   difference ‘\’ apply to languages. The set relations of inclusion ‘⊆’, strict inclusion
   ‘⊂’, and equality ‘=’ apply as well.

   Before introducing the complement of a language, the notion of universal language 
   is needed: it is defined as the set of all the strings, over an alphabet Σ, of any
   length including zero. Clearly, the universal language is infinite and can be viewed
   as the union of all the powers of the alphabet:

   $$ L_{universal} = Σ^0 ∪ Σ ∪ Σ^2 ∪ ... $$

   The complement of a language L over an alphabet Σ, denoted by $ ¬L $, is the set
   difference:

   $$ ¬L = L_{universal} \ L $$

   that is, the set of the strings over alphabet Σ that are not in L. When the alphabet is
   understood, the universal language can be expressed as the complement of the empty
   language:

   $$ L_{universal} = ¬∅ $$

   Example 2.8 (language complement) The complement of a finite language is always
   infinite; for instance, the set of strings of any length except two is:

   $$ ¬( \{a, b\}^2 ) = \{ε\} ∪ \{a, b\} ∪ \{a, b\}^3 ∪ ... $$


<a id="P014"></a>

   On the other hand, the complement of an infinite language may or may not be finite,
   as shown on one side by the complement of the universal language, and on the other
   side by the complement of the set of the strings of even length over alphabet {a}:

   $$ L = \{ a^{2n} \text{ | n ≥ 0 } \}  \qquad  ¬L = \{ a^{2n+1} \text{ | n ≥ 0 } \} $$

   Moving to set difference, consider an alphabet Σ = {a, b, c} and languages:

   $$ \begin{aligned}
      L_1 &= \{ x | \quad |x|_a = |x|_b = |x|_c ≥ 0 \} \\
      L_2 &= \{ x | \quad |x|_a = |x|_b ∧ |x|_c = 1 \}
      \end{aligned}$$

   Then the set differences are:

   $$ L_1 \ L_2 = {ε} ∪ \{ x | \quad |x|_a = |x|_b = |x|_c ≥ 2 \} $$

   which represents the set of the strings that have the same number (except one) of
   occurrences of letters a, b, and c, and:

   $$ L_2 \ L_1 = \{x| \quad |x|_a = |x|_b \neq |x|_c ∧ |x|_c = 1\} $$

   which is the set of the strings that have one c and the same number of occurrences
   (except one) of a and b.                                                      ■

## 2.2.4 Star and Cross

   Most artificial and natural languages include sentences that can be lengthened at will,
   so causing the number of sentences in the language to be unbounded. On the other
   hand, all the operations so far defined, with the exception of complement, do not
   allow to write a finite formula denoting an infinite language. In order to enable the
   definition of an infinite language, the next essential development extends the power
   operation to the limit.


### 2.2.4.1 Star

   The star ➎ operation ‘∗’ is defined as the union of all the powers of the base language:

   $$ L^∗ = \bigcup^∞_{h=0} L^h = L^0 ∪ L^1 ∪ L^2 ∪ ... = \{ε\} ∪ L ∪ L^2 ∪ ... $$

   ➎ Also known as Kleene star and as reflexive transitive closure by concatenation.

<a id="P015"></a>

   Example 2.9 (language star) For the language $ L = \{ ab, ba \} $, the star is:

   $$ L^∗ = \{ε, ab, ba, abab, abba, baab, baba, ...\} $$

   Every nonempty string of the ‘starred’ language $ L^∗ $ can be segmented into substrings,
   which are the sentences of the base language L.

   Notice that, if the base language contains at least one nonempty string, the starred
   language is infinite.

   It may happen that the starred and base languages are identical, as in:

   $$ L = \{ a^{2n} | n ≥ 0 \}      \qquad   L^∗ = \{ a^{2n} | n ≥ 0 \} = L $$ 

   An interesting special case occurs when the base is an alphabet Σ; then the star $ Σ^∗ $
   contains all the strings ➏ obtained by concatenating terminal characters. This language
   is the same as the previous universal language ➐ of alphabet Σ.

   It is obvious that any formal language is a subset of the universal language over the 
   same alphabet, and the relation $ L ⊆ Σ^∗ $ is often written to say that L is a language
   over an alphabet Σ. Some useful properties of star follow:

   | properties of star           |  meaning                        |
   |------------------------------|---------------------------------|
   | $ L ⊆ L^∗ $                $ | monotonicity
   | **if** $ x ∈ L^∗ ∧ y ∈ L^∗ $ **then** $ xy ∈ L^∗ $ | closure by concatenation
   | $ (L^∗)^∗ = L^∗            $ | idempotence
   | $ (L^∗)^R = (L^R)^∗        $ | commutativity with reversal

   Example 2.10 (idempotence) The monotonicity property states that any language is included 
   in its star. But for language $ L = \{ a^{2n} | n ≥ 0 \} $, the equality $ L^∗ = L $ 
   follows from the idempotence property and from the fact that L can be equivalently
   defined by the starred formula $ {aa}^∗ $.                                    ■

   For the empty language ∅ and empty string ε, we have the set identities:

   $$ ∅^∗ = \{ ε \}           \qquad\qquad      \{ ε \}^∗ = \{ ε \} $$

   ➏ The length of a sentence in $ Σ^∗ $ is unbounded, yet it may not be considered infinite. A specialized
   branch of formal language theory (see Perrin and Pin [7]) is devoted to the so-called infinitary
   or omega-languages, which include also sentences of infinite length. They effectively model the
   situations when a perpetual system can receive or produce messages of infinite length.

   ➐ Another name for it is free monoid. In algebra, a monoid is a structure provided with an associative
   composition law (concatenation) and a neutral element (empty string).


<a id="P016"></a>

   Example 2.11 (identifiers) Many artificial languages assign a name or identifier to
   eachentity, i.e., variable, file, document, subprogram, andobject. A common naming
   rule prescribes that an identifier is a string that has the initial character in the set
   {A, B, ..., Z } and contains any number of letters or digits {0, 1, ..., 9}, e.g.,
   LOOP3A2.

   By using the alphabets $ Σ_A $ and $ Σ_N $ :

   $$ Σ_A = \{ A, B, ..., Z \}         \qquad   Σ_N = \{0, 1, ..., 9 \} $$

   the language of identifiers $ I ⊆ (Σ_A ∪ Σ_N )^∗ $ is:

   $$ I = Σ_A (Σ_A ∪ Σ_N )^∗ $$

   To introduce a variant, prescribe that the length of the identifiers should not exceed
   five. By defining an alphabet $ Σ = Σ_A ∪ Σ_N $, the language is:

   $$ I_5 = Σ_A (Σ^0 ∪ Σ^1 ∪ Σ^2 ∪ Σ^3 ∪ Σ^4 ) \\
          = Σ^A (  ε ∪ Σ   ∪ Σ^2 ∪ Σ^3 ∪ Σ^4)  $$

   The formula expresses the concatenation of language $ Σ_A $ , the sentences of which
   are single characters, with the language constructed as the union of powers. A more
   elegant writing is:

   $$ I_5 = Σ_A (ε ∪ Σ)^4 $$


### 2.2.4.2 Cross

   A useful though dispensable operator, derived from star, is the cross ➑ ‘+’:

   $$ L^+ = \bigcup^∞_{h=1} = L ∪ L^2 ∪ ... $$

   Cross differs from star because the union excludes the power zero. The following
   relations hold:

   $$ L^+ ⊆ L*∗                                 \qquad   
      ε ∈ L^+ \text{ if and only if } ε ∈ L     \qquad 
      L^+ = LL^∗ = L^∗L                         $$

   Star and cross are called iteration operators.

   Example 2.12 (language cross) Two applications of cross to finite sets:

   $$ \{ ab, bb \}^+ = \{ ab, b^2 , ab^3 , b^2 ab, abab, b^4 , ... \}  \\
      \{  ε, aa \}^+ = \{ ε, a^2 , a^4 , ... \} = \{ a^{2n} | n ≥ 0 \} $$

   ➑ Or irreflexive closure by concatenation.


<a id="P017"></a>

   Not surprisingly, a language can usually be defined by various formulas, which differ
   by their use of operators.

   Example 2.13 (controlling string length) Two ways of defining the strings of four
   or more characters:

   * concatenating the strings of length 4 with arbitrary strings: $ Σ^4 · Σ^∗ $
   * constructing the 4-th power of the set of nonempty strings: $ (Σ^+)^4 $     ■


## 2.2.5 Language Quotient

   Operations like concatenation, star, or union lengthen the strings or increase the
   cardinality of the set of strings they operate upon. On the other hand, given two
   languages, the right quotient operation $ L'|_R L″ $ shortens the sentences of the first
   language L′ by cutting a suffix, which is a sentence of the second language L″ . The
   right quotient of L′ with respect to L″ is defined as:

   $$ L = L'|_R L″ = \{ y \text{ | ∃ z such that } yz ∈ L' ∧ z ∈ L″ $$

   Example 2.14 (quotient language) Consider two languages L′ and L″ :

   $$ L' = \{ a^{2n} b^{2n} | n > 0 \}          \qquad   
      L″ = \{ b^{2n+1}      | n ≥ 0 \}          $$

   Their right quotients are:

   $$ L'|_R L″ = \{ a^r b^s \text{ | (r ≥ 2 is even) ∧ (1 ≤ s < r is odd)} \} \\
               = \{ a^2 b, a^4 b, a^4 b^3 , ... \}                            \\
      L″|_R L' = ∅                                                            $$

   A dual operation is the left quotient $ L″|_L L' $ that shortens the sentences of language
   L' by cutting a prefix, which is a sentence of language L″ .

   More operations will be introduced later, in order to transform or translate a formal
   language by replacing the terminal characters with other characters or strings.

2.3 Regular Expressions and Languages
=====================================

   Theoretical investigation on formal languages has invented various categories of
   languages,inawayreminiscentoftheclassificationofnumericaldomainsintroduced
   much earlier by number theory. Such categories are characterized by mathematical
   and algorithmic properties.


<a id="P018"></a>

   Thefirstfamilyofformallanguagestobeconsiderediscalledregular (orrational)
   and can be defined by an astonishing number of different approaches. Regular
   languages have been independently discovered in disparate scientific fields: the study
   of input signals driving a sequential circuit 9 to a certain state, the lexicon of
   programming languages modeled by simple grammar rules, and the simplified analysis
   of neural behavior. Later such approaches have been complemented by a logical
   definition based on a restricted form of predicates.

   To introduce the family, the first definition will be algebraic, by using the 
   operationsofunion,concatenation,andstar;thenthefamilywillbedefinedagainbymeans
   ofcertainsimplegrammarrules;last,Chap.3describesthealgorithmforrecognizing
   regular languages in the form of an abstract machine, the finite automaton. 10

## 2.3.1 Definition of Regular Expression

   A language over alphabet Σ = {a 1 , a 2 , ..., a n } is regular if it can be expressed
   by applying for a finite number of times the operations of concatenation, union, and
   star,startingwiththeunitarylanguages 11 {a 1 },{a 2 },…,{a n }ortheemptystringε.
   More precisely, a regular expression (r.e.) is a string r containing the terminal
   characters of the alphabet Σ and the following metasymbols 12 :
   “∪” union “·” concatenation “∗” star
   “ε” empty (or null) string “( )” parentheses
   in accordance with the following rules:
      # rule meaning
      1 r = ε empty (or null) string
      2 r = a unitary language
      3 r = (s ∪ t ) union of expressions
      4 r = (s · t ) or r = (st ) concatenation of expressions
      5 r = (s) ∗ iteration (star) of an expression

   where the symbols s and t are regular (sub)expressions.

   For expressivity, the metasymbol cross is allowed in an r.e., since it can be
   expressed using rules 4 and 5 as (s) + = (s (s ∗ )). For economy, some parentheses
   9 A digital component incorporating a memory.

   10 The language family can also be defined by the form of the logical predicates characterizing
   language sentences, e.g., as in [8].

   11 A unitary language contains one sentence.

   12 In order to prevent confusion between terminals and metasymbols, the latter should not be in the
   alphabet. If not so, the metasymbols must be suitably recoded to make them distinguishable.

<a id="P019"></a>

   canbedroppedbystipulatingthefollowingprecedencebetweenoperators:firstapply
   star, then concatenation, and last union. Furthermore, the union of three (or more)
   terms, e.g., ((r ∪ s) ∪ t) and (r ∪ (s ∪ t)), can be simplified as (r ∪ s ∪ t) because
   union is an associative operation. The same remark applies to concatenation.
   Itiscustomarytowritethecup‘∪’symbolasaverticalbar‘|’,calledanalternative.

   In more theoretical books on formal languages, the metasymbol ‘∅’ (empty set)
   is allowed in an r.e., which permits to obtain the empty string ‘ε’ from the basic
   operations through the identity {ε} = ∅ ∗ . We prefer the more direct and practical
   presentation shown above.

   The rules from 1 to 5 compose the syntax of the r.e., to be formalized later by
   means of a grammar (Example 2.31 on p.33). The meaning or denotation of an r.e.
   r is a language L r over the alphabet Σ, defined by the correspondence shown in
   Table 2.1.

   Example 2.15 (language of the multiples of 3) Let the alphabet be Σ = {1}, where
   character ‘1’ may be viewed as a pulse or signal. An r.e. e and the language L e it
   denotes are:
   e = (111) ∗ L e =
   ?
   1 n | n mod 3 = 0
   ?
   Language L e contains the character sequences multiple of three. Notice that when
   dropping parentheses the language changes, due to the precedence of star over
   concatenation:
   e 1 = 111 ∗ = 11 (1) ∗ L e 1 =
   ?
   1 n | n ≥ 2
   ?
   ?
   Example 2.16 (language of integers) Let the alphabet be Σ = {+, −, d }, where
   character d denotes any decimal digit 0, 1, …, 9. The expression e:
   e = (+ ∪ − ∪ ε) d d ∗ ≡ (+ | − | ε) d d ∗
   produces the language L e = {+, −, ε} {d } {d } ∗ of the integers with or without
   sign, such as +353, −5, 969, and +001. ?
   Table 2.1 Language L r denoted by a regular expression r
      # Regular expression r Language L r denoted by r
      1 ε {ε}
      2 a {a} where a ∈ Σ
      3 s ∪ t or more often s | t L s ∪ L t
      4 s · t or more often st L s · L t or more often L s L t
      5 s ∗ , s + L ∗
      s , L
      +
      s

<a id="P020"></a>

   Actually, the correspondence between an r.e. and its denoted language is so direct
   that it is customary to refer to the language L e by the r.e. e itself.

   We introduce the established terminology for two language families. A language
   isregular ifitisdenotedbyaregularexpression.Theemptylanguage∅isconsidered
   a regular language as well, despite there is not an r.e. that defines it. The collection
   of all regular languages is called the family REG of regular languages.

   Another simple family of languages is the collection of all the finite languages,
   and it is called FIN. Thus, a language is in the family FIN if its cardinality is finite,
   as for instance the language of 32-bit binary numbers.

   BycomparingthefamiliesREGandFIN,itiseasytoseethateveryfinitelanguage
   is regular, i.e., FIN ⊆ REG. In fact, a finite language is the union of finitely many
   stringsx 1 ,x 2 ,…,x k ,eachonebeingtheconcatenationoffinitelymanycharacters,i.e.,
   x i = a 1 a 2 ... a n i . Then an r.e. producing such finite language is simply the union of
   k termsx i ,eachoneconcatenatingn i characters.SincefamilyREGincludesnonfinite
   languages, too, the inclusion between the two families is proper, i.e., FIN ⊂ REG.
   More language families will be later introduced and compared with REG.


## 2.3.2 Derivation and Language

   We formalize the mechanism through which a given r.e. e produces the denoted
   language. For the moment, we suppose that r.e. e is fully parenthesized (except for
   the atomic terms), and we introduce the notion of subexpression (s.e.) in the next
   example:
   e 0 =
   ?
   subexpression e 1 of e 0
   ?
   (a ∪ (bb)) ∗
   ?
   subexpression e 2 of e 0
   ? ?
   c +
   ?
   ∪ (a ∪ (bb))
   subexpr. s of e 2
   ?
   ?
   This r.e. e 0 is structured as the concatenation of two parts e 1 and e 2 , to be called
   subexpressions (of e 0 ). In general, an s.e., f , of an r.e. e is a well-parenthesized
   substring immediately occurring inside the outermost parentheses. This means that
   thereisnotanotherwell-parenthesizedsubstringofe thatcontainsf .Intheexample,
   the substring labelled s is not an s.e. of e 0 , but it is an s.e. of e 2 .
   When an r.e. is not fully parenthesized, in order to identify its subexpressions one
   has to insert (or to imagine) the missing parentheses, in agreement with the operator
   precedence. We recall that three or more terms combined by union do not need to be
   pairwise parenthesized, because union is an associative operation. The same remark
   applies to three or more concatenated terms.

   Aunionoriteration(starandcross)operatoroffersdifferentchoicesforproducing
   strings. By making a choice, one obtains an r.e. that defines a less general language,
   which is included in the original one. We say that an r.e. is a choice of another one
   in the following three cases:

<a id="P021"></a>

   1. r.e. e k (with 1 ≤ k ≤ m and m ≥ 2) is a choice of the union:
   (e 1 ∪ ··· ∪ e k ∪ ··· ∪ e m )
   2. r.e. e m = e · ··· · e
   m times
   (with m ≥ 1) is a choice of the star e ∗ or cross e +
   3. the empty string ε is a choice of the star e ∗
   Let e ? be an r.e. By substituting some choice for e ? , a new r.e. e ?? can be derived from
   e ? . The corresponding relation, called derivation, between two regular expressions
   e ? and e ?? is defined next.

   Definition 2.17 (derivation 13 ) We say that an r.e. e ? derives an r.e. e ?? , written
   e ? ⇒ e ?? , if one of the two conditions below holds true:
   1. r.e. e ?? is a choice of e ?
   2. r.e. e ? is the concatenation of m ≥ 2 s.e., as follows:
   e ? = e ? 1 ... e ? k ... e ? m
   and r.e. e ?? is obtained from e ? by substituting an s.e., say e ?
   k , with a choice of e
   ?
   k ,
   say e ??
   k , as shown below:
   ∃k, 1 ≤ k ≤ m, such that e ??
   k
   is a choice of e ? k ∧ e ?? = e ? 1 ... e ??
   k
   ... e ? m
   Such a derivation ⇒ is called immediate as it makes exactly one choice. If two
   or more immediate derivations are applied in series, thus making as many choices
   altogether, we have a multi-step derivation. We say that an r.e. e 0 derives an r.e. e n
   in n ≥ 1 steps, written e 0
   n
   = ⇒ e n , if the following immediate derivations apply:
   e 0 ⇒ e 1 e 1 ⇒ e 2 ... e n−1 ⇒ e n
   The notation e ?
   +
   = ⇒ e ?? says that an r.e. e ? derives an r.e. e ?? in n ≥ 1 steps, without
   specifyingn.Ifthenumberofstepsisn = 0,thentheidentitye 0 = e n holdsandsays
   that the derivation relation is reflexive. We also write e ?
   ∗
   = ⇒ e ?? if it holds e ?
   +
   = ⇒ e ?? or
   e ? = e ?? . ?
   Example 2.18 (immediate and multi-step derivation) Immediate derivations:
   a ∗ ∪ b + ⇒ a ∗ a ∗ ∪ b + ⇒ b +
   ?
   a ∗ ∪ bb
   ? ∗
   ⇒
   ?
   a ∗ ∪ bb
   ? ?
   a ∗ ∪ bb
   ?
   13 Also called implication.


<a id="P022"></a>

   Noticethatthesubstringsofther.e.consideredmustbechoseninorderfromexternal
   to internal, if one wants to produce all possible derivations. For instance, it would be
   unwise, starting from e ? = (a ∗ ∪ bb) ∗ , to choose
   ?
   a 2 ∪ bb
   ? ∗ , because a ∗
   is not
   an s.e. of e ? . Although the value 2 is a correct choice for the star, such a premature
   choice would rule out the derivation of a valid sentence such as a 2 bba 3 .
   Multi-step derivations:
   a ∗ ∪ b + ⇒ a ∗ ⇒ ε that is a ∗ ∪ b +
   2
   = ⇒ ε or also a ∗ ∪ b +
   +
   = ⇒ ε
   a ∗ ∪ b + ⇒ b + ⇒ bbb or also a ∗ ∪ b +
   +
   = ⇒ bbb
   ?
   Some expressions produced by derivation from an expression r contain the
   metasymbols of union, star, and cross. Others just contain terminal characters or the
   empty string, and possibly redundant parentheses, which can be canceled. The latter
   expressions compose the language denoted by the r.e.

   The language defined by a regular expression r is:
   L r =
   ?
   x ∈ Σ ∗ | r
   ∗
   = ⇒ x
   ?
   Two r.e. are equivalent if they define the same language.

   The coming examples hows that different derivation orders may produce the same
   sentence.

   Example 2.19 (derivation orders) Compare the derivations 1 and 2 below:
   steps 1 -st 2 -nd 3 -rd
   1: a ∗ (b ∪ c ∪ d ) f
   +
   ⇒ aaa (b ∪ c ∪ d ) f
   +
   ⇒ aaacf
   +
   ⇒ aaacf
   2: a ∗ (b ∪ c ∪ d ) f
   +
   ⇒ a ∗ cf
   +
   ⇒ aaacf
   +
   ⇒ aaacf
   At line 1, the first step takes the leftmost s.e. (a ∗ ) and chooses a 3 = aaa, whereas
   at line 2 it takes another s.e. (b ∪ c ∪ d ) and chooses c. Since such choices are
   mutuallyindependent,they canbeapplied inanyorder.Thus,bythesecondstep,we
   obtain the same r.e. aaacf
   + . The third step takes s.e. f + , chooses f , and produces
   the sentence aaacf . Since the last step is independent of the other two, it could be
   performed before, after, or between them. ?

### 2.3.2.1 Ambiguity of Regular Expressions

   The next example conceptually differs from the preceding one with respect to how
   different derivations produce the same sentence.

   Example 2.20 (ambiguous regular expression) The language over alphabet {a, b}
   such that every sentence contains one or more letters a is defined by:
   (a ∪ b) ∗ a (a ∪ b) ∗

<a id="P023"></a>

   wherethecompulsorypresenceofana isevident.Clearly,everysentencecontaining
   two or more occurrences of a can be obtained by multiple derivations, which differ
   withrespecttothecharacteridentifiedasthecompulsoryone.Forinstance,sentence
   aa offers two possibilities:
   (a ∪ b) ∗ a (a ∪ b) ∗ ⇒ (a ∪ b) a (a ∪ b) ∗ ⇒ aa (a ∪ b) ∗ ⇒ aaε = aa
   (a ∪ b) ∗ a (a ∪ b) ∗ ⇒ εa (a ∪ b) ∗ ⇒ εa (a ∪ b) ⇒ εaa = aa
   ?
   Asentence,andther.e.thatderivesit,issaidtobeambiguousif,andonlyif,itcanbe
   obtained via two structurally different derivations. Thus, sentence aa is ambiguous,
   while sentence ba is not ambiguous, because only one set of choices is possible,
   corresponding to the derivation:
   (a ∪ b) ∗ a (a ∪ b) ∗ ⇒ (a ∪ b) a (a ∪ b) ∗ ⇒ ba (a ∪ b) ∗ ⇒ baε = ba
   Ambiguousdefinitionsareasourceoftroubleinmanysettings,althoughtheymaybe
   usefulincertainapplications.Ingeneral,suchdefinitionsshouldbeavoidedalthough
   they may have the advantage of concision over unambiguous definitions. We would
   like to have an algorithm for checking if an r.e. is ambiguous, but for that we have
   to wait till Chap. 3.

   For the time being, we just state a simple sufficient condition for a sentence and
   its r.e. to be ambiguous. To this end, given an r.e. f , we number its letters and we
   obtain a numbered r.e., which we denote by f
   ? :
   f
   ?
   = (a 1 ∪ b 2 ) ∗ a 3 (a 4 ∪ b 5 ) ∗
   Notice that the numbered r.e. f
   ?
   defines a language over the numbered alphabet
   {a 1 , b 2 , a 3 , a 4 , b 5 }, in general larger than the original one.

   An r.e. f is ambiguous if the language defined by the numbered r.e. f
   ?
   associated
   to f contains two distinct strings x and y that become identical when the numbers are
   erased. For instance, the strings a 1 a 3 and a 3 a 4 of language f
   ?
   prove the ambiguity
   of the string aa of language f .

   The same criterion can be applied to an r.e. e that includes the metasymbol ε. For
   instance,considerther.e.e = (a | ε) + .Thenthenumberedr.e.ise ? = (a 1 | ε 2 ) + ,
   from which the two strings a 1 ε 2 and ε 2 a 1 can be derived that are mapped to the
   same ambiguous string a.

   Notice that the sufficient condition does not cover cases like the following. The
   r.e.

   ?
   a + | b
   ? +
   b, numbered
   ?
   a +
   1
   | b 2
   ? +
   b 3 , ambiguously derives the string aab
   with the unique numbering a 1 a 1 b 3 , yet with two distinct derivations:
   ?
   a + | b
   ? +
   b ⇒
   ?
   a + | b
   ? ?
   a + | b
   ?
   b
   +
   = ⇒ a + a + b
   +
   = ⇒ aab
   and
   ?
   a + | b
   ? +
   b ⇒
   ?
   a + | b
   ?
   b ⇒ a + b ⇒ aab

<a id="P024"></a>

   Similarly, the r.e (a ∗ | b ∗ ) c has the ambiguous derivations:
   ?
   a ∗ | b ∗
   ?
   c ⇒ a ∗ c ⇒ εc = c and
   ?
   a ∗ | b ∗
   ?
   c ⇒ b ∗ c ⇒ εc = c
   which are not detected by our condition.

   The concept of ambiguity will be thoroughly studied for grammars.


## 2.3.3 Other Operators

   When regular expressions are used in practice, it is convenient to add to the basic
   operators of union, concatenation, and star, the derived operators of power and
   cross. Moreover, for better expressivity other derived operators may be practical:
   repetition from k ≥ 0 to n > k times: [a] n
   k
   = a k ∪ a k+1 ∪ ··· ∪ a n
   option: [a] = [a] 1
   0
   = a 0 ∪ a 1 = ε ∪ a
   interval of an ordered set: the short notation (0 ... 9) represents any digit in
   the ordered set {0, 1, ..., 9}; similarly, the notations (a ... z ) and (A ... Z )
   respectively represent any lowercase/uppercase letter
   Sometimes other set operations are also used: intersection, set difference, and
   complement; the regular expressions using such operators are called extended, although
   the name is not standard and one has to specify the operators allowed case by case.
   Example 2.21 (extended r.e. with intersection) This operator provides a straightfor-
   ward formulation when a string, to be valid, must obey two or more conditions. To
   illustrate, let the alphabet be {a, b} and assume a valid string must (i) contain the
   substring bb and (ii) have an even length. Condition (i) is imposed by r.e.:
   (a | b) ∗ bb (a | b) ∗
   condition (ii) by r.e.:
   ?
   (a | b) 2
   ? ∗
   and the language is defined by the r.e. extended with intersection:
   ?
   (a | b) ∗ bb (a | b) ∗
   ?
   ∩
   ?
   (a | b) 2
   ? ∗
   Thesamelanguagecanbedefinedbyabasicr.e.withoutintersection,buttheformula
   is more complicated. It says that the substring bb can be surrounded by two strings,
   both of even length or both of odd length:
   ? (a | b) 2 ? ∗
   bb
   ? (a | b) 2 ? ∗
   | (a | b)
   ? (a | b) 2 ? ∗
   bb (a | b)
   ? (a | b) 2 ? ∗
   ?

<a id="P025"></a>

   Furthermore,itissometimessimplertodefinethesentencesofalanguageexnegativo,
   by stating a property they should not have.

   Example 2.22 (extendedr.e.withcomplement)LetLbethesetofstringsoveralpha-
   bet {a, b} not containing aa as substring. Its complement is:
   ¬L =
   ?
   x ∈ (a | b) ∗ | x contains substring aa
   ?
   easily defined by r.e. (a | b) ∗ aa (a | b) ∗ , whence an extended r.e. for L is:
   L = ¬
   ?
   (a | b) ∗ aa (a | b) ∗
   ?
   We also show a definition of language L by a basic r.e.:
   L = (ab | b) ∗ (a | ε)
   which most readers are likely to find less readable. ?
   Actually, it is no coincidence that the languages in Examples 2.21 and 2.22 admit
   also an r.e. without intersection or complement. A theoretical result, to be presented
   in Chap. 3, states that the language produced by any r.e. extended with complement
   and intersection is regular, therefore by definition can be produced by a nonextended
   r.e. as well.


## 2.3.4 Closure Properties of Family REG

   Let op be an operator to be applied to one or two languages, to produce another
   language. A language family is closed under operator op if the language, obtained
   by applying op to any languages of the family, belongs to the same family.
   Property 2.23 (closure of REG) The family REG of regular languages is closed
   under the operators of concatenation, union, and star; therefore, it is closed also
   under any derived operator, such as cross. ?
   Thisclosurepropertydescendsfromtheverydefinitionsofr.e.andoffamilyREG(p.

   20). In spite of its theoretical connotation, Property 2.23 is very relevant in practice:
   two regular languages can be combined by using the above operations, at no risk
   of losing the nice features of family REG. This will have an important practical
   consequence, to permit the compositional design of the algorithms used to check
   if an input string is valid for a language. Furthermore, we anticipate that the REG
   family is closed under intersection, complement, and reversal, too, which will be
   proved later.

   From the above property, we derive an alternative definition of family REG.

<a id="P026"></a>

   Property 2.24 (characterization of REG) The family of regular languages, REG, is
   the smallest language family such that: (i) it contains all finite languages and (ii) it
   is closed by concatenation, union, and star. ?
   Proof Bycontradiction,assumethereisasmallerfamilyF,i.e.,F ⊂ REG,thatmeets
   conditions (i) and (ii). Take any language L e ∈ REG defined by an r.e. e. Language
   L e isobtainedbystartingfromunitarylanguages,andorderlyapplyingtheoperators
   present in e. From the assumption, it follows that L e belongs also to family F.
   Then F contains any regular language and thus contradicts the strict inclusion F ⊂
   REG. ?
   We anticipate that other language families exist which are closed under the same
   operators of Property 2.23. Chief among them is the family, CF, of context-free
   languages, to be introduced soon. From Property 2.24, it follows a (strict) containment
   relation between these two families, i.e., REG ⊂ CF.

2.4 Linguistic Abstraction: Abstract and Concrete Lists
=======================================================

   If one recalls the programming languages he is familiar with, he may observe that,
   althoughsuperficiallydifferentintheiruseofkeywordsandseparators,theyareoften
   quite similar at a deeper level. By shifting focus from concrete to abstract syntax,
   we can reduce the bewildering variety of language constructs to a few essential
   structures. The verb ‘to abstract’ means:
   to consider a concept without thinking of a specific example, WordNet 2.1
   By abstracting away from the actual characters that represent a language construct,
   we perform a linguistic abstraction. This is a language transformation that replaces
   the terminal characters of the concrete language with others taken from an abstract
   alphabet. Abstract characters should be simpler and suitable to represent similar
   constructs from different artificial languages. 14
   By this approach, the abstract syntax structures of existing artificial languages
   are easily described as composition of few elementary paradigms, by means of
   standard language operations: union, iteration, and substitution (later defined). Starting
   from the abstract language, a concrete or real language is obtained by the reverse
   transformation, metaphorically also called ‘coating with syntax sugar.’
   Factoringalanguageintoitsabstractandconcretesyntaxpaysoffinseveralways.

   When studying different languages, it affords much conceptual economy. When
   designing compilers, abstraction helps for portability across different languages, if
   14 The idea of language abstraction is inspired by the research line in linguistics that aims at
   discovering the underlying similarities between human languages, disregarding morphological and
   syntactic differences.


<a id="P027"></a>

   thecompilerfunctionsaredesignedtoprocessabstract,insteadofconcrete,language
   constructs. Thus, parts of, say, a C compiler can be reused for similar languages, like
   Java.

   Since the number of abstract paradigms in use is surprisingly small, we will be
   able to present most of them in this chapter, starting from the ones conveniently
   specified by regular expressions, i.e., the lists.

   An abstract list contains an unbounded number of elements e of the same type.
   It is defined by r.e. e + , or by r.e. e ∗ if an empty list is permitted. For the time being,
   a list element is viewed as a terminal character. In later refinements, however, an
   element may be a string from another formal language; for instance, think of a list
   of numbers.


## 2.4.1 Lists with Separators and Opening/ClosingMarks

   In many real cases, adjacent elements must be divided by a string called separator,
   denoted s in abstract syntax. Thus, in a list of numbers, a separator is needed to
   delimit the end of a number and the beginning of the next one.

   A list with separators is defined by r.e. e (se) ∗ , saying that the first element e
   can be followed by zero or more pairs se. The equivalent definition (es) ∗ e differs
   by giving evidence to the last element.

   In many concrete cases, there is another requirement, intended for legibility or
   computer processing: to make the start and end of the list easily recognizable by
   respectively prefixing and suffixing some special signs: in the abstract, the initial
   character or opening mark i, and the final character or closing mark f .

   Lists with separators and opening/closing marks are defined as:
   ie (se) ∗ f
   Example 2.25 (some concrete lists) Lists are everywhere in languages, as shown by
   typical examples.

   instruction block as in
   begin instr 1 ; instr 2 ; ...; instr n end
   where instr may stand for assignment, goto, if-statement, write-statement, etc.;
   corresponding abstract and concrete terms are:
   abstract alphabet concrete alphabet
   i begin
   e instr
   s ;
   f end

<a id="P028"></a>

   procedure parameters as in
   procedureWRITE ( par 1 , par 2 , … par n )
   i e s e ... ... e f
   Furthermore, should an empty parameter list be legal, as in a declaration like
   procedureWRITE(), the r.e. becomes i
   ?
   e (se) ∗
   ?
   f
   array definition as in
   array MATRIX [ int 1 , int 2 , … int n ]
   i e s e ... ... e f
   where each int is an interval such as 10 … 50 ?

## 2.4.2 Language Substitution

   The above examples have illustrated the mapping from concrete to abstract symbols.
   Moreover, language designers find it useful to work by stepwise refinement, as done
   in any branch of engineering when a complex system is divided into its components,
   atomic or otherwise. To this end, we introduce the new language operation of
   substitution, which replaces a terminal character of a language called the source, with
   a sentence of another language called the target. As always, the source alphabet is
   Σ and the source language is L ⊆ Σ ∗ . Consider a sentence of L containing one or
   more occurrences of a source character b:
   x = a 1 a 2 ... a n ∈ L where a i = b for some 1 ≤ i ≤ n
   Let Δ be another alphabet, called target, and L b ⊆ Δ ∗ be the image language of b.
   The substitution of language L b for b in string x produces a set of strings:
   {y| y = y 1 y 2 ... y n ∧ if a i ?= b then y i = a i else y i ∈ L b }
   whicharealanguageoverthealphabet(Σ \ {b}) ∪ Δ.Noticethatallthecharacters
   other than b are left unchanged.

   By the usual approach, we can extend the substitution operation from one string
   to the whole source language, by applying it to every source sentence.

   Example 2.26 (Example 2.25 continued) Resuming the case of a parameter list, the
   abstract syntax is:
   ie (se) ∗ f
   and the substitutions to be applied are tabulated below:

<a id="P029"></a>

   abstract char. image language
   i L i = procedure ?procedure identifier? ‘(’
   e L e = ?parameter identifier?
   s L s = ‘,’
   f L f = ‘)’
   Thus, the opening mark i is replaced with a string of language L i , where the
   procedure identifier has to agree with the reference manual of the programming
   language. Clearly, the target languages of such substitutions depend on the ‘syntax
   sugar’ of the concrete language intended for. Notice that the four above substitutions
   are independent of one another and can be applied in any order. ?
   Example 2.27 (identifiers with dash) In certain programming languages, long
   mnemonic identifiers can be constructed by appending alphanumeric strings
   separated by a dash: thus, string LOOP3-OF-35 is a legal identifier. More precisely, the
   leftmost word, LOOP3, must initiate with a letter, the others may also initiate with a
   digit; and adjacent dashes, as well as a trailing dash, are forbidden.

   As a first approximation, each language sentence is a nonempty list of elements
   e, which are alphanumeric words, separated by a dash:
   e (‘_’e) ∗
   However, the first word must be different from the others and may be taken to be the
   opening mark i of a possibly empty list:
   i (‘- ’e) ∗
   By substituting to i the language (A ... Z ) (A ... Z | 0 ... 9) ∗ , and to e the
   language (A ... Z | 0 ... 9) + , the final r.e. is obtained. ?
   Thisisanoverlysimpleinstanceofsyntaxdesignbyabstractionandstepwiserefine-
   ment, a method to be further developed now and after the introduction of grammars.
   Other language transformations are studied in Chap. 5.


## 2.4.3 Hierarchical or Precedence Lists

   A frequently recurrent construct is a list such that each element is in turn a list of a
   different type. The first list is attached to level 1, the second to level 2, and so on.
   However, the present abstract paradigm, called hierarchical list, is restricted to lists
   having a bounded number of levels. The case of unbounded levels is studied later by
   using grammars, under the name of nested structures.

   A hierarchical list is also called a list with precedence, because the list at level
   l ≥ 2 bounds its elements more strongly than the list at level l − 1. In other words,

<a id="P030"></a>

   the elements at higher level, i.e., with higher precedence (or more priority), must be
   assembled into a list, and each such list becomes an element at the next lower level,
   i.e., with lower priority.

   Ofcourse,eachlevelmayhaveitsownopening/closingmarksandseparator.Such
   delimiters are usually distinct level by level, in order to avoid confusion.
   The structure of a hierarchical list with k ≥ 2 levels is:
   hierarchy of lists level precedence/priority
   list 1 = i 1 list 2 (s 1 list 2 ) ∗ f 1 1 lowest/min
   list 2 = i 2 list 3 (s 2 list 3 ) ∗ f 2 2
   … = …
   list k = i k e k (s k e k ) ∗ f k k highest/max
   Inthisschema,onlythehighestlevelmaycontainatomicelements,e k .Butacommon
   variant permits, at any level l (1 ≤ l < k), atomic elements e l to occur side by side
   with lists of level l + 1. A few concrete examples follow.

   Example 2.28 (two hierarchical lists) Structures in programming languages:
   block of print instructions as in
   begin instr 1 ; instr 2 ; … instr n end
   where instr is a print instruction, e.g.,WRITE (var 1 ,var 2, …,var n) , i.e., a list (from
   Example 2.25); there are two levels (k = 2):
   level 1 (low) list of instructions instr separated by semicolon ‘;’, opened by
   begin and closed by end
   level 2 (high) list of variables var separated by comma ‘,’, with i 2 = WRITE‘(’
   and f 2 = ‘)’
   arithmetic expression without parentheses the precedences of arithmetic 
   operators determine how many levels the hierarchy has; for instance, the operators
   ‘×’, ‘÷’ and ‘+’, ‘−’ are layered on two levels (k = 2) and the expression:
   term
   3
   n
   +
   term
   5
   n
   × 7
   n
   × 4
   ...

   −
   term
   8 × 2 ÷ 5
   .....................

   +
   term
   8 +
   term
   3
   is a two-level list, with neither opening nor closing marks; at level 1, we find a
   list of terms term = list 2 , separated by signs ‘+’ and ‘−’, i.e., by low precedence
   operators; at level 2 we see a list of numbers n = e 2 , separated by signs ‘×’ and
   ‘÷’, i.e., by high precedence operators; one may go further and introduce a level
   3 with an exponentiation sign ‘∗∗’ as separator ?

<a id="P031"></a>

   Ofcourse,hierarchicalstructuresareomnipresentinnaturallanguagesaswell.Think
   of a list of nouns:
   father,mother,son and daughter
   Here we may observe a difference with respect to the abstract paradigm: the 
   penultimate element uses a distinct separator and, possibly in order to warn the listener
   of an utterance that the list is approaching the end. Furthermore, the items in the list
   may be enriched by second-level qualifiers, such as a list of adjectives.

   In all sorts of documents and written media, hierarchical lists are extremely
   common. For instance, a book is a list of chapters, separated by white pages, between a
   front and back cover. A chapter is a list of sections, a section is a list of paragraphs,
   and so on.

2.5 Context-Free Generative Grammars
====================================

   We start the study of the family of context-free languages, which plays the
   central role in compilation. Initially invented by linguists for natural languages in the
   1950s,context-freegrammarshaveprovedthemselvesextremelyusefulincomputer
   science applications: all existing technical languages have been defined using such
   grammars. Moreover, since the early 1960s, efficient algorithms have been found to
   analyze, recognize, and translate sentences of context-free languages. This chapter
   presentstherelevantpropertiesofcontext-freelanguages,illustratestheirapplication
   by many typical cases, and compares and combines them with regular languages.
   At last, we position the context-free model in the classical hierarchy of grammars
   and computational models due to Chomsky, and we briefly examine some 
   contextsensitive models.


## 2.5.1 Limits of Regular Languages

   Regular expressions are very practical for describing lists and related paradigms
   but fall short of the capacity needed to define other frequently occurring constructs.
   A relevant case are the block structures (or nested parentheses) present in many
   technical languages, schematized by:
   begin begin begin … end
   1-st inner block
   begin … end
   2-nd inner block
   … end
   outer block
   end
   Example 2.29 (simple block structure) Let the alphabet be shortened to {b, e} and
   consider a quite limited case of nested structure, such that all opening marks precede
   all closing marks. Clearly, opening/closing marks must have identical count:
   L 1 =
   ?
   b n e n | n ≥ 1
   ?

<a id="P032"></a>

   We argue that language L 1 cannot be defined by a regular expression, but we have
   to defer the formal proof to a later section. In fact, since each string must have all
   letters b left of any letter e, either we write an overly general r.e. such as b + e + ,
   which unfortunately defines illegal strings like b 3 e 5 , or we write a too restricted r.e.
   that exhaustively lists a finite sample of strings up to a bounded length. On the other
   hand, if we comply with the condition that the count of the two letters is the same
   by writing (be) + , illegal strings like bebe creep in. ?
   For defining this and other useful languages, regular or not, we move to the formal
   model of generative grammars.


## 2.5.2 Introduction to Context-Free Grammars

   A generative grammar or syntax 15 is a set of simple rules that can be repeatedly
   applied in order to generate all and only the valid strings.

   Example 2.30 (palindromes) The language L, over the alphabet Σ = {a, b}, is first
   defined, using the reversal operation, as:
   L =
   ?
   uu R | u ∈ Σ ∗ ? = {ε, aa, bb, abba, baab, ..., abbbba, ...}
   It contains strings having specular symmetry, called palindromes, of even-length.
   The following grammar G contains three rules:
   pal → ε pal → apala pal → bpalb
   The arrow ‘→’ is a metasymbol, exclusively used to separate the left part of a rule
   from the right part.

   To derive the strings, just replace the symbol pal, called nonterminal, with the
   right part of a rule, for instance
   pal ⇒ apala ⇒ abpalba ⇒ abbpalbba ⇒ ...

   The derivation process can be chained and terminates when the last string obtained
   no longer contains a nonterminal symbol. At that moment, the generation of the
   sentence is concluded. We complete the derivation:
   abbpalbba ⇒ abbεbba = abbbba
   Incidentally, the language of palindromes is not regular.

   15 Sometimes the term grammar has a broader connotation than syntax, as when some rules for
   computing the meaning of sentences are added to the rules for enumerating them. When necessary,
   the intended meaning of the term will be made clear.


<a id="P033"></a>

   Next,weenrichtheexampleintoalistofpalindromesseparatedbycommas,exem-
   plified by sentence ‘abba, bbaabb, aa’. The grammar adds two list-generating
   rules to the previous ones:
   list → pal ‘,’ list
   list → pal
   pal → ε
   pal → a pal a
   pal → b pal b
   The top left rule says: the concatenation of palindrome, comma, and list produces a
   (longer) list; notice that the comma is between quotes, to stress that it is a terminal
   symbol like a and b. The other rule says that a list can be made of one palindrome.
   Now there are two nonterminal symbols: list and pal. The former is termed axiom
   because it defines the intended language. The latter defines certain component
   substrings, also called constituents of the language, the palindromes. ?
   Example 2.31 (metalanguage of regular expressions) A regular expression that
   defines a language over a fixed terminal alphabet, say Σ = {a, b}, is a formula,
   i.e.,astringoverthealphabetΣ r.e. = {a, b, ‘ ∪ ’, ‘ ∗ ’, ‘ε’, ‘ ( ’, ‘)’},wherethe
   quotes around the r.e. metasymbols indicate their use as grammar terminals (con-
   catenation ‘·’ could be added as well). Such formulas in turn are the sentences of the
   language L r.e. defined by the next grammar. Looking back to the definition of r.e. on
   p.18, we write the rules of grammar G r.e. :
   1: expr → ‘ε’
   2: expr → a
   3: expr → b
   4: expr → ‘(’ expr ‘∪’ expr ‘)’
   5: expr → ‘(’ expr expr ‘)’
   6: expr → ‘(’ expr ‘)’ ‘∗’
   where numbering is only for reference. A derivation is (quotes are omitted):
   expr ⇒
   4
   (expr ∪ expr) ⇒
   5
   ((expr expr) ∪ expr) ⇒
   2
   ((a expr) ∪ expr)
   ⇒
   6
   ??
   a (expr) ∗
   ?
   ∪ expr
   ?
   ⇒
   4
   ??
   a ((expr ∪ expr)) ∗
   ?
   ∪ expr
   ?
   ⇒
   2
   ??
   a ((a ∪ expr)) ∗
   ?
   ∪ expr
   ?
   ⇒
   3
   ??
   a ((a ∪ b)) ∗
   ?
   ∪ expr
   ?
   ⇒
   3
   ??
   a ((a ∪ b)) ∗
   ?
   ∪ b
   ?
   = e
   Since the last string can be interpreted as an r.e. e, it denotes a second language L e
   over alphabet Σ, which is the set of the strings starting with letter a, plus string b:
   L e = {a, b, aa, ab, aaa, aba, ...} ?
   A word of caution: this example displays two language levels, since the syntax
   defines certain strings to be understood as definitions of other languages. To avoid

<a id="P034"></a>

   terminologicalconfusion,wesaythatthesyntaxG r.e. staysatthemetalinguisticlevel,
   that is, over the linguistic level, or, equivalently, that the syntax is a metagrammar.
   To set the two levels apart, it helps to consider the alphabets: at meta-level the
   alphabet is Σ r.e. = {a, b, ‘ ∪ ’, ‘ ∗ ’, ‘ε’, ‘ ( ’, ‘)’}, whereas the final language
   has alphabet Σ = {a, b}, devoid of metasymbols.

   Ananalogywithhumanlanguagemayalsoclarifytheissue.AgrammarofRussian
   can be written, say, in English. Then it contains both Cyrillic and Latin characters.
   HereEnglishisthemetalanguageandRussianthefinallanguage,whichonlycontains
   Cyrillic characters. Another example of language versus metalanguage is provided
   by XML, the metanotation used to define a variety of Web document types such as
   HTML.

   Definition 2.32 (context-free grammar) A context-free (CF) (or type 2 or BNF 16 )
   grammar G is defined by four entities:
   V nonterminal alphabet, a set of symbols termed nonterminals or 
   metasymbols
   Σ terminal alphabet, a set of symbols termed terminals, disjoint from the
   preceding set
   P a set of syntactic rules (or productions)
   S ∈ V a particular nonterminal termed axiom
   A rule of set P is an ordered pair X → α, with X ∈ V and α ∈ (V ∪ Σ ) ∗ . Two or
   more rules (n ≥ 2):
   X → α 1 X → α 2 ... X → α n
   with the same left part X can be concisely grouped in:
   X → α 1 | α 2 | ··· | α n or X → α 1 ∪ α 2 ∪ ··· ∪ α n
   We say that the strings α 1 , α 2 , …, α n are the alternatives of X. ?

## 2.5.3 ConventionalGrammar Representations

   To prevent confusion, the metasymbols ‘→’, ‘|’, ‘∪’, and ε should not be used for
   terminal or nonterminal symbols (Example 2.31 is a motivated exception).
   Moreover, the terminal and nonterminal alphabets must be disjoint, i.e., Σ ∩ V = ∅.
   16 Type 2 comes from Chomsky classification (p. 105). Backus Normal Form, or also Backus–
   Naur Form, comes from the names of John Backus and Peter Naur, who pioneered the use of such
   grammars for defining programming languages.


<a id="P035"></a>

   Table 2.2 Different typographic styles for writing grammar rules
   Nonterminal Terminal Example
   A compound word between
   angle brackets, e.g.,
   ?if sentence?
   A word written as it is,
   without any special marks
   ?if sentence? → if ?cond?
   then
   ?sentence?
   else
   ?sentence?
   A word written as it is,
   without any special marks; it
   may not contain blank spaces;
   e.g., sentence or sentence_list
   A word written in bold, in
   italic or quoted, e.g., then,
   then or ‘then’
   if_sentence → if cond then
   sentence else sentence or
   if_sentence → if cond then
   sentence else sentence or
   if_sentence → ‘if’ cond ‘then’
   sentence ‘else’ sentence
   An uppercase Latin letter A word written as it is,
   without any special marks
   F → if C then D else D
   In professional and scientific practice, different styles are used to represent 
   terminals and nonterminals, as specified in Table 2.2. In the first style, the grammar of
   Example 2.30 becomes:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   ?sentence? → ε
   ?sentence? → a ?sentence? a
   ?sentence? → b ?sentence? b
   Alternative rules may be grouped together:
   ?sentence? → ε | a ?sentence? a | b ?sentence? b
   If a technical grammar is large, of the order of a few hundred rules, it should be
   written with care, to facilitate searching for specific definitions, making changes and
   cross referencing. The nonterminals should be identified by self-explanatory names,
   and the rules should be divided into sections and be numbered for reference. On the
   other hand, in very simple examples the third style of Table 2.2 is more suitable, that
   is, to have disjoint short symbols for terminals and nonterminals.

   In this book, for simplicity we often adopt the following style:
   • lowercase Latin letters near the beginning of the alphabet {a, b, ...} for terminal
   characters
   • uppercase Latin letters {A, B, ..., Z } for nonterminal symbols
   • lowercase Latin letters near the end of the alphabet {..., r, s, ..., z } for strings
   over the alphabet Σ, i.e., including only terminals
   • lowercaseGreekletters{α, β, ..., ω }forthestringsoverthecombinedalphabet
   V ∪ Σ

<a id="P036"></a>

   Table 2.3 Classification of grammar rule forms
   Class Description Model
   Terminal Either RP contains only terminals
   or is the empty string
   → u | ε
   Empty or null RP is the empty string → ε
   Initial or axiomatic LP is the grammar axiom S →
   Recursive LP occurs in RP A → αAβ
   Self-nesting or self-embedding LP occurs in RP between nonempty
   strings
   A → αAβ
   Left-recursive LP is the prefix of RP A → Aβ
   Right-recursive LP is the suffix of RP A → β A
   Left-right-recursive or two-side
   recur.

   The two cases above together A → Aβ A
   Copy, renaming, or categorization RP consists of one nonterminal A → B
   Identity LP and RP are identical A → A
   Linear RP contains at most one
   nonterminal (the rest is terminal)
   → uBv | w
   Left-linear or type 3 As the linear case above, but the
   nonterminal is prefix
   → Bv | w
   Right-linear or type 3 As the linear case above, but the
   nonterminal is suffix
   → uB | w
   Homogeneous normal RP consists of either n ≥ 2
   nonterminals or one terminal
   → A 1 ... A n | a
   Chomsky normal or homogeneous
   of degree 2
   RP consists of either two
   nonterminals or one terminal
   → BC | a
   Greibach normal or real-time RP is one terminal possibly
   followed by nonterminals
   → aσ | b
   Operator form RP consists of two nonterminals
   separated by one terminal, which is
   called operator (more generally, RP
   is devoid of adjacent nonterminals)
   → AaB
   Rule Types
   Ingrammarstudies,therulesmaybeclassifieddependingontheirform,withtheaim
   of making the study of language properties more immediate. For future reference, in
   Table 2.3 we list some common rule forms along with their technical names. Each
   rule form is next schematized, with symbols adhering to the following stipulations:
   a and b are terminals; u, v, and w denote strings of terminals and may be empty; A,
   B, and C are nonterminals; α and β denote strings containing a mix of terminals and
   nonterminals, and may be empty; lastly, σ denotes a string of nonterminals.
   The classification of Table 2.3 is based on the form of the right part RP of a rule,
   except the recursive rules, which may also consider the left part LP. We omit all the
   rule parts that are irrelevant for the classification. By the Chomsky classification,

<a id="P037"></a>

   the left-linear and right-linear forms are also known as type 3 grammars. Most rule
   forms listed in Table 2.3 will occur in the book, and the remaining ones are listed for
   general reference.

   We will see that some of the grammar forms can be forced on any given grammar
   and leave the language unchanged. Such forms are called normal.


## 2.5.4 Derivation and Language Generation

   Wereconsiderandformalizethenotionofstringderivation.Letβ = δ Aη beastring
   containinganonterminalA,whereδandηareanystrings,possiblyempty.LetA → α
   be a rule of grammar G and let γ = δ αη be the string obtained by replacing the
   nonterminal A in β with the rule right part α.

   The relation between two such strings is called derivation. We say that string β
   derives string γ for grammar G, and we write
   β = ⇒
   G
   γ
   or simply β ⇒ γ when the grammar name is understood. Rule A → α is applied in
   such a derivation and string α reduces to nonterminal A.

   Now consider a chain of derivations of length n ≥ 0:
   β 0 ⇒ β 1 ⇒ ··· ⇒ β n
   which can be shortened to:
   β 0
   n
   = ⇒ β n
   Ifn = 0,foreverystringβ wepositβ
   0
   = ⇒ β,thatis,thederivationrelationisreflexive.

   To express derivations of any length, we write:
   β 0
   ∗
   = ⇒ β n or β 0
   +
   = ⇒ β n
   if the length of the chain is n ≥ 0 or n ≥ 1, respectively.

   The language generated or defined by a grammar G, starting from nonterminal
   A, is the set of the terminal strings that derive from nonterminal A in one or more
   steps, in formula:
   L A (G) =
   ?
   x ∈ Σ ∗ | A
   +
   = ⇒ x
   ?
   If the nonterminal is the axiom S, we have the language generated by G:
   L(G) = L S (G) =
   ?
   x ∈ Σ ∗ | S
   +
   = ⇒ x
   ?
   Sometimes, we need to consider the derivations that produce strings still containing
   nonterminals. A string form generated by a grammar G, starting from nonterminal
   A ∈ V, is a string α ∈ (V ∪ Σ ) ∗ such that it holds A
   ∗
   = ⇒ α. In particular, if 
   nonter38 2 Syntax
   minal A is the axiom, the string form is termed sentential form. Clearly, a sentence
   is a sentential form devoid of nonterminals.

   Example 2.33 (book structure) The grammar G l below defines the structure of a
   book. The book contains a front page f and a nonempty series of chapters, which is
   derivedfromnonterminalA.Eachchapterstartswithatitlet andcontainsanonempty
   series of lines l, which is derived from nonterminal B. Grammar G l :
   G l
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → f A
   A → At B | t B
   B → l B | l
   We list a few derivations. From nonterminal A, it derives the string form t Bt B and
   thestringt l l t l ∈ L A (G l ).FromtheaxiomS,itderivesthesententialformsf At l B
   and f t Bt B, and the sentence f t l t l l l.

   The language generated starting from nonterminal B is L B (G l ) = l + . The
   language L(G l ) generated by grammar G l is also defined by r.e. f
   ?
   t l +
   ? + , and this
   shows that the language is in the family REG. In fact, language L(G l ) is a case of
   an abstract hierarchical list. ?
   A language is context-free if there exists a context-free grammar that generates it.
   The empty set (or language) ∅ is context-free, too. The family of all the context-free
   languages (including ∅) is denoted by CF. Two grammars G and G ? are equivalent
   if they generate the same language, i.e., L(G) = L(G ? ).

   Example 2.34 (equivalent grammars) The next grammar G l2 is clearly equivalent
   to the grammar G l of Example 2.33:
   G l2
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → f X
   X → X t Y | t Y
   Y → l Y | l
   since the only change affects the way nonterminals are identified. Also the following
   grammar G l3 :
   G l3
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → f A
   A → At B | t B
   B → Bl | l
   is equivalent to G l . The only difference from G 1 is in the third row, which defines
   nonterminal B by a left-recursive rule, instead of the right-recursive rule of G l .
   Clearly, the two collections of derivations of any length n ≥ 1:
   B
   n
   =⇒
   G l
   l n and B
   n
   =⇒
   G l3
   l n
   generate the same language L B (G l ) = L B (G l3 ) = l + . ?

<a id="P039"></a>


## 2.5.5 Erroneous Grammars and Useless Rules

   Whenwritingagrammar,attentionshouldbepaidtomakesurethatallthenontermi-
   nals are well-defined and each of them effectively helps to produce some sentence.
   Without that, some rules may turn out to be unproductive.

   A grammar G is called clean (or reduced) under the following conditions:
   1. every nonterminal A is reachable from the axiom, that is, there exists a derivation
   S
   ∗
   = ⇒ αAβ
   2. every nonterminal A is well-defined, that is, it generates a nonempty language,
   i.e., L A (G) ?= ∅
   It is often straightforward to check by inspection whether a grammar is clean. The
   following algorithm formalizes the checks.


### 2.5.5.1 Grammar Cleaning

   The grammar cleaning algorithm operates in two phases. First, it pinpoints the
   undefined nonterminals and eliminates the rules that contain such nonterminals; second,
   it identifies the unreachable nonterminals and eliminates the rules containing them.
   phase 1 ComputethesetDEF ⊆ V ofthewell-definednonterminals.ThesetDEF
   isinitializedwiththenonterminalsthatoccurintheterminalrules,theruleshaving
   a terminal string as their right part:
   DEF :=
   ?
   A| (A → u) ∈ P with u ∈ Σ ∗
   ?
   Then this transformation is repeatedly applied until convergence is reached:
   DEF := DEF ∪ {B| (B → D 1 ... D n ) ∈ P ∧ ∀i D i ∈ (Σ ∪ DEF )}
   NoticethateachsymbolD i (1 ≤ i ≤ n)isaterminalinΣ oranonterminalalready
   present in DEF. At each iteration, two outcomes are possible:
   • a new nonterminal is found that occurs as left part of a rule having as right
   part a string of terminals or well-defined nonterminals
   • else the termination condition is reached
   The nonterminals that belong to the complement set V \ DEF are undefined and
   are eliminated by the algorithm together with any rule where they occur.

   phase 2 A nonterminal is reachable from the axiom if, and only if, there exists a
   path in the following directed graph (called reachability graph), which represents

<a id="P040"></a>

   a binary relation (called produce) between nonterminals:
   A
   produce
   −−−−→ B
   Such a relation says that a nonterminal A produces a nonterminal B if, and only
   if, there exists a rule A → αBβ, where α and β are any strings.

   Clearly, a nonterminal C is reachable from the axiom S if, and only if, in the
   graph there is a path directed from S to C. The unreachable nonterminals are the
   complementwithrespecttothenonterminalalphabetV.Thealgorithmeliminates
   them together with the rules where they occur, because they do not generate any
   sentence.

   Notice that if a grammar generates the empty language ∅, all rules and nonterminals,
   including the axiom, are deleted by the algorithm. Therefore, no clean grammar
   exists to generate the empty language.

   Quite often the requirement below is added to the cleanness conditions:
   • the grammar must not permit any circular derivation A
   +
   = ⇒ A
   Infact,suchderivationsareinessentialbecause,ifastringx isobtainedbymeansofa
   circular derivation, e.g., A ⇒ A ⇒ x, it can also be obtained by means of the shorter
   noncircular derivation A ⇒ x. Moreover, circular derivations cause ambiguity, a
   negative phenomenon discussed in Sect. 2.5.10.

   An identity rule (see Table 2.3) permits a one-step circular derivation. Thus, such
   a rule is inessential (and ambiguous) and should be canceled from the grammar. In
   this book, we assume grammars are always clean and noncircular.

   Example 2.35 (unclean grammars)
   • the grammar {S → aAS b, A → b} is unclean and does not generate any
   sentence, that is, it generates the empty set ∅
   • grammar {S → a, A → b} has an unreachable nonterminal A; the same (finite)
   language {a} is generated by the clean grammar {S → a}
   • grammar {S → aAS b | A, A → S | c} has a circular derivation S ⇒ A ⇒ S;
   the noncircular grammar {S → aS S b | c} is equivalent
   • notice that circularity may also come from the presence of an empty rule, as for
   instance in the following grammar fragment:
   ?
   X → X Y | ...

   Y → ε | ...

   circular derivation: X ⇒ X Y ⇒ X
   ?
   Finally,weobservethatagrammar,althoughclean,maystillcontainredundantrules,
   as in the next example.


<a id="P041"></a>

   Example 2.36 (grammar with redundant rules)
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   1: S → aAS b
   2: S → aBS b
   3: S → ε
   ?
   4: A → c
   5: B → c
   One of the rule pairs (1, 4) and (2, 5), which generate exactly the same sentences,
   should be canceled. ?

## 2.5.6 Recursion and Language Infinity

   Anessentialpropertyofmosttechnicallanguagesistobeinfinite.Westudyhowthis
   property follows from the grammar rule forms. In order to generate an unbounded
   number of strings, the grammar has to derive strings of unbounded length. To this
   end, recursive rules are necessary, as next argued.

   An n-step derivation of the form A
   n
   = ⇒ xAy (n ≥ 1) is called recursive, or
   immediately recursive if n = 1. Similarly, the nonterminal A is called recursive. If the
   strings x or y are empty, the recursion is termed left or right, respectively.
   Property 2.37 (language infinity) Let grammar G be clean and devoid of circular
   derivations.ThenlanguageL(G)isinfiniteif,andonlyif,grammarG hasarecursive
   derivation. ?
   Proof Clearly,withoutrecursionanyderivationofgrammarG hasaboundedlength;
   therefore, language L(G) would be finite.

   Conversely, assume grammar G offers a recursive derivation A
   n
   = ⇒ xAy (n ≥ 1),
   where, because of the hypothesis of noncircularity, not both strings x and y may be
   empty.ThenthereexistsaderivationA
   +
   = ⇒ x m Ay m foreverym ≥ 1.Bythecleanness
   hypothesis of grammar G, nonterminal A can be reached from the axiom through a
   derivation S
   ∗
   = ⇒ uAv, and by the same reason also nonterminal A derives at least one
   terminal string A
   +
   = ⇒ w. When we combine these derivations, we obtain a derivation
   for every m ≥ 1:
   S
   ∗
   = ⇒ uAv
   +
   = ⇒ ux m Ay m v
   +
   = ⇒ ux m wy m v with m ≥ 1
   and such derivations in all generate an infinite language. ?
   In order to see whether a grammar has recursions, we reuse the binary relation
   produce on p.40: a grammar does not have any recursions if, and only if, the relation
   graph does not contain any circuits.

   We illustrate Property 2.37 by means of two grammars that generate a finite
   language and an infinite one (arithmetic expressions), respectively.


<a id="P042"></a>

   Example 2.38 (finite language) Consider the grammar reported below with its
   derivations:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → aBc
   B → ab | C a
   C → c
   derivations
   S ⇒ aBc ⇒ aabc
   S ⇒ aBc ⇒ aC ac ⇒ acac
   This grammar does not have any recursion and allows just two derivations, which
   define the finite language {aabc, acac}. ?
   The next example is a most common paradigm of so many artificial languages. It
   will be replicated and transformed over and over in the book.

   Example 2.39 (arithmetic expressions) The grammar G below:
   G =
   ?
   {E, T, F }, {i, +, ×, ‘)’, ‘ ( ’}, P, E
   ?
   has this rule set P:
   P
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   E → E + T | T
   T → T × F | F
   F → ‘ ( ’E ‘)’ | i
   The language of G:
   L(G) = {i, i + i + i, i × i, (i + i) × i, ...}
   is the set of the arithmetic expressions over letter i, with the signs of sum ‘+’
   and product ‘×’, and parentheses ‘( )’. Nonterminal F (factor) is recursive but not
   immediately. Nonterminals T (term) and E (expression) are immediately recursive,
   both to the left. Such properties are evident from the circuits in the (reachability)
   graph of the produce relation of G:
   E T F
   produce produce
   produce
   produce produce
   Since grammar G is clean and noncircular, language L(G) is infinite. ?

<a id="P043"></a>


## 2.5.7 SyntaxTree and CanonicalDerivation

   The derivation process can be visualized as a syntax tree, for better legibility. A tree
   is a directed ordered graph that does not contain any circuit, such that every node
   pair is connected by exactly one directed path. An arc N 1 → N 2 defines a father (or
   parent) node–sibling (or child) node relation, customarily drawn from top to bottom
   as in a genealogical tree. The siblings of a node are ordered from left to right. The
   degree (also called arity) of a node is the number of its siblings. A tree contains only
   one node without parent, termed root.

   ConsideranodeN:thesubtreeofN isthetreethathasnodeN asrootandcontains
   all the siblings of N, all of their siblings, etc., that is, all the descendants of N. The
   nodes without siblings are termed leaves or terminal nodes. The sequence of all the
   leaves, read from left to right, is the frontier of the tree. A syntax tree has the axiom
   as root and a sentence as frontier.

   Toconstructatree,consideraderivation.ForeachruleA 0 → A 1 A 2 ... A r (r ≥ 1)
   used in the derivation, draw a small tree (or elementary tree) that has A 0 as root and
   A 1 A 2 ... A r as siblings, which may be terminals or nonterminals. If the rule is
   A 0 → ε, draw one sibling labeled with ε. Such elementary trees are then pasted
   together, by uniting each nonterminal sibling, say A i , with the root node that has the
   same label A i , which is used to expand A i in the subsequent derivation step.
   Example 2.40 (syntax tree) The arithmetic expression grammar (Example 2.39) is
   reproduced in Fig. 2.1, and the rules are numbered for reference in the syntax tree
   construction. The derivation below:
   E ⇒
   1
   E + T ⇒
   2
   T + T ⇒
   4
   F + T ⇒
   6
   i + T ⇒
   3
   i + T × F (2.3)
   ⇒
   4
   i + F × F ⇒
   6
   i + i × F ⇒
   6
   i + i × i
   corresponds to the following syntax tree:
   E 1
   E 2
   T 4
   F 6
   i
   + T 3
   T 4
   F 6
   i
   × F 6
   i
   frontier =
   = i + i × i

<a id="P044"></a>

   grammar rule elementary tree
   1: E → E + T
   E
   E + T
   2: E → T
   E
   T
   3: T → T × F
   T
   T × F
   4: T → F
   T
   F
   5: F → ‘(’E ‘)’
   F
   ( E )
   6: F → i
   F
   i
   Fig.2.1 Rules of arithmetic expressions (Example 2.39) and corresponding elementary trees
   where the labels of the rules applied are displayed. The frontier, evidenced by the
   dashed line, yields the sentence i + i × i.

   Noticethatthesametreerepresentsotherequivalentderivations,likethenextone:
   E ⇒
   1
   E + T ⇒
   3
   E + T × F ⇒
   6
   E + T × i ⇒
   4
   E + F × i (2.4)
   ⇒
   6
   E + i × i ⇒
   2
   T + i × i ⇒
   4
   F + i × i ⇒
   6
   i + i × i
   andmanyothers,whichdifferfromoneanotherintheruleapplicationorder.Deriva-
   tions (2.3) and (2.4) are termed left and right, respectively. ?
   A syntax tree of a sentence x can be encoded in a text, by enclosing each subtree
   between brackets (or parentheses), which may be subscripted with the subtree root

<a id="P045"></a>

   symbol. 17 Thus, the previous tree is encoded by the parenthesized expression:
   ? ?
   [[i] F ] T
   ?
   E
   +
   ?
   [[i] F ] T × [i] F
   ?
   T
   ?
   E
   or by the graphical variant:
   i
   F
   T
   E
   + i
   F
   T
   × i
   F
   T
   E
   Therepresentationcanbesimplifiedbydroppingthenonterminallabels,thusobtain-
   ing a skeleton tree (left):
   ?
   ?
   ?
   ?
   i
   +
   ?
   ?
   ?
   i
   ×
   ?
   i
   ?
   ?
   i
   +
   ?
   ?
   i
   ×
   ?
   i
   skeleton tree condensed skeleton tree
   or the corresponding parenthesized string (without subscripts):
   ? ?
   [[i]]
   ?
   +
   ?
   [[i]] × [i]
   ? ?
   A further simplification of the skeleton tree consists of shortening the nonbifurcat-
   ing paths, thus resulting in the condensed skeleton tree (right). The nodes fused
   together represent the grammar copy rules. The corresponding parenthesized
   sentence is (without subscripts):
   ?
   [i] +
   ?
   [i] × [i]
   ? ?
   Example 2.41 (abstract syntax tree) Within compilers, an arithmetic expression
   is sometimes represented by a simplified tree, called abstract syntax tree, wherein
   all the grammatical details, e.g., nonterminals, are omitted and only the operator
   structure is retained. Three possible representations are shown below:
   17 It is necessary to assume that the brackets are not in the terminal alphabet.

<a id="P046"></a>

   +
   i ×
   i i
   ×
   (+)
   i i i
   i
   or
   simply
   ×
   +
   i i i
   i
   i + i × i (i + i + i) × i
   Thetreenodesarelabeledbyoperators,andtheirparent–siblingrelationshipdirectly
   models the expression structure. The subexpression parentheses are optional in the
   abstract tree, whereas they are necessary for correctly computing the value when the
   expression is in string form (above right).

   Later we will see that also regular expressions can be usefully represented by
   abstract syntax trees. ?
   Some approaches tend to view a grammar as a device for assigning structure to
   sentences. From this standpoint, a grammar defines a set of syntax trees, that is, a
   tree language instead of a string language. 18

### 2.5.7.1 Left and Right Derivations

   A derivation of p ≥ 1 steps:
   β 0 ⇒ β 1 ⇒ ··· ⇒ β p
   where
   β i = δ i A i η i and β i+1 = δ i α i η i with 0 ≤ i ≤ p − 1
   is called left (also leftmost) or right (also rightmost), if it holds δ i ∈ Σ ∗ or η i ∈ Σ ∗ ,
   respectively, for every 0 ≤ i ≤ p − 1.

   In words, at each step a left derivation or a right one expands the rightmost
   nonterminal or the leftmost one, respectively. A letter l or r may be subscripted to
   the arrow sign ‘⇒’, to make explicit the derivation order.

   Notice there exist other derivations that are neither left nor right, because the
   nonterminal symbol expanded is not always either leftmost or rightmost, or because
   at some step it is leftmost and at some other step it is rightmost.

   18 For an introduction to the theory of tree languages, their grammars, and automata, see [3,4].

<a id="P047"></a>

   Returning to Example 2.40 on p.43, derivation (2.3) is left and is denoted by
   E
   +
   = ⇒
   l
   i + i + i × i, derivation (2.4) is right, whereas the derivation (2.5) below:
   E =⇒
   l,r
   E + T = ⇒
   r
   E + T × F = ⇒
   l
   T + T × F = ⇒ T + F × F (2.5)
   = ⇒
   r
   T + F × i = ⇒
   l
   F + F × i = ⇒
   r
   F + i × i =⇒
   l,r
   i + i × i
   isneitherleftnorright.Thethreederivationshavethesamesyntaxtree.Theexample
   actually illustrates an essential property of context-free grammars.

   Property 2.42 (leftandrightderivations)Everysentenceofacontext-freegrammar
   can be generated by a left derivation and by a right one. ?
   Therefore,itdoesnotdoanyharmtousejustleft(orright)derivationsinthedefinition
   (on p.37) of the language generated by a grammar.

   On the other hand, more complex grammar types, as the context-sensitive
   grammars, do not share this nice property, which is quite important for obtaining efficient
   algorithms for string recognition and parsing, as we will see.


## 2.5.8 Parenthesis Languages

   Manyartificiallanguagesincludeparenthesizedornestedstructures,madeofmatch-
   ing pairs of opening/closing marks. Any such occurrence may in turn contain other
   matching pairs. The marks are abstract elements that have different concrete
   representations in distinct settings. Thus, the block structures of Pascal are enclosed
   within begin … end, while in the C language curly brackets ‘{ }’ are frequently
   used.

   A massive use of parenthesis structures characterizes the mark-up language
   XML, which offers the possibility of inventing new matching pairs. An
   example is the pair ?title? ... ?/title?, used to delimit the document title. Similarly,
   in the notation of L a T E X, a mathematical formula is enclosed between the marks
   \begin{equation} ... \end{equation}. .

   Whenaparenthesisstructuremaycontainanotheroneofthesamekind,itiscalled
   self-nesting.Self-nestingispotentiallyunboundedinartificiallanguages,whereasin
   naturallanguagesitsuseismoderate,becauseitcausesadifficultyofcomprehension
   by breaking the flow of discourse. Next comes an example of complex German
   sentence, 19 with as many as three nested relative clauses:
   der Mann der die Frau die das Kind das die Katze füttert sieht liebt schläft
   19 The man who loves the woman (who sees the child (who feeds the cat)) sleeps.

<a id="P048"></a>

   S OBJECT
   OBJECT ‘{ }’ | ‘{’ MEMBERS ‘}’
   MEMBERS PAIR PAIR , MEMBERS
   PAIR STRING : VALUE
   VALUE STRING OBJECT ARRAY num | bool
   STRING “ ” | “ CHARS ”
   ARRAY ‘[ ]’ | ‘[’ ELEMENTS ‘]’
   ELEMENTS VALUE VALUE , ELEMENTS
   CHARS CHAR CHAR CHARS
   CHAR char
   Fig.2.2 Official JSON grammar: uppercase and lowercase words denote nonterminals and 
   terminals, respectively, and quotes mark metasymbols here used as terminals
   Returning to technical applications, in Fig. 2.2 we reproduce the syntax of JSON, a
   recent language that features recursive nesting of two different parenthesized
   constructs:
   Example 2.43 (JSON grammar) The JavaScript Object Notation (JSON) is a
   lightweight data-interchange format, which is easy for humans to read and write,
   and efficient for machines to parse and generate. The grammar of JSON is shown in
   Fig. 2.2. Symbol num is a numeric constant, and symbol bool stands for true, false,
   ornull.Symbol char denotesany8-bitcharacter,exceptthoseoccurringasterminals
   in the grammar rules.

   The syntactic paradigm of JSON is that of the nested lists of arbitrary depth,
   enclosedincurly(listofpairs)orsquare(arraylist)brackets,withelementsseparated
   by comma. Four sample JSON sentences are:
   {“integer”:7,“logical”:false }
   {“vector”:[ 1,3 ],“list”:{“integer”:1,“letter”:“a”} }
   {“matrix2x2”:[ [ 1,3 ],[ 5,8 ] ] }
   {“timetable”:{“airport”:[“JFK”,“LAX”],“dept.h”:[ 13,20 ] } }
   Clearly, JSON is meant to represent so-called semi-structured data, i.e., large data
   sets of mixed type. Here are the (partially condensed) syntax trees of the two top
   sentences, with node names shortened to their initials:

<a id="P049"></a>

   S
   O
   { M
   P
   S
   “ C
   integer
   ”
   : V
   7
   , M
   P
   S
   “ C
   logical
   ”
   : V
   false
   }
   A:array,C:chars,E:elements,M:members,
   O:object,P:pair,S:string,V:value
   S
   O
   { M
   P
   S
   “ C
   vector
   ”
   : V
   A
   [ E
   V
   1
   , E
   V
   3
   ]
   , M
   P
   S
   “ C
   list
   ”
   : V
   O
   { M
   P
   S
   “integer”
   : V
   1
   , M
   P
   S
   “letter”
   : V
   “a”
   }
   }
   Some subtrees are condensed (dashed edges), e.g., chars and string. ?
   By abstracting from any concrete representation and content of parenthesized
   constructs, the parenthesis paradigm is known as Dyck language. The terminal alphabet
   contains one or more pairs of opening/closing marks. An example is the alphabet
   Σ = {‘ ( ’, ‘)’, ‘[’, ‘]’} and the parenthesis sentence ‘ ???? [ ]( )
   ?? ’.

   Dyck sentences are characterized by the following cancellation rule that checks
   whether parentheses are well nested: given a string, repeatedly substitute the empty
   string ε for a pair of adjacent matching parentheses:
   ( ) ⇒ ε and [ ] ⇒ ε
   and obtain another string. Repeat until the transformation does not apply any longer.
   The original string is correct if, and only if, the last one is empty.

   Example 2.44 (Dyck language) To aid the eye, we encode the left (open) 
   parentheses as a, b, …, and the right (closed) ones as a ? , b ? , …. With the alphabet

<a id="P050"></a>

   Σ =
   ?
   a, a ? , b, b ?
   ? , the Dyck language is generated by the grammar below (left):
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → aS a ? S
   S → bS b ? S
   S → ε
   valid Dyck string (but invalid for language L 1 )
   a a a a ? a ?
   1 st nest
   a a a a ? a ? a ?
   2 nd nest chained to 1 st
   a ?
   Notice that we need nonlinear rules (the first two) to generate the Dyck language. To
   see why, compare with the language L 1 of Example 2.29 on p.31. Language L 1 , by
   recoding its alphabet {b, e} as
   ?
   a, a ?
   ? , is strictly included in the Dyck language,
   since it disallows any string with two or more nests concatenated, as there are in
   the string above (right). Such sentences have a branching syntax tree that requires
   nonlinear rules for its derivation. ?
   Another way of constraining a grammar to produce nested constructs is to force each
   rule to be parenthesized.

   Definition 2.45 (parenthesized grammar) Let G = (V, Σ, P, S ) be a grammar
   with an alphabet Σ without parentheses. The parenthesized grammar G p has an
   extended alphabet Σ ∪ {‘ ( ’, ‘)’} and these rules:
   A → ‘ ( ’α‘)’ where A → α is a rule of G
   The grammar is distinctly parenthesized if every rule has the form:
   A → ‘ A (’α‘) A ’ B → ‘ B (’β ‘) B ’ ...

   where ‘ A (’ and ‘) A ’, etc, are parentheses subscripted (on the left or right for better
   legibility) with the nonterminal name A, etc. ?
   Each sentence produced by such grammars exhibits a parenthesized structure.
   Example 2.46 (parenthesisgrammar)Theparenthesizedversionofthegrammarfor
   palindrome lists (p. 33) is:
   ?
   list → ‘ ( ’pal‘, ’list‘)’
   list → ‘ ( ’pal‘)’
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   pal → ‘ ( ’‘)’
   pal → ‘ ( ’apala‘)’
   pal → ‘ ( ’bpalb‘)’
   Now, the original sentence aa is parenthesized as ‘ ? (a ( ) a)
   ? ’ or is distinctly
   parenthesized as ‘ list ( pal (a pal ( ) pal a) pal ) list ’. ?
   A notable effect of the presence of parentheses is to allow a simpler checking of
   string correctness, to be discussed in Chap. 4.


<a id="P051"></a>


## 2.5.9 Regular Composition of Context-Free Languages

   If the basic operations of regular languages, i.e., union, concatenation, and star, are
   applied to context-free languages, the result remains a member of the CF family, to
   be shown next.

   Let G 1 = (Σ 1 , V 1 , P 1 , S 1 ) and G 2 = (Σ 2 , V 2 , P 2 , S 2 ) be grammars that
   define languages L 1 and L 2 , respectively. We may safely assume that their
   nonterminal sets are disjoint, i.e., V 1 ∩ V 2 = ∅. Moreover, we stipulate that symbol
   S, to be used as axiom of the grammar under construction, is not used by either
   grammar, i.e., S / ∈ (V 1 ∪ V 2 ).

   union The grammar G of language L 1 ∪ L 2 contains all the rules of G 1
   and G 2 , plus the initial rules S → S 1 | S 2 . In formula it is:
   G = (Σ 1 ∪ Σ 2 , {S } ∪ V 1 ∪ V 2 , {S → S 1 | S 2 } ∪ P 1 ∪ P 2 , S )
   concatenation The grammar G of language L 1 L 2 contains all the rules of G 1 and
   G 2 , plus the initial rule S → S 1 S 2 . It is:
   G = (Σ 1 ∪ Σ 2 , {S } ∪ V 1 ∪ V 2 , {S → S 1 S 2 } ∪ P 1 ∪ P 2 , S )
   star The grammar G of language L ∗
   1 contains all the rules of G 1 and the
   initial rules S → S S 1 | ε.

   cross From the identity L + = L · L ∗ , the grammar of language L + can
   be written by applying the concatenation construction to L and L ∗ ,
   but it is better to produce the grammar directly. The grammar G
   of language L +
   1
   contains all the rules of G 1 and the initial rules
   S → S S 1 | S 1 .

   From all this, we have:
   Property 2.47 (closure of CF) The family CF of context-free languages is closed
   with respect to union, concatenation, star, and cross. ?
   Example 2.48 (union of languages) The language L below:
   L =
   ?
   a i b i c ∗ | i ≥ 0
   ?
   component L 1
   ∪
   ?
   a ∗ b i c i | i ≥ 0
   ?
   component L 2
   = L 1 ∪ L 2
   has sentences of the form a i b j c k (i, j, k ≥ 0) with i = j or j = k, such as
   a 5 b 5 c 2 a 5 b 5 c 5 b 5 c 5

<a id="P052"></a>

   ThegrammarsG 1 andG 2 forthecomponentlanguagesL 1 andL 2 arestraightforward:
   G 1
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S 1 → X C
   X → aX b | ε
   C → cC | ε
   G 2
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S 2 → AY
   Y → bY c | ε
   A → aA | ε
   We just add to the rules of G 1 and G 2 , the alternatives S → S 1 | S 2 , to trigger a
   derivation with either grammar G 1 or G 2 . ?
   A word of caution: if the nonterminal sets overlap, this construction produces a
   grammar that generates a language typically larger than the union. To see why,
   replace grammar G 2 with the trivially equivalent grammar G ? 2 :
   G ? 2
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S ?
   2
   → AX
   X → bX c | ε
   A → aA | ε
   Then the putative union grammar, i.e.,
   ?
   S → S 1 | S ?
   2
   ?
   ∪ P 1 ∪ P ?
   2 , would also
   allow hybrid derivations that use rules from both grammars, and for instance, it
   would generate string abcbc, which is not in the union language.

   Notably, Property 2.47 holds for both families REG and CF, but only the former
   is closed under intersection and complement, to be seen later.

   Grammar of Mirror Language
   When one examines the effect of string reversal on the sentences of a CF language,
   one immediately sees that the family CF is closed with respect to reversal (the same
   as family REG). Given a grammar, the rules that generate the mirror language are
   obtained by reversing the right part of every rule.


## 2.5.10 Ambiguity in Grammars

   In natural language, the common linguistic phenomenon of ambiguity shows up
   when a sentence has two or more meanings. Ambiguity is of two kinds, semantic
   or syntactic. Semantic ambiguity occurs in the clause a hot spring, where the noun
   denotes either a coil or a season. A case of syntactic (or structural) ambiguity is the
   clause halfbakedchicken, which has different meanings depending on the structure
   assigned:
   ?
   [ half baked ] chicken
   ?
   or
   ?
   half [ baked chicken ]
   ?
   Although ambiguity may cause misunderstanding in human communication,
   negative consequences are counteracted by the availability of nonlinguistic clues for
   choosing the intended interpretation.


<a id="P053"></a>

   Artificial languages too can be ambiguous, but the phenomenon is less deep than
   in human languages. In most situations, ambiguity is a defect to be removed or
   counteracted.

   A sentence x defined by grammar G is syntactically ambiguous if it is generated
   with two different syntax trees. Then the grammar too is called ambiguous.
   Example 2.49 (ambiguity of a sentence) Consider again the arithmetic expression
   language of Example 2.39 on p.42, but define it with a different grammar G ? , 
   equivalent to the previous one:
   G ? : E → E + E | E × E | ‘ ( ’E ‘)’ | i
   The left derivations:
   E ⇒ E × E ⇒ E + E × E ⇒ i + E × E ⇒ i + i × E ⇒ i + i × i (2.6)
   E ⇒ E + E ⇒ i + E ⇒ i + E × E ⇒ i + i × E ⇒ i + i × i (2.7)
   generate the same sentence, but with different trees:
   E
   E
   E
   i
   + E
   i
   × E
   i
   E
   E
   i
   + E
   E
   i
   × E
   i
   tree of derivation (2.6) tree of derivation (2.7)
   Therefore, sentence i + i × i is ambiguous, as well as grammar G ? .

   Now pay attention to the meaning of the two readings of the same expression.
   The left tree interprets the sentence as (i + i) × i, the right one as i + (i × i).
   The latter interpretation is likely to be preferable, as it agrees with the traditional
   operatorprecedence.Anotherambiguoussentenceisi + i + i,whichhastwotrees
   that differ in the subexpression association order: from left to right, or the other way.
   Thus, grammar G ? is ambiguous.

   A major defect of this grammar is that it does not force the expected traditional
   precedence of multiplication over addition. ?
   On the other hand, for the grammar G of Example 2.39 on p. 42, each sentence has
   onlyoneleftderivation,thereforeallthesentencesgeneratedby G areunambiguous,
   and grammar G is unambiguous as well.


<a id="P054"></a>

   It may be noticed that the new grammar G ? is smaller than the old one G. This
   manifests a frequent property of ambiguous grammars, which is their conciseness
   with respect to the equivalent unambiguous ones. In special situations, when one
   wants the simplest grammar possible for a language, ambiguity may be tolerated,
   but, in general, conciseness cannot be bought at the cost of equivocation.
   The degree of ambiguity of a sentence x of a language L(G) is the number of
   distinct syntax trees deriving the sentence. For a grammar, the degree of ambiguity
   is the maximum degree of any ambiguous sentence. The next derivation shows that
   such a degree may be unbounded.

   Example 2.50 (Example 2.49 continued) Sentences i + i + i and i + i × i + i have
   ambiguity degree 2 and 5, resp. Here are the five skeleton trees of the latter:
   i + i × i +i i + i × i + i i + i × i + i
   i + i × i + i i + i × i + i
   Clearly, longer sentences cause the ambiguity degree to grow unbounded. ?
   An important practical problem is to check a given grammar for ambiguity. This is
   an example of a seemingly simple problem, for which no general algorithm exists:
   the problem is undecidable. 20 This means that any general procedure for checking
   a grammar for ambiguity may be forced to examine longer and longer sentences,
   without ever reaching the certainty of the answer. On the other hand, for a specific
   grammar, with some ingenuity one can often prove nonambiguity by applying some
   form of inductive reasoning.

   In practice, this is unnecessary, and usually two approaches suffice. First, a small
   number of rather short sentences, which cover all grammar rules, are tested, by
   constructing their syntax trees and checking that they are unique. If the sentences
   passthetest,onehastoproceedtocheckwhetherthegrammarcomplieswithcertain
   conditions characterizing the so-called deterministic context-free languages, to be
   fully studied in Chap. 4. Such conditions are sufficient, although not necessary, to
   ensure nonambiguity.

   Even better is to prevent the problem when a grammar is designed, by avoiding
   some common pitfalls to be explained next.


## 2.5.11 Catalogue of Ambiguous Forms and Remedies

   Following the definition, an ambiguous sentence displays two or more structures,
   each one possibly associated with a sensible interpretation. Though ambiguity cases
   20 A proof can be found in [9].


<a id="P055"></a>

   are abundant in natural languages, communication clarity is not seriously impaired
   because the sentences are uttered or written in a living context made of gestures,
   intonation,presuppositions,etc,whichhelpsselecttheinterpretationintendedbythe
   author. On the contrary, most existing compilers cannot tolerate ambiguity, because
   they are not capable to select the correct meaning out of the many that would be
   possible for an ambiguous sentence in a programming language, with the negative
   consequence of an unpredictable behavior of the program.

   Now we classify the most common ambiguity types, and we show how to remove
   them by modifying the grammar, or in some cases the language.


### 2.5.11.1 Ambiguity from Bilateral Recursion

   A nonterminal symbol A is bilaterally (two-side) recursive if it is both left and
   right-recursive, i.e., A
   +
   = ⇒ Aγ and A
   +
   = ⇒ β A. We distinguish the cases when the two
   derivations are produced by the same rule or by different rules.

   Example 2.51 (bilateral recursion from same rule) The grammar G 1 below:
   G 1 : E → E + E | i
   generates string i + i + i with two different left derivations:
   E ⇒ E + E ⇒ E + E + E ⇒ i + E + E ⇒ i + i + E ⇒ i + i + i
   E ⇒ E + E ⇒ i + E ⇒ i + E + E ⇒ i + i + E ⇒ i + i + i
   Ambiguitycomesfromtheabsenceofafixedgenerationorderforthestring,i.e.,from
   the left or right. When looking at the intended meaning of the string as an arithmetic
   formula,thisgrammardoesnotspecifytheapplicationorderofarithmeticoperations.
   In order to remove ambiguity, observe that this language is a list with separators,
   i.e.,L(G 1 ) = i(+i) ∗ .Thisisaparadigmweareabletodefinewitharight-recursive
   rule E → i + E | i or with a left-recursive one E → E + i | i. Both rules are
   unambiguous. ?
   Example 2.52 (left and right recursions in different rules) A second case of bilateral
   recursive ambiguity is grammar G 2 :
   G 2 : A → aA | Ab | c
   This language too is regular: L(G 2 ) = a ∗ cb ∗ . It is the concatenation of two lists a ∗
   and b ∗ , with a letter c interposed. Ambiguity disappears if the two lists are derived
   by separate rules, thus suggesting the grammar (left):
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → AcB
   A → aA | ε
   B → bB | ε
   ?
   S → aS | X
   X → X b | c

<a id="P056"></a>

   An alternative remedy is to decide that the first list should be generated before the
   second one (or conversely), as done above (right). ?
   Were mark that a double recursion on the same nonterminal does not cause ambiguity
   byitself,providedthatthetworecursionsarenotleftandright.Observethegrammar:
   S → +S S | ×S S | i
   that defines the so-called prefix polish expressions with the signs of addition and
   multiplication (further studied in Chap. 5), such as the expression + + ii × ii.
   Notwithstanding two rules are doubly recursive, the grammar is not ambiguous, and
   as a matter of fact one recursion is right but the other is not left.


### 2.5.11.2 Ambiguity from Union

   If two languages L 1 = L(G 1 ) and L 2 = L(G 2 ) share some sentence, that is, their
   intersection is not empty, the grammar G of the union language, constructed as
   explained on p. 51, generates such sentence with two different trees, respectively
   using the rules of G 1 or G 2 , and is therefore ambiguous.

   Example 2.53 (union of overlapping languages) In language and compiler design,
   there are various causes for overlap:
   1. When one wants to single out a special pattern that requires specific processing,
   within a general class of phrases. Consider the additive arithmetic expressions
   with constants C and variables i. A grammar is:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   E → E + C | E + i | C | i
   C → 0 | 1D | ··· | 9D
   D → 0D | ··· | 9D | ε
   Now, assume the compiler has to single out such expressions as i + 1 or 1 + i,
   because they have to be translated to machine code, by using increment instead
   of addition. To this end, we add the rules:
   E → i + 1 | 1 + i
   Unfortunately, the new grammar is ambiguous, since a sentence like 1 + i is
   generated by the original rules, too.

   2. When the same operator is overloaded, that is, used with different meanings in
   different constructs. In the Pascal language, a sign ‘+’ denotes both addition in:
   E → E + T | T T → V V → ...


<a id="P057"></a>

   and set union in:
   E ins → E ins + T ins | T ins T ins → V
   To remove such ambiguities, we need a severe grammar surgery: either the two
   ambiguousconstructsaremadedisjointortheyarefusedtogether.Disjoiningthecon-
   structs is unfeasible in the previous examples, because string ‘1’ cannot be removed
   from the set of integer constants derived from nonterminal C. To enforce a special
   treatment of value ‘1’, if one accepts a syntactic change to the language, it suffices to
   add the operator inc (for increment by 1) and replace rule E → i + 1 | 1 + i with
   rule E → inci.

   In the latter example, ambiguity is semantic and is caused by the double
   meaning (polysemy) of the operator ‘+’. A remedy is to collapse together the rules for
   arithmeticandsetexpressions,generatedfromnonterminalsE andE ins ,respectively,
   thus giving up a syntax-based separation. The semantic analyzer will take care of
   it. Alternatively, if modifications are permissible, one may replace the sign ‘+’ by a
   character ‘∪’ in the set expressions. ?
   In the example below, removing the overlapping constructs actually succeeds.
   Example 2.54 (disambiguation of grammars [10])
   1. The grammar G below (left):
   G
   ?
   S → bS | cS | D
   D → bD | cD | ε
   S
   +
   = ⇒ bbcS ⇒ bbcD ⇒ bbc
   S ⇒ D
   +
   = ⇒ bbcD ⇒ bbc
   isambiguoussinceL(G) = {b, c} ∗ = L D (G).Infact,thetwoderivationsabove
   (right) produce the same result. By deleting the rules of D, which are redundant,
   we have S → bS | cS | ε, no longer ambiguous.

   2. The grammar below (left):
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → B | D
   B → bBc | ε
   D → d De | ε
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → B | D | ε
   B → bBc | bc
   D → d De | d e
   where nonterminals B and D respectively generate strings b n c n and d n e n , with
   n ≥ 0, has just one ambiguous sentence: the empty string ε. A remedy is to
   generate ε directly from the axiom, as done above (right). ?

<a id="P058"></a>


### 2.5.11.3 Ambiguity from Concatenation

   Concatenating languages may cause ambiguity, if a suffix of a sentence of language
   one is also a prefix of a sentence of language two.

   Recall that the concatenation grammar G of L 1 L 2 (p. 51) contains the rule S →
   S 1 S 2 inadditiontotherules(byhypothesisnotambiguous)ofG 1 andG 2 .Ambiguity
   arises in G if the following sentences exist in the languages:
   u ? ∈ L 1 u ? v ∈ L 1 v z ?? ∈ L 2 z ?? ∈ L 2 with v ?= ε
   Then the string u ? v z ?? of language L 1 · L 2 is ambiguous, via the derivations:
   S ⇒ S 1 S 2
   +
   = ⇒ u ? S 2
   +
   = ⇒ u ? v z ?? S ⇒ S 1 S 2
   +
   = ⇒ u ? v S 2
   +
   = ⇒ u ? v z ??
   Example 2.55 (concatenation of Dyck languages) Consider the concatenation L =
   L 1 L 2 of the Dyck languages (p. 49) L 1 and L 2 over alphabets
   ?
   a, a ? , b, b ?
   ?
   and
   ?
   b, b ? , c, c ?
   ? , respectively. A sentence of L is aa ?
   bb ? cc ? . The standard grammar
   G of L is:
   G
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → S 1 S 2
   S 1 → aS 1 a ? S 1 | bS 1 b ? S 1 | ε
   S 2 → bS 2 b ? S 2 | cS 2 c ? S 2 | ε
   For grammar G, sentence aa ? bb ? cc ? is derived in two ways:
   aa ? bb ?
   S 1
   cc ?
   S 2
   aa ?
   S 1
   bb ? cc ?
   S 2
   ?
   To remove such an ambiguity, one should block moving a string from the suffix of
   language one to the prefix of language two (and conversely).

   If the designer is free to modify the language, a simple 21 remedy is to interpose
   a new terminal as a separator between the two languages. In our example, with
   character ‘?’ as separator, the (concatenation) language L 1 ?L 2 is easily defined
   without ambiguity by a grammar with initial rule S → S 1 ?S 2 .


### 2.5.11.4 Unique Decoding

   A nice illustration of concatenation ambiguity comes from the study of codes in
   information theory. An information source is a process that produces a message, i.e.,
   a sequence of symbols from a finite set Γ = {A, B, ..., Z }. Each such symbol is
   then encoded into a string over a terminal alphabet Σ, typically binary, and a coding
   function maps each symbol into a short terminal string, termed its code.

   21 A more complex unambiguous grammar equivalent to G exists, having the property that every
   string not containing any letter c, such as string bb ? , is in language L 1 but not in L 2 . On the other
   hand, notice that in so doing string bcc ? b ? is assigned to language L 2 .

<a id="P059"></a>

   For instance, consider the following source symbols and their mapping into a
   binary code over alphabet Σ = {0, 1}:
   Γ =
   ?
   01
   A,
   10
   C ,
   11
   E ,
   001
   R
   ?
   Message ARRECA is encoded as 01001001111001. By decoding this string,
   the original text is the only one that is obtained. Message coding is expressed by
   grammar G 1 :
   G 1
   ?
   mes → Ames | C mes | E mes | Rmes | A | C | E | R
   A → 01 C → 10 E → 11 R → 001
   Grammar G 1 generates a message such as AC by concatenating the corresponding
   codes, as displayed in the syntax tree:
   mes
   A
   0 1
   mes
   C
   1 0
   Asgrammar G 1 isclearlyunambiguous,everyencodedmessage,i.e.,everysentence
   of language L(G 1 ), has one and only one syntax tree corresponding to the decoded
   message.

   On the contrary, the next bad choice of codes:
   Γ =
   ?
   00
   A,
   01
   C ,
   10
   E ,
   010
   R
   ?
   renders ambiguous the grammar:
   ?
   mes → Ames | C mes | E mes | Rmes | A | C | E | R
   A → 00 C → 01 E → 10 R → 010
   Consequently, message 00010010100100 is decipherable in two ways:
   ARRECA and ACAEECA. The defect is twofold. (i) The identity:
   01
   init
   · 00 · 10 = 010
   init
   · 010

<a id="P060"></a>

   holds for two sequences of concatenated codes. (ii) The initial code 01 of the left
   sequence is prefix of the initial code 010 of the right one.

   Code theory studies conditions such as the negation of (ii) that make a code set
   uniquely decipherable.


### 2.5.11.5 Other Ambiguous Situations

   The next case is similar to the ambiguity of regular expressions (p. 22).

   Example 2.56 (analogy with r.e.) Consider the left grammar:
   ?
   1: S → DcD
   2: D → bD | cD | ε
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   1: S → BcD
   2: B → bB | ε
   3: D → bD | cD | ε
   Nonterminal D generates language {b, c} ∗ . Rule 1 says that a sentence contains at
   least one letter c. The same structure is defined by the ambiguous regular expression
   {b, c} ∗ c {b, c} ∗ : every sentence with two or more letters c is ambiguous as the
   distinguished occurrence of c is not fixed. This defect can be repaired by imposing
   that the distinguished c is, say, the leftmost one, as the right grammar above shows.
   Notice in fact that nonterminal B may not derive a string containing letter c. ?
   Example 2.57 (setting an order on rules) In the left grammar:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   1: S → bS c
   2: S → bbS c
   3: S → ε
   ?
   1: S → bS c | D
   2: D → bbDc | ε
   rules1and2maybeappliedinanyorder,thusproducingtheambiguousderivations:
   S ⇒ bbS c ⇒ bbbS cc ⇒ bbbcc
   S ⇒ bS c ⇒ bbbS cc ⇒ bbbcc
   Remedy: impose that rule 1 is applied before rule 2 in any derivation, as the right
   grammar above shows. ?

### 2.5.11.6 Ambiguity of Conditional Statements

   A notorious case of ambiguity in the programming languages with conditional
   instructions occurred in the first version of language Algol 60, 22 a milestone for
   22 The official version [11] removed the ambiguity.


<a id="P061"></a>

   the application of CF grammars. Consider the grammar:
   S → ifbthenS elseS | ifbthenS | a
   wherelettersbandastandforaBooleanconditionandanynonconditionalconstruct,
   respectively; for brevity both are left undefined. The first and second alternatives
   produce a two-leg and a one-leg conditional, respectively.

   Ambiguity arises when two sentences are nested, with the outermost being a
   two-way conditional. For instance, examine the two readings:
   ifbthen ifbthenaelsea
   2-way conditional
   1-way conditional
   ifbthen ifbthena
   1-way conditional
   elsea
   2-way conditional
   caused by the ‘dangling’ else. It is possible to eliminate the ambiguity at the cost of
   complicatingthegrammar.Assumewedecidetochoosetheleftskeletontree,which
   binds the else to the immediately preceding if. The corresponding grammar is:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → S E | S T
   S E → ifbthenS E elseS E | a
   S T → ifbthenS E elseS T | ifbthenS
   Observe that the syntax class S has been split into two classes. Class S E defines a
   two-leg conditional such that its nested conditionals are in the same class S E . The
   other class S T defines a one-leg conditional, or a two-leg one such that the first and
   second nested conditionals are of class S E and S T , respectively. This excludes the
   two combinations:
   ifbthenS T elseS T ifbthenS T elseS E
   ThefactsthatonlyclassS E mayprecede else andthatonlyclassS T definesaone-leg
   conditional, disallow the derivation of the right skeleton tree.

   If the language syntax can be modified, a simpler solution exists: many language
   designers have introduced a closing mark for delimiting conditional constructs. See
   the next use of the closing mark end_if:
   S → ifbthenS elseS end_if | ifbthenS end_if | a
   Indeed, this is a sort of parenthesizing (p. 50) of the original grammar.


<a id="P062"></a>


### 2.5.11.7 Inherent Ambiguity of Language

   Inalltheprecedingexamples,wehavefoundthatalanguageisdefinedbyequivalent
   grammars, some ambiguous some not. Yet this is not always the case. A language is
   called inherently ambiguous if every grammar that defines it is ambiguous. 
   Surprisingly enough, inherently ambiguous languages exist!
   Example 2.58 (unavoidable ambiguity from union) Recall the language L of
   Example 2.48 on p.51:
   L =
   ?
   a i b j c k | i, j, k ≥ 0 ∧ (i = j ∨ j = k )
   ?
   Language L can be equivalently defined by means of the union:
   L = L 1 ∪ L 2 =
   ?
   a i b i c ∗ | i ≥ 0
   ?
   ∪
   ?
   a ∗ b i c i | i ≥ 0
   ?
   of two nondisjoint languages L 1 and L 2 .

   We intuitively argue that any grammar G of language L is necessarily ambiguous.
   Thegrammar G onp.51unitestherulesofthecomponentgrammars G 1 and G 2 and
   is obviously ambiguous for every sentence x ∈
   ?
   ε, abc, ..., a i b i c i , ...

   ?
   shared
   by languages L 1 and L 2 . Any such sentence is produced by G 1 by means of rules
   checking that |x| a = |x| b . This check is possible only for a syntax structure of the
   type:
   a ... a ab b ... b cc ... c
   Now the same string x, viewed as a sentence of language L 2 , must be generated by
   grammar G 2 with a structure of the type:
   a ... aa b ... b bc c ... c
   in order to perform the equality check |x| b = |x| c . No matter which grammar
   variation we make, the two exponent equality checks are unavoidable and the grammar
   remains ambiguous for such sentences. ?
   In reality, inherent language ambiguity is rare and it scarcely affects technical
   languages, if ever.


## 2.5.12 Weak and Structural Equivalence

   It is not enough for a grammar to generate correct sentences, as it should also assign
   to each of them a suitable structure, in agreement with the intended meaning. This
   structural adequacy requirement has already been invoked at times, for instance
   when discussing operator precedence in hierarchical lists.


<a id="P063"></a>

   We ought to reexamine the notion of grammar in light of structural adequacy.
   Recall the definition on p. 38: two grammars G and G ? are equivalent if they define
   the same language, i.e., L(G) = L(G ? ). Such a definition, to be qualified now as
   weak equivalence, poorly fits with the real possibility of substituting one grammar
   for the other in technical language processors such as compilers. The reason is that
   the two grammars are not guaranteed to assign the same meaningful structure to
   every sentence.

   We need a more stringent definition of equivalence, which is only relevant for
   unambiguousgrammars.GrammarsG andG ? arestronglyorstructurallyequivalent,
   if (i) L(G) = L(G ? ) and (ii) in addition grammars G and G ? assign to each sentence
   two syntax trees, which may be considered structurally similar.

   Condition(ii)shouldbeformulatedinaccordancewiththeintendedapplication.A
   plausibleformulationis:twosyntaxtreesarestructurallysimilarifthecorresponding
   condensed skeleton trees (p. 45) are equal.

   Strong equivalence implies weak equivalence, but the former is a decidable
   property, unlike the latter. 23
   Example 2.59 (structural adequacy of arithmetic expressions) The difference
   betweenstrongandweakequivalenceismanifestedbythecaseofarithmeticexpres-
   sions, such as 3 + 5 × 8 + 2, first viewed as a list of digits separated by addition and
   multiplication signs. Here is a first grammar G 1 :
   G 1
   ?
   E → E + C | E × C | C
   C → 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
   The syntax tree of the previous sentence is:
   E
   E
   E
   E
   C
   3
   + C
   5
   × C
   8
   + C
   2
   ?
   ?
   ?
   3 + 5
   × 8
   + 2
   23 The decision algorithm, e.g., in [12], is similar to the one for checking the equivalence of finite
   automata, to be presented in the next chapter.


<a id="P064"></a>

   In the condensed skeleton (right), nonterminals and copy rules are dropped.
   Here is a second grammar G 2 :
   G 2
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   E → E + T | T
   T → T × C | C
   C → 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
   Grammars G 1 and G 2 are weakly equivalent. When we observe the syntax tree of
   the same sentence:
   E
   E
   E
   T
   C
   3
   + T
   T
   C
   5
   × C
   8
   + T
   C
   2
   ?
   ?
   3 +
   ?
   5 × 8
   + 2
   weseethatthecondensedskeletondiffersfromthepreviousone:itcontainsasubtree
   with frontier 5 × 8, associated with a multiplication. Therefore, grammars G 1 and
   G 2 are not structurally equivalent.

   Iseitheroneofthegrammarspreferable?Concerningambiguity,bothofthemare
   allright.ButonlygrammarG 2 isstructurallyadequate,ifoneconsidersalsomeaning.
   Infact,sentence3 + 5 × 8 + 2denotesacomputationtobeexecutedinthetraditional
   order 3 + (5 × 8) + 2 = (3 + 40) + 2 = 43 + 2 = 45: this is the semantic 
   interpretation. The parentheses that specify the evaluation order
   ?
   (3 + (5 × 8)) + 2
   ?
   can be mapped on the subtrees of the skeleton tree produced by G 2 .

   Instead, grammar G 1 produces the parenthesizing
   ?
   ((3 + 5) × 8) + 2
   ?
   = 66,
   which is inadequate, because by giving precedence to the first addition over 
   multiplication, it returns a wrong semantic interpretation, i.e., the value 66. Incidentally,
   grammar G 2 is more complex because enforcing operator precedence requires more
   nonterminals and rules.

   It is crucial for a grammar intended for driving a compiler to be structurally
   adequate, as we will see in Chap. 5 on syntax-directed translation.


<a id="P065"></a>

   A case of structural equivalence is illustrated by the next grammar G 3 24 :
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   E → E + T | T + T | C + T | E + C | T + C | C + C | T × C | C × C | C
   T → T × C | C × C | C
   C → 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 grammar G 3
   Now the condensed skeleton trees of any arithmetic expression coincide for grammars
   G 2 and G 3 . They are structurally equivalent. ?
   Generalized Structural Equivalence
   Sometimes a similarity criterion looser than the strict identity of condensed skeleton
   trees is more suitable. This consists in requesting that the two corresponding trees
   shouldbeeasilymappedoneontotheotherbysomesimpletransformation.Theidea
   can be differently materialized: one possibility is to have a bijective correspondence
   between the subtrees of one tree and the subtrees of the other tree. For instance, the
   two grammars:
   S → S a | a S → aS | a
   arejustweaklyequivalentingeneratinglanguageL = a + ,sincethecondensedskele-
   ton trees differ, as in the example of sentence aa:
   ?
   ?
   a
   a
   ?
   a
   ?
   a
   However, the two grammars may be considered structurally equivalent in a 
   generalizedsense,aseachleft-lineartreeoftheformergrammarcorrespondstoaright-linear
   tree of the latter. The intuition that the two grammars are similar is satisfied because
   their trees are specularly identical, i.e., they coincide by turning left-recursive rules
   into right-recursive.


## 2.5.13 GrammarTransformations and Normal Forms

   We are going to study a range of transformations that are useful to obtain grammars
   having certain desired properties, without affecting the language. Normal forms are
   restricted rule patterns, yet still allowing any context-free language to be defined.
   Such forms are widely used in theoretical papers, to simplify formal statements and
   24 Incidentally, grammar G 3
   has more rules because it does not exploit copy rules to express the
   inclusion of syntax classes. Categorization and the ensuing taxonomies reduce description
   complexity in any area of knowledge.


<a id="P066"></a>

   theoremproofs.Otherwise,inappliedwork,normalformgrammarsareusuallynota
   good choice, because they are often larger and less readable; moreover, most normal
   forms are not strongly equivalent to the original grammar. However, some normal
   formshaveimportantpracticalusesforlanguageanalysisinChap.4.Inparticular,the
   nonleft-recursive form is needed by top-down deterministic parsers and the operator
   form is exploited by parallel parsing algorithms.

   We start the survey of transformations from the simple ones. Let a grammar
   G = (V, Σ, P, S ) be given.


### 2.5.13.1 Nonterminal Expansion

   A general-purpose transformation preserving language is nonterminal expansion,
   which consists of replacing a nonterminal with its alternatives.

   Replace rule A → αBγ with rules:
   A → αβ 1 γ | αβ 2 γ | ··· | αβ n γ n ≥ 1
   where B → β 1 | β 2 | ··· | β n are all the alternatives of B. Clearly, the language
   does not change, since the two-step derivation A ⇒ αBγ ⇒ αβ i γ becomes the
   immediate derivation A ⇒ αβ i γ, to the same effect.


### 2.5.13.2 Axiom Elimination from Right Parts

   At no loss of generality, every right part of a rule may exclude the presence of
   the axiom, i.e., it may be a string over alphabet Σ ∪ (V \ {S }). To this end, just
   introduce a new axiom S 0 and the rule S 0 → S.


### 2.5.13.3 Nullable Nonterminals and Elimination of Empty Rules

   A nonterminal A is nullable if it can derive the empty string, i.e., there exists
   derivation A
   +
   ε. Consider the set named Null ⊆ V of nullable nonterminals. Set
   Null is computed by the following logical clauses, to be applied in any order until
   the set ceases to grow, i.e., a fixed point is reached:
   A ∈ Null if
   ⎧
   ⎨
   ⎩
   (A → ε) ∈ P
   (A → A 1 A 2 ... A n ) ∈ P with A i ∈ V \ { A }
   and for every 1 ≤ i ≤ n it holds A i ∈ Null
   Row one finds the nonterminals that are immediately nullable, and row two finds
   those that derive a string of nullable nonterminals.


<a id="P067"></a>

   Table 2.4 Elimination of copy rules
   Nullable Original grammar Nonnullable grammar (to be cleaned)
   No S → SAB | AC S → SAB | SA | SB | S | AC | C
   Yes A → aA | ε A → aA | a
   Yes B → bB | ε B → bB | b
   No C → cC | c C → cC | c
   Example 2.60 (computing nullable nonterminals) Examine this grammar:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → S AB | AC
   A → aA | ε
   B → bB | ε
   C → cC | c
   Result: Null = {A, B}. Notice that if rule S → AB were added to the grammar, the
   result would include S ∈ Null. ?
   The normal form without nullable nonterminals, in short nonnullable, is defined by
   the condition that no nonterminal is nullable, but the axiom. Clearly, the axiom is
   nullable if, and only if, the empty string is in the language.

   To construct the nonnullable form, first compute set Null, then do so:
   • for each rule (A → A 1 A 2 ... A n ) ∈ P, with A i ∈ V ∪ Σ, add as alternatives the
   strings obtained from the right part, by deleting in all possible ways any nullable
   nonterminal A i
   • remove the empty rules A → ε, for every nonterminal A ?= S
   If the grammar thus obtained is unclean or circular, it should be cleaned with the
   known algorithms (p. 39).

   Example 2.61 (Example 2.60 continued) In Table 2.4, column one lists nullability.
   The other columns list the original rules and those produced by the transformation,
   to be cleaned by removing the circular alternative S → S. ?

### 2.5.13.4 Copies or SubcategorizationRules andTheir Elimination

   Acopy(orsubcategorization)rulehastheformA → B,whereB ∈ V isanonterminal
   symbol. Any such rule is tantamount to the relation L B (G) ⊆ L A (G), which means
   that the syntax class B is included in the class A.


<a id="P068"></a>

   For a concrete example, related to programming languages, these rules:
   iterative_phrase → while_phrase | for_phrase | repeat_phrase
   introduce three subcategories of iterative phrase: while, for, and repeat.
   Copy rules can be eliminated, yet many more alternatives have to be introduced
   andgrammarlegibilityusuallydeteriorates.Noticethatcopyeliminationreducesthe
   height of the syntax trees by shortening the derivations.

   Now we present an algorithm for eliminating copy rules, not for practical utility,
   but because in Chap. 3 the same transformation is applied to eliminate the 
   spontaneousmovesofafiniteautomaton.Forsimplicity,weassumethatthegivengrammar
   G is in nonnullable normal form and that the axiom S does not occur in any rule
   right part.

   For a grammar G and nonterminal A, we define the set Copy(A) ⊆ V, containing
   A and the nonterminals that are immediate or transitive copies of A:
   Copy(A) =
   ?
   B ∈ V | ∃ a derivation A
   ∗
   = ⇒ B
   ?
   (2.8)
   To compute the set Copy, apply the following logical clauses until a fixed point is
   reached:
   A ∈ Copy(A) initialization
   C ∈ Copy(A) if B ∈ Copy(A) ∧ (B → C) ∈ P
   Then construct the rule set P ? of a new grammar G ? , equivalent to G and copy-free,
   as follows:
   P ? := P \ {A → B| A, B ∈ V } cancelation of copy rules
   P ? := P ? ∪
   ?
   A → α
   α ∈
   ?
   (Σ ∪ V ) ∗ \ V
   ?
   ∧
   (B → α) ∈ P ∧ B ∈ Copy(A)
   ?
   Theeffectisthatanonimmediate(multiple-step)derivationA
   +
   = ⇒ B ⇒ αofG shrinks
   to the immediate (one-step) derivation A ⇒ α of G ? . Notice that the transformation
   retains all the original noncopy rules.

   We note that if, contrary to our simplifying hypothesis, grammar G contains
   nullablenonterminals,thedefinitionofsetCopyatformula (2.8)anditscomputation
   must consider also the derivations of the form:
   A
   +
   = ⇒ BC
   +
   = ⇒ B
   where nonterminal C is nullable.


<a id="P069"></a>

   Example 2.62 (copy-free rules for arithmetic expressions) By applying the copy
   elimination algorithm to grammar G 2 (Example 2.59):
   G 2
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   E → E + T | T
   T → T × C | C
   C → 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
   we obtain these copy sets:
   Copy(E) = {E, T, C } Copy(T) = {T, C } Copy(C) = {C }
   The copy-free grammar G 3 equivalent to G 2 is:
   G 3
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   E → E + T | T × C | 0 | 1 | 2 | ··· | 9
   T → T × C | 0 | 1 | 2 | ··· | 9
   C → 0 | 1 | 2 | ··· | 9
   ?
   It is worth repeating that copy rules are very convenient for reusing certain rule
   blocks, which correspond to syntactic subcategories. A grammar with copy rules
   is more concise than one without them, and it highlights the generalization and
   specialization of language constructs. For these reasons, the reference grammars of
   technical languages often include copy rules.


### 2.5.13.5 Repeated Right Parts and Invertible Grammars

   In Example 2.62 on p. 69, the last grammar G 3 contains two rules with a repeated
   right part: E → T × C and T → T × C. On the other hand, a grammar such as
   G 2 , which does not include any rules with repeated right parts (RRP), is qualified as
   invertible. Before showing that every grammar admits an equivalent invertible form,
   we discuss the role of RRP in grammars.

   A convenient use of RRP is when two nonterminals C 1 and C 2 include in their
   languagesacommonpartRandalsotwoidiosyncraticparts,respectivelydenotedA 1
   andA 2 ,schematically:C 1 → R | A 1 andC 2 → R | A 2 .SincepartRcanbetheroot
   of arbitrarily complex constructs, the RRP form may save considerable duplication
   in the grammar rules.

   Aconcreteexampleoccursinmanyprogramminglanguages:twoconstructsexist
   fortheassignmentstatement,oneofgeneraluseandonerestrictedtovariableinitial-
   ization,saytoassignaconstantvalue.Clearly,thelatterofferspartofthepossibilities
   of the former, and some saving of grammar rules may result from having a 
   nonterminal R for the common subset.

   On the other hand, the presence of RRP rules moderately complicates syntax
   analysis, when the algorithm is of the simplest bottom-up type, in particular the
   operator precedence analyzer to be presented in Chap. 4.


<a id="P070"></a>

   WeshowhowtoconvertagrammarG = (V, Σ, P, S )intoanequivalentinvert-
   ible grammar G ? =
   ?
   V ? , Σ, P ? , S ?
   ? . 25
   For simplicity, we start from a grammar G that is nonnullable, i.e., free from
   empty rules. In the algorithm, for uniformity the nonterminals of G (including its
   axiom) are denoted by subscripted letters B, while the nonterminals of G ? are the
   axiomS ? andtheothernonterminalsdenotedbysubscriptedlettersA.Theideaisthat
   each nonterminal A i of G ? , except its axiom S ? , uniquely corresponds to a nonempty
   subset of nonterminals of G, i.e., [B 1 , ..., B r ], with r ≥ 1, to be also denoted as
   set(A i ).

   Wehavetofocusonthesimilarityoftworulessuchthattheirrightparts,although
   not necessarily identical, only differ in the nonterminal names, as for instance the
   rulesB 3 → uB 2 v B 4 w andB 1 → uB 3 v B 2 w,whereu,v,andw arepossiblyempty
   terminal strings. We say that such rules have the same right part pattern, which we
   may indicate by the string u − v − w; clearly, a special case of such similarity is
   when two rules have RRP.

   Construction of Grammar G ?
   Initially, sets V ? and P ? are empty. Then the algorithm creates new nonterminals and
   rules, until the last step does not produce any new rule. Then the algorithm creates
   the axiomatic rules and terminates:
   1. for each maximal set 26 of terminal rules of G that have a RRP u:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   B 1 → u
   B 2 → u
   ...

   B m → u
   where u ∈ Σ + and each B l ∈ V
   (any of the nonterminals B l may also be the axiom S) add to V ? the nonterminal
   [B 1 , B 2 , ..., B m ], if not already present, and add to set P ? the rule:
   [B 1 , B 2 , ..., B m ] → u
   2. for each right part pattern ρ of G, of the form:
   ρ = x 0 − x 1 − ··· − x j − ··· − x r ,where r ≥ 1 and each x j ∈ Σ ∗
   find in set P all the rules that have the same right part pattern ρ:
   P ρ =
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   B 1 → x 0 B 1,1 x 1 ... B 1,j x j ... B 1,r x r
   ...

   B m → x 0 B m,1 x 1 ... B m,j x j ... B m,r x r
   25 We refer to the book by Harrison [13] for more details and a correctness proof.
   26 This means that any proper subset included in set P ρ
   is not considered.


<a id="P071"></a>

   Let P ρ ⊆ P be the set of such rules. Examine every sequence σ of r nonterminals
   in V ? (not necessarily distinct), where number r is the same as in the pattern ρ:
   σ =
   ?
   A σ 1 , ..., A σ j , ..., A σ r
   ?
   with A σ j ∈ V ?
   Notice that A σ j is a nonterminal already present in V ? . Therefore, there are |V ? | r
   distinct sequences σ.

   Then, select from set P ρ every maximal subset of rules, named P ρ,σ , defined as
   follows:
   P ρ,σ =
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   B h 1 → x 0 B h 1 ,1 x 1 ... B h 1 ,j x j ... B h 1 ,r x r
   ...

   B h t → x 0 B h t ,1 x 1 ... B h t ,j x j ... B h t ,r x r
   with 1 ≤ t ≤ m
   where, for each ‘column’ j with 1 ≤ j ≤ r, the inclusion holds:
   ?
   B h 1 ,j , B h 2 ,j , ..., B h t ,j
   ?
   ⊆ set(A σ j ) (2.9)
   Notice that all the indexes h 1 , …, h t are distinct and that the set inclusion holds:
   {h 1 , ..., h t } ⊆ {1, ..., m}. From such set of rules, create the nonterminal
   [B h 1 , B h 2 , ..., B h t ], if not already present in V ? , and add to P ? the rule:
   [B h 1 , B h 2 , ..., B h t ] → x 0 A σ 1 x 1 A σ 2 x 2 ... A σ r x r
   The algorithm repeats the previous step 2 until it reaches a fixed point. Then let
   the current nonterminal and rule sets be V ? and P ? , respectively.

   3. To finish, in V ? find every nonterminal, say A i , that contains the axiom S in
   set(A σ i ) and create the axiomatic rules:
   S ? → A i such that S ∈ set(A i )
   Example 2.63 (elimination of repeated right parts) We apply the construction to the
   noninvertible grammar G below:
   G
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → cS | cB 1 d B 2
   B 1 → aa ? | bb ? | aB 1 a ? B 1 | bB 1 b ? B 1
   B 2 → aa ? | aB 2 a ? B 2
   The right part patterns of G are:
   c − ε c − d − ε aa ? bb ? a − a ? − ε b − b ? − ε

<a id="P072"></a>

   In Table 2.5, we tabulate the nonterminals and rules stepwise created. The invertible
   grammar G ? includes the rules computed by the algorithm:
   G ?
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S ? → [S]
   [S] → c[B 1 ]d [B 1 , B 2 ] | c[B 1 , B 2 ]d [B 1 , B 2 ] | c[S]
   [B 1 ] → bb ? | a[B 1 ]a ? [B 1 ] | a[B 1 ]a ? [B 1 , B 2 ] |
   b[B 1 ]b ? [B 1 ] | b[B 1 ]b ? [B 1 , B 2 ] |
   b[B 1 , B 2 ]b ? [B 1 ] | b[B 1 , B 2 ]b ? [B 1 , B 2 ]
   [B 1 , B 2 ] → aa ? | a[B 1 , B 2 ]a ? [B 1 , B 2 ]
   Grammar G ? is more combinatorial than the original grammar G. ?

### 2.5.13.6 Chomsky and Binary Normal Form

   In the Chomsky normal form, there are only two rule types:
   homogeneous binary A → BC where B, C ∈ V
   terminal with singleton right part A → a where a ∈ Σ
   Moreover,iftheemptystringisinthelanguage,thereistheaxiomaticruleS → ε,but
   the axiom S is not allowed in any rule right part. With such constraints, any internal
   node of a syntax tree may have either two nonterminal siblings or one terminal
   sibling.

   Given a grammar, under the simplifying hypothesis of being in nonnullable form,
   we explain how to obtain a Chomsky normal form. Each rule A 0 → A 1 A 2 ... A n
   of length n > 2 is converted into a rule of length 2, by singling out the first symbol
   A 1 and the remaining suffix A 2 ... A n . Then a new ancillary nonterminal is created,
   named ?A 2 ... A n ?, and the new rule:
   ?A 2 ... A n ? → A 2 ... A n
   Now the original rule is replaced by:
   A 0 → A 1 ?A 2 ... A n ?
   After this transformation, the rule right parts of length 2 may still contain terminals,
   which must be transformed into nonterminals. If a rule has the form A → aB, with
   a ∈ Σ, it must be replaced by the following rules, where ?a? is a new ancillary
   nonterminal:
   A → ?a? B ?a? → a
   Proceed similarly in the case the right parts are ab or Bb, with b ∈ Σ, by adding the
   new ancillary nonterminal ?b?.

   Continue applying the same series of transformations to the grammar thus
   obtained, until all the rules are in the form requested.


<a id="P073"></a>

   Table 2.5 Invertible grammar rules created for Example 2.63
   step 1
   maximal set of terminal rules with
   right part aa ? :
   ?
   B 1 → aa ? , B 2 → aa ?
   ?
   rule added to P ? : [B 1 ,B 2 ] → aa ?
   maximal set of terminal rules with
   right part bb ? :
   ?
   B 1 → bb ?
   ?
   rule added to P ? : [B 1 ] → bb ?
   step 2
   V ? = {[B 1 ], [B 1 , B 2 ]}
   right part pattern ρ 1 = c − ε
   P ρ 1 = {S → cS }
   the possible sequences σ are listed along the rules
   created:
   ?[B 1 ]? none
   ?[B 1 ,B 2 ]? none
   no rule is created because the inclusion condition (2.9) is false
   for every σ
   V ? = {[B 1 ], [B 1 , B 2 ]}
   right part pattern ρ 2 = c − d − ε
   P ρ 2 = {S → cB 1 d B 2 }
   ?[B 1 ][B 1 ]? none
   ?[B 1 ][B 1 ,B 2 ]? [S] → c[B 1 ]d[B 1 ,B 2 ]
   ?[B 1 ,B 2 ][B 1 ]? none
   ?[B 1 ,B 2 ][B 1 ,B 2 ]? [S] → c[B 1 ,B 2 ]d[B 1 ,B 2 ]
   V ? = {[B 1 ], [B 1 , B 2 ], [S]},
   right part pattern ρ 2 = c − d − ε
   P ρ 2 = {S → cB 1 d B 2 }
   now V ? has been updated with [S], but it is unproductive to
   reconsider the pattern ρ 2 = c − d − ε, since no rule with such
   pattern contains S in its right part
   V ? = {[B 1 ], [B 1 , B 2 ],[S]}
   right part pattern
   ρ 1 = c − ε
   P ρ 1 = {S → cS }
   ?[S]? [S] → c[S]
   V ? = {[B 1 ], [B 1 , B 2 ], [S]}
   right part pattern ρ 3 = a − a ? − ε
   P ρ 3 =
   ?
   B 1 → aB 1 a ? B 1
   B 2 → aB 2 a ? B 2
   ?
   ?[B 1 ][B 1 ]? [B 1 ] → a[B 1 ]a ? [B 1 ]
   ?[B 1 ][B 1 ,B 2 ]? [B 1 ] → a[B 1 ]a ? [B 1 ,B 2 ]
   ?[B 1 ,B 2 ][B 1 ,B 2 ]? [B 1 ,B 2 ] → a[B 1 ,B 2 ]a ? [B 1 ,B 2 ]
   ?[S][X]? X ∈ V ? none
   ?[X][S]? X ∈ V ? none
   V ? = {[B 1 ], [B 1 , B 2 ], [S]}
   right part pattern ρ 4 = b − b ? − ε
   P ρ 4 =
   ?
   B 1 → bB 1 b ? B 1
   ?
   ?[B 1 ][B 1 ]? [B 1 ] → b[B 1 ]b ? [B 1 ]
   ?[B 1 ][B 1 ,B 2 ]? [B 1 ] → b[B 1 ]b ? [B 1 ,B 2 ]
   ?[B 1 ,B 2 ][B 1 ]? [B 1 ] → b[B 1 ,B 2 ]b ? [B 1 ]
   ?[B 1 ,B 2 ][B 1 ,B 2 ]? [B 1 ] → b[B 1 ,B 2 ]b ? [B 1 ,B 2 ]
   ?[S][X]? X ∈ V ? none
   ?[X][S]? X ∈ V ? none
   step 3
   creates the axiomatic rule S ? → [S]
   Example 2.64 (conversion into Chomsky normal form) The grammar below:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → d A | cB
   A → d AA | cS | c
   B → cBB | d S | d

<a id="P074"></a>

   becomes the following equivalent grammar, in Chomsky normal form:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → ?d ? A | ?c? B
   A → ?d ? ?AA? | ?c? S | c
   B → ?c? ?BB? | ?d ? S | d
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   ?d ? → d
   ?c? → c
   ?AA? → AA
   ?BB? → BB ?
   This form is used in mathematical essays, but rarely in practical work. We observe
   that an almost identical transformation would permit to obtain a grammar where
   every rule right part contains at most two nonterminals. Such form will be used in
   the next section on operator grammars.


### 2.5.13.7 Operator Grammar

   Agrammarisinoperatorformif,foreachrulerightpart,thereisatleastoneterminal
   symbol between any two nonterminal symbols, in formula:
   for every rule A → α it holds α ∩
   ?
   (Σ ∪ V ) ∗ V V (Σ ∪ V ) ∗
   ?
   = ∅
   Since such grammar form is used in some efficient language parsing algorithms, we
   explain how to obtain it, starting from a generic grammar. In reality, most technical
   languages include a majority of rules that already comply with the operator form,
   and a few rules that need to be transformed. For instance, the usual grammar of
   arithmetic expressions is already in operator form:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   E → E + T | T
   T → T × F | F
   F → a | ‘ ( ’E ‘)’
   whereasinthenextgrammaroftheprefix Booleanexpressions,whereeveryoperator
   precedes its operand(s), the first rule is not in operator form:
   ?
   E → BE E | ¬E | a
   B → ∧ | ∨
   Given a grammar G, we explain how to obtain an equivalent grammar H in operator
   form. 27
   27 Following [14], where a correctness proof of the transformation can be found.

<a id="P075"></a>

   1. For each existing nonterminal X and for each terminal a, we create a new 
   nonterminal,denotedX a ,withthestipulationthatnonterminalX a generatesastringu if,
   and only if, nonterminal X generates the string ua. Therefore, for the languages
   respectively generated by nonterminals X and X a by using the rules of grammar
   G and H, the identity holds:
   L G (X) = L H (X a )a ∪ L H (X b )b ∪ ··· ∪ {ε}
   where the union spans all the terminals and the empty string is omitted if it is not
   in L G (X). Clearly, if in L G (X) no string ends by, say, b, the language L H (X b ) is
   empty; therefore, nonterminal X b is useless.

   2. For every rule A → α of G, for every nonterminal X occurring in the right part
   α, and for all the terminals a, b, …, replace X by the set of strings:
   X a a X b b ... ε (2.10)
   where the empty string ε is only needed if X is nullable.

   3. To finish the construction, for every rule A → β a computed at (2.10), we create
   the rule A a → β.

   As in similar cases, the construction may produce also useless nonterminals.
   Example 2.65 (conversion into operator normal form—from [14]) The grammar G
   with rules S → aS S | b is not in operator form. The nonterminals created at step
   1 are S a and S b . The rules created by (2.10) are:
   S → aS a aS a a | aS a aS b b | aS b bS a a | aS b bS b b | b
   The empty string is not in L G (S). Then from step 3, we obtain the rules:
   S a → aS a aS a | aS b bS a S b → aS a aS b | aS b bS b | ε
   At last, since nonterminal S a generates nothing, we delete rules S a → aS a aS a |
   aS b bS a and S b → aS a aS b , and we obtain grammar H:
   H
   ?
   S → aS b bS b b | b
   S b → aS b bS b | ε
   ?
   The operator form, combined with a sort of precedence relation between terminal
   symbols, yields the class of operator precedence grammars, to be studied in Chap. 4
   for their nice application to serial and parallel parsing algorithms.


<a id="P076"></a>


### 2.5.13.8 Conversion of Recursion from Left to Right

   Another normal form, termed nonleft-recursive, is characterized by the absence of
   left-recursive rules or derivations (l-recursions). It is indispensable for the top-down
   parsers to be studied in Chap. 4. We explain how to transform l-recursion into right
   recursion.

   Transformation of Immediate l-Recursion
   The more common and easier case is when the l-recursion to be eliminated is
   immediate. Consider all l-recursive alternatives of a nonterminal A:
   A → Aβ 1 | Aβ 2 | ··· | Aβ h h ≥ 1
   wherenostringβ i isempty,andlettheremainingalternativesofA,whichareneeded
   to terminate the recursion, be:
   A → γ 1 | γ 2 | ··· | γ k k ≥ 1
   Create a new ancillary nonterminal A ? and replace the previous rules with the next
   ones:
   ?
   A → γ 1 A ? | γ 2 A ? | ··· | γ k A ? | γ 1 | γ 2 | ··· | γ k
   A ? → β 1 A ? | β 2 A ? | ··· | β h A ? | β 1 | β 2 | ··· | β h
   Now every original derivation involving l-recursive steps, as for instance
   A ⇒ Aβ 2 ⇒ Aβ 3 β 2
   l-recursive
   ⇒ γ 1 β 3 β 2
   is replaced by the equivalent derivation:
   A ⇒ γ 1 A ? ⇒ γ 1 β 3 A ?
   right-recursive
   ⇒ γ 1 β 3 β 2
   Example 2.66 (conversionofimmediatel-recursionintorightrecursion)Intheusual
   grammar of arithmetic expressions:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   E → E + T | T
   T → T × F | F
   F → ‘ ( ’E ‘ ) ’ | i
   nonterminals E and T are immediately l-recursive. By applying the transformation,
   the right-recursive grammar below is obtained:
   ?
   E → T E ?
   E ? → +T E ? | +T
   ?
   T → F T ? | F
   T ? → ×F T ? | ×F
   F → ‘ ( ’E ‘ ) ’ | i

<a id="P077"></a>

   Actually, in this case, but not always, a simpler solution is possible, to specularly
   reverse the l-recursive rules, thus obtaining:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   E → T + E | T
   T → F × T | F
   F → ‘ ( ’E ‘)’ | i
   ?
   Transformation of Nonimmediate Left Recursion
   Thenextalgorithmisusedtoeliminatenonimmediatel-recursion.Wepresentitunder
   the simplifying assumptions that grammar G is homogeneous, nonnullable and with
   singleton terminal rules; in other words, the rules are like those of the Chomsky
   normal form, but more than two nonterminals are permitted in a rule right part. The
   reader should be able to generalize the algorithm to any context-free grammar.
   Thealgorithmhastwonestedloops.Theinternalloopappliesnonterminalexpan-
   sion to turn nonimmediate l-recursion into immediate l-recursion, and so it shortens
   the derivation length. The external loop turns immediate l-recursion into right 
   recursion, and to do so it creates ancillary nonterminals.

   Let V = {A 1 , A 2 , ..., A m }bethenonterminalalphabetandletA 1 betheaxiom.
   For orderly scanning, we view the nonterminals as an (arbitrarily) ordered set, from
   1 to m.

   Input: a grammar with left recursion (l-recursion)
   Output: an equivalent grammar without l-recursion
   for i := 1 to m do
   for j := 1 to i − 1 do
   replace each rule of type A i → A j α (i > j) by the rules:
   A i → γ 1 α | γ 2 α | ... | γ k α
   where A j → γ 1 | γ 2 | ... | γ k are the alternatives of A j
   // note this may create immediate l-recursions
   eliminate, through the previous algorithm, all the immediate
   l-recursions that may have appeared as an alternative of A i ,
   thus creating the ancillary nonterminal A ? i
   The idea is to modify the rules in such a way that, in the resulting grammar, if
   the right part of a rule A i → A j ... starts with a nonterminal A j , then it is i < j, i.e.,
   the left part nonterminal A i precedes A j in the ordering. This ensures the algorithm
   terminates. 28
   28 A proof of termination and correctness can be found in [9] or in [15].


<a id="P078"></a>

   Table 2.6 Turning recursion from left to right
   i j Algorithm action Transformation of grammar G 3
   1 Skipped Eliminate the immediate left recursion
   of nonterminal A 1 (there is none)
   Unchanged
2 1 Replace the rule A 2 → A 1 d with those
   obtained by expanding nonterminal A 1
   ?
   A 1 → A 2 a | b
   A 2 → A 2 c | A 2 ad | bd | e
   2 Terminated Eliminate two immediate left
   recursions of nonterminal A 2 and
   finally obtain grammar G ? 3
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   A 1 → A 2 a | b
   A 2 → bd A ? 2 | eA ? 2 | bd | e
   A ? 2 → cA ? 2 | ad A ? 2 | c | ad
   Example 2.67 (conversion of nonimmediate l-recursion into right recursion) We
   apply the algorithm to the grammar G 3 below:
   G 3
   ?
   A 1 → A 2 a | b
   A 2 → A 2 c | A 1 d | e
   which has the l-recursion A 1 ⇒ A 2 a ⇒ A 1 d a. In Table 2.6, we list the steps that
   produce grammar G ? 3 , which is free from l-recursion. ?
   It would be straightforward to modify the algorithm to turn right recursion into
   left recursion, which is a conversion sometimes applied to speed up the bottom-up
   parsing algorithms of Chap. 4.


### 2.5.13.9 Greibach and Real-Time Normal Form

   In the real-time normal form, every rule starts with a terminal:
   A → aα where a ∈ Σ and α ∈ (Σ ∪ V ) ∗
   A special case of real-time form is the Greibach normal form:
   A → aα where a ∈ Σ and α ∈ V ∗
   Everyrulestartswithaterminal,followedbyzeroormorenonterminals.Tobeexact,
   both forms exclude the empty string from the language.

   Thedesignation‘real-time’willbelaterunderstoodasapropertyofthepushdown
   automaton that recognizes the language. At each step, the automaton reads and
   consumes an input character, i.e., a terminal; therefore, the total number of steps
   equals the length of the string to be recognized.

   For simplicity, we assume that the given grammar is nonnullable, and we explain
   how to obtain the above forms. For the real-time form: first eliminate all the left

<a id="P079"></a>

   recursions, and then, by elementary transformations, expand any nonterminal that
   occurs in the first position of a rule right part, until a terminal prefix is produced.
   Subsequently, continue for the Greibach form: if in any position other than the first,
   a terminal occurs, replace it by an ancillary nonterminal, and add the terminal rule
   that derives the terminal.

   Example 2.68 (conversion into Greibach normal form) The grammar below:
   ?
   A 1 → A 2 a
   A 2 → A 1 c | bA 1 | d
   is converted into Greibach form in the following three steps:
   1. Eliminate l-recursions by the step:
   A 1 → A 2 a A 2 → A 2 ac | bA 1 | d
   and then:
   A 1 → A 2 a
   ?
   A 2 → bA 1 A ? 2 | d A ? 2 | d | bA 1
   A ? 2 → acA ? 2 | ac
   2. Expand the nonterminals in first position until a terminal prefix surfaces:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   A 1 → bA 1 A ? 2 a | d A ? 2 a | d a | bA 1 a
   A 2 → bA 1 A ? 2 | d A ? 2 | d | bA 1
   A ? 2 → acA ? 2 | ac
   3. Substitute the ancillary nonterminals for any terminal in a position other than the
   first:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   A 1 → bA 1 A ? 2 ?a? | d A ? 2 ?a? | d ?a? | bA 1 ?a?
   A 2 → bA 1 A ? 2 | d A ? 2 | d | bA 1
   A ? 2 → a ?c? A ? 2 | a ?c?
   ?a? → a
   ?c? → c
   If the transformation were stopped before the last step, the grammar would be in
   real-time form, but not in Greibach form. ?

<a id="P080"></a>


### 2.5.13.10 Final Remarks on Normal Forms

   Among the many ways that CF grammars may be classified is that of ‘shape of the
   rules’, e.g., Chomsky normal form, operator normal form, real-time and Greibach
   normal form, linear, and right-linear. We have shown that some of these kinds, e.g.,
   the first four above, can be used to define any CF language.

   Other grammar forms with the same property have been studied in the literature,
   such as the double Greibach normal form, which is a special case of real-time form
   and has rules of the following shapes:
   A → aαb where a, b ∈ Σ and α ∈ V +
   A → u where u ∈ Σ +
   Clearly, the shape of the rules makes it impossible to have left-recursive and 
   rightrecursive derivations, i.e., all the recursive derivations are self-nesting (see Table 2.3
   on p. 36). We have seen on p. 76 that left-recursive derivations can be replaced by
   right-recursiveones.Nowweshowhowtoeliminatealsoright-recursivederivations,
   by converting a grammar into the double Greibach form.

   Example 2.69 (conversion into double Greibach normal form) The language
   {a ∗ a n b n | n ≥ 1}, defined by the grammar below (left) through using 
   rightrecursive and self-nesting rules:
   ?
   S → aS | X
   X → aX b | ab
   ?
   S → aS b | ab | aY b
   Y → aY a | aa | a
   (2.11)
   is also defined by the grammar (right) without any use of left/right-recursive
   rules. ?
   How would we go, if we wanted to prove that any CF grammar can be converted
   into the double Greibach normal form? To answer once and for all this and similar
   questions for various rule shapes, a more general form of grammar rule, called
   position restricted, has been studied. 29 A grammar is position restricted of type
   (m 1 , ..., m n+1 ), where n ≥ 2 and each m i is a nonnegative integer, if each rule is
   eitheraterminalruleA → u,withu anonemptyterminalstring,orhasthefollowing
   form:
   A → v 1 A 1 v 2 ... v n A n v n+1
   where |v 1 | = m 1 ,...,|v n+1 | = m n+1 and ∀i A i ∈ V,v i ∈ Σ ∗
   (2.12)
   For instance, any rule in Chomsky normal form A → A 1 A 2 is position restricted of
   type (m 1 = 0, m 2 = 0, m 3 = 0).

   29 We refer to Blattner and Ginsburg [16] for a complete presentation.


<a id="P081"></a>

   Atheoreticalresultofsomegeneralityandinterestsaysthat,foreverycontext-free
   grammarandforeverytuple(m 1 , ..., m n+1 )withn ≥ 2,thereexistsanequivalent
   position restricted grammar of type (m 1 , ..., m n+1 ).

   We observe that the result above says more than the already stated properties
   about the operator/Greibach/double-Greibach grammar normal form. In particular,
   it says that each one of such grammar forms can be further restricted to have only
   two nonterminal symbols in the right part, provided that we allow also terminal rules
   A → u where the length of u is not necessarily one. Thus, the following different
   restrictedformsenjoythepropertyofbeingnormalforms(lettersbandcareterminal
   characters):
   position restricted type rule name
   m 1 = 0, m 2 = 1, m 3 = 0 A → A 1 bA 2 binary infix operator form
   m 1 = 1, m 2 = 0, m 3 = 0 A → bA 1 A 2 binary Greibach form
   m 1 = 0, m 2 = 0, m 3 = 1 A → A 1 A 2 b reversed binary Greibach form
   m 1 = 1, m 2 = 0, m 3 = 1 A → bA 1 A 2 c binary double Greibach form
   To illustrate, we show the (ugly) grammar G in binary Greibach form equivalent to
   the grammars (2.11) above.

   G
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → ab | aab | aAZ
   A → a | aa | aAA
   Z → aZ B | ab
   B → b
   L A (G) =
   ?
   a k | k ≥ 1
   ?
   L Z (G) = {a n b n | n ≥ 1}
   Obviously, if in a certain normal form we permit to have also some rules with extra
   shapes, the resulting grammar type remains a fortiori a normal form. For instance,
   we may allow in the binary infix operator form also rules A → aA 1 bA 2 of type
   (m 1 = 1, m 2 = 1, m 3 = 0) that feature a prefix operator a and an infix operator b.
   Having a richer set of shapes usually results in a grammar that is more readable, and
   especially that has a structure better suited to the language semantic.

   Weaddalastremarkonthelinearrules(justonenonterminalsymbolintheirright
   part). Such rules are not considered in the definition of the position restricted form,
   since such a form requires at least 3-tuples of integers. The reason is that linear rules
   alone do not suffice to generate all the context-free languages, as we will discuss
   later.

   To finish, although not all the grammar transformations studied will be used in
   this book, in particular not the Chomsky and Greibach ones, still practicing with
   them is recommended as an exercise for becoming fluent in grammar design and
   manipulation, a skill certainly needed in language and compiler engineering.

<a id="P082"></a>

2.6 Grammars of Regular Languages
=================================

   Since regular languages are a rather limited class of context-free languages, it is
   not surprising that their grammars admit severe restrictions, to be next considered.
   Furthering the study of regular languages, we will also see that longer sentences
   have unavoidable repetitions, a property that can be exploited to prove that certain
   context-free languages are not regular. Other contrastive properties of the families
   REG and CF will emerge in Chaps. 3 and 4 from the consideration of the amount
   of memory needed to check whether a string is in a language, which is finite for the
   former family and unbounded for the latter.


## 2.6.1 From Regular Expression to Grammar

   For a language defined with a regular expression, it is straightforward to write a
   grammar,byanalyzingtheexpressionandmappingitssubexpressionsontogrammar
   rules. At the construction heart, the iterative operators (star and cross) are replaced
   by unilaterally recursive rules.

   Algorithm 2.70 (conversionofregularexpressionintogrammar) First,weidentify
   and label with a different number each subexpression contained in the given regular
   expression r. From the very definition of r.e., the possible cases and corresponding
   grammar rules (with uppercase nonterminals) are listed in Table 2.7. Notice that we
   allow an empty string ε as a term.

   To shorten the grammar, if in any row a term r i is a terminal or ε, we do not
   introduce a corresponding nonterminal E i , but we write it directly in the rule. Notice
   that rows 3 and 4 offer the choice of left or right-recursive rules.

   To apply this conversion scheme, each subexpression label is assigned to a
   nonterminalasadistinguishingsubscript.Atthestart,theaxiomisassignedtothewhole
   r.e. An example suffices to understand the procedure. ?
   Example 2.71 (from r.e. to grammar) The expression e below:
   e = (abc) ∗ ∪ (f f ) +
   is analyzed into the arbitrarily numbered subexpressions shown in the tree below,
   which is a sort of abstract syntax tree (see p. 45) of the r.e. with added numbering:

<a id="P083"></a>

   Table 2.7 From regular subexpression to grammar rules
      # Regular (sub)expression Grammar rule Detail
      1 r = r 1 · r 2 · ··· · r k E → E 1 E 2 ... E k k ≥ 2
      2 r = r 1 ∪ r 2 ∪ ··· ∪ r k E → E 1 | E 2 | ··· | E k k ≥ 2
      3 r = (r 1 ) ∗ E → E E 1 | ε or E → E 1 E | ε
      4 r = (r 1 ) + E → E E 1 | E 1 or E → E 1 E | E 1
      5 r = b E → b b ∈ Σ
      6 r = ε E → ε
   Table 2.8 Mapping the regular expression of Example 2.71 onto grammar rules
   Row in Table 2.7 Regular (sub)expression Grammar rule
      2 E 1 ∪ E 2 E 0 → E 1 | E 2
      3 E ∗
      3
      E 1 → E 1 E 3 | ε
      4 E +
      4
      E 2 → E 2 E 4 | E 4
      1 abc E 3 → abc
      1 f f E 4 → f f
      ∪ (E 0 )
      ∗ (E 1 )
      • (E 3 )
      a b c
      + (E 2 )
      • (E 4 )
      f f
   We see that symbol E 0 (corresponding to the whole r.e. e) is the union of 
   subexpressions E 1 and E 2 , symbol E 1 is the star of subexpression E 3 , etc.

   ThemappingofTable2.7yieldsthegrammarrulesinTable2.8.AxiomE 0 derives
   the sentential forms E 1 and E 2 , nonterminal E 1 generates the form E ∗
   3 , and from that
   the string form (abc) ∗ . Similarly, nonterminal E 2 generates the sentential form E +
   4
   and finally the string form (f f ) + . ?
   We have several important remarks on the properties of grammars obtained from
   regular expressions through the mapping above.

   First, any recursive rule in such a grammar is not self-nesting since it is either
   left- or right-recursive (lines 3 and 4 of Table 2.7). Moreover, the only recursive
   derivations in such grammars are immediate and correspond to the rules of lines 3 and

<a id="P084"></a>

   4. Therefore, such grammars never produce self-nesting derivations. We anticipate
   that the absence of such derivations in a grammar ensures that the language generated
   is regular.

   Second, notice that if a regular expression is ambiguous (see the definition on p.
   22), the corresponding grammar is so as well (see Example 2.75 on p. 85). Third,
   the grammar can be much more concise than the regular expression, because it saves
   the rules that define repeated subexpressions. To see that, consider the r.e. below:
   e ? = (abc) ∗ ∪
   ?
   f (abc) ∗ f
   ? +
   which differs from e of Example 2.71 by the repetition of subexpression (abc) ∗
   in the second subexpression. Clearly, the grammar constructed for e ? can reuse the
   rules E 1 → E 1 E 3 | ε and E 3 → abc inside the rules for the second term of e ? :
   E 4 → f E 1 f .

   We have seen how to map each r.e. operator onto equivalent grammar rules, to
   generate the same language. It follows that every regular language is context-free.
   Moreover, we intuitively know about context-free languages that are not regular,
   e.g., palindromes and Dyck language, and we shall see a formal proof of that in
   Example 2.81 on p. 90. Therefore, the following property holds.

   Property 2.72 (language family inclusion) The family REG of regular languages
   is strictly included in the family CF of context-free languages, i.e., REG ⊂ CF. ?

## 2.6.2 Linear and Unilinear Grammars

   Algorithm 2.70 converts a regular expression into a grammar and substantially
   preserves the r.e. structure. But for a regular language, it is possible to constrain the
   grammar to a very simple rule form, called unilinear or type 3. Such a form gives
   evidence to some fundamental properties and leads to a straightforward construction
   of the automaton that recognizes the strings of a regular language.

   We recall that a grammar is linear if every rule has the form:
   A → uBv where u, v ∈ Σ ∗ and B ∈ (V ∪ {ε}) (2.13)
   that is, at most one nonterminal occurs in the rule right part.

   When we draw a syntax tree, we see that it never branches into two subtrees,
   but that it has a linear structure made by a stem with leaves directly attached to it.
   Linear grammars are not powerful enough to generate any context-free language (an
   exampleistheDycklanguage),buttheyalreadyexceedthepowerneededforregular
   languages. For instance, the following well-known subset of the Dyck language is
   generated by a linear grammar, though it is not regular (to be proved on p. 94).

<a id="P085"></a>

   Example 2.73 (nonregular linear language) The linear grammar S → bS e | be
   generates language L 1 :
   L 1 =
   ?
   b n e n | n ≥ 1
   ?
   = {be, bbee, ...} ?
   Definition 2.74 (left-linearity and right-linearity) The rules of the following forms
   are called right-linear and, symmetrically, left-linear:
   right-linear rule A → uB where u ∈ Σ ∗ and B ∈ (V ∪ ε)
   left-linear rule A → Bu with the same stipulations
   Both cases are linear and obtained by deleting on either side one of the two terminal
   stringsthatembracenonterminalBinalineargrammar;see(2.13).Agrammarwhere
   all the rules are either right-linear or left-linear is termed unilinear or of type 3. 30 ?
   Foraright-lineargrammar,everysyntaxtreehasanobliquestemslopingtotheright,
   or to the left for a left-linear grammar. Moreover, if the grammar is recursive, it is
   necessarily right-recursive.

   Example 2.75 (unilineargrammar)Thestringscontainingasubstringaaandending
   with a letter b are defined by the (ambiguous) regular expression:
   (a | b) ∗ aa (a | b) ∗ b
   The language is generated by the unilinear grammars G r and G l :
   right-linear grammar left-linear grammar
   G r
   ?
   S → aS | bS | aaA
   A → aA | bA | b
   G l
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → Ab
   A → Aa | Ab | Baa
   B → Ba | Bb | ε
   An equivalent nonunilinear grammar is obtained by the algorithm on p. 82:
   ?
   E 1 → E 2 aaE 2 b
   E 2 → E 2 a | E 2 b | ε
   With grammar G l , the leftward trees of the ambiguous sentence baaab are:
   30 Within the Chomsky hierarchy (p. 105).


<a id="P086"></a>

   S
   A
   B
   B
   B
   ε
   b
   a
   aa
   b
   S
   A
   A
   B
   B
   ε
   b
   aa
   a
   b
   ?
   Example 2.76 (parenthesis-free arithmetic expressions) The language L:
   L = {a, a + a, a × a, a + a × a, ...}
   is defined by the right-linear or left-linear grammars G r or G l :
   G r
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → a
   S → a + S
   S → a × S
   G l
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → a
   S → S + a
   S → S × a
   By the way, neither grammar is adequate to impose a precedence between arithmetic
   operations. ?
   Strictly Unilinear Grammars
   The unilinear rule form can be further constrained, with the aim of simplifying the
   coming discussion on the theoretical properties and the construction of automata for
   language recognition. A grammar is strictly unilinear if every rule contains at most
   one terminal symbol, i.e., if it has the form:
   A → aB (or A → Ba) where a ∈ (Σ ∪ ε) and B ∈ (V ∪ ε)
   A further simplification is possible: to impose that the only terminal rules are the
   emptyones.Inthiscase,wemayassumethatthegrammarcontainsjustthefollowing
   rule types:
   A → aB | ε (or A → Ba | ε) where a ∈ Σ and B ∈ V
   Summarizing the discussion, we may indifferently use a grammar in unilinear form
   or strictly unilinear form, and we may additionally choose to have as terminal rules
   only the empty ones.


<a id="P087"></a>

   Example 2.77 (Example 2.76 continued) By adding ancillary nonterminals, the
   right-linear grammar G r is transformed into an equivalent strictly right-linear
   grammar G ? r (left) and also into one with null terminal rules G ??
   r
   (right):
   G ? r
   ?
   S → a | aA
   A → +S | ×S
   G ??
   r
   ?
   S → aA
   A → +S | ×S | ε
   ?

## 2.6.3 Linear Language Equations

   We continue the study of unilinear grammars, and we show that the languages they
   generate are regular. The proof consists of transforming the unilinear rules into a set
   of linear equations, which have regular languages as their solution. In Chap. 3, we
   will see that every regular language can be defined by a unilinear grammar, and this
   proves the identity of the language families defined by regular expressions and by
   unilinear grammars.

   For simplicity, consider a grammar G = (V, Σ, P, S ) in strictly right-linear
   form with null terminal rules (the case of left-linear grammars is analogous). Every
   rule can be transcribed into a linear equation that has as unknowns the languages
   generated from each nonterminal. For each nonterminal A, we shorten L A (G) as L A :
   L A =
   ?
   x ∈ Σ ∗ | A
   +
   = ⇒ x
   ?
   In particular, it holds L(G) ≡ L S . A string x ∈ Σ ∗ is in language L A if:
   • string x is empty, and set P contains rule A → ε
   • string x is empty, set P contains rule A → B, and it holds ε ∈ L B
   • string x = ay starts with character a, set P contains rule A → aB, and string
   y ∈ Σ ∗ is in the language L B
   Let n = |V | be the number of nonterminals of grammar G. Each nonterminal A i is
   defined by a set of alternatives:
   A i → aA 1 | bA 1 | ··· | aA n | bA n | ··· | A 1 | ··· | A n | ε
   some possibly missing. 31 We write the corresponding linear equation:
   L A i = aL A 1 ∪ bL A 1 ∪ ··· ∪ aL A n ∪ bL A n ∪ ··· ∪ L A 1 ∪ ··· ∪ L A n ∪ ε
   The last term disappears if the rule does not contain the alternative A i → ε.
   31 In particular, alternative A i
   → A i is never present since the grammar is noncircular.


<a id="P088"></a>

   This system of n simultaneous equations in n unknowns (languages generated by
   nonterminals) can be solved through the well-known Gaussian elimination method,
   by applying the following formula to break recursion.

   Property 2.78 (Arden identity) The equation in the unknown language X:
   X = K X ∪ L (2.14)
   where K is a nonempty language and L is any language, has one and only one
   solution:
   X = K ∗ L (2.15)
   ?
   It is simple to see that language K ∗ L is a solution of (2.14), since by substituting it
   for the unknown in both sides, the equation turns into the identity:
   K ∗ L =
   ?
   K K ∗ L
   ?
   ∪ L
   We omit the proof that the only solution is (2.15).

   Example 2.79 (language equations) The right-linear grammar below:
   ?
   S → sS | eA
   A → sS | ε
   defines a list of (possibly missing) elements e, divided by a separator s. It is
   transcribed into the system:
   ?
   L S = sL S ∪ eL A
   L A = sL S ∪ ε
   First, substitute the second equation into the first:
   ?
   L S = sL S ∪ e (sL S ∪ ε)
   L A = sL S ∪ ε
   Then apply the distributive property of concatenation with respect to union, to
   factorize variable L S as a common suffix:
   ?
   L S = sL S ∪ esL S ∪ e = (s ∪ es) L S ∪ e
   L A = sL S ∪ ε
   Now apply Arden identity to the first equation, thus obtaining:

<a id="P089"></a>

   ?
   L S = (s ∪ es) ∗ e
   L A = sL S ∪ ε
   and finally obtain L A = s (s ∪ es) ∗ e ∪ ε, by substitution. ?
   It would be straightforward to write linear equations also for a grammar that is
   unilinear, but not in the strict sense.

   We have thus proved that every unilinearly generated language is regular. A
   complementary method for computing the regular expression of a language defined by a
   finite automaton will be described in Chap. 3.

2.7 Comparison of Regular and Context-Free Languages
====================================================

   In this section, we present certain useful properties and techniques that permit to
   compare more closely regular and context-free languages, and we present a new
   very expressive model that combines regular expressions and grammar rules.
   Next, we introduce a property that permits to understand the scope of regular
   languages,inordertorealizewhichconstructscanbethusdefinedandwhichinstead
   requirethefullpowerofcontext-freegrammars.Recallfirstthat,inordertogenerate
   aninfinitelanguage,agrammarhastoberecursive(Property2.37onp.41),asonlya
   recursive derivation, such as A
   +
   = ⇒ uAv, can be iterated for n (unbounded) times and
   can thus produce string u n Av n . This fact leads to observe that any sufficiently long
   sentence has to include a recursive derivation in its generation, therefore it contains
   certain substrings that can be repeated for unboundedly many times, thus producing
   a longer sentence of the language. The observation will be stated more precisely,
   first for the unilateral grammar, then for the context-free ones.

   Property 2.80 (pumping of strings) Take a unilinear grammar G. For any
   sufficiently long sentence x, meaning of length greater than some constant that only
   depends on the grammar, it is possible to find a factorization x = t uv, where
   substring u is not empty, such that, for every n ≥ 0, the string t u n v is in the language
   (it is customary to say that the given sentence can be ‘pumped’ by injecting the
   substring u for arbitrarily many times). ?
   Proof Take a strictly right-linear grammar and let k be the number of nonterminal
   symbols. Observe the syntax tree of any sentence x of length k or more. Clearly, two
   nodes exist with the same nonterminal label A:

<a id="P090"></a>

   S
   a 1 ...

   a 2 ...

   ... A
   b 1 ...

   b 2 ...

   ... A
   c 1 ...

   c 2 ...

   ...

   t
   u
   v
   x
   Consider the factorization into t = a 1 a 2 ..., u = b 1 b 2 ... and v = c 1 c 2 ... c m .
   Therefore, there is a recursive derivation:
   S
   +
   = ⇒ t A
   +
   = ⇒ t uA
   +
   = ⇒ t uv
   that can be repeated to generate the strings t uuv, t u ... uv, and t v. ?
   Property 2.80 is next exploited to demonstrate that a quite simple context-free
   language is not regular.

   Example 2.81 (language with two equal powers) Consider the familiar context-free
   language L 1 :
   L 1 =
   ?
   b n e n | n ≥ 1
   ?
   and assume by contradiction that it is regular. Take a sentence x = b k e k , with k
   largeenough,andbreakitintothreesubstringsx = t uv,withsubstringu notempty.
   Depending on the positions of the two divisions, the substrings t, u, and v are as in
   the following scheme:
   b ... b
   t
   b ... b
   u
   b ... be ... ... ... e
   v
   b ... b
   t
   b ... ... be ... ... e
   u
   e ... e
   v
   b ... ... ... be ... e
   t
   e ... e
   u
   e ... e
   v
   Pumping, i.e., repeating, the middle substring u will lead to a contradiction in all
   cases. For row one, if u is repeated twice, the number of letters b exceeds the number
   of letters e, thus causing the pumped string not to be in the language. For row two,
   when repeating u twice, the string t uuv contains a pair of substrings be and does
   not conform to the language structure. Finally for row three, when repeating u, the

<a id="P091"></a>

   number of e exceeds the number of b. In all cases, the pumped strings are not valid
   and Property 2.80 is contradicted. This completes the proof that the language is not
   regular. ?
   Example 2.81 above should have convinced the reader that the regular family is too
   narrow for modeling some typical constructs of technical languages. Yet, it would
   be foolish to discard regular expressions, as they perfectly fit for modeling some
   most common parts of technical languages: on one hand, there are the substrings
   that make the so-called lexicon, e.g., numerical constants and identifiers, and on the
   other hand, there are many constructs that are variations over the list paradigm, e.g.,
   lists of procedure parameters or instructions.

   Role of Self-nesting Derivations
   Since we have ascertained that the REG family is strictly included within the CF
   family, we focus on what makes some typical languages not regular, e.g., the two-power
   language, Dyck or palindromes. A careful observation reveals that their grammars
   have a common feature. All of them use some recursive derivation that is neither left
   nor right, but is self-nesting:
   A
   +
   = ⇒ αAβ α ?= ε and β ?= ε
   A grammar is not self-nesting if, for all nonterminals A, every derivation A
   +
   = ⇒ αAβ
   has either α = ε or β = ε.

   On the contrary, self-nesting derivations cannot be obtained with the grammars
   that we know to generate regular languages, namely those equivalent to a regular
   expression (construction of Table 2.7 on p. 83) and the left-linear or right-linear
   grammars; all of them produce only unilateral recursion.

   It is the absence of self-nesting recursion that allowed us to solve linear equations
   by the Arden identity (p. 88). Disallowing self-nested derivations drastically reduces
   the generative capacity of context-free grammars, as next stated.

   Property 2.82 (regularity of a language) A language L is regular if, and only if,
   there exists a nonself-nesting grammar for L. ?
   We have already argued that, if L is regular, then it is generated by a nonself-nesting
   (right-linear) grammar. The proof that, if L is generated by a nonself-nesting
   grammar, then L is regular, relies on an exhaustive analysis of all the possible derivations
   and is omitted. 32
   32 The proof can be found in the book by Harrison [13]. Other interesting properties of 
   nonselfnesting grammars are in [17].


<a id="P092"></a>

   Example 2.83 (nonself-nesting grammar) The grammar G below:
   G
   ?
   S → AS | bA
   A → aA | ε
   does not permit self-nesting derivations, though it is not unilinear. Thus, language
   L(G) is regular, as we can see by solving the language equations:
   ?
   L S = L A L S ∪ bL A
   L A = aL A ∪ ε
   Arden identity
   =======⇒
   applied to L A
   ?
   L S = L A L S ∪ bL A
   L A = a ∗
   and obtain L S = (a ∗ ) ∗ ba ∗ = a ∗ ba ∗ , by substitution and simplification. ?
   Context-Free Languages Over One-Letter Alphabet
   Given a grammar G, by Property 2.82 the absence of self-nesting derivations is
   a sufficient condition for the regularity of L(G). But the presence of self-nesting
   derivations does not necessarily cause the language to be nonregular. On the way
   to illustrate this fact, we take the opportunity to mention a curious property of the
   context-free languages that have a one-letter (or unary) alphabet.

   Property 2.84 (context-free language over a one-letter alphabet) Every language
   defined by a context-free grammar over a one-letter alphabet Σ, |Σ | = 1, is
   regular. ?
   Notice that the sentences x over a one-letter alphabet are in one-to-one 
   correspondence with the integers, via mapping x ↔ n if and only if |x| = n.

   Example 2.85 (grammar over a one-letter alphabet) The grammar G below:
   G: S → aS a | ε
   hastheself-nestingderivationS ⇒ aS a,butL(G) = {a n a n | n ≥ 0} = (aa) ∗ is
   regular. A right-linear grammar equivalent to G is easily obtained:
   S → aaS | ε
   by shifting to suffix the nonterminal S that was placed in the middle. ?
   From Property 2.84, it follows that any unary language that is not regular cannot be
   either context-free. An example is the set of all prime numbers represented as unary
   strings:
   ?
   a, a 3 , a 5 , a 7 , a 11 , ...

   ? .


<a id="P093"></a>


## 2.7.1 Limits of Context-Free Languages

   In order to understand what cannot be done with context-free grammars, we study
   the unavoidable repetitions that are found in the longer sentences of such languages,
   much as we did for regular languages. We will see that longer sentences necessarily
   contain two substrings that can be repeated for the same unbounded number of times
   by applying a self-nesting derivation. This property will be exploited to prove that
   context-free grammars are unable to generate certain languages where three or more
   parts are repeated for the same number of times.

   Property 2.86 (language with three equal powers) The language L below:
   L =
   ?
   a n b n c n | n ≥ 1
   ?
   is not context-free. ?
   Proof Bycontradiction,assumethatagrammar G ofLexistsandimaginethesyntax
   treeofsentencex = a n b n c n (n ≥ 0).Focusonthepathsfromroot(axiomS)toleaf:
   at least one of such paths must have a length increasing with the length of x. Since
   n is unbounded, such a path necessarily traverses two nodes labeled with the same
   nonterminal, i.e., A. The situation is schematized in the draft tree (dashed edges may
   contain other nodes):
   S
   t A
   u A
   v
   w
   z
   where strings t, u, v, w, and z are terminal. The corresponding derivation:
   S
   +
   = ⇒ t Az
   +
   = ⇒ t uAwz
   +
   = ⇒ t uv wz
   contains a recursive subderivation from A to A, which can be repeated any number
   j ≥ 0 of times, thus producing strings of type:
   y = t u ...... u
   for j times
   v w ...... w
   for j times
   z
   Now examine all the possibilities for the strings u and w:

<a id="P094"></a>

   • both contain only one and the same character, say a. Thus, as j increases, string y
   will have more a than b, hence is not in the language.

   • stringucontainstwoormoredifferentcharacters,forinstanceu = ... a ... b ....
   Then, by repeating the recursive part of the derivation, we obtain uu = ... a ...
   b ... a ... b ..., wherein characters a and b are mixed up, hence string y is not
   in the language. We omit the analogous case when string w contains two different
   characters.

   • stringucontainsonlyonecharacter,saya,andstringw containsonlyonedifferent
   character, say b. When j increases, string y contains a number of a greater than
   that of c; hence, it is not valid. ?
   This reasoning exploits the possibility of pumping a sentence by repeating a recursive
   derivation. It is a useful conceptual tool for proving that certain languages are not in
   the family CF.

   Although the language with three equal powers does not have any practical
   relevance, it illustrates a kind of agreement or concordance that cannot be enforced by
   context-free rules. The next case considers a construct more relevant for technical
   languages.

   Language of Copies or Replica
   An outstanding abstract paradigm is replica, to be found in many technical contexts,
   whenever two lists contain elements that must be pairwise identical or more 
   generally agree with each other. A concrete case is provided by the declaration and the
   invocation of the same procedure: the correspondence between a ‘formal parameter’
   list, like, e.g., P(f 1 : int,f 2 : real,...), and a matching ‘actual parameter’ list, like
   P(i,r,...). A similar example from English is:
   cats,vipers,crickets and lions are respectively
   mammals,snakes, insects and mammals
   In the most abstract form, the two lists are made by the same alphabet and replica is
   the following language L replica :
   L replica =
   ?
   uu| u ∈ Σ +
   ?
   Let Σ = {a, b}. In some respect, a sentence x = abbbabbb = uu is analogous
   to a palindrome y = abbbbbba = uu R , but string u is copied in x and specularly
   reversed in y. We may say that the symmetry of the sentences of L replica is 
   translational, not specular. Strangely enough, whereas palindromes are a most simple
   context-free language, the replica language is not context-free. The reason is that
   the two symmetries require quite different control mechanisms: a LIFO (last in first
   out) pushdown stack for specular symmetry and a FIFO (first in first out) queue for
   translational symmetry. We will see in Chap. 4 that the algorithms (or automata) that
   recognize context-free languages use a LIFO memory.


<a id="P095"></a>

   In order to show that replica is not in CF, one should apply again the pumping
   reasoning, but before doing so, we have to filter the language and render it similar
   to the three equal power languages of Property 2.86.

   We focus on the following subset of language L replica , obtained by means of an
   intersection with a regular language:
   L abab =
   ?
   a m b n a m b n | m, n ≥ 1
   ?
   = L replica ∩ a + b + a + b +
   We state (anticipating the proof on p. 213) that the intersection of a context-free
   language with a regular one is always a context-free language. Therefore, if we
   prove that L abab is not in CF, we may conclude the same for L replica .

   Forbrevity,theanalysisofthepossiblecasesforpumpingstringsisomitted,since
   it closely resembles the discussion in the previous proof (p. 93).


## 2.7.2 Closure Properties of Families REG and CF

   We know that language operations are used to combine existing languages into new
   ones with the aim of extending, filtering, or modifying a given language. Yet not all
   the operations preserve the class or family the given languages belong to. When the
   result of an operation exits from the starting family, it cannot be generated with the
   same type of grammar.

   We continue the comparison between regular and context-free languages, and in
   Table 2.9 we resume their closure properties with respect to language operations:
   some closures are already known, others are immediate, and a few need to await the
   results of automata theory to be proved. We denote a generic context-free language
   and a regular language as L and R, respectively. A few comments and examples
   follow.

   • A nonmembership, such as ¬L / ∈ CF, means that the left term does not always
   belong to the family. Anyway, this does not exclude, for instance, that the
   complement of some context-free language is still context-free.

   • The reversal of language L(G) is generated by the mirror grammar, which is
   obtained by reversing the rule right parts. Clearly, if grammar G is right-linear,
   the mirror one is left-linear and defines a regular language.

   Table 2.9 Closure properties of language families REG and CF
   Reversal Star Union or
   concatenation
   Complement Intersection
   R R ∈ REG R ∗ ∈ REG R 1 ⊕ R 2 ∈ REG ¬R ∈ REG R 1 ∩ R 2 ∈ REG
   L R ∈ CF L ∗ ∈ CF L 1 ⊕ L 2 ∈ CF ¬L / ∈ CF L 1 ∩ L 2 / ∈ CF
   L ∩ R ∈ CF

<a id="P096"></a>

   • We know that the star, union, and concatenation of context-free languages are
   context-free. Let G 1 and G 2 be the grammars of languages L 1 and L 2 , let S 1
   and S 2 be their axioms, and suppose that their nonterminal sets are disjoint, i.e.,
   V 1 ∩ V 2 = ∅. To obtain the new grammar in the three cases, add to the rules of G 1
   and G 2 the following axiomatic rules:
   star union concatenation
   S → S S 1 | ε S → S 1 | S 2 S → S 1 S 2
   In the case of union, if the grammars are right-linear, so is the new grammar.
   On the contrary, although the new rules introduced for concatenation and star are
   not right-linear, we have observed before that the grammars are not self-nesting
   and define regular languages (p. 91); the family REG is in fact closed under such
   operations (Property 2.23 on p. 25).

   • The complement of a regular language is regular; see the construction using finite
   automata in Chap. 3 (p. 189).

   • In general, the intersection of context-free languages is not context-free, as
   witnessed by the language with three equal powers (Property 2.86 on p. 93):
   ?
   a n b n c n | n ≥ 1
   ? = ?
   a n b n c + | n ≥ 1
   ?
   ∩
   ?
   a + b n c n | n ≥ 1
   ?
   where the two components are easily defined by context-free grammars.

   • As a consequence of the De Morgan identity, in general the complement of a
   context-free language is not context-free. As L 1 ∩ L 2 = ¬ (¬L 1 ∪ ¬L 2 ), if the
   complementwerecontext-free,acontradictionwouldensuesincetheunionoftwo
   context-free languages is context-free.

   • On the other hand, the intersection of a context-free language and a regular one is
   context-free. The proof will be given on p. 213.

   In order to make a grammar more discriminatory, the last property can be exploited
   to filter a language with a regular one, which forces some constraints on the original
   sentences.

   Example 2.87 (regularfilterontheDycklanguage(p.49)) Itisinstructivetoseehow
   the freely parenthesized sentences of a Dyck language L D of alphabet Σ =
   ?
   a, a ?
   ?
   can be filtered, by intersecting with the regular languages:
   L 1 = L D ∩ ¬
   ?
   Σ ∗ a ? a ? Σ ∗
   ? = ?
   aa ?
   ? ∗
   L 2 = L D ∩ ¬
   ?
   Σ ∗ a ? aΣ ∗
   ? = ?
   a n
   ?
   a ?
   ? n
   | n ≥ 0
   ?
   The first intersection preserves the sentences that do not contain any substring a ? a ? ,
   i.e.,iteliminatesallthesentenceswithnestedparentheses.Thesecondfilterpreserves
   thesentencesthathaveexactlyoneparenthesisnest.LanguagesL 1 andL 2 arecontext-
   free, and the former is even regular. ?

<a id="P097"></a>


## 2.7.3 AlphabeticTransformations

   It is a common feeling to find conceptually similar the languages that only differ by
   theirconcretesyntax,i.e.,thechoiceofterminals.Forinstance,indifferentlanguages
   multiplication is denoted by a sign ×, a sign ∗, or a dot.

   The term alphabetic transliteration or homomorphism refers to the linguistic
   operation that replaces individual characters by others.

   Definition 2.88 (alphabetic transliteration or homomorphism 33 ) Consider two
   alphabets: source Σ and target Δ. An alphabetic transliteration is a function:
   h: Σ → Δ ∪ {ε}
   Thetransliterationorimageofacharacterc ∈ Σ ish(c),i.e.,anelementofthetarget
   alphabetΔ.Ifh(c) = ε,thecharacterciserased.Wecallnonerasingatransliteration
   such that, for all source character c, the image h(c) is in Δ.

   The image of a source string a 1 a 2 ... a n , with a i ∈ Σ, is the string h(a 1 )h(a 2 )
   ... h(a n ) obtained by concatenating the images of the individual characters. The
   image of the empty string ε is itself, i.e., h(ε) = ε. ?
   Transliteration is compositional: the image of the concatenation of two strings v and
   w is the concatenation of the images of the two strings:
   h(v · w) = h(v) · h(w)
   Example 2.89 (text printer) An obsolete text printer cannot print Greek letters and
   instead it prints the special character ‘?’. Moreover, the text sent to the printer may
   contain control characters, such as start-text and end-text, which are not printable.
   Such text transformation (disregarding the uppercase letters) is described by the
   transliteration:
   h(c) = c if c ∈ { a, b, ..., z, 0, 1, ..., 9 }
   h(c) = c if c is a punctuation mark or a blank space
   h(c) = ? if c ∈ { α, β, ..., ω }
   h(start-text) = h(end-text) = ε
   An example of transliteration is:
   h(start-text const. π has value 3.14 end-text
   source string
   ) – text sent to printer
   = const. ? has value 3.14
   target string
   – printout on paper
   ?
   33 We anticipate that it is a simple case of the translation functions studied in Chap. 5.

<a id="P098"></a>

   An interesting special case of erasing homomorphism is the projection: a function
   that erases some source characters and leaves the others unchanged.

   Transliteration into Words
   The preceding qualification of transliteration (or homomorphism) as alphabetic
   means that the image of a character is still a character (or the empty string), not
   alongerstring.Otherwise,anexampleofnonalphabetictransliterationistheconver-
   sion of an assignment statement a → b + c into the form a := b + c through the
   function:
   h(‘ → ’) = ‘ := ’ and h(c) = c for any other c ∈ Σ
   This case is also called transliteration into words.

   One more example. The vowels with umlaut of the German alphabet have strings
   of two characters as their images in the English alphabet:
   h(ä) = ae h(ö) = oe h(ü) = ue

### 2.7.3.1 Language Substitution

   A further generalization leads us to a language transformation termed substitution,
   already informally introduced when discussing linguistic abstraction on p. 28. Now
   a source character can be replaced by any string of a specified language over another
   alphabet.

   Such language transformation is commonly used in language technology and
   compilation to separate a language definition into two levels, syntactic and
   lexical. The syntactic level is defined by a context-free grammar having as terminal
   alphabet a set of elements called lexical classes. The lexical level defines each
   lexical class as a formal language over the keyboard characters. To illustrate, the
   syntax of a programming language defines declarations, statements, expressions,
   etc., by means of grammar rules that contain as terminals the lexical class symbols,
   typically the following: keyword (begin, if, then, …), operators (such as plus or
   times), identifier, integer_constant, real_constant, comment. Then, an identifier is
   defined in the lexicon as, say, the formal language generated by the regular 
   expression (a ... z ) (a ... z | 0 ... 9) ∗ , etc. (The syntactic-lexical decomposition will
   be further discussed on p. 447.)
   Substitution is also useful in the early phases of language design, in order to leave
   a syntactic construct unspecified in the working grammar, and to expand it later as
   project proceeds. The construct is denoted by a symbol, e.g., ?ARRAY?, temporarily
   consideredasterminal.Astheprojectprogressesbystepwiserefinements,thesymbol
   will be substituted with the definition of the corresponding construct, by means of
   grammar rules or regular expression. See Example 2.92 at the end of this section.
   Formally, given a source alphabet Σ = {a, b, ...}, a substitution h associates
   each character a, b, …with a language h(a) = L a , h(b) = L b , …over the target

<a id="P099"></a>

   alphabet Δ. By applying substitution h to a source string a 1 a 2 ... a n , where every
   a i is a character, we obtain a set of strings:
   h(a 1 a 2 ... a n ) =
   ?
   y 1 y 2 ... y n | ∀1 ≤ i ≤ n y i ∈ L a i
   ?
   We may say that a transliteration into words is a substitution such that each image
   language contains only one string, and if the string has length one or zero, the
   transliteration is alphabetic.


### 2.7.3.2 Closure Under AlphabeticTransformation

   Let L be a source language, context-free or regular, and let h be a substitution such
   that the image of each source character is a language in the same family as the source 
   language. Then the substitution maps the set of source sentences, i.e., L, onto a set
   of image strings, called the image or target language, i.e., L ? = h(L). Is the target
   language still a member of the same family as the source language? The answer is
   affirmative and will be given by means of a construction that is valuable for modifying
   without effort the regular expression or the grammar of the source language.
   Property 2.90 (closure under substitution – I) The family CF is closed under the
   operation of substitution with languages of the same family (therefore also under
   transliteration). ?
   Proof LetG bethegrammaroflanguageLoveralphabetΣ,andlethbeasubstitution
   such that, for every a ∈ Σ, the language L a is context-free. Then language L a is
   defined by a grammar G a with axiom S a . Moreover, we assume that the nonterminal
   setsofgrammars G, G a , G b ,…arepairwisedisjoint(otherwiseitsufficestorename
   the common nonterminals).

   Next, we construct the grammar G ? of language h(L) by transliterating the rules
   of G by means of the following mapping f :
   f (a) = S a for every terminal a ∈ Σ
   f (A) = A for every nonterminal A of G
   The rules of grammar G ? are constructed as follows:
   • apply transliteration f to every rule A → α of G, and so replace each terminal
   character with the axiom of the corresponding target grammar
   • add to G the rules of grammars G a , G b , …
   It is clear that the new grammar G ? generates language h(L(G)). ?
   If the substitution h is a simple transliteration, constructing grammar G ? is more
   direct: in G replace every terminal character c ∈ Σ with its image h(c).

   For regular languages, it holds a result analogous to Property 2.90.


<a id="P100"></a>

   Property 2.91 (closure under substitution – II) The family REG is closed under
   substitution with regular languages (therefore also under transliteration). ?
   Essentially, the same construction in the proof of Property 2.90 can be applied to the
   source language r.e., and it computes the target language r.e.

   Example 2.92 (transliterated grammar) The source language i (‘;’i) ∗ , defined by
   rules:
   S → i‘;’S | i
   schematizes a program that includes a list of instructions i separated by semicolons.
   Now, imagine that the instructions have to be defined as assignments. Then, the
   following transliteration g into words is appropriate:
   g (i) = v ‘←’e
   where v is a variable and e an expression. This produces the grammar:
   ?
   S → A‘;’S | A
   A → v ‘←’e
   As a next refinement, the definition of arithmetic expression can be plugged in by
   means of a substitution h(e) = L E , where the image language L E is well-known.
   Suppose it is defined by a grammar with axiom E. The grammar of the language
   after expression expansion is:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → A‘;’S | A – list of instructions separated by semicolon
   A → v ‘←’E – instruction expanded as an assignment
   E → ... – the usual rules for arithmetic expressions
   As a last refinement, the symbol v, which stands for a variable, should be replaced
   with the regular language of identifier names. ?

## 2.7.4 Grammars with Regular Expressions

   Thelegibilityofaregularexpressionisespeciallygoodforlistsandsimilarstructures,
   and it would be apity to do without them when defining technical languages by means
   of grammars. Since we know that recursive rules are indispensable for parenthesis
   structures, it is attractive to combine r.e. and grammar rules into one notation, called
   extended context-free grammar or EBNF, 34 which takes the best of each formalism:
   34 Extended BNF.


<a id="P101"></a>

   simplyenough,weallowarulerightparttobeanr.e.overterminalsandnonterminals.

   Such extended grammars have a nice graphical representation, i.e., the syntax charts
   to be shown in Chap. 4 on p. 237, which represents the blueprint of the flowchart of
   a syntax analyzer.

   First, since family CF is closed under all the r.e. operations, the language family
   defined by EBNF grammars coincides with family CF.

   In order to appreciate the clarity of the extended rules with respect to the basic
   ones, we examine a few typical constructs of programming languages.

   Example 2.93 (EBNF grammar of declaration lists) Consider a list of variable
   declarations:
   char text1, text2;
   real temp, result;
   int alpha, beta2, gamma;
   to be found in the programming languages, with possible syntactic variations.
   The alphabet is Σ = {c, i, r, v, ‘, ’, ‘ : ’, ‘;’}, where characters c, i, and r
   stand for the keywords char, int, and real, and character v for a variable name. The
   language of declaration lists is defined by r.e. e D , below (center):
   e D =
   ?
   (c | i | r ) v (‘,’v ) ∗ ‘;’
   ? +
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   D → DE | E
   E → AN ‘;’
   A → c | i | r
   N → v ‘,’N | v
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   D → E +
   E → AN ‘;’
   A → c | i | r
   N → v (‘,’v ) ∗
   We apply Algorithm 2.70 on p. 82 and convert this r.e. into the above (left) basic
   grammar with axiom D for language e D . The grammar uses recursive rules for
   nonterminals D and N, is a little longer than the r.e., and is subjectively less perspicuous
   in evidencing the two-level list structure of the declarations, which is immediately
   visible in the r.e. Moreover, the choice of the metasymbols A, E, and N is arbitrary
   and may cause some confusion when several individuals jointly work on a grammar.
   The grammar in the right part is EBNF: in the first and last rule, we respectively see
   the cross and star operators already present in the r.e. ?
   Definition 2.94 (EBNF grammar) An extended context-free (or EBNF) grammar
   G = (V, Σ, P, S ) contains exactly |V | ≥ 1 rules, each one in the form A → α,
   where A is a nonterminal and α is an r.e. over the alphabet V ∪ Σ. For a better
   legibility and concision, also other derived operators are permitted in the r.e., like
   cross, power, and option. ?
   We add to the preceding language (Example 2.93) some typical block structures.

<a id="P102"></a>

   Example 2.95 (Algol-like language) A language block B embraces an optional
   declarative part D followed by a mandatory imperative part I, between the marks b
   (begin) and e (end):
   B → b [D] I e
   The definition of the declarative part is taken from the preceding example:
   D →
   ?
   (c | i | r ) v (‘, ’v ) ∗ ‘;’
   ? +
   The imperative part is a list of phrases F separated by semicolon:
   I → F (‘;’F ) ∗
   Last, a phrase F can be an assignment a or a block B:
   F → a | B
   As an exercise, though worsening legibility, we eliminate as many nonterminals as
   possible by applying nonterminal expansion (p. 66) to D:
   B → b
   ? ?
   (c | i | r ) v (‘,’v ) ∗ ‘;’
   ? + ?
   I e
   A further expansion of I leads to:
   B → b
   ?
   (c | i | r ) v (‘, ’v ) ∗ ‘;’
   ? ∗
   F (‘;’F ) ∗ e
   Last, symbol F can be eliminated, thus obtaining a one-rule grammar G ? :
   B → b
   ?
   (c | i | r ) v (‘, ’v ) ∗ ‘;’
   ? ∗
   (a | B)
   ?
   ‘;’ (a | B)
   ? ∗
   e
   This rule cannot be reduced to an r.e., because nonterminal B cannot be eliminated.
   In fact, it is needed to generate nested blocks, e.g., bb ... ee, by a self-nesting
   derivation, in agreement with Property 2.82 on p. 91. ?
   Example 2.96 (EBNF grammar for JSON) The JSON grammar of Example 2.43,
   shown in Fig. 2.2 on p. 48, is equivalent to the next EBNF form:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → ?OBJECT?
   ?OBJECT? → ‘{’ (ε | ?MEMBERS?) ‘}’
   ?MEMBERS? → ?PAIR? (, ?PAIR?) ∗
   ?PAIR? → ?STRING? : ?VALUE?
   ?VALUE? → ?STRING? | ?OBJECT? | ?ARRAY? | num | bool
   ?STRING? → “ (ε | ?CHARS?) ”
   ?ARRAY? → ‘[’ (ε | ?ELEMENTS?) ‘]’
   ?ELEMENTS? → ?VALUE? (, ?VALUE?) ∗
   ?CHARS? → ?CHAR? +
   ?CHAR? → char

<a id="P103"></a>

   A further variant makes use of the optionality operator ‘[ ]’ and replaces the
   second rule above with ?OBJECT? → ‘{’ [?MEMBERS?] ‘}’ (here square brackets are
   metasymbols); and similarly for rules STRING and ARRAY. ?
   Usually, language reference manuals specify grammars by means of EBNF rules.
   Anyway, beware that an excessive grammar conciseness is often contrary to clarity.
   Moreover, if a grammar is split into smaller rules, it may be easier for the compiler
   writer to associate simple specific semantic actions to each rule, as we will see in
   Chap. 5.


### 2.7.4.1 Derivations andTrees in Extended Grammars

   The right part α of an extended rule A → α of a grammar G is an r.e., which in
   general derives an infinite set of strings: each of them can be viewed as the right
   part of a nonextended rule with unboundedly many alternatives. For instance, rule
   A → (aB) + stands for the infinitely many alternatives:
   A → aB | aBaB | ...

   The notion of derivation can be defined for extended grammars, too, via the notion
   of r.e. derivation introduced on p. 20.

   Briefly, for an EBNF grammar G, consider a rule A → α, where α is an r.e.

   possibly containing the choice operators star, cross, union, and option. Let α ? be a
   string that derives from α, according to the definition of r.e. derivation, and does not
   contain any choice operator. For every pair of (possibly empty) strings δ and η, there
   is a one-step derivation:
   δ Aη ⇒ G δ α ? η
   Then one can define a multi-step derivation that starts from the axiom and produces
   a terminal string, and consequently can define the language generated by an EBNF
   grammar, in the same manner as for basic grammars. An example should suffice to
   clarify.

   Example 2.97 (derivation in an EBNF grammar) The grammar G below:
   G
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   E → [+ | −] T
   ?
   (+ | −) T
   ? ∗
   T → F
   ?
   (× | /) F
   ? ∗
   F → a | ‘ ( ’E ‘)’
   generates arithmetic expressions with four infix binary operators ‘+’, ‘−’, ‘×’, and
   ‘/’, two prefix unary operators ‘+’ and ‘−’ (both optional), and parentheses, i.e.,
   subexpressions.Terminalastandsforanumericargument.Squarebracketsaremeta-
   symbols denoting option. The left derivation:
   E ⇒ T + T − T ⇒ F + T − T ⇒ a + T − T ⇒ a + F − T ⇒ a + a − T
   ⇒ a + a − F × F ⇒ a + a − a × F ⇒ a + a − a × a

<a id="P104"></a>

   corresponds to the syntax tree:
   E
   T
   F
   a
   + T
   F
   a
   − T
   F
   a
   × F
   a
   Notice that the node degree can be unbounded, e.g., a node of type T can have 1, 3,
   5, 7, …siblings. Consequently, the tree breadth increases and its height decreases,
   compared to the tree of an equivalent nonextended grammar. ?
   Ambiguity in Extended Grammars
   It is obvious that an ambiguous BNF grammar remains such also when viewed as
   an EBNF grammar. Moreover, a different form of ambiguity may arise in an EBNF
   grammar, caused by the ambiguity of an r.e. in a rule right part.

   Recallthatanr.e.isambiguous(p.22)ifitderivesastringthroughtwodifferentleft
   derivations. For instance, the r.e. a ∗ b | ab ∗ , numbered a ∗
   1 b 2
   | a 3 b ∗
   4 , is ambiguous
   because string ab can be derived as a 1 b 2 or a 3 b 4 . As a consequence, the extended
   grammar:
   S → a ∗ bS | ab ∗ S | c
   is ambiguous as well.

2.8 More General Grammars and Language Families
===============================================

   We have seen that context-free grammars cover the main constructs occurring in
   technical languages, namely hierarchical lists and nested structures, but fail with
   other syntactic structures even as simple as the replica language or the three equal
   power language on p. 93.

   From the early days of language theory, such shortcomings have motivated much
   research on more powerful formal grammars. It is fair to say that none of the formal
   models has been successful: the more powerful ones are too obscure and difficult
   to use, and those marginally superior to the context-free one do not offer significant
   advantages. In practice, the application of such grammars to compilation has been

<a id="P105"></a>

   episodicandquicklyabandoned. 35 Sincethebasisofallthesubsequentdevelopments
   is the grammar classification due to the linguist Noam Chomsky, it is appropriate
   to briefly present it for reference, before moving on to more applied topics in the
   coming chapters.


## 2.8.1 Chomsky Classification of Grammars

   The historical classification of phrase-structure grammars based on the form of the
   rewriting rules is shown in Table 2.10. Surprisingly enough, very small differences
   in the rule form determine substantial changes in the properties of the corresponding
   language family, both in terms of decidability and of algorithmic complexity.
   A rewriting rule has a left part and a right part, and both parts are strings on the
   terminal and nonterminal alphabets Σ and V. The left part is replaced by the right
   part in the derivation process. Chomsky defined four rule types, characterized as
   follows:
   • a rule of type 0 can replace an arbitrary nonempty string over terminals and
   nonterminals, with another arbitrary string
   • a rule of type 1 is more constrained than a type 0 rule: the right part of a rule must
   be at least as long as the left part
   • a rule of type 2 is context-free: the left part is one nonterminal
   • a rule of type 3 coincides with the unilinear form (p. 85)
   For completeness, Table 2.10 lists the names of the automaton model, i.e., abstract
   string recognition algorithm, that corresponds to each rule type, although the notion
   of automaton will not be introduced until the next chapter. The language families are
   strictly included one inside the other from bottom to top, and this inclusion chain
   justifies the name of hierarchy.

   Partly anticipating later matters, the difference between rule types is mirrored
   by differences in the computational resources needed to recognize the strings.
   Concerning space, i.e., memory complexity for string recognition, type 3 uses a finite
   memory, whereas the others need an unbounded memory.

   Other properties are worth mentioning, without any claim to completeness. All
   four language families are closed under union, concatenation, star, reversal, and
   intersection with a regular language. Yet for other operators their properties differ:
   for instance, families 1 and 3, but not 0 and 2, are closed under complement.
   Concerning the decidability of various properties, the difference between the
   apparently similar types 0 and 1 is striking. For type 0, it is undecidable (more
   precisely semi-decidable) whether a string is in the language generated by a
   gram35 In computational linguistic, some grammar types more powerful than the context-free one have
   gained acceptance. We mention the possibly best known example: the Tree Adjoining Grammar
   (TAG) of Joshi, described, e.g., in [18].


<a id="P106"></a>

   Table 2.10 Chomsky grammar classification with corresponding languages and machines
   Grammar type Rule form Lang. family Recognizer model
   Type 0 β → α with
   α, β ∈ (Σ ∪ V) +
   Recursively
   enumerable
   Turing machine
   Type 1
   context-dependent
   context-sensitive
   β → α with
   α, β ∈ (Σ ∪ V) +
   and |β | ≤ |α|
   Contextual
   context-depend.

   Turing machine with
   space complexity
   limited by input length
   Type 2 context-free
   BNF
   A → α with A ∈ V
   and α ∈ (Σ ∪ V) ∗
   Context-free CF
   algebraic
   Pushdown automaton
   Type 3 unilinear
   either right-linear
   or left-linear
   A → uB (right)
   A → Bu (left) with
   A, u ∈ V, Σ ∗ and
   B ∈ (V ∪ {ε})
   Regular REG rational
   finite-state
   Finite automaton
   mar. For type 1 the same problem is decidable, though its time complexity is not
   polynomial.Last,onlyfortype3theequivalenceproblemoftwogrammarsisdecid-
   able.

   We finish with two examples of grammars and languages of type 1 (context-
   sensitive).

   Example 2.98 (type 1 grammar of the three equal power language) The language
   of three equal powers, proved on p. 93 to be not CF, is:
   L =
   ?
   a n b n c n | n ≥ 1
   ?
   Language L is generated by the context-sensitive (type 1) grammar:
   ?
   1: S → aS BC
   2: S → abC
   ?
   3: C B → BC
   4: bB → bb
   ?
   5: bC → bc
   6: cC → cc
   For grammars of type 0 and 1, a derivation cannot be represented as a tree, because
   a rule left part typically contains more than one symbol. However, the derivation
   can be visualized as a graph, where the application of a rule such as BA → AB is
   displayed as a bundle of arcs (or hyper-edge) that connect the left part nodes to the
   right part ones.

   Coming from our experience with context-free grammars, we would expect to be
   able to generate all the sentences through left derivations. But, if we try to generate
   sentence aabbcc by proceeding from left to right:
   S
   1
   = ⇒ aS BC
   2
   = ⇒ aabC BC
   5
   = ⇒ aabcBC halt !

<a id="P107"></a>

   S
   a S B C
   a b C
   • •
   B C
   • •
   b b
   • •
   b c
   • •
   c c
   C B → B C
   bB → bb
   bC → bc
   cC → cc
   Fig.2.3 Graph representation of a context-sensitive (type 1) derivation (Example 2.98)
   surprisingly, the derivation gets stuck before eliminating all the nonterminals. To
   generate this string, we need a nonleftmost derivation, shown in Fig. 2.3.
   Intuitively,thederivationproducestherequestednumberoflettersa withrule1(a
   type 2 self-nesting rule) and then with rule 2. In the sentential form, letters b, B, and
   C appearwiththecorrectnumberofoccurrences,butmixedup.Toproducethevalid
   string, all the letters C must be shifted to the right, by using rule 3: C B → BC, as
   far as they reach the suffix position.

   First, the application of rule 3 reorders the sentential form, to the aim that letter
   B becomes adjacent to letter b. This enables rule 4: bB → bb. Then, the derivation
   continuesinthesamemanner,byalternatingrules3and4,untilthesententialformis
   left with just C as nonterminal. The occurrences of C are transformed into terminals
   c,bymeansofrule5: bC → bc,followedbyrepeatedapplicationsofrule6: cC →
   cc. ?

<a id="P108"></a>

   We stress the finding that the language generated by a type 1 grammar may not
   coincide with the sentences generated only through left derivations, 36 unlike type 2
   grammars. This is a cause of difficulty in string recognition algorithms.

   Type 1 grammars have the power to generate the replica language, i.e., lists with
   agreement between elements, which is a construct we have singled out as practically
   relevant but exceeding the power of context-free grammars.

   Example 2.99 (type 1 grammar of replica with center) The language L:
   L =
   ?
   ycy| y ∈ {a, b} +
   ?
   contains sentences such as aabcaab, where a prefix and a suffix are divided by a
   central separator c and must be equal.

   To simplify the grammar, we assume that sentences are terminated on the right
   with an end-of-text or terminator, i.e., ?. Here is a grammar for L:
   S → X ? X A → X A ? A ? A → AA ? A ? ? → a B ? a → ba
   X → aX A X B → X B ? A ? B → BA ? B ? ? → b B ? b → bb
   X → bX B B ? A → AB ? A ? a → aa X a → ca
   B ? B → BB ? A ? b → ab X b → cb
   To generate a sentence, the grammar follows this strategy. First it generates a 
   palindrome, say aabX BAA, where symbol X marks the center and the right half is
   uppercase. Then the right half, modified as B ? AA, is reflected and converted into
   A ? A ? B ? in a few steps. Last, the primed uppercase symbols are rewritten as aab and
   the center symbol X is converted into c.

   We illustrate the derivation of sentence aabcaab. For legibility, at each step we
   underline the left part of the rule being applied:
   S ⇒ X ? ⇒ aX A ? ⇒ aaX AA ? ⇒ aabX BAA ?
   ⇒ aabX B ? AA ? ⇒ aabX AB ? A ? ⇒ aabX A ? B ? A ?
   ⇒ aabX A ? AB ? ? ⇒ aabX AA ? B ? ? ⇒ aabX A ? A ? B ? ?
   ⇒ aabX A ? A ? b ⇒ aabX A ? ab ⇒ aabX aab ⇒ aabcaab
   For instance, the rule applied in the fifth step is X B → X B ? . ?
   Notice that if the same strategy used for generation were applied in reverse order, it
   would check whether a string is a valid sentence. Starting from the given string, the
   algorithm should store on a memory tape the strings obtained after each reduction,
   i.e., the reverse of derivation. Such a procedure is essentially a Turing machine
   computation that never goes out of the tape portion containing the original string,
   but may overprint its symbols.

   36 For any type 1 grammar, the language generated using just left derivations is, quite surprisingly,
   context-free.


<a id="P109"></a>


## 2.8.2 Further Developments and Final Remarks

   The complication of type 1 grammars already for such simple examples should
   convince the reader of the difficulty of designing and applying context-sensitive
   rules to more realistic cases. It is a fact that the interaction between type 1 grammar
   rules is hard to understand and control.

   Truly, type 1 and 0 grammars can be viewed as a particular notation for writing
   algorithms.Allsortsofproblemscanbeprogrammedinthisway,evenmathematical
   ones, through the very simple string rewriting mechanism. 37 Not surprisingly, using
   such an elementary mechanism as the only data and control structure makes the
   algorithm description very tangled.

   Undoubtedly, the development of language theory toward models with a higher
   computationalcapacityhasamathematicalandspeculativeinterest,but,untilnow,it
   has been almost irrelevant for language engineering and compilation. For historical
   honesty, we mention that context-sensitive grammars have been occasionally
   considered by language and compiler designers. The programming language Algol 68
   was defined with a special class of type 1 grammars termed ‘2-level grammar,’ also
   known as VW-grammar. 38
   Further Attempts: Conjunctive and Boolean Grammars
   This notwithstanding, research efforts toward new grammar models have continued
   and have proposed quite different approaches, the practicality of which is open to
   debate. We finish with a recent interesting example: the conjunctive grammars and
   the more general Boolean grammars of Okhotin [21]. To understand the idea behind
   them, it helps to view each collection of alternative rules A → β 1 | β 2 | ··· | β m
   of an ordinary context-free grammar G as the logical or (disjunction) of m rules.
   In other words, nonterminal A can derive any string of its language L A (G) starting
   with the alternative β 1 or β 2 , etc. What is different in a conjunctive grammar is the
   use of the logical and (conjunction, denoted by &) in addition to or. For a single
   nonterminal A, each collection of rules has the form:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   A →
   ?
   α 1,1 &α 1,2 & ... &α 1,n 1
   ?
   |
   ?
   α 2,1 &α 2,2 & ... &α 2,n 2
   ?
   |
   ... |
   ?
   α m,1 &α m,2 & ... &α 2,n m
   ?
   where each term α is a string of terminal and nonterminal symbols, as usual. Each
   parenthesizedsubruleistheconjunctionofoneormorecases;theparenthesesmaybe
   omitted since and takes precedence over or. If all the values n 1 , n 2 , …, n m are equal
   37 An extreme case is a type 1 grammar presented in [12] to generate the prime numbers encoded
   in unary base, i.e., the language {a n | n is a prime number}.

   38 From Van Wijngarten [19]; see also Cleaveland and Uzgalis [20].


<a id="P110"></a>

   to 1, the rule is the ordinary context-free rule A → α 1,1 | ··· | α m,1 . We explain
   how a conjunctive grammar defines a language by means of a familiar example.
   Example 2.100 (conjunctive grammar of the three equal power language) This
   language L, already defined by a type 1 grammar in the Example 2.98 above, is the
   intersection of two context-free languages:
   L =
   ?
   a + b n c n | n ≥ 1
   ?
   ∩
   ?
   a n b n c + | n ≥ 1
   ?
   and is generated by the following conjunctive grammar:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → AB&DC
   A → aA | a
   B → bBc | bc
   C → cC | c
   D → aDb | ab
   In this grammar, there is only one conjunctive rule, which says that S derives a string
   x if, and only if, the following derivations hold:
   ?
   AB
   ∗
   = ⇒ x
   ?
   and
   ?
   DC
   ∗
   = ⇒ x
   ?
   TherulesforthenonterminalsAandBarecontext-free,andstringABderivesstrings
   a + b n c n .Similarly,stringC Dderivesstringsa n b n c + ;therefore,theaxiomS derives
   all and only the valid strings a n b n c n . ?
   Example 2.100 gives the positive impression that conjunctive grammars are an
   elegant and simple extension of context-free grammars, which permits to define in a
   natural way some context-sensitive languages. Unfortunately, such simplicity is lost
   for other quite basic context-sensitive languages such as the replica with center of
   the Example 2.99 above, the conjunctive grammar of which [21] is (subjectively)
   complicated. Moreover, for the language replica (without center), no conjunctive
   grammar is known, and one has to resort to the more powerful model of Boolean
   grammars. As the name says, such grammars can use in their rules not just
   conjunction and disjunction, but also complementation, written ‘¬’. A rule of the form
   A → ¬B says that a string is in the language L A (G) if it is in the complement of the
   language defined by nonterminal B.

   Example 2.101 (Boolean grammar of replica) We recall the definition:
   replica = L =
   ?
   yy| y ∈ {a, b} +
   ?

<a id="P111"></a>

   This language is defined by the following Boolean grammar G [21]:
   G
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → ¬AB&¬BA&C
   A → X AX | a
   B → X BX | b
   C → X X C | aa | ab | ba | bb
   X → a | b
   In grammar G, there is only one Boolean rule, which says that a string x is defined
   by nonterminal S if:
   • x is not defined by the concatenation of the languages of A and B, and
   • x is not defined by the concatenation of the languages of B and A, and
   • x is defined by the language of C
   We detail the languages involved:
   L A (G) =
   ?
   uav | u,v ∈ (a | b) ∗ with |u| = |v |
   ?
   L B (G) =
   ?
   ubv | u,v ∈ (a | b) ∗ with |u| = |v |
   ?
   hence:
   L AB (G) =
   ?
   uav xby| u,v,x,y ∈ (a | b) ∗ with |u| = |v |,|x| = |y|
   ?
   L BA (G) =
   ?
   ubv xay| u,v,x,y ∈ (a | b) ∗ with |u| = |v |,|x| = |y|
   ?
   Notice that language L AB (G) contains all the strings of even length such that a letter
   a and a letter b are mismatched. Such strings are of the form:
   uav xby with |u| = |x| and |v | = |y|
   The form of language L BA (G) is similar, with the only difference that, for the
   mismatched letters, b is on the left of a. Clearly, a string that presents such a mismatch
   is not a replica.

   Then, the first rule of grammar G defines any string that, being defined by C, is
   of even length and does not contain any mismatching pair, in formula:
   L S (G) = L AB (G) ∩ L BA (G) ∩ (aa | ab | ba | bb) + ?
   Grammar G of Example 2.101 correctly defines language replica and applies the
   principle that sometimes definitions ex negativo may be simpler than those ex
   positivo. But the reasoning behind it is perhaps too mathematically ingenious for a
   practical use in the technical definitions of languages.

   Another difficulty is that the introduction of negation makes such grammars much
   more difficult to control, since in general the use of negation can lead to logical

<a id="P112"></a>

   contradiction, as when we have a grammar rule S → ¬S. For that reason, a rigorous
   formulation of a language defined by a Boolean grammar is far from trivial. In
   particular,thenotionofderivation,whichisfundamentalinallthekindsofChomsky
   grammars, is lost and has to be replaced by the formalism of recursive language
   equations.

   To sum up our evaluation of such new models, although conjunctive grammars
   only moderately increase the power of context-free grammars, they are quite easy to
   understand. Moreover, the algorithms for parsing with such grammars are similar to
   the classic algorithms for context-free grammars and may pay an acceptable extra
   cost in terms of efficiency. On the other hand, general Boolean grammars are too
   difficult, at least at their current development stage.

   In conclusion, we have to admit that the state of the art in formal language
   theory does not entirely satisfy the need for a powerful and practical formal grammar
   model, capable of accurately defining the entire range of constructs found in
   technical languages. Context-free grammars are the best available compromise between
   expressivityandsimplicity.Thecompilerdesignerwillsupplementtheirweaknesses
   byothermethodsandtools,termedsemantic,whichcomefromgeneral-purposepro-
   gramming methodologies. These methods and tools will be introduced in Chap. 5.
   References
   1. Searls DB (2002) The language of genes. Nature 420:211–217
   2. RozenbergG(ed)(1997)Handbookofgraphgrammarsandcomputingbygraphtransformation:
   volume i. Foundations. World Scientific Publishing Co., Inc., River Edge
   3. Comon H, Dauchet M, Gilleron R, Löding C, Jacquemard F, Lugiez D, Tison S, Tommasi M
   (2007) Tree automata techniques and applications. http://www.grappa.univ-lille3.fr/tata
   4. Gecseg F, Steinby M (1997) Tree languages. In: Rozenberg G, Salomaa A (eds) Handbook of
   formal languages, vol. 3: beyond words. Springer, New York, pp 1–68
   5. CrespiReghizziS,GiammarresiD,LonatiV(2019,toappear)Two-dimensionalmodels.In:Pin
   J-É (ed) Handbook of automata theory, vol. I: theoretical foundations. European Mathematical
   Society, Zürich, pp 281–315
   6. Giammarresi D, Restivo A (1997) Two-dimensional languages. In: Rozenberg G, Salomaa A
   (eds) Handbook of formal languages, vol. 3: beyond words. Springer, New York, pp 215–267
   7. Perrin D, Pin JE (2004) Infinite words: automata, semigroups, logic and games. Academic
   Press, New York
   8. Thomas W (1997) Languages, automata, and logic. In: Rozenberg G, Salomaa A (eds)
   Handbook of formal languages, vol. 3: beyond words. Springer, New York, pp 389–455
   9. Hopcroft J, Ullman J (1969) Formal languages and their relation to automata. Addison-Wesley,
   Reading
   10. McNaughton R (1982) Elementary computability, formal languages and automata. 
   PrenticeHall, Englewood Cliffs
   11. Naur P (1963) Revised report on the algorithmic language ALGOL 60. Commun ACM 6:1–33
   12. Salomaa A (1973) Formal languages. Academic Press, New York
   13. Harrison M (1978) Introduction to formal language theory. Addison Wesley, Reading
   References 113
   14. Autebert J, Berstel J, Boasson L (1997) Context-free languages and pushdown automata. In:
   RozenbergG,SalomaaA(eds)Handbookofformallanguages,vol.1:word,language,grammar.
   Springer, New York, pp 111–174
   15. Crespi Reghizzi S, Della Vigna P, Ghezzi C (1976) Linguaggi formali e compilatori. ISEDI,
   Milano
   16. BlattnerM,GinsburgS(1982)Position-restrictedgrammarformsandgrammars.TheorComput
   Sci 17:1–27
   17. Anselmo M, Giammarresi D, Varricchio S (2003) Finite automata and non-self-embedding
   grammars. In: CIAA 2002, Springer LNCS, pp 47–56
   18. Jurafsky D, Martin JH (2009) Speech and language processing: an introduction to natural
   languageprocessing,speechrecognition,andcomputationallinguistics.Prentice-Hall,Englewood
   Cliffs
   19. Van Wijngarten A (1969) Report on the algorithmic language ALGOL 68. Numer Math 22:79–
   218
   20. Cleaveland J, Uzgalis R (1977) Grammars for programming languages. North-Holland,
   Amsterdam
   21. Okhotin A (2013) Conjunctive and boolean grammars: the true general case of the context-free
   grammars. Comput Sci Rev 9:27–59

<a id="P115"></a>

Chapter 3 Finite Automata as Regular Language Recognizers
=========================================================

3.1 Introduction
================

   Regular expressions and grammars are widely used in the specifications of technical
   languages, but the actual design and implementation of a compiler need a way to
   describe the algorithms, termed recognizers or acceptors, that examine a string and
   decideifitisavalidsentenceofthelanguage.Inthischapterwestudytherecognizers
   for regular languages, in Chap.4 those for context-free languages, and in Chap.5 we
   extend the recognizer to perform also translation.

   The need of recognizing whether a text is valid for a given language is quite
   common, especially as a first step for text processing or translation. A compiler
   analyzes a source program to check its correctness; a document processor makes a spell
   check on the words, and then it verifies the syntax; and a graphic user interface must
   check that data are correctly entered. Such a control is carried out by a recognition
   procedure, which can be conveniently specified by using minimalist models, termed
   abstract machines or automata. The advantages are that the automata do not depend
   on the programming languages and techniques, i.e., on the implementation, and that
   in this way their properties (such as their time and memory complexity) are more
   clearly related with the source language family.

   In this chapter we briefly introduce more general automata, and then we focus
   on those that have a finite memory, because they match the regular language family
   and have countless applications in computer science and beyond. First, we consider
   the deterministic machines and we describe some basic methods for cleaning and
   minimizing.Then,wemotivateandintroducenondeterministicmodels,andweshow
   that they correspond to the unilinear grammars of the previous chapter. Conversion
   back to deterministic models follows.

   A central part deals with the transformations back and forth from regular 
   expressionstoautomata.Themethodspresentedare:BMC forthedirectionfromautomaton
   to regular expression, and Thompson and Berry–Sethi for the other one. The latter
   © Springer Nature Switzerland AG 2019
   S. Crespi Reghizzi et al., Formal Languages and Compilation,
   Texts in Computer Science, https://doi.org/10.1007/978-3-030-04879-2_3

<a id="P116"></a>

   method relies on the theory of local regular languages, which have a simpler 
   recognition procedure that makes use of a sliding window.

   The ambiguous regular expressions are used for string matching, and they need
   advanced recognition methods capable of returning the tree structure of a string, i.e.,
   to perform parsing; thus, the BSP parser is presented.

   Then, we return to the operations of complement and intersection on regular
   languages,fromthestandpointoftheirrecognizers,andweintroducethecomposition
   of automata through cartesian product. The chapter ends with a synopsis of the
   interrelation between finite automata, regular expressions and grammars.

   In the compilation process, finite automata have several uses to be described here
   and in Chap.5: in lexical analysis for extracting the shortest meaningful strings from
   atextandformakingsimpletranslations,andinstaticflowanalysisandoptimization
   for modeling and analyzing program properties.

3.2 Recognition Algorithms and Automata
=======================================

   Tocheckifastringisvalidforaspecifiedlanguage,weneedarecognitionalgorithm,
   a type of algorithm that produces a yes/no answer, commonly referred to in the
   computational complexity studies as a decision algorithm. For instance, a famous
   problem is to decide whether two given graphs are isomorphic: the problem domain
   (a pair of graphs) differs from the case of language recognition, but the answer is
   again yes/no.

   For the string membership problem, the input domain is a set of strings over an
   alphabetΣ.Theapplicationofarecognitionalgorithmαtoagivenstringx isdenoted
   as α (x). We say that a string x is recognized or accepted if it holds α (x) = yes,
   otherwise it is rejected. The language recognized is denoted L (α) and is the set of
   accepted strings:
   L (α) =
   ?
   x ∈ Σ ∗ | α (x) = yes
   ?
   A recognition algorithm is usually assumed to terminate for every input, so that the
   membership problem is decidable. However, it may happen that, for some string x,
   the algorithm does not terminate; i.e., the value of α (x) is undefined; hence string x
   is not a valid sentence of language L (α). In such a case we say that the membership
   problem for L is semidecidable, or also that language L is recursively enumerable.
   Inprinciple,ifthemembershipproblemofanartificiallanguageissemidecidable,
   we cannot exclude that, for some input strings, the compiler will fall into an endless
   loop. In practice we do not have to worry about such decidability issues, central as
   they are for computation theory, 1 because in language processing the only language
   families of concern are decidable, and efficiently so.

   1 Many books cover the subject, e.g., Bovet and Crescenzi [1], Floyd and Beigel [2], Hopcroft and
   Ullman [3], Kozen [4], McNaughton [5] and Rich [6].


<a id="P117"></a>

   Incidentally, even within the family of context-free languages, some decision
   problems, different from string membership, are undecidable: in Chap.2 we have
   mentioned the problem of deciding if two grammars are weakly equivalent, and the
   one of checking if a grammar is ambiguous.

   In fact, recognizing a string is just the first step of the compilation process. In
   Chap.5wewillstudythetranslationfromalanguagetoanother:clearlythecodomain
   (orimage)ofatranslationalgorithmismuchmorecomplexthanapureyes/no,since
   it is itself a set of strings, the target (or destination) language.

   All the algorithms, including those for string recognition, may be ranked in
   complexityclasses,measuredbytheamountofcomputationalresources(timeormemory,
   i.e., space) required to solve the problem. In the field of compilation, it is common to
   consider time rather than space complexity. With rare exceptions, all the problems
   of interest for compilation have a low time complexity: linear or at worst polynomial
   with respect to the size of the problem input. Time complexity is closely related
   with electric power consumption, which is a significant parameter for portable
   programmed devices.

   A tenet of computation theory is that the complexity of an algorithm should be
   measured as the number of steps, rather than as the actual execution time spent by
   a program implementing the algorithm. The reason is that the complexity should
   be a property of the algorithm and should not depend on the actual implementation
   and processing speed of a particular computer. Even so, several choices are open for
   modeling a computational step: they may range from a Turing machine move, to a
   machineinstructionortoahigh-levellanguagestatement.Hereweconsiderastepto
   be an elementary operation of an abstract machine or automaton, as customary in all
   the theoretical and applied studies on formal languages. This approach has several
   advantages: it gives evidence to the relation between the algorithmic complexity and
   the family of languages under consideration; it allows to reuse optimized abstract
   algorithms with a low cost of adaptation to the language, and it avoids a premature
   commitment and many tedious details of implementation. Moreover, sufficient hints
   will be given for a programmer to easily transform the automaton into a program,
   by hand or by using widespread compiler generation tools.

   3.2.1 A General Automaton
   An automaton or abstract machine is an ideal computer that features a very small set
   of simple instructions. Starting from the Turing machine of the 1930s, the research
   on abstract machines has spawned many models, but only few of them are important
   for the scope of the book. 2 In its most general form, a recognizer is schematized in
   Fig.3.1.Itcomprisesthreeparts:inputtape,controlunitand(auxiliary)memory.The
   2 Otherbookshaveabroaderanddeepercoverageofautomatatheory,suchasSalomaa[7],Hopcroft
   and Ullman [3,8], Harrison [9], Shallit [10] and the handbook [11]; for finite automata, a specific
   reference is Sakarovitch [12].


<a id="P118"></a>

   a 1 a 2 ...

   a i
   ... a n
   control unit
   M 1 M 2 ... M j ... M m
   input tape
   (auxiliary) memory tape
   input read head
   (read only)
   memory head
   (read and write)
   Fig.3.1 General model (Turing machine) of a recognizer automaton
   controlunithasalimitedstore,representedbyafinitesetofstates;ontheotherhand,
   the auxiliary memory has an unbounded capacity. The read-only input tape contains
   the given input or source string, one character per case, and the string length n is 0
   if the string is empty. The tape cases on the left and right of the input string contain
   two delimiters: the start-of-text mark ?, and the end-of-text mark or terminator ?.
   A peculiarity of such automaton is that the auxiliary memory is also a tape,
   instead of the random access memory or countermemory used in other computational
   models. The memory tape can be read and written, and contains a string of m ≥ 0
   symbols from another alphabet (called memory alphabet).

   The automaton can perform the following instantaneous actions: reading the
   current character a i from input, shifting the read head, reading the current symbol M j
   from memory and replacing it with another symbol, moving the memory head and
   changing the current state of the control unit to the next one.

   The automaton processes the source by performing a series of moves; the choice
   ofamovedependsonthecurrenttwosymbols(inputandmemory)andonthecurrent
   state. A move may have the following effects, some of which may be missing:
   • shifting the input head to the left or right by one position
   • overwritingthecurrentmemorysymbolwithanotherone,andshiftingthememory
   head to the left or right by one position
   • changing the state of the control unit.

   Amachineisunidirectional iftheinputheadonlymovesfromlefttoright:thisisthe
   modelto beconsideredin thebook,because itwellrepresents howthe text isusually
   processed. For the unidirectional machines the start-of-text mark is superfluous.

<a id="P119"></a>

   At any time, the future behavior of the machine depends on a 3-tuple, called an
   (instantaneous) configuration, made by the following pieces of information:
   • the suffix of the input string still unread, which lies on the right of the input head
   • the contents of the memory tape and the position of the memory head
   • the state of the control unit.

   The initial configuration has the input head positioned on character a 1 , i.e., just to
   the right of the start-of-text mark, the control unit in an initial state and the memory
   containing a specific starting symbol (or sometimes a fixed string).

   Thenthemachineperformsacomputation,i.e.,asequenceofmoves,thatleadsto
   newconfigurations.Ifforaconfigurationatmostonemovecanbeapplied,thechange
   of configuration is deterministic. A nondeterministic (or indeterministic) automaton
   is essentially a manner of representing an algorithm that in some situations may
   explore alternative paths.

   A configuration is final if the control unit is in a state specified as final and the
   input head is on the terminator. Sometimes, instead of being, or in addition to being
   in a final state, a final configuration is qualified by the fact that the memory tape
   contains a specified symbol or string: a frequent choice is for the memory to be
   empty.

   The source string x is accepted if the automaton, starting in the initial 
   configuration with x ? as input, performs a computation that leads to a final configuration
   (a nondeterministic automaton may reach a final configuration by different 
   computations). The language accepted or recognized by the machine is the set of accepted
   strings.

   Notice that a computation terminates either when the machine has entered a final
   configurationorwheninthecurrentconfigurationnomovecanbeappliedanylonger.

   In the latter case the source string is not accepted by that computation, but it may be
   accepted by another computation if the machine is nondeterministic.

   Twoautomatathatacceptthesamelanguagearecalledequivalent.Ofcourse,two
   machines, though equivalent, may belong to different models or may have different
   computational complexities.

   Turing Machine
   The preceding description (see Fig.3.1) substantially reproduces the automaton
   model introduced by A. Turing in 1936 and widely taken as the best available
   formalization of any sequential algorithm. The family of languages accepted is termed
   recursively enumerable. In addition, a language is termed decidable (or recursive)
   if there exists a Turing machine that accepts it and halts for every input string. The
   family of decidable languages is smaller than the one of recursively enumerable
   languages.


<a id="P120"></a>

   Such a machine is the recognizer of two of the language families of the Chomsky
   classification (p. 105). The languages generated by type 0 grammars are exactly the
   recursivelyenumerableones.Thelanguagesofcontext-sensitiveortype1grammars,
   on the other hand, correspond to the languages accepted by a Turing machine that is
   constrained in its use of the memory: the length of the memory tape is bounded by
   the length of the input string.

   Turing machines are not relevant for practical applications, but they are a
   significant comparison term for the practical machine models used to recognize and
   transform technical languages. Such models can be viewed as Turing machines with
   drastic memory limitations. When no auxiliary memory is available, we have the
   finite-state or simply finite automaton, the most fundamental type of computing
   machine, which corresponds to the regular languages. If the memory is organized
   as an LIFO (last in first out) store, the machine is termed a pushdown automaton
   (studied in Chap.4) and recognizes the context-free languages.

   The memory limitations have a profound effect on the properties and in particular
   ontheperformanceofautomata.Consideringtheworst-casetimecomplexity,which
   is a primary parameter for program efficiency, a finite automaton is able to recognize
   a string in linear time, or more exactly in real time, that is, with a number of steps
   equal to the input length. In contrast, a space-bounded Turing machine may take
   a non-polynomial time to recognize a context-sensitive language, another reason
   making it unpractical to use. The context-free language recognizers lie in between:
   the number of steps is bounded by a polynomial of small degree of the input string
   length.

3.3 Introduction to Finite Automata
===================================

   Finite automata are surely the simplest and most fundamental abstract computational
   device. Their mathematical theory is very stable and deep, and they are able to
   support innumerable applications in diverse areas, from digital circuit design to system
   model checking and speech recognition, to mention just a few. Our presentation will
   focus on some aspects important for language and compiler design, but, to make
   the presentation self-contained, we briefly introduce the essential definitions and
   theoretical results.

   Conformingtothegeneralscheme,afiniteautomatoncompriseswhatfollows:an
   input tape with the source string x ∈ Σ ∗ ; a control unit; and a reading head, which
   is initially placed on the first character of x and scans the string as far as the end,
   unless an error occurs before. Upon reading a character, the automaton updates the
   control unit state and advances its reading head. Upon reading the last character, the
   automaton accepts string x if, and only if, its state is accepting (final).
   A well-known representation of an automaton is by a state-transition diagram or
   graph. This is a directed graph with the states as nodes. Each arc is labeled with a
   terminalandrepresentsthestatechangeortransitioncausedbyreadingtheterminal.

<a id="P121"></a>

   state-transition diagram (graph) state-transition table
   q 2
   q 0 q 3 q 4
   q 1
   → →
   Δ
   0
   •
   •
   0 ∪ Δ
   0 ∪ Δ
   0 ∪ Δ
   current current character
   state 0 1 ... 9 •
   → q 0 q 2 q 1 ... q 1 −
   q 1 q 1 q 1 ... q 1 q 3
   q 2 − − ... − q 3
   q 3 q 4 q 4 ... q 4 −
   q 4 → q 4 q 4 ... q 4 −
   Fig. 3.2 State-transition diagram or graph (left) and state-transition table (right) for numerical
   (fixed-point decimal) constants (Example3.1)
   Example 3.1 (numerical constants) The set L of fixed-point decimal constants has
   alphabet Σ = Δ ∪ { 0, • }, where Δ = { 1, 2, 3, 4, 5, 6, 7, 8, 9 } is the set of
   nonzero digits. The regular expression (r. e.) of L is:
   L =
   ?
   0 ∪ Δ (0 ∪ Δ) ∗
   ?
   • (0 ∪ Δ) +
   The recognizer of language L is specified by the state-transition diagram or by the
   equivalentstate-transitiontableinFig.3.2.Forconvenience,iftwoormorearcswith
   distinct labels span the same two nodes, only one arc is drawn with multiple labels.
   Thus,thearcq 0
   Δ
   − → q 1 representsabundleofninearcswithlabels1,2,…,9andcan
   be equivalently written as q 0
   1,2,...9
   −−−−→ q 1 . The initial and final states q 0 and q 4 are
   marked by an incoming and outgoing arrow “→”, respectively. The transition table
   is the incidence matrix of the graph: for each pair (current state,current character)
   the corresponding matrix entry contains the next state.

   Given the source string 0 • 2•, the automaton transits through the states q 0 ,
   q 2 , q 3 and q 4 . In the last state, since no transition allows reading the character •,
   the computation stops before consuming the entire input, and the source string is
   rejected. On the other hand, the input string 0 • 21 would be accepted. If we prefer
   to modify the language so that constants such as 305•, which just ends with a
   decimal point, are legal, then the state q 3 must be marked as final, too. ?
   We have seen that an automaton may have two or more final states, but only one
   initial state, otherwise it would not be deterministic.


<a id="P122"></a>

3.4 Deterministic Finite Automata
=================================

   The previous ideas are formalized in the next definition.

   Definition 3.2 (deterministic finite automaton) A deterministic finite automaton
   (DFA) M comprises five items:
   Q the state set (finite and not empty)
   Σ the input or terminal alphabet
   δ: Q × Σ → Q the transition function
   q 0 ∈ Q the initial state
   F ⊆ Q the set of final states (may be empty). ?
   The transition function δ specifies the moves: the meaning of δ (q, a) = r is that
   the machine M, in the current state q, reads an input symbol a and moves to the
   next state r. If the value δ (q, a) is undefined, automaton M stops, and we assume it
   enters the error state (more on that later). As an alternative notation, we indicate the
   movefromstateq tostater readingcharactera,asq
   a
   − → r,i.e.,bythecorresponding
   arc of the state-transition graph.

   Automaton M processes a non-empty string x by a series of moves. Take x =
   a b: on reading the first character, the first step δ (q 0 , a) = q 1 leads to state q 1 ,
   and then to state q 2 by the second step δ (q 1 , b) = q 2 . In short, instead of writing
   δ
   ? δ (q
   0 , a), b
   ?
   = q 2 ,wecombinethetwostepsintooneδ (q 0 , a b) = q 2 ,tosaythat
   on reading string a b the machine M moves to state q 2 . Notice that now the second
   argument of function δ is a string. A special case is the empty string, for which we
   assume no change of state:
   ∀ q ∈ Q δ (q, ε) = q
   Following the stipulations above, the transition function δ has the domain Q × Σ ∗ ,
   and it is recursively defined as follows:
   δ (q, y a) = δ
   ?
   δ (q, y), a
   ?
   wherea ∈ Σ and y ∈ Σ ∗
   We observe a correspondence between the values of δ and the paths in the 
   statetransition graph: δ (q, y) = q ? if, and only if, there exists a path from nodeq to node
   q ? ,suchthattheconcatenatedlabelsofthepatharcsmakestring y.Wesaythatstring
   y is the label of the path, and the path itself represents a computation of automaton
   M.

   A string x is recognized or accepted by automaton M if it is the label of a path
   from the initial state to a final state, i.e., δ (q 0 , x) ∈ F. Notice that the empty string
   ε is recognized if, and only if, the initial state is also final, i.e.,q 0 ∈ F. The language
   L (M) recognized or accepted by automaton M is:
   L (M) =
   ?
   x ∈ Σ ∗ | δ (q 0 , x) ∈ F
   ?

<a id="P123"></a>

   The languages accepted by such automata are termed finite-state recognizable. Two
   (finite) automata are equivalent if they accept the same language.

   Example 3.3 (Example 3.1 continued) The automaton M of p. 121 is defined by:
   Q = { q 0 , q 1 , q 2 , q 3 , q 4 } state set
   Σ = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, • } (input) alphabet
   q 0 = q 0 initial state
   F = { q 4 } final state set
   Examples of transitions:
   δ (q 0 , 3 • 1) = δ
   ?
   δ (q 0 , 3•), 1
   ?
   = δ
   ?
   δ
   ?
   δ (q 0 , 3), •
   ? , 1 ?
   = δ
   ?
   δ (q 1 , •), 1
   ?
   = δ (q 3 , 1) = q 4
   Since it holds q 4 ∈ F, string 3 • 1 is accepted. On the contrary, since state
   δ (q 0 , 3•) = q 3 is not final, string 3• is rejected, as well as string 02 because
   function δ (q 0 , 02) = δ (δ (q 0 , 0), 2) = δ (q 2 , 2) is undefined. ?
   The automaton executes one step per input character, and the total number of steps
   executed equals the length of the input string. Such machine is very efficient and
   recognizes the input in real time by a single left-to-right scan.


## 3.4.1 Error State andTotal Automaton

   If a move is not defined in a state q when reading an input character a, we say that
   the automaton falls into the error state q err . The automaton can never exit from the
   error state, thus justifying its other name of sink or trap state. Obviously the error
   state is not final.

   The state-transition function δ can be made total by adding the error state and the
   transitions from/to it:
   ∀ q ∈ Q ∀ a ∈ Σ if δ (q, a) is undefined, then set δ (q, a) = q err
   ∀ a ∈ Σ set δ (q err , a) = q err
   The recognizer of numerical constants of Fig.3.2, completed with the error state,
   is shown in Fig.3.3. Since a computation entering the error state is trapped there
   and cannot reach a final state, the total automaton accepts the same language as

<a id="P124"></a>

   q 2 q err
   q 0 q 3 q 4
   q 1
   M →
   →
   Δ
   0
   •
   •
   0 ∪ Δ
   0 ∪ Δ
   0 ∪ Δ
   •
   0 ∪ Δ
   •
   •
   0 ∪ Δ ∪ •
   Fig.3.3 Recognizer of numerical constants completed with the sink (error or trap) state
   the original one. It is customary not to represent the error state, nor specifying the
   transitions that touch it.


## 3.4.2 Clean Automaton

   An automaton may contain useless parts that do not contribute to any accepting
   computation, which are best eliminated. Notice that the following concepts hold
   for nondeterministic finite automata as well, and in general for any kind of abstract
   machine.

   A state q is reachable from a state p if a computation exists going from p to q. A
   state is accessible if it can be reached from the initial state, and it is post-accessible
   if a final state can be reached from it. A state is called useful if it is accessible and
   post-accessible; otherwise it is useless. In other words, a useful state lays on some
   path from the initial state to a final one. An automaton is clean or reduced if every
   state is useful.

   Property 3.4 (clean automaton existence) For every finite automaton there exists
   an equivalent clean automaton. ?
   To construct the clean machine, we identify the useless states and we delete them
   together with all the arcs that touch them.

   Example 3.5 (eliminationofuselessstates) Figure3.4showsamachinewithuseless
   states and the corresponding clean machine. Notice that the error state is never
   postaccessible, hence always useless. ?

<a id="P125"></a>

   unclean automaton
   not accessible
   state (useless)
   not post-accessible
   state (useless)
   → →
   b
   a
   a
   c
   c
   a
   b
   clean automaton
   → →
   a
   a
   b
   Fig.3.4 Automaton with useless states (top) and equivalent clean automaton (bottom)

## 3.4.3 Minimal Automaton

   We focus on the automata that recognize the same language, using different state
   sets and transition functions. A central property says that out of such equivalent
   machines, there exist one, and only one, that is the smallest in the following sense.
   Property 3.6 (minimal automaton uniqueness) For every finite-state language, the
   (deterministic)finiterecognizerminimalwithrespecttothenumberofstatesisunique
   (apart from renaming the states). ?
   Conceptually,thisstatementisfundamental,asitpermitstorepresenteverycollection
   of equivalent machines by a standard one, which moreover is minimal; in practice,
   for compiler applications of finite automata, the requirement of minimality is not
   common.

   We describe the standard minimization algorithm. First, we need to introduce a
   binary relation between equivalent states, and then we show how to compute it. 3
   We assume that the given automaton is clean. Some of its states can be redundant,
   in the sense that they can be merged at no consequence for the strings accepted or
   3 Other subtler and more efficient algorithms have been invented. We refer the reader to the survey
   in [13].


<a id="P126"></a>

   rejected. Any two such states are termed undistinguishable from each other, and the
   corresponding binary relation is termed undistinguishability (or of Nerode).
   Definition 3.7 (stateundistinguishability)Twostates p andq areundistinguishable
   if, and only if, for every input string x ∈ Σ ∗ , either both the next states δ (p, x) and
   δ (q, x) are final, or neither one is. The complementary relation is termed 
   distinguishability. ?
   Spelling out the condition, two states p and q are undistinguishable if, by starting
   from them and scanning the same arbitrarily chosen input string x, it never happens
   that one computation reaches a final state and the other does not. Notice that:
   1. Thesinkstateq err isdistinguishablefromeveryotherstate p,whichbyhypothesis
   is post-accessible; therefore for some string x, δ (p, x) is a final state, while
   δ (q err , x) = q err .

   2. Two states p and q are distinguishable if p is final and q is not, because it holds
   δ (p, ε) ∈ F and δ (q, ε) / ∈ F.

   3. Two states p and q are distinguishable if, for some input character a, their next
   states δ (p, a) and δ (q, a) are distinguishable.

   In particular, notice that state p is distinguishable from state q if the sets of labels
   of the arcs outgoing from p and from q are different. In that case, there exists a
   character a such that the move from state p reaches a state p ? , while the move from
   q is not defined; i.e., it reaches the sink; then, from the condition 3 above, p and q
   are distinguishable.

   Undistinguishability as a relation is symmetric, reflexive and transitive; i.e., it is
   an equivalence relation. Its equivalence classes are computed by a straightforward
   procedure next described by means of an example.

   Example 3.8 (equivalence classes of undistinguishable states) For the automaton
   M of Fig.3.5 (top), we construct a square table of size | Q | × | Q | to contain the
   undistinguishability relation. More precisely, since the relation is symmetric and
   reflexive, it suffices to fill the cases under the main diagonal, which form a triangular
   matrix of side | Q | − 1.

   The procedure marks with “×” the case (p, q), when it discovers that states p
   and q are distinguishable. Initially, we mark with “×” every case (p, q) such that
   only one state is final:
   q 1
   q 2 × ×
   q 3 × ×
   q 0 q 1 q 2

<a id="P127"></a>

   q 0 q 1 q 2 q 3
   M →
   ↑
   →
   a b
   a
   b
   b
   a
   b
   a
   q 0 q 1 [q 2 , q 3 ]
   M →
   →
   a b
   b
   a
   b
   a
   Fig.3.5 Non-minimal automaton M of Example3.8 (top). Minimal automaton M ? (bottom)
   Then, we examine each unmarked case (p, q), and for every character a, we
   consider the next statesr = δ (p, a) and s = δ (q, a). If case (r, s) is marked, meaning
   thatstater isdistinguishablefromstates,thenwealsomarkthecase(p, q)because
   states p and q too are distinguishable. Otherwise, if case (r, s) is not marked, the
   state pair (r, s) is written into case (p, q), as a future obligation, if case (r, s) will
   get marked, to mark case (p, q) as well.

   The pairs of next states are listed in the table on the left. The result of this step is
   the table on the right:
   q 1
   (1,1)
   (0,2)
   q 2 × ×
   q 3 × ×
   (3,3)
   (2,2)
   q 0 q 1 q 2
   q 1 ×
   q 2 × ×
   q 3 × ×
   (3,3)
   (2,2)
   q 0 q 1 q 2

<a id="P128"></a>

   Notice that case (1, 0) is marked because the state pair (0, 2) ≡ (2, 0) was already
   marked. Now all the cases are filled and the algorithm terminates. The cases not
   marked with “×” identify the undistinguishable state pairs, here only the pair
   (q 2 , q 3 ). By definition, an equivalence class contains all the pairwise undistinguish-
   able states. Here the equivalence classes are [q 0 ], [q 1 ] and [q 2 , q 3 ]. ?
   It is worth analyzing what happens when the transition function δ is not total. To this
   end, imagine to modify the automaton M of Fig.3.5 as follows. We erase the
   selfloopδ (q 3 , a) = q 3 ,byredefiningthefunctionasδ (q 3 , a) = q err .Asaconsequence,
   statesq 2 andq 3 becomedistinguishable,becauseδ (q 2 , a) = q 3 andδ (q 3 , a) = q err .
   Now every equivalence class is a singleton, meaning that the automaton is minimal.

### 3.4.3.1 Constructionof the Minimal Automaton

   The minimal automaton M ? , equivalent to the given automaton M, has for states the
   equivalence classes of the undistinguishability relation. It is simple to construct the
   transition function of M ? . Machine M ? contains the arc:
   C 1
   [..., p r , ...]
   b
   − →
   C 2
   [..., q s , ...]
   between the equivalence classes C 1 and C 2 , if, and only if, machine M contains an
   arc p r
   b
   − → q s , between two states, respectively, belonging to the two classes. Notice
   that the same arc of M ? may derive from several arcs of M.

   Example 3.9 (Example3.8 continued) The minimal automaton M ? is in Fig.3.5
   (bottom). It has the smallest number of states of all equivalent machines. In fact, we
   could easily check that by merging any two states, the resulting machine would not
   be equivalent and would accept a language larger than the original. ?
   From this, it is now straightforward to check whether two given machines are 
   equivalent. First, minimize both machines, and then compare their state-transition graphs
   to see whether: they are isomorphic, corresponding arcs have the same label, and
   corresponding states have the same attributes as initial and final. 4
   Obvious economy reasons would make the minimal machine a preferable choice,
   butthesavingisoftennegligibleforthecasesofconcernincompilerdesign.Whatis
   more,incertainsituationsstateminimizationshouldbeavoided.Whentheautomaton
   is enriched with actions computing an output function, to be seen in Sect.3.9 and
   in Chap.5, two states that are undistinguishable for the recognizer may produce
   different output actions, and merging would spoil the intended function.

   Finally,weanticipatethattheuniquenesspropertyoftheminimalautomatondoes
   not hold for the nondeterministic machines to be introduced next.

   4 Other more efficient equivalence tests do without first minimizing the automaton.

<a id="P129"></a>


## 3.4.4 From Automaton to Grammar

   Without much effort we are going to realize that finite automata and unilinear (or
   type 3) grammars of Definition 2.74 on p. 85 are alternative but equivalent notations
   to define the same language family. First, we show how to construct a right-linear
   grammar equivalent to a given automaton.

   The nonterminal set V of the grammar is the state set Q of the automaton, and the
   axiom is the initial state. For each move q
   a
   − → r, the grammar has the rule q → ar.

   If state q is final, the grammar has also a terminal rule q → ε.

   Clearly, a one-to-one (bijective) correspondence holds between automaton
   computations and grammar derivations: a string x is accepted by the automaton if, and
   only if, it is generated by a derivation q 0
   +
   = ⇒ x.

   Example 3.10 (from automaton to right-linear grammar) The automaton and
   corresponding grammar are:
   q 0 q 1 q 2 →
   ↓
   →
   b
   c
   a, b
   a
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   q 0 → a q 0 | bq 1 | ε
   q 1 → cq 0 | a q 2 | bq 2
   q 2 → ε
   Sentence bca is recognized in three steps by the automaton and derives from the
   axiom in 3 + 1 steps:
   q 0
   q 0 →bq 1
   ====⇒ bq 1
   q 1 →cq 0
   ====⇒ bcq 0
   q 0 →a q 0
   ====⇒ bca q 0
   automaton moves
   q 0 →ε
   ===⇒ bca ε
   acceptance
   = bca
   Observe that the grammar contains empty rules, but of course it can be turned into
   the non-nullable normal form through the transformation on p. 66. First, we find the
   set of nullable nonterminals Null = { q 0 , q 2 }, and then we construct the equivalent
   rules:
   q 0 → a q 0 | bq 1 | a | ε q 1 → cq 0 | a q 2 | bq 2 | a | b | c
   Now, symbol q 2 is deleted because its only rule is empty. At last, grammar cleaning
   produces the rules:
   q 0 → a q 0 | bq 1 | a | ε q 1 → cq 0 | a | b | c
   Of course, the empty rule q 0 → ε must stay there because it is needed to make the
   empty string into the language. ?

<a id="P130"></a>

   We observe that in the non-nullable form, a move entering a final state r, q
   a
   − → r,
   may correspond to two grammar rules: q → ar | a, the former producing the
   nonterminal r, the latter of the terminal type.

   We have seen that conversion from automaton to grammar is straightforward, but
   to make the reverse transformation, we need to modify the automaton definition to
   allow nondeterministic behavior.

3.5 Nondeterministic Automata
=============================

   A right-linear grammar may contain two alternative rules:
   A → a B | a C
   where a ∈ Σ and A, B, C ∈ V
   C A B
   a a
   thatstartwiththesamecharactera.Inthiscase,byconvertingtherulesintomachine
   transitions, two arcs with an identical label exit from the same state A and enter two
   distinct states B and C. This means that in state A, reading character a, the machine
   isfreetochoosethenextstatetoenter,anditsbehaviorisnotdeterministic.Formally,
   the transition function takes two values, δ (A, a) = { B, C }.

   Similarly, a copy rule:
   A → B where B ∈ V A B
   ε
   is represented by an unusual machine transition from state A to state B, which does
   notreadanyterminalcharacter(itwouldbeoddtosaythatitreadstheemptystring).

   A move or arc that does not read an input character is termed a spontaneous or
   epsilon move (ε-move). Spontaneous moves too may cause nondeterminism, as in
   the following situation:
   C A B
   ε a
   where in state A the automaton can choose to move without reading to B, or to read
   the current character, and if it is a, to move to C.


<a id="P131"></a>


## 3.5.1 Motivation of Nondeterminism

   The mapping from grammar rules to transitions has forced us to introduce identically
   labeled arcs with different destinations and spontaneous moves, which are the main
   forms of nondeterminism. Since this may appear a useless theoretical concept, we
   hasten to list its multiple pros.


### 3.5.1.1 Concision

   Defininga language with a nondeterministic machine often results in a more readable
   and compact definition, as in the next example.

   Example 3.11 (penultimate character) Every sentence of language L 2 is 
   characterized by the presence of a letter b in the penultimate position (2nd from the end), e.g.,
   a ba a bb. The r.e. and the nondeterministic recognizer N 2 are shown in Fig.3.6.
   We have to explain how machine N 2 works: given an input string, the machine
   seeksacomputation,i.e.,apathfromtheinitialstatetothefinal,labeledwiththeinput
   string, and if it succeeds, the string is accepted. Thus, string ba ba is recognized
   with the computation:
   q 0
   b
   − → q 0
   a
   − → q 0
   b
   − → q 1
   a
   − → q 2
   Notice that other computations are possible, for instance the path:
   q 0
   b
   − → q 0
   a
   − → q 0
   b
   − → q 0
   a
   − → q 0
   fails to recognize, because it does not reach the final state.

   The same language is accepted by the deterministic automaton M 2 as shown
   in Fig.3.7. Clearly, this machine is not just larger than the nondeterministic type
   in Fig.3.6, but it makes it less perspicuous that the penultimate character must
   be b. ?
   To strengthen the argument, consider a generalization of language L 2 to language
   L k ,suchthat,forsomek ≥ 2,thek-thcharacterbeforethelastisaletterb.Withlittle
   thought, we see that the nondeterministic automaton would have k + 1 states, while
   one could prove that the number of states of the minimal deterministic machine is an
   L 2 = (a | b) ∗ b (a | b)
   q 0 q 1 q 2
   N 2 → →
   b
   a, b
   a, b
   Fig.3.6 R.e. and corresponding nondeterministic machine N 2 checking that the penultimate
   character is b

<a id="P132"></a>

   Fig.3.7 Deterministic
   recognizer M 2 of the strings
   having a letter b next to the
   last one
   p 0 p 1 p 3
   p 2
   M 2 →
   ↓
   →
   b b
   a
   a
   b
   a
   a b
   exponential function of k. In conclusion, nondeterminism sometimes allows much
   shorter definitions.


### 3.5.1.2 Left–Right Interchange and Language Reflection

   Nondeterminismalsoarisesinstringreversal,whenagivendeterministicmachineis
   transformed to recognize the reversal L R of language L. This is sometimes required
   when for some reason, a string must be scanned from right to left.

   The new machine is straightforward to derive: interchange the initial and final
   states 5 and reverse all the arrows. Clearly, this may give birth to nondeterministic
   transitions.

   Example 3.12 (automatonofreversedlanguage)Thelanguagewithbaspenultimate
   character (L 2 of Fig.3.6) is the reflection of the language with the second character
   equal to b:
   L ? =
   ?
   x ∈ {a, b} ∗ | bis the second character ofx
   ?
   L 2 = (L ? ) R
   Language L ? is recognized by the deterministic automaton M ? (left):
   q 0 q 1 q 2 ←
   ← M ?
   b a, b
   a, b
   q 0 q 1 q 2 → →
   b
   a, b
   a, b
   5 If the machine has multiple final states, multiple initial states result, thus causing another form of
   nondeterminism to be dealt with later.


<a id="P133"></a>

   By transforming the automaton M ? as explained above, we obtain the 
   nondeterministic machine for language L 2 (right), which is identical to the one in
   Fig.3.6. ?
   As a further motivation, we anticipate that nondeterministic machines are the
   intermediate product of some procedures for converting r.e. to automata, widely
   used for designing lexical analyzers or scanners.


## 3.5.2 NondeterministicRecognizers

   We precisely define the concept of nondeterministic finite-state computation, first
   without spontaneous moves. A nondeterministic finite automaton N, without
   spontaneous moves, is defined by:
   • the state set Q
   • the terminal alphabet Σ
   • two subsets of Q: the set I of initial states and the set F of final states
   • the transition relation δ, included in the Cartesian product Q × Σ × Q.

   Notice that the machine may have multiple initial states. The graphic representation
   of the machine is as for the deterministic case.

   Asbefore,acomputationoflengthn isaseriesofn transitionssuchthattheorigin
   of each transition coincides with the destination of the preceding one:
   q 0
   a 1
   −→ q 1
   a 2
   −→ q 2 ...

   a n
   −→ q n
   In brief, the computation is also written q 0
   a 1 a 2 ...a n
   −−−−−→ q n . The computation label is
   the string a 1 a 2 ... a n .

   A computation is successful if the first state q 0 is initial and the last state q n is
   final. A string x is recognized or accepted by the automaton, if it is the label of a
   successful computation.

   Let us focus on the empty string. We stipulate that every state is the origin and
   termination of a computation of length 0, which has the empty string ε as label. It
   follows that the empty string is accepted by an automaton if, and only if, there exists
   an initial state that is also final.

   The language L N recognized by automaton N is the set of accepted strings:
   L (N) =
   ?
   x ∈ Σ ∗ | q
   x
   − → r withq ∈ I andr ∈ F
   ?

<a id="P134"></a>

   Example 3.13 (findingawordinatext) Givenastringorword y andatext x,does x
   contain y as substring? The following machine recognizes the texts that contain one
   or more occurrences of y, that is, the language (a | b) ∗ y (a | b) ∗ . We illustrate
   with the word y = bb:
   p q
   r → →
   b b
   a, b
   a, b
   String a bbb is the label of several computations originating in the initial state:
   p
   a
   − → p
   b
   − → p
   b
   − → p
   b
   − → p p
   a
   − → p
   b
   − → p
   b
   − → p
   b
   − → q unsuccessful
   p
   a
   − → p
   b
   − → p
   b
   − → q
   b
   − → r p
   a
   − → p
   b
   − → q
   b
   − → r
   b
   − → r successful
   The first two computations do not find the word looked for. The last two find the
   word, respectively, at positions a b bb and a bb b. ?

### 3.5.2.1 Transition Function

   The transition relation δ of a nondeterministic automaton can still be 
   considered a finite function, though one computing sets of values. For a machine N =
   (Q, Σ, δ, I, F), devoid of spontaneous moves, the functionality of the 
   statetransition function δ is the following:
   δ: Q × Σ → ℘ (Q)
   where symbol ℘ (Q) indicates the powerset of set Q, i.e., the set of all the subsets of
   Q.Nowthemeaningoffunctionδ (q, a) = [ p 1 , p 2 , ..., p k ]isthatthemachine,
   on reading a in the current state q, can arbitrarily move to any of the states p 1 , …,
   p k . As we did for deterministic machines, we extend the function δ to any string y,
   including the empty one, as follows:
   ∀ q ∈ Q δ (q, ε) = [ q ]
   ∀ q ∈ Q ∀ y ∈ Σ ∗ δ (q, y) =
   ?
   p | q
   y
   − → p
   ?
   In other words, it holds p ∈ δ (q, y) if there exists a computation labeled y from q
   to p. For instance, for the automaton of Example3.13 we have:
   δ (p, a) = [ p ] δ (p, a b) = [ p, q ] δ (p, a bb) = [ p, q, r ]

<a id="P135"></a>

   Therefore, we can reformulate the language accepted by automaton N as:
   L (N) =
   ?
   x ∈ Σ ∗ | ∃ q ∈ I such thatδ (q, x) ∩ F ?= ∅
   ?
   i.e., the set computed by function delta must contain a final state, for a string to be
   recognized.


## 3.5.3 Automata with SpontaneousMoves

   Another kind of nondeterministic behavior occurs when an automaton changes state
   without reading a character, thus performing a spontaneous move, represented by an
   ε-arc.Sucharcswillprovetobeexpedientforassemblingtheautomatathatrecognize
   a regular composition of finite-state languages. The next example illustrates the case
   for union and concatenation.

   Example 3.14 (compositional definition of numeric constants) This language
   includes constants such as 90 • 01. The substring preceding the fractional point
   may be missing, as in •01 and it may not contain leading zeroes. Trailing zeroes
   are permitted at the end of the fractional part. The language is defined by the r.e.:
   L = (ε | 0 | N ) • (0 ... 9) + where N = (1 ... 9) (0 ... 9) ∗
   The automaton in Fig.3.8 mirrors the structure of the expression. Notice that the
   presenceofspontaneousmovesdoesnotaffectthewayamachineperformsrecogni-
   tion: a string x is recognized by a machine with spontaneous moves if it is the label
   of a path that originates in an initial state and terminates in a final state.
   Observe that by taking the spontaneous move from state A to state C, the integer
   part N vanishes. String 34 • 5 is accepted with the computation:
   A
   3
   − → B
   4
   − → B
   ε
   − → C
   •
   − → D
   5
   − → E
   ?
   A C D E
   B
   → →
   ε, 0
   1 ... 9
   ε
   • 0 ... 9
   0 ... 9
   0 ... 9
   Fig. 3.8 Automaton with spontaneous moves that recognizes the numerical constants of
   Example3.14

<a id="P136"></a>

   On the other hand, the number of steps (time complexity) of the computation can
   exceed the length of the input string, because of the presence of ε-arcs. As a
   consequence, the recognition algorithm no longer works in real time. Yet, the time
   complexity remains linear, because it is possible to assume that the machine does
   not perform a cycle of spontaneous moves in any computation.

   The family of languages recognized by such nondeterministic automata is also
   called finite-state.


### 3.5.3.1 Uniqueness of the Initial State

   The definition of nondeterministic machine on p. 133 allows two or more initial
   states. However, it is easy to construct an equivalent machine with only one: add to
   the machine a new state q 0 , which will be the only initial state, and add the ε-arcs
   going from it to the former initial states of the automaton. Clearly any computation
   of the new automaton accepts a string if, and only if, the old automaton does so.
   This transformation trades the form of nondeterminism linked with multiple initial
   states, with the form related to spontaneous moves. We will see on p. 142 that such
   moves can be eliminated as well.


## 3.5.4 Correspondence Between Automata and Grammars

   We collect in Table 3.1 the mapping between nondeterministic automata, also with
   spontaneous moves, and unilinear grammars. The correspondence is so direct as to
   witnesses that the two models are essentially just notational variants. Consider a
   right-linear grammar G = (V, Σ, P, S ) and a nondeterministic automaton N =
   ( Q, Σ, δ, q 0 , F ), which from the preceding discussion we may assume to have a
   single initial state. Initially, assume the grammar rules are strictly unilinear (p. 86).
   The states Q match the nonterminals V. The initial state corresponds to the axiom.
   Notice (row 3) that the pair of alternatives p → a q | ar corresponds to a pair of
   nondeterministic moves. A copy rule (row 4) matches a spontaneous move. A final
   state (row 5) matches a nonterminal having an empty rule.

   It is easy to see that every grammar derivation matches a machine computation,
   and conversely, so that the following statement ensues.

   Property 3.15 (finite automaton and unilinear grammar) A language is recognized
   by a finite automaton if, and only if, it is generated by a unilinear grammar. ?
   Notice that the statement concerns also the left-linear grammars, since from Chap.2
   we know that they have the same generative capacity as the right-linear ones.
   If a grammar contains non-empty terminal rules of type p → a, with a ∈ Σ, the
   automaton is modified to include a new final state f , different from those listed at
   row 5 of Table 3.1, and the move
   p
   f →
   a
   .


<a id="P137"></a>

   Table3.1 Correspondence between nondeterministic finite automata (NFA) and right-linear
   grammars
      # Right-linear grammar NFA
      1 Nonterminal alphabet V = Q State set Q = V
      2 Axiom S = q 0 Initial state q 0 = S
      3 p → a q where a ∈ Σ and p, q ∈ V
      p q
      a
      4 p → q where p, q ∈ V
      p q
      ε
      5 p → ε Final state
      p →
   Example 3.16 (right-linear grammar and nondet. automaton) The grammar that
   matches the automaton of numeric constants (Fig.3.8 on p. 135) is:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   A → 0C | C | 1 B | ... | 9 B
   C → • D
   E → 0 E | ... | 9 E | ε
   ?
   B → 0 B | ... | 9 B | C
   D → 0 E | ... | 9 E
   where nonterminal A is the axiom.

   Next, we drop the assumption of unilinearity in the strict sense. Observe the
   right-linear grammar (left) that matches the automaton (right) below:
   ?
   S → a a X | ε
   X → b X | b
   S
   q
   X
   f →
   ↓
   →
   a a
   b
   b
   The non-strictly right-linear rule S → a a X is converted to a cascade of two arcs,
   with an intermediate state q after the first character a. Moreover, the new final state
   f mirrors the last derivation step, which uses rule X → b. ?

## 3.5.5 Ambiguity of Automata

   An automaton is ambiguous if it accepts a string with two different computations.
   Clearly, from the definition it follows that every deterministic automaton is not
   ambiguous. It is interesting to link the notions of ambiguity for automata and for
   unilinear grammars. From knowing that there is a one-to-one (bijective) 
   correspondence between computations and grammar derivations, it follows that an automaton
   is ambiguous if, and only if, the right-linear equivalent grammar is ambiguous, i.e.,
   if the grammar generates a sentence with two distinct syntax trees.


<a id="P138"></a>

   Example 3.17 (ambiguity of automaton and grammar) The automaton of
   Example3.13 on p. 134, reproduced below:
   p q
   r → →
   b b
   a, b
   a, b ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   p → a p | b p | bq
   r → ar | br | ε
   q → br
   recognizes string a bbb with two distinct successful paths. The equivalent grammar
   (right) generates the same string with two trees:
   p
   a p
   b p
   b q
   b r
   ε
   p
   a p
   b q
   b r
   b r
   ε
   Therefore both models are ambiguous with the same degree of ambiguity. ?

## 3.5.6 Left-Linear Grammars and Automata

   We recall that family REG is also defined by using left-linear grammars. By 
   interchanging left and right, it is simple to discover the mapping between such grammars
   and finite automata. Observe the forms of left-linear rules:
   G: A → B a A → B A → ε
   Consider such a grammar G (axiom A) and the reversed language L R = ( L (G)) R .
   Language L R is generated by the reversed grammar, denoted G R , obtained (p. 95)
   by transforming the rules of the first form into A → a B, while the other two forms
   are unchanged. Since grammar G R is right-linear, we know how to construct a finite
   automaton N R for L R .Toobtaintheautomatonoftheoriginallanguage L,wemodify
   automaton N R , by reversing the arc arrows and interchanging the initial and final
   states.


<a id="P139"></a>

   Example 3.18 (from left-linear grammar to automaton) Given grammar G:
      G
      ⎧
      ⎪
      ⎨
      ⎪
      ⎩
      S → Aa | Ab
      A → B b
      B → B a | B b | ε
   the reversed grammar G R is:
      G R
      ⎧
      ⎪
      ⎨
      ⎪
      ⎩
      S → a A | b A
      A → b B
      B → a B | b B | ε
   The corresponding automaton N R that recognizes the mirror language (L(G)) R and
   the recognizer N for language L (G) are:
      S A B
      → →
      a, b
      b
      a, b
      S A B
      ← ←
      a, b
      b
      a, b
      recognizer N R of
      ?
      L (G)
      ? R
      recognizer N of L (G)
   Incidentally, language (L(G)) R has been met before: the penultimate letter of each
   sentence must be b. ?

3.6 From Automaton to Regular Expression:BMC Method
===================================================

   In applications, one has sometimes to compute the r.e. for the language defined by
   a machine. We already know a rather indirect method: since an automaton is easily
   converted into an equivalent right-linear grammar, the r.e. of the language can be
   computedbysolvinglinearsimultaneousequations,asseenonp.87.Thenextdirect
   elimination method, named BMC after Brzozowski and McCluskey, is often more
   convenient.

   Forsimplicitysupposethattheinitialstatei isuniqueandnoarcentersit;similarly
   the final state t is unique and without outgoing arcs. Otherwise, just add a new initial
   state i connected by spontaneous moves to the ex-initial states; similarly introduce
   a new unique final state t. Every state other than states i and t is called internal. We
   construct an equivalent automaton, termed generalized, which is more flexible as it
   allows the arc labels to be not just terminal characters, but also regular languages;

<a id="P140"></a>

   i.e., a label can be an r.e. The idea is to eliminate the internal states one by one,
   while compensating by introducing new arcs labeled with r.e., until only the initial
   and final states are left. Then the label of arc i → t is the r.e. of the language.
   InFig.3.9,observeonthetopaninternalstateq withalltheadjoiningarcs;onthe
   bottom, the same machine fragment after eliminating state q and with compensatory
   arcs is labeled with the same strings produced when traversing state q. To avoid too
   many superpositions, some arcs and labels are not shown, but of course, for each
   state pair p i and r j there should be an arc p i
   H i J ∗ K j
   −−−−−→ r j . Notice that some states
   before eliminating node q
   p 1 r 1
   p 2 r 2
   .

   .

   . q
   .

   .

   .

   p h
   .

   .

   .

   r k
   J
   H 1
   H 2
   H h
   K 1
   K 2
   K k
   after eliminating node q
   p 1 r 1
   p 2 r 2
   .

   .

   .

   .

   .

   .

   p h
   .

   .

   .

   r k
   H 1 J ∗ K k
   H 2 J ∗ K k
   H h J ∗ K k
   H 1 J ∗ K 2
   H 2 J ∗ K 2
   H h J ∗ K 2
   H 1 J ∗ K 1
   H 2 J ∗ K 1
   H h J ∗ K 1
   Fig.3.9 BMC algorithm: deleting a node and compensating with new generalized transition arcs

<a id="P141"></a>

   p q
   ↓
   ↓
   a
   b
   a
   b
   i
   p q
   t
   →
   →
   ε
   a
   b
   ε
   a
   b
   Fig.3.10 Automaton before (left) and after (right) normalizing (Example3.19)
   p i and r j may coincide. Clearly, the set of the strings that may be read when the
   original automaton moves from state p i to state r j coincides with the language that
   labels the arc p i → r j of the new automaton.

   In order to compute the r.e. of the language of a given automaton, the above
   transformation is applied over and over, every time by eliminating an internal state.
   Example 3.19 (from Sakarovitch [12]) The automaton is shown in Fig.3.10, before
   and after normalizing. In Fig.3.11 we trace the execution of the BMC algorithm, by
   eliminating the states in the order q and p. The elimination order does not affect
   the result; though, as in solving simultaneous equations by elimination, it may yield
   more or less complex yet equivalent solutions. For instance, the order p andq would
   produce the r.e. (a ∗ b) + a + | a ∗ . ?
   i
   p
   t
   →
   →
   ε
   ε
   a
   b b ∗ a
   i
   p
   t
   →
   →
   ε
   ε
   a | b b ∗ a = b ∗ a
   i
   t
   →
   →
   (b ∗ a) ∗
   Fig. 3.11 From left to right: the automaton of Fig.3.10 after eliminating node q, unifying the
   self-loops, simplifying the r.e. and eliminating node p

<a id="P142"></a>

3.7 Elimination of Nondeterminism
=================================

   We have argued for the use of nondeterministic machines in the language 
   specifications and transformations, but the final stage of a project usually requires an efficient
   implementation, which can only be provided by a deterministic machine. There are
   rareexceptions,likewhenthecosttobeminimizedisthetimeneededtoconstructthe
   automaton,insteadofthetimespentinrecognizingstrings.Thishappensforinstance
   in a text editor, when a search algorithm is implemented through an automaton to be
   used only one time, to find a given string.

   Next, we describe an algorithm for turning a nondeterministic automaton into an
   equivalent deterministic one; as a corollary every unilinear grammar can be made
   non-ambiguous. The algorithm consists of two sequential phases:
   1. Elimination of spontaneous moves, thus obtaining a machine that in general is
   nondeterministic. We recall that, if the machine has multiple initial states, an
   equivalentmachinecanbeobtainedwithonlyoneinitialstate,whichisconnected
   to the former initial states via spontaneous moves.

   2. Replacementofseveralnondeterministictransitionswithonetransitionthatenters
   a new state. This phase is called powerset construction, because the new states
   correspond to the subsets of the state set.

   We anticipate that a different method that combines the two phases into one is
   Algorithm 3.43 on p. 163.


## 3.7.1 Elimination of SpontaneousMoves

   Since spontaneous moves match the copy rules of the equivalent right-linear
   grammar, one way to eliminate such moves is to use the grammar transformation that
   removes copy rules (p. 67). But first we show how to directly eliminate the ε-arcs
   on the graph of the automaton.

   For the automaton graph, we call ε-path a path made only by ε-arcs. The next
   algorithm initially enriches the automaton with a new ε-arc between any two states
   that are connected by an ε-path. Then, for any length-two path made by an ε-arc
   followed by a non-ε-arc, that we may also call a scanning move, the algorithm adds
   the arc that scans the same terminal character. At last, any state linked to a final state
   by an ε-arc is made final, and all the spontaneous arcs are deleted, as well as all the
   useless states.


<a id="P143"></a>

   Algorithm 3.20 (direct elimination of spontaneous moves) Let δ be the original
   state-transition graph, and let F ⊆ Q be the set of final states.

   Input: a finite-state automaton with ε-moves
   Output: an equivalent finite-state automaton without ε-moves
   1 Transitive closure of the ε-paths:
   repeat
   if graph δ contains a path p
   ε
   − → q
   ε
   − → r (with distinct p, q and r) then
   add the arc p
   ε
   − → r to graph δ
   until no more arcs have been added in the last iteration
   2 Backward propagation of the scanning moves over the ε-moves:
   repeat
   if graph δ contains a path p
   ε
   − → q
   b
   − → r (with distinct p and q) then
   add the arc p
   b
   − → r to graph δ
   until no more arcs have been added in the last iteration
   3 New final states: F := F ∪
   ?
   q | the ε-arc q
   ε
   − → f is in δ and f ∈ F
   ?
   4 Clean-up: delete all the ε-arcs then and all the states that are not accessible
   from the initial state ?
   Example 3.21 (ε-move elimination by Algorithm 3.20) Consider the automaton in
   Fig.3.12, top left. On the top right it is shown the graph after step 1; in the middle
   after steps 2, 3 and the part of 4 that deletes ε-arcs; then, since state B becomes
   inaccessible it is deleted.

   As said, an alternative way to solve the problem transforms the automaton into a
   right-linear grammar, from which the copy rules are then eliminated as explained on
   p. 67. 6 The bottom part of Fig.3.12 shows the details. ?
   After this machine transformation, if the result is not deterministic yet, the next
   transformation must be applied.

   6 Althoughthecopy-ruleeliminationalgorithmonp.67doesnothandleemptyrules,forright-linear
   grammars it continues to work also when empty rules are in the grammar.


<a id="P144"></a>

   automaton with ε-arcs after the transitive closure of ε-paths
   S A B
   C D
   → →
   ε ε
   a
   ε
   b
   c
   d
   e
   S A B
   C D
   → →
   ε ε
   a
   ε
   b
   c
   d
   e
   ε
   ε
   ε
   after back-propagation, final state creation, and deletion of ε-arcs
   S A B
   C D
   → → → →
   →
   e
   a
   b
   c
   d
   e
   e
   original left-linear grammar
   S → A
   C → aS | bD
   A → B | eD
   D → S | cC | dA
   B → ε
   after eliminating copy rules and a rule unreachable from axiom
   S → ε | eD
   C → aS | bD
   A → ε | eD
   D → ε | eD | cC | dA
   B → ε
   Fig. 3.12 Automaton of Example3.21 (top left). The automaton after step 1 of Algorithm 3.20
   (top right). After steps 2, 3 and the first part of step 4, with state B nonaccessible (middle). The
   equivalent grammar and the version after elimination of copy rules (bottom)

## 3.7.2 Construction of Accessible Subsets

   Given a nondeterministic automaton N without spontaneous moves, we explain
   how to construct an equivalent deterministic machine, denoted by M ? . We start by
   observing that, if machine N contains the following nondeterministic moves:
   p
   a
   → p 1 p
   a
   → p 2 ... p
   a
   → p k

<a id="P145"></a>

   then, after reading letter a, it can be in any of the next states p 1 , p 2 , …, p k , i.e., in a
   state of uncertainty. Therefore, to simulate this behavior, in machine M ? we create a
   new state, that we may qualify as collective, and we name it by means of the set of
   states:
   [ p 1 , p 2 , ..., p k ]
   To connect the new state to the others, we proceed to construct the outgoing arcs
   according to the following rule. If the collective state contains the states p 1 , p 2 , …,
   p k , for each of them we consider, in the machine N, the outgoing arcs are labeled
   with the same letter a:
   p 1
   a
   − → [q 1 , q 2 , ...] p 2
   a
   − → [r 1 , r 2 , ...] etc.

   and we merge together the next states:
   [q 1 , q 2 , ...] ∪ [r 1 , r 2 , ...] ∪ ...

   thus obtaining the collective state reached by the transition:
   [ p 1 , p 2 , ..., p k ]
   a
   − → [q 1 , q 2 , ..., r 1 , r 2 , ..., ...]
   If such a state does not already exist, it is added to the current state set of M ? . The
   next Algorithm 3.22 formalizes such a construction. 7
   Given a nondeterministic automaton N = (Q,Σ,δ,q 0 , F) having one initial
   state 8 and without spontaneous moves, the algorithm constructs an equivalent 
   deterministic automaton M ? .

   Algorithm 3.22 (powerset construction) Machine M ? =
   ?
   Q ? , Σ, δ ? , [q 0 ], F ?
   ?
   is
   defined by the following components:
   1. The state set Q ? is the powerset of Q, i.e., Q ? = ℘ (Q)
   2. Initial state: [q 0 ]
   3. Final states: F ? =
   ?
   p ? ∈ Q ? | p ? ∩ F ?= ∅
   ? , i.e., the states that contain a final
   state of N
   4. The transition function δ ? is as follows, for all the states p ? ∈ Q ? and for all the
   characters a ∈ Σ:
   p ?
   a
   − →
   ?
   s ∈ Q | q ∈ p ? ∧ arc
   ?
   q
   a
   − → s
   ?
   is in N
   ?
   ?
   7 If an NFA is represented by the equivalent left-linear grammar, the powerset algorithm above
   performs exactly as the construction for transforming a context-free grammar into invertible form
   by eliminating the repeated right parts of rules, which was presented in Chap.2 on p. 70.
   8 If the given automaton N has multiple initial states, the initial state of machine M ? is the set of all
   of them.


<a id="P146"></a>

   A B C D N →
   →
   b b a
   a, b, c a, b
   A A B A B C A C D M →
   →
   b
   a, c
   b
   a
   b
   a, c
   b
   a
   c
   c
   Fig.3.13 From nondeterministic machine N (top) to deterministic machine M ? (bottom)
   Notice that at line 4, if an arc q
   a
   − → q err leads to the error state, it is not added to the
   collective state. In fact, a computation entering the sink never recognizes any string
   and can be ignored.

   Sincethestatesofmachine M ? arethesubsetsof Q,intheworstcasethecardinality
   of Q ? is exponentially larger than that of Q. This confirms the previous findings that
   deterministic machines may be larger; remember the exponential explosion of the
   number of states in the language that has a specific character in the k-th position
   before the end (p. 131).

   Algorithm 3.22 can be improved: machine M ? often contains states inaccessible
   from the initial state, hence useless. Instead of erasing them with the clean-up 
   procedure, it is better to altogether avoid their creation: we draw only the collective states
   that can be reached from the initial state.

   Example 3.23 (determinization by powerset construction) The nondeterministic
   automaton N in Fig.3.13 (top) is transformed to the deterministic one M ? (bot-
   tom). From δ (A, b) = { A, B } we draw the collective state [ A, B ], and from this
   the transitions:
   [ A, B ]
   a
   − →
   ?
   δ (A, a) ∪ δ (B, a)
   ?
   = [ A]
   [ A, B ]
   b
   − →
   ?
   δ (A, b) ∪ δ (B, b)
   ?
   = [ A, B, C ]
   Then, we create the transitions from the new collective state [ A, B, C ]:
   [ A, B, C ]
   a
   − →
   ?
   δ (A, a) ∪ δ (B, a) ∪ δ (C, a)
   ?
   = [ A, C, D ] final state
   [ A, B, C ]
   b
   − →
   ?
   δ (A, b) ∪ δ (B, b) ∪ δ (C, b)
   ?
   = [ A, B, C ] self-loop

<a id="P147"></a>

   etc. The algorithm ends when step 4, applied to the current state set Q ? , does not
   generateanynewstate.Noticethatnotallthesubsetsof Q correspondtoanaccessible
   state; e.g., the subset and state [ A, C ] would be useless. ?
   Tojustifythealgorithmcorrectness,weshowthatastringx isrecognizedbymachine
   M ? if, and only if, it is accepted by machine N. If a computation of N accepts string
   x, there exists a path labeled with x from the initial state q 0 to a final state q f . The
   algorithm ensures then that in M ? there exists a labeled path x from [q 0 ] to a state
   ?
   ..., q f , ...

   ?
   containing q f .

   Conversely, if string x is the label of a successful computation of M ? , from the
   initial state [q 0 ] to a final state p ? ∈ F ? , then by definition state p ? contains at least
   one final state q f of N. By construction, there exists in N a path labeled with x from
   q 0 to q f . We summarize with a fundamental statement.

   Property 3.24 (recognitionpowerofdeterministicautomata) Everyfinite-statelan-
   guage can be recognized by a deterministic finite automaton. ?
   This property ensures that the recognition algorithm of finite-state languages works
   inrealtime.Thismeansthatitcompletesthejobwithinanumberoftransitionsequal
   to the length of the input string (or fewer if an error occurs before the string has been
   entirely scanned).

   As a corollary of Property3.24, for every language recognized by a finite 
   automaton, there exists an unambiguous unilinear grammar, which is the one (p. 129)
   naturally corresponding to the deterministic automaton. This also says that, for any
   finite-state language, we have a procedure to eliminate ambiguity from the
   grammar; that is, finite-state languages cannot be inherently ambiguous (p. 62).

3.8 From Regular Expression to Recognizer
=========================================

   When a language is specified with a regular expression, instead of a unilinear
   grammar, we do not yet know how to construct its automaton. Since this requirement
   is quite common in applications such as compilation, document processing and
   text searching, several methods have been invented. They differ with respect to the
   automaton being deterministic or not, with or without spontaneous moves, as well
   as regarding the algorithmic complexity of the construction.

   We describe two construction methods. The first, due to Thompson, is termed
   structural or modular, as it analyzes the expression into smaller and smaller 
   subexpressions until the atomic ones. Then the subexpression recognizers are constructed
   and interconnected into a graph that implements the language operations (union,
   concatenation and star) present in the expression. In general, the result is 
   nondeterministic with spontaneous moves.

   The second method, named after Berry and Sethi, builds a deterministic machine,
   typically non-minimal in the number of states. The Thompson method, however,

<a id="P148"></a>

   can be also combined with the powerset Algorithm 3.22, to the effect of directly
   producing a deterministic automaton.

   At the end of this section, we will be able to transform language specifications
   back and forth from automata, grammars and regular expressions. This proves that
   the three models are equivalent.


## 3.8.1 Thompson Structural Method

   Given an r.e., we analyze it into simple parts, we produce the corresponding 
   component automata, and we interconnect them to obtain the complete recognizer. In this
   construction, each component machine is assumed to have exactly one initial state
   without incoming arcs and one final state without outgoing arcs. If not so, simply
   introduce two new states, as done for the BMC algorithm on p. 139. The 
   Thompson algorithm 9 incorporates the mapping rule between simple r.e. and automata
   schematized in Table 3.2. The machines in Table 3.2 have many nondeterministic
   bifurcations,withoutgoingε-arcs.ThevalidityoftheThompsonmethodcomesfrom
   it being an operational reformulation of the closure properties of regular languages
   under concatenation, union and star (stated in Chap.2 on p. 25).

   Example 3.25 (fromr.e.toautomatonbytheThompsonmethod)Decomposether.e.

   (a ∪ ε) b ∗ into subexpressions, identified by the names E i,j :
   ?
   0
   ?
   1
   (
   2
   a )
   3
   ∪ (
   4
   ε)
   5
   ?
   6
   ·
   ?
   7
   (
   8
   b)
   9
   ? ∗
   10
   ?
   11
   ?
   a
   E 2,3
   ∪ ε
   E 4,5
   ?
   E 1,6
   · b
   E 8,9
   ∗
   E 7,10
   E 0,11
   Then, apply the mapping to each subexpression, thus producing the automaton
   of Fig.3.14. Notice that we have moderately simplified the construction, to avoid
   a proliferation of states. Of course, several states are redundant and could be
   coalesced. ?
   Some existing tools use improved versions of the algorithm to avoid constructing
   redundant states. Other tool versions combine the algorithm with the one for the
   eliminationofspontaneousmoves.ItisinterestingtolookattheThompsonalgorithm
   as a converter from the notation of an r.e. to the state-transition graph of a machine.
   Fromthisstandpoint,thisisatypicalcaseofsyntax-directedanalysisandtranslation,
   where the syntax is the one of the r.e. Such an approach to translator design is
   presented in Chap.5.

   9 Originally presented in [14]. It forms the base of the popular tool lex (or GNU flex) used for
   building scanners.


<a id="P149"></a>

   Table 3.2 From component subexpression to component subautomaton. A rectangle depicts a
   component subautomaton with its unique initial (left) and final (right) states
   Atomic expressions
   → → → →
   a ε
   Concatenation of expressions
   i ? t ? i ?? t ??
   −→ −→
   ε
   Union of expressions
   i ? t ?
   i t
   i ?? t ??
   → →
   ε
   ε
   ε
   ε
   Star closure of an expression
   i
   i ? t ?
   t → →
   ε
   ε ε
   ε
   ε
   ε
   → →
   ε
   ε
   a
   ε
   ε
   ε
   ε ε b ε
   ε
   ε
   Fig.3.14 Machine derived from r.e. (a ∪ ε) b ∗ (Example 3.25) through the structural (Thomp-
   son) method

<a id="P150"></a>


## 3.8.2 Berry–Sethi Method

   Another classical method, due to Berry and Sethi [15], derives a deterministic
   automaton that recognizes the language of a given regular expression. To justify
   this construction, we preliminary introduce the local languages, 10 a simple 
   subfamilyofregularlanguages,andtherelatednotionsoflocalautomatonandlinearregular
   expression.


### 3.8.2.1 LocallyTestable Language and Local Automaton

   Some regular languages are extremely simple to recognize, as it suffices to test if
   certain short substrings are present. An example is the set of the strings that start
   with b, end with a or b, and contain only ba or a b as substrings.

   Definition 3.26 (local sets) For a language L over an alphabet Σ, the set of initials
   is:
   Ini(L) =
   ?
   a ∈ Σ | a Σ ∗ ∩ L ?= ∅
   ?
   i.e., the starting characters of the sentences. The set of finals is:
   Fin(L) =
   ?
   a ∈ Σ | Σ ∗ a ∩ L ?= ∅
   ?
   i.e., the ending characters of the sentences. The set of digrams (factors) is:
   Dig(L) =
   ?
   x ∈ Σ 2 | Σ ∗ x Σ ∗ ∩ L ?= ∅
   ?
   i.e., the substrings of length two present in the sentences. The three sets above are
   called local. The complementary digrams are:
   Dig(L) = Σ 2 \ Dig(L)
   ?
   Example 3.27 (local language) The local sets for language L 1 = (a bc) ∗ are:
   Ini(L 1 ) = { a } Fin(L 1 ) = { c } Dig(L 1 ) = { a b, bc, ca }
   and the complement of the digram set is:
   Dig(L 1 ) = { a a, a c, ba, bb, cb, cc }
   ?
   10 We follow the conceptual path of Berstel and Pin [16].


<a id="P151"></a>

   We observe that the non-empty sentences of this language are precisely defined by
   the three local sets, in the sense of the following identity:
   L 1 \ { ε } =
   ?
   x
   Ini(x) ∈ { a } ∧ Fin(x) ∈ { c }
   ∧ Dig(x) ⊆ { a b, bc, ca }
   ?
   We generalize the example to the next definition.

   Definition 3.28 (local language) A language L is called local (or locally testable)
   if it satisfies the following identity:
   L \ { ε } =
   ?
   x
   Ini(x) ∈ Ini(L) ∧ Fin(x) ∈ Fin(L)
   ∧ Dig(x) ⊆ Dig(L)
   ?
   (3.1)
   In other words, the non-empty phrases of language L are defined precisely by sets
   Ini, Fin and Dig. ?
   Notice that Definition 3.28 only refers to the non-empty strings; hence it does not
   say anything about the inclusion of string ε in language L. For instance, language
   L 2 = (a bc) + = L 1 \ { ε } is also local.

   Clearly,notallthelanguagesarelocal,butitshouldbeclearthateverylanguage L
   satisfiesamodifiedcondition(3.1),wheretheequalsign“=”isreplacedbyinclusion
   “⊂”.Infact,bydefinitioneverysentencestarts(resp.ends)withacharacterofIni(L)
   (resp. Fin(L)) and its digrams are included in Dig(L). But such conditions may be
   also satisfied by other strings that do not belong to the language. When this happens,
   the language is not local, as it does not include all the strings that can be generated
   from Ini, Fin and Dig.

   Example 3.29 (non-local language) For language L 2 = b (a a ) + b, we have:
   Ini(L 2 ) = Fin(L 2 ) = { b }
   Dig(L 2 ) = { a a, a b, ba } Dig(L 2 ) = { bb }
   The length of every sentence of language L 2 is even. On the other hand, among the
   strings that start and end with letter b and that do not contain the digram bb, some
   existsofoddlength,suchasba a a b,andalsoeven-lengthstringssuchasba ba a b.
   Thus, the language defined by condition (3.1) strictly includes language L 2 , which
   therefore is not local. ?

<a id="P152"></a>

   Automata Recognizing Local Languages
   Our present interest 11 for local languages comes from the notable simplicity of their
   recognizers. To recognize a string, the machine scans it from left to right, checks
   that the initial character is in Ini, verifies that any pairs of adjacent characters are
   in Dig, and finally checks that the last character is in Fin. Such operations can be
   easily performed by a finite automaton. An automaton recognizing a local language
   L ⊆ Σ ∗ , specified by the local sets Ini, Fin and Dig of L, is speedily obtained, as
   next specified.

   Algorithm 3.30 (fromlocalsetstorecognizer) Therecognizerofthelocallanguage
   specified by the three local sets is constructed as follows.

   • The state set is { q 0 } ∪ Σ, meaning that each non-initial state is identified by a
   terminal character.

   • The final state set includes set Fin and, if the empty string ε is in the language,
   also the state q 0 ; no other state is final.

   • The transitions are q 0
   a
   − → a if a ∈ Ini, and a
   b
   − → b if a b ∈ Dig. ?
   Such an automaton is in the state identified by letter b if, and only if, the character
   most recently read is b. We may think that the machine has a sliding window with a
   width of two characters, which triggers the transition from the previous state to the
   current one if the digram is listed in the set Dig. The automaton produced by this
   construction is deterministic and has the following property that characterizes the
   family of local automata.

   Definition 3.31 (local automaton) A deterministic automaton A =
   ( Q, Σ, δ, q 0 , F ) is called local if it satisfies the condition:
   ∀ a ∈ Σ |{ δ (q, a) | q ∈ Q }| ≤ 1 (3.2)
   which forbids that two identically labeled arcs enter distinct states. ?
   We check that the recognizer computed by Algorithm 3.30 satisfies condition (3.2):
   in fact, each non-initial state is identified by a letter a ∈ Σ and all the arcs labeled
   witha enterthestateidentifiedbya.Therefore,suchanautomatonislocalaccording
   to Definition 3.31, and in addition it has two special properties:
   1. The state set is Q = { q 0 } ∪ Σ. Therefore, the number of states is |Σ | + 1.
   2. All and only the arcs labeled with the character a ∈ Σ enter state a. 
   Consequently, no arc enters the initial state q 0 .

   11 In Chap.5, local languages and automata are also used to model the control-flow graph of a
   program.


<a id="P153"></a>

   If a local automaton additionally satisfies the two conditions 1 and 2 above, it is
   called normalized.

   We show that any local automaton A admits an equivalent normalized machine
   A ? .Consideramachine Asatisfyingcondition(3.2),butnotcondition2above.Then,
   automaton Acontainstwo(ormore)distinctlylabeledarcsthatenterthesamestateq,
   i.e., p
   a
   − → q
   b
   ←− r,wherea ?= b.Toobtainthenormalizedmachine A ? ,wesplitstateq
   into two states, sayq a andq b , with the new arcs p
   a
   − → q a andq b
   b
   ←− r. Consequently,
   every arc q
   c
   − → s outgoing from q is replaced by the arcs q a
   c
   − → s and q b
   c
   − → s. By
   repeating this transformation for all the states of machine A, we obtain a machine
   that meets condition 2, since for all the letters a ∈ Σ, all the arcs ...

   a
   − → ... enter
   state q a . But such a machine may still have arcs that enter the initial state and violate
   condition 1: for instance, the pattern p c
   a
   − → q 0 a
   b
   − → r b . To comply with condition 1,
   we simply add the new initial state q ?
   0
   and the arc q ?
   0
   b
   − → r b . Since all the preceding
   machine transformations preserve the language recognized, the machine A ? thus
   obtained is equivalent to A and is normalized local. Clearly, in general a normalized
   local machine has more states than the minimal equivalent local machine, since in
   the latter two or more states q a , q b , … may be coalesced into a single state q.
   The final point we want to make 12 is that local automata (normalized or
   nonnormalized) and local sets define the same language family.

   Property 3.32 (local languages and local automata) For any language L, the three
   conditions below are equivalent:
   1. Language L is local (Definition 3.28).

   2. Language L is recognized by a local automaton (Definition 3.31).

   3. Language L is recognized by a normalized local automaton (Definition 3.31 and
   conditions 1 and 2 on p. 152) ?.

   Example 3.33 (two local automata) We examine two languages. First, for the r.e.
   (a bc) ∗ of Example3.27, the normalized local automaton produced by Algorithm
   3.30 is shown in Fig.3.15 (left).

   Second,forther.e.a (b | c) ∗ ,theminimallocalautomatonisinFig.3.15(right).
   The larger normalized automaton is in Fig.3.17 on p. 158. ?
   Composition of Local Languages Over Disjoint Alphabets
   Before applying the sliding window idea to a generic regular expression, we need
   another conceptual step, based on the following observation: the basic operations,
   when applied to local languages, preserve the locality property, provided that the
   terminal alphabets of the languages to be combined are disjoint.

   12 We refer to [16] for a formal proof.


<a id="P154"></a>

   q 0 a
   b
   c → →
   ↓
   a
   b
   c
   a
   q 0 q 1 → →
   a
   b, c
   Fig.3.15 Two local automata (Example3.33). Left: normalized automaton for language (a bc) ∗
   (Example3.27). Right: non-normalized automaton for language a (b | c) ∗ , see the normalized
   automaton on p. 158
   Property 3.34 (local language closure) Given two local languages L ? ⊆ Σ ?∗ and
   L ?? ⊆ Σ ??∗ over disjoint alphabets, i.e., Σ ? ∩ Σ ?? = ∅, the languages obtained
   by union L ? ∪ L ?? , concatenation L ? · L ?? and star L ?∗ (and cross L ?+ too) are
   local. ?
   Proof Given the normalized local automata of L ? and L ?? , which we also call L ? and
   L ?? with a slight abuse, the next simple Algorithm 3.35 constructs a normalized local
   automaton for the resulting language by combining the component machines. ?
   Letq ?
   0 andq
   ??
   0
   betheinitialstates,andlet F ? and F ?? bethefinalstatesets,ofmachines
   L ? and L ?? . In general, the combined recognizer has the same states and transitions
   as L ? and L ?? have, with some adjustments on the initial and final states and on the
   related arcs. ?
   Algorithm 3.35 (compositionofnormalizedlocalautomataoverdisjointalphabets)
   We separately consider the three operations.

   For the union L ? ∪ L ?? :
   • The new initial state q 0 is the merging of the initial states q ?
   0
   and q ??
   0 .

   • The arcs are those of L ? plus L ?? .

   • Thesetoffinalstatesis F ? ∪ F ?? ifε / ∈ L ? ∪ L ?? ;otherwise,itis F ? ∪ F ?? ∪ {q 0 }
   For the concatenation L ? · L ?? :
   • The initial state is q ?
   0
   • The arcs are those of L ? , plus those of L ?? , except the ones exiting from the initial
   state q ??
   0 , such as q
   ??
   0
   a
   − → q ?? .

   • For each omitted arc q ??
   0
   a
   − → q ?? of L ?? , and for every final state q ? ∈ F ? , add the arc
   q ?
   a
   − → q ?? .

   • The set of final states is F ?? if ε / ∈ L ?? ; otherwise, it is F ? ∪ F ?? .
   For the star L ?∗ :

<a id="P155"></a>

   subexpression combined recognizer
   atomic elements
   a b c
   p 0 a q 0
   b
   r 0 c
   ↓
   ↓
   ↓
   ↓
   ↓
   ↓
   concatenation
   and union
   ab | c
   a
   b
   p 0 r 0
   c
   →
   →
   →
   star
   (ab | c) ∗
   p 0 r 0 a
   b
   c
   →
   ↓
   →
   →
   Fig.3.16 Stepwisecompositionofnormalizedlocalautomataforthelinearr.e.(a b | c) ∗ (Exam-
   ple3.36)
   • The initial state is q ?
   0
   • The arcs are those of L ? , plus the following: for every final state q ∈ F ? , for every
   arc q ?
   0
   a
   − → r in L ? (exiting from the initial state), add the arc q
   a
   − → r.

   • The final state set is F ? ∪
   ?
   q ?
   0
   ?
   . ?
   We easily see that the recognizer produced by Algorithm 3.35 is by construction
   a normalized local automaton and correctly recognizes the union, concatenation or
   star of local languages.

   Example 3.36 (composition of local automata) In Fig.3.16 we apply Algorithm
   3.35 to r.e. (a b | c) ∗ . The recognizer is constructed, starting from the atomic
   subexpressions (characters a, b and c), through the steps:
   concatenate characters a and b
   unite subexpression a b to character c
   apply star to subexpression a b | c ?

<a id="P156"></a>


### 3.8.2.2 Linear Regular Expression and Its Local Sets

   Inagenericr.e.,ofcourseaterminalcharactermayberepeated.Anr.e.iscalledlinear
   if no terminal character is repeated. For instance, r.e. (a bc) ∗ is linear, whereas r.e.
   (a b) ∗ a is not.

   Property 3.37 (linear r.e.) The language defined by a linear r.e. is local. ?
   Proof Consider any two non-overlapping subexpressions that occur in the linear r.e.
   Due to linearity, such subexpressions necessarily have disjoint alphabets. Since the
   whole r.e. is obtained by composing subexpressions, from Property3.34 on p. 154 it
   follows that the language is local. ?
   Noticethatthelinearityofanr.e.isasufficientconditionforthelanguagetobelocal,
   but it is not necessary. For instance, the r.e. (a b) ∗ a, though it is not linear, defines
   a local language.

   Now we know that the language of a linear r.e. is local, and the problem of
   constructing the recognizer melts down to computing the sets Ini, Fin and Dig of the
   language. We next explain how to orderly perform the job.

   Computing the Local Sets of a Regular Language
   InTable3.3welisttherulesforcomputingthethreelocalsetsofaregularexpression.
   Such rules apply to any r.e., but in fact we just use linear expressions. First, we must
   check if the r.e. e is nullable, i.e., ε ∈ L (e). To this end, we inductively define the
   predicate Null(e) on the structure of e by means of the rules in Table 3.3 (top part).
   To illustrate, we have:
   Null
   ?
   (a | b) ∗ b a
   ?
   = Null
   ?
   (a | b) ∗
   ?
   ∧ Null (b a )
   = true ∧ (Null (b) ∧ Null (a ))
   = true ∧ (false ∧ false) = false
   Given a linear r.e., we compute the local sets, and then we apply Algorithm 3.30 (p.
   152), which returns the local automaton that recognizes the language.

   Example 3.38 (recognizer of a linear r.e.) For the r.e. a (b | c) ∗ , the local sets
   are:
   Ini = { a } Fin = { b, c } ∪ { a } = { a, b, c }
   Dig = { a b, a c } ∪ { bb, bc, cb, cc } = { a b, a c, bb, bc, cb, cc }
   By Algorithm 3.30, we produce the normalized local automaton in Fig.3.17. ?
   Numbered Regular Expression
   Given any r.e. e over an alphabet Σ, we next specify how to obtain another regular
   expression, denoted by e ? , that is linear. We make distinct all the terminals that occur

<a id="P157"></a>

   Table 3.3 Rules for computing predicate Null and the local sets Ini, Fin and Dig
   Nullability predicate
   Null(∅) = false
   Null(ε) = true
   Null(a) = False for every terminal a
   Null(e ∪ e ? ) = Null(e) ∨ Null(e ? )
   Null(e · e ? ) = Null(e) ∧ Null(e ? )
   Null(e ∗ ) = true
   Null(e + ) = Null(e)
   Set of initials
   Ini(∅) = ∅
   Ini(ε) = ∅
   Ini(a) = { a } for every terminal a
   Ini(e ∪ e ? ) = Ini(e) ∪ Ini(e ? )
   Ini(e · e ? ) = if Null(e) then Ini(e) ∪ Ini(e ? ) else Ini(e) end if
   Ini(e ∗ ) = Ini(e + ) = Ini(e)
   Set of finals
   Fin(∅) = ∅
   Fin(ε) = ∅
   Fin(a) = { a } for every terminal a
   Fin(e ∪ e ? ) = Fin(e) ∪ Fin(e ? )
   Fin(e · e ? ) = if Null(e ? ) then Fin(e) ∪ Fin(e ? ) else Fin(e ? ) end if
   Fin(e ∗ ) = Fin(e + ) = Fin(e)
   Set of digrams
   Dig(∅) = ∅
   Dig(ε) = ∅
   Dig(a) = ∅ for every terminal a
   Dig(e ∪ e ? ) = Dig(e) ∪ Dig(e ? )
   Dig(e · e ? ) = Dig(e) ∪ Dig(e ? ) ∪ Fin(e) · Ini(e ? )
   Dig(e ∗ ) = Dig(e + ) = Dig(e) ∪ Fin(e) · Ini(e)

<a id="P158"></a>

   Fig.3.17 Normalized local
   automaton of Example3.38,
   which recognizes the
   language defined by the
   linear r.e. a (b | c) ∗
   q 0 a
   b
   c →
   ↓
   ↑
   →
   a b
   c
   b
   c
   b
   c
   in e by numbering each of them, say from left to right, with an integer subscript. The
   alphabetofe ? isdenotedΣ N andiscalledthenumberedinputalphabet.Forinstance,
   ther.e.e = (a b) ∗ a becomese ? = (a 1 b 2 ) ∗ a 3 andthenumberedalphabetisΣ N =
   { a 1 , b 2 , a 3 }. 13 Notice that the choice for numbering is immaterial, provided that
   all the subscripts are distinct.


### 3.8.2.3 From Generic Regular Expression to Deterministic Recognizer

   We are ready to present an algorithm for obtaining a deterministic recognizer for
   any regular expression, through a sequence of steps based on the previous results
   for local languages and linear regular expressions. The next Algorithm 3.39 will be
   optimized into the Berry–Sethi algorithm on p.161.

   Algorithm 3.39 (from r.e. to deterministic automaton)
   1. From the original r.e. e over alphabet Σ, derive the numbered linear r.e. e ? ? over
   alphabet Σ N ∪ { ? } where ? is the terminator, with ? / ∈ Σ.

   2. Build the normalized local automaton that recognizes the (local) language
   L (e ? ?). The automaton has three state types: (i) an initial state q 0 , (ii) for each
   element c ∈ Σ N a state neither initial nor final and (iii) a unique final state ?.
   3. Tag each state with the set of all the symbols that label the arcs outgoing from
   that state:
   • The initial state q 0 is tagged with set Ini(e ? ?).

   • The final state ? is tagged with the empty set ∅.

   • For each state c neither initial nor final, the set is called set of followers of c
   in the r.e. e ? ?; such a set is denoted as Fol(c) and is a subset of Σ N ∪ { ? };
   set Fol is computed from the digrams, in this way:
   Fol(a i ) =
   ?
   b j | a i b j ∈ Dig(e ? ?)
   ?
   a i andb j may coincide
   thesetsFol(c)altogetherareequivalenttothelocalsetDigand,withtheother
   two local sets Ini and Fin, they characterize a local language.

   13 In Sect.2.3.2.1 we defined in the same way a numbered r.e., for the purpose of formulating a
   rough criterion for checking if an r.e. is ambiguous.


<a id="P159"></a>

   4. Merge all the states that are tagged with the same set of followers. The obtained
   automatonisequivalenttothepreviousone.Infact,sincetherecognizedlanguage
   is local, the states tagged with equal sets of followers are undistinguishable (see
   Definition 3.7 on p.126).

   5. Remove the numeric subscript from the symbols that label the automaton arcs,
   withtheeffectofreturningtoalphabetΣ. 14 Byconstruction,theresultingautoma-
   ton, which may be nondeterministic, accepts language L (e ?).

   6. Constructthedeterministicequivalentautomatonbyapplyingtheaccessiblesub-
   setconstruction(Sect.3.7.2onp.144).Whentwoormorestates,eachonetagged
   with a set, are merged into one state, the tag of the latter is the union of all the
   tags. The resulting automaton recognizes language L (e ?).

   7. Remove from the automaton the final state, tagged with ∅, and all its incoming
   arcs, which carry ? as label. Mark as final the states of the resulting automaton
   tagged with a set that includes the terminator ?.

   The resulting automaton is deterministic and recognizes language L (e). ?
   Historical note: restricting Algorithm 3.39 to steps 1, 2, 5 and 7 yields another
   classical method called GMY (due to Glushkov and to McNaughton–Yamada), to
   obtainanondeterministicreal-timemachinethatrecognizesthelanguageofageneric
   r.e. Now, each state is identified by one letter of the numbered alphabet Σ N . In
   contrasttotheresultofAlgorithm3.39,aGMY automatonmaybenondeterministic.

   For instance, from r.e. a | a b numbered a 1 | a 2 b 3 , the nondeterministic moves
   q 0
   a
   − → a 1 and q 0
   a
   − → a 2 are obtained.

   Example 3.40 (det.recognizerforanr.e.) Forr.e.e = (a | bb) ∗ (a c) + ,weapply
   Algorithm 3.39 and obtain the intermediate results of Fig.3.18. Explanation of the
   steps from a machine to the next one in Fig.3.18 are as follows:
   1. The numbered version of e is e ? = (a 1 | b 2 b 3 ) ∗ (a 4 c 5 ) + , and the alphabet is
   Σ N = { a 1 , b 2 , b 3 , a 4 , c 5 }.

   2. Automaton A 1 is local normalized, and the local sets for building it are:
   Ini(e ? ?) = { a 1 , b 2 , a 4 } Fin(e ? ?) = { ? }
   Dig(e ? ?) = {a 1 a 1 , a 1 b 2 , a 1 a 4 , b 2 b 3 , b 3 a 1 , b 3 b 2 , b 3 a 4 , a 4 c 5 , c 5 a 4 , c 5 ?}
   3. In automaton A 2 , the initial state is tagged with Ini(e ? ?) = { a 1 , b 2 , a 4 } and
   the other states are tagged with sets of followers. Three distinct states are tagged
   with the same set, since Ini(e ? ?) = Fol(a 1 ) = Fol(b 3 ) = { a 1 , b 2 , a 4 }.
   14 This is an example of transliteration (homomorphism) as defined on p. 97.

<a id="P160"></a>

   numbered r.e.

   e = (a 1 ∪ b 2 b 3 ) ∗ (a 4 c 5 ) +
   normalized local automaton ↓
   A – rename states →
   a 4 c 5
   q 0 a 1
   b 2 b 3
   A 1 →
   ↓
   a 1
   b 2
   a 4
   b 2
   a 1
   a 4
   b 3
   a 1
   b 2
   a 4
   c 5
   a 4
   c 5
   a 4 ∅
   a 1 a 4 b 2 a 1 a 4 b 2
   b 3 a 1 a 4 b 2
   A 2 →
   ↓
   a 1
   b 2
   a 4
   b 2
   a 1
   a 4
   b 3
   a 1
   b 2
   a 4
   c 5
   a 4
   merge the states with equal names ↓
   A – relabel transitions →
   c 5
   a 4 ∅
   a 1 a 4 b 2 b 3 A 3 →
   ↓
   b 2
   a 4
   a 1
   b 3
   c 5
   a 4
   c 5
   a 4 ∅
   a 1 a 4 b 2 b 3 A 4 →
   ↓
   b
   a
   a
   b
   c
   a
   build accessible subsets ↓
   A – final adjustment →
   a 1 a 4 b 2 c 5 a 4 ∅
   a 1 a 4 b 2 b 3
   c 5
   ↑
   A 5
   ↓
   b
   a
   a
   c
   b
   b
   c
   a
   a 1 a 4 b 2 c 5 a 4
   a 1 a 4 b 2 b 3
   c 5
   ↑
   A 6
   →
   b
   a
   a
   c
   b
   b
   c
   a
   Fig.3.18 StepsofAlgorithm3.39fortherecognizerofr.e.e = (a | bb) ∗ (a c) + (Example3.40)

<a id="P161"></a>

   4. Automaton A 3 is obtained by merging all the identically tagged states.
   5. Automaton A 4 recognizes language L
   ?
   (a | bb) ∗ (a c) + ?
   ?
   and is 
   nondeterministic, due to the two a-labeled arcs from the initial state.

   6. The deterministic automaton A 5 recognizes language L (e ?).

   7. Thedeterministicautomaton A 6 istherecognizerofe = (a | bb) ∗ (a c) + and
   its (unique) final state is the one tagged with set { a 4 ? }. ?

### 3.8.2.4 Deterministic Recognizer by Berry–Sethi Algorithm

   We combine in Algorithm 3.39 (p. 158) the construction of the normalized local
   automaton and the subsequent step that makes it deterministic, thus obtaining the
   Berry–Sethi algorithm (Algorithm 3.41). 15
   For the reader’s convenience, we remind that the numbered version of a r.e. e
   over an alphabet Σ is denoted by e ? , and by e ? ? if followed by the end of text. By
   construction r.e. e ? ? is linear and its alphabet is Σ N ∪ { ? }. For any symbol a i ∈
   Σ N , the set of followers of a i , i.e., Fol(a i ) ⊆ Σ N ∪ { ? }, is defined as Fol(a i ) =
   ?
   b j | a i b j ∈ Dig(e ? ?)
   ? .

   Insteadofstartingfromthelocalautomatonforthelinearr.e.e ? ?andtransforming
   itintoanequivalentdeterministicautomaton,theoptimizedalgorithmincrementally
   generates the states of the final deterministic machine, each one identified by the set
   of followers of the last letter that has been read. Thus, the construction avoids state
   duplication and performs determinization on the fly, by unifying the states reached
   through distinctly subscripted versions a i ,a j ∈ Σ N of the same letter a ∈ Σ.
   Eachstateq istaggedandidentifiedbyasetofnumberedlettersandtheterminator
   ?.Suchasetisdenotedbythenotation I (q) ⊆ Σ N ∪ { ? }(“I”standsforInternal).
   We observe that in general I (q) contains several elements, which just differ in their
   subscript, such as b 2 and b 5 . It is convenient to refer to all such elements having in
   common the letter b ∈ Σ by means of the notation I b (q) = { b 2 , b 5 } ⊆ I (q). We
   also say that the class of an element b j ∈ Σ N is b.

   Algorithm 3.41 (Berry–Sethialgorithm—BS) Eachstateistaggedandidentifiedby
   asubsetofΣ N ∪ { ? }.Astateiscreatedmarkedasunvisited,anduponexamination
   it is marked as visited to prevent the algorithm from reexamining it. The final states
   are those containing the end-of-text mark ?.

   15 For a thorough justification of the method we refer to [15].


<a id="P162"></a>

   Input: the sets Ini and Fol of a numbered r.e. e ? ?
   Output: recognizer A = (Σ, Q, q 0 , δ, F ) of the unnumbered r.e. e
   I (q 0 ) := Ini(e ? ?) // create initial state q 0
   unmark state q 0 // state q 0 is unexamined
   Q := { q 0 } // initialize state set Q
   δ := ∅ // initialize transition function δ
   while ∃ unmarked q ∈ Q do //process each unexamined state q
   foreach a ∈ Σ do // scan each input symbol a
   I (q ? ) := ∅ // create new empty state q ?
   unmark state q ? // state q ? is unexamined
   foreach a i ∈ I a (q) do // scan each a i of class a in q
   I (q ? ) := I (q ? ) ∪ Fol (a i ) // tag q ? by a i -followers
   if q ? ?= ∅ then // if new state q ? is not empty then
   if q ? / ∈ Q then // if q ? is not in state set Q then
   Q := Q ∪
   ?
   q ?
   ?
   // update state set Q
   δ := δ ∪
   ?
   q
   a
   − → q ?
   ?
   // update transition function δ
   mark state q // state q has been examined
   F := { q ∈ Q | I (q) contains ? } // create final state set F
   Example 3.42 (Berry-Sethi algorithm) Apply Algorithm 3.41 to the r.e. e =
   (a | bb) ∗ (a c) + already considered in Example3.40 on p. 159. The determinis-
   tic automaton obtained is shown in Fig.3.19. For the numbered r.e. e ? ?, the sets of
   initials and followers are:
   e ? ? = (a 1 | b 2 b 3 ) ∗ (a 4 c 5 ) + ?
   Ini(e ? ?) = { a 1 , b 2 , a 4 }
   c ∈ Σ N Fol(c)
   a 1 a 1 b 2 a 4
   b 2 b 3
   b 3 a 1 b 2 a 4
   a 4 c 5
   c 5 a 4 ?
   We mention that the automaton thus produced may contain (moderately) more states
   than are necessary, but of course it can be minimized by the usual method. ?

<a id="P163"></a>

   Fol(a 1 ) ∪ Fol(a 4 )
   = { a 1 , b 2 , a 4 , c 5 }
   Fol(c 5 ) =
   { a 4 ,
   Ini(e ) = Fol(b 3 )
   = { a 1 , b 2 , a 4 }
   Fol(b 2 )
   = { b 3 }
   Fol(a 4 )
   = { c 5 }
   →
   →
   b
   a
   b
   b
   c
   a c
   a
   Fig.3.19 Direct construction of a deterministic automaton for the r.e. e = (a | bb) ∗ (a c) + of
   Example3.42 by BS algorithm
   Use of Algorithm BS for Determinizing an Automaton
   Algorithm BS is a valid alternative to the powerset construction (Algorithm 3.22)
   for converting a nondeterministic machine N into a deterministic one M. Actually,
   it is more flexible, since it applies to all nondeterministic forms, including multiple
   initial states and ε-arcs. We show how to proceed.

   Algorithm 3.43 (determinization of a finite automaton by algorithm BS)
   1. Distinctly number the labels of the non-ε-arcs of automaton N and so obtain the
   numbered automaton N ? , which has σ N as alphabet.

   2. Compute the local sets Ini, Fin and Fol for language L (N ? ), by inspecting the
   graph of N ? and exploiting the identity εa = a ε = a.

   3. Construct the deterministic automaton M by applying algorithm BS (Algorithm
   3.41) to the sets Ini, Fin and Fol. ?
   We justify the validity of Algorithm 3.43 by noting that the language recognized by
   automaton N ? corresponds to all the successful computations, i.e., all the paths from
   an initial state to a final one, and that the sets Ini, Fin and Fol, which can be easily
   derivedfromtheautomatongraphbyvisualinspection,characterizeexactlythelabels
   of such successful computations. Thus, the language accepted by N ? coincides with
   the set of the strings that can be obtained from the local sets in all possible ways,
   and it satisfies condition (3.1) on p. 151 and is local. We illustrate with an example.
   Example 3.44 (determinization of an automaton by algorithm BS) Given the
   nondeterministic automaton N of Fig.3.20 (top), by numbering the arc labels we obtain

<a id="P164"></a>

   nondeterministic automaton N
   A B C
   →
   ↓
   ←
   b
   ε
   b
   b
   a
   a
   numbered automaton N
   A B C
   →
   ↓
   ←
   b 1
   ε
   b 2
   b 5
   a 3
   a 4
   deterministic automaton M
   b 2 b 5 b 1 a 3 a 4 b 2 a 3 a 4 b 5
   ↓
   → ←
   a
   b
   b
   b
   a
   Fig.3.20 Automaton N with spontaneous moves (top), numbered version N ? (middle) and 
   deterministic machine M constructed by algorithm BS (bottom) (Example3.44)
   automaton N ? (middle).Thelanguageof N ? islocalandnon-nullable.Then,compute
   the set of initials (left below):
   Ini
   ?
   L (N ? ) ?
   ?
   = { b 1 , a 3 , a 4 }
   c ∈ Σ N Fol(c)
   b 1 b 2 b 5 ?
   b 2 b 1 a 3 a 4
   a 3 b 2 b 5 ?
   a 4 a 3 a 4
   b 5 a 3 a 4
   andnotethatεa 4 = a 4 andεa 3 = a 3 .Proceedtocomputethesetsoffollowers(right
   above). At last, apply algorithm BS (Algorithm 3.41) to construct the deterministic
   automaton M of Fig.3.20 (bottom). ?

<a id="P165"></a>

3.9 String Matching of Ambiguous Regular Expressions
====================================================

   Some recent widespread applications of regular expressions concern the analysis
   of large data sets, where r.e. is used to define patterns that need to be recognized.
   Sometimes the structure of the string to match is not defined uniquely; in this case
   it is typically described through an ambiguous r.e.

   In this section we present a generalization of the Berry–Sethi method that allows
   one to build, from a possibly ambiguous r.e., a deterministic finite transducer, the
   output of which encodes all the possible ways to match the given string and the r.e.
   In Chap.2 we have explained (Definition 2.17 on p. 21) how a string derives from
   an r.e., and in the continuation we show a tree representation of derivations and the
   way to compute it.


## 3.9.1 Trees for Ambiguous r.e.andTheir Phrases

   In the following, we assume that regular expressions are inductively defined as
   customary, with concatenation and union having k ≥ 2 arguments, subexpressions
   enclosed within parentheses when necessary for operator precedence, and with the
   cross operator “+”. The Kleene star operator “∗” is derived from the cross operator,
   with e ∗ = (e) + | ε. For convenience, we list the definition of an r.e. r:
   r = ε r = a ∈ Σ r = r 1 | ... | r k r = r 1 · ... · r k r = (s) r = (s) +
   where r, r i (with 1 ≤ i ≤ k) and s are r.e. (see also Sect.2.3.1). The following r.e.
   will serve as running example:
   Example 3.45 (running example) The r.e. below contains all the operators
   ?
   (a ) + | b a | a b a
   ? ∗
   b (3.3)
   and is equivalent to the following, without star:
   e =
   ? ?
   (a ) + | b a | a b a
   ? +
   | ε
   ?
   b (3.4)
   ?
   Weintroduceanenrichedrepresentationofanr.e.,calledmarked,whichhastheeffect
   of inserting, in each phrase generated, certain new (meta)symbols that represent the
   derivation from the r.e. Intuitively, this process combines two familiar ideas: first the
   r.e. is parenthesized (similarly to the parenthesization of a context-free grammar),
   and then it is numbered to make each terminal symbol distinct.


<a id="P166"></a>

   Definition 3.46 (marked regular expression) Given an r.e. e over an alphabet Σ,
   theassociatedmarkedregularexpression,shortlym.r.e.,denotedby ˘ e,isconstructed
   through a two-step transformation.

   Step 1 is a sort of translation: first (i) it replaces every (round) parenthesis present
   in r.e. e by a square bracket, to be viewed as a metasymbol of ˘ e; then (ii) it wraps
   each subexpression occurring between a pair of square brackets with a new pair
   of parentheses, which are instead meant as terminals for ˘ e. All the other original
   terminals, operators and epsilon letters of e are left unchanged in ˘ e.

   Step2takestheresultofstep1andattachesadistinctsubscripttoalltheterminals,
   both new and original, but not to the metasymbols.

   Here is a snapshot for the r.e. e above:
   e = (a b) + | c
   step 1.i
   ===⇒ [a b] + | c rewrite pair ( ) as [ ]
   step 1.ii
   ====⇒ ([a b] + ) | c wrap with a new pair ( )
   step 2
   ===⇒ 1 ([a 2 b 3 ] + ) 1 | c 4 = ˘ e add subscript to terminals
   Notice that the metasymbols “[ ]”, “|” and “+” do not have a subscript. The use of
   identical subscripts for matching parentheses is just aesthetic.

   More formally, the following (recursive) rules formalize the transformation:
   step 1 Applytoe thetranslationfunction T ?? definedinductivelyasfollows,where
   all the e i are r.e. (1 ≤ i ≤ k) and a ∈ Σ:
   T ? ε ? = ε
   T ? a ? = a
   T ? (e) ? =
   ? ?
   T ? e ?
   ? ?
   T ? (e) + ? =
   ? ?
   T ? e ?
   ? + ?
   T ? e 1 · ... · e k ? = T ? e 1 ? · ... · T ? e k ?
   T ? e 1 | ... | e k ? = T ? e 1 ? | ... | T ? e k ?
   step 2 Intheexpression T ? e ? obtainedfromstep1,numberthefollowingsymbols,
   say, by using an integer counter increasing from left to right: input symbols,
   epsilon symbols and open round parentheses. Attach to each such symbol
   the number as a subscript. Attach to each closed round parenthesis the same
   subscript as the twin open parenthesis. Thus, we have obtained a linear r.e.
   since all the m.r.e. terminals are made distinct. ?
   Applying the function T of Definition 3.46 to the snapshot (step 1) yields:
   T ? (a b) + | c ? = T ? (a b) + ? | T ? c ?
   = (
   ?
   T ? a b ?
   ? +
   ) | c
   = (
   ?
   T ? a ? T ? b ?
   ? +
   ) | c
   = ([a b] + ) | c

<a id="P167"></a>

   and then attaching the subscripts (step 2) yields 1 ([a 2 b 3 ] + ) 1 | c 4 as before. For
   the r.e. e of the running example (Example3.45) the final m.r.e. ˘ e follows:
   ˘ e = 1
   ??
   2 (
   ?
   3 ([a 4 ] + ) 3
   | b 5 a 6 | a 7 b 8 a 9
   ? + )
   2
   | ε 10
   ??
   1
   b 11 (3.5)
   Thereadermayobservethatanm.r.e.issimilartothenumberedr.e.definedonp.156
   foralgorithmBS (Algorithm3.41),butthatinadditionitcontainssquarebracketsand
   numbered parentheses. The reason for numbering parentheses and epsilons, and not
   squarebracketsandoperators,issimple:epsilonsandparentheseshavechangedtheir
   role from metasymbols to terminals of a new alphabet, while operators and square
   brackets have the role of metasymbols. In fact, the square brackets surrounding the
   argument of an iterator “+” or some other subexpression have taken the role of the
   original parentheses present in the r.e. e.

   Thus, the terminal alphabet of an m.r.e., denoted Ω, is the union of two parts:
   Ω = Σ N ∪ M N , respectively, called the numbered input alphabet (already used
   on p. 156), and the set of numbered metasymbols. For instance, for m.r.e. (3.5) the
   terminal alphabet is:
   Ω = { a 4 , b 5 , a 6 , a 7 , b 8 , a 9 , b 11 }
   numbered input alphabet Σ N
   ∪ { 1 (, ) 1 ,
   2 (, ) 2 , 3 (, ) 3 , ε 10 }
   numbered metasymbols M N
   and the metasymbols are M = { ‘[’, ‘]’, ‘ + ’, ‘ · ’, ‘ | ’ }.

   Foranynumberedinputsymbol,e.g.,b 8 ∈ Σ N ,werefertothe“plain”symbolb ∈
   Σ it comes from, by saying that b 8 is of class b. Also, we introduce for convenience
   a function flatten from L (˘ e) to L (e) that, given any string ω ∈ L (˘ e), returns the
   corresponding input phrase by deleting all the subscripts and then canceling all the
   epsilons and round parentheses. Thus, flatten
   ?
   1 ( 2 (a 7 b 8 a 9 ) 2 ) 1 b 11
   ?
   = a ba b and
   flatten
   ?
   1 (ε 10 ) 1 b 11
   ?
   = b.

   Since an m.r.e. has a parenthesized form (see p. 44), it helps to represent its 
   structureasatreegraphnamedmarkedabstractsyntaxtree(MAST),whichisinductively
   defined by the clauses in the following table:
   r.e. type marked abstract syntax tree (MAST)
   ε or a ∈ Σ a leaf node labeled epsilon or a
   e 1 | ... | e k a node “|” with k children, each being the root of e j subtree
   e 1 · ... · e k a node “·” with k children, each being the root of e j subtree
   [e] + node “+” with one child which is the root of e subtree
   [e] root node of e tree
   The MAST for m.r.e. (3.5) associated with the r.e. (3.3) is depicted in Fig.3.21.
   Strings and Trees Defined by m.r.e.

   Now we consider a string ω ∈ (Σ N ∪ M N ) ∗ generated from an m.r.e. ˘ e through
   derivations. We know that ω includes numbered input symbols and numbered

<a id="P168"></a>

   e = (a) + | b a | a b a
   +
   | ε b
   ˘ e =
   1 ( 2 ( 3 ([a 4 ]
   +
   ) 3 | b 5 a 6 | a 7 b 8 a 9
   +
   ) 2 | ε 10 ) 1 b 11
   •
   •
   1 (
   |
   •
   2 (
   +
   |
   •
   3 ( +
   a 4
   ) 3
   •
   b 5 a 6
   •
   a 7 b 8 a 9
   ) 2
   ε 10
   ) 1
   b 11
   Fig.3.21 Markedabstractsyntaxtree(MAST)oftherunningexampler.e.e –cf.thecorresponding
   m.r.e. ˘ e (3.5)
   metasymbols, which altogether provide a complete description of how the phrase
   flatten(ω) ∈ Σ ∗ is derived from the original r.e. e.

   Suppose that r.e. e is ambiguous, and consider an ambiguous phrase x. We show
   that two or more strings are derived from the m.r.e. ˘ e, all of which are mapped by
   function flatten to phrase x; each one encodes with numbered symbols and
   metasymbols, one of the alternative derivations of x from e.

   To illustrate, the m.r.e. (3.5) derives the following two strings ω 1 and ω 2 , which
   correspond to the same ambiguous phrase a ba b:
   ω 1 = 1 ( 2 ( 3 (a 4 ) 3 b 5 a 6 ) 2 ) 1 b 11 and ω 2 = 1 ( 2 (a 7 b 8 a 9 ) 2 ) 1 b 11 (3.6)
   Clearly, such strings have a parenthesized form that encodes a sort of syntax tree;
   therefore, it is appropriate to call them linearized syntax trees (LST). Next, we show
   how to move from such linearized tree to the tree graph in a way consistent with the
   representation of the r.e. by the MAST tree in Fig.3.21. To reduce confusion, we call
   the tree of ω a marked syntax tree (MST), instead of “marked abstract syntax tree.”

<a id="P169"></a>

   •
   •
   1 (
   •
   2 (
   +
   •
   3 (
   +
   a 4
   ) 3
   •
   b 5 a 6
   ) 2
   ) 1
   b 11
   •
   •
   1 (
   •
   2 (
   +
   •
   a 7 b 8 a 9
   ) 2
   ) 1
   b 11
   ω 1 =
   1 ( 2 ( 3 (a 4 ) 3 b 5 a 6 ) 2 ) 1 b 11
   ω 2 =
   1 ( 2 (a 7 b 8 a 9 ) 2 ) 1 b 11
   Fig. 3.22 Two MST trees for the ambiguous phrase a ba b of r.e. e =
   ?
   ((a ) + | b a | a b a ) + | ε
   ?
   b – cf. Fig.3.21. The two trees have as frontiers the strings ω 1
   and ω 2 of L (˘ e), called linearized syntax trees (LST)
   The MST tree of an LST ω, defined by the m.r.e. ˘ e, is obtained from the MAST
   of the m.r.e. ˘ e by applying essentially the same steps that are performed for deriving
   (Definition 2.17 on p. 21) ω from ˘ e. It suffices to outline the construction:
   • When one alternative, say e j , is chosen out of e 1 | ... | e k , the subtrees of the
   discarded alternatives are eliminated from the MAST; the father node, labeled “|”,
   becomes useless and is erased, taking care to preserve the path in the tree (this has
   the effect of shortening the path).

   • When an iterator “+” is expanded by choosing the number of repetitions, an equal
   number of subtrees is appended to the iterator node “+”, each subtree orderly
   corresponding to the substring generated in each iteration.

   The MST trees obtained with the construction outlined, starting from the two LST
   ω 1 , ω 2 ∈ L (˘ e)(see(3.6)),areshowninFig.3.22.Clearly,thefactthatphrasea ba b
   has two different LST can be taken as a new criterion of ambiguity. The formal
   definitions and notations follow.

   Definition 3.47 (linearized syntax trees and ambiguity) Let e and ˘ e be an r.e and
   the corresponding m.r.e. Let x ∈ L (e) be a phrase.


<a id="P170"></a>

   1. By taking all the LST of a single phrase x, or those of all the phrases in L (e), we
   respectively define the LST set of one phrase or of a whole language, as follows:
   LST(x) = { ω ∈ L (˘ e) | x = flatten(ω) }
   LST(e) =
   ?
   x ∈L (e)
   LST(x)
   2. Let ω ∈ LST(x) and τ be the tree graph MST having ω as frontier. Then τ and
   ω are, respectively, the graphic and the linearized representation of a possible
   derivation of phrase x from r.e. e.

   3. A phrase x ∈ L (e) is ambiguous if, and only if, set LST(x) contains two or more
   strings; then we say that the r.e. is ambiguous. The ambiguity degree of x is the
   cardinality of set LST(x). ?
   We show that Definition 3.47 specifies ambiguity more accurately than the
   previous criterion of Sect.2.3.2.1 on p. 22, which was based on a simple but naive way
   of numbering an r.e.

   Example 3.48 (comparison of ambiguity criteria) Consider the r.e. e 1 =
   ?
   (a ) + | b
   ? +
   b and its m.r.e. ˘ e 1 = 1 ([ 2 ([a 3 ] + ) 2 | b 4 ] + ) 1 b 5 . For the phrase
   a a b, we have LST(a a b) = { ω 1 , ω 2 } where ω 1 = 1 ( 2 (a 3 a 3 ) 2 ) 1 b 5 and ω 2 =
   1 ( 2 (a 3 ) 2 2 (a 3 ) 2 ) 1 b 5 ; therefore the ambiguity degree of a a b is two. Figure3.23
   shows the two MST. The structure corresponding to ω 1 is obtained by iterating once
   the outer cross and twice the inner, whereas ω 2 iterates twice the outer cross and
   once the inner for each outer repetition.

   Ontheotherhand,thetestonp.22failstodetecttheambiguityofstringa a b,since
   fromthenaivelynumberedr.e.e ?
   1
   = ?
   (a 1 ) + | b 2
   ? +
   b 3 onlyonestringderivesthat
   matches phrase a a b, namely string a 1 a 1 b 2 . Differently stated, the two letters a
   match the same terminal a 1 of the numbered r.e. ?

### 3.9.1.1 Infinite ambiguity

   We focus on certain r.e. where the ambiguity degree is not finite, starting with an
   example.

   Example 3.49 (infinite ambiguitydegree) Ther.e.e 1 = (a | ε) + defineslanguage
   a ∗ . We consider the associated m.r.e:
   ˘ e 1 = 1 ([a 2 | ε 3 ] + ) 1 (3.7)
   and we examine some phrases and their LST. The set LST(a) of phrase a includes
   the following infinite list of strings, each string being represented by a different MST
   graph:
   1 (a 2 ) 1 1 (ε 3 a 2 ) 1 1 (a 2 ε 3 ) 1 1 (ε 3 a 2 ε 3 ) 1 1 (ε 3 ε 3 a 2 ) 1
   ...


<a id="P171"></a>

   e 1 = (a) + | b
   +
   b
   ˘ e 1 =
   1 ( 2 ([a 3 ]
   +
   ) 2 | b 4
   +
   ) 1 b 5
   phrase x = aab
   LST(x) =
   ω 1 =
   1 ( 2 (a 3 a 3 ) 2 ) 1 b 5
   ω 2 =
   1 ( 2 (a 3 ) 2 2 (a 3 ) 2 ) 1 b 5
   •
   •
   1 (
   +
   |
   •
   2 (
   +
   a 3
   ) 2
   b 4
   ) 1
   b 5
   •
   •
   1 (
   +
   •
   2 (
   +
   a 3 a 3
   ) 2
   ) 1
   b 5
   •
   •
   1 (
   +
   •
   2 (
   +
   a 3
   ) 2
   •
   2 (
   +
   a 3
   ) 2
   ) 1
   b 5
   Fig.3.23 MAST of the r.e. e 1 of Example3.48 (top right). The two MST below represent the LST
   ω 1 and ω 2 (bottom)
   Also,theemptystringεbelongstolanguage L (e 1 )andhasinfiniteambiguitydegree,
   since LST(ε) contains 1 (ε 3 ) 1 , 1 (ε 3 ε 3 ) 1 , etc. ?
   It is not difficult to see that every phrase has an infinite ambiguity degree, because
   the subexpression (a | ε) is under cross “+” and is nullable.

   It is customary to call problematic an r.e., as the one above, that generates some
   infinitely ambiguous phrase. More precisely, we define an r.e. as problematic if it
   contains (or coincides with) a subexpression of the form ( f ) ∗ or ( f ) + where f is
   nullable; i.e., phrase ε is in L ( f ). It can be proved 16 that an r.e. generates a phrase
   of infinite ambiguity degree if, and only if, it is problematic. In the next presentation
   of a string matching algorithm, we start from non-problematic r.e., and we defer the
   case of a problematic r.e. to the end.

   16 A rigorous discussion on problematic r.e. and ambiguity is in Frisch and Cardelli [17].

<a id="P172"></a>


## 3.9.2 Local Recognizer of Linearized SyntaxTrees

   From Definition 3.47 we know that the m.r.e. ˘ e associated to an r.e e defines all
   the linearized syntax trees LST(x) ⊆ L (˘ e) of every phrase x ∈ L (e). Of course,
   we can also view an m.r.e. as an r.e. over the alphabet Ω = Σ N ∪ M N (p. 167). By
   construction,thesymbolsoccurringinΣ N areterminalsmadedistinctbyasubscript.
   Therefore, the m.r.e. is a linear r.e. and, from Property3.37 on p. 156, the language
   L (˘ e) is local. It follows that the recognizer of L (˘ e) is a sliding window machine,
   which can be immediately obtained through Algorithm 3.30 on p. 152. We introduce
   the construction in the next example.

   Example 3.50 (local LST recognizer for non-problematic r.e.) The middle part of
   Fig.3.24 on p. 173 shows the LST recognizer for the ambiguous (non-problematic)
   r.e. e (3.4) on p. 165 (see also Figs.3.21 and 3.22). The initial state and those entered
   by labeled input symbols are thicker. The bottom part of Fig.3.24 shows a 
   languageequivalent machine, which we may call “state trimmed”: it is a machine with arcs
   labeled by a string or more generally by an r.e. To obtain the state-trimmed machine
   we use a method similar to the BMC elimination in Sect.3.6: each maximal path
   composed by a series of arcs labeled with a metasymbol, followed by an arc with
   an input symbol label, is collapsed into a new arc, and the new arc label is the
   concatenation of the old labels. Therefore, in the state-trimmed graph each label is
   a string starting with zero or more metasymbols, and ending with a numbered input
   symbolortheterminator?.SuchstringsarecalledLSTsegmentsorsimplysegments,
   and the input symbol at their end is called the end symbol.

   InthemiddlepartofFig.3.24weobservethat,sincethether.e.isnon-problematic,
   i.e.,notinfinitelyambiguous,everypathconnectingtwothicknodeswithouttravers-
   ing a thick node is acyclic. Therefore, the number of strings that appear as arc labels
   of the bottom graph is finite. ?

## 3.9.3 The Berry–SethiParser

   Wearealmostreadyforthefinalstep:startingfromthelocalrecognizerofLST(˘ e ?),
   we construct a machine, called Berry–Sethi parser (BSP), that recognizes an input
   phrase x ∈ L (e) and generates the set of linearized syntax trees LST(x) of x, thus
   identifying one or more (if phrase x is ambiguous) ways by which phrase x can
   derive from r.e. e.

   For future comparison, in Fig.3.25 we show the classic BS recognizer (built by
   Algorithm 3.41 on p. 161) for the running example r.e. (3.3). We recall or sharpen
   certain terms pertaining to the classic BS recognizer, which are instrumental for the
   construction of the parser.

   • The BS recognizer is now denoted by A BS , and its components are accordingly
   denoted by Q BS and δ BS .


<a id="P173"></a>

   ˘ e = 1 (
   2 ( 3 ([a 4 ]
   +
   ) 3 | b 5 a 6 | a 7 b 8 a 9
   +
   ) 2 | ε 10 ) 1 b 11
   local recognizer of L(˘ e )
   4 8
   2 5 9 12
   0 1 6 10 14 13
   3 7 11 15
   →
   →
   1 (
   2 (
   ε 10
   3 (
   b 5
   a 7
   ) 1
   a 4
   a 6
   b 8
   b 11
   a 4
   ) 3
   3 (
   b 5
   a 7
   a 7
   ) 2
   a 9
   3 (
   a 7
   ) 2
   ) 1
   ) 2
   b 5
   3 (
   b 5
   state-trimmed recognizer of L(˘ e )
   8
   5 9
   0 6 10 14
   11 15
   →
   →
   1 ( 2 (a 7
   1 ( 2 (b 5
   1 ( 2 ( 3 (a 4
   1 (ε 10 ) 1 b 11
   ) 3 b 5
   a 4 , ) 3 3 (a 4
   ) 3 ) 2 ) 1 b 11
   a 6
   b 8
   b 5
   a 7
   3 (a 4
   ) 2 ) 1 b 11
   b 5
   ) 2 ) 1 b 1
   1
   a 9
   a 7
   ) 3 a 7
   3 (a 4
   Fig.3.24 m.r.e. (3.5) of Example3.45 on p. 165 (top). The local recognizer of the LST language
   (middle).Thelanguage-equivalentrecognizerretainsthenodesenteredbyinputcharacters(bottom),
   which are the thick ones of the local recognizer above. The arc labels of the bottom graph are called
   segments

<a id="P174"></a>

   a 6
   a 4
   b 5
   a 7
   b 11
   a 4
   b 5
   a 7
   b 11
   b 8
   a 6
   a 9
   q 0
   q 1
   q 3
   q 2
   ↑
   A BS
   ← →
   a
   b
   a
   a
   b
   a
   numbered r.e.:
   e = ((a 4 ) + | b 5 a 6 | a 7 b 8 a 9 ) ∗ b 11
   Fig.3.25 Classic BS recognizer A BS = ( Q BS , Σ, δ BS , q 0 , F BS ) for the running example r.e. e
   (Example3.45), obtained from the numbered r.e. e ? (p. 156)
   • Each state q ∈ Q BS is identified by a set of numbered input symbols, denoted by
   I (q) ⊆ Σ N ∪ { ? }.

   • For every terminal a ∈ Σ, we denote by I a (q) ⊆ I (q) the set of all the marked
   symbolsofclassa thatarepresentin I (q).Forinstance,inFig.3.25,itis I (q 0 ) =
   { a 4 , b 5 , a 7 , b 11 }, I a (q 0 ) = { a 4 , a 7 }, I b (q 0 ) = { b 5 , b 11 } and I b (q 2 ) = ∅.
   Theparser(alsoknownasLST generator),named A BSP ,isadeterministicfinite-state
   machine called a transducer, 17 which extends the recognizer A BS with an output
   function denoted by ρ. At each transition, function ρ outputs a piece of information
   essentially consisting of metasymbols. The parser input is the string ? x, with x ∈
   Σ + , where for convenience the start-of-text symbol ? is attached as a prefix. The
   initial state is start, and the initial transition from start reads ? and outputs the first
   piece. When string x has been entirely scanned, the concatenation of such pieces
   encodes the set LST(x), the cardinality of which is the ambiguity degree of x.
   Output Function
   Next, we have to specify the output function ρ. For that we have to briefly return
   to A BS : we recall that an arc q
   a
   − → q ? in δ BS is associated with a set of digrams
   bc ∈ Dig(e ? ?),wheree ? istheinputnumberedversionofr.e.e,anda ∈ Σ,b ∈ Σ N
   and c ∈ Σ N ∪ { ? }. Notice that the digrams are such that the symbols b and c are
   in the sets I a (q) and I (q ? ), respectively.

   Then, the corresponding value ρ (q,a) of the output function is a set of strings
   over the alphabet M N of numbered metasymbols. Such strings are precisely the
   substrings of the fragments of linearized syntax tree that may be enclosed between
   symbols b and c.

   To illustrate, we look-ahead to the transducer in Fig.3.26 on p. 175, the states
   of which (except for start) coincide with the states of the A BS in Fig.3.25. The
   17 The transducer model will be discussed at length in Chap.5.


<a id="P175"></a>

   a 6
   a 4
   b 5
   a 7
   b 11
   a 4
   b 5
   a 7
   b 11
   b 8
   a 6
   a 9
   q 0
   q 1
   q 3
   q 2
   →
   start
   ←
   →
   a
   a 4 , ε, a 4
   a 4 , ) 3 3 ( , a 4
   a 4 , ) 3 , b 5
   a 4 , ) 3 , a 7
   a 4 , ) 3 ) 2 ) 1 , b 11
   a 7 , ε, b 8
   initial slice
   ,
   1 ( 2 ( 3 ( , a 4
   ,
   1 ( 2 ( , b 5
   ,
   1 ( 2 ( , a 7
   ,
   1 ( ε 10 ) 1 , b 11
   b
   b 5 , ε, a 6
   b 11 , ε,
   a
   slice
   a 6 ,
   3 ( , a 4
   a 6 , ε, b 5
   a 6 , ε, a 7
   a 6 , ) 2 ) 1 , b 11
   a
   a 4 , ε, a 4
   a 4 , ) 3 3 ( , a 4
   a 4 , ) 3 , b 5
   a 4 , ) 3 , a 7
   a 4 , ) 3 ) 2 ) 1 , b 11
   a 7 , ε, b 8
   b
   b 5 , ε, a 6
   b 11 , ε,
   b 8 , ε, a 9
   a
   slice
   split in two
   for space reasons
   a 6 ,
   3 ( , a 4
   a 6 , ε, b 5
   a 6 , ε, a 7
   a 6 , ) 2 ) 1 , b 11
   a 9 ,
   3 ( , a 4
   a 9 , ε, b 5
   a 9 , ε, a 7
   a 9 , ) 2 ) 1 , b 11
   Fig. 3.26 Graph of the finite transducer A constructed by Algorithm 3.55 as BSP parser for the
   running example r.e. e (Example3.45). The sets of initials Ini SE and of followers Fol SE are listed
   in Table 3.5. Each arc carries an input letter and the output slice, i.e., a set of 3-tuples (the curly
   brackets around each slice are omitted)
   Table 3.4 Some values of the output function ρ of the Berry–Sethi parser (BSP)
   Diagram Metasymbolic string 3-tuple representation
   a 4 a 4 empty string ?a 4 , ε, a 4 ?
   a 4 a 4 ) 3 3 ( ?a 4 , ) 3 3 (, a 4 ?
   output function ρ associated with the transition q 0
   a
   − → q 1 includes, among others,
   the following two strings of numbered metasymbols: the empty string ε and “) 3 3 (”,
   which are enclosed inside the digram a 4 a 4 . We represent each such string as the
   middle component within a 3-tuple, having as first and last components the elements
   of the digram, as schematized in Table 3.4. Later on, a set of 3-tuples will be called
   a slice.

   It helps to observe how the output function values are also visible in the local
   recognizer in Fig.3.24 (top). The strings ε and “) 3 3 (” respectively correspond to
   the arc 8
   a 4
   −→ 8 and to the path 8
   ) 3
   −→ 12
   3 (
   −→ 4
   a 4
   −→ 8. Still in Fig.3.26, for the same
   arc q 0
   a
   − → q 1 the output contains, associated with the digram a 4 b 11 , also the 3-tuple
   ?a 4 , ) 3 ) 2 ) 1 , b 11 ?, which in the local recognizer corresponds to the path:

<a id="P176"></a>

   8
   ) 3
   −→ 12
   ) 2
   −→ 13
   ) 1
   −→ 7
   b 11
   −→ 11
   We stress that the output produced by any transducer transition is of bounded size;
   therefore the machine only needs a finite memory.


### 3.9.3.1 Segments of Linearized SyntaxTree

   To make algorithmic the construction of the transducer, we have to precisely define
   how to split each LST into the segments that are emitted by the output function ρ at
   each step.

   Consider a possibly empty phrase x ∈ L (e) with |x | = n ≥ 0. Let ω ∈ L (˘ e ?)
   beanLST suchthatx = flatten(ω).Wehavealreadyobservedthatstringω,whichwe
   prefer to terminate by ?, can be factorized into what we now call the decomposition
   into segments:
   ω ? = ζ 1 · ... · ζ j · ... · ζ n · ζ n+1
   = μ 1 a 1
   ζ 1
   · ... · μ j a j
   ζ j
   · ... · μ n a n
   ζ n
   · μ n+1 ?
   ζ n+1
   where n ≥ 0 and
   ∀1 ≤ j ≤ n + 1 μ j ∈ M ∗
   N
   and a j ∈ Σ N ? replaces a n+1
   (3.8)
   Each term ζ j is a segment, each term μ j denotes a (possibly empty) string of
   numbered metasymbols, and each symbol a j (obviously flatten(a j ) ∈ Σ) is called the
   end symbol of ζ j . The end symbol ? of the last segment is sometimes unnecessary,
   and we may drop it or leave it implicit, thus identifying ζ n+1 with μ n+1 . For every
   LST ω, the decomposition into segments is obviously unique.

   Definition 3.51 (segment) A segment is a string of type ζ = μa h or ζ = μ ?, with
   μ ∈ M ∗
   N
   and a h ∈ Σ N . Given an r.e. e and its m.r.e. ˘ e, we define the set, denoted
   SE(e), of all the segments that occur in some string of the marked language L (˘ e ?)
   (notice the end of text), as follows (n ≥ 0):
   SE(e) =
   ?
   ζ j | ζ 1 ... ζ j ... ζ n+1 ∈ L (˘ e ?) and ζ j is a segment ?
   ?
   Itisessentialtoobservethat,foreverynon-problematicr.e.e,theset SE (e)isfinite.
   Therefore, given e, it is possible to pre-compute SE (e) and to use it as building
   block in the construction of the Berry–Sethi parser. This approach is similar to the
   use of the symbols of the numbered input alphabet Σ N in the definition of the sets
   of initials and followers in the context of on the classic BS recognizer.

   Next,wetransposefromstringsofcharacterstosequencesofsegmentsthefamiliar
   conceptsofsetsofinitialsandfollowers,byusingthesegmentsinsteadoftheatomic
   letters of the alphabet. Such a transposition is possible since the number of segments
   in the set SE (e) is finite and can be viewed as a (larger) alphabet of symbols.

<a id="P177"></a>

   Table3.5 Set of initial segments and all the sets of follower segments for the r.e. e of the running
   example (Example3.53). The total number of segments is 17
   ˘ e = 1 (
   ?
   2 (
   ?
   3 ([a 4 ] + ) 3 | b 5 a 6 | a 7 b 8 a 9
   ? +
   ) 2 | ε 10
   ?
   ) 1 b 11
   Segment set Segments
   Ini SE (e)
   1 (ε 10 ) 1 b 11 1 ( 2 (a 7 1 ( 2 (b 5 1 ( 2 ( 3 (a 4
   Fol SE (e, a 4 ) a 4 ) 3 a 7 ) 3 b 5 ) 3 3 (a 4 ) 3 ) 2 ) 1 b 11
   Fol SE (e, b 5 ) a 6
   Fol SE (e, a 6 ) a 7 b 5
   3 (a 4
   ) 2 ) 1 b 11
   Fol SE (e, a 7 ) b 8
   Fol SE (e, b 8 ) a 9
   Fol SE (e, a 9 ) a 7 b 5
   3 (a 4
   ) 2 ) 1 b 11
   Fol SE (e, b 11 ) ?
   Definition 3.52 (initial/follower segment) Given an r.e. e, consider its 
   decomposition into segments ζ 1 ... ζ n+1 ∈ L (˘ e) ?. The set of the initial segments of e is:
   Ini SE (e) = { ζ 1 | ζ 1 ... ζ n+1 ∈ L (˘ e ?) }
   Let ζ j = μ j a h , with 1 ≤ j ≤ n, be a segment having a h ∈ Σ N (not ?) as end
   symbol. The set of the segments that follow a h , for short Fol SE , is:
   Fol SE (e, a h ) =
   ?
   ζ j+1 | ζ 1 ... ζ j ζ j+1 ... ζ n+1 ∈ L (˘ e ?) and ζ j = μ j a h
   ?
   We also say that ζ j+1 is a follower of a h and that Fol SE (e, a h ) is the set of followers
   of a h . The set Ini SE (e) and all the sets Fol SE (e, a h ) for a h ∈ Σ N are finite and
   computable. ?
   We illustrate the definition with an example.

   Example 3.53 (initial/follower segments) Table3.5lists thesetIni SE andallthesets
   Fol SE of the m.r.e. ˘ e of the running example. The sets can be calculated by scanning
   them.r.e.fromlefttoright,applyingessentiallythesamerulesforcomputingthelocal
   sets (Table 3.3 on p. 157). We notice that, if we erase all the numbered metasymbols
   M N in Table 3.5, the initial and follower symbols coincide with those of the classic
   BS method for the same r.e. e. ?

<a id="P178"></a>


### 3.9.3.2 Generation of the Parser

   We put together the previous intuitions and definitions into Algorithm 3.55, which
   constructs the BSP parser of an r.e. e. As anticipated, it is convenient to model the
   parser as a deterministic finite transducer, called A. To save effort, we reuse some
   definitions of the BS recognizer (evidenced by a subscript BS) in the specifications
   of the transducer A.

   Definition 3.54 (finite transducer for BSP) Given an r.e. e over the input alphabet
   Σ, the deterministic finite transducer A is specified as follows:
   • The input alphabet is Σ ∪ { ? } and the initial state is start.

   • The state set Q, transition function δ and final state set F are:
   Q = Q BS ∪ { start }, with I (start) = { ? }
   δ = δ BS ∪
   ?
   start
   ?
   − → q 0
   ? , whereq
   0 is the initial state of A BS
   F = F BS , i.e., the final states of Acoincide with those of A BS .

   • For the output function ρ, the domain is the same as the domain of δ and the range
   is:
   O = ℘
   ??
   Σ N ∪ { ? }
   ?
   × M ∗
   N
   ×
   ?
   Σ N ∪ { ? }
   ??
   in words, the domain is a set of sets of 3-tuples (a set of 3-tuples is a slice)
   • The output function ρ: Q ×
   ?
   Σ ∪ { ? }
   ?
   → O is defined as follows:
   ∀q, q ? ∈ Q BS ∀a ∈ Σ
   ?
   q
   a
   − → q ?
   ?
   ∈ δ BS ∀b, c ∈ I a (q), I (q ? ) :
   ?
   ρ (start, ?) = { ??, μ, c? | μc ∈ Ini SE (e) } – initial slice
   ρ (q, a) = { ?b, μ, c? | μc ∈ Fol SE (e, b) } – slice of a-arc
   where a is an input letter (unnumbered), symbols b and c are numbered and 
   respectively belong to the alphabets b ∈ Σ N and c ∈ Σ N ∪ { ? }. In addition symbol b is
   of class a, i.e., a = flatten(b). ?
   Notice that the output range is defined as a the powerset of the Cartesian product of
   three sets. Therefore, the value of the output function ρ is a finite set of 3-tuples of
   the form already considered in Table 3.4 on p. 175. We call such a set a slice, since
   it constitutes a section of the directed acyclic graph DAG that will be introduced in
   Sect.3.9.3.3 to represent all the LST of a phrase. The pseudo-code constructing the
   BSP transducer is in Algorithm 3.55.


<a id="P179"></a>

   Algorithm 3.55 (Berry–Sethi parser algorithm—BSP) The deterministic Berry–
   Sethi transducer is defined by:
   Input: the sets Ini BSP and Fol BSP of a marked regular expression ˘ e
   Output: parser A = (Σ ∪ {?}, Q, start, δ, ρ, F ) of e as transducer
   I (start) := { ? } // create initial state start
   Q := { start } // initialize state set Q
   δ := ∅ // initialize transition function δ
   ρ := ∅ // initialize output function ρ
   I (q 0 ) :=
   ?
   b j | μb j ∈ Ini SE (e)
   ?
   // create new state q 0
   unmark state q 0 // state q 0 is unexamined
   SLICE :=
   ? ?
   ?, μ, b j
   ?
   | μb j ∈ Ini SE (e)
   ?
   // initialize slice
   Q := Q ∪ { q 0 } // update state set Q
   δ := δ ∪
   ?
   start
   ?
   − → q 0
   ?
   // update transition function δ
   ρ := ρ ∪
   ?
   start
   SLICE
   −−−→ q 0
   ?
   // update output function ρ
   mark state start // state start has been examined
   while ∃ unmarked q ∈ Q do //process each unexamined state q
   foreach a ∈ Σ do // scan each input symbol a
   I (q ? ) := ∅ // create new empty state q ?
   unmark state q ? // state q ? is unexamined
   SLICE := ∅ // initialize 3-tuple slice
   foreach a i ∈ I a (q) do // scan each a i of class a in q
   // tag q ? by all end symbols of an a i -follower
   I (q ? ) := I (q ? ) ∪
   ?
   b j | μb j ∈ Fol SE (e, a i )
   ?
   // update 3-tuple slice of output function ρ
   SLICE := SLICE ∪
   ? ?
   a i , μ, b j
   ?
   | μb j ∈ Fol SE (e, a i )
   ?
   if I (q ? ) ?= ∅ then // if new state q ? is not empty then
   if q ? / ∈ Q then // if q ? is not in state set Q then
   Q := Q ∪
   ?
   q ?
   ?
   // update state set Q
   δ := δ ∪
   ?
   q
   a
   − → q ?
   ?
   // update transition function δ
   ρ := ρ ∪
   ?
   q
   SLICE
   −−−→ q ?
   ?
   // update output function ρ
   mark state q // state q has been examined
   F := { q ∈ Q | I (q) contains ? } // create final state set F
   We explain Algorithm 3.55, starting from beginning. As customary, the 
   statetransition and output functions δ and ρ are presented as labels of arcs in machine A
   graph. In the first steps, the initial state start, identified by the tag ?, is created, then

<a id="P180"></a>

   the state q 0 (it was the initial state of A BS ), and the arc start
   ?
   − → q 0 with the output ρ
   defined by the initial segments Ini SE (e).

   Then, the external while loop examines every state q ∈ Q and marks it to avoid
   reexamining. For each state q, the outer for loop creates a new state q ? as the target
   of an arc q
   a
   − → • outgoing from q, with a ∈ Σ. The inner for loop examines every
   numbered symbol a i of class a in I a (q), and it includes in the new set I (q ? ) all the
   end symbols b j of the segments Fol SE (e, a i ). The value of the output function ρ
   for the same arc (variable SLICE) is computed using the metasymbolic part μ of the
   same segments. As a last step, the algorithm identifies as final the states that contain
   ?.

   We remark that the BSP state-transition graph is almost identical to that of the
   classical Berry–Sethi recognizer, with the only addition of the initial state start and
   transition start
   ?
   − → q 0 (with the initial slice). In fact, the BSP construction may be
   viewed as a generalization of the BS method.

   Example 3.56 (finite transducer representing BSP) The finite transducer A for r.e.
   e in Fig.3.26 has been computed by Algorithm 3.55, by using as parameters the sets
   of initials and followers Ini SE and Fol SE listed in Table 3.5. By construction, every
   BSP state is accessible from the initial state and is connected to a final state. ?
   Output Computed by BSP
   A comparison with the BS recognizer A BS in Fig.3.25 on p. 174 confirms that, if we
   disregard the output function ρ and the initial state start of transducer A, the 
   statetransitiongraphsof A and A BS areidentical.Moreover,foranypairofcorresponding
   states of A and A BS , the sets I (•) are identical. Therefore, the following relation
   holds between the languages recognized by the two automata: L (A) = ? L (A BS );
   i.e., the two languages recognized just differ by the presence of the start-of-text.
   Of course, the output function ρ makes the essential difference. We show on the
   example that the string output by transducer A for input x represents LST(x).
   Example 3.57 (output of BSP) We give the string ? b as input to the transducer A
   in Fig.3.26, and we compute the output ρ (? b):
   δ (start, ? b) = q 3
   ρ (start, ? b) =
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   ??,
   1 ( 2 ( 3 (, a 4 ?
   ??,
   1 ( 2 (, b 5 ?
   ??,
   1 ( 2 (, a 7 ?
   ??,
   1 ( ε 10 ) 1 , b 11 ?
   ⎫
   ⎪
   ⎬
   ⎪
   ⎭
   ·
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   ?b 5 , ε, a 6 ?
   ?b 11 , ε, ??
   ⎫
   ⎪
   ⎬
   ⎪
   ⎭
   slice of start
   ?
   − → q 0 slice of q 0
   b
   − → q 3
   String b is unambiguous and has just one LST, which is recognizable as the 
   concatenation of the 3-tuples ??,
   1 ( ε 10 ) 1 , b 11 ? · ?b 11 , ε, ??, and represents the LST:

<a id="P181"></a>

   1 ( ε 10 ) 1 b 11 . Notice that ρ (? b) includes also some 3-tuples, e.g., ??, 1 ( 2 (, a 7 ?
   and ?b 5 , ε, a 6 ?, that do not contribute to any LST for the input b and seem 
   unproductive.

   Infact,everysuch3-tuplewilloccurintheoutputofsomelongerphrase.Consider
   the input ? ba b, which has just one linearized tree:
   LST(ba b) = 1 ( 2 ( b 5 a 6 ) 2 ) 1 b 11
   The output is ρ (start, ? ba b) = ρ (start, ? b) · ρ (q 3 , a b), and the 3-tuple
   ?b 5 , ε, a 6 ?, previously useless for the input b, is now necessary.

   ?

### 3.9.3.3 Constructionof Linearized SyntaxTrees

   The output of the BSP transducer for input string x is a sequence of slices and can
   be interpreted as a directed acyclic graph (DAG), denoted by DAG(x), acting as a
   compact representation of all the LST of x.

   Each node of graph DAG(x) corresponds to a pair ?slice number, 3-tuple?,
   where the first component is the ordinal position of a slice in the DAG and the
   second is a 3-tuple present in that slice. Each arc of the DAG is labeled by strings of
   numbered symbols and metasymbols, which are of the same type as the segments ζ j
   occurring in an LST; see (3.8). On the DAG we show how to identify certain directed
   paths such that, by concatenating the labels on their arcs, one obtains all the LST of
   phrase x.

   Example 3.58 (DAG representing all the LST) Consider Fig.3.27, which is based
   on the accepting run of the BSP of Fig.3.26 for the input x =? a a b. The top part
   reports the transducer execution trace, including the states traversed and the output
   slices produced at each step. The middle part shows the DAG obtained from the
   execution trace above. The DAG contains 19 nodes, each one being identified by
   a pair (slicenumber,3-tuple), but for clarity we do not inscribe the slice number,
   which is instead represented by the column alignment. Two nodes in adjacent slices,
   i.e., in slice-i and slice-(i + 1), are connected by an arc if the third symbol in the
   3-tuple of the left node is equal to the first symbol in the 3-tuple of the right node.
   Wedefineasinitial andfinal anodethathasthestart-markerasfirstcomponentor
   the end-marker as third component, respectively; final nodes are also marked by an
   exitarrow.ADAGpathfromaninitialnodetoafinalnodeisqualifiedasrecognizing.

   By concatenating the middle components (of 3-tuples), i.e., the segments, of all the
   nodes on a recognizing path, we obtain a string over the alphabet Ω, which is an LST
   of the input string (disregarding the final ?).

   In Fig.3.26 there are two recognizing paths, evidenced by solid arcs and nodes;
   they spell out the two LST of phrase a a b. As a minor remark, on the exit arrow that
   marks a final node, the final segment is shown abbreviated without the ?. Thus, the
   segment sequence of the topmost recognizing path is:
   1 ( 2 ( 3 (a 4
   a 4 ) 3 ) 2 ) 1 b 11 an LST of phrase a a b

<a id="P182"></a>

   To avoid clogging the drawing, only a few of the DAG arcs that do not contribute to
   any recognizing path are actually drawn (as dashed) in Fig.3.27.

   The bottom part of Fig.3.27 depicts the marked syntax trees corresponding to the
   two LST. ?

### 3.9.3.4 Extending BSP to Problematic Regular Expressions

   It remains to explain the construction of BSP in the case of a problematic r.e., which
   by definition (p. 170) generates one or more problematic phrases x that have an
   infinite ambiguity degree; i.e., the cardinality of set LST(x) is unbounded. We also
   know that such r.e. contains an iterated subexpression, ( f ) ∗ or ( f ) + , where the
   language of f is nullable.

   Therefore, the segmented form of LST(x) necessarily contains some segment of
   unbounded size. It follows that the BSP transducer computed by our Algorithm 3.55
   on p. 179 would not be a finite-state machine, and the entire parser construction
   collapses.

   To find a way out of this dead end, we study the LST set of a problematic phrase
   x and we argue that, in practice, not much is lost if we cut set LST(x) by keeping
   just finitely many representative LST strings. Thus, we obtain a finite subset that
   we call acyclic and we denote by LST acyclic (x) ⊆ LST(x). After thus restoring the
   indispensable condition that all the segments are finite, we can use Algorithm 3.55
   anew, for a problematic r.e. too.

   Example 3.59 (bounding the ambiguity degree) In Example3.49 on p. 170, the
   (problematic)r.e.e 1 = (a | ε) + ,withm.r.e. ˘ e 1 = 1 ([a 2 | ε 3 ] + ) 1 ,generatesonly
   problematic phrases, such as a. The set LST(a) contains infinitely many LST, and a
   few of them are listed in Example3.49.

   In practice, we can scarcely imagine a situation requiring all the possible LST
   of such infinitely ambiguous phrases. Just a few LST suffice, which can (somewhat
   subjectively) represent all the possibilities. The remaining LST strings are simply
   removed from language L (˘ e). A plausible criterion is to discard all the strings that
   containtwoormoreidenticallynumberedcopiesoftheemptystring,i.e.,asubstring
   ε k ...ε k , without an input symbol of Σ N in between. We call cyclic such strings to
   discard, and consequently acyclic those we choose to keep. Thus, we partition set
   LST(a) into a finite set of representative LST and an infinite set of discarded LST:
   1 (a 2 ) 1 1 (ε 3 a 2 ) 1 1 (a 2 ε 3 ) 1 1 (ε 3 a 2 ε 3 ) 1
   finite set of representative LST —acyclic LST
   1 (ε 3 ε 3 a 2 ) 1 1 (a 2 ε 3 ε 3 ) 1 1 (ε 3 ε 3 a 2 ε 3 ) 1
   ...

   infinite set of discarded LST—cyclic LST
   Intuitively, our approach uses the acyclic LST set to construct the finite-state 
   transducer. Before doing that, we observe the local recognizer of language LST(e) and

<a id="P183"></a>

   BSP (transducer A) accepting path for string aab with function ρ
   a 4
   b 5
   a 7
   b 11
   a 4
   b 5
   a 7
   b 11
   b 8
   a 4
   b 5
   a 7
   b 11
   b 8
   a 6
   a 9
   start
   q 0
   q 1 q 1
   q 2
   ↑
   ,
   1 ( 2 ( 3 ( , a 4
   ,
   1 ( 2 ( , b 5
   ,
   1 ( 2 ( , a 7
   ,
   1 ( ε 10 ) 1 , b 11
   a
   a 4 , ε, a 4
   a 4 , ) 3 3 ( , a 4
   a 4 , ) 3 , b 5
   a 4 , ) 3 , a 7
   a 4 , ) 3 ) 2 ) 1 , b 11
   a 7 , ε, b 8
   a
   a 4 , ε, a 4
   a 4 , ) 3 3 ( , a 4
   a 4 , ) 3 , b 5
   a 4 , ) 3 , a 7
   a 4 , ) 3 ) 2 ) 1 , b 11
   a 7 , ε, b 8
   b
   b 5 , ε, a 6
   b 11 , ε,
   b 8 , ε, a 9
   ρ(start, ) = ρ 1
   ρ(q 0 , a) = ρ 2 ρ(q 1 , a) = ρ 3
   ρ(q 1 , b) = ρ 4
   DAG(aab) — all arcs of recognizing paths (solid) with few others (dashed)
   ,
   1 ( 2 ( 3 (, a 4
   ,
   1 ( 2 (, b 5
   ,
   1 ( 2 (, a 7
   ,
   1 ( ε 10 ) 1 , b 11
   a 4 , ε, a 4
   a 4 , ) 3 3 (, a 4
   a 4 , ) 3 , b 5
   a 4 , ) 3 , a 7
   a 4 , ) 3 ) 2 ) 1 , b 11
   a 7 , ε, b 8
   a 4 , ε, a 4
   a 4 , ) 3 3 (, a 4
   a 4 , ) 3 , b 5
   a 4 , ) 3 , a 7
   a 4 , ) 3 ) 2 ) 1 , b 11
   a 7 , ε, b 8
   b 5 , ε, a 6
   b 11 , ε,
   b 8 , ε, a 9
   ρ 1 : slice 1
   (initial)
   ρ 2 : slice 2 ρ 3 : slice 3 ρ 4 : slice 4
   − − →
   exit
   ε
   rec. arc
   1 ( 2 ( 3 (a 4
   1 ( 2 ( 3 (a 4
   rec. arc
   a 4
   ) 3 3 (a 4
   rec. arc
   ) 3 ) 2 ) 1 b 11
   non-rec. arc
   1 ( 2 (a 7
   non-rec. arc
   a 4
   ) 3 b 5
   MST of aab MST of aab
   •
   •
   1 (
   •
   2 (
   +
   •
   3 (
   +
   a 4 a 4
   ) 3
   ) 2
   ) 1
   b 11
   •
   •
   1 (
   •
   2 (
   +
   •
   3 (
   +
   a 4
   ) 3
   •
   3 (
   +
   a 4
   ) 3
   ) 2
   ) 1
   b 11
   ω 1 = 1 ( 2 ( 3 (a 4 a 4 ) 3 ) 2 ) 1 b 11 ω 2 = 1 ( 2 ( 3 (a 4 ) 3 3 (a 4 ) 3 ) 2 ) 1 b 11
   Fig.3.27 LST construction for string a a b of Example3.58. Accepting path of the BSP A (top).
   DAG of string a a b and the two recognizing paths (middle). The brackets and ordinal numbers of
   the 3-tuples are omitted. Marked and linearized syntax trees (bottom)

<a id="P184"></a>

   local LST recognizer
   of problematic r.e. (a | ε) +
   with m.r.e. 1 ([a 2 | ε 3 ] + ) 1
   state-trimmed infinitely
   ambiguous LST recognizer
   2
   1 4
   0 3 5
   ↑ ↓
   1 (
   a 2
   ε 3
   a 2
   ε 3
   ) 1
   ε 3
   a 2
   ) 1
   2
   0 5
   ←
   →
   1 (a 2 | 1 (ε 3 [ε 3 ] ∗ a 2
   1 (ε 3 [ε 3 ] ∗ ) 1
   a 2 | ε 3 [ε 3 ] ∗ a 2
   ) 1 ε 3 [ε 3 ] ∗ ) 1
   state-trimmed finitely ambiguous LST recognizer
   2
   0 5
   ←
   →
   1 (a 2 , 1 (ε 3 a 2
   1 (ε 3 ) 1
   a 2 , ε 3 a 2
   ) 1 , ε 3 ) 1
   Fig.3.28 Local LST recognizer for the problematic (infinitely ambiguous) r.e. e 1 of Example3.59
   (top left). The equivalent state-trimmed machine (top right). The same state-trimmed machine with
   ambiguity degree cut down to a finite value (bottom)
   thestate-trimmedone,showninFig.3.28(top).Wecomparewiththerecognizerofa
   non-problematicr.e.inFig.3.24onp.173(topandmiddle),tobringouttheessential
   difference. For the non-problematic case, every path that connects two thick nodes
   without traversing a thick node is acyclic. Therefore in the state-trimmed graph,
   every arc label is a regular language of finite cardinality.

   On the other hand, for the problematic case, the acyclicity property falls for the
   local LST recognizer shown in Fig.3.28 (top left). Here some paths from thick node
   to thick node are cyclic, e.g., path 0, 1, 3, 3, …, 3, 2. Therefore, when eliminating

<a id="P185"></a>

   the thin nodes while preserving equivalence, some arc labels of the state-trimmed
   machine in Fig.3.28 (top right) contain the iteration operator and define an infinite
   language. See, e.g., r.e. 1 (a 2 |
   1 (ε 3 [ε 3 ] ∗ a 2 on arc 0 → 2.

   ?
   As anticipated, we want to keep only a finite number of ambiguous LST for a
   given phrase. To this effect, for each pair of thick nodes we compute the set of paths
   that connect the first node to the second one and satisfy the following two constraints
   that ensure finiteness:
   1. The path does not traverse any intermediate thick node.

   2. The path includes at most one arc labeled with the same numbered empty string
   symbol ε k .

   Thelabelofsuchpreservedpathiscalledanacyclicsegment (ASE)ofLST.TheASE
   are written on the label of the arc that connects two (formerly thick) nodes in the
   state-trimmed machine. Figure3.28 (bottom) shows the resulting state-trimmed
   recognizer with ambiguity degree cut down to a finite value. Notice that, in accordance
   with constraint 2 above, the label of the cyclic path 2 → 3 → 3 → ... → 4 → 5 in
   Fig.3.28 (top left) is not an ASE; therefore it is not attached to the arc 2 → 5 of the
   bottom graph in the same figure.

   Definition 3.60 (acyclic marked language and LST) Let ASE(e) ⊆ SE(e) be the
   set of all the acyclic segments of an r.e. e (of course such ASE is based on the m.r.e.
   ˘ e of e). For r.e. e, we define the acyclic marked language L acyclic (˘ e) generated by
   the m.r.e. ˘ e, as:
   L acyclic (˘ e) =
   ?
   ζ 1 ... ζ j ... ζ n+1 ∈ L (˘ e ?) | n ≥ 0 and everyζ j ∈ ASE(e)
   ?
   For a phrase x ∈ L (e), we define the set of acyclic linearized syntax trees, as:
   LST acyclic (x) = LST(x) ∩
   ?
   L acyclic (˘ e) \ { ? }
   ?
   The quotient “\” (Sect.2.2.5) cancels the trailing end-of-text ?, and the intersection
   filters out any string that contains a non-acyclic segment. ?
   Notice that if an r.e. e is non-problematic, the two notions of marked language
   coincide: L acyclic (˘ e) = L (˘ e). To construct the BSP parser of a problematic r.e., we
   proceedasforanon-problematicone,takingcaretousetheASE setinsteadoftheSE
   set. We compute the sets of initial segments and follower segments, which we call
   Ini ASE (e)andFol ASE (e),respectively.Atlast,weconstructtheBSP finitetransducer
   through the same Algorithm 3.55 on p. 179, using in the pseudo-code the sets Ini ASE
   and Fol ASE instead of Ini SE and Fol SE , respectively.

   Example 3.61 (BSP for problematic r.e.) We illustrate the problematic case on r.e.
   e 1 of Example3.59. The acyclic segment set ASE, and the various sets Ini ASE and

<a id="P186"></a>

   Table 3.6 Set of acyclic initial segments and all the sets of acyclic follower segments for the
   problematic r.e. e 1 of Example3.59 on p. 182
   ˘ e 1 = 1 ([a 2 | ε 3 ] + ) 1
   Segment set Acyclic segments (total of 7 sorted as in Table 3.5)
   ASE(e 1 ) a 2 ε 3 a 2 ε 3 ) 1 ?
   1 (a 2 1 (ε 3 a 2 1 (ε 3 ) 1 ? ) 1 ?
   Ini ASE (e 1 )
   1 (a 2 1 (ε 3 a 2 1 (ε 3 ) 1 ?
   Fol ASE (e 1 , a 2 ) a 2 ε 3 a 2 ε 3 ) 1 ? ) 1 ?
   Fol ASE are listed in Table 3.6. The whole procedure is carried out in Fig.3.29: finite
   transducer A of e 1 (top), two execution traces (overlapped) for phrases ε and a
   (middle left), corresponding (also overlapped) two DAG (middle right), and acyclic
   LST (and MST) obtained (bottom), only one for ε (left) and four for a (right). The
   traces and DAG of strings ε and a overlap as ε is a prefix of a. An LST (at least) is
   eventually found for each problematic phrase ε and a. ?
   Noticehoweverthatphrasea hasfouracyclicLST,whereasphraseεhasonlyoneand
   itappearstobenon-ambiguous:inotherwords,theBSP failstodetecttheambiguity
   of ε. This is a minor flaw of our construction: since infinite ambiguity is cut down
   to finite, certain (somewhat pathological) phrases, though correctly recognized, are
   presented as if they were not ambiguous.

   We hint to one remedy which requires a slight correction of the acyclic segment
   definition.Infiniteambiguityshouldbecutbydiscardingallthesegmentsthatcontain
   three or more (instead of two or more) equally numbered empty string symbols
   ε k without any input symbol in between, i.e., ...ε k ...ε k ...ε k .... In this way,
   phrase ε would be left with two LST, namely 1 (ε 3 ) 1 as before, and 1 (ε 3 ε 3 ) 1 , and
   its ambiguity would be discovered. We omit the simple adjustments of the BSP
   construction.


### 3.9.3.5 An operational condition for r.e.ambiguity

   We return to the problem of deciding r.e. ambiguity, to present a decision method
   that can be easily applied to the BSP transducer graph. We recall that an r.e. e is
   ambiguousifsomephrase x ∈ L (e)canbederivedindifferentwaysthatcorrespond
   to distinct syntax trees, LST or MST. This is equivalent to having two or more LST
   in the set LST(x) (Definition 3.47). We show how to check the latter condition on
   the transducer graph, first for the non-problematic case.

   Definition 3.62 (operational ambiguity condition) An r.e. e is ambiguous if its
   BSP transducer has a state q and an input symbol a, including q = start and a =?,
   such that (i) the output slice ρ (q, a) contains two 3-tuples with an identical third
   component, i.e., ?a h , μ, c? and
   ?
   a k , μ ? , c
   ? , and (ii) the first components a
   h and a k
   of these 3-tuples are in the same class a. ?

<a id="P187"></a>

   a 2
   start
   →
   q 0
   →
   ρ(start, ) =
   ⎧
   ⎨
   ⎩
   ,
   1 ( , a 2
   ,
   1 ( ε 3 , a 2
   ,
   1 ( ε 3
   ) 1 ,
   ⎫
   ⎬
   ⎭
   a
   ρ(q 0 , a) =
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   a 2 , ε, a 2
   a 2 , ε 3 , a 2
   a 2 , ) 1 ,
   a 2 , ε 3 ) 1 ,
   ⎫
   ⎪
   ⎬
   ⎪
   ⎭
   Berry-Sethi parsing
   of a problematic r.e.

   BSP accepting paths for strings ε and a overlapped DAG(ε) and DAG(a)
   a 2 a 2
   start
   →
   q 0
   −→
   accept ε q 0
   −→
   accept a
   ,
   1 ( , a 2
   ,
   1 ( ε 3 , a 2
   ,
   1 ( ε 3 ) 1 ,
   a
   a 2 , ε, a 2
   a 2 , ε 3 , a 2
   a 2 , ) 1 ,
   a 2 , ε 3 ) 1 ,
   ρ(start, ) = ρ 1
   ρ(q 0 , a) = ρ 2
   ε:
   1 ( ε 3 ) 1
   acyclic LST of ε and a
   a:
   1 ( a 2 ) 1 , 1 ( a 2 ε 3 ) 1 , 1 ( ε 3 a 2 ) 1 , 1 ( ε 3 a 2 ε 3 ) 1
   ρ 1 : slice 1 ρ 2 : slice 2
   ,
   1 ( , a 2
   ,
   1 ( ε 3 , a 2
   ,
   1 ( ε 3 ) 1 ,
   a 2 , ε, a 2
   a 2 , ε 3 , a 2
   a 2 , ) 1 ,
   a 2 , ε 3 ) 1 ,
   −−→
   exit
   1 ( ε 3 ) 1
   −−→
   exit
   ) 1
   −→ ε 3 ) 1
   rec. arc
   1 ( a 2
   1 ( a 2
   1 ( ε 3 a 2
   1 ( ε 3 a 2
   MST of ε MST of a
   •
   1 ( +
   ε 3
   ) 1
   •
   1 ( +
   a 2
   ) 1
   •
   1 (
   +
   a 2 ε 3
   ) 1
   •
   1 (
   +
   ε 3 a 2
   ) 1
   •
   1 (
   +
   ε 3 a 2 ε 3
   ) 1
   Fig. 3.29 Procedure for constructing the acyclic LST of the ambiguous strings ε and a for the
   problematic r.e. e 1 of Example3.59. Notice that the ambiguity of ε is not preserved
   Toillustrate,inFig.3.26thearcq 0
   a
   − → q 1 outputsaslicecontainingtwo3-tupleswith
   (i) the same end symbol c = a 4 and (ii) start symbols a h = a k = a 4 . In Fig.3.27,
   string a a b exhibits the corresponding form of ambiguity. The BSP ambiguity
   condition (Definition 3.62) is also satisfied, still in the graph of Fig.3.26, by the arc
   q 2
   a
   − → q 0 that outputs two 3-tuples with (i) the same end symbol c = b 5 and (ii) start
   symbols a h = a 6 and a k = a 9 . For instance, this transition is executed in analyzing
   the ambiguous string a ba b.

   To finish, we briefly discuss the problematic case. The BSP ambiguity condition
   (Definition 3.62) is valid for most problematic r.e. Yet, for certain such r.e. it may
   fail to detect ambiguity. A case is the formerly discussed r.e. e 1 of Example3.59,
   though the failure there is only for the particular phrase ε. In other words, for a
   problematic r.e., within the current definition of acyclic segment (Sect.3.9.3.4), the

<a id="P188"></a>

   BSP ambiguity condition is sufficient, yet not necessary in all cases. However, it
   could be slightly generalized so as to have it characterize ambiguity without any
   exception, as hinted at the end of Sect.3.9.3.4.

   3.10 Expressions with Complement and Intersection
   Before leaving regular languages, we complete the study of the operations that were
   left suspended in Chap.2: complement, intersection and set difference. This will
   allow us to extend regular expressions with such operations, to the purpose of
   writing more concise or expressive language specifications. By using the properties of
   finite automata, now we can state and prove a property (Property3.63) anticipated
   in Chap.1.

   Property 3.63 (closure of REG under complement and intersection) Let L and L ?
   be regular languages. Their complement ¬L and intersection L ∩ L ? are regular
   languages. ?
   Proof First we show how to build the recognizer of the complement language¬L =
   Σ ∗ \ L. We assume that the recognizer M of language L is deterministic, with
   initial state q 0 , state set Q, final state set F and transition function δ. The recognizer
   is constructed by Algorithm 3.64.

   In the first step of Algorithm 3.64, we complete automaton M by adding the error
   orsinkstate p andthearcstoandfromit,inordertomaketotalthetransitionfunction
   δ of M. Then (steps two and three) machine M is complemented and the recognizer
   M of the complement language ¬ L (M) is obtained, i.e., L (M) = ¬ L (M).

   Notice that final and non-final states are interchanged from M to M. To justify
   the construction, first observe that if a computation of automaton M accepts a string
   x ∈ L (M), then the corresponding computation of automaton M terminates in a
   non-final state, so that x / ∈ L (M). Second, if a computation of M does not accept
   a string y, two cases are possible: either the computation ends in a non-final state q
   or it ends in the sink p. In both cases the corresponding computation of M ends in
   a final state, meaning that string y is in the language L (M). Thus we conclude that
   L (M) = ¬ L (M).

   Then, in order to show that the intersection of two regular languages is regular, it
   suffices to quote the well-known De Morgan identity:
   L ∩ L ? = ¬
   ?
   ¬ L ∪ ¬ L ?
   ?
   because, from knowing that the languages ¬ L and ¬ L ? are regular, it follows that
   their union too is regular, as well as its complement. ?

<a id="P189"></a>

   Algorithm 3.64 (deterministic recognizer for complement)
   Input: A deterministic finite-state automaton M
   Output: A deterministic finite-state automaton M complement of M
   1 create a new state p / ∈ Q, the sink, and the state set of M is Q ∪ { p }
   2 the transition function δ of M is:
   δ (q, a) = δ (q, a), where δ (q, a) ∈ Q
   δ (q, a) = p, where δ (q, a) is not defined
   δ (p, a) = p, for every character a ∈ Σ
   3 the final state set of M is F = ( Q \ F ) ∪ { p } ?
   Asacorollary,thesetdifferenceoftworegularlanguages L and L ? isregularbecause
   of the identity:
   L \ L ? = L ∩ ¬ L ?
   Example 3.65 (complement automaton) Figure3.30 shows three machines: the
   (deterministic)givenone M,theintermediateonecompletedwithsinkandthe(deter-
   ministic) recognizer M of the complement language. ?
   Fortheconstructiontowork,theoriginalmachinehastobedeterministic.Otherwise
   the language accepted by the constructed machine may be not disjoint from the
   originalone.Suchafactfactwouldviolatethecharacteristicpropertyofcomplement,
   i.e., L ∩ ¬ L = ∅.Seethefollowingcounterexample,whereapseudo-complement
   machine mistakenly accepts string a, which actually is in the original language:
   nondeterministic
   original automaton
   pseudo-automaton
   of complement language
   q 0 q 1 → →
   a
   a
   q 0 q 1 p →
   ↓
   →
   a a
   a
   a
   Finally we mention that the complement construction may produce unclean or
   non-minimal machines. These however can always be cleaned through Example3.5
   on p. 124 and minimized as explained in Sect.3.4.3.1 on p. 128.


<a id="P190"></a>

   original automaton M to be complemented
   q 0 q 1 q 2 → →
   a
   b
   a
   b
   automaton M completed with sink state complemented recognizer M
   q 0 q 1 q 2
   p
   → →
   b
   a
   a
   b
   a
   b
   a, b
   q 0 q 1 q 2
   p
   →
   ↓ ↓
   ↑
   b
   a
   a
   b
   a
   b
   a, b
   nondeterministic
   original automaton
   pseudo-automaton
   of complement language
   q 0 q 1 → →
   a
   a
   q 0 q 1 p →
   ↓
   →
   a a
   a
   a
   Fig.3.30 Construction of the recognizer M of complement language ¬L (M) (Example3.65)

## 3.10.1 Product of Automata

   Afrequentlyusedtechniqueconsistsinsimulatingtwo(ormore)machinesbyasingle
   one that has as state set the cartesian product of the two state sets. We present this
   technique for the case of the recognizer of the intersection of two regular languages.
   Incidentally, the closure proof of family REG under intersection, based on the
   De Morgan identity (p. 188), already gives a procedure for recognizing intersection:
   given two finite deterministic recognizers, first construct the recognizers of the
   complement languages, then the recognizer of their union (by the Thompson method on
   p.148).Fromthelatter(andafterdeterminizingifneeded),constructthecomplement
   machine, which is the desired result.

   Moredirectly,theintersectionoftworegularlanguagesisacceptedbythecartesian
   product of the given machines M ? and M ?? . We assume the two machines to be free
   from spontaneous moves, yet not necessarily deterministic.

   The product machine M has state set Q ? × Q ?? , i.e., the cartesian product of the
   two state sets. This means that each state is a pair
   ?
   q ? , q ??
   ? , where the left (right)

<a id="P191"></a>

   component is a state of the first (second) machine. For such a pair or product state
   ?
   q ? , q ??
   ? , we construct the outgoing arc:
   ?
   q ? , q ??
   ?
   a
   − →
   ?
   r ? , r ??
   ?
   if,andonlyif,thereexistthearcsq ?
   a
   − → r ? in M ? andq ??
   a
   − → r ?? in M ?? .Inotherwords,
   such an arc exists in M if, and only if, its projection on the left (respectively right)
   component exists in M ? (respectively in M ?? ).

   The initial state set I of the product machine M is the product I = I ? × I ?? of the
   initial state sets of the component machines. The final state set of M is the product
   of the final state sets of the components, i.e., F = F ? × F ?? .

   To justify the construction correctness, consider any string x in the intersection
   language. Since string x is accepted by a computation of machine M ? as well as by
   one of machine M ?? , it is also accepted by the one of machine M that traverses the
   state pairs respectively traversed by the two computations. Conversely, if string x is
   not in the intersection, at least one of the computations by M ? or M ?? does not reach
   a final state. Hence the computation of M does not reach a final state either, because
   the latter are pairs of final states.

   Example 3.66 (intersection and product machine—Sakarovitch [12]) The 
   recognizer M of the strings that contain as substrings both digrams a b and ba is naturally
   specified through the intersection of languages L ? and L ?? :
   L ? = (a | b) ∗ a b (a | b) ∗ L ?? = (a | b) ∗ ba (a | b) ∗
   The cartesian product M of the recognizers M ? and M ?? of the component languages
   isinFig.3.31.Asusual,thestatepairsofthecartesianproductthatarenotaccessible
   from the initial state pair can be discarded. ?
   The cartesian product method can be exploited for operations different from 
   intersection, such as union or exclusive union. In the case of union, it would be easy to
   modify the product machine construction to accept a string if at least one (instead
   of both as in the intersection) component machine accepts it. This would give us an
   alternativeconstructionfortherecognizeroftheunionoftwolanguages.Butthesize
   of the product machine is typically much larger than that of the machine obtained
   through the Thompson method: in the worst case, its number of states is equal to the
   product of the two machine sizes, instead of the sum.


### 3.10.1.1 Extended and Restricted Regular Expression

   A regular expression is extended if it uses other operators beyond the basic ones
   (union concatenation star and cross): complement, intersection and set difference.
   For instance, the strings that contain one or multiple occurrences of the digram a a
   as a substring and do not end with the digram bb are defined by the extended r.e.:
   ?
   (a | b) ∗ a a (a | b) ∗
   ?
   ∩ ¬
   ?
   (a | b) ∗ bb
   ?

<a id="P192"></a>

   component
   machines
   M and M
   1 2 3
   A 1A 2A 3A
   B 2A 2B 2C
   C 3A 3B 3C
   product machine M
   M →
   →
   a b
   a, b a, b
   M
   ↓
   ↓
   b
   a
   a, b
   a, b
   → →
   a
   b
   b
   b
   b
   a a
   a a
   a
   b
   a, b a, b
   a, b a, b
   Fig. 3.31 Recognizers of component languages (top and left) and the product machine (bottom
   right) that recognizes their intersection (Example3.66). State 2B can be removed
   The next realistic example (Example3.67) shows the practicality of using extended
   regular expressions for a greater expressivity.

   Example 3.67 (identifier) Suppose that the valid identifiers, e.g., variable names,
   may contain letters a ... z, digits 0 ... 9 (not in the first position) and one or more
   dashes“_”.Dashesmaynotoccurinthefirstnorinthelastposition,andconsecutive
   dashes are forbidden. A sentence of this language is after_the_2nd_test.

   The next extended regular expression prescribes that (i) every string starts with
   a letter, (ii) it does not contain consecutive dashes, and (iii) it does not end with a
   dash:
   (a ... z ) + (a ... z | 0 ... 9 | _) ∗
   (i)
   ∩
   ¬
   ?
   (a ... z | 0 ... 9 | _) ∗ _ _ (a ... z | 0 ... 9 | _) ∗
   ?
   (ii)
   ∩

<a id="P193"></a>

   ¬
   ?
   (a ... z | 0 ... 9 | _) ∗ _
   ?
   (iii)
   The r.e. above is extended with intersection and complement. ?
   When a language is specified through an extended r.e., of course we can construct
   its recognizer by applying the complement and cartesian product methods. Thus an
   equivalent non-extended r.e. can be obtained, if desired.


### 3.10.1.2 Star-FreeLanguage

   We know that adding complement and intersection operators to an r.e. does not
   enlarge the family of regular languages, because such operators can be eliminated
   and replaced by basic ones. On the other hand, removing star from the permitted
   operators causes a loss of generative capacity. The language family shrinks into a
   subfamilyoffamilyREG,variouslynamedasaperiodicorstar-freeornon-counting.

   Sincesuchafamilyisrarelyconsideredintherealmofcompilation,ashortdiscussion
   suffices.

   Consider the operator set comprising union, concatenation, intersection and
   complement. Starting with the terminal elements of the alphabet and the empty set ∅, 18
   we can write a so-called star-free r.e. by using only these operators. Notice that
   having intersection and complement is essential to compensate somehow for the loss of
   star (and cross), otherwise just finite languages would be definable through star-free
   regular expressions.

   A language is called star-free if there exists a star-free r.e. that defines it. First,
   observe that the universal language over an alphabet Σ is star-free, as it is the
   complement of the empty set ∅; i.e., it is defined by the star-free r.e. ¬∅ = Σ ∗ .
   Second, a useful subclass of star-free languages has already been studied without
   even knowing the term: indeed the local languages on p. 150 can be defined without
   star or cross. Recall that a local language is precisely defined by three local sets:
   initials, finals and permitted (or forbidden) digrams. Its specification can be directly
   mapped onto the intersection of three star-free languages, as follows.

   Example 3.68 (star-free r.e. of local and non-local languages) The sentences of
   the local language (a bc) + (Example3.27 on p. 150) start with a letter a, end with
   a letter c and do not contain any digram from { a a, a c, ba, bb, cb, cc }. The
   language is therefore specified by the star-free r.e.:
   (a ¬∅) ∩ (¬∅c) ∩
   ?
   ¬
   ?
   ¬∅ (a a | a c | ba | bb | cb | cc) ¬∅
   ? ?
   18 Forthestar-freefamily,weallowinanr.e.theemptyset∅asanewmetasymbol.Thisisnecessary,
   e.g., to define the universal language as ¬∅ (complement of ∅), since we may not use the Kleene
   star expression Σ ∗ .


<a id="P194"></a>

   Here is another example. Language L 2 = a ∗ ba ∗ is star-free because it can be
   converted into the equivalent star-free r.e.:
   a ∗
   ¬ (¬∅b¬∅)
   b
   b
   a ∗
   ¬ (¬∅b¬∅)
   On the other hand, this language is not local as its local sets do not suffice for
   excluding spurious strings. Among the strings that start and end with a letter a or b
   and may contain digrams { a a, a b, ba }, as prescribed by language L 2 , unluckily
   we find string, say, a ba b, not belonging to L 2 . ?
   The family of star-free languages is strictly included in the regular language family.
   In particular, it excludes the languages characterized by certain counting properties
   that justify the other name of “non-counting” often given to the family. An example
   is the regular language:
   ?
   x ∈ { a | b } ∗ | |x | a is an even number
   ?
   This language is accepted by a machine that has a circuit of length two in its graph,
   i.e., a modulo-2 counter (or flip-flop) of the letters a encountered. Such a language
   cannot be defined with an r.e without using star or cross.

   An empirical observation is that, in the panorama of artificial and human
   languages, the operation of counting letters or substrings modulo some integer constant
   (in the intuitive sense of the previous example) is rarely needed, if ever. In other
   words, the string classification based on the congruence classes modulo some
   integer is usually uncorrelated with their being valid sentences or not. For reasons that
   may have to do with the organization of the human mind or perhaps with the 
   robustnessofnoisycommunication,noneoftheexistingtechnicallanguagesdiscriminates
   sentences from illegal strings on the basis of modulo counting properties. Indeed,
   it would be strange if a computer program were considered valid depending on the
   number of its instructions being, say, a multiple of three or not!
   Therefore, in principle it would be enough to deal with the subfamily of 
   aperiodic or non-counting regular languages, when modeling compilation and artificial
   languages. Yet, on one side star-free r.e. is often less readable than the basic ones,
   and on the other side in different fields of computer science, counting objects is of
   the uttermost importance. For instance, a most common digital component is the
   flip-flop or modulo-2 counter, which recognizes the language (11) ∗ , obviously not a
   star-free one. 19
   19 For the theory of star-free languages we refer to McNaughton and Papert [18].

<a id="P195"></a>

   3.11 Summary of the Relations Between Regular Expressions,
   Grammars and Automata
   On leaving the topic of regular languages and finite automata, it is convenient to
   recapitulate the relations and transformations between the various formal models
   associated with this language family.

   Figure3.32represents,bymeansofaflowgraph,theconversionmethodsbackand
   forthfromregular expressionsandautomataofdifferenttypes.For instance,weread
   thatalgorithmGMY,explainedonp.159,convertsanr.e.intoanautomatondevoidof
   spontaneous moves. In a similar way, Fig.3.33 represents the direct correspondence
   between right-linear grammars and finite automata. The spontaneous transitions,
   i.e., ε-moves, correspond to the copy rules of grammars; both can be eliminated (as
   seen on p. 142 and p. 67). The relations between left-linear grammars and automata
   are not listed, because they are essentially similar to the right-linear case, thanks
   regular
   expression
   nondet. automaton
   with ε-moves
   nondet. automaton
   without ε-moves
   deterministic
   automaton
   GMY method
   on p. 159
   BS method
   for r.e.

   on p. 159
   Thompson method
   on p. 148
   method of elimination
   of ε-moves on p. 142
   powerset
   method on p. 145
   complement
   method
   on p. 189
   BMC method
   on p. 140
   BS method
   for automaton
   on p. 163
   Fig. 3.32 Conversion methods between r.e. and (nondeterministic and deterministic) finite
   automata, with and without ε-moves
   automaton
   with ε-moves
   automaton
   without ε-moves
   right-linear
   grammar
   right-linear grammar
   without copy rules
   on p. 136 on p. 136
   Fig.3.33 Correspondence between right-linear grammars and finite automata, in general 
   nondeterministic

<a id="P196"></a>

   regular
   expression
   context-free
   grammar
   right-linear
   grammar
   left-linear
   grammar
   syntax-directed translation
   method on p. 82
   linear equation
   system method
   on p. 87
   slightly modified method
   of linear equation system
   Fig.3.34 Correspondence between regular expressions and grammars
   to the left/right duality of grammar rules and the arrow-reversing transformation of
   automaton moves.

   At last, Fig.3.34 lists the relations between regular expressions and grammars.
   The three figures give evidence to the equivalence of the three models used for regular
   languages: regular expressions, unilinear grammars and finite automata. Finally, we
   recall that regular languages are a very restricted subset of the context-free ones,
   which are indispensable for defining artificial languages.

   References
   1. BovetD,CrescenziP(1994)Introductiontothetheoryofcomplexity.Prentice-Hall,Englewood
   Cliffs
   2. Floyd RW, Beigel R (1994) The language of machines: an introduction to computability and
   formal languages. Computer Science Press, New York
   3. Hopcroft J, Ullman J (1979) Introduction to automata theory, languages, and computation.
   Addison-Wesley, Massachusetts
   4. Kozen D (2007) Theory of computation. Springer, London
   5. McNaughton R (1982) Elementary computability, formal languages and automata. 
   PrenticeHall, Englewood Cliffs
   6. Rich E (2008) Automata, computability, and complexity: theory and applications. Pearson
   Education, New York
   7. Salomaa A (1973) Formal languages. Academic Press, New York
   8. Hopcroft J, Ullman J (1969) Formal languages and their relation to automata. Addison-Wesley,
   Massachusetts
   9. Harrison M (1978) Introduction to formal language theory. Addison Wesley, Massachusetts
   10. Shallit J (2008) A second course in formal languages and automata theory, 1st edn. Cambridge
   University Press, New York
   11. Rozenberg G, Salomaa A (eds) (1997) Handbook of formal languages, vol. 1: word, language,
   grammar. Springer, New York
   References 197
   12. Sakarovitch J (2009) Elements of automata theory. Cambridge University Press, Cambridge
   13. Watson B (1994) A taxonomy of finite automata minimization algorithms, Report. Department
   ofMathematicsandComputerScience,Eindhoven;EindhovenUniversityofTechnology,Eind-
   hoven, The Netherlands
   14. Thompson K (1968) Regular expression search algorithm. Commun ACM 11(6):419–422
   15. Berry G, Sethi R (1986) From regular expressions to deterministic automata. Theor Comput
   Sci 48(1):117–126
   16. Berstel J, Pin JE (1996) Local languages and the Berry-Sethi algorithm. Theor Comput Sci
   155(2):439–446
   17. Frisch A, Cardelli L (2004) Greedy regular expression matching. In: Díaz J, Karhumäki J,
   Lepistö A, Sannella D (eds) ICALP. Springer, Berlin, pp 618–629
   18. McNaughton R, Papert S (1971) Counter-free automata. The MIT Press, Cambridge

<a id="P199"></a>

Chapter 4 Pushdown Automata and Parsing
=======================================

4.1 Introduction
================

   The algorithms for recognizing whether a string is a legal sentence require more
   memory and time resources for context-free languages than for the regular ones.
   This chapter presents several algorithms, first as abstract automata with a pushdown
   memory stack, then as parsing 1 (or syntax analysis) procedures that produce the
   syntax tree of a sentence. We recall that for regular languages defined by unilinear
   grammars, parsing has little interest because their syntax structure is predeterminate
   (left or right-linear), and for languages defined by ambiguous regular expressions
   we have presented a parsing algorithm in Chap. 3.

   Similarlytounilineargrammars,itispossibletomatchalsotherulesofacontext-
   free grammar to the moves of an automaton which is not a pure finite-state machine
   since it also has a pushdown stack memory. In contrast to the finite-state case, such
   a pushdown machine can be made deterministic only for a subfamily of context-free
   languages known as deterministic, briefly DET. Moreover, the presence of two
   different memory devices, i.e., finite state and stack, introduces a variety of functioning
   modes and makes the theoretical study of such machines more involved.

   The chapter starts with the essentials of pushdown automata and compares the
   general and deterministic cases interms of expressivity and of closure under language
   operations. Two simplified types of deterministic machine are examined: the simple
   deterministic model and the input-driven one.

   The chapter continues with several useful parsing algorithms, which altogether
   meettherequirementsofcompilerwriting.Itistraditionaltoclassifysuchalgorithms
   ontwodimensions:deterministicversusnondeterministic,andpushdown-basedver-
   sus tabular. We carefully develop the deterministic pushdown-based methods, which
   1 From Latin pars, partis, in the sense of dividing a sentence into its parts or constituents.
   © Springer Nature Switzerland AG 2019
   S. Crespi Reghizzi et al., Formal Languages and Compilation,
   Texts in Computer Science, https://doi.org/10.1007/978-3-030-04879-2_4

<a id="P200"></a>

   are further divided into the classes of bottom-up (or shift-reduce) and top-down (or
   predictive) parsers, depending on the construction order of the syntax tree. The
   leading bottom-up LR(k) method is first presented, then the top-down method based on
   the LL(k) grammars, and lastly the bottom-up operator-precedence method, which
   is motivated by its suitability for parallel execution.

   We show how to adjust a grammar to deterministic parsing and how to make a
   deterministic parser more selective by using a longer prospection. Subsequently we
   returnto theory and we compare the language families that correspond to the various
   parsers considered.

   Then we address the relevant problem of how to improve parsing performance
   on multiprocessor (multi-core) computers. To enable a parallel execution of parsing
   steps, the parser must be able to make decisions by examining a limited part of the
   sourcetext,sothatseveralworkerscanindependentlyanalyzedifferenttextparts.We
   present the parallel parsing algorithm PAPAGENO based on the efficient sequential
   operator-precedence parser (for unextended BNF), and we analyze its complexity.
   A discussion on the deterministic parsing methods ends this part.

   All the deterministic methods impose specific restrictions on the form of grammar
   rules. The last method considered, called Earley parser, is fully general and is able
   to cope with ambiguous and nondeterministic grammars, but it pays the price of
   nonlinear time complexity. It uses tables instead of a stack.

   The chapter ends with a brief discussion on syntactic errors, error diagnosis and
   recovery in parsing, and incremental parsing.

   It is important to observe that our presentation of classical parsing methods is novel
   and much improved over orthodox ones. Such methods, i.e., LR(k), LL(k) and
   Earley, have a long individual history behind them, which has left much tedious and
   irrelevant diversity that we sweep away in our unified presentation. Our approach
   is based on representing the grammar as a network of finite automata. Automata
   networks are a natural representation for grammars, especially when regular 
   expressions occur in the rule right parts, i.e., for the extended context-free form or EBNF
   (p.100)frequentlyusedinthelanguagereferencemanuals.Accordingly,thesequen-
   tial parsers we present take such extended context-free grammars. Starting from the
   bottom-updeterministicmethods,weareabletoincrementallydevelopthetop-down
   and the general parsing algorithms, respectively, as a specialization and a 
   generalization of the bottom-up method.

4.2 Pushdown Automaton
======================

   Everycompilerincludesarecognitionalgorithm,whichisessentiallyafiniteautoma-
   ton enriched with an auxiliary memory organized as a pushdown or LIFO stack of
   unbounded capacity. The stack stores the symbols A 1 ... A k :
   Z 0 | A 1 A 2 ...

   top
   A k
   LIFO stack (pushdown tape)
   a 1 a 2 ...

   current
   a i ... a n ?
   input or source string (input tape)
   k, n ≥ 0

<a id="P201"></a>

   The input or source string a 1 ... a n is often delimited on the right by an end-marker
   ?. The following three operations apply to a stack:
   pushing operation push(B) inserts symbol B on top, i.e., onto the right
   of symbol A k ; several push operations push(B 1 ), push(B 2 ), ...,
   push(B n ) can be combined in one command push(B 1 B 2 ... B m )
   that inserts B 1 B 2 ... B m , with B m on top (m ≥ 1)
   popping operation pop, if the stack is not empty, removes the top symbol A k
   (k ≥ 1)
   emptiness test predicate empty is true if, and only if, k = 0 (no symbols in the
   stack)
   Sometimes it is convenient to imagine that a special symbol is painted on the stack
   bottom, i.e., on the left of symbol A 1 . Such a symbol is denoted Z 0 and is termed
   bottom: it can be read, that is, its presence on the stack top can be checked, yet it
   may not be pushed or popped. The presence of Z 0 on the stack top means that the
   stack is empty.

   Themachinereadsthesourcecharactersa 1 ... a n fromlefttorightbymeansofa
   readinghead.Thecharactera i underthereadingheadistermedcurrent (1 ≤ i ≤ n).
   At each instant the machine configuration is specified by the remaining portion of
   the input string left to be read, the current state, and the stack contents. With a move
   an automaton can:
   • read the current character, and either shift the reading head or perform a move
   (called spontaneous) without reading
   • read and pop the top symbol, or read the bottom Z 0 if the stack is empty
   • compute the next state from the current values of the state, character and top-of-
   stack symbol
   • push one or more symbols (or even none) onto the stack

## 4.2.1 From Grammar to Pushdown Automaton

   Weshowhowthegrammarrulescanbeinterpretedasinstructionsofanondetermin-
   istic pushdown machine that recognizes the language. The machine is so simple as
   to work without any internal state, and it uses only the stack for memory. Intuitively,
   themachineoperation ispredictiveorgoal-oriented: thestackserves asanagendaof
   predicted future actions. The stack stores nonterminal and terminal grammar
   symbols. If the stack contains symbols A 1 ... A k (from bottom to top), the machine first
   executes the action prescribed by A k , then the action for A k−1 , and so on until the
   last action for A 1 . The action for A k has to recognize if, starting from the current
   character a i , the source string contains a substring w that derives from A k . If so, the
   action will eventually shift the reading head of |w| positions. Naturally enough, the
   goal may recursively spawn subgoals if, for recognizing the derivation from A k , it is

<a id="P202"></a>

   Table 4.1 Correspondence between grammar rules and moves of a nondeterministic pushdown
   machine with one state. Symbols cc and top represent the current input character and the stack top
   symbol, respectively
      # Grammar rule Automaton move Comment
      1 A → B A 1 ... A m
      with m ≥ 0
      If top = A then pop; push
      (A m ... A 1 B)
      To recognize A, orderly recognize
      B A 1 ... A m
      2 A → b A 1 ... A m
      with m ≥ 0
      If cc = b and top = A then
      pop; push (A m ... A 1 ); shift
      reading head
      Character b is expected as next and
      is read, so it remains to orderly
      recognize A 1 ... A m
   3 A → ε If top = A then pop The empty string deriving from A
   is recognized
   4 For any character
   b ∈ Σ
   if cc = b and top = b then
   pop; shift reading head
   Character b is expected as next and
   is read
   5 Acceptance
   condition
   If cc =? and the stack is
   empty then accept; halt
   The input string has been entirely
   scanned, and the agenda has no
   goals
   necessary to recognize other nonterminal symbols. The initial goal is the grammar
   axiom: the machine task is to recognize if the source string derives from the axiom.
   Algorithm 4.1 (from grammar to nondeterministic one-state pushdown machine)
   Given a grammar G = (V, Σ, P, S ), Table 4.1 explicates the correspondence
   between rules and moves. Letter b denotes a terminal, letters A and B denote
   nonterminals, and letter A i can be any symbol. The rule form shapes the move. For rules
   of form 2, the right-hand side starts with a terminal and the move is triggered on
   reading it. Instead, rules of form 1 and 3 give rise to spontaneous moves that do not
   check the current character. Move 4 checks that a terminal surfacing on the stack
   top (having been previously pushed by a move of type 1 or 2) matches the current
   character. Lastly, move 5 accepts the string if the stack is empty upon reading the
   end-marker.

   Initially the stack contains only the bottom symbol Z 0 and the grammar axiom
   S, and the reading head is on the first input character. At each step, the automaton
   choosesamove(notdeterministically),whichisdefinedinthecurrentconfiguration,
   and executes it. The machine recognizes the string if there exists a computation that
   endswithmove5.Accordingly,wesaythatthismachinemodelrecognizesasentence
   by empty stack. ?
   Surprisingly enough, this automaton never changes state and the stack is the only
   memory it has. Later on, we will be obliged to introduce states in order to make
   deterministic the machine behavior.


<a id="P203"></a>

   Table 4.2 Pushdown machine moves for the grammar rules of Example4.2
      # Grammar rule Automaton move
      1 S → a S If cc = a and top = S then pop; push (S); shift
      2 S → A If top = S then pop; push (A)
      3 A → a Ab If cc = a and top = A then pop; push (b A); shift
      4 A → a b If cc = a and top = A then pop; push (b); shift
      5 If cc = b and top = b then pop; shift
      6 If cc =? and the stack is empty then accept; halt
      Example 4.2 (from a grammar to a pushdown automaton) The moves of the 
      recognizer of the language L below:
      L =
      ?
      a n b m | n ≥ m ≥ 1
      ?
   are listed in Table4.2 next to the grammar rules. The choice between moves 1 and
   2 is not deterministic, since move 2 may be taken also when character a is current;
   similarly for choosing between moves 3 and 4. ?
   It is easy to see that this machine accepts a string if, and only if, the grammar
   generates it. In fact, for each accepting computation there exists a corresponding
   derivation and conversely; in other words, the automaton simulates the leftmost
   derivations of the grammar. For instance, the derivation:
   S ⇒ A ⇒ a Ab ⇒ a a bb
   mirrors the successful trace in Fig.4.1 (left). Yet Algorithm4.1 does not have any
   apriori information about which of the possible derivations will succeed, if any at
   all, and has to explore all the possibilities, including the computations that end in
   error as the one traced (right). Moreover, the source string is accepted by different
   computations if, and only if, it is generated by different left derivations, i.e., if it is
   ambiguous for the grammar.

   With some thought, we may see that the mapping of Table4.1 on p. 202 is one to
   one,thusitcanbeappliedtheotherwayround,totransformthemovesofapushdown
   machine (of the model considered) into the rules of an equivalent grammar. This
   remark allows us to state an important theoretical fact, which links grammars and
   pushdown automata.

   Property 4.3 (CF languages and one-state pushdown automata) The family of
   context-free languages coincides with the family of the languages accepted by a

<a id="P204"></a>

      stack input x
      Z 0 |S aabb
      ···|A aabb
      ···|bA abb
      ···|bb bb
      ···|b b
      Z 0 |
      stack input x
      Z 0 |S aabb
      ···|S abb
      ···|S bb
      Z 0 |A bb
      error – cannot move
   Fig.4.1 Two computations: accepting (left) and rejecting by error (right)
   nondeterministic pushdown machine that recognizes by empty stack and has only
   one state. ?
   We stress that the mapping above from pushdown automaton to grammar does not
   work when the automaton has two or more states; other methods will be developed
   for that case.

   It may appear that in little space and without effort we have already reached
   the objective of the chapter: to obtain a procedure for building the recognizer of a
   language defined by a grammar. Unfortunately, the automaton is nondeterministic
   and in the worst case it is forced to explore all the computation paths, with a time
   complexity non-polynomial with respect to the source string length; more efficient
   algorithms are wanted for practical application.


### 4.2.1.1 ComputationalComplexity of Pushdown Automata

   We compute a worst-case upper bound to the number of steps that are needed to
   recognize a string with the previous pushdown machine. For simplicity, we consider
   a grammar G in the Greibach normal form (p. 78), which features rules starting with
   a terminal and not containing other terminals. Therefore, the machine constructed
   by Algorithm 4.1 is free from spontaneous moves (types 1 and 3 of Table4.1 on p.
   202) and it never pushes a terminal character onto the stack.

   Forastring x oflengthn,thederivation S
   +
   = ⇒ x hasexactlyn steps,ifitexists.The
   same number of moves is performed by the automaton to recognize string x. Let K
   be the maximum of the number of alternative rules A → α 1 | α 2 | ... | α k , for
   any nonterminal A. At each step of a leftmost derivation, the leftmost nonterminal,
   say A, is rewritten by choosing one out of k ≤ K alternatives. It follows that the
   number of possible derivations of length n is at most K n . Since in the worst case the
   algorithm is forced to compute all the derivations before finding the accepting one
   or declaring failure, the time complexity for recognition is exponential in n.

<a id="P205"></a>

   However, this result is overly pessimistic. At the end of this chapter, a clever
   algorithm for string recognition in polynomial time will be described, which uses a
   linked list data structure instead of a LIFO stack.


## 4.2.2 Definition of Pushdown Automaton

   We are going to define several pushdown machine models. In order to expedite
   the presentation, we trust the reader to adapt to the present context the analogous
   concepts already seen for finite automata. A pushdown automaton M is defined by
   seven items:
   Q a (non-empty) finite set of states of the control unit
   Σ an input alphabet
   Γ a stack (or memory) alphabet
   δ a transition function
   q 0 ∈ Q an initial state
   Z 0 ∈ Γ an initial stack symbol
   F ⊆ Q a set of final states
   Ingeneralsuchamachineisnondeterministic.Thedomainandrangeofthetransition
   function δ are expressed by Cartesian products:
   domain Q × (Σ ∪ { ε }) × Γ
   range ℘ ( Q × Γ
   ∗ ), i.e., the powerset of the set Q × Γ ∗
   The moves, i.e., the values of function δ, are classified as reading and spontaneous,
   as follows:
   reading move
   δ (q, a, A) = { (p 1 , γ 1 ), (p 2 , γ 2 ), ..., (p n , γ n ) }
   with q, a, A resp. in Q, Σ, Γ , and p i , γ i resp. in Q, Γ
   ∗
   for 1 ≤ i ≤ n.

   The machine, in state q with A on stack top, reads character a and enters one of
   the states p i after performing these operations: pop and push(γ i ).

   spontaneous move
   δ (q, ε, A) = { (p 1 , γ 1 ), (p 2 , γ 2 ), ..., (p n , γ n ) }
   with the same stipulations as before.

   Themachine,instateq with Aonstacktopandwithoutreadinganinputcharacter,
   enters one of the states p i after performing these operations: pop and push(γ i ).

<a id="P206"></a>

   Notes: The choice of the i-th action out of n possibilities is not deterministic; the
   readingheadautomaticallyshiftsforwardoninput;thetopsymbolisalwayspopped;
   andthestringpushedontothestackmaybeempty.Althoughanymoveperformsapop
   that erases the top symbol, the same symbol can be pushed anew by the same move,
   ifthecomputationneedstokeepitonstack.Thus,themoveδ (q, a, A) = {(p,A)}
   checks the presence of A on stack top and lets the stack as it is.

   From the definition, it is clear that the machine behavior can be nondeterministic
   for two causes, to be carefully studied later: (i) the range of the transition function
   comprises a set of alternative actions and (ii) the machine may have the alternative
   between a reading and a spontaneous move.

   Wesaythatapushdownautomatonfreefromspontaneousmoveshasthereal-time
   property.

   Short Notation
   Certain patterns of moves which frequently occur can be conveniently shortened with
   the following abbreviations. If, for all stack symbols in A ∈ Γ (including the initial
   stack symbol Z 0 ), the move δ (q, a, A) = {(p, Aγ)} is defined, we may represent
   all such moves by the new notation:
   δ (q, a, ε) = { (p, γ) }
   and in particular by δ (q, a, ε) = { (p, ε) } if γ is the empty string. In the latter
   case, the only effect of the move is to change state, and the move can be simply
   represented by an arc q
   a
   − → p as in a finite automaton.

   Thus, having the empty string as third argument of δ indicates that the move
   is performed whatever the top symbol, which is left untouched. The meaning of a
   spontaneous move of the form δ (q, ε, ε) = { (p, γ) } is similar.

   The instantaneous configuration of a machine M is a 3-tuple:
   (q, y, η ) ∈ Q × Σ ∗ × Γ
   +
   which specifies:
   q the current state
   y the rest (suffix) of the source string x, to be read
   η the stack content
   The initial configuration of machine M is (q 0 , x, Z 0 ) or (q 0 , x ?, Z 0 ), if the
   end-marker is there.

   By applying a move, a transition from a configuration to another occurs, to be
   denoted as (q, y, η ) ?→ ( p, z, λ). A computation is a chain of zero or more
   transitions, denoted by
   ∗
   ?− →. As customary, a cross instead of a star, i.e.,
   +
   ?− →, denotes a
   computation with at least one transition.


<a id="P207"></a>

   Depending on the move, the following transitions are possible:
   current conf. next conf. applied move move type
   ?
   q, a z, η A
   ?
   ( p, z, η γ ) δ (q, a, A) = { (p, γ), ... } reading
   ?
   q, a z, η A
   ?
   ( p, a z, η γ ) δ (q, ε, A) = { (p, γ), ... } spontaneous
   A string x is recognized (or accepted) by final state if there exists a computation
   that entirely reads the string and terminates in a final state:
   (q 0 , x, Z 0 )
   ∗
   ?− → (q, ε, λ) q is a final state and λ ∈ Γ
   ∗
   The language recognized by machine M is the set of all accepted strings.

   Noticethatwhenthemachinerecognizesandhalts,thestackcontainssomestring
   λ not further specified, since the recognition modality is by final state; in particular,
   string λ is not necessarily empty.


### 4.2.2.1 State-Transition Diagram for Pushdown Automata

   The transition function of a pushdown automaton can be visualized as a 
   statetransitiongraph,althoughitsreadabilityissomewhatlessenedbytheneedtospecify
   stack operations. This is shown in the next example.

   Example 4.4 (language and automaton of palindromes) The language L =
   ?
   u u R | u ∈ { a, b } ∗
   ?
   of the even-length palindromes (p. 32) is accepted by final
   state by the pushdown recognizer below:
   q 0 q 1
   q 2
   ↓
   ↑
   ,Z 0
   Z 0
   ,Z 0
   Z 0
   a,A
   ε
   b,B
   ε
   a,ε
   A
   b,ε
   B
   a,A
   ε
   b,B
   ε
   The stack alphabet has three symbols: the bottom Z 0 , and the symbols A and B
   indicating that a character a or b was read, respectively. For instance, arc q 0
   a,ε
   A
   −−→ q 0
   denotes any reading move (q 0 , X A) ∈ δ (q 0 , a, X), with X ∈ Γ .

   The machine behaves nondeterministically in the state q 0 on reading an input
   character a with symbol A on stack top, as it may stay in q 0 and push A, or it may
   go to state q 1 , pop A and push nothing; similarly with b and B.


<a id="P208"></a>

   stack input x state
   Z 0 | aa q 0
   ···|A a q 0
   ···|AA q 0
   failure: no move is defined,
   i.e., δ (q 0 , , A) = ∅
   stack input x state
   Z 0 | aa q 0
   ···|A a q 0
   Z 0 | q 1
   Z 0 | finished q 2
   recognition by final state
   Fig.4.2 Two computations of the automaton of Example4.4 for the input string x = a a
   In Fig.4.2 we trace two computations, among others possible, for string x = a a.
   Since the computation (right) entirely reads string a a and reaches a final state, the
   string is accepted. Another example is the empty string ε, recognized by the move
   corresponding to the arc from q 0 to q 2 . ?

### 4.2.2.2 Varieties of Pushdown Automata

   It is worth noticing that the pushdown machine of the definition differs from the
   one derived from a grammar through the mapping of Algorithm 4.1 on p. 202 in the
   following two aspects: to recognize a string, it performs state transitions and checks
   if the current state is final. These and other differences are discussed next.
   Accepting Modes
   Two different ways of deciding acceptance when a computation ends have been
   encountered so far: when the machine enters a final state or when the stack is empty.
   The former mode by final state disregards the stack content, whereas the latter by
   empty stack disregards the current machine state.

   The two modes can also be combined into recognition by final state and empty
   stack. A natural question to ask is whether these and other acceptance modes are
   equivalent.

   Property 4.5 (acceptance modes) For the family of nondeterministic pushdown
   automata, the three acceptance modes below:
   • by empty stack
   • by final state
   • combined (empty stack and final state)
   have the same capacity with respect to language recognition. ?

<a id="P209"></a>

   The statement says that any of the three acceptance modes can be simulated by any
   other. In fact, if the automaton recognizes by final state, it can be easily modified by
   adding new states so that it recognizes by empty stack. Simply, when the original
   machine enters a final state, the second machine enters a new state and it stays there
   as it empties the stack by spontaneous moves until the bottom symbol pops up.
   Vice versa, to convert an automaton recognizing by empty stack to the final state
   mode, do the following:
   • whenever the stack becomes empty, add a new final state f and a move leading to
   it
   • on performing a move to state q, when the stack of the original machine ceases to
   be empty, let the second machine move from state f to state q
   Similar considerations could be made for the third acceptance mode.


## 4.2.3 One Family for Context-Free Languages and Pushdown

   Automata
   We are going to show that the language accepted by a pushdown machine that
   uses also the states is context-free. Combined with Property4.3 (p. 203), that every
   context-free language can be recognized by a pushdown machine, this leads to the
   next central property of context-free languages, analogous to the characterization of
   the regular languages by finite automata.

   Property 4.6 (CFandpushdownautomata) ThefamilyCFofcontext-freelanguages
   coincides with the family of the languages recognized by pushdown automata. ?
   Proof Let L = L (M) be the language recognized by a pushdown machine M. To
   simplifytheconstruction,presentedasAlgorithm4.7below,weassumethatmachine
   M has only one final state, it accepts only if the stack is empty, and each transition
   has either form:
   q i q j
   a,A
   B C
   or
   q i q j
   a,A
   ε
   where a is a terminal or the empty string, and A, B and C are stack symbols. Thus
   a move pushes either two symbols or none. The initial stack symbol and state are Z
   and q 0 , respectively. Symbol Z plays similarly to the stack bottom Z 0 , except that
   it may be popped at the end. In this way the machine accepts by empty stack and
   final state. It turns out that the assumptions above do not reduce the generality of the
   machine. ?

<a id="P210"></a>

   Algorithm 4.7 (from PDA to grammar) We construct a grammar G equivalent to
   machine M. The construction may produce useless nonterminals and rules that later
   can be removed by cleaning. In the resulting grammar, the axiom is S and all the
   other nonterminal symbols are formed by a 3-tuple containing two states q i , q j and
   a stack symbol A of M, written as
   ?
   q i , A, q j
   ? .

   Grammar rules are constructed in such a way that each computation represents a
   leftmost derivation. The old construction for stateless automata (Algorithm4.1 on p.
   202)createsanonterminalforeachstacksymbol.Yetnowwehavetotakecareofthe
   statesaswell.Tothisend,eachstacksymbol A isassociatedwithmultiplenontermi-
   nals marked with two states that have the following meaning. String z derives from
   nonterminal
   ?
   q i , A, q j
   ?
   if, and only if, the automaton starting in the state q i with
   symbol A on the stack top performs a computation that reads string z, enters state
   q j and deletes symbol A from the stack. According to this principle, the grammar
   rules that rewrite the axiom have the form:
   S →
   ?
   q 0 , Z, q f
   ?
   where Z is the initial stack symbol, and q 0 and q f are the initial and final states,
   respectively. The other grammar rules are obtained as next specified:
   1. Moves of the form
   q i q j
   a,A
   ε
   where the two states may coincide and a
   may be empty, are converted into rule
   ?
   q i , A, q j
   ?
   → a.

   2. Moves of the form
   q i q j
   a,A
   B C
   where the two states may coincide, are
   converted into the set of rules:
   ?
   ?q i , A, q x ? → a
   ?
   q j , C, q y
   ? ?
   q y , B, q x
   ?
   | ∀ states q x and q y of M
   ?
   We omit the correctness proof
   2
   of the construction, and we go to an example.

   Example 4.8 (grammar equivalent to pushdown machine) The language L:
   L =
   ?
   a n b m | n > m ≥ 1
   ?
   is accepted by the pushdown automaton M in Fig.4.3 (which does not use the short
   notationintroducedonp.206).Atthebeginning,thestackcontentis Z.Theautoma-
   ton upon reading a character a stores it as a symbol A on the stack. Then, it pops a
   stack symbol for each character b. At the end it checks that at least one symbol A is
   left and empties the stack, including the initial symbol Z.

   2 See for instance [1–4].


<a id="P211"></a>

   pushdown automaton
   M
   q 0
   q 1
   q 2
   q 3
   ←
   ←
   b,A
   ε
   ε, A
   ε
   ε, Z
   ε
   a,Z
   AZ
   a,A
   AA
   b,A
   ε
   ε, A
   ε
   grammar G
   q 0 , Z, q 3 a q 0 , A, q 2 q 2 , Z, q 3
   q 0 , A, q 1 a q 0 , A, q 1 q 1 , A, q 1
   q 0 , A, q 1 b
   q 0 , A, q 2 a q 0 , A, q 1 q 1 , A, q 2
   q 0 , A, q 2 a q 0 , A, q 2 q 2 , A, q 2
   q 0 , A, q 2 ε
   q 2 , Z, q 3 ε
   q 1 , A, q 1 b
   q 1 , A, q 2 ε
   Fig. 4.3 Pushdown automaton M (left) and equivalent grammar G (right) for Example4.8. The
   axiom is ?q 0 , Z, q 3 ?
   The grammar rules are listed next to the automaton: notice that the axiom is
   ?q 0 , Z, q 3 ?. We do not list the useless rules created by step 2 of the construction
   above, such as the rule:
   ?q 0 , A, q 1 ? → a ?q 0 , A, q 3 ? ?q 3 , A, q 1 ?
   that contains the undefined nonterminal ?q 0 , A, q 3 ?. To understand the mapping
   between the two models, it helps to compare the following computation and the
   leftmost derivation, both of five steps:
   (q 0 , a a b, Z )
   5
   ?− → (q 3 , ε, ε) ?q 0 , Z, q 3 ?
   5
   = ⇒ a a b
   The computation and corresponding derivation tree with numbered steps are in
   Fig.4.4. The next properties hold at each step:
   • the string prefix read by the machine and the terminal prefix generated by the
   derivation are identical
   • the stack content is the reversal of the string obtained by concatenating the middle
   symbols (A and Z) of each 3-tuple in the derived string
   • any two consecutive 3-tuples in the derived string are chained together by the
   identity of the states, which are marked by equal arrows as below:
   ?
   q 0 , Z, q 3
   ?
   +
   = ⇒ a a
   ?
   q 0 , A,
   ↓
   q 1
   ?? ↓
   q 1 , A,
   ⇓
   q 2
   ?? ⇓
   q 2 , Z, q 3
   ?
   Such a stepwise correspondence between transition and derivation ensures that the
   two models define the same language. ?

<a id="P212"></a>

   computation of machine M
   (q 0 , aab, Z ) → (q 0 , ab, Z A)
   → (q 0 , b, Z AA)
   → (q 1 , ε, Z A)
   → (q 2 , ε, Z )
   → (q 3 , ε, ε)
   q 0 ,Z,q 3
   a q 0 ,A,q 2
   a q 0 ,A,q 1
   b
   q 1 ,A,q 2
   ε
   q 2 ,Z,q 3
   ε
   1
   2
   3 4
   5
   Fig.4.4 A computation of machine M (left) and the corresponding tree for Example4.8 (right)

### 4.2.3.1 Real-Time Pushdown Automata

   An automaton works in real time if at each step it reads an input character, i.e., if
   it does not involve any spontaneous moves. This definition applies to finite-state
   machines and pushdown machines, both deterministic and nondeterministic. In
   particular, a pushdown automaton has the real-time property if the transition function
   δ, as defined in Sect. 4.2.2 on p. 205, is such that for all the states q ∈ Q and for all
   the stack symbols A ∈ Γ , the value of δ (q, ε, A) is undefined; in other words, the
   domain of the transition function is Q × Σ × Γ instead of Q × (Σ ∪ { ε }) × Γ .
   For finite automata, we already know that real-time automata are as powerful as
   those that have spontaneous moves, since the latter can be always eliminated. Next
   we show that a similar property holds for nondeterministic pushdown machines.
   Property 4.9 (real-time PDA) For every context-free language there exists a
   nondeterministic pushdown machine that has the real-time property and recognizes the
   language. ?
   ToproveProperty4.9,consideralanguage L thatwemayassume,withoutanylossof
   generality,tobedefinedbyagrammar G inthereal-timenormalform(p.78).Recall
   that in this form each rule starts with a terminal, thus it has the form 2 of Table4.1 on
   p.202: A → b A 1 ... A m ,withm ≥ 0andb ∈ Σ.Thereforeemptyrules A → εare
   excluded (we disregard the axiomatic rule S → ε needed if the empty string is in L).
   It follows that the automaton, constructed from such a grammar by Algorithm4.1,
   shifts the input head with every move, i.e., it has the real-time property.
   Moreover,foranyinputstring x,eachcomputationstopsassoonastheautomaton
   readsthelastcharacter,afterexecutingexactly|x |steps(ofcoursethemachinemay
   stop at an earlier time and reject the string if the transition function is undefined).
   Such a behavior of an abstract machine is also known as online computation.

<a id="P213"></a>

   Example 4.10 (real-time pushdown automaton) The language L below:
   L =
   ?
   a + b n c n | m, n ≥ 1
   ?
   ∪
   ?
   a m b + d m | m, n ≥ 1
   ?
   isdefinedbythefollowinggrammar,whereeachrulestartswithaterminalcharacter:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → a X
   X → a X | bY c | bc
   Y → bY c | bc
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → a W d
   W → a W d | b Z | b
   Z → b Z | b
   For brevity, instead of listing the moves of the real-time pushdown automaton M
   that would be built by Algorithm4.1, we describe in words how a more abstract
   equivalent real-time machine works. We prefer to write language L as:
   L =
   ?
   a m b n c n | m, n ≥ 1
   ?
   ∪
   ?
   a m b n d m | m, n ≥ 1
   ?
   The automaton starts reading string a m and, nondeterministically, either (i) it does
   not push anything onto the stack or (ii) it pushes m symbols, say, string A m . Then,
   in the case (i), machine M reads string b n and pushes, say, string B n onto the stack,
   subsequently it reads string c m and removes one symbol B for each letter c, and
   finally it recognizes by empty stack. Similarly, in the case (ii), machine M reads
   string b n without storing anything onto stack, then it reads string d m and removes
   one symbol A for each letter d, and finally it recognizes by empty stack. Clearly,
   automaton M has the real-time property and is nondeterministic. ?
   The question whether it is possible to obtain an equivalent deterministic real-time
   machine is postponed to Sect. 4.3.4.


## 4.2.4 Intersection of Regular and Context-Free Languages

   As an illustration of previous results, we prove the useful property, stated in Chap.2
   in Table2.9 on p. 95 that the intersection of a context-free and a regular language
   is context-free. Take a grammar G and a finite automaton A. We explain how
   to construct a pushdown automaton M that recognizes the intersection language
   L (G) ∩ L (A).

   First, starting from grammar G we construct by Algorithm4.1 on p. 202, a
   onestate pushdown automaton N that recognizes language L (G) by empty stack. Then
   weconstructaCartesianproductmachine M,tosimulatetherespectivecomputations
   of the component machines N and A. The construction of M is essentially the same
   explained for two finite machines (p. 190), with the difference of the presence of a

<a id="P214"></a>

   q r
   s q s r s
   A →
   →
   a
   a
   a
   N →
   a,ε
   A
   a ,A
   ε
   M →
   →
   a ,A
   ε
   a,ε
   A
   a ,A
   ε
   Fig.4.5 ProductmachinefortheintersectioninExample4.11oftheDycklanguageandtheregular
   language a ∗ a ?+ over alphabet
   ?
   a, a ?
   ?
   stack. The state set of M is the Cartesian product of the state sets of machines N
   and A, which in this case has the same cardinality as the latter state set. Machine M
   performs the same operations on the stack as machine N does. For M, recognition
   is by final state and empty stack. The final states of M are those that contain a final
   state of the finite machine A. Notice that the product machine is deterministic if both
   component machines are so.

   It is easy to see that a computation of M empties the stack and enters a final state,
   i.e., it recognizes a string, if, and only if, the string is accepted by empty stack by
   N and is accepted also by A, which reaches a final state. It follows that machine M
   accepts the intersection of the two languages.

   Example 4.11 (intersection of a context-free and a regular language) We want
   to intersect the Dyck language and the regular language a ∗ a ?+ , both over alphabet
   ?
   a, a ?
   ?
   (as in Example2.87 on p. 96), thus resulting in the language
   ?
   a n a ?n | n ≥ 1
   ? .

   It is straightforward to imagine a one-state pushdown machine N that accepts the
   Dyck language by empty stack. Machine N, the finite automaton A and the resulting
   product machine M are depicted in Fig.4.5. Clearly the resulting machine simulates
   both component machines step by step. For instance, the arc from { q, s } to { r, s }
   that reads a ? manipulates the stack exactly as automaton N does (it pops symbol A)
   and simulates the same state transition of automaton A from state q to state r. ?
   In general, the pushdown automaton N may have more than one state. Then, the
   product machine state set contains a number of states that is at most the product
   of the number of states of machines N and A, since, of course, some states of the
   Cartesian product may be useless.


<a id="P215"></a>

4.3 Deterministic Pushdown Automata and Languages
=================================================

   It is important to further the study of the deterministic recognizers and corresponding
   languages, because they are widely adopted in compilers thanks to their 
   computational efficiency. When we observe a pushdown machine (as defined on p. 205), we
   may find three nondeterministic situations, namely the uncertainty between:
   1. reading moves, if, for a state q, a character a and a stack symbol A, the transition
   function δ has two or more values, i.e., |δ (q, a, A)| > 1
   2. aspontaneousmoveandareadingmove,ifbothmovesδ (q, ε, A)andδ (q, a, A)
   are defined
   3. spontaneous moves, if for some state q and symbol A, the function δ (q, ε, A)
   has two or more values, i.e., |δ (q, ε, A)| > 1
   If none of the three forms occurs in the transition function, the pushdown machine
   is deterministic. The language recognized by a deterministic pushdown machine is
   called (context-free) deterministic, and the family of such languages is named DET.
   Example 4.12 (forms of nondeterminism) The one-state recognizer of language
   L = { a n b m | n ≥ m > 0 }, shown in Table4.2 on p. 203, is nondeterministic of
   form 1:
   δ (q 0 , a, A) = { (q 0 , b), (q 0 , b A) }
   and also of form 2:
   δ (q 0 , ε, S) = { (q 0 , A) } δ (q 0 , a, S) = { (q 0 , S) }
   The same language L is accepted by the deterministic machine M 2 :
   M 2 =
   ?
   { q 0 , q 1 , q 2 }, { a, b }, { A, Z 0 }, δ, q 0 , Z 0 , { q 2 }
   ?
   q 0 q 1 q 2
   M 2 → →
   b,A
   ε
   a,ε
   A
   b,A
   ε
   In the state q 0 , machine M 2 reads character a and pushes symbol A without reading
   the stack, i.e., by applying move
   a, Z 0
   Z 0 A
   or
   a, A
   A A . Upon reading the first character b, it
   pops an A and moves to state q 1 . Then, it pops an A for each further character b. If
   there is an excess of letters b with respect to letters a, the computation ends in error.
   Upon reading the terminator ?, the machine goes to final state q 2 without reading
   the stack top symbol A or Z 0 . ?

<a id="P216"></a>

   Table 4.3 Closure properties of language family DET, in relation to CF and REG
   Operation New property Known property
   Reversal D R / ∈ DET D R ∈ CF
   Union D 1 ∪ D 2 / ∈ DET, D ∪ R ∈ DET D 1 ∪ D 2 ∈ CF
   Complement ¬D ∈ DET ¬L / ∈ CF
   Intersection D ∩ R ∈ DET D 1 ∩ D 2 / ∈ CF
   Concatenation D 1 · D 2 / ∈ DET, D · R ∈ DET D 1 · D 2 ∈ CF
   Star D ∗ / ∈ DET D ∗ ∈ CF
   Although we were able to find a deterministic pushdown machine for the language
   of the example, this is impossible for other context-free languages, in other words,
   the family DET is a proper subfamily of CF, as we will see.


## 4.3.1 Closure Properties of Deterministic Languages

   Deterministic languages are a proper subclass of context-free languages and have
   specific properties. Starting from the known properties (in Table2.9 on p. 95), we
   list the properties of deterministic languages in Table4.3. We symbolize a language
   belonging to the families CF, DET and REG with L, D and R, respectively. Next
   we argue for the listed properties and support them by examples. 3
   reversal
   The language L below:
   L =
   ?
   a n b n e
   ?
   ∪
   ?
   a n b 2n d
   ?
   for n ≥ 1
   satisfies the two equalities |x | a = |x | b if the sentence ends with letter e, and
   2|x | a = |x | b if it ends with letter d. Language L is not deterministic, but the
   reversed language L R is so. In fact, it suffices to read the first character to decide
   which equality has to be checked.

   union
   Example4.15onp.219showsthattheunionofdeterministiclanguages(as L ? and
   L ?? obviously are) is in general nondeterministic. From the De Morgan identity
   it follows that D ∪ R = ¬(¬D ∩ ¬R ), which is a deterministic language, for
   the following reasons: the complement of a deterministic or regular language
   3 It may be superfluous to recall that a statement such as D R
   / ∈ DET means that there exists some
   language D such that D R is not deterministic.


<a id="P217"></a>

   is, respectively, deterministic or regular, and the intersection of a deterministic
   language and a regular one is deterministic (discussed below).

   complement
   The complement of a deterministic language is deterministic. The proof (simi-
   lar to the one for regular languages on p. 189) constructs the recognizer of the
   complement by creating a new sink state and interchanging final and non-final
   states. 4 Alternatively, Heilbrunner method [5] transforms the LR(1) grammar of
   a deterministic language into the grammar of its complement. It follows that if
   a context-free language has a non-context-free one as complement, it cannot be
   deterministic.

   intersection with a regular language
   The machine constructed by the Cartesian product of a deterministic pushdown
   machine and a deterministic finite automaton is deterministic. To show that the
   intersection of two deterministic languages may go out of DET, recall that the
   language with three equal exponents (p. 93) is not context-free. Yet it can be
   defined by the intersection of two languages, both deterministic:
   ?
   a n b n c n | n ≥ 0
   ? = ?
   a n b n c ∗ | n ≥ 0
   ?
   ∩
   ?
   a ∗ b n c n | n ≥ 0
   ?
   concatenation and star
   When two deterministic languages are concatenated, it may happen that a 
   deterministic pushdown machine can not localize the frontier between the strings of the 
   firstandsecondlanguages.Thereforeitisunabletodecidethepointforswitching
   fromtheformertothelattertransitionfunction.Asanexample,takethelanguages
   L 1 and L 2 :
   L 1 = a + ∪
   ?
   a i b i | i ≥ 1
   ?
   L 2 = c + ∪
   ?
   b i c i | i ≥ 1
   ?
   and notice that their concatenation L 1 · L 2 :
   L 1 · L 2 =
   ?
   a
   j
   b k c l | j = k or k = l with j, k, l ≥ 1
   ?
   ∪ a + c +
   is the disjoint union of the inherently ambiguous language L of Example2.58
   on p. 62 and a regular language we may disregard. Since any sentence in L has
   two distinct syntax trees, any pushdown machine that recognizes the sentence
   necessarily performs two distinct computations and cannot be deterministic. The
   situation for the star of a deterministic language is similar.

   concatenation with a regular language
   The recognizer of D · R can be constructed by cascade composition of a 
   deterministicpushdownmachineandadeterministicfiniteautomaton.Moreprecisely,
   when the pushdown machine enters the final state that recognizes a sentence of
   D, the new machine moves to the initial state of the finite recognizers of R and
   simulates its computation until the end.

   4 See for instance [1,2].


<a id="P218"></a>

   On the other hand, Table4.3 says that the basic operations of regular expressions
   may spoil determinism when applied to a deterministic language. This creates some
   difficulty to language designers: when two existing technical languages are united,
   the result is not granted to be deterministic (it may even be ambiguous). The same
   danger threatens the concatenation and the star or cross of deterministic languages.
   In practice, the designer should check that, after transforming the language under
   development, determinism is not lost.

   It is worth mentioning another important difference between DET and CF. While
   the equivalence of two CF grammars or pushdown automata is undecidable, an
   algorithm exists for checking if two deterministic automata are equivalent. 5

## 4.3.2 NondeterministicLanguages

   Inthissectionwereasonaboutthecontext-freelanguagesthatcannotbeacceptedby
   a deterministic pushdown automaton, starting from the next fundamental inclusion
   property.

   Property 4.13 (family inclusion) The family DET of deterministic languages is
   strictly included in the family CF of context-free languages. ?
   Proof The statement follows from two known facts. First, the inclusion DET ⊆
   CF is obvious since a deterministic pushdown automaton is a special case of the
   nondeterministicone.Second,itholdsDET ?= CFbecausecertainclosureproperties
   (Table4.3) differentiate a family from the other. ?
   This completes the proof, but it is worthwhile exhibiting a few typical nondetermin-
   isticcontext-freelanguagesinordertoevidencesomelanguageparadigmsthatought
   to be carefully avoided by language designers who strive for compiler efficiency.
   Lemma of Double Service
   A valuable technique for proving that a context-free language is not deterministic
   is based on the analysis of the sentences that are prefix of each other. Let D be a
   deterministic language, let x ∈ D be a sentence, and suppose there exists another
   sentence y ∈ D that is a prefix of x, i.e., it holds x = y z, where the strings x, y and
   z may be empty.

   Now, we define another language called the double service of language D, by
   inserting a new terminal—the sharp sign ?—between strings y and z:
   ds(D) =
   ?
   y ?z | y ∈ D ∧ z ∈ Σ ∗ ∧ y z ∈ D
   ?
   5 Senizergues algorithm [6] is quite complex and is not presented in this book.

<a id="P219"></a>

   For instance, the double service of the (finite) language F = { a, a b, bb } is:
   ds(F) = { a ?b, a ?, a b?, bb? }
   Clearly, the original sentences are terminated by ? and may be followed or not by a
   suffix. It follows that D ? ⊆ ds(D) and in general the containment is strict (as in the
   preceding example).

   Lemma 4.14 (doubleservice) Iflanguage D isdeterministic,thenitsdoubleservice
   language ds (D) is deterministic as well. ?
   Proof We are going to transform a deterministic recognizer M of language D into
   a deterministic one M ? of the double service language of D. To simplify the
   construction, 6 we assume that automaton M functions online (p. 212). This means that
   as it scans the input string, it can at once decide if the scanned string is a sentence.
   Now, consider the computation of M that accepts string y. If string y is followed by
   a sharp sign ?, then the new machine M ? reads the sharp and accepts if the input is
   finished, because y ? ∈ ds(D) if y ∈ D. Otherwise, the computation of M ? proceeds
   deterministically and scans string z exactly as machine M would do to recognize
   string y z. ?
   The reason for the curious name “double service” of the language is now clear: the
   automaton simultaneously performs two services inasmuch as it has to recognize a
   prefixandalongerstring.Lemma4.14hasthepracticalimplicationthatifthedouble
   service of a CF language L is not CF (and hence not D), then the language L itself
   is not deterministic.

   Example 4.15 (nondeterministic union of deterministic languages) The language L
   below:
   L =
   ?
   a n b n | n ≥ 1
   ?
   ∪
   ?
   a n b 2n | n ≥ 1
   ?
   = L ? ∪ L ??
   which is the union of two deterministic languages L ? and L ?? , is not deterministic.
   Intuitively,anautomatonfor L hastoreadanumberoflettersa andstorethenumber
   on the stack. Then, if the input string is in L ? , e.g., a a bb, it must pop an a upon
   reading a letter b. But if the input string is in L ?? , e.g., a a bbbb, it must read two
   letters b before popping one a. As the machine does not know which the correct
   choice is (for that it should count the number of b while examining an unbounded
   substring), it is obliged to carry on both computations nondeterministically.
   More rigorously, assume by contradiction that language L is deterministic. Then
   by Lemma 4.14 also its double service language ds(L) would be deterministic, and
   6 See Floyd and Beigel [7] for further reading.


<a id="P220"></a>

   fromtheclosurepropertyofDET (p.216)underintersectionwithregularlanguages,
   the language L R :
   L R = ds(L) ∩
   ?
   a + b + ?b +
   ?
   is deterministic, too. But language L R is not context-free, hence certainly not a
   deterministic one, because its sentences have the form a i b i ?b i , for i ≥ 1, with
   three equal exponents (p. 93), a well-known non-context-free case. ?
   The next example applies the same method to the basic paradigm of palindromic
   strings.

   Example 4.16 (palindromes and nondeterminism) The language L of palindromes
   is defined by grammar:
   S → a S a | b S b | a | b | ε
   Toprovethatlanguage L isnotdeterministic,weintersectitsdoubleservicelanguage
   ds(L) with a regular one, with the aim of obtaining a language that is not in CF.
   Consider the language L R :
   L R = ds(L) ∩
   ?
   a ∗ ba ∗ ?ba ∗
   ?
   A string of the form a i ba j ?ba k is in L R if, and only if, condition j = i and k = i
   holds. But then this language L R is again the well-known non-CF one with three
   equal exponents. ?

## 4.3.3 Determinism and Language Unambiguity

   If a language is accepted by a deterministic automaton, every sentence is recognized
   with exactly one computation and it is provable that the language can be generated
   by a non-ambiguous grammar.

   The construction of Algorithm4.7 on p. 210 produces a grammar equivalent to
   a pushdown automaton, which simulates computations by means of derivations:
   the grammar generates a sentence with a leftmost derivation if, and only if, the
   machineperformsacomputationthatacceptsthesentence.Itfollowsthattwodistinct
   derivations (of the same sentence) correspond to distinct computations, which on a
   deterministic machine necessarily scan different strings.

   Property 4.17 (grammar unambiguity condition) Let M be a deterministic
   pushdown machine. Then the corresponding grammar of L (M) obtained with
   Algorithm4.7 on p. 210 is not ambiguous. ?
   Yet, of course other grammars of L (M) may be ambiguous.


<a id="P221"></a>

   A consequence is that any inherently ambiguous context-free language L is
   nondeterministic, i.e., cannot be recognized by a deterministic pushdown machine.
   Suppose by contradiction that language L is deterministic, then the preceding property
   states that a non-ambiguous equivalent grammar does exist. This contradicts the
   very definition of inherent ambiguity (p. 62) that every grammar of the language
   is ambiguous. The discussion above confirms what has been already said about the
   irrelevance of inherently ambiguous languages for technical applications.

   Example 4.18 (inherently ambiguous language and nondeterminism) The 
   inherently ambiguous language of Example2.58 (p. 62) is the union of two languages:
   L A =
   ?
   a i b i c ∗ | i ≥ 0
   ?
   ∪
   ?
   a ∗ b i c i | i ≥ 0
   ?
   = L 1 ∪ L 2
   whicharebothdeterministic(bytheway,thisisanotherproofthatfamilyDET isnot
   closedunderunion).Intuitively,torecognizelanguageL A ,differentstrategiesmustbe
   adoptedforthestringsofthetwolanguages.Forlanguage L 1 thelettersa arepushed
   ontothestackandpoppedfromituponreadingb.Thesamehappensforlanguage L 2 ,
   butforlettersbandc.Therefore,anysentencebelongingtobothlanguagesisaccepted
   bytwodifferentcomputationsandtheautomatonisnondeterministic. ?
   The notion of ambiguity applies to any automaton type. An automaton is ambiguous
   if it recognizes a sentence by two distinct computations. We observe that the 
   determinism condition is more stringent than the absence of ambiguity: the family of
   deterministic pushdown automata is strictly contained in the one of non-ambiguous
   pushdown automata. To clarify the statement, we show two examples.

   Example 4.19 (relation between nondeterminism and unambiguity) The language
   L A ofthepreviousexample(Example4.18)isnondeterministicandevenambiguous,
   becausecertainsentencesarenecessarilyacceptedbydifferentcomputations.Onthe
   other hand, the language L I of Example4.15 on p. 219:
   L I =
   ?
   a n b n | n ≥ 1
   ?
   ∪
   ?
   a n b 2n | n ≥ 1
   ?
   = L ? ∪ L ??
   thoughnondeterministic(asarguedthere)isunambiguous.Infact,eachsublanguage
   L ? and L ?? is easily defined by an unambiguous grammar, and the union is still
   unambiguous because the components are disjoint. The two recognition strategies
   described in Example4.18 are implemented by distinct computations, but at most
   one of them may succeed for any given string. ?

## 4.3.4 Subclasses of Deterministic Pushdown Automata

   and Languages
   By definition, the family DET is associated with the most general type of 
   deterministic pushdown machine, the one featuring several states and using final states for

<a id="P222"></a>

   acceptance. Various limitations on the internal states and acceptance modes, which
   do not have any consequence in the nondeterministic case, cause a restriction of the
   language family recognized by a deterministic machine. The main limitations 7 are
   briefly mentioned:
   automaton with only one state
   Acceptance is necessarily by empty stack, and it is less powerful than recognition
   by final state.

   limitation on the number of states
   Any language family obtained by limiting the number of states is more restricted
   than DET. A similar loss is caused if a limit is imposed on just the number of final
   states or, more generally, on the number of final configurations of the machine.
   real-time functioning
   A machine with the real-time property (Sect. 4.2.3.1) may not have spontaneous
   moves, yet certain deterministic context-free languages need them to be 
   recognized by means of a deterministic pushdown automaton. We illustrate with the
   language L of Example4.10 on p. 213:
   L =
   ?
   a m b n c n | m, n ≥ 1
   ?
   ∪
   ?
   a m b n d m | m, n ≥ 1
   ?
   (4.1)
   A nondeterministic real-time machine recognizing language L was there
   described, yet a simple reasoning suffices to prove that a deterministic machine
   M D recognizing L with the real-time property does not exist.

   Weobservethatthetwosublanguagesin(4.1)haveacommonprefixofunbounded
   length of the form a m b n . After processing such a prefix for an input string x, any
   machine M D needs to have recorded in the stack a string of symbols, say A m B n ,
   which remember the values m and n, otherwise it would be later impossible to
   checkatleastoneoftheidentities|x | b = |x | c or|x | a = |x | d .Anyway,tocheck
   the latter identity, the machine must first perform n spontaneous moves to clear
   thestackfromsymbols B.Therefore,themachineviolatesthereal-timeproperty.

   Noticehoweverthatthemachineisdeterministic,becausethenextinputcharacter
   c or d determines the move to be performed.

   In many practical situations, technical languages are designed so as to be 
   deterministic. For instance, this is the case for almost all the programming languages and
   for the families XML, HTML, etc, of markup languages. Different approaches exist
   for ensuring that a language is deterministic, by imposing some conditions on the
   language or on the grammar thereof. Depending on the condition, one obtains a
   different subfamily of DET.

   Two simple cases are next described to introduce the topic. Others, much more
   important for the applications, will be obtained with the conditions LL(k), LR(k)
   and operator precedence.

   7 For further reading, see [1,8].


<a id="P223"></a>


### 4.3.4.1 Simple Deterministic Languages

   A grammar is called simple deterministic if it satisfies the next conditions:
   1. every rule right part starts with a terminal character, hence empty rules and the
   rules starting with a nonterminal symbol are excluded.

   2. for any nonterminal A, there do not exist any alternatives that start with the same
   character:
   ? ( A → a α | a β ) with a ∈ Σ, α, β ∈ (Σ ∪ V ) ∗ and α ?= β
   An example of a simple deterministic grammar is: S → a S b | c.

   Clearly,ifweconstructthepushdownmachinefromasimpledeterministicgram-
   mar by means of Algorithm4.1 on p. 202, we obtain a deterministic machine.
   Moreover, this machine consumes a character with each move, i.e., it works in real time,
   which is consistent with such grammar rules being in the real-time normal form (p.
   78). We hasten to say that anyway the simple deterministic condition is too 
   inconvenient for a practical use.


### 4.3.4.2 Parenthesis Languages and Input-Driven Languages

   It is easy to check that the parenthesized languages (introduced in Chap. 2 on p. 47),
   generated by parenthesized grammars, are deterministic. We assume that in such a
   grammaranytworulesdifferintheirright-handsides,i.e.,thegrammarisinvertible,
   or that the grammar is distinctly parenthesized (p. 50); either assumption ensures
   that the grammar is unambiguous.

   Any sentence generated by a parenthesized grammar has a bracketed structure that 
   marks the start and end of the right part of each rule used in the derivation. A simple
   recognition algorithm scans the input string, and it localizes a substring that does not
   contain any parentheses and is enclosed between two matching parentheses. Then
   the set of the right parts of the grammar rules is searched for such a parenthesized
   substring. If none of such parts matches, the input string is rejected. Otherwise
   the substring is reduced to the corresponding nonterminal, i.e., the left part of the
   matching rule, thus producing a shorter input string to be recognized. Then the
   algorithm resumes scanning the new input string in the same way, and at last it
   recognizesandhaltswhenthestringisreducedtotheaxiom.Itwouldnotbedifficult
   to encode the algorithm by means of a deterministic pushdown machine.

   If a grammar defines a nondeterministic language or if it is ambiguous, 
   parenthesizingthegrammar(seeDefinition2.45onp.50)removesbothdefects.Forinstance,
   thelanguageoftheeven-lengthpalindromesischangedintoadeterministiconewhen
   itsgrammar S → a S a | b S b | ε(onp.220)isparenthesized(hereweuseletters
   a and b to tag brackets) as follows:
   S → ‘ a (’a S a ‘) a ’
   alternative a
   | ‘ b (’b S b‘) b ’
   alternative b
   | ‘(’ ‘)’
   center

<a id="P224"></a>

   q 8
   q 1 q 6
   q 0 q 3 q 4 q 5 q 9
   q 2 q 7
   → →
   ( )
   a (,ε
   Z A
   b (,ε
   Z B
   a
   b
   a (,ε
   A
   b (,ε
   B
   ( )
   a
   ) a ,A
   ε
   b
   ) b ,B
   ε
   ) a ,Z A
   ε
   ) b ,Z B
   ε
   Fig. 4.6 The pushdown automaton for the language of even length palindromes. This machines
   qualifies as input-driven, see Definition 4.20 below
   Now,themidpointofasentencesuchasa bbbba ismarkedbythesubstring“()”in
   the parenthesized version “ a (a b (b b (b()b) b b) b a ) a ”. The corresponding 
   deterministic pushdown machine is shown in Fig.4.6; notice that it has the real-time
   property.

   We recall that move
   a (,ε
   A
   (or initially
   a (,ε
   Z A
   ) reads terminal “ a (” without reading
   or popping anything from stack and pushes symbol A (or initially Z A ) onto stack.
   Moreover,whenamovedoesnotperformeitherapoporapushonreadingaterminal,
   say b, we simply denote the move as q 2
   b
   − → q 3 instead of q 2
   b,ε
   ε
   −−→ q 3 .

   BycarefullyobservingFig.4.6,wediscoveracloseassociationbetweenterminals
   and move types:
   • push operations, also known as calls, are performed for terminals “ a (” and “ b (”
   • popoperations,alsoknownasreturns,areperformedforterminals“) a ”and“) b ”
   • internal operations, i.e., exclusively on the state, are performed for terminals a, b,
   “(” and “)”
   When such an association exists, each terminal symbol commands one and only one
   move type. It is convenient to group together the terminals that command the same
   move type, thus partitioning the terminal alphabet Σ into three disjoint subsets,
   respectively, named Σ call , Σ return and Σ internal . Their values for the example of
   Fig.4.6 are:
   • Σ call = { ‘ a (’, ‘ b (’ } contains all the terminals commanding push
   • Σ return = { ‘) a ’, ‘) b ’ } contains all the terminals commanding pop
   • Σ internal = { a, b, ‘(’, ‘)’ } contains all the terminals commanding an internal
   (also called neutral) move
   A pushdown automaton that features such a 3-partition of the terminal alphabet is
   appropriately named input-driven (ID), or also visibly pushdown, where both names
   suggest that the current input character determines which action has to be under taken

<a id="P225"></a>

   on the stack. Next we formally define the input-driven property for a pushdown
   machine. 8
   Definition 4.20 (input-driven deterministic pushdown machine) Let M be a 
   deterministic pushdown automaton with input alphabet Σ, stack alphabet Γ and state set
   Q. Machine M has the input-driven (ID) property if alphabet Σ is the disjoint union
   of three alphabets Σ call , Σ return and Σ internal , such that the state-transition function
   δ has this form:
      # domain range move description
      1 Q × Σ call Q × Γ a push move does not read (so does not check
      either) the top-of -stack symbol, and writes one
      or more symbols onto the stack
      2 Q × Σ return × Γ Q a pop move reads (and so checks) the top-of-stack
      symbol,thendeletesit;butifthetopsymbolis Z 0 ,
      i.e., the stack is empty, the move proceeds without
      deleting Z 0
      3 Q × Σ internal Q aninternal movejustignoresthestack,i.e.,itdoes
      not read or write on it
      Machine M recognizes the input string if, by starting with an empty stack, it reaches
      a final state when the input has been entirely scanned. A language recognized by an
      ID machine is called input-driven. ?
   Noticethatspontaneousmovesareexcluded.Givenapushdownmachine,itisimme-
   diate to verify the property ID and to split the alphabet into the three subsets. In
   particular, any deterministic finite automaton is clearly identical to an ID machine
   with the trivial 3-partition Σ = Σ internal . Thus, family REG is strictly included in
   family ID.

   Since the moves of an ID machine are more constrained than those of a generic
   deterministic real-time pushdown machine, it is not surprising that some 
   deterministic real-time CF languages cannot be recognized by an ID machine. A situation
   preventing a language from being ID occurs when the same terminal symbol is
   associated with different move types, e.g., push and pop. The deterministic language:
   ?
   a m b m c n a n | m, n ≥ 1
   ?
   8 For simplicity, we consider deterministic pushdown machines, but the property ID can be defined
   forthenondeterministiccaseaswell.Itisnoteworthythatthefamiliesofthelanguages,respectively,
   recognized by deterministic ID machines and by nondeterministic ones, coincide. In other words,
   every nondeterministic ID automaton can be always transformed into an equivalent deterministic
   ID machine in a way similar to the determinization of a finite-state machine. For more information,
   see [9,10].


<a id="P226"></a>

   is not of type ID because letter a acts as a call symbol in the prefix a m b m , as the
   machinemuststorethecountoflettersa tocheckthatthenumberoflettersbisequal,
   whereas it acts as a return symbol in the suffix c n a n . Notice that both prefix and
   suffix are separately of type ID, yet their concatenation is not. Moreover, the union
   of the prefix and suffix sets, i.e., { a m b m | m ≥ 1 } ∪ { c n a n | n ≥ 1 }, presents
   the same conflict, thus proving that the union of two ID languages in general is not
   input-driven.

   On the other hand, concatenation and union preserve the input-driven property
   if the two languages combined have the same 3-partitioned alphabet. We can check
   this statement for the following languages L 1 and L 2 :
   L 1 =
   ?
   a m b m | m ≥ 1
   ?
   L 2 =
   ?
   a n c n | n ≥ 1
   ?
   (4.2)
   Clearly, letter a triggers a push move, and letters b and c a pop move, therefore the
   3-partitions of L 1 and L 2 are identical:
   Σ call = { a } Σ return = { b, c } Σ internal = ∅ (4.3)
   For both concatenation L 1 · L 2 and union L 1 ∪ L 2 , we leave it to the reader to
   combine the pushdown automata of the two languages into one, and to check that
   the combined one has the ID property.

   Thisexamplepromptsanotherremark,ifwe,respectively,viewlettera asanopen
   bracket, and letters b and c as closed ones: in an ID language the same open bracket
   can match different closed brackets. This breaks the traditional rule of a parenthesis
   language that the open and closed parenthesis types are in a one-to-one relation.
   Now, we consider a language composition case that looks more problematic for
   the ID property. We concatenate the language L 1 of line (4.2) above, with language
   L 3 = a + b, which is recognized by the finite automaton having the transitions:
   q 0
   a
   − → q 1 q 1
   a
   − → q 1 q 1
   b
   − → q 2 with state q 2 final
   Clearly, according to Definition4.20 (case 3) all the moves are internal, i.e., letters
   a and b are in Σ internal , thus clashing with the assignment of a to class Σ call in
   language L 1 . But it would be wrong to conclude that language L 1 · L 3 violates the
   ID condition! The pushdown machine for L 1 · L 3 , shown in Fig.4.7, has the same
   3-partition as in (4.3). Notice that all symbols C pushed onto the stack by the moves
   that follow state q 3 are useless, because there is no need to count the number of
   letters a in a + ; their presence in the stack is just an artifact of assigning letter a to
   subset Σ call .

   Another lesson from the above example is that, for each regular language, we can
   arbitrarily choose a 3-partition and construct an ID machine accepting the language.
   For instance, if we choose the trivial 3-partition Σ = Σ return , all the machine moves
   are of type pop, thus the stack stays empty; such a machine would then work as a
   finite automaton accepting the same language.


<a id="P227"></a>

   q 0 q 1 q 2 q 3 q 4 q 5 → →
   a,ε
   S
   a,ε
   A
   b,A
   ε
   b,S
   ε
   b,A
   ε
   b,S
   ε
   a,ε
   C
   a,ε
   C
   b,C
   ε
   Fig.4.7 The automaton of type ID that recognizes language { a m b m | m ≥ 1 } · a + b
   Table 4.4 Properties of the input-driven family ID and of the subfamilies ID part characterized by
   the same alphabet partition, along with the known properties of family DET
   Operation Property of ID and ID part Property of DET
   Reversal I R ∈ ID D R / ∈ DET
   Union I ?
   part
   ∪ I ??
   part ∈ ID part
   D 1 ∪ D 2 / ∈ DET
   Complement ¬I part ∈ ID part ¬D ∈ DET
   Intersection I ?
   part
   ∩ I ??
   part ∈ ID part
   D 1 ∩ D 2 / ∈ CF
   Concatenation I ?
   part
   · I ??
   part ∈ ID part
   D 1 · D 2 / ∈ DET
   Star I ∗
   part ∈ ID part
   D ∗ / ∈ DET

### 4.3.4.3 Properties of Input-Driven Languages

   Input-driven languages are quite remarkable for their mathematical properties. To
   start, notice that the family ID of input-driven languages is strictly included in the
   family DET and strictly contains family REG. Then, within the family ID, we focus
   on the set of the languages characterized by the same 3-partition of the terminal
   alphabet, and we call each such set a partitioned ID subfamily, to be denoted by
   ID part . The closure properties of (sub)family ID and ID part are listed in Table4.4. 9
   WesymbolizealanguagebelongingtofamilyIDandtothesubfamilywithagiven3-
   partitionas I and I part ,respectively.Forcomparison,wealsolisttheknownproperties
   of family DET.

   Notice that every subfamily ID part has the same closure properties as the
   regular languages. In particular, closure under intersection, which follows from closure
   under union and complement, and from the De Morgan identity, implies the 
   decidability of the emptiness question: is it true that I ?
   part
   ∩ I ??
   part = ∅? On the contrary,
   for unrestricted deterministic languages the same question is undecidable. Having a
   decision algorithm for intersection emptiness is useful for a class of important 
   appli9 For a justification of the properties we have not discussed, we refer the reader to [9–11].

<a id="P228"></a>

   cations (out of scope for this book), such as the formal verification of a hardware
   and software system modeled by means of an automaton. 10
   Coming back to the compiler business, a relevant question is whether family
   ID is a good model for defining artificial and technical languages. The answer is
   yes and no. Truly, some data definition languages such as the markup languages
   XML and HTML, or the Javascript Object Notation JSON, admit a grammar of type
   ID. This is not surprising, if we consider that their sentences essentially consist of
   nested and concatenated parenthetical constructs, and that we have introduced the
   ID machines as recognizers for parenthesis languages. Web documents and
   semistructured databases are often encoded in XML. Distinct opening and closing marks
   are used to delimit the document parts, to allow an efficient recognition and 
   transformation thereof. Therefore, the XML family is deterministic. The XML grammar
   model, technically known as Document Type Definition or DTD, is similar to the
   parenthesis grammar model, but it uses various regular expression operators in the
   grammar rules, as a distinguishing feature. 11
   Example 4.21 (input-driven model of language JSON) Language JSON was
   designed as a compact replacement of XML for representing nested data structures.
   It is closer to the input-driven model, as it uses only two bracket pairs: square and
   curly. We construct a deterministic ID recognizer for a large fragment of JSON, in a
   modular way.

   Consider the EBNF grammar of JSON in Example2.96 on p. 102. For brevity, we
   dropterminalsnumandbool,whichdonotneedastackfortheirrecognition,andwe
   schematize nonterminal STRING as a terminal s, for the same reason. A 3-partition
   of the alphabet is:
   Σ call = { ‘{’, ‘[’ } Σ return = { ‘}’, ‘]’ } Σ internal = { s, ‘:’, ‘,’ }
   (terminalsarequotedtoavoidconfusion).Byrepeatedlysubstitutinggrammarrules,
   we obtain a grammar with one rule (V stands for VALUE):
   V → s | ‘[’
   ?
   V (, V ) ∗
   ?
   | ε ‘]’
   alternative ARRAY
   | ‘{’
   ?
   s:V (, s:V ) ∗
   ?
   | ε ‘}’
   alternative OBJECT
   (4.4)
   Inrule (4.4), weconsiderthe ARRAY alternative and theterminal rule V → sfora
   string. The case of the OBJECT alternative is similar and is omitted. Figure4.8 shows
   the deterministic ID machine for arrays. Stack symbols A and Z (initial removable
   symbol)countandmatchsquarebrackets.Anarrayelementcanbeastring,anarray,
   or a comma-separated list thereof, or even nothing. Recognition is by final state (in
   10 For an introduction to system model checking, see [12,13].

   11 Such DTD grammars, also named regular tree grammars, generate context-free languages, but
   they differ from context-free grammars in several ways; see [14,15] for more information.

<a id="P229"></a>

   q 0 q 1 q 2 q 3 q 4 q 5 → →
   [,ε
   Z
   [,ε
   A
   s
   ],A
   ε
   ],Z
   ε
   ,
   ],A
   ε
   ],Z
   ε
   [,ε
   A
   s
   ,
   ],A
   ε
   ],Z
   ε
   Fig.4.8 State-transitiongraphofadeterministicIDmachineforJSON arrays,definedbyrule(4.4)
   q 5 ). Table4.5 tabulates the computation for the input [s,[s,[],s]], which specifies a
   3-level array. Since language JSON has a parenthesis structure, from the trace of the
   ID machine we can construct the abstract tree of the input string. In a trace, we pair
   the push and pop operations that work on the same stack symbol, we create a tree
   Table 4.5 Recognition of input [ s,[ s,[ ],s ] ] by the ID machine of Fig.4.8
   Machine configuration Machine action
   State Input Stack Input Stack Move type
   q 0 [ s,[ s,[ ],s ] ] Z 0 | Initial configuration
   q 0 [ s,[ s,[ ],s ] ] Z 0 | [ push Z Push
   q 1 s,[ s,[ ],s ] ] ···| Z s — Internal
   q 2 ,[ s,[ ],s ] ] ···| Z , — Internal
   q 3 [ s,[ ],s ] ] ···| Z [ Push A Push
   q 1 s,[ ],s ] ] ···| Z A s — Internal
   q 2 ,[ ],s ] ] ···| Z A , — Internal
   q 3 [ ],s ] ] ···| Z A [ Push A Push
   q 1 ],s ] ] ···| Z AA ] Pop A Pop
   q 4 ,s ] ] ···| Z A , — Internal
   q 3 s ] ] ···| Z A s — Internal
   q 2 ] ] ···| Z A ] Pop A Pop
   q 4 ] ···| Z ] Pop Z Pop
   q 5 Finished Z 0 | Final configuration

<a id="P230"></a>

   Fig.4.9 JSON abstract
   syntax tree of the input [ s,[ s,
   [ ],s ] ] constructed from the
   computation trace in
   Table4.5
   ARRAY
   [ s , ARRAY
   [ s , ARRAY
   [ ε ]
   , s ]
   ]
   node for each pair, and we append to the node (from left to right) as its siblings the
   input characters and the inner nodes in between. Figure4.9 shows an abstract tree
   for JSON. It would not be difficult to automate such a tree construction procedure,
   thus a parser from our JSON ID pushdown recognizer. Yet its relation to a specific
   grammar,e.g.,theEBNF oneinExample2.96,isratherindirect.Bettermethodswill
   be explained in the next sections, to directly construct a parser starting from a CF
   grammar. ?
   We will return to language JSON in Sect. 4.8, to see how to automatically obtain
   a deterministic (and even parallel) parser for JSON starting from a grammar of the
   operator-precedence type. Such grammars define a language family intermediate
   between ID and DET.

   On the other hand, the constructs of most programming languages, such as Java,
   are more complex than those of data representation languages and are beyond the
   capability of ID languages. Differently said, parenthesizing a grammar to make it
   input-driven is too drastic a remedy to be acceptable by programmers and language
   users.Nevertheless,theideathatinalanguagecertainterminalcharactersmayactas
   opening parentheses and certain others as closing ones is present in all the artificial
   languages.

4.4 Syntax Analysis:Top-Down and Bottom-Up Constructions
========================================================

   The rest of the chapter explains the difference between top-down and bottom-up
   sentencerecognition,andsystematicallycoversthepracticalparsingalgorithmsused
   by compilers, also when grammars are extended with regular expressions.

   Consider a grammar G. If a source string is in the language L (G), a syntax
   analyzer or parser scans the string and computes a derivation or syntax tree; otherwise
   it stops and prints the configuration where an error was detected (diagnosis); 
   afterward it may resume parsing and skip the substrings contaminated by the error (error
   recovering), in order to offer as much diagnostic help as possible with a single scan

<a id="P231"></a>

   of the source string. Thus an analyzer is simply a recognizer capable of recording a
   string derivation and possibly of offering error treatment. Therefore, when a
   pushdown machine performs a move corresponding to a grammar rule, it has to save the
   rule label into some kind of data structure. Upon termination, such a data structure
   will represent the syntax tree.

   If the source string is ambiguous, the result of the analysis is a set of trees, also
   called a tree forest. In that case the parser may decide to stop as soon as it finds a
   derivation, or to exhaustively produce all the derivations.

   Weknow(p.46)thatthesamesyntaxtreecorrespondstomanyderivations,notably
   the leftmost and rightmost ones, as well as to less relevant others. Depending on the
   derivation being leftmost or rightmost, and on the construction order, we obtain two
   most important parser classes:
   top-down analysis
   Constructs the leftmost derivation by starting from the axiom, i.e., the tree root,
   and growing the tree toward the leaves. Each algorithm step corresponds to a
   derivation step.

   bottom-up analysis
   Constructs the rightmost derivation, but in the reversed order, i.e., from the leaves
   to the root of the tree. Each step corresponds to a reduction.

   The complete algorithms are described in the next sections. Here their functioning
   is best introduced through an example.

   Example 4.22 (tree visit orders) Consider the grammar below:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   1: S → a S A B
   3: A → b A
   5: B → c B
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   2: S → b
   4: A → a
   6: B → a
   and the sentence a 2 b 3 a 4 . The orders corresponding to the top-down and bottom-up
   visits are in Fig.4.10. In each node the framed number gives the visit order and the
   nonterminal subscript indicates the grammar rule applied.

   A top-down analyzer grafts the rule right part under the node representing the
   corresponding left part, which is a nonterminal. If the right part contains terminal
   symbols, these have to orderly match the characters of the source string. The
   procedure terminates when all the nonterminal symbols have been matched to terminal
   ones or to empty strings.

   On the other hand, starting with the source text, a bottom-up analyzer reduces to
   a nonterminal node a source substring that matches a rule right part, as it finds it in
   a left-to-right scan. After reducing, it scans again the modified text to find the next
   reduction, until the text is reduced to the axiom. ?

<a id="P232"></a>

   top-down tree visit
   S 1
   a S 1
   a S 2
   b
   A 3
   b A 3
   b A 4
   a
   B 6
   a
   A 4
   a
   B 6
   a
   1
   2
   3
   4
   5
   6
   7
   8
   9
   bottom-up tree visit
   S 1
   a S 1
   a S 2
   b
   A 3
   b A 3
   b A 4
   a
   B 6
   a
   A 4
   a
   B 6
   a
   9
   6
   1
   4
   3
   2
   5
   7
   8
   Fig.4.10 Left-to-right, top-down and bottom-up visits of a syntax tree for Example4.22
   The dichotomy top-down/bottom-up can be made to apply to extended BNF
   grammars, too, and we postpone the discussion to later sections.

   In principle, syntax analysis would work as well from right to left, by scanning
   the reversed source string; in reality all the existing languages are designed for
   leftto-right processing, as it happens in the natural language where the reading direction
   matches the uttering order by a speaker. Moreover, reversing the scanning direction
   may cause loss of language determinism, because family DET is not closed under
   reversal (p. 216).


<a id="P233"></a>

   Finally, the diffusion of parallel processing techniques motivates the study of new
   algorithms that would parse a long text—such as a complex web page—by means
   of parallel processes, each one acting on a subtext.

4.5 Grammar as Network of Finite Automata
=========================================

   We are going to represent a grammar as a network of finite machines. This form has
   several advantages: it offers a pictorial representation, gives evidence to the
   similarities across different parsing algorithms, allows to directly handle grammars with
   regular expressions, and maps quite nicely on recursive descent parser implementa-
   tions.

   In a grammar, each nonterminal is the left part of one or more alternatives. On the
   other hand, if a grammar G is in the extended context-free form (EBNF) on p. 100, a
   rulerightpartmaycontaintheunionoperator,whichmakesitpossibletodefineeach
   nonterminalbyjustonerule A → α,whereαisanr.e.overthealphabetofterminals
   and nonterminals. The r.e. α defines a regular language, the finite recognizer M A of
   which is easily constructed through the methods of Chap. 3.

   Inthetrivialcasewhenthestringαcontainsjustterminalsymbols,theautomaton
   M A recognizes the language L A (G) generated by grammar G starting from
   nonterminal A. But since in general the right part α includes nonterminal symbols, we
   next examine what to do with an arc labeled by a nonterminal B. This case can be
   thought of as the invocation of another automaton, namely the one associated with
   rule B → β. Noticethatsymbol B maycoincide with A,thuscausingtheinvocation
   to be recursive.

   In this chapter, to avoid confusion we call such finite automata as “machines”,
   and we reserve the term “automaton” for the pushdown automaton that accepts the
   language L (G).Itisconvenient,thoughnotstrictlynecessary,forthemachinestobe
   deterministic;ifnot,theycanbemadedeterministicthroughthemethodsofChap.3.

   Definition 4.23 (recursive net of finite deterministic machines)
   • Let Σ and V = { S, A, B, ... } be, respectively, the terminal and nonterminal
   alphabets, and let S be the axiom, of an EBNF grammar G.

   • For each nonterminal A there is exactly one (extended) grammar rule A → α and
   the rule right part α is an r.e. over alphabet Σ ∪ V.

   • We denote the grammar rules by S → σ, A → α, B → β, .... Symbols R S , R A ,
   R B , ... denote the regular languages over alphabet Σ ∪ V, defined by the r.e. σ,
   α, β, ..., respectively.

   • Symbols M S , M A , M B , ... are the names of the (finite deterministic) machines
   that accept the corresponding regular languages R S , R A , ... . The collection of all
   such machines, i.e., the net, is denoted by symbol M.

   • To prevent confusion, the names of the states of any two machines are made
   disjoint, say, by appending the machine name as a subscript. The state set of a

<a id="P234"></a>

   machine M A isdenoted Q A = { 0 A , ..., q A , ... },itsonlyinitialstateis0 A and
   its final state set is F A ⊆ Q A . The state set Q of a net M is the union of all the
   states of the component machines:
   Q =
   ?
   M A ∈M
   Q A
   The transition function of all the machines, i.e., of the whole net, will be denoted
   by the same name δ as for the individual machines, at no risk of confusion since
   the machine state sets are all mutually disjoint.

   • For a state q A , the symbol R (M A , q A ), or for brevity R (q A ), denotes the regular
   language over alphabet Σ ∪ V that is accepted by machine M A starting from q A .
   For the initial state, we have R (0 A ) ≡ R A .

   • It is convenient to stipulate that every machine M A may not have any arc like
   q A
   0 A
   c
   (c ∈ Σ ∪ V) which enters the initial state 0 A . Such a
   normalization ensures that the initial state may not be visited twice, i.e., it is not
   recirculated or reentered, within a computation that does not leave machine
   M A . ?
   Westressthatanyrule A → αisfaithfullyrepresentedbymachine M A . 12 Therefore,
   amachinenetM = { M S , M A , ... }isessentiallyanotationalvariantofagrammar,
   andwemaygoonusingalreadyknownconceptssuchasderivationandreduction.In
   particular,theterminallanguage L (M)(overalphabetΣ)defined(orrecognized)by
   the machine net coincides with the language generated by the grammar, i.e., L (G).
   We need to extend the previous definition to the terminal language defined by a
   generic machine M A , but starting from any state. For any state q A , not necessarily
   initial, we define the following language:
   L (M A , q A ) = L (q A ) =
   ?
   y ∈ Σ ∗ | η ∈ R (q A ) ∧ η
   ∗
   = ⇒ y
   ?
   The formula above contains a string η over terminals and nonterminals, accepted
   by machine M A when starting in the state q A . The derivations originating from η
   produce all the terminal strings of language L (q A ).

   In particular, from previous stipulations it follows that:
   L (M A , 0 A ) = L (0 A ) ≡ L A (G)
   and for the axiom it is:
   L (M S , 0 S ) = L (0 S ) = L (M) ≡ L (G)
   12 Of course two different but equivalent r.e. are represented by machines that are equivalent or
   identical.


<a id="P235"></a>

   Example 4.24 (machine net for arithmetic expressions) The EBNF grammar below
   has alphabet { +, −, ×, /, a, ‘(’, ‘)’ }, three nonterminals E, T and F (axiom
   E), and therefore three rules:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   E → [+ | −]T
   ?
   (+ | −)T
   ? ∗
   T → F
   ?
   (× | /) F
   ? ∗
   F → a | ‘(’ E ‘)’
   Figure4.11 shows the machines of net M.

   We list a few languages defined by the net and the component machines, to 
   illustrate the definitions:
   R (M E , 0 E ) = R (M E ) = [+ | −]T
   ?
   (+ | −)T
   ? ∗
   R (M T , 1 T ) = R (1 T ) =
   ?
   (× | /) F
   ? ∗
   L (M E , 0 E ) = L (0 E ) = L E (G) = L (G) = L (M)
   = { a, a + a, a × (a − a ), ... }
   L (M F , 0 F ) = L (0 F ) = L F (G) = { a, (a ), (−a + a ), ... }
   L (M F , 2 F ) = L (2 F ) = { ‘)’ }
   machine M E
   0 E 1 E 2 E E → →
   +, −
   T
   +, −
   T
   machine M T
   minimal form normalized form
   0 T 1 T T → →
   F
   ×, /
   0 T 1 T 2 T T →
   ↓
   F
   ×, /
   F
   machine M F
   0 F 1 F 2 F 3 F
   F →
   →
   a
   ( E )
   Fig.4.11 Machine net of Example4.25; machine M T is shown in the minimal form (left) and after
   normalizing its initial state (right)

<a id="P236"></a>

   We show two examples of derivations (Chap. 2 on p. 46) and the corresponding
   reductions:
   derivations
   T − T ⇒ F × F × F − T and T − T
   2
   = ⇒ a × F × F − T
   reductions
   F × F × F − T reduces to T − T denoted F × F × F − T ? T − T
   a × F × F − T
   2
   ? T − T
   ?
   Example 4.25 (running example) The grammar G and machine net M are shown
   in Fig.4.12. The language can be viewed as obtained from the Dyck language by
   allowing a character a to replace a well-parenthesized substring; alternatively, it can
   be viewed as a set of simple expressions, with terms a, a concatenation operation
   with the operator left implicit, and parenthesized subexpressions. All the machines
   inMaredeterministicandtheirinitialstatesarenotreentered.Mostfeaturesneeded
   to exercise different aspects of parsing are present: self-nesting, iteration, branching,
   multiple final states and a nullable nonterminal.

   To show how it works, we list some of the languages defined by the machine net,
   along with their aliases:
   R (M E , 0 E ) = R (0 E ) = T ∗
   R (M T , 1 T ) = R (1 T ) = E )
   L (M E , 0 E ) = L (0 E ) = L (G) = L (M)
   = { ε, a, a a, (), a a a, (a ), a (), ()a, ()(), ... }
   L (M T , 0 T ) = L (0 T ) = L T (G) = { a, (), (a ), (a a ), (()), ... }
   ?
   To identify the machine states, an alternative convention, quite used for BNF (non-
   extended) grammars, relies on marked grammar rules. For instance, the states of
   machine M T in Fig.4.12 which represents the alternatives T → ‘(’ E ‘)’ | a,
   have the following aliases:
   0 T ≡ T → •a | •( E )
   1 T ≡ T → (• E )
   2 T ≡ T → ( E •)
   3 T ≡ T → a • | ( E )•
   (4.5)
   where the bullet • is a character not in the terminal alphabet Σ.


<a id="P237"></a>

   EBNF grammar G (axiom E)
   Σ = { a, ‘(’, ‘)’ } V = { E, T } P =
   ⎧
   ⎨
   ⎩
   E → T ∗
   T → ‘(’ E ‘)’ | a
   machine net M
   0 E 1 E E →
   ↓ ↓
   T
   T
   0 T 1 T 2 T 3 T
   T →
   →
   ( E )
   a
   Fig.4.12 EBNF grammar G and net M of the running example (Example4.25)

## 4.5.1 Syntax Charts

   As a short intermission, we cite the use of machine nets for the documentation of
   technicallanguages.Inmanybranchesofengineeringitiscustomarytousegraphics
   for technical documentation in addition to textual documents, which in our case
   are grammars and regular expressions. The so-called syntax charts are frequently
   includedin thelanguage referencemanuals,as apictorial representationof extended
   context-free grammars, and under the name of transition networks they are used
   in computational linguistics to represent the grammars of natural languages. The
   popularity of syntax charts derives from their readability as well as from the fact
   that this representation serves two purposes at once: to document the language and
   to describe the control flow of parsing procedures, as we will see.

   Actually syntax charts differ from machine nets only with respect to their graphic
   style and naming. First, every finite machine graph is converted to the dual graph
   or chart by interchanging arcs and nodes. The nodes of a chart are the symbols
   (terminal and non-) occurring in the right part of a rule A → α. In a chart the arcs
   arenotlabeled;thismeansthestatesarenotdistinguishedbyaname,butonlybytheir
   position in the graph. The chart has one entry point tagged by a dart with the chart
   nonterminal A, and one exit point similarly tagged by a dart, but corresponding to
   one or more final states. Different graphic styles are found in the manuals to visually
   differentiate the two classes of symbols, i.e., terminal and nonterminal.


<a id="P238"></a>

   E → [ + | − ] T ( + | − ) T
   ∗
   syntax chart of E
   + +
   − −
   T E →
   →
   T → F ( × | / ) F
   ∗
   syntax chart of T
   ×
   /
   F T →
   →
   F → a | ‘(’ E ‘)’ syntax chart of F
   a
   (
   E
   ) F → →
   Fig. 4.13 Syntax charts equivalent to the machine net of arithmetic expressions in Fig.4.11 on
   p. 235
   Example 4.26 (syntax charts of arithmetic expressions—Example4.24) The
   machines of the net (Fig.4.11 on p. 235) are redrawn as syntax charts in Fig.4.13.
   The nodes are grammar symbols with dashed and solid shapes around for 
   nonterminals and terminals, respectively. The machine states are not drawn as graph nodes
   nor do they bear a label. Syntax charts are an equivalent representation of a grammar
   or machine net. ?
   In traversing a chart from entry to exit, we obtain any possible right part of the
   corresponding syntax rule. For instance, a few traversing paths (some with loops) in

<a id="P239"></a>

   the chart of nonterminal E are the following:
   T + T T + T − T + T − T ...

   It should be clear how to construct the syntax charts of a given EBNF grammar: by
   means of known methods, first construct the finite recognizer of each r.e. occurring
   as the right part of a grammar rule, then adjust the machine graphic representation
   to the desired convention.


## 4.5.2 Derivation for Machine Nets

   For EBNF grammars and machine nets, the previous definition of derivation, which
   models a rule such as E → T ∗ as an infinite set of BNF alternative rules E →
   ε | T | T T | ..., has shortcomings because a derivation step, e.g., E ⇒ T T T,
   replacesa nonterminal E by astring ofpossibly unbounded length; thusa multi-step
   computation inside a machine is equated to just one derivation step. For parsing
   applications, a more analytical definition is needed to split such a large step into a
   series of state transitions.

   We recall that a BNF grammar is right-linear (RL) (see Definition2.74 on p. 85)
   if every rule has the form A → u B or A → ε, where u ∈ Σ ∗ and B ∈ V (B may
   coincide with A). Every finite-state machine can be represented by an equivalent RL
   grammar (see Sect. 3.44 on p. 129) that has machine states as nonterminal symbols.

### 4.5.2.1 Right-Linearized Grammar

   Foreachmachine M A ∈ M,itisstraightforwardtowritetheRL grammarequivalent
   with respect to the regular language R (M A , 0 A ) ⊆ (Σ ∪ V ) ∗ , to be denoted as
   ˆ
   G A . The nonterminals of
   ˆ
   G A are the states of Q A , so its axiom is 0 A . If an arc
   p A
   X
   − → r A is in the transition function δ of M A , in
   ˆ
   G A there exists a rule p A → X r A
   (for any terminal or nonterminal symbol X ∈ Σ ∪ V), and if p A is a final state of
   M A , the empty rule p A → ε exists.

   Notice that X may be a nonterminal B, therefore a rule of
   ˆ
   G A may have the form
   p A → B r A , which is still RL since the first (leftmost) symbol of the right part is
   viewed as a “terminal” symbol for grammar
   ˆ
   G A . With this provision, the identity
   L
   ?
   ˆ
   G A
   ?
   = R (M A , 0 A ) clearly holds.

   Next, for every RL grammar of the net, we replace by 0 B every symbol B ∈ V
   occurring in a rule such as p A → B r A , thus obtaining rules of form p A → 0 B r A .
   TheresultingBNF grammarisdenoted
   ˆ
   G andisnamedtheright-linearizedgrammar
   of the net: it has terminal alphabet Σ, nonterminal set Q and axiom 0 S .

   Therulerightpartshavelengthzeroortwo,andtheymaycontaintwononterminal
   symbols, thus the grammar is not RL. Obviously, grammars
   ˆ
   G and G are equivalent,
   i.e., they both generate language L (G).


<a id="P240"></a>

   Example 4.27 (right-linearized grammar
   ˆ
   Gof the running example)
   ˆ
   G E
   ?
   0 E → 0 T 1 E | ε
   1 E → 0 T 1 E | ε
   ˆ
   G T
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   0 T → a 3 T | ‘(’1 T
   1 T → 0 E 2 T
   2 T → ‘)’3 T
   3 T → ε
   As said, in the right-linearized grammars we choose to name nonterminals by their
   alias states. An instance of non-RL rule is 1 T → 0 E 2 T . ?
   By using the right-linearized grammar
   ˆ
   G instead of G, we obtain derivations the
   steps of which are elementary state transitions instead of multi-step transitions on a
   machine. An example should suffice to clarify.

   Example 4.28 (derivation in a right-linearized grammar) For grammar G (in
   Fig.4.12 on p. 237) the “classical” leftmost derivation:
   E ⇒
   G
   T T ⇒
   G
   a T ⇒
   G
   a ( E ) ⇒
   G
   a () (4.6)
   is expanded into the series of truly atomic derivation steps of the right-linearized
   grammar
   ˆ
   G:
   0 E ⇒
   ˆ
   G
   0 T 1 E ⇒
   ˆ
   G
   a 1 E ⇒
   ˆ
   G
   a 0 T 1 E
   ⇒
   ˆ
   G
   a (1 T 1 E ⇒
   ˆ
   G
   a (0 E 2 T 1 E ⇒
   ˆ
   G
   a (ε2 T 1 E
   ⇒
   ˆ
   G
   a ()3 T 1 E ⇒
   ˆ
   G
   a ()ε1 E ⇒
   ˆ
   G
   a ()ε
   = a ()
   (4.7)
   which show how it works. ?
   We may also consider reductions, such as a (1 T 1 E ? a 0 T 1 E .


### 4.5.2.2 Call Sites and Machine Activation

   An arc q A
   B
   − → r A with nonterminal label B is also named a call site for machine M B ,
   and its destination r A is the corresponding return state. Parsing can be viewed as a
   process that upon reaching a call site activates the specified machine. The activated
   machine performs scanning operations and further calls on its own graph, until it
   reaches a final state, where it performs a reduction and finally goes back to the return
   state. The axiomatic machine M S is initially activated by the program that invokes
   the parser.

   When we observe a left-linearized derivation, we see that at every step the
   nonterminal suffix of the derived string contains the current state of the active machine,

<a id="P241"></a>

   followed by the return states of the suspended ones. Such states are ordered from
   right to left according to the machine activation sequence.

   Example 4.29 (derivation and machine return point) In derivation (4.7), observe
   0 E
   ∗
   = ⇒ a (0 E 2 T 1 E : machine M E is active and its current state is 0 E ; previously,
   machine M T was suspended and will resume in state 2 T ; an earlier activation of M E
   was also suspended and will resume in state 1 E . ?

## 4.5.3 Initial and Look-AheadCharacters

   The deterministic parsers to be presented implement a pushdown automaton (PDA).
   To choose the next move, such parsers use the stack contents (actually just the top
   symbol), the automaton state and the next input character, which is the token or
   lexeme returned by the lexical analyzer. To make the choice deterministic, they rely
   on the information obtained by preprocessing the grammar, which consists of the
   sets of the tokens that may be encountered for each possible move. For instance,
   suppose in the current state p A there are two outgoing arcs:
   q A
   B
   ←− p A
   C
   − → r A
   Tochoosethearctofollow,ithelpstoknowwhichtokensmaystartlanguages L (0 B )
   and L (0 C ). Such token sets are named initial.

   Moreover, another situation occurs in bottom-up parsing, when the PDA
   completely scans the right part α of a rule A → α and thus reaches the final state f A
   of machine M A . Then, to confirm that the reduction is due, it helps to know which
   tokens may follow nonterminal A in a derivation. Concretely, consider the machine
   net in Fig.4.14:
   After scanning substring a b, the PDA can be either in state f A or in state f B , and
   the reduction to be applied can be either a b ? A or b ? B. How do we choose? By
   checking whether the next input token is character c or d, respectively. To enable the
   PDA to perform such checks, we pre-calculate the token sets and we assume that the
   stringsarealwaysfollowedbytheend-of-texttoken.Nonterminal A canbefollowed
   by { c, ? } and nonterminal B by { d }. Such sets of terminals are named follow sets.
   Furthermore,theinformationprovidedbythefollowsetscanbemademoreaccu-
   rate. We observe that nonterminal A cannot be followed by terminal c if the input
   string is ea b. A sharper definition should take into account the previous 
   computation, i.e., the PDA configuration. Thus the exact sets, to be named look-ahead sets,
   associated with each configuration, are as follows: set { c } for nonterminal A with
   configuration 1 S and set { ? } for nonterminal A with configuration f S ; while for
   the nonterminal B with configuration 3 S (the only one existing), the look-ahead set
   is { d }.

   The sets of initial and look-ahead tokens are now formalized by means of 
   procedural definitions.


<a id="P242"></a>

   EBNF grammar
   S → Ac | aB d | eA A → ab B → b
   machine net
   4 S
   0 S 1 S f S
   2 S 3 S
   S →
   →
   e
   A
   a
   c
   B
   d
   A
   0 A 1 A f A A → →
   a b
   0 B f B B → →
   b
   Fig.4.14 Grammar and network for illustrating the computation of follow sets
   Initials
   We need to define the set of the initial characters for the strings recognized starting
   from a given state.

   Definition 4.30 (set Ini of initials) The set Ini ⊆ Σ of a state q A is:
   Ini(q A ) = Ini
   ?
   L (q A )
   ? = ?
   a ∈ Σ | a Σ ∗ ∩ L (q A ) ?= ∅
   ?
   ?
   Notice that set Ini may not contain the null string ε. A set Ini(q A ) is empty if, and
   onlyif,itholds L (q A ) = { ε },i.e.,theemptystringistheonlyonegeneratedstarting
   from state q A .

   Let symbol a be a terminal, symbols A and B be possibly identical nonterminals,
   and take two states q A and r A of the same machine M A . Set Ini is computed by
   applying the following logical clauses until a fixed point is reached.

   It holds a ∈ Ini(q A ) if, and only if, one or more of the three following conditions
   are met:
   1. ∃ arc q A
   a
   − → r A
   2. ∃ arc q A
   B
   − → r A ∧ a ∈ Ini(0 B ), excluding the case 0 A
   A
   − → r A
   3. ∃ arc q A
   B
   − → r A ∧ L (0 B ) is nullable ∧ a ∈ Ini(r A )
   Therestrictionin2excludesanimmediateleftrecursion,whichdoesnotaddanything
   new. For the running example: Ini(0 E ) = Ini(0 T ) = { a, ‘(’ }.


<a id="P243"></a>

   Look-Ahead
   Assaid,asetoflook-aheadcharactersisassociatedwitheachPDAparserconfigura-
   tion. Without going into details, we assume that such a configuration is encoded by
   one or more machine states. Remember that many such states are possibly entered
   bytheparserafterprocessingsometext.Inwhatfollows,weconsiderthelook-ahead
   set of each state separately.

   A pair ?state, token? is named a candidate (also known as item). More precisely,
   a candidate is a pair ?q B , a ? in Q ×
   ?
   Σ ∪ { ? }
   ? . The intended meaning is that
   token a is a legal look-ahead for the current activation of machine M B in state q B .
   When parsing begins, the initial state of the axiomatic machine M S is encoded by
   candidate ?0 S , ??, which says that the end-of-text character ? is expected when
   the entire input is reduced to the axiom S. To calculate the candidates for a given
   grammar or machine net, we use a function traditionally named closure.

   Definition 4.31 (closure function) Let C be a set of candidates. The closure of C is
   the function defined by applying the following clause (4.8):
   closure(C) = C
   ?0 B , b? ∈ closure(C) if
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   ∃ candidate ?q, a ? ∈ C and
   ∃ arc q
   B
   − → r in M and
   b ∈ Ini
   ?
   L (r) · a
   ?
   (4.8)
   until a fixed point is reached. ?
   Thus,functionclosurecomputesthesetofthemachinesthatarereachedfromagiven
   stateq throughoneormoreinvocations,withoutanyinterveningstatetransition.For
   each reachable machine M B , represented by the initial state 0 B , the function returns
   alsoanyinputcharacterb thatcanlegallyoccurwhenthemachineterminates;sucha
   character is part of the look-ahead set. Notice the effect of term Ini
   ?
   L (r) · a
   ?
   when
   state r is final, i.e., when ε ∈ L (r) holds: character a is included in the set.
   When clause (4.8) terminates, the closure of C contains a set of candidates, with
   some of them possibly associated with the same state q, as follows:
   { ?q, a 1 ?, ..., ?q, a k ? }
   For conciseness we group together such candidates and we write:
   ?
   q, {a 1 , ..., a k }
   ?
   instead of { ?q, a 1 ?, ..., ?q, a k ? }. The collection { a 1 , a 2 , ..., a k } is termed
   the look-ahead set of state q in the closure of C. By construction the look-ahead set
   of a state is never empty. To simplify the notation of singleton look-ahead, we write
   ?q, b? instead of
   ?
   q, { b }
   ? .


<a id="P244"></a>

   Example 4.32 (closure computation) We list a few values computed by the closure
   function for the grammar of Example4.25 (Fig.4.12 on p. 237):
   C function closure C
   ?0 E , ?? ?0 E , ??
   ?0 T ,{ a, ‘(’, ? }?
   ?1 T , ?? ?1 T , ??
   ?0 E , ‘)’? ?0 T ,{ a, ‘(’, ‘)’ }?
   For the top row we explain how the closure of candidate ?0 E , ?? is obtained.
   First, it trivially contains itself. Second, since arc 0 E
   T
   − → 1 E calls machine M T with
   initial state 0 T , by clause (4.8) the closure contains the candidate with state 0 T and
   with look-ahead set computed as Ini
   ? L (1
   E ) ?
   ? . Since language L (1
   E ) is nullable,
   character ? is in the set, as well as the initials of L (1 E ), i.e., characters a and ‘(’.
   Since no nonterminal arc departs from 0 T , the fixed point is now reached, hence a
   further application of closure would be unproductive.

   In the bottom row the result contains ?1 T , ?? by reflexivity, and ?0 E , ‘)’?
   because arc 1 T
   E
   − → 2 T calls machine M E and Ini
   ? L (2
   T )
   ?
   = { ‘)’ }. The result
   contains also ?0 T , { a, ‘(’, ‘)’ }? because arc 0 E
   T
   − → 1 E calls machine M T , language
   L (1 E ) is nullable and it holds { a, ‘(’, ‘)’ } = Ini
   ?
   L (1 E )‘)’
   ? . Three 
   applications of clause (4.8) are needed to reach the fixed point. ?
   The closure function is a basic component for constructing the ELR(1) parsers to be
   next presented.

4.6 Bottom-Up Deterministic Analysis
====================================

   We outline the construction of the deterministic bottom-up algorithms that is widely
   applied to automatically implement parsers for a given grammar. The formal 
   conditionthatallowssuchaparsertoworkwasdefinedandnamedLR(k)byitsinventorD.

   E. Knuth [16]. The parameter k ≥ 0 specifies how many consecutive characters are
   inspected to let the parser move deterministically. Since we consider EBNF
   grammars and we assume to be k = 1 (in accordance with common use), the condition is
   referred to as ELR(1). The language family accepted by the parsers of type LR(1)
   (orELR(1))isexactlythefamilyDET ofdeterministiccontext-freelanguages;more
   of that in Sect. 4.9.

   The ELR(1) parsers implement a deterministic automaton equipped with a
   pushdown stack and with a set of internal states, to be named macro-states (for short
   m-states) to prevent confusion with the machine net states. An m-state consists of
   a set of candidates (on p. 243). The automaton performs a series of moves of two
   types. A shift move reads an incoming character, i.e., token or lexeme, and applies
   a state-transition function to compute the next m-state; then the token and the next

<a id="P245"></a>

   m-state are pushed onto the stack. A reduction (or reduce) move applies as soon as
   the sequence of topmost stack symbols matches the sequence of character labels of
   a recognizing path in a machine M A , upon the condition that the current token is
   present in the current look-ahead set.

   The first condition for determinism is that, in every parser configuration, if a shift
   move is permitted then a reduction move is impossible. The second is that in every
   configuration at most one reduction move be possible.

   A reduction move is also in charge of growing the fragment of the syntax tree
   that the parser has so far computed. More precisely, in general the tree is a forest,
   since multiple subtrees may be still disconnected from the root that will be eventually
   created.

   Toupdatethestack,areductionmoveperformsseveralactions:itpopsthematched
   topmost part (the so-called handle) of the stack, then it pushes onto the stack the
   nonterminalsymbolrecognizedandthenextm-state.Noticethatareductiondoesnot
   consume the current token, which will be checked and consumed by the subsequent
   shift move that scans the input.

   The PDA accepts the input string if the last move reduces the stack to the initial
   configuration and the input has been entirely scanned. The latter condition can be
   expressed by saying that the special end-marker character ? is the current input
   token.

   Since the parser construction is quite complex to understand in its full generality,
   we start from a few simple cases.


## 4.6.1 From Finite Recognizers to Bottom-Up Parser

   To start from known ground, we view an r.e. e as an EBNF grammar that has only
   one nonterminal and one rule S → e. The methods of Chap. 3, especially that by
   BerryandSethi,constructafinitedeterministicautomatonthatrecognizestheregular
   language L (S) = L (e). Next, we slightly generalize such grammar by introducing
   several nonterminals, while still remaining within the domain of regular languages.
   Hierarchicallists(Chap.2p.29)areanaturalexample,whereateachleveladifferent
   nonterminal specified by an r.e. generates a list.

   Example 4.33 (bottom-up parser of hierarchical lists) A sentence is a list of lists,
   with the outer list S using letter s as separator, and with the inner lists I using
   letter a as element and letter t as separator as well as end-marker. All such lists
   are not empty. The EBNF grammar G 1 and machine net M 1 are shown in Fig.4.15.
   Althoughlanguage L (G 1 )isregular,themethodsofChap.3donotallowtoconstruct
   a recognizer, unless we eliminate nonterminal I to derive a one rule grammar, which
   we do not intend to do. Instead we construct a deterministic PDA directly from
   the network. To find the information to be kept in a PDA state, consider the input
   string a t a t s a t ?. The automaton is in the initial state 0 S , whence an I-labeled arc
   originates:thissignifiesthePDAhastorecognizeastringderivingfromnonterminal
   I. Therefore the automaton enters the initial state of machine M I without reading

<a id="P246"></a>

   EBNF grammar G 1 machine net M 1
   S → I ( s I ) ∗
   0 S 1 S 2 S S →
   ↓
   I
   s
   I
   I → ( a t ) +
   0 I 1 I 2 I I → →
   a
   t
   a
   Fig.4.15 Grammar G 1 and network M 1 for Example4.33 (two-level list)
   any character, i.e., by a spontaneous transition. As conventional with spontaneous
   moves, we say that the PDA is in one of the states { 0 S , 0 I } and we name this set a
   macro-state (m-state). The reader should notice the similarity between the concepts
   ofm-stateandofaccessiblestate subsetofafinitenondeterministicmachine(Chap. 3
   on p. 145).

   The PDA stores the current m-state { 0 S , 0 I } on the stack for reentering it in
   the future, after a series of transitions. Then, it performs a shift operation associated
   with the transition 0 I
   a
   − → 1 I of machine M I . The operation shifts the current token
   a and pushes the next m-state { 1 I } onto the stack. Then, after transition 1 I
   t
   − → 2 I ,
   the stack top is { 2 I } and the current token is t.

   These and the subsequent moves until recognition are listed in Table4.6. At each
   step, the first row shows the input tape and the second one shows the stack contents;
   the already scanned prefix of the input is shaded; since the stack bottom {0 S , 0 I }
   doesnotchange,itisrepresentedonlyonceinthefirstcolumn;steps1and14perform
   the initialization and the acceptance test, respectively; the other steps perform shift
   or reduction as commented aside.

   Returning to the parsing process, two situations require additional information to
   bekeptinthem-statesfortheprocesstobedeterministic.Thefirstoccurswhenthere
   is a choice between a shift and a reduction, known as potential shift-reduce conflict;
   and the second, when the PDA has decided for a reduction but the handle still has to
   be identified, known as potential reduce-reduce conflict.

   The first time needing to choose between shift (of a) or reduction (to I), occurs
   at step 4, since the m-state { 2 I } contains the final state of machine M I . To arbitrate
   the conflict, the PDA makes use of look-ahead set (defined on p. 243), which is the
   set of tokens that may follow the reduction under consideration. The look-ahead set
   of m-state { 2 I } contains the tokens s and ?, and since the next token a differs from
   them, the PDA rules out a reduction to I and shifts token a. After one more shift on
   t, the next shift/reduce choice occurs at step 6; the reduction to I is selected because
   the current token s is in the look-ahead set of state 2 I . To locate the handle, the PDA
   looks for a topmost stack segment that is delimited on the left by a stack symbol
   containing the initial state of machine M I . In this example there is just the following

<a id="P247"></a>

   Table 4.6 Parsing trace of string x = a t a t s a t ? (Example4.15 on p. 246)
   stack string to be parsed (with end-marker) and stack contents
   effect after #
   bottom 1 2 3 4 5 6 7
   {0 S 0 I }
   a t a t s a t
   1
   initialization
   of the stack
   a t a t s a t
   2
   a {1 I }
   terminal shift
   on a: 0 I
   a
   − → 1 I
   a t a t s a t
   3
   a {1 I } t {2 I }
   terminal shift
   on t: 1 I
   t
   −→2 I
   a t a t s a t
   (do not
   reduce)
   4
   a {1 I } t {2 I } a {1 I }
   terminal shift
   on a: 2 I
   a
   − → 1 I
   a t a t s a t
   5
   a {1 I } t {2 I } a {1 I } t {2 I }
   terminal shift
   on t: 1 I
   t
   → 2 I
   I s a t
   6
   since s follows
   I
   reduce
   atat I
   I s a t
   7
   I {1 S }
   nonterminal
   shift
   on I: 0 S
   I
   − → 1 S
   I s a t
   (do not
   reduce)
   8
   I {1 S }
   s {2
   S 0 I }
   terminal shift
   on s: 1 S
   s
   − → 2 S
   I s a t
   9
   I {1 S }
   s {2
   S 0 I
   }a
   {1 I }
   terminal shift
   on a: 0 I
   a
   − → 1 I
   I s a t
   10
   I {1 S }
   s {2
   S 0 I
   }a
   {1 I } t {2 I }
   terminal shift
   on t: 1 I
   t
   → 2 I
   I s I
   11
   I {1 S }
   s {2
   S 0 I }
   since follows
   I
   reduce at I
   I s I
   12
   I {1 S }
   s {2
   S 0 I
   }I
   {1 S }
   nonterminal
   shift
   on I: 2 S
   I
   − → 1 S
   S
   13
   since follows
   I
   reduce I sI
   S
   the input string is reduced to the axiom S
   14
   the stack contains only the axiom {0 S 0 I }
   stop and
   accept

<a id="P248"></a>

   possibility:
   bottom top
   segment (handle) to be reduced by: a t a t ? I
   { 0 S 0 I } a { 1 I } t { 2 I } a { 1 I } t { 2 I }
   But more complex situations occur in other languages.

   The reduction a t a t ? I denotes a subtree, the first part of the syntax tree
   produced by the parser. To make the stack contents consistent after reducing, the PDA
   performs a nonterminal shift, which is a sort of shift move where a nonterminal
   is shifted, instead of a terminal. The justification is that a string that derives from
   nonterminal I has been found, hence parsing resumes from m-state { 0 S , 0 I } and
   applies the corresponding transition 0 S
   I
   − → 1 S of machine M S .

   In Table4.6, whenever a reduction to a nonterminal, say I, is performed, the
   covered input substring is replaced by I. In this way, the nonterminal shift move that
   follows a reduction appears to read such a nonterminal as if it were an input symbol.
   In the steps 8–9, after shifting separator s, a new inner list (deriving from I) is
   expected in the state 2 S ; this causes the initial state 0 I of M I to be united with 2 S
   to form a new m-state. A new situation occurs in the steps 10–12. Since ? belongs
   to the look-ahead set of state 2 I , the reduction ... ? I is chosen; furthermore, the
   topmost stack segment congruent with a string in the language L (0 I ) is found to be
   string a t (disregarding the intervening m-states in the stack).

   Step 12 shifts nonterminal I and produces a stack where a reduction is applied to
   nonterminal S. In the last PDA configuration, the input has been entirely consumed
   andthestackcontainsonlytheinitialm-state.Therefore,theparserstopsandaccepts
   the string as valid, with the syntax tree below:
   S
   I
   a t a t
   s I
   a t
   which has been grown bottom-up by the sequence of reduction moves. ?
   Unfortunately, finding the correct reduction may be harder than in the previous
   example. The next example shows the need for more information in the m-states, to
   enable the algorithm to uniquely identify the handle.


<a id="P249"></a>

   EBNF grammar G 2 machine net M 2
   S → aE | E
   0 S 1 S 2 S
   S →
   →
   a
   E
   E
   E → (aa) +
   0 E 1 E 2 E E → →
   a
   a
   a
   Fig.4.16 Grammar G 2 and network M 2 for Example4.34
   Example 4.34 (grammar requiring more information to locate handles) In this
   contrivedexample,machine M S ofnetM 2 (seeFig.4.16)recognizesbyseparatecompu-
   tationsthestrings L (0 E ) = (a a) + ofevenlengthandthoseofoddlengtha · L (1 S ).
   After processing string a a a, the stack configuration of the PDA is as below:
   top
   { 0 S 0 E } a { 1 S 1 E 0 E } a { 2 E 1 E } a { 1 E 2 E }
   An essential ability of ELR(1) parsers is to carry on multiple tentative analyzes at
   once.Aftershiftinga,thepresenceofthreestatesinm-state{ 1 S , 1 E , 0 E }saysthat
   the PDA has found three different computations for analyzing the first token a:
   0 S
   a
   − → 1 S 0 S
   ε
   − → 0 E
   a
   − → 1 E 0 S
   a
   − → 1 S
   ε
   − → 0 E
   ThePDAwillcarryonthemultiplecandidatesuntilone,andonlyone,ofthemreaches
   a final state that commands a reduction; at that moment all remaining attempts are
   abandoned.

   Observeagainthestack:sincethem-statesaremathematicalsets,thetwom-states
   { 2 E , 1 E } and { 1 E , 2 E } are identical. Let now the terminator ? be the next token;
   since state 2 E is final, a reduction ... ? E is commanded, which raises the problem
   of locating the handle in the stack. Should the reduction start from slot { 0 S , 0 E }
   or slot { 1 S , 1 E , 0 E }? Clearly there is not enough information in the stack to make
   a decision, which ultimately depends on the parity class—odd versus even—of the
   string hitherto analyzed.

   Different ways out of this uncertainty exist, and here we show how to use a
   linked list to trace simultaneous computation threads. In the stack, a pointer chain
   (solid arrows) that originates from each non-initial state of machine M E reaches

<a id="P250"></a>

   the appropriate initial state of the same machine. Similarly, a pointer chain (dashed
   arrows) is created for the transitions of machine M S . For evidence, the final states
   (here only 2 E ) are encircled and a division separates the initial and non-initial states
   (if any):
   stack
   bottom
   a a a
   stack
   top
   0 S
   0 E
   1 S
   1 E
   0 E
   1 E
   2 E
   1 E
   2 E
   With such structure, the PDA, starting from the top final state 2 E , locates the handle
   a a ? E by following the chain 2 E → 1 E → 0 E . ?
   At first glance, the parser enriched with pointers may appear to trespass the power of
   anabstractpushdownautomaton,butacloserconsiderationshowsthecontrary.Each
   pointer that originates in the stack position i points to an item present in position
   i − 1.Thenumberofsuchitemshasaboundthatdoesnotdependontheinputstring,
   but only on the grammar: an m-state can contain at most all the network states (here
   we disregard the presence of look-ahead information, which is also finite). To select
   one out of the states present in an m-state, we need a finite number of pointer values.
   Therefore a stack element is a collection of pairs (state, pointer) taken out of a finite
   numberofpossiblevalues.Thecollectionscanbeviewedasthesymbolsofthestack
   alphabet of a PDA.

   After this informal illustration of several features of deterministic parsers, in the
   next sections we formalize their construction.


## 4.6.2 Construction of ELR(1) Parsers

   Given an EBNF grammar represented by a machine net, we show how to construct
   anELR(1) parserif certain conditions aremet. The method operates inthree phases:
   1. From the net we construct a DFA, to be called a pilot automaton. 13 A pilot state,
   namedmacro-state(m-state),includesanon-emptysetofcandidates,i.e.,ofpairs
   of states and terminal tokens (look-ahead set).

   13 For the readers acquainted with the classical theory of LR(1) parsers, we give the terminological
   correspondences. The pilot is named “recognizer of viable LR(1) prefixes” and the candidates are
   “LR(1) items”. The macro-state-transition function we denote by theta (to avoid confusion with
   the state-transition function of the network machine denoted delta) is traditionally named “go to”
   function.


<a id="P251"></a>

   2. Thepilotisexaminedtochecktheconditionsfordeterministicparsing,byinspect-
   ing the components of each m-state and the arcs that go out from it. Three types
   of failure may occur: a shift-reduce conflict signifies that both a shift and a 
   reduction are possible in a parser configuration; a reduce-reduce conflict signifies that
   two or more reductions are similarly possible; and a convergence conflict occurs
   when two different parser computations that share a look-ahead token, lead to the
   same machine state.

   3. If the previous test is passed, then we construct the deterministic PDA, i.e., the
   parser.ThepilotDFAisthefinite-statecontrollerofPDA,towhichitisnecessary
   to add certain features for pointer management, which are needed to cope with
   the reductions having handles of unbounded length.

   At last, it would be a simple exercise to encode the PDA in a programming language
   of some kind.

   Now,wepreparetoformallypresenttheprocedure(Algorithm4.35)forconstruct-
   ing the pilot automaton. Here are a few preliminaries and definitions. The pilot is a
   DFA, named P, defined by the following entities:
   • the set R of m-states
   • the pilot alphabet is the union Σ ∪ V of the terminal and nonterminal alphabets,
   to be also named the grammar symbols
   • the initial m-state, I 0 , is the set: I 0 = closure (?0 S , ??), where the closure
   function was defined on p. 243
   • the m-state set R = { I 0 , I 1 , ... } and the state-transition function
   ϑ: R × (Σ ∪ V ) → R are computed starting from I 0 as next specified
   Let ? p A , ρ?, with p A ∈ Q and ρ ⊆ Σ ∪ { ? }, be a candidate and X be a grammar
   symbol.Theshiftunder X,qualifiedasterminal/nonterminaldependingonthenature
   of X, is as follows:
   ?
   ϑ (? p A , ρ?, X ) = ?q A , ρ? if the arc p A
   X
   − → q A exists
   the empty set otherwise
   For a set C of candidates, the shift under a symbol X is the union of the shifts of the
   candidates in C:
   ϑ (C, X ) =
   ?
   ∀ candidate γ∈C
   ϑ(γ, X)

<a id="P252"></a>

   Algorithm 4.35 (construction of the ELR(1)pilot graph) Computation of the
   mstate set R = { I 0 , I 1 , ... } and of the state-transition function ϑ (the algorithm uses
   auxiliary variables R ? and I ? ):
   R ? := { I 0 } - - prepare the initial m-state I 0
   - - loop that updates the m-state set and the state-transition function
   do
   R := R ? - - update the m-state set R
   - - loop that computes possibly new m-states and arcs
   for
   ?
   each m-state I ∈ R and symbol X ∈ Σ ∪ V
   ?
   do
   - - compute the base of an m-state and its closure
   I ? := closure
   ? ϑ (I, X) ?
   - - check if the m-state is not empty and add the arc
   if
   ?
   I ? ?= ∅
   ?
   then
   add arc I
   X
   − → I ? to the graph of ϑ
   - - check if the m-state is a new one and add it
   if
   ?
   I ? / ∈ R
   ?
   then
   add m-state I ? to the set R ?
   end if
   end if
   end for
   while
   ?
   R ?= R ?
   ?
   - - repeat if the graph has grown
   ?
   It helps to introduce a classification for the m-states and the pilot moves.

### 4.6.2.1 Base,Closure and Kernel of m-State

   For an m-state I the set of candidates is assigned to two classes, named base and
   closure. The base includes the non-initial candidates:
   I |base = { ?q, π ? ∈ I | q is not an initial state }
   Clearly for the m-state I ? computed in the algorithm, the base I ?
   |base
   coincides with
   the pairs computed by ϑ(I, X).

   The closure contains the remaining candidates of m-state I:
   I |closure = { ?q, π ? ∈ I | q is an initial state }
   Notice that by this definition the base of the initial m-state I 0 is empty and all other
   m-states have a non-empty base, while their closure may be empty.


<a id="P253"></a>

   Sometimes we do not need the look-ahead sets; the kernel of an m-state is the
   projection on the first component of every candidate:
   I |kernel = { q ∈ Q | ?q, π ? ∈ I }
   Two m-states that have the same kernel, i.e., differing just for some look-ahead
   sets, are called kernel-equivalent. Some simplified parser constructions to be later
   introduced rely on kernel equivalence to reduce the number of m-states. We observe
   that for any two kernel-equivalent m-states I and I ? , and for any grammar symbol
   X, the next m-states ϑ(I, X) and ϑ(I ? , X) are either both defined or neither one,
   and are kernel-equivalent.

   The next condition that may affect determinism occurs when two states within an
   m-state have outgoing arcs labeled with the same grammar symbol.

   Definition 4.36 (multiple-transition property and convergence) A pilot m-state I
   has the multiple-transition property (MTP) if it includes two candidates ?q, π ? and
   ?r, ρ? with q ?= r, such that for some grammar symbol X both transitions δ (q, X)
   and δ (r, X) are defined.

   Thenthem-state I andthetransition(i.e.,pilotarc)ϑ(I, X)arecalledconvergent
   if it holds δ (q, X) = δ (r, X). A convergent transition has a convergence conflict if
   π ∩ ρ ?= ∅, meaning that the look-ahead sets of the two candidates overlap. ?
   It is time to illustrate the previous construction and definitions by means of the
   running example.

   Example 4.37 (pilotgraphoftherunningexample) ThepilotgraphP ofthemachine
   net of Example4.25 (see Fig.4.12 on p. 237) is shown in Fig.4.17. Each m-state is
   split by a double line into the base (top) and the closure (bottom). Either part can be
   missing: I 0 has no base and I 2 has no closure. The look-ahead tokens are grouped by
   state and the final states are encircled. Macro-states I 1 and I 4 are kernel-equivalent,
   and we observe that any two equally labeled arcs leaving I 1 and I 4 reach two 
   kernelequivalent m-states. None of the m-states has the MTP, therefore no arc of the pilot
   graph is convergent. ?
   In the next section determinism is examined in depth and a determinism condition
   is formulated. A few examples illustrate the cases when the condition is satisfied or
   violated, in the possible ways.


## 4.6.3 ELR(1) Condition

   The presence of a final candidate f A in m-state I instructs the parser to examine a
   possible reduction. The look-ahead set specifies which tokens should occur next, to
   confirm the decision to reduce. Clearly to avoid a conflict, any such token should
   not label an arc going out from I. In general two or more paths can lead from the

<a id="P254"></a>

   0 E
   0 T a (
   1 E
   0 T a (
   1 T a (
   0 E )
   0 T a ( )
   2 T a ( 3 T a (
   1 E )
   0 T a ( )
   1 T a ( )
   0 E )
   0 T a ( )
   2 T a ( ) 3 T a ( )
   I 0 I 1
   I 2 I 3
   I 4
   I 5 I 6
   I 7
   I 8
   P →
   T
   a
   a
   T
   (
   (
   E )
   a
   a
   T
   ( T
   T
   (
   (
   E
   a
   )
   Fig.4.17 ELR(1) pilot graph P of the machine net in Fig. 4.12
   initial state of a machine to the same final state, therefore two or more reductions
   may be applied when the parser enters m-state I. To choose the correct reduction,
   the parser has to store additional information in the stack, as later explained; here
   too potential conflicts must be excluded. The next conditions ensure that all steps
   are deterministic.

   Definition 4.38 (ELR(1)condition) AnEBNF grammaroritsmachinenetmeetsthe
   conditionELR(1)ifthecorrespondingpilotsatisfiesthetwofollowingrequirements:

<a id="P255"></a>

   1. Every m-state I satisfies the next two clauses:
   no shift-reduce conflict:
   for all the candidates ?q, π ? ∈ I s.t. state q is final
   and for all the arcs I
   a
   − → I ? that go out from I with terminal label a
   it must hold a / ∈ π
   (4.9)
   no reduce-reduce conflict:
   for all the candidates ?q, π ?,?r, ρ? ∈ I s.t. states q,r are final
   it must hold π ∩ ρ = ∅
   (4.10)
   2. No transition of the pilot graph has a convergence conflict. ?
   If the grammar is purely BNF, then the previous condition is referred to as LR(1) 14
   instead of ELR(1). We anticipate that convergence conflicts never occur in the BNF
   grammars.

   Therunningexample(Fig.4.17)doesnothaveeithershift-reduceorreduce-reduce
   conflicts. First, consider requirement (1). Clause (4.9) is met in every m-state that
   contains a final state; for instance in I 6 , the look-ahead set associated with reduction
   is disjoint from the labels of the outgoing arcs. Clause (4.10) is trivially met as there
   is not any m-state that contains two or more final states. Second, notice that no pilot
   arc is convergent, as already observed, so also requirement (2) is satisfied.
   To clarify the different causes of conflict, we examine a few examples. To start,
   the contrived Example4.39 illustrates shift-reduce conflicts.

   Example 4.39 (shift-reduce conflicts) The grammar of Fig.4.18 has a shift-reduce
   conflict in m-state I 1 , where character b occurs in the look-ahead set of final state
   1 A and also as the label of the arc going out to m-state 2 S . ?
   Next in Example4.40, we consider a grammar with a pilot that has m-states with
   two reductions, some of which give rise to a reduce-reduce conflict.

   Example 4.40 (reduce-reduce conflicts in a nondeterministic language) Consider
   the nondeterministic language:
   L =
   ?
   a n b n | n ≥ 1
   ?
   ∪
   ?
   a 2n b n | n ≥ 1
   ?
   generated by the grammar below:
   S → A | B A → a Ab | a b B → a a B b | a a b
   Since language L is nondeterministic, every grammar that generates it necessarily
   has conflicts. Figure4.19 shows the pilot graph of the grammar. The pilot graph has
   14 The original acronym comes for “Left-to-right Rightmost” [16].


<a id="P256"></a>

   EBNF grammar
   S → Abc | ab A → a
   machine net
   4 S 3 S 0 S 1 S 2 S
   ↑
   S
   →
   a
   A b b
   c
   0 A 1 A A → →
   a
   pilot graph
   4 S 3 S
   0 S
   0 A b
   1 S
   1 A b
   2 S
   shift-reduce
   conflict
   I 4 I 3
   I 0 I 1
   I 2
   a
   A b b
   c
   Fig.4.18 Grammar with net and pilot with shift-reduce conflict (Example4.39)
   a reduce-reduce conflict in the m-state I 12 since the final states 3 A and 4 B have
   identical look-ahead sets { b }. To understand how this affects parsing, consider the
   analysis of string a a a a bb: after reading its prefix a a a a, the next token b is
   compatiblewith bothreductionsa b ? A anda a b ? B,which makesthe analysis
   nondeterministic. ?
   Finally in Example4.41 we show the case of convergent transitions and convergence
   conflicts,whichareacharacteristicofEBNF grammarsnotpossessedbypurelyBNF
   grammars.

   Example 4.41 (convergence conflicts) Consider the EBNF grammar and the
   corresponding machines in Fig.4.20. To illustrate the notion of convergent 
   transition with and without conflict, we refer to the pilot in Fig.4.20. Macro-state
   I 5 = { ?2 S , ??, ?4 S , e? } satisfies the multiple-transition property, since the two
   arcs δ (2 S , c) and δ (4 S , c) are present in the net of Fig.4.20. In the pilot graph,
   the two arcs are fused into the arc ϑ(I 5 , c), which is therefore convergent. Anyway
   there is not any convergence conflict as the two look-ahead sets { ? } and { e } are
   disjoint. Similarly m-state I 8 has the MTP, but this time there is a convergence
   conflict, because the look-ahead set is the same, i.e., { e }. The pilot in Fig.4.20 does not

<a id="P257"></a>

   EBNF grammar
   S → A | B A → a (b | Ab) B → aa (b | B b)
   machine net
   0 S 1 S
   S →
   →
   A, B
   0 A 1 A 2 A 3 A
   A →
   →
   a
   A b
   b
   0 B 1 B 2 B 3 B 4 B B → →
   a a
   B b
   b
   pilot graph
   1 S
   0 S
   0 A
   0 B
   1 A
   1 B
   0 A b
   2 A 3 A
   4 B
   3 A b
   2 B
   1 A b
   0 B
   b
   0 A
   3 B
   2 A b
   4 B
   3 A b
   4 B
   b
   3 A
   2 B
   b
   1 A
   0 B
   b
   0 A
   1 B
   b
   1 A
   0 A b
   3 B b 4 B b
   I 0 I 1
   I 2
   I 3 I 4
   I 5
   I 6
   I 7
   I 8
   I 9
   I 10
   I 11
   I 12
   reduce-reduce
   conflict
   I 13
   I 14 I 15
   a
   A | B
   a
   b
   A
   b
   A b
   B
   a
   b
   b
   a
   a
   b
   B
   b
   A
   b
   A
   Fig.4.19 Grammar with net and pilot with reduce-reduce conflict (Example4.40)

<a id="P258"></a>

   EBNF grammar
   S → ab (c | d) | bc | Ae A → aS
   machine net
   0 S 1 S 2 S
   4 S 3 S
   5 S
   S →
   →
   a
   b
   A
   b
   c, d
   c
   e
   0 A 1 A 2 A
   A →
   →
   a S
   pilot graph
   0 S
   0 A e
   I 0 1 S
   1 A e
   0 S
   e
   0 A
   I 1
   1 S
   e
   1 A
   0 S
   e
   0 A
   I 4
   2 A
   e I
   7
   4 S I 2
   2 S
   4 S e
   I 5
   2 S
   e
   4 S
   I 8
   3 S
   I 9
   3 S e
   I 10
   3 S e
   I 11
   5 S I 3 5 S e I 6
   a
   b
   A
   a
   b
   A
   S
   a
   b
   A
   S
   c
   convergence
   conflict
   d
   c
   e
   d
   c
   convergent
   e
   Fig.4.20 Grammar with net and pilot (Example4.41); the double-line arcs in the pilot are 
   convergent; the pilot has a convergence conflict

<a id="P259"></a>

   have any shift-reduce conflicts and is trivially free from reduce-reduce conflicts, but
   having a convergence conflict, it violates the ELR(1) condition. ?
   Similarly Fig.4.23 shows a grammar, its machine net representation and the 
   corresponding pilot graph. The pilot has a convergent transition that is not conflicting, as
   the look-ahead sets are disjoint.


### 4.6.3.1 Parser Algorithm

   Given the pilot DFA of an ELR(1) grammar or machine net, we explain how to
   obtain a deterministic pushdown automaton DPDA that recognizes and parses the
   sentences.

   At the cost of some repetition, we recall the three sorts of abstract machines
   involved: the net M of DFA M S , M A , …, with state set Q = { q, ... } (states are
   drawnascircularorovalnodes);thepilotDFAP withm-stateset R = { I 0 , I 1 , ... }
   (drawn as rectangular nodes); and the DPDA to be next defined. As said, the DPDA
   stores in the stack the series of m-states entered during the computation, enriched
   with additional information used in the parsing steps. Moreover the m-states are
   interleaved with grammar symbols. The current m-state, i.e., the one on top of stack,
   determines the next move: either a shift that scans the next token, or a reduction
   of a topmost stack segment (handle) to one of the final states (i.e., a nonterminal)
   included in the current m-state. The absence of shift-reduce conflicts makes the
   choice between shift and reduction deterministic. Similarly the absence of 
   reducereduce conflicts allows the parser to uniquely identify the final state of a machine.
   However this leaves open the problem to find the stack portions to be used as handle.
   For that two designs will be presented: the first uses a finite pushdown alphabet; the
   second uses unbounded integer pointers and, strictly speaking, no longer qualifies
   as a pushdown automaton.

   First we specify the pushdown stack alphabet. Since for a given net M there are
   finitely many different candidates, the number of m-states is bounded; the number
   of candidates in any m-state is bounded by | Q |, by assuming that all the candidates
   with the same state are coalesced. Moreover we assume that the candidates in an
   m-state are (arbitrarily) ordered, so that each one occurs at a position (or offset),
   whichwillbereferredtobyotherm-statesusingapointernamedcandidateidentifier
   (cid).

   The DPDA stack elements are of two types: grammar symbols, i.e., elements
   of Σ ∪ V, and stack m-states (sms). An sms, which is denoted by J, contains an
   ordered set of triples, named stack candidates, of the form:
   ?q A , π, cid? where
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   q A ∈ Q (state)
   π ⊆ Σ ∪ { ? } (look-ahead set)
   1 ≤ cid ≤ | Q | or cid = ⊥ (candidate identifier)
   where ⊥ represents the nil value for the cid pointer. For readability, a cid value will
   be prefixed by a ? marker, e.g., ?1, ?2, etc.


<a id="P260"></a>

   We introduce a natural surjective mapping, to be denoted by μ, from the set of
   sms to the set of m-states defined by: μ(J) = I if, and only if, by deleting the third
   field (cid) from the stack candidates of J, we obtain exactly the candidates of I. As
   said, in the stack the sms are interleaved with grammar symbols.

   Algorithm 4.42 (ELR(1)parserA as a DPDA) Let the current stack be as follows:
   J[0] a 1 J[1] a 2 ... a k
   top
   J[k]
   where a i is a grammar symbol.

   initialization
   The initial stack just contains the sms below:
   J 0 = {s | s = ?q, π, ⊥? for every candidate ?q, π ? ∈ I 0 }
   (thus μ(J 0 ) = I 0 , the initial m-state of the pilot).

   shift move
   Let the top sms be J and I = μ(J); and let the current token be a ∈ Σ. Assume
   that m-state I has transition ϑ(I, a) = I ? . The shift move does as follows:
   1. push token a on stack and get the next token
   2. push on stack the sms J ? computed as follows:
   J ? =
   ?
   ?
   q A ? , ρ, ?i
   ? ?q A , ρ, ?j ? is at position i in J
   and q A
   a
   − → q A ? ∈ δ
   ?
   (4.11)
   ∪
   ?
   ?0 B , σ, ⊥? ?0 B , σ ? ∈ I ? |closure
   ?
   (4.12)
   (thus μ(J ? ) = I ? )
   (note: by the last condition in (4.11), state q A ? is of the base of I ? )
   reduction move (non-initial state)
   The current stack is as follows:
   J[0] a 1 J[1] a 2 ... a k J[k]
   and let the corresponding m-states be I[i] = μ( J[i]) with 0 ≤ i ≤ k; also let the
   current token be a.

   Assume that by inspecting I[k], the pilot chooses the reduction candidate c =
   ?q A , π ? ∈ I[k], where q A is a final but non-initial state. Let t k
   = ?q A , ρ, ?i k ? ∈
   J[k] be the (only) stack candidate such that the current token a satisfies a ∈ ρ.

<a id="P261"></a>

   From i k a cid chain starts, which links stack candidate t k to a stack candidate
   t k−1 = ? p A , ρ, ?i k−1 ? ∈ J[k − 1], and so on until a stack candidate t h ∈ J[h] is
   reached that has a nil cid (therefore its state is initial):
   t h = ?0 A , ρ, ⊥?
   The reduction move does as follows:
   1. grow the syntax forest by applying the following reduction:
   a h+1 a h+2 ... a k ? A
   2. pop the following stack symbols:
   J[k] a k J[k − 1] a k−1 ... J[h + 1] a h+1
   3. execute the nonterminal shift move ϑ( J[h], A) (see below)
   reduction move (initial state)
   It differs from the preceding case in the state of the chosen candidate c =
   ?0 A , π, ⊥?, which is final and initial. The parser move grows the syntax forest
   by the reduction ε ? A and performs the nonterminal shift move corresponding
   to ϑ( I[k], A).

   nonterminal shift move
   It is the same as a shift move, except that the shifted symbol A is a nonterminal.
   The only difference is that the parser does not read the next input token at line (1)
   of Shift move.

   acceptance
   The parser accepts and halts when the stack is J 0 , the move is the nonterminal
   shift defined by ϑ( I 0 , S ) and the current token is ?. ?
   For shift moves, we note that the set computed at line (4.11) contains multiple
   candidates that have the same state whenever arc ϑ( I, a ) is convergent. It may
   help to look at the situation of a shift move (Eqs.(4.11) and (4.12)) schematized in
   Fig.4.21.

   Example 4.43 (parsing trace) The step-by-step execution of the parser on input
   string (( )a ) produces the trace shown in Fig.4.22. For clarity there are two parallel
   tracks: the input string, progressively replaced by the nonterminal symbols shifted
   on the stack; and the stack, containing stack m-states. The stack has one more entry
   than the scanned prefix; the suffix yet to be scanned is to the right of the stack. Inside
   a stack element J[k], each 3-tuple is identified by its ordinal position, starting from
   1 for the first 3-tuple; a value ?i in a cid field of element J[k + 1] encodes a pointer
   to the i-th 3-tuple of J[k].

   For readability, the m-state number appears framed in each stack element, e.g., I 0
   is denoted 0 ; the final candidates are encircled, e.g.,
   1 E . To avoid clogging, the

<a id="P262"></a>

   machine M A pilot: I = ϑ(I, a) pushdown stack: ... J aJ
   q A q A
   q A
   a
   B
   ...

   ...

   q A , π
   ...

   a
   − →
   q A , π
   ...

   0 B , σ
   ...

   ...

   1 ...

   ...

   i q A ,
   ...

   a
   q A ,
   ...

   0 B , σ,
   ...

   Fig.4.21 From left to right: machine transition q A
   a
   − → q ? A , shift move I
   a
   − → I ? and stack 
   configuration.Thecid value?i inthetopmoststackm-statepointstothecandidateatoffseti intheunderlying
   stack m-state
   look-ahead sets are not shown but can be found in the pilot graph on p. 254 (they are
   only needed for convergent transitions, which do not occur here).

   Figure4.22 highlights the shift moves as dashed forward arrows that link two
   topmost stack candidates. For instance, the first (from the figure top) terminal shift,
   on ‘(’, is ?0 T , ⊥?
   (
   − → ?1 T , ?2?. The first nonterminal shift is the third one, on E,
   i.e., ?1 T , ?3?
   E
   − → ?2 T , ?1?, soon after the null reduction ε ? E.

   Inthesamefigure,thereductionhandlesareevidencedbymeansofsolidbackward
   arrows that link the candidates involved. For instance, for reduction ( E ) ? T the
   stack configuration shows the chain of three pointers ?1, ?1 and ?3—these three
   stack elements form the handle and are popped—and finally the nil pointer ⊥—this
   stack element is the reduction origin and is not popped. The nil pointer marks the
   initial candidate ?0 T , ⊥?. In the same stack element, a dotted arrow from candidate
   ?0 T , ⊥? to candidate ?0 E , ⊥? points to the candidate from which the subsequent
   shift on T starts: see the dashed arrow in the stack configuration below. The solid
   self-loop on candidate ?0 E , ⊥? (at the 2 nd shift move on “(”—see the effect on the
   line below) highlights the null reduction ε ? E, which does not pop anything from
   the stack, and immediately triggers a shift on E.

   We observe that the parser must store on stack the scanned grammar symbols,
   because in a reduction move, at step 2 of the algorithm, they may be necessary for
   selecting the correct reduction.

   Returning to the example, the reductions are executed in the order:
   ε ? E ( E ) ? T a ? T T T ? E ( E ) ? T T ? E
   which matches a rightmost derivation but in reversed order. ?
   We examine more closely the case of convergent conflict-free arcs. Going back
   to the scheme in Fig.4.21 (p. 262), notice that the stack candidates linked by a cid
   chain are mapped onto m-state candidates that have the same machine state (here the
   stack candidate
   ?
   q ? A , ρ, ?i
   ?
   is linked via ?i to ?q A , ρ, ?j ?): the look-ahead set π
   of such m-state candidates is in general a superset of the set ρ included in the stack
   candidate, due to the possible presence of convergent transitions; the two sets ρ and

<a id="P263"></a>

   stack string to be parsed (with end-marker) and stack contents
   effect after
   bottom 1 2 3 4 5
   0
   0 E ⊥
   0 T ⊥
   ( ( ) a )
   initialization
   of the stack
   ( ( ) a )
   3
   1 T 2
   0 E ⊥
   0 T ⊥
   shift on (
   ( ( ) a )
   3
   1 T 2
   0 E ⊥
   0 T ⊥
   6
   1 T 3
   0 E ⊥
   0 T ⊥
   shift on (
   ( ( E ) a )
   3
   1 T 2
   0 E ⊥
   0 T ⊥
   6
   1 T 3
   0 E ⊥
   0 T ⊥
7 2 T 1
   reduction ε E
   and shift on E
   ( ( E ) a )
   3
   1 T 2
   0 E ⊥
   0 T ⊥
   6
   1 T 3
   0 E ⊥
   0 T ⊥
7 2 T 1 5 3 T 1 shift on )
   ( T a )
   3
   1 T 2
   0 E ⊥
   0 T ⊥
   4
   1 E 2
   0 T ⊥
   reduction
   (E ) T
   and shift on T
   ( T a )
   3
   1 T 2
   0 E ⊥
   0 T ⊥
   4
   1 E 2
   0 T ⊥
5 3 T 2 shift on a
   ( T T )
   3
   1 T 2
   0 E ⊥
   0 T ⊥
   4
   1 E 2
   0 T ⊥
   4
   1 E 1
   0 T ⊥
   reduction a T
   and shift on T
   ( E )
   3
   1 T 2
   0 E ⊥
   0 T ⊥
8 2 T 1
   reduction T T
   E
   and shift on E
   ( E )
   3
   1 T 2
   0 E ⊥
   0 T ⊥
8 2 T 1 2 3 T 1 shift on )
   T
   1
   1 E 1
   0 T ⊥
   reduction
   (E ) T
   and shift on T
   E
   reduction T E
   and accept
   (do not shift on
   E)
   Fig.4.22 Parsing steps for string (( )a ); the grammar and the ELR(1) pilot are in Figs.4.12 and
   4.17 on p. 254; the name J h of a sms maps onto the corresponding m-state μ(J h ) = I h

<a id="P264"></a>

   π coincide when no convergent transition is taken on the pilot automaton at parsing
   time.

   An ELR(1) grammar with convergent arcs is studied next.

   Example 4.44 (convergent arcs in the pilot) The net, the pilot graph, and the trace
   of a parse are shown in Fig.4.23, where for readability the cid values in the stack
   candidates are visualized as backward pointing arrows. Stack m-state J 3 contains
   two candidates, which just differ in their look-ahead sets. In the corresponding pilot
   m-state I 3 , the two candidates are the targets of a convergent non-conflicting m-state
   transition (highlighted with a double line in the pilot graph). ?
   Quite frequently the base of some m-state includes more than one candidate. Since
   this feature distinguishes the bottom-up parsers from the top-down ones to be later
   studied, we present an example (Example4.45).

   Example 4.45 (severalcandidatesinthebaseofapilotm-state) WerefertoFig.4.38
   on p. 289. The grammar below:
   S → a ∗ N N → a N b | ε
   generatesthelanguage{ a n b m | n ≥ m ≥ 0 }.Thebaseofm-states I 1 , I 2 , I 4 and I 5
   includes two candidates. The pilot satisfies the ELR(1) condition for the following
   reasons:
   • Atmostonereductioncandidateispresentinanym-state,thereforereduce-reduce
   conflicts are ruled out.

   • Noshift-reduceconflictispresent,becauseforeverym-statethatincludesareduc-
   tion candidate with look-ahead set ρ, no outgoing arc is labeled with a token that
   is in ρ. We check this: for m-states I 0 , I 1 and I 2 , the label a of the outgoing arcs
   is not included in the look-ahead sets for those m-states; for m-states I 4 and I 5 ,
   token b labeling the outgoing arcs is not in the look-ahead of the final candidates.
   A step-by-step analysis of string a a a b with the pilot in Fig.4.38 would show that
   several parsing attempts proceed simultaneously, which correspond to paths in the
   machines M S and M N of the net. ?
   Next, we show the case of non-conflicting reductions in the same m-state.

   Example 4.46 (multiple reductions in an ELR(1)m-state) Figure4.24 shows the
   grammar of a finite language, along with its machine network representation and the
   correspondingpilotautomaton.InthepilotofFig.4.24,them-states I 3 and I 6 include
   two final candidates, but no reduce-reduce conflict is present as the look-ahead sets
   of any two final candidates that are in the same m-state are disjoint. Referring to
   m-state I 3 , if symbol ? is scanned after reading a letter a, then the reduction a ? A
   must be executed, corresponding to derivation S ⇒ A ⇒ a; instead, if a second a is

<a id="P265"></a>

   machine net
   2 S
   0 S 1 S 4 S 0 A 1 A 2 A
   3 S
   A →
   →
   b
   e
   e
   S →
   →
   a
   A
   b
   d
   A
   pilot graph
   0 S
   1 S
   0 A d
   3 S
   1 A d
   0 A
   2 A d
   2 S 4 S
   I 0
   I 1
   I 2
   I 3
   I 4 I 5
   a b
   e
   convergent
   arc
   A
   A
   d
   parse traces
   J 0 a J 1 b J 2 e J 3 d
   0 S
   1 S
   0 A d
   3 S
   1 A d
   0 A
   2 A d
   2 A
   J 0 a J 1 A J 4 d J 5
   0 S
   1 S
   0 A d
   2 S 4 S
   reductions be A (above) and aAd S (below)
   Fig. 4.23 ELR(1) net and pilot with convergent arcs (double line), and parsing trace of string
   a bed ?, for Example4.44

<a id="P266"></a>

   EBNF grammar
   S → B a | A | b (B | Aa) A → a B → a
   machine net
   1 S
   0 S 4 S
   2 S 3 S
   S →
   →
   A
   b
   A
   a
   B
   B a
   0 A 1 A
   A →
   →
   a
   0 B 1 B
   B →
   →
   a
   pilot graph
   0 S
   0 A
   0 B a
   1 S 4 S
   1 A
   1 B a
   1 A a
   1 B
   2 S
   0 A a
   0 B
   3 S
   I 0
   I 1
   I 2
   I 3
   I 4
   I 5
   I 6
   B a
   A
   a
   b
   A
   a
   a
   B
   Fig.4.24 Grammar with net and pilot with non-conflicting reductions for Example4.46
   scannedafterthefirstone,thenthecorrectreductionisa ? B becausethederivation
   is S ⇒ B a ⇒ a a.

   Similarly but with reference to m-state I 6 , if the input symbol is ? after reading
   string ba, then a reduction a ? B must be performed and the derivation is S ⇒
   b B ⇒ ba; otherwise, if after prefix ba the input is a, then the correct reduction is
   a ? A and the derivation is S ⇒ b Aa ⇒ ba a. ?

<a id="P267"></a>


## 4.6.4 Simplifications forBNF Grammars

   Historically, the original theory of shift-reduce parsers applied only to pure BNF
   grammars and, although attempts were made to extend it to EBNF grammars, to our
   knowledge the present one is the first systematic presentation of ELR(1) parsers,
   based on the machine net representation of EBNF grammars. Since the widespread
   parser generation tools (such as Bison) as well as the best known textbooks still use
   non-extended grammars, we briefly comment on how our approach relates to the
   classical theory. 15 The reader more interested to practical parser design should jump
   to p. 270, where we indicate the useful simplifications made possible by pure BNF
   grammars.


### 4.6.4.1 ELR(1)Versus Classical LR(1) Definitions

   In a BNF grammar each nonterminal A has finitely many alternative rules A → α |
   β | ....Sinceanalternativedoesnotcontainiterative,i.e.,star,orunionoperations,
   it is straightforward to represent nonterminal A as a nondeterministic finite machine
   (NFA) N A that has a very simple acyclic graph: from the initial state 0 A as many
   legs originate as there are alternative rules for A. Each leg exactly reproduces an
   alternative and ends in a different final state. Therefore the graph has the form of
   a tree, such that the only one branching node is the initial state. Such a machine
   is nondeterministic if two alternatives start with the same grammar symbol. We
   compare such an NFA with the DFA machine M A we would build for the same set of
   alternativerules.FirstweexplainwhytheLR(1)pilotconstructedwiththetraditional
   method is substantially equivalent to our pilot from the point of view of its use in
   a parser. Then we show that no m-state in the traditional LR(1) pilot can exhibit
   the multiple-transition property (MTP). Therefore the only pertinent conditions for
   parser determinism are the absence of shift-reduce and reduce-reduce conflicts; see
   clauses (4.9) and (4.10) of Definition4.38 on p. 254.

   Our machine M A may differ from N A in two ways: determinism and minimality.
   First, machine M A is assumed to be deterministic, but this assumption is made out
   of convenience, not of necessity. For instance, consider a nonterminal C with two
   alternatives: C → if E then I | if E then I else I. Determinization has the effect
   of normalizing the alternatives by left factoring the longest common prefix; this
   yields the equivalent EBNF grammar C → if E then I (ε | else I ). The pilots
   constructed for the original and for the extended grammar are equivalent in the sense
   that either both ones have a conflict or neither one has it.

   Second, we allow and actually recommend that the graph of machine M A be
   minimal with respect to the number of states, provided that the initial state is not
   reentered. Clearly in the graph of machine N A , no arc may reenter the initial state,
   15 We mainly refer to the original paper by Knuth [16] and to its dissemination in many classical
   textbooks such as [17,18]. There have been numerous attempts to extend the theory to EBNF
   grammars; we refer to the method of Borsotti et al. [19] which includes further references.

<a id="P268"></a>

   grammars
   BNF EBNF
   S → abc | abd | bc | Ae
   A → aS
   S → ab (c | d) | bc | Ae
   A → aS
   machine nets
   1 S 2 S 3
   1S
   0 S 1 S 2 S 3
   2S
   4 S 3
   3S
   5 S 3
   4S
   S →
   →
   →
   →
   →
   a
   a
   b
   A
   b
   b
   c
   d
   c
   e
   0 S 1 S 2 S
   4 S 3 S
   5 S
   S →
   →
   a
   b
   A
   b
   c, d
   c
   e
   net common part
   0 A 1 A 2 A
   A →
   →
   a S
   Fig.4.25 BNF and EBNF grammars and their machine nets { M S , M A }
   exactly as in machine M A ; but the final states of the grammar alternatives are all
   distinct in N A , while in M A they are merged together by determinization and state
   minimization. Moreover, also several non-final states of N A are possibly coalesced
   if they are undistinguishable for M A .

   The effect of such a state reduction on the pilot is that some pilot arcs may
   become convergent. Therefore in addition to checking conditions (4.9) and (4.10),
   Definition4.38 imposes that any convergent arc be free from conflicts. Since this
   point is quite subtle, we illustrate it by the next example.

   Example 4.47 (state-reduction and convergent transitions) Consider the equivalent
   BNF and EBNF grammars and the corresponding machines in Fig.4.25. After 
   determinizing, the states 3
   1S
   , 3
   2S
   , 3
   3S
   , 3
   4S
   of machine N S are equivalent and are merged
   intothestate3 S ofmachine M S .TurningourattentiontotheLR andELR conditions,
   wefindthattheBNF grammarhasareduce-reduceconflictcausedbythederivations
   below:
   S ⇒ Ae ⇒ a S e ⇒ a bce and S ⇒ Ae ⇒ a S e ⇒ a a bce
   On the other hand, for the EBNF grammar the pilot P { M S , M A } , already shown in
   Fig.4.20 on p. 258, has two convergent arcs (double line), one with a conflict and the

<a id="P269"></a>

   other without. Arc I 8
   c
   − → I 11 violates the ELR(1) condition because the look-ahead
   sets of 2 S and 4 S in I 8 are not disjoint. ?
   Thus, the state minimization of the net machines may cause the ELR pilot to have a
   convergence conflict whereas the LR pilot has a reduce-reduce conflict. On the other
   hand, shift-reduce conflicts equally affect the LR and ELR pilots.

   From these findings, it is tempting to assert that, given a BNF grammar and an
   equivalentnet,eitherbothonesareconflict-freeorbothoneshaveconflicts(although
   not always of the same type). But the assertion needs to be made more precise, since
   starting from an ELR(1) net, several equivalent BNF grammars can be obtained by
   removing the regular expression operations in different ways. Such grammars may
   or may not be LR(1), but at least one of them is LR(1): the right-linearized grammar
   (p. 239). 16
   To illustrate the discussion, it helps us consider a simple example where the
   machine net is ELR(1), yet another equivalent grammar obtained by a fairly natural
   transformation, has conflicts.

   Example 4.48 (grammar transformations may not preserve ELR(1)) A phrase S
   has the structure E (s E ) ∗ , where a construct E has either the form b + b n e n or the
   form b n e n e, with n ≥ 0. The language is defined by the EBNF grammar and net
   in Fig.4.26, which are ELR(1). On the contrary, there is a conflict in the equivalent
   BNF grammar below:
   S → E s S | E E → B F | F e F → b F e | ε B → b B | b
   In fact, consider the corresponding net (only the part that has changed is redrawn)
   and its partial pilot graph (the whole pilot has many more m-states and arcs which
   here do not matter) shown in Fig.4.27. The m-state I 1 has a shift-reduce conflict due
   to the indecision whether in analyzing a legal string of the language such as bbe,
   the parser, after shifting the first character b, should perform a reduction b ? B or
   shift the second character b.

   On the other hand, the right-linearized grammar below (derived from the original
   ELR(1) net), which is BNF by construction:
   0 S → 0 E 1 S 0 E → b 1 E | 0 F 3 E 0 F → ε | b 1 F
   1 S → ε | s 2 S 1 E → b 1 E | 0 F 2 E 1 F → 0 F 2 F
   2 S → 0 E 1 S 2 E → ε 2 F → e 3 F
   3 E → e 4 E 3 F → ε
   4 E → ε
   16 A rigorous presentation of these and other theoretical properties is in [20].

<a id="P270"></a>

   EBNF grammar
   S → E (sE ) ∗ E → b + F | F e F → bF e | ε
   machine net
   0 S 1 S 2 S
   S →
   ↓
   E
   s
   E
   4 E 3 E 0 E 1 E 2 E
   E
   ↓
   → ←
   b F F
   b
   e
   0 F 1 F 2 F 3 F F →
   ↓
   →
   b F e
   Fig.4.26 An EBNF grammar that is ELR(1) (Example4.48)
   is surely of type LR(1). Notice in fact that it postpones any reduction decision as
   long as possible, and thus avoids any conflicts. ?

### 4.6.4.2 Superfluous Features

   ForLR(1)grammarsthatdonotuseregularexpressions,somefeaturesoftheELR(1)
   parsing algorithm (p. 260) become superfluous. We briefly discuss them to highlight
   thedifferencesbetweenextendedandbasicshift-reduceparsers.Sincenowthegraph
   of every machine is a tree, two arcs originating from distinct states may not enter the
   same state, therefore in the pilot the convergent arcs never occur. Moreover, if the
   alternatives A → α | β | ..., are recognized in distinct final states q α, A , q β, A ,
   ... of machine M A , the bookkeeping information needed for reductions becomes
   useless:ifthecandidatechosenbytheparsercorrespondstostateq α, A ,thereduction
   move simply pops 2 × |α| stack elements and performs the reduction α ? A. Since
   the pointers to a preceding stack candidate are no longer needed, the stack m-states
   coincide with the pilot ones.

   Second, for a related reason the parser can do without the interleaved grammar
   symbols on the stack, because the pilot of an LR(1) grammar has the well-known
   property that all the arcs entering an m-state carry the same terminal/nonterminal
   label. Therefore, when in the current m-state the parser decides to reduce by using a

<a id="P271"></a>

   machine net (partial)
   0 S 1 S 2 S 3 S S →
   ↓
   →
   E s S
   ...

   0 E 1 E 2 E
   E
   ↓
   →
   B F F
   0 B 1 B 2 B
   B →
   ↓
   →
   b B
   pilot graph (partial)
   0 S
   0 E s
   0 B b s
   0 F
   e
   1 B
   b s
   1 F e
   0 B b s
   0 F
   e
   ...

   I 0
   I 1
   shift-reduce
   conflict
   b b
   Fig.4.27 An equivalent network that has conflicts (Example4.48)
   certain final candidate, the reduction handle is uniquely determined by the final state
   contained in the candidate.

   The above simplifications have another consequence: for BNF grammars the use
   of machine nets and their states becomes subjectively less attractive than the classical
   notation based on marked grammar rules (p. 236).


## 4.6.5 Parser ImplementationUsing aVector Stack

   Beforefinishingwithbottom-upparsing,wepresentaparserimplementationalterna-
   tive to Algorithm4.42 on p. 260, where the memory of the analyzer is a vector stack,
   i.e., an array of elements such that any element can be directly accessed by means of
   an integer index. There are two reasons for presenting the new implementation: this
   technique anticipates the implementation of nondeterministic tabular parsers (Sect.
   4.11); and it is potentially faster. On the other hand, a vector stack as data type is

<a id="P272"></a>

   moregeneralthanapushdownstack,thereforethisparsercannotbeviewedasapure
   DPDA.

   As before, the elements in the vector stack are of two alternating types: 
   vectorstack m-states, vsms and grammar symbols. A vsms, denoted by J, is a set of pairs,
   namedvector-stackcandidates,theform?q A , elemid?ofwhichsimplydiffersfrom
   the earlier stack candidates because the second component is a positive integer named
   element identifier (elemid) instead of the candidate identifier (cid). Notice also that
   nowthesetisnotordered.Thesurjectivemappingfromvector-stackm-statestopilot
   m-statesisdenotedμ,asbefore.Eachelemid pointsbacktothevector-stackelement
   containing the initial state of the current machine, so that when a reduction move is
   performed, the length of the string to be reduced—and the reduction handle—can be
   obtaineddirectlywithoutinspectingthestackelementsbelowthetopone.Clearlythe
   value of elemid ranges from 1 to the maximum vector-stack height, which linearly
   depends on the input length.

   Algorithm 4.49 (ELR(1)parserthatusesavectorstack) Thecurrent(vector-)stack
   is denoted by:
   J[0] a 1 J[1] a 2 ... a k J[k]
   with k ≥ 0, where the top element is J[k].

   initialization
   The analysis starts by pushing on the stack the following element:
   J 0 = { s | s = ?q, π, 0? for every candidate ?q, π ? ∈ I 0 }
   shift move
   Assume that the top element is J with I = μ(J), the variable k stores the index
   of the top stack element, and the current input character is a. Assume that by
   inspecting I, the shift ϑ( I, a ) = I ? has been chosen. The parser move does the
   following:
   1. push token a on stack and get next token
   2. push on stack the vsms J ? (more precisely do k + + and J[k] := J ? ) that
   contains the following candidates:
   if
   cand. c = ?q A , ρ, i ? ∈ J and
   q A
   a
   − → q ? A ∈ δ
   then
   J ? contains cand.

   ?
   q A ? , ρ, i
   ?
   end if
   if
   cand. c = ?0 A , π ? ∈ I ?
   |closure
   then
   J ? contains cand. ?0 A , π, k ?
   end if
   (thus μ(J ? ) = I ? )

<a id="P273"></a>

   nonterminal shift move
   It applies after a reduction move that started in a final state of machine M B . It is
   the same as a shift move, except that the shifted symbol B is a nonterminal, i.e.,
   ϑ( I, B ) = I ? . The difference with respect to a shift move is that the parser does
   not read the next token.

   reduction move (non-initial state)
   The stack is as follows:
   J[0] a 1 J[1] a 2 ... a k J[k]
   and let the corresponding m-states be I[0] I[1] ... I[k]. Assume that the pilot
   chooses the reduction candidate c = ?q A , π ? ∈ I[k], where q A is a final but
   non-initial state of machine M A ; let ?q A , ρ, h ? be the (only) candidate in J[k]
   corresponding to c. The reduction move does:
   1. pop the following vector-stack elements:
   J[k] a k J[k − 1] a k−1 ... J[h + 1]
   2. build a portion of the syntax forest by applying the following reduction:
   a h+1 ... a k ? A
   3. execute the nonterminal shift move ϑ( J[h], A)
   reduction move (initial state)
   It differs from the preceding case in that, for the chosen reduction candidate
   c = ?0 A , π, i ? ∈ J[k], state 0 A is initial and final; then necessarily the elemid
   field i is equal to k, the current top stack index. Therefore reduction ε ? A has
   to be applied. The parser move does the following:
   1. build a portion of the syntax forest corresponding to this reduction
   2. go to the nonterminal shift move ϑ( I, A)
   acceptance
   The parser accepts and halts when the stack is J 0 , the move is the nonterminal
   shift defined by ϑ( I 0 , S ) and the current token is ?. ?
   Although the algorithm uses a stack, it cannot be viewed as a PDA because the stack
   alphabet is unbounded since a vsms contains integer values.

   Example 4.50 (parsing trace) Figure4.28, to be compared with Fig.4.22 on p. 263,
   shows the same step-by-step execution of the parser as in Example4.43, on input
   string ( ( ) a ); the graphical conventions are the same.


<a id="P274"></a>

   stack bottom string to be parsed (with end-marker) and stack contents (with indices)
   effect after
   0 1 2 3 4 5
   0
   0 E 0
   0 T 0
   ( ( ) a )
   initialization
   of the stack
   ( ( ) a )
   3
   1 T 0
   0 E 1
   0 T 1
   shift on (
   ( ( ) a )
   3
   1 T 0
   0 E 1
   0 T 1
   6
   1 T 1
   0 E 2
   0 T 2
   shift on (
   ( ( E ) a )
   3
   1 T 0
   0 E 1
   0 T 1
   6
   1 T 1
   0 E 2
   0 T 2
7 2 T 1
   reduction ε E
   and shift on E
   ( ( E ) a )
   3
   1 T 0
   0 E 1
   0 T 1
   6
   1 T 1
   0 E 2
   0 T 2
7 2 T 1 5 3 T 1 shift on )
   ( T a )
   3
   1 T 0
   0 E 1
   0 T 1
   4
   1 E 1
   0 T 2
   reduction
   (E ) T
   and shift on T
   ( T a )
   3
   1 T 0
   0 E 1
   0 T 1
   4
   1 E 1
   0 T 2
5 3 T 2 shift on a
   ( T T )
   3
   1 T 0
   0 E 1
   0 T 1
   4
   1 E 1
   0 T 2
   4
   1 E 1
   0 T 3
   reduction a T
   and shift on T
   ( E )
   3
   1 T 0
   0 E 1
   0 T 1
8 2 T 0
   reduction T T
   E
   and shift on E
   ( E )
   3
   1 T 0
   0 E 1
   0 T 1
8 2 T 0 2 3 T 0 shift on )
   T
   1
   1 E 0
   0 T 1
   reduction
   (E ) T
   and shift on T
   E
   reduction T E
   and accept
   (do not shift on
   E)
   Fig. 4.28 Tabulation of parsing steps for string (( )a ); the parser uses a vector stack and is
   generated by the grammar in Fig.4.12 with the ELR (1) pilot in Fig. 4.17

<a id="P275"></a>

   In every vsms on the stack, the second field of each candidate is the elemid index,
   which points back to some inner position of the stack. Index elemid is equal to the
   current stack position for all the candidates in the closure part of a stack element,
   and it points to some previous position for the candidates of the base. The reduction
   handles are highlighted by means of solid backward pointers, and by a dotted arrow
   to locate the candidate to be shifted soon after reducing. Notice that now the arrows
   span a longer distance than in Fig.4.22, as index elemid goes directly to the origin
   of the reduction handle. The forward shift arrows are the same as those in Fig.4.22
   and are not shown.

   The results of the analysis, i.e., the execution order of the reductions and the
   obtained syntax tree, are identical to those of Example4.43.

   To complete the exemplification, we apply the vector-stack parser to a previous
   net featuring a convergent arc (Fig.4.23 on p. 265): the parsing traces are shown
   in Fig.4.29, where the integer pointers are represented by backward pointing solid
   arrows. ?

## 4.6.6 Lengthening the Look-Ahead

   More often than not, technical grammars meet the ELR(1) or LR(1) condition, but
   one may find a grammar that needs some adjustment. Here we consider certain
   grammar transformations for BNF grammars and for simplicity we omit the EBNF
   case that, to our knowledge, has been less investigated. However, remember that any
   EBNF grammar can always be transformed into an equivalent BNF one: for instance
   into the right-linearized form; see Sect. 4.5.2.1 on p. 239.

   In Sect. 4.6 on p. 244, we have intuitively hinted to the meaning of a look-ahead
   of length two (or more): two (or more) consecutive characters are inspected to let the
   parsermovedeterministically.Forinstance,supposethatagrammarhasanalternative
   for the axiom: S → Acd | B ce; and that both nonterminals A and B are nullable,
   i.e., A → ε and B → ε. Then in the initial m-state I 0 of the pilot, there obviously is
   a conflict between the reductions of A and B, since both of them have look-ahead
   c if we look at the first following character, that is if we set k = 1. But, if we look
   at the next two consecutive characters that may follow nonterminals A and B, i.e.,
   if we set k = 2, then we, respectively, see strings cd and ce, which are different,
   and the conflict vanishes. The situation would be more involved if d and e were
   nonterminals, say D and E, since then we should inspect their initial characters; but
   the next examples will help to understand. To sum up, a BNF grammar is of type
   LR(k), for k ≥ 1, if it has a deterministic bottom-up parser that uses a look-ahead
   of length k.

   An interesting case to discuss is when a grammar is of type LR, though with a
   look-ahead of two or more, i.e., LR(k) with k > 1. Then, we may wish to transform
   the grammar into an equivalent one with lower look-ahead length, possibly down to
   k = 1. In principle, this target can always be achieved; see Sect. 4.9 on p. 346. Here

<a id="P276"></a>

   pilot graph
   0 S
   1 S
   0 A d
   3 S
   1 A d
   0 A
   2 A d
   2 S 4 S
   I 0
   I 1
   I 2
   I 3
   I 4 I 5
   a b
   e
   convergent
   arc
   A
   A
   d
   parse traces
   J 0 a J 1 b J 2 e J 3 d
   0 S
   1 S
   0 A d
   3 S
   1 A d
   0 A
   2 A d
   2 A
   J 0 a J 1 A J 4 d J 5
   0 S
   1 S
   0 A d
   2 S 4 S
   reductions be A (above) and aAd S (below)
   Fig.4.29 Parsing steps of the parser using a vector stack for string a bed ? recognized by the net
   in Fig.4.23, with the pilot here reproduced for convenience

<a id="P277"></a>

   we show a few useful BNF grammar transformations from LR(2) down to LR(1),
   which do not pretend to be exhaustive.

   From Sect. 4.6.4.1 we know that the net machines of BNF grammars do not
   contain any circuits and that their state-transition graphs are trees, where the
   rootto-leaf paths of a machine M A are in one-to-one correspondence with the alternative
   rules of nonterminal A; see for instance the machines in Fig.4.31. First we introduce
   twotransformationsusefulforloweringthevalueofthelook-aheadlengthparameter
   k, then we hint a possible transformation for non-LR(k) grammars. 17
   Grammar with Reduce-Reduce Conflict
   Suppose a BNF grammar is of type LR(2), but not LR(1). The situation is as in
   Fig.4.30,whereweseetworules A → αand B → β,theirmachinesandanm-state
   of the pilot. We consider a reduce-reduce conflict, which is apparent as the m-state
   contains two reduction candidates, with final states f A and f B , that have the same
   look-ahead, say character a. Since the grammar is BNF, it is convenient to represent
   machine states with marked rules as mentioned in Sect. 4.5 on p. 237, enriched with
   the look-ahead. In this case we have:
   reduction reduction representation
   A → α •, { a } B → β •, { a } marked rule
   ? f A , a ? ? f B , a ?
   state candidate
   Both marked rules denote reductions as the bullet • is at their end and have the same
   look-ahead set { a }. However, since the grammar is assumed to be of type LR(2),
   the candidates are surely discriminated by the characters that follow the look-ahead
   character a.

   To obtain an equivalent LR(1) grammar, we apply a transformation called early
   scanning. The idea is to lengthen the right part of certain rules by appending to them
   the character a causing the conflict. In this way, the new rules will have in their
   lookahead sets the characters that follow symbol a, which from the LR(2) hypothesis do
   not overlap.

   More precisely, the transformation introduces two new nonterminals ? Aa ?,
   ? B a ?, and their rules, as below:
   ? Aa ? → αa ? B a ? → β a
   For preserving grammar equivalence, we then have to adjust the rules containing
   nonterminals A or B in their right parts. The transformation must ensure that the
   derivation below:
   ? Aa ?
   +
   = ⇒ γ a
   17 For a broader discussion of all such grammar transformations, we refer to [21].

<a id="P278"></a>

   BNF rules corresponding machines m-state of pilot graph
   A → α
   B → β
   0 A f A
   0 B f B
   A →
   B →
   →
   →
   α
   path from 0 A to f A
   with label sequence α
   β
   path from 0 B to f B
   with label sequence β
   ... ...

   f A
   a
   ... ...

   f B
   a
   ... ...

   reduce-reduce
   conflict
   Fig.4.30 BNF grammar not of type LR(1) with a reduce-reduce conflict
   exists if, and only if, the original grammar has the following derivation:
   A
   +
   = ⇒ γ
   andcharactera canfollownonterminal A.ThenextExample4.51,withbothmarked
   rules and pilot graphs, instances an early scanning case.

   Example 4.51 (early scanning) Consider the BNF grammar G 1 below:
   ?
   S → Abb
   S → B bc
   ?
   A → a A
   B → a B
   ?
   A → a
   B → a
   Grammar G 1 is of type LR(2), but not LR(1) because it clearly has a reduce-reduce
   conflictbetweenmarkedrules A → a •, { b }and B → a •, { b }:thelook-aheadis
   b forbothones.ThesituationisasinFig.4.31,wheretheconflictappearsinthepilot
   m-state I 1 . By increasing the length parameter to k = 2, we see that such reductions,
   respectively, have look-ahead { bb } and { bc }, which are disjoint strings. Thus, by
   applying early scanning, we obtain the LR(1) grammar below:
   S → ? Ab? b ? Ab? → a ? Ab? ? Ab? → a b
   S → ? B b? c ? B b? → a ? B b? ? B b? → a b
   For the two reductions, the marked rules are ? Ab? → a b•, { b } and ? B b? →
   a b•, { c }, which do not conflict because their look-ahead sets are disjoint. Now
   the situation becomes as in Fig.4.32, where no conflict occurs in the m-states I 1 or
   I 2 , which replace the conflicting one in Fig.4.31. Therefore, after the early scanning

<a id="P279"></a>

   machine net
   3 S 2 S 3 S 0 S 4 S 5 S 6 S
   S
   ↓
   ← →
   S → Abb S → B bc
   A b b B b c
   0 A 1 A 2 A A → →
   ↓
   A → a
   A → aA
   a A
   0 B 1 B 2 B B → →
   ↓
   B → a
   B → aB
   a B
   partial pilot graph
   ...

   ...

   0 S
   0 A
   b
   0 B
   1 A
   b
   1 B
   0 A
   b
   0 B
   ...

   ...

   I 0
   I 1
   reduce-reduce
   conflict
   a
   a
   A
   B
   A
   B
   Fig.4.31 BNF grammar G 1 of Example4.51 before early scanning
   transformation,thereduce-reduceconflicthasvanishedandthegrammarhasbecome
   of type LR(1). ?
   Grammar with a Shift-Reduce Conflict
   Another common source of conflict occurs when an m-state of the pilot contains two
   marked rules, which are a shift and a reduction candidate, as below:
   A → α • a β, π B → γ •, { a }
   Thesecandidatescauseashift-reduceconflict,duetocharactera:ashiftoncharacter
   aisenabledbycandidate A → α • a β;andcharacterabelongstothelook-aheadset
   { a } of the reduction candidate B → γ • (the look-ahead set π of the shift candidate
   is not relevant for the conflict).

   How to fix the conflict: create a new nonterminal ? B a ? to do the same service as
   nonterminal B followed by terminal a; and replace rule B → γ with rule ? B a ? →
   γ a. For consistency, adjust all the rules having nonterminal B in their right parts so

<a id="P280"></a>

   machine net
   2 S 1 S 0 S 3 S 4 S
   S
   ↓
   ← →
   Ab
   b
   B b
   c
   2
   Ab
   0
   Ab
   1
   Ab
   3
   Ab
   Ab
   →
   →
   a
   Ab
   b
   2
   B b
   0
   B b
   1
   B b
   3
   B b
   B b
   →
   →
   a
   B b
   b
   partial pilot graph
   ...

   ...

   0 S
   0
   Ab
   b
   0
   B b
   c
   1
   A b
   b
   1
   B b
   c
   0
   A b
   b
   0
   B b
   c
   ...

   3
   A b
   b
   3
   B b
   c
   ...

   I 0
   I 1
   I 2
   no conflict
   no conflict
   a
   a
   Ab
   B b
   Ab
   B b
   b
   Fig.4.32 BNF grammar G 1 of Example4.51 after early scanning
   as to preserve grammar equivalence. For instance, if there is a rule like X → B a c,
   replace it with rule X → ? B a ? c. Since we assume the grammar is of type LR(2),
   the look-ahead set of rule ? B a ? → γ a may not include character a, 18 and so the
   conflict disappears.

   The transformation is more involved if the character a, which causes the conflict
   between candidates A → α • a β, π and B → γ •, { a }, is produced as initial
   character by a derivation from a nonterminal C, which immediately follows 
   nonterminal B in a sentential form, like:
   S
   +
   = ⇒ ... B C ...

   +
   = ⇒ ... B a ... ...

   18 More generally, in this case the LR(2) assumption implies that the look-ahead set (with k = 1)
   of rule ? B a ? → γ a and the set Ini
   ?
   L (β) · π
   ?
   may not overlap.


<a id="P281"></a>

   Transformation: create a new nonterminal, named ?a/ L C ?, to generate the strings
   obtained from those derived from C by cutting prefix a. Formally:
   ?a/ L C ?
   +
   = ⇒ γ if and only if C
   +
   = ⇒ a γ in the original grammar
   Then, the affected grammar rules are adjusted so as to preserve equivalence. Now,
   the grammar obtained is amenable to early scanning, which will remove the conflict.
   This transformation is called left quotient, since it is based on the operation / L of
   the same name, defined on p. 17.

   ThenextExample4.52showsthetransformation.Forbrevity,werepresentitonly
   with marked rules.

   Example 4.52 (leftquotientpreparationforearlyscanning)ConsidertheBNF
   grammar G 2 below:
   S → Ad A → a b
   S → B C B → a C → bC C → c
   Grammar G 2 is of type LR(2), but not LR(1) as evidenced by the following 
   shiftreduce conflict, due to character b:
   A → a • b, { d } B → a •, { b, c }
   After the left quotient transformation applied to nonterminal C, we obtain the
   grammar below (nonterminal C is left-quotiented by b and c):
   S → Ad A → a b
   S → B b ?b/ L C ? B → a ?b/ L C ? → b ?b/ L C ? ?b/ L C ? → c
   S → B c ?c/ L C ? ?c/ L C ? → ε
   Now, the shift-reduce conflict can be removed by applying the early scanning 
   transformation, as done in Example4.51, by means of two more nonterminals ? B b? and
   ? B c?, as follows:
   S → A d A → a b
   S → ? B b? ?b/ L C ? ? B b? → a b ?b/ L C ? → b ?b/ L C ? ?b/ L C ? → c
   S → ? B c? ?c/ L C ? ? B c? → a c ?c/ L C ? → ε
   The shift-reduce conflict has vanished, and there are not reduce-reduce conflicts.
   In fact, the only rules that reduce together are A → a b•, { d } and ? B b? →
   a b•, { b, c}, but their look-ahead sets (with k = 1) are disjoint. Therefore, the
   grammar has the LR(1) property. ?

<a id="P282"></a>

   To sum up, grammar transformations based on left quotient and early scanning can
   help to lower the value of k, for a given grammar meeting the LR condition with
   parameter k larger than one. Actually, such transformations are special instances
   of an important theoretical result: for any LR(k) grammar with k > 1, there is an
   equivalent LR (1) grammar (see below Property4.94 on p. 349). Notice, however,
   that the LR (1) grammar may be much larger than the given LR(k) grammar.

   Transformation of Non-LR(k) Grammars
   We have seen some transformations that allow to reduce the look-ahead length k of
   a given LR(k) grammar. Unfortunately, if the language is deterministic but the given
   grammar is not of type LR(k) for some k ≥ 1, as for instance if it is ambiguous,
   then we do not have any systematic way to turn the grammar into an LR(k) one
   for some k ≥ 1. It remains the possibility to study the languages generated by the
   nonterminals involved in parsing conflicts, to identify conflict causes and finally to
   adjust such subgrammars for the LR(1) property.

   Quite often, an effective grammar transformation is to turn left-recursive rules or
   derivations into right-recursive ones; see Sect. 2.5.13.8 on p. 76. The reason stems
   from the fact that a LR parser carries on multiple computations until one of them
   reachesareduction,whichisdeterministicallydecidedandapplied.Aright-recursive
   rule (more generally a derivation) has the effect of delaying the decision moment,
   whereas a left-recursive rule does the opposite. In other words, with right-recursive
   rules the automaton accumulates more information on stack before reducing. Truly,
   the stack grows larger with right-recursive rules, but such an increase in memory
   occupation is negligible for modern computers.

4.7 DeterministicTop-Down Parsing
=================================

   Asimplerandveryflexibletop-downparsingmethod,traditionallycalledELL(1), 19
   applies if an ELR(1) grammar satisfies further conditions. Although less general
   than the ELR(1), this method has several assets, primarily the ability to anticipate
   parsingdecisionsthusofferingabettersupportforsyntax-directedtranslation,andto
   be implemented by a neat modular structure made of recursive procedures that mirror
   the graphs of network machines. We informally introduce the idea by an example.
   19 The acronym means Extended, Left to right, Leftmost, with length of look-ahead equal to one.
   Deterministictop-downparserswereamongthefirsttobeconstructedbycompilationpioneers,and
   their theory for BNF grammars was shortly after developed by [22,23]. A sound method to extend
   such parsers to EBNF grammars was popularized by [24] recursive descent compiler, systematized
   in the book [25], and included in widely known compiler textbooks, e.g., [17], where top-down
   deterministic parsing is presented independently of shift-reduce methods. Taking a novel approach
   from[26],thissectionshowsthattop-downparsingforEBNF grammarsisacorollaryoftheELR(1)
   parserconstructionthatwehavepresentedabove.ForpureBNF grammars,therelationshipbetween
   LR(k) and LL(k) grammar and language families has been carefully investigated in the past, in
   particular by [27].


<a id="P283"></a>

   machine net
   0 S 1 S 2 S S →
   ↓
   {
   P
   ‘,’
   { ‘,’ }
   P
   0 P 3 P
   1 P 2 P
   P →
   →
   a { a }
   P
   a
   c
   { c }
   Fig. 4.33 Machine net for lists of balanced strings (Example4.53). On the arcs and final darts
   exiting furcating nodes, the tokens within braces (so-called guide sets, see p. 299 or p. 307) tell the
   parser which branch to follow
   Example 4.53 (list of balanced strings) The machine net in Fig.4.33, equivalent to
   the EBNF grammar:
   S → P
   ?
   ‘, ’ P
   ? ∗
   P → a P a | c
   generatesthelanguage{ a n ca n | n ≥ 0 }
   ?
   ‘, ’ { a n ca n | n ≥ 0 }
   ? ∗ .Intuitively,
   a deterministic top-down parser is a goal-oriented or predictive algorithm. Given a
   source string x, the initial goal is to find a leftmost derivation from axiom S to x,
   which entails to choose one of the arcs leaving 0 S . Clearly, the choice must agree
   with the current input token: thus, for string x = a ca ?, current parser state 0 S and
   current token a, the only possibility is arc 0 S
   P
   − → 1 S , which invokes machine M P ,
   i.e.,theactivationofmachine M S issuspendedinthestate0 S ,tobenamedthereturn
   state, and state 0 P becomes active.

   Two moves of M P are open, but only arc 0 P
   a
   − → 1 P agrees with the input token,
   whichisthenconsumed.Inthestate1 P withtokenc,machine M P isinvokedasecond
   time (notice that the preceding activation of M P has not been completed), setting
   state1 P asreturnstate;theparserentersstate0 P ,thistimechoosingmove0 P
   c
   − → 3 P .

   The steps made so far spell the derivation S ⇒ P ... ⇒ a P ... ⇒ a c .... Since
   thecurrentstate3 P isfinal,thelastactivationof M P terminates,andtheparserreturns
   to the (most recent) return state, 1 P , reactivating machine M P , and moves over arc
   1 P
   P
   − → 2 P to2 P .Thenextmoveconsumestokena (thelast),andentersstate3 P ,final.
   The only pending return state 0 S is resumed, moving over arc 0 S
   P
   − → 1 S to 1 S , final.

   The string is accepted as legal since it has been entirely scanned (equivalently the
   current token is ?), the current state is final for the axiom machine, and there are no
   pendingreturnstates.Thecompleteleftmostderivationis S ⇒ P ⇒ a P a ⇒ a ca.

   On the other hand, if the input were a ca , c ?, after processing a ca the parser is
   in the state 1 S , the next token is a comma, and arc 1 S
   ,
   − → 2 S is taken. ?
   Syntactic Procedures
   Ourtalkingofmachinesinvoked,suspendedandreturnedto,suggeststhatamachine
   is similar to a subroutine, and hints to an implementation of the top-down parser by
   means of a set of so-called syntactic procedures, one per nonterminal symbol. The

<a id="P284"></a>

   procedure S
   LOOP: call P
   1: if cc = ‘ ’ then
   accept
   stop
   2: else if cc = ‘,’ then
   cc := next
   goto LOOP
   3: else
   error
   end if
   end procedure
   procedure P
   1: if cc = ‘a’ then
   cc := next
   call P
   1a: if cc = ‘a’ then
   cc := next
   1b: else
   error
   end if
   2: else if cc = ‘c’ then
   cc := next
   3: else
   error
   end if
   end procedure
   Fig.4.34 Syntactic procedures of a recursive descent parser (Example4.75 below)
   syntactic procedures for the net in Fig.4.33 are listed in Fig.4.34. Parsing starts in
   the axiomatic procedure S and scans the first character of the string. Function next
   invokes the scanner or lexical analyzer, and returns the next input token. Comparing
   the procedure and the corresponding machine, we see that the procedure 
   controlflow graph reproduces (details apart) the machine state-transition graph. We make
   the correspondence explicit. A machine state is the same as a program point inside
   the procedure code, a nonterminal label on an arc is the same as a procedure call,
   and the return state corresponds to the program point following the call; furcation
   states correspond to conditional statements. The analogy goes further: an invoked
   procedure, like a machine, can call another procedure, and such nested calls can
   reach unbounded depth if the grammar has recursive rules. Accordingly, we say that
   the parsers operate by recursive descent.

   Deterministic Choice at Furcation Points
   We have skipped over a critical point: if the current state bifurcates on two arcs
   such that at least one label is nonterminal, how do we decide which branch to take?
   Similarly, if the state is final and an arc exits from it, how do we decide whether to
   terminate or to take the arc? More generally, how can we make sure that the
   goaldirected parser is deterministic in every furcating state? Deferring to later sections

<a id="P285"></a>

   machine net
   1 S
   0 S 2 S
   S →
   →
   a N
   N
   a
   0 N 1 N 2 N 3 N N →
   ↓
   →
   a N b
   Fig. 4.35 Net of Example4.54. The bifurcation in the node 0 S causes a nondeterministic choice
   (see also the ELR(1) pilot graph on p. 289)
   grammar machine
   E → E + a | a 0 E 1 E 2 E 3 E E → →
   a
   E
   +
   a
   Fig.4.36 Aleft-recursivegrammarandmachinethatmakestop-downdeterministicchoicesimpos-
   sible. The case of left-recursive grammars is discussed at length on p. 286 and p. 309
   the full answer, we anticipate some simple examples that hint to the necessity of
   restrictive conditions to be imposed on the net, if we want the parser to be 
   deterministic.

   Example 4.54 (choices at furcating states) The net in Fig.4.35 defines language
   { a ∗ a n b n | n ≥ 0 }. In the state 0 S with an a as current token, the choice of either
   arc leaving the state is nondeterministic, since the arc invoking N transfers control
   to state 0 N , from where the a-labeled arc to 1 N originates. Differently stated, the
   syntactic procedure S is unable to decide whether to call procedure N or to move
   over arc 0 S
   a
   − → 1 S , since both choices are compatible with an input string such as
   a a a b .... To make the right decision, one has to look a long distance ahead of the
   current token, to see if the string is, say, a a a b ? or a a a bbb ?. We shall see that
   this confusing situation is easily diagnosed by examining the macro-states of the
   ELR(1) pilot. ?
   The next example of nondeterministic bifurcation point is caused by a
   leftrecursive rule in the grammar depicted in Fig.4.36. In the state 0 E , both arcs agree
   with the current token a, and the parser cannot decide whether to call machine M E
   or to move to state 3 E by a terminal shift.

   Both examples exhibit non-ambiguous grammars that are ELR(1), and thus witness
   that the deterministic top-down method is less general than the bottom-up one. We
   aregoingtostatefurtherconditions(namedELL(1))ontheELR(1)pilotgraph,that
   ensure deterministic top-down parsing.


<a id="P286"></a>


## 4.7.1 ELL(1) Condition

   Before we characterize top-down deterministically parsable grammars or networks,
   we recall the multiple-transition and convergence property. Given an ELR(1) net M
   and its ELR(1) pilot P, a macro-state has the multiple-transition property (p. 253)
   MTP if it contains two candidates such that from their states two identically labeled
   arcs originate. Here we are more interested in the opposite case and we say that an
   m-state has the single-transition property (STP) if the MTP does not hold for the
   m-state.

   Definition 4.55 (ELL(1)condition) AmachinenetMmeetstheELL(1)condition
   if the following three clauses are 20 satisfied:
   1. there are no left-recursive derivations
   2. the net meets the ELR(1) condition, i.e., it does not have either shift-reduce, or
   reduce-reduce, or convergence conflicts (p. 253)
   3. the net has the single-transition property (STP) ?
   Example 4.56 (clausesofthe ELL (1)condition)Wecheckthatthemachinenetthat
   introduced top-down parsing on p. 283, reproduced in Fig.4.37 for convenience,
   meets the three conditions. First, the grammar does not admit any left-recursive
   derivation, since neither nonterminal S nor nonterminal P derives a string that starts
   with S and P, respectively. Second, the pilot shown in Fig.4.37 does not have
   conflicts, therefore it meets the ELR(1) condition. Third, every m-state of the pilot has
   the STP. Let us check it just on the arcs outgoing from m-state I 3 : the arc to I 4
   matches only one arc of the net, i.e., 1 P
   P
   − → 2 P ; similarly, the arc to I 6 matches
   0 P
   a
   − → 1 P , and the arc to I 8 matches 0 P
   c
   − → 3 P only.

   We have seen the recursive descent parser for this net in Fig.4.34. ?
   Some previous examples that raised difficulties for deterministic decisions at 
   furcation points are now revisited to illustrate violations of the ELL(1) condition.
   Clause 1: the grammar in Fig.4.36 on p. 285 has an immediate left-recursive
   derivation on nonterminal E; but also non-immediate left-recursive derivations are
   excluded by clause 1. To check if a grammar or net has a left-recursive derivations,
   we canuse the simple method described in Chap.2on p. 41. We mention that certain
   types of left-recursive derivations necessarily violate also clause 3 or clause 2; in
   other words, the three clauses overlap to some extent.

   Moreover,ifagrammarviolatesclause1,itisalwayspossibletochangeitinsuch
   a way that left-recursive derivations are turned into right-recursive ones, by applying
   20 The historical acronym “ELL(1)” has been reused over and over in the past by several authors
   with slight different meanings. We hope that reusing again the acronym will not be considered an
   abuse.


<a id="P287"></a>

   machine net
   0 S 1 S 2 S S →
   ↓
   {
   P
   ‘,’
   { ‘,’ }
   P
   0 P 3 P
   1 P 2 P
   P →
   →
   a { a }
   P
   a
   c
   { c }
   ELR (1) pilot
   0 S
   0 P ‘,’
   1 S
   2 S
   0 P ‘,’
   1 P ‘,’
   0 P a
   2 P ‘,’ 3 P ‘,’
   1 P a
   0 P a
   2 P a 3 P a
   I 0
   I 1
   I 2
   I 3
   I 4
   I 5
   I 6
   I 7
   I 8
   P →
   P
   a
   c
   ‘,’
   P
   P a
   a
   c
   P a
   a
   c
   c
   a
   Fig.4.37 Machine net (reproduced from Fig. 4.33 on p. 283) and its ELR(1) pilot. The ELL(1)
   condition is satisfied
   the conversion method presented on p. 76. Therefore, in practice the condition that
   excludesleft-recursivederivationsisnevertoocompellingforthecompilerdesigner.
   Discussion of the Single-Transition Property
   The last clause of Definition4.55 states that the net must satisfy STP; we discuss
   some cases of violation.


<a id="P288"></a>

   Example 4.57 (violations of STP in ELR (1) nets) Three cases are examined, none
   of them left-recursive. First, the net in Fig.4.38 has two candidates in the base of
   m-state I 1 (and also in other m-states). This says that the bottom-up parser has to
   carryontwosimultaneousattemptsatparsinguntilareductiontakesplace,whichby
   the ELR(1) property is unique, since the pilot does not have either shift-reduce, or
   reduce-reduce, or convergence conflicts. The failure of the STP clause is consistent
   with the fact that a top-down parser cannot carry on more than one parsing attempt
   at a time, since just one syntactic procedure is active at any time, and its run-time
   configuration consists of only one machine state.

   Second, an older example, the net in Fig.4.23 on p. 265 illustrates the case of a
   candidate in the base of an m-state, I 3 , that is entered by a convergent arc. Third, we
   observe the net of grammar S → b (a | S c) | a in Fig.4.39. The pilot has two
   convergent transitions, therefore it does not have the STP, yet it contains only one
   candidate in each m-state base. ?
   For a given grammar, the STP condition has two important consequences: first,
   the range of parsing choices at any time is restricted to one; second, the presence
   of convergent arcs in the pilot is automatically excluded. Therefore, some technical
   complications needed in bottom-up parsing can be dispensed with, as we shall see.
   Now, the book offers two alternative reading paths:
   1. PathonegoesthroughSect.4.7.2andcontinuesthebottom-upshift-reducetheory
   of previous sections; it introduces, step by step, the parser simplifications that are
   made possible by ELL(1) conditions.

   2. On the other hand, path two bypasses the step-by-step development and jumps to
   the construction of top-down deterministic parsers directly from the grammar or
   machine net.

   Readers aiming for immediate practical indications for building such parsers should
   skipSect.4.7.2andjumpaheadtoSect.4.7.3onp.305.Conversely,readersinterested
   in understanding the relationship between bottom-up and top-down parsers should
   continue reading the next section.


## 4.7.2 Step-by-Step Derivation ofELL(1) Parsers

   Starting from the familiar bottom-up parser, we derive step by step the structural
   simplifications that become possible, as we add one by one the STP and the no left
   recursion conditions to the ELR(1) condition. 21
   Tostart,wefocusontwoimportanteffects.First,conditionSTP permitstogreatly
   reducethenumberofm-states,downtothenumberofnetstates.Second,convergent
   21 The reader is referred to [26] for a more formal presentation of the various steps, including the
   proofs of their correctness.


<a id="P289"></a>

   machine net
   1 S
   0 S 2 S
   S →
   →
   a N
   N
   a
   0 N 1 N 2 N 3 N N →
   ↓
   →
   a N b
   pilot graph
   0 S
   0 N
   1 S
   1 N
   0 N b
   1 S
   1 N b
   0 N b
   2 S
   2 S
   2 N
   2 S
   2 N b
   3 N 3 N b
   I 0
   I 1 I 2
   I 3 I 4 I 5
   I 6 I 7
   P →
   a a
   a
   N N N
   b b
   Fig.4.38 ELR(1) net of Example4.57 with multiple candidates in the m-state base of I 1 , I 2 , I 4 and I 5

<a id="P290"></a>

   machine net
   0 S 3 S
   1 S 2 S
   S →
   →
   b
   a
   S
   c
   a
   pilot graph
   0 S
   3 S 2 S
   1 S
   0 S c
   3 S c
   1 S c
   0 S c
   3 S c 2 S c
   I 0
   I 1
   I 2
   I 3 I 4
   I 5
   I 6
   I 7
   P →
   a
   b
   c
   S
   a
   b S
   a
   b
   c
   Fig. 4.39 ELR(1) net of Example4.57 with just one candidate in each m-state base, yet with
   convergent arcs (evidenced by double arrows)
   arcs disappear and consequently the chain of stack pointers can be eliminated from
   the parser.

   Then,weaddtherequirementthatthegrammarisnotleft-recursive,andweobtain
   a parser specified by a parser control-flow graph or PCFG, essentially isomorphic to
   themachinenetgraphs.Theresultisatop-downpredictiveparserofthetypealready
   considered, which is able to construct the syntax tree in pre-order as it proceeds.

### 4.7.2.1 SingleTransition Property and Pilot Compaction

   Recall that the kernel of an m-state (defined on p. 253) is the projection on the
   first component of every candidate, i.e., the result of deleting the look-ahead sets.
   The relation between kernel-identical m-states is an equivalence and the set of all
   kernel-equivalent states forms an equivalence class.

   We show that kernel-identical m-states can be safely coalesced into an m-state
   and that the resulting smaller pilot behaves as the original one, when used to control

<a id="P291"></a>

   a parser. We hasten to say that in general such a transformation does not work for an
   ELR(1) pilot, but it turns out to be correct under the STP hypothesis.

   The next algorithm defines the merging operation, which coalesces two 
   kernelidenticalm-states I 1 and I 2 ,suitablyadjuststhepilotgraphandthenpossiblymerges
   more kernel-identical m-states.

   Algorithm 4.58 (operationMerge( I 1 , I 2 ))
   1. replace m-states I 1 and I 2 by a new kernel-identical m-state, denoted by I 1,2 ,
   where for each candidate the look-ahead set is the union of the corresponding
   ones in the merged m-states:
   ? p, π ? ∈ I 1,2 ⇐⇒ ? p, π 1 ? ∈ I 1 and ? p, π 2 ? ∈ I 2 and π = π 1 ∪ π 2
   2. m-state I 1,2 becomes the target for all the labeled arcs that entered I 1 or I 2 :
   I
   X
   − → I 1,2 ⇐⇒ I
   X
   − → I 1 or I
   X
   − → I 2
   3. for each pair of arcs leaving I 1 and I 2 , labeled X, the target m-states (which
   necessarily are kernel-identical) are merged:
   if ϑ( I 1 , X ) ?= ϑ( I 2 , X ) then call Merge
   ?
   ϑ(I 1 , X ), ϑ( I 2 , X ) ?
   ?
   Clearly the merge operation terminates and produces a graph with fewer nodes.
   We observe that the above procedure resembles the one for minimizing deterministic
   finite automata.

   ByapplyingtheMergealgorithm(Algorithm4.58)tothemembersofeveryequiv-
   alence class, we construct a new graph called the compact pilot, 22 denoted by C.
   Example 4.59 (compact pilot) We reproduce (from the running example on p. 237
   and 254) in Fig.4.40 the machine net, the original ELR(1) pilot and, in the bottom
   part,thecompactpilot,wherethem-stateshavebeenrenumberedtogiveevidenceto
   thecorrespondencewiththenetstates.Moreprecisely,themappingfromthem-state
   bases (which by the STP hypothesis are singletons) to the machine states that are
   not initial is one to one: for instance m-state K 1 T is identified by state 1 T . On the
   other hand, the initial states 0 E and 0 T repeatedly occur in the closure part of several
   m-states. To obtain a complete one-to-one mapping including also the initial states,
   22 Forthereaderacquaintedwiththeclassicaltheory:LR(0)andLALR(1)pilots(whicharehistorical
   simpler variants of LR(1) parsers not considered here, see for instance [17]) have in common with
   compact pilots the property that kernel-identical m-states cannot exist. However, neither LR(0) nor
   LALR(1) pilots have to comply with the STP condition.


<a id="P292"></a>

   machine net
   0 E 1 E E →
   ↓ ↓
   T
   T 0 T 1 T 2 T 3 T T →
   →
   ( E )
   a
   pilot graph
   0 E
   0 T a (
   1 E
   0 T a (
   1 T a (
   0 E )
   0 T a ( )
   2 T a ( 3 T a (
   1 E )
   0 T a ( )
   1 T a ( )
   0 E )
   0 T a ( )
   2 T a ( ) 3 T a ( )
   I 0 I 1
   I 2 I 3
   I 4
   I 5 I 6
   I 7
   I 8
   P →
   T
   a
   a
   T
   (
   (
   E )
   a
   a
   T
   ( T
   T
   (
   (
   E
   a
   )
   compact pilot graph
   0 E
   0 T a (
   1 E )
   0 T a ( )
   1 T a ( )
   0 E )
   0 T a ( )
   2 T a ( ) 3 T a ( )
   K 0 E ≡ I 0 K 1 E ≡ I 1,4
   K 3 T ≡ I 2,5
   K 1 T ≡ I 3,6
   K 2 T ≡ I 7,8
   C →
   T
   a
   a
   T
   (
   T
   (
   E )
   (
   a
   Fig. 4.40 From top to bottom: machine net M, ELR(1) pilot graph P and compact pilot C; the
   equivalenceclassesofm-statesare:{ I 0 },{ I 1 , I 4 },{ I 2 , I 5 },{ I 3 , I 6 }and{ I 7 , I 8 };them-states
   of C are named K 0 E , …, K 3 T to evidence their correspondence with the states of net M

<a id="P293"></a>

   we will in a moment extract the initial states from the m-states, to obtain the already
   mentioned parser control-flow graph.

   The look-ahead sets in the compact pilot C are larger than in P: e.g., in K 1 T the
   set in the first row is the union of the corresponding look-ahead sets in the merged
   m-states I 3 and I 6 . This loss of precision is not harmful for parser determinism,
   thanks to the stronger constraints imposed by STP. ?
   The following statement says that the compact pilot can be safely used as a parser
   controller.

   Property 4.60 (compactpilot) LetpilotsP andC be,respectively,theELR(1)pilot
   and the compact pilot of a net M satisfying STP. The ELR(1) parsers controlled by
   P and by C are equivalent, i.e., they recognize language L (M) and construct the
   same syntax tree for every string x ∈ L (M). ?
   Weomitacompleteproof 23 andwejustoffersomejustification.First,itispossibleto
   prove that if pilot C has an ELR(1) conflict, then also pilot P has, which is excluded
   by hypothesis. In particular, it is easy to see that an m-state I 1,2 = Merge(I 1 , I 2 )
   cannot contain a reduce-reduce conflict between two candidates ? p,π ? and ?r,ρ?,
   with π ∩ ρ ?= ∅, such that states p and r are final and non-initial, because the STP
   rules out the presence of two candidates in the base of I 1 , and I 1,2 has the same
   kernel.

   Having excluded the presence of conflicts in pilot C, we argue that the parsers
   controlled by C and P recognize the same language. Consider a string accepted by
   the latter parser. Since any m-state created by Merge encodes exactly the same cases
   for reduction and for shift as the merged m-states of P, the two parsers will perform
   exactly the same moves. Moreover, the stack m-states and their stack candidates
   (p. 259) of the two parsers can only differ in the look-ahead sets, which are here
   irrelevant. In particular, the chains of candidate identifiers cid created by the parsers
   are identical, since the offset of a candidate ?q, { ... }? inside an m-state remains
   the same after merging. Therefore, at any time the two parser stacks store the same
   elements, and the compact parser recognizes the same strings and constructs the
   same tree.

   To finish, it is impossible that an illegal string is accepted by the C-based parser
   and rejected by the original parser, because, the first time that the P-based parser
   stops by error, say, for an impossible terminal shift, also the C parser will stop with
   the same error condition.

   At the risk of repetition, we stress that Property4.60 does not hold in general
   for an ELR(1) but not STP-compliant pilot, because the graph obtained by merging
   kernel-equivalent m-states is not guaranteed to be free from conflicts.

   23 It can be found in [26].


<a id="P294"></a>


### 4.7.2.2 Candidate Identifiers or Pointers Unnecessary

   Thanks to the STP property, Algorithm4.42 (p. 260) will be further simplified to
   removetheneedforcid (orstackpointers).Werecallthatacid wasneededtofindthe
   reach of a non-empty reduction move into the stack: elements were popped until the
   cid chainreachedaninitialstate,theend-of-listsentinel.UnderSTP hypothesis,that
   test is now replaced by a simpler device, to be later incorporated in the final ELL(1)
   parser (Algorithm4.73 on p. 311). With reference to the “old” Algorithm4.42, only
   shift and reduction moves are modified.

   First, let us focus on the situation when the old parser, with element J on top of
   stack and J |base = ?q A , ...?, performs the shift of X (terminal or non-) from state
   q A , which is necessarily non-initial; the shift would require to compute and record a
   non-null cid into the sms J ? to be pushed on stack. In the same situation, the “new”
   parser behaves differently: it cancels from the top-of-stack element J all candidates
   otherthan?q A , ...?,sincetheycorrespondtodiscardedparsingalternatives.Notice
   that the STP implies that the canceled candidates are necessarily in the closure of J,
   hence they contain only initial states. After eliminating from the sms the irrelevant
   candidates, the new parser can identify the reduction to be made when a final state
   of machine M A is entered, by using a simple rule: keep popping the stack until the
   first occurrence of initial state 0 A is found.

   Second, consider a shift of X (terminal or non-) from an initial state ?0 A , π ?,
   which is necessarily in the closure of element J. Notice that in this case the new
   parser leaves element J unchanged and pushes the element ϑ(J, X) on the stack.
   The other candidates present in J are not canceled because they may be the origin
   of future nonterminal shifts.

   Since cid are not used by the new parser, a stack element is identical to an m-state
   (of the compact pilot). Thus, a shift move first trims the top-of-stack element by
   discarding some candidates, then it pushes the input token and the next m-state. The
   next algorithm lists only the moves that differ from Algorithm4.42.

   Algorithm 4.61 (pointerless parserA PL ) Let the pilot be compacted; m-states are
   denoted K i and stack symbols H i ; the set of candidates of H i is weakly included in
   K i .

   shift move
   Letthecurrentcharacterbea, H bethetop-of-stackelement,containingcandidate
   ?q A , π ?. Let q A
   a
   − → q ? A and ϑ(K, a) = K ? be, respectively, the state transition
   and m-state transition, to be applied. The shift move does:
   1. if q A ∈ K |base (i.e., state q A is not initial), eliminate all other candidates from H,
   i.e., set H equal to K |base
   2. push token a on stack and get next token
   3. push H ? = K ? on stack

<a id="P295"></a>

   reduction move (non-initial state)
   Let the stack be as follows:
   H[0] a 1 H[1] a 2 ... a k H[k]
   Assume that the pilot chooses the reduction candidate ?q A , π ? ∈ K[k], where
   state q A is final but non-initial. Let H[h] be the topmost stack element such that
   0 A ∈ K[h] |kernel . The move does:
   1. grow the syntax forest by applying the reduction:
   a h+1 a h+2 ... a k ? A
   and pop the stack symbols:
   H[k] a k H[k − 1] a k−1 ... H[h + 1] a h+1
   2. execute the nonterminal shift move ϑ(K[h], A)
   reduction move (initial state)
   It differs from the preceding case in that, for the chosen reduction candidate
   ?0 A , π ?, the state is initial and final. Reduction ε ? A is applied to grow the
   syntax forest. Then the parser performs the nonterminal shift move ϑ(K[k], A).
   nonterminal shift move
   It is the same as a shift move, except that the shifted symbol is a nonterminal. The
   only difference is that the parser does not read the next input token at line 2 of
   shift move. ?
   Clearly this reorganization removes the need of the cid (or of the vector stack) used
   in previous algorithms.

   Property 4.62 (pointerless pilot) If the ELR(1) pilot (compact or not) of an EBNF
   grammar or machine net satisfies the STP condition, the pointerless parser A PL of
   Algorithm4.61 is equivalent to the ELR(1) parser A of Algorithm4.42. ?
   It is useful to support this statement by some arguments to better understand how
   the two algorithms are related.

   First, after parsing the same string, the stacks of parsers A and A PL contain the
   same number k of stack elements, respectively, J[0] ... J[k] and K[0] ... K[k].
   Foreverypairofcorrespondingelements,thesetofstatesincludedin K[i]isasubset
   of the set of states included in J[i] because Algorithm4.61 may have discarded a
   few candidates.


<a id="P296"></a>

   Second, we examine the stack elements at the same position i and i − 1. The
   following relations hold:
   in J[i], candidate ?q A , π, ?j ? points to ? p A , π, ?l ? in J[i − 1] |base
   if and only if
   candidate ?q A , π ? ∈ K[i] and the only candidate in K[i − 1] is ? p A , π ?
   in J[i], candidate ?q A , π, ?j ? points to ?0 A , π, ⊥? in J[i − 1] |closure
   if and only if
   candidate ?q A , π ? ∈ K[i]
   and elem. K[i − 1] equals the projection of J[i − 1] on ?state, look-ahead?
   Then, by the way the reduction moves operate, parser A performs reduction
   a h+1 a h+2 ... a k ? A if and only if parser A PL performs the same reduction.
   To complete the presentation of the new parser, we list an execution trace.
   Example 4.63 (pointerless parser trace for compact pilot) Consider Fig.4.40 on p.
   292. Given the input string (( )a ) ?, Fig.4.41 shows the execution trace, which
   should be compared with the one of the ELR(1) parser in Fig.4.22 on p. 263, where
   cid are used. The same graphical conventions are used: the m-state (of the compact
   pilot) in each cell is framed, e.g., K 0 S is denoted 0 S , etc.; the final candidates are
   encircled, e.g.,
   3 T
   , etc.; and the look-ahead is omitted to avoid clogging. So a
   candidate appears as a pure machine state. Of course, here there are no pointers, and
   the initial candidates canceled by Algorithm4.61 from an m-state are stricken out.
   Notice that, if initial candidates had not been canceled, several instances of the
   initialstateoftheactivemachinewouldoccurinthestackm-statestobepoppedwhen
   areductionstarts,thuscausingalossofdeterminism.Forinstance,thefirst(fromthe
   figure top) reduction ( E ) ? T of machine M T pops the three stack elements K 3 T ,
   K 2 T , K 1 T ,andthelastonewouldhavecontainedthe(nowstrickenout)candidate0 T ,
   which is initial for machine M T , but is not the correct initial state for the reduction:
   the correct one, unstriked, is below in the stack, in m-state K 1 T , which is not popped
   and is the origin of the nonterminal shift on T following the reduction. We point
   out that Algorithm4.61 avoids to cancel an initial candidate from a stack element,
   if a shift move has to be later executed starting from it: see the Shift move case in
   Algorithm4.61. The motivation for not canceling is twofold. First, these candidates
   will not cause any premature stop of the series of pop moves in the reductions that
   may come later, or said differently, they will not break any reduction handle. For
   instance, the first (from the figure top) shift on ‘(’ of machine M T keeps both initial
   candidates 0 E and 0 T in the stack m-state K 1 T , as the shift originates from the initial
   candidate0 T .Thiscandidatewillinsteadbecanceled(i.e.,itwillshowstriked)when
   a shift on E is executed (soon after reduction T T ? E), as the shift originates from
   the non-initial candidate 1 T . Second, some of such initial candidates may be needed
   for a subsequent nonterminal shift move. ?

<a id="P297"></a>

   stack string to be parsed (with end-marker) and stack contents
   effect after
   bottom 1 2 3 4 5
   0 E
   0 E
   0 T
   ( ( ) a )
   initialisation
   of the stack
   ( ( ) a )
   1 T
   1 T
   0 E
   0 T
   shift on (
   ( ( ) a )
   1 T
   1 T
   0 E
   0 T
   1 T
   1 T
   0 E
   0 T
   shift on (
   ( ( E ) a )
   1 T
   1 T
   0 E
   0 T
   1 T
   1 T
   0 E
   0 T
   reduction ε
   E
   ( ( E ) a )
   1 T
   1 T
   0 E
   0 T
   1 T
   1 T
   0 E
   0 T
   2 T 2 T 3 T 3 T
   shifts on E and
   )
   ( T a )
   1 T
   1 T
   0 E
   0 T
   1 E
   1 E
   0 T
   reduction
   (E) T
   and shift on T
   ( T a )
   1 T
   1 T
   0 E
   0 T
   1 E
   1 E
   0 T
   3 T 3 T shift on a
   ( T T )
   1 T
   1 T
   0 E
   0 T
   1 E
   1 E
   0 T
   1 E
   1 E
   0 T
   reduction a
   T
   and shift on T
   ( E )
   1 T
   1 T
   0 E
   0 T
   2 7 2 T 3 7 3 T
   reduction
   TT E
   and shifts
   on E and )
   T
   1 E
   1 E
   0 T
   reduction
   (E) T
   and shift on T
   E
   reduction T
   E
   and accept
   (do not shift on
   E)
   Fig.4.41 StepsofthepointerlessparsingAlgorithm4.61A PL ;irrelevantandpotentiallyconfusing
   initial candidates are canceled by shift moves as explained in the algorithm

<a id="P298"></a>

   To sum up, we have shown that condition STP permits to construct a simpler 
   shiftreduce parser, which has a smaller stack alphabet and does not need pointers to
   manage reductions. The same parser can be further simplified, if we make another
   hypothesis: that the grammar is not left-recursive.


### 4.7.2.3 Stack Contractionand Predictive Parser

   The last development to be presented transforms the already compacted pilot graph
   into the control-flow graph of a predictive or goal-oriented parser. The way the
   parser uses the stack differs from the previous two models, the shift-reduce and
   the pointerless parsers. More precisely, now a terminal shift move, which always
   executesapushoperationinthepreviousparsers,issometimesimplementedwithout
   a push and sometimes with multiple pushes. The former case happens when the shift
   remainsinsidethesamemachine:thepredictiveparserdoesnotpushanelementupon
   performing a terminal shift, but it updates the top-of-stack element to record the new
   state of the active machine. Multiple pushes happen when the shift determines one
   or more transfers from the current machine to others: the predictive parser performs
   a push for each transfer.

   Now,theessentialinformationtobekeptonstackisthesequenceofmachinesthat
   have been activated and have not reached a final state (where a reduction occurs). At
   each parsing time, the current or active machine is the one that is doing the analysis,
   and the current state is kept in the top-of-stack element. Previous non-terminated
   activationsofthesameorothermachinesareinasuspended state.Foreachsuspended
   machine M A , a stack entry is needed to store the state q A , from where the machine
   will resume the computation when control is returned after performing the relevant
   reductions. A major advantage of predictive parsing is that the construction of the
   syntax tree can be anticipated: the parser can generate online the left derivation of
   the input.

   Control-Flow Graph of Parser
   Moving from the above considerations, we adjust the compact pilot graph C to make
   it isomorphic to the original machine net M, and thus we obtain a graph named
   parser control-flow graph or PCFG, because it represents the blueprint of the parser
   code. First, every m-node of C that contains many candidates is split into as many
   nodes.Second,thekernel-equivalentnodesarecoalescedandtheoriginallook-ahead
   sets are combined into one. The third step creates new arcs, named call arcs, which
   represent the transfers of control from a machine to another. At last, each call arc
   is labeled with a set of terminals, named guide set, which will determine the parser
   decision to transfer control to the called machine.

   Definition 4.64 (parser control-flow graph or PCFG) Every node of the PCFG,
   denoted by F, is identified and denoted (at no risk of confusion) by a state q of
   machine net M. Moreover, every final node f A additionally contains a set π of

<a id="P299"></a>

   terminals, named prospect set. 24 Such nodes are therefore associated with the pair
   ? f A , π ?. The prospect set is the union of the look-ahead sets π i
   of every candidate
   ? f A , π i ? existing in the compact pilot graph C, as follows:
   π =
   ?
   ∀ ? f A ,π i ?∈C
   π i (4.13)
   The arcs of pilot F are of two types, named shift and call (respectively depicted as
   solid and dashed arrows):
   1. There exists in F a shift arc q A
   X
   − → r A with X terminal or non-, if the same arc is
   in the machine M A .

   2. There exists in F a call arc q A
   γ 1
   − → 0 A 1 , where A 1 is a nonterminal possibly
   different from A, if arc q A
   A 1
   −→ r A is in the machine M A , hence necessarily some
   m-state K of pilot C contains candidates ?q A , π ? and
   ?
   0 A 1 , ρ
   ? ; and the next
   m-state ϑ(K, A 1 ) contains candidate
   ?
   r A , π r A
   ? .

   ?
   The call arc label γ 1 ⊆ Σ ∪ { ? }, named guide set, is specified in the next
   definition, which also specifies the guide sets of terminal shift arcs and of the arrows
   (darts) that mark final states.

   Definition 4.65 (guide set)
   1. Foreverycallarcq A
   γ 1
   − → 0 A 1 associatedwithanonterminalshiftarcq A
   A 1
   −→ r A ,
   a terminal b is in the guide set γ 1 , also written as b ∈ Gui(q A ??? 0 A 1 ), if, and
   only if, one of the following conditions holds:
   b ∈ Ini
   ? L (0
   A 1 )
   ?
   (4.14)
   A 1 is nullable and b ∈ Ini
   ? L (r
   A )
   ?
   (4.15)
   A 1 and L (r A ) are both nullable and b ∈ π r A (4.16)
   ∃ in F a call arc 0 A 1
   γ 2
   − → 0 A 2 and b ∈ γ 2 (4.17)
   2. For every terminal shift arc p
   a
   − → q (with a ∈ Σ), we set Gui(p
   a
   − → q) := { a }.

   3. Foreverydartthattagsafinalnodecontainingcandidate? f A , π ?(with f A final),
   we set Gui( f A →) := π. ?
   24 Although traditionally the same word “look-ahead” has been used for both shift-reduce and
   topdown parsers, the sets differ and we prefer to differentiate their names.


<a id="P300"></a>

   Guide sets are usually represented as arc labels enclosed within braces.

   Relations(4.14)–(4.16)arenotrecursiveandrespectivelyconsiderthatterminalb
   isgeneratedby M A 1 calledby M A ;orby M A butstartingfromstater A ;orthattermi-
   nalb followsmachine M A .Equation (4.17)isrecursiveandtraversesthenetasfaras
   the chain of call sites activated. We observe that Eq.(4.17) induces the set inclusion
   relation γ 1 ⊇ γ 2 between any two concatenated call arcs q A
   γ 1
   − → 0 A 1
   γ 2
   − → 0 A 2 .

   Next, we move to a procedural interpretation of a PCFG: all the arcs (except
   nonterminal shifts) can be viewed as conditional instructions, enabled if the current
   charactercc belongstotheassociatedguideset:thusaterminalshiftarclabeledwith
   { a } is enabled by predicate cc ∈ { a }, simplified to cc = a; a call arc labeled with
   set γ represents the procedure invocation conditioned by predicate cc ∈ γ; a final
   node dart labeled with set π is interpreted as a conditional return-from-procedure
   instructiontobeexecutedifcc ∈ π.TheremainingPCFGarcsarenonterminalshifts,
   which are viewed as unconditional return-from-procedure instructions.

   The essential properties of guide sets are next stated (see [26] for a proof).
   Property 4.66 (disjointness of guide sets)
   1. For every node q of the PCFG of a grammar that satisfies the ELL(1) condition,
   the guide sets of any two arcs originating from q are disjoint.

   2. IftheguidesetsofaPCFGaredisjoint,thenthemachinenetsatisfiestheELL(1)
   condition of Definition4.55. ?
   The first statement says that, for the same state, membership in different guide sets
   is mutually exclusive. The second statement is the converse of the first: it makes the
   conditionofhavingdisjointguidesets,acharacteristicpropertyofELL(1)grammars
   (a marginal case contradicting statement 2 of Property4.66 will be briefly discussed
   attheendofthissectiononp.304).Wealsoanticipatethatstatement2canbedirectly
   checked on the PCFG, thus saving the effort to build the ELR(1) pilot.

   Example 4.67 (PCFGoftherunningexample)ThePCFGoftherunningexampleis
   represented in Fig.4.42, with the same layout as the machine net for comparability.
   In the PCFG there are new nodes (here only node 0 T ), which derive from the initial
   candidates (except those containing the axiom 0 S ) extracted from the closure part of
   the m-states of the compact pilot C. After creating such new nodes, the closure part
   of the nodes of C (except node I 0 ) becomes redundant and is eliminated from the
   contents of the PCFG nodes.

   Assaid,theprospectsetsareneededonlyinthefinalstatesandhavethefollowing
   properties (see also (4.13)):
   • The prospect set of a final state that is not initial coincides with the look-ahead set
   of the corresponding node in the compact pilot C. This is the case of nodes 1 E and
   3 T .


<a id="P301"></a>

   machine network compact pilot graph
   0 E 1 E M E →
   ↓ ↓
   T
   T
   0 T 1 T 2 T 3 T M T → →
   ( E )
   a
   0 E
   0 T a (
   1 E )
   0 T a ( )
   1 T a ( )
   0 E )
   0 T a ( )
   2 T a ( ) 3 T a ( )
   K 0 E K 1 E
   K 3 T
   K 1 T
   K 2 T
   C →
   T
   a
   a
   T
   (
   T
   (
   E )
   (
   a
   parser control-flow graph
   0 E ) 1 E ) M E →
   ↑ ↑
   T
   T
   0 T 1 T 2 T 3 T a ( ) M T → →
   (
   E
   )
   a
   a (
   a (
   a ( )
   Fig.4.42 The parser control-flow graph F (PCFG) of the running example, from the net and the
   compact pilot
   • The prospect set of a final state that is also initial, e.g., 0 E , is the union of
   the look-ahead sets of every candidate ?0 E , π ? occurring in C. For instance,
   ?0 E , { ‘)’, ? }? takes terminal ‘)’ from m-state K 1 T
   and ? from K 0 E .

   Solid arcs represent the shifts, already present in the machine net and in the pilot
   graph. Dashed arcs represent the calls, labeled by guide sets, the computation of
   which follows:
   • the guide set of call arc 0 E ??? 0 T (and of 1 E ??? 0 T as well) is { a, ‘(’ }, since
   both terminals a and ‘(’ can be shifted starting from state 0 T , which is the call arc
   destination (see (4.14))
   • the guide set of call arc 1 T ??? 0 E includes the following terminals:
   – a and ‘(’, since from state 0 E (the call arc destination) one more call arc goes
   out to 0 T and has guide set { a, ‘(’ } (see (4.17))

<a id="P302"></a>

   grammar G network M
   S → X d
   +
   0 S 1 S 2 S → →
   M S
   X
   d
   X
   X → a Y b | ε | ε 0 X 1 X 2 X 3 X →
   M X
   ↓ ↓ ↓
   a Y b
   Y → c X | c | a
   0 Y 1 Y 2 Y →
   M Y
   →
   c
   X
   c
   b
   a
   Fig.4.43 Grammar and net of Example4.68
   – ‘)’,sincetheshiftarc1 T
   E
   − → 2 T (associatedwiththecallarc)isinmachine M T ,
   language L (0 E ) is nullable and it holds ‘)’ ∈ Ini(2 T ) (see (4.16)).

   Observe the chain of two call arcs, 1 T ??? 0 E and 0 E ??? 0 T ; relation (4.17)
   implies that the guide set of the former arc includes that of the latter.

   In accordance with Property4.66, the terminal labels of all the arcs (shift and call)
   that originate from the same node do not overlap. ?
   Since relation (4.15) has not been used, we illustrate it in the next example.
   Example 4.68 (from the pilot to the PCFG) Consider the EBNF grammar G and net
   M in Fig.4.43. The machine net M is ELL(1); its pilot P in Fig.4.44 does not have
   any ELR(1) conflicts and also satisfies the ELL condition. The PCFG of net M,
   derived from P, is shown in Fig.4.45. We check that the prospect sets in the PCFG
   states that result from merging the m-states of the pilot graph are the union of their
   look-ahead sets (see (4.13)):
   • the prospect set of state 0 X is π 0 X = { b, d }, and is the union of the look-ahead
   set { b } of candidate ?0 X , { b }? in m-state I 7 , and of the look-ahead set { d } of
   candidate ?0 X , { d }? in m-state I 0
   • similarly, the prospect set π 1 X = { b, d } is the union of the look-ahead sets of
   candidates ?1 X , { b }? in I 8 and ?1 X , { d }? in I 3
   • the prospect set π 3 X = { b, d } derives from candidates ?3 X , { b }? in I 10 and
   ?3 X , { d }? in I 5

<a id="P303"></a>

   2 Y b
   I 6
   1 Y b
   0 X b
   I 7
   1 X b
   0 Y b
   I 8
   2 X
   b I
   9
   3 X d
   I 5 2 X d
   I 4
   1 X d
   0 Y b
   I 3
   3 X b I 10
   0 S
   0 X d
   P →
   I 0
   1 S
   I 1
   2 S
   0 X d
   I 2
   X
   d
   X
   a a
   Y b
   a
   c
   X
   c
   a
   c
   Y
   b
   a
   Fig.4.44 ELR(1) pilot of the machine net in Fig.4.43
   0 S 1 S 2 S S → →
   X
   d
   X
   0 X b d 1 X b d 2 X 3 X b d X → →
   ↓ ↓
   a Y b
   2 Y b 1 Y 0 Y ← Y ←
   c
   X
   c
   a
   { a d }
   { a d }
   { a c } { a b }
   Fig.4.45 Parser control-flow graph derived from the pilot graph in Fig.4.44

<a id="P304"></a>

   Next, we explain how the guide sets on the call arcs are computed:
   • Gui(0 S ??? 0 X ) is the union of terminal a on the shift arc from 0 X (see (4.14))
   and of terminal d on the shift arc from 1 S , which is a successor of the nonterminal
   shift arc 0 S
   X
   − → 1 S associated with the call arc, because nonterminal X is nullable
   (see (4.15))
   • Gui(2 S ??? 0 X ) is the union of terminal a on the shift arc from 0 X (see (4.14))
   and of terminal d on the shift arc from 1 S , which is a successor of the nonterminal
   shift arc 2 S
   X
   − → 1 S associated with the call arc, because nonterminal X is nullable
   (see (4.15))
   • Gui(1 Y ??? 0 X ) is the union of terminal a on the shift arc from 0 X (see (4.14))
   and of terminal b in the prospect set of 2 Y , which is the destination state of the
   nonterminal shift arc 1 Y
   X
   − → 2 Y associated with the call arc, because nonterminal
   X is nullable and additionally state 2 Y is final, i.e., language L (2 Y ) is nullable
   (see (4.16))
   • Gui(1 X ??? 0 Y ) includes only the terminals a and c that label the shift arcs from
   state 0 Y (see (4.14)); it does not include terminal b since Y is not nullable.
   Notice the two call arcs 0 S ??? 0 X and 2 S ??? 0 X , and the call arc 1 Y ??? 0 X ,
   although directed to the same machine M X , have different guide sets, because they
   are associated with different call sites.

   Inthisexamplewehaveusedthreerelations(4.14)–(4.16)forcomputingtheguide
   sets on call arcs. Relation (4.17) is not needed because in the PCFG no concatenated
   call arcs occur (that case has been already illustrated in the preceding example in
   Fig.4.42). ?
   After thus obtaining the PCFG of a net, a short way remains to construct the 
   deterministic predictive parser.

   Observation on a Marginal Case
   Going back to Property4.66 on p. 300 (disjunction of guide sets), it must be said that
   statement 2 fails in some peculiar cases, which are however marginal in practical
   grammars. The contrived grammar below is an example of such exception. Notice
   that nonterminal C generates language { ε }. Despite the guide sets on the two call
   arcs in M S (below, left) are disjoint, the grammar pilot violates the STP, and hence
   the condition ELL(1):
   G
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   1: S → Aa | B b
   2: A → C | d
   3: B → C
   4: C → ε

<a id="P305"></a>

   machine net of G (partial) pilot of G
   1 S
   0 S 3 S
   2 S
   S →
   →
   A
   B
   a
   b
   0 A 1 A
   0 C
   0 B 1 B
   A →
   →
   C →
   →
   B →
   →
   C
   d
   C
   γ A = { a d }
   γ B = { b }
   0 S
   0 A a
   0 B b
   0 C a b
   1 A a
   1 B b
   ... ...

   I 0
   I 1
   C
   double
   non-convergent
   transition
   d
   −→
   ...

   A
   −→
   −→
   B
   This kind of exception can be removed by adding to statement 2 of Property4.66
   the hypothesis that the grammar does not have any nonterminal generating 
   exclusively the empty string ε. Notice that such an hypothesis still allows to have nullable
   nonterminals,providedthateachsuchnonterminalgeneratesalsonon-emptystrings.
   In fact, grammar G has an equivalent form with disjoint guide sets: S → Aa | b
   and A → ε | d. It is structurally similar to G, but now neither one of nonterminals
   S and A generates exclusively string ε, thus the additional hypothesis is satisfied.
   Therefore, the pilot has the STP and the new grammar satisfies condition ELL(1)
   (p. 286). We finally observe that nonterminals generating only ε obviously play an
   irrelevant role in a grammar, and that they can always be eliminated as shown above;
   therefore such marginal cases have no practical impact.


## 4.7.3 Direct Construction ofTop-Down Predictive Parsers

   Section4.7.2hasrigorouslyderivedstepbysteptheparsercontrol-flowgraphPCFG
   starting from the ELR(1) pilot graph of the given machine net, by assuming it
   complies with the ELL(1) condition on p. 286. As promised, we indicate a way to check
   the ELL(1) condition and then to derive the PCFG directly from the net, bypassing
   the calculation of the pilot.

   From the Net to the Parser Control-Flow Graph
   For the reader who has not gone through the formal steps of Sect. 4.7.2, we briefly
   introduce the notion of parser control-flow graph. The PCFG, denoted by F, of a
   machine net M, is a graph that has the same nodes and includes the same arcs as the
   original graph of the net; thus each node corresponds to a machine state. In F the
   following elements are also present, which do not occur in the net M:
   call arcs
   An arc (depicted as dashed) links every node q that is the origin of a nonterminal
   shift q
   B
   − → r, to the initial state of machine M B . The label of a call arc is a set of
   terminals,namedaguideset.Intuitively,acallarcrepresentsa(possiblyrecursive)
   conditional invocation of the parsing procedure associated with machine M B ,

<a id="P306"></a>

   subjected to the condition that the current input token is in the guide set. Node r
   is named the return state of the invocation.

   prospect sets
   In F every final node q A carries a prospect set as additional information. The set
   contains every token that can immediately follow the recognition of a string that
   derives from nonterminal A; its precise definition follows in Eq. (4.18), (4.19) on
   p. 306.

   guide sets
   Let us name arrow a terminal shift arc, a call arc, and also a dangling arrow
   (named dart or exit arrow) that exits a final state; a guide set is a set of terminals,
   to be associated with every arrow in F:
   • for a shift of terminal b, the guide set coincides with { b }
   • for a call arc q A ??? 0 B , the guide set indicates the tokens that are consistent with
   theinvocationofmachine M B ;theguidesetγ ofcallarcq A ??? 0 B isindicatedby
   writing q A
   γ
   − → 0 B or, equivalently, γ = Gui(q A ??? 0 B ); its exact computation
   follows in Eq. (4.20) on p. 307
   • for the dart of a final state, the guide set equals the prospect set
   Notice that nonterminal shift arcs have no guide set.

   After the PCFG has been constructed, the ELL(1) condition can be checked using
   the following test (which rephrases Property4.66 on p. 300).

   Definition 4.69 (direct ELL(1) test) Amachinenetworksatisfiesthedirect ELL(1)
   condition if in the corresponding parser control-flow graph, for every pair of arrows
   originating from the same state q, the associated guide sets are disjoint. ?
   We list the rules for computing the prospect and guide sets.

   Equations for Prospect Sets
   We use a set of recursive equations to compute the prospect and guide sets for all
   the states and arcs of the PCFG; the equations are interpreted as instructions to
   iteratively compute the sets. To compute the prospect sets of the final states, we need
   to compute also the prospect sets of the other states, which are eventually discarded.
   1. If the graph includes any nonterminal shift arc q i
   A
   − → r i (therefore also the call
   arc q i ??? 0 A ), then the prospect set π 0 A for the initial state 0 A of machine M A
   is computed as:
   π 0 A := π 0 A ∪
   ?
   q i
   A
   − → r i
   ?
   Ini
   ?
   L (r i )
   ?
   ∪ if Nullable
   ?
   L (r i )
   ?
   then π q i else ∅
   ?
   (4.18)

<a id="P307"></a>

   2. If the graph includes any terminal or nonterminal shift arc p i
   X i
   −→ q, then the
   prospect set π q of state q is computed as:
   π q :=
   ?
   p i
   X i
   −→ q
   π p i (4.19)
   The two sets of rules apply in an exclusive way to disjoints sets of nodes, because
   in a normalized machine no arc enters the initial state. To initialize the computation,
   we assign the end-marker to the prospect set of 0 S : π 0 S := { ? }. All other sets are
   initialized to empty.

   Equations for Guide Sets
   The equations make use of prospect sets.

   1. For each call arc q A ??? 0 A 1 associated with a nonterminal shift arc q A
   A 1
   −→ r A ,
   such that possibly other call arcs 0 A 1 ??? 0 B i depart from state 0 A 1 , the guide set
   Gui(q A ??? 0 A 1 )ofthecallarcisdefinedasfollows,seealsorules(4.14)–(4.17)
   on p. 299:
   Gui(q A ??? 0 A 1 ) :=
   ?
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   Ini
   ?
   L (A 1 )
   ?
   if Nullable(A 1 ) then Ini
   ? L (r
   A )
   ?
   else ∅ endif
   if Nullable(A 1 ) ∧ Nullable
   ? L (r
   A )
   ?
   then π r A
   else ∅ endif
   ?
   0 A 1 ???0 B i
   Gui(0 A 1 ??? 0 B i )
   (4.20)
   2. For a final state f A ∈ F A , the guide set of the tagging dart equals the prospect
   set:
   Gui( f A →) := π f A (4.21)
   3. For a terminal shift arc q A
   a
   − → r A with a ∈ Σ, the guide set is simply the shifted
   terminal:
   Gui
   ?
   q A
   a
   − → r A
   ?
   := { a } (4.22)
   Initially all the guide sets are empty.

   Tocomputethesolutionforprospectandguidesets,theaboverulesarerepeatedly
   applied until none of the sets has changed in the last iteration; at that moment the set
   values give the solution.


<a id="P308"></a>

   0 S 1 S 2 S S →
   ↓
   {
   P
   ‘,’
   { ‘,’ }
   P
   0 P 3 P
   1 P 2 P
   P →
   { a, ‘,’,
   ↑
   a { a }
   P
   a { a }
   c
   { c }
   { a, c }
   { a, c }
   { a, c }
   parser control-flow graph
   Fig.4.46 Parser control-flow graph (PCFG) for Example4.70 (formerly Example4.53 on p. 283);
   the call arcs are dashed; the guide sets are in braces
   Notice that the rules for computing prospect sets are consistent with the 
   definition of look-ahead set given in Sect.4.5.3; furthermore, the rules for computing the
   guide sets are consistent with the definition of parser control-flow graph listed in
   Definition4.64 on p. 298.

   Example 4.70 (PCFG for list of balanced strings) The net in Fig.4.33 on p. 283 is
   now represented by the PCFG in Fig.4.46. The following table shows the 
   computation of the prospect sets for the PCFG in Fig.4.46.

   Node Init. Equation Arc instance Prospect set
   0 S { ? } { ? }
   1 S ∅ (4.19) 0 S
   P
   − → 1 S { ? }
   2 S ∅ (4.19) 1 S
   ‘,’
   −→ 2 S { ? }
   0 P ∅ (4.18) 0 S
   P
   − → 1 S
   ?
   ‘,
   ?
   ?
   ∪ { ? } ∪
   2 S
   P
   − → 1 S
   ?
   ‘,
   ?
   ?
   ∪ { ? } ∪
   1 P
   P
   − → 2 P { a } ∪ ∅ =
   ?
   a, ‘,
   ? , ?
   ?
   1 P ∅ (4.19) 0 P
   a
   − → 1 P
   ?
   a, ‘,
   ? , ?
   ?
   2 P ∅ (4.19) 1 P
   P
   − → 2 P
   ?
   a, ‘,
   ? , ?
   ?
   3 P ∅ (4.19) 2 P
   a
   − → 3 P
   ?
   a, ‘,
   ? , ?
   ?
   ∪
   0 P
   c
   − → 3 P
   ?
   a, ‘,
   ? , ?
   ? = ?
   a, ‘,
   ? , ?
   ?
   The guide sets are computed by three equations, but the last two are trivial and we
   comment only Eq. (4.20) that applies to the three call arcs of the graph. The guide
   set of the call arc 0 S ??? 0 P is simply Ini ((0 P )) = { a, c } since nonterminal P is
   not nullable and the call arc is not concatenated with other call arcs. Similarly, the
   guide sets of 2 S ??? 0 P and 1 P ??? 0 P have the same value.


<a id="P309"></a>

   Finally, it is immediate to check that the guide sets of bifurcating arrows are
   mutually disjoint, therefore the net is ELL(1). This confirms the earlier empirical
   analysis of the same net. ?
   The running example that follows illustrates the cases not covered and permits to
   compare the alternative approaches, based on the pilot graph or on the guide sets,
   for testing the ELL(1) condition.

   Example 4.71 (running example—computing the prospect and guide sets) The
   following two tables show the computation of most prospect and guide sets for the PCFG
   of Fig.4.42 (p. 301) here reproduced in Fig.4.47. For both tables, the computation
   is completed at the third step.

   prospect sets of states
   0 E 1 E 0 T 1 T 2 T 3 T
   ? ∅ ∅ ∅ ∅ ∅
   ) ? ) ? a ( ) ? a ( ) ? a ( ) ? a ( ) ?
   ) ? ) ? a ( ) ? a ( ) ? a ( ) ? a ( ) ?
   guide sets of call arcs
   0 E ??? 0 T 1 E ??? 0 T 1 T ??? 0 E
   ∅ ∅ ∅
   a ( a ( a ( )
   a ( a ( a ( )
   The disjointness test on guide sets confirms that the net is top-down 
   deterministic. ?
   We examine the guide sets of the cascaded call arcs 1 T
   γ 1
   − → 0 E
   γ 2
   − → 0 T , and we
   confirm the general property that the guide set upstream (γ 1 ) includes the one
   downstream (γ 2 ).

   Violations of ELL(1)Condition
   We know that the failure of the ELL(1) condition for a net that is ELR(1) can be
   due to two causes: the presence of left-recursive derivations, and the violation of
   the single-transition property in the pilot graph. First, we argue that left recursion
   implies non-disjointness of guide sets. Consider in the PCFG the bifurcation shown
   below:

<a id="P310"></a>

   0 E ), 1 E ), M E →
   ↑ ↑
   T
   T
   0 T 1 T 2 T 3 T a, (, ), M T → →
   (
   E
   )
   a
   a, (
   a, (
   a, (, )
   Fig.4.47 The parser control-flow graph (PCFG) of the running example
   0 A q A r A s A
   γ = { ... b ... }
   A
   b
   { b }
   By Eq. (4.20) the guide set γ of the call arc q A ??? 0 A includes set Ini
   ?
   L(A)
   ? ,
   which includes terminal b, hence it overlaps with the guide set of the terminal shift
   under b.

   Second, the next example shows the consequence of STP violations.

   Example 4.72 (guide sets in a non-ELL (1)grammar) The grammar with rules
   S → a ∗ N and N → a N b | εofExample4.57(p.288)violatesthesingletonbase
   property as shown in Fig.4.38, hence it is not ELL(1). This can be verified also
   bycomputingtheguidesetsonthePCFG.Wehave:Gui(0 S ??? 0 N ) ∩ Gui(0 S
   a
   − →
   1 S ) = { a } ?= ∅andGui(1 S ??? 0 N ) ∩ Gui(1 S
   a
   − → 1 S ) = { a } ?= ∅:theguidesets
   on the arcs departing from states 0 S and 1 S are not disjoint. ?

### 4.7.3.1 Predictive Parser

   Predictive parsers come in two styles, as deterministic pushdown automata (DPDA),
   and as recursive syntactic procedures. Both are straightforward to obtain from the
   PCFG.


<a id="P311"></a>

   For the automaton, the pushdown stack elements are the nodes of the PCFG: the
   top-of-stack element identifies the state of the active machine, while inner stack
   elementsrefertoreturnstatesofsuspendedmachines,inthecorrectorderofsuspension.
   There are four sorts of moves. A scan move, associated with a terminal shift arc,
   reads the current token, cc, as the corresponding machine would do. A call move,
   associated with a call arc, checks the enabling predicate, saves on stack the return
   state and switches to the invoked machine without consuming token cc. A return
   move is triggered when the active machine enters a final state, the prospect set of
   which includes token cc: the active state is set to the return state of the most recently
   suspended machine. A recognizing move terminates parsing.

   Algorithm 4.73 (predictive recognizer as DPDAA)
   • Thestackelements arethestatesof PCFGF. Thestackisinitialized withelement
   ?0 S ?.

   • Let ?q A ? be the top-of-stack element, meaning that the active machine M A is in
   state q A . Different move types are next specified:
   scan move
   if the shift arc q A
   cc
   −→ r A exists, then scan the next token and replace the stack
   top by ?r A ? (the active machine does not change)
   call move
   if there exists a call arc q A
   γ
   − → 0 B such that cc ∈ γ, let q A
   B
   − → r A be the
   corresponding nonterminal shift arc; then pop, push element ?r A ? and push
   element ?0 B ?
   return move
   if q A is a final state and token cc is in the prospect set associated with q A , then
   pop
   recognition move
   if M A is the axiom machine, q A is a final state and cc =?, then accept and halt
   − in any other case, reject the string and halt ?
   It should be clear that, since the guide sets at bifurcating arrows are disjoint
   (Property4.66 or Definition4.69), in every parsing configuration at most one move
   is possible, i.e., the algorithm is deterministic.

   Computing Leftmost Derivations
   To construct the syntax tree, the recognizer is extended with an output function,
   thus turning the DPDA into a so-called pushdown transducer (a model studied in
   Chap.5). The algorithm produces the sequence of grammar rules of the leftmost
   derivation of the input string, but since the grammar G is in EBNF form, we have

<a id="P312"></a>

   Table 4.7 Derivation steps computed by a predictive parser; the current token is cc = b
   Parser move RL rule used
   Scan move for transition q A
   b
   − → r A q A =⇒
   ˆ G
   br A
   Call move for call arc q A
   γ
   − → 0 B
   and transition q A
   0 B
   −→ r A
   q A =⇒
   ˆ G
   0 B r A
   Return move from state q A ∈ F A q A =⇒
   ˆ G
   ε
   to use the rules of the equivalent right-linearized (RL) grammar
   ˆ
   G, as anticipated
   in Sect. 4.5.2 on p. 239. The reason why we do not use EBNF derivations is that a
   derivation step may consist of an unbounded number of moves of the parser. Using
   the RL grammar, such a large derivation step is expanded into a series of derivation
   steps that match the parser moves. We recall that the syntax tree for grammar
   ˆ
   G is
   essentially an encoding of the syntax tree for the EBNF grammar G, such that each
   node has at most two children.

   For each type of parser move, Table4.7 reports the output, i.e., the rule of
   ˆ
   G,
   which is printed. The correspondence is so direct that no justification is needed and
   an example suffices.

   Example 4.74 (running example—trace of predictive parser DPDA) The RL
   grammar (from Example4.27 on p. 240) is the following:
   0 E → 0 T 1 E | ε 1 E → 0 T 1 E | ε
   0 T → a 3 T | ‘(’ 1 T 1 T → 0 E 2 T
   2 T → ‘)’ 3 T 3 T → ε
   For the input string x = (a ), we simulate the computation including (from left to
   right) the stack content and the remaining input string, the test (predicate) on the
   guide set, and the rule used in the left derivation. Guide and prospect sets are those
   of the PCFG of Fig.4.47 on p. 310.

   For the original EBNF grammar, the corresponding derivation is:
   E ⇒ T ⇒ ( E ) ⇒ (T ) ⇒ (a ) ?

<a id="P313"></a>

   stack x predicate left derivation
   ?0 E ?
   (a ) ? (∈ γ = {a (} 0 E ⇒ 0 T 1 E
   ?1 E ??0 T ?
   (a ) ? scan 0 E
   +
   = ⇒ (1 T 1 E
   ?1 E ??1 T ?
   a ) ? a ∈ γ = {a (} 0 E
   +
   = ⇒ (0 E 2 T 1 E
   ?1 E ??2 T ??0 E ?
   a ) ? a ∈ γ = {a (} 0 E
   +
   = ⇒ (0 T 1 E 2 T 1 E
   ?1 E ??2 T ??1 E ??0 T ?
   a ) ? scan 0 E
   +
   = ⇒ (a 3 T 1 E 2 T 1 E
   ?1 E ??2 T ??1 E ??3 T ?
   ) ? ) ∈ π = {a () ?} 0 E
   +
   = ⇒ (a ε1 E 2 T 1 E
   ?1 E ??2 T ??1 E ?
   ) ? ) ∈ π = {) ?} 0 E
   +
   = ⇒ (a ε2 T 1 E
   ?1 E ??2 T ?
   ) ? scan 0 E
   +
   = ⇒ (a )3 T 1 E
   ?1 E ??3 T ?
   ? ?∈ π = {a () ?} 0 E
   +
   = ⇒ (a )ε1 E
   ?1 E ?
   ? ?∈ π = {) ?} accept 0 E
   +
   = ⇒ (a )ε
   Parser Implementation by Recursive Procedures
   We have seen in the introduction to predictive parsers on p. 283 that they are
   often implemented using recursive procedures. Each machine is transformed into
   a parameter-less syntactic procedure, having a control-flow graph matching the
   corresponding PCFG subgraph, so that the current state of a machine is encoded at
   run-time by the value of the program counter. Parsing starts in the axiom 
   procedure and successfully terminates when the input has been exhausted, unless an error
   has occurred before. The standard run-time mechanisms of procedure invocation
   and return from procedure, respectively, implement the call and return moves. An
   example should suffice to show how the procedure pseudo-code is obtained from the
   PCFG.

   Example 4.75 (recursive descent parser) From the PCFG of Fig.4.47 on p. 310,
   we obtain the syntactic procedures shown in Fig.4.48. The pseudo-code can be
   optimized in several ways, which we leave out. ?
   Any practical parser has to cope with input errors, to provide a reasonable error
   diagnosis, and to be able to resume parsing after an error. A few hints on error
   management in parsing are in Sect. 4.12.


<a id="P314"></a>

   machine network
   0 E 1 E M E →
   ↓ ↓
   T
   T
   0 T 1 T 2 T 3 T M T → →
   ( E )
   a
   recursive descent parser
   program ELL PARSER
   cc = next
   call E
   if cc then accept
   else reject end if
   end program
   procedure E
   - - optimized
   while cc ∈ {a(} do
   call T
   end while
   if cc ∈ {) then
   return
   else
   error
   end if
   end procedure
   recursive descent parser
   procedure T
   - - state 0 T
   if cc ∈ {a} then
   cc = next
   else if cc ∈ {(} then
   cc = next
   - - state 1 T
   if cc ∈ {a()} then
   call E
   else
   error
   end if
   - - state 2 T
   if cc ∈ {)} then
   cc = next
   else
   error
   end if
   else
   error
   end if
   - - state 3 T
   if cc ∈ {a() then
   return
   else
   error
   end if
   end procedure
   Fig.4.48 Main program and syntactic procedures of a recursive descent parser (Example4.75 and
   Fig.4.47); function next is the programming interface to the lexical analyzer or scanner; function
   error is the messaging interface
   4.7.4 A Graphical Method for Computing Guide Sets
   In this section we show how to compute by hand the guide sets by examining the
   net graph, for a machine net of small size. In this case, the present approach can
   profitably replace the preceding method based on the prospect set Eqs.(4.18) and
   (4.19), and on the guide set Eqs.(4.20)–(4.22). Since the equations require to know
   which nonterminals are nullable, and to have the set of the initials of the language
   acceptedstartingfromanetstate,webeginwiththese.Thenweconsidertheprospect
   and guide sets, and we exemplify the rules on complete nets. Finally, we show how

<a id="P315"></a>

   this approach may also provide additional insight into the rules for computing the
   ELR pilot look-ahead.

   Nullable Nonterminal
   A nonterminal A or its machine M A is nullable if (i) the initial state 0 A of M A is
   final or if (ii) in M A there is a path that connects state 0 A to a final state n A (n ?= 0)
   and the path label consists only of nullable nonterminals B, C, …, N, all of which
   differ from A. See the two cases on this machine:
   0 A 0 A 1 A
   ... n A
   A →
   →
   A →
   →
   B C N
   (i) (ii)
   nullable path
   All the states on the path may have other outgoing arcs. If the grammar is BNF, an
   alternative is to use the procedure for identifying the nullable nonterminals (on p.
   66).

   Set of Initials
   In what follows, we abbreviate Ini(L (q A )) in Ini(q A ) and Ini(L (B)) in Ini(B), as
   done in Definition4.30, and we remind that Ini(B) = Ini(0 B ). The case B = A is
   permitted, unless differently stated.

   The set Ini(q A ) of the initial terminals of the language of a state q A of a machine
   M A includes (see Definition4.30):
   (i) any terminal a that labels an arc from state q A
   (ii) the set Ini(B) of the initials of any nonterminal B that labels an arc from q A ,
   i.e., q A
   B
   − → r A , excluding the case 0 A
   A
   − → r A
   (iii) ifnonterminal B isnullable,alsothesetIni(r A )oftheinitialsofthelanguage
   of the destination state r A ; this implies to recursively go on adding initials as
   far as a nullable nonterminal path extends beyond B
   See the three cases on this partial net graph:
   ... (i)
   ... q A
   r A ...

   (ii and iii)
   A →
   ...

   a
   B ...

   nullable path (iii)
   Ini(q A ) = { a } ∪ Ini(B) ∪ Ini(r A )
   if nonterminal
   B isnullable

<a id="P316"></a>

   A set of initials may be empty, this is the case for the set of the initials of the
   language of a final state without outgoing arcs. 25
   A pitfall to be avoided is to involve a prospect set (see below). For instance, if
   state q A is final, this is not a reason for adding the prospect set π A to set Ini(q A ). In
   fact, the characters in the prospect set π A are not generated by machine M A , but by
   the machines that invoke M A .

   Prospect Set
   For a nonterminal A or its machine M A , all the final states have the same prospect
   set π A , conventionally written on the exit arrows. Thus, it suffices to compute set π A
   only once. Consider all the machines M X (including the case X = A) that have arcs
   labeled by A, i.e., q X
   A
   − → r X , and repeat these steps for each of such arcs, i.e., call
   sites, and machines (see (4.18) and (4.19)):
   (i) any terminal a that labels an arc from state r X
   (ii) the set Ini(B) of the initials of any nonterminal B that labels an arc from r X ,
   i.e., r X
   B
   − → s X
   (iii) ifnonterminal B isnullable,alsothesetIni(s X )oftheinitialsofthelanguage
   of state s X ; this implies to recursively go on adding initials as far as a nullable
   nonterminal path extends beyond B
   (iv) if from state r X there is a nullable nonterminal path to a final state f X , also
   the prospect set π X of M X , which has to be already known
   See the four cases on this partial net graph:
   ... (i)
   ... q X r X s X ... (ii and iii)
   ...

   f A f X
   (iv)
   X →
   → π X
   ... A
   a
   B ...

   nullable path
   A →
   → π A
   ...

   prospect set π A includes { a } ∪ Ini(B) ∪ Ini(s X )
   if nonterminal
   B is nullable
   ∪ π X
   if path
   r X → f X
   is nullable
   A particular instance of case (iv) is when state r X itself is final. The prospect set π A
   includes all the contributions collected as above for each arc q A
   A
   − → r X . In addition,
   25 More generally, we have an empty initial set for the language of a state connected to a final
   state without outgoing arcs, when the connection is exclusively through one (or more) nonterminal
   path(s) and every nonterminal on such path(s) can generate only the empty string ε. Indeed, this is
   a pathological case of no practical interest.


<a id="P317"></a>

   theaxiomaticprospectsetπ S alwaysincludestheterminator?,whichmaypropagate
   to other prospect sets. For this reason, a prospect set may never be empty, as at least
   it contains only the terminator ?.

   Another alternative graphical approach for computing the prospect set π A of
   machine M A , if the pilot is available, is to collect from its graph all the look-ahead
   terminals of all the final states f A of M A .

   Guide Set
   Eachterminalarc,exitarrowandcallarcofamachine M X hasanon-emptyguideset,
   conventionally written on the arc or arrow. The guide set on a terminal arc contains
   only the arc label. The guide set on an exit arrow of a final state f X is the prospect
   set of M X , i.e., Gui( f X →) = π X . Thus, we only need the rules for computing the
   guide set Gui(q X ??? 0 A ) on a call arc of M X , associated with a nonterminal arc
   q X
   A
   − → r X invoking machine M A :
   ... q X r X s X
   f X X →
   → π X
   Gui(f X →)
   ... A B ...

   0 A
   ...

   0 C
   ...

   A →
   C
   Gui(q X 0 A )
   call arc
   C →
   ...

   Gui(0 A 0 C )
   call arc
   call path
   We distinguish four cases, which correspond to those listed in (4.20):
   (i) put the initials Ini(A) into Gui(q X ??? 0 A )
   (ii) if nonterminal A is nullable, put also the initials Ini(r X ) into Gui(q X ??? 0 A );
   if nonterminal B is nullable as well, put also the initials Ini(s X ); this implies
   to recursively go on adding initials as far as a nullable path extends beyond B
   (iii) if nonterminal A is nullable and there is a nullable path B, …from state r X to
   a final state f X , put also the prospect set π X of M X into Gui(q X ??? 0 A ); this
   comprises the case that state r X itself is final
   (iv) if the call arc q X ??? 0 A is followed by a call arc 0 A ??? 0 C , put also the guide
   set Gui(0 A ??? 0 C ), which has to be already known, into Gui(q X ??? 0 A );
   and so on similarly for all the call arcs that may follow
   Incase(iv),apitfalltobeavoidedistoputtheguidesetofanexitarrowintoaguideset
   onacallarcpath;theirnamesimilaritymaymislead.Forinstance,ifstate0 A isfinal,
   this is not a reason for adding the guide set Gui(0 A →) = π A to Gui(q X ??? 0 A ).
   In fact, set π A contains all the terminals expected at the end of a computation of M A
   started from anywhere in the net, not only from q X
   A
   − → r X . Instead, in the case (iii),
   the prospect set to correctly add is π X of the invoking machine M X .


<a id="P318"></a>


### 4.7.4.1 Examples of Graphical Solution

   We conclude with four examples. First, we resume Example4.70 and 4.71 (running
   example), already iteratively solved, and we show the graphical approach. Then,
   we solve three more examples, only graphically. We omit the (trivial) guide sets on
   terminal arcs and pre-compute the initials only if necessary.

   Example 4.76 (graphical solution of Example4.70 and 4.71) See Fig.4.46 for the
   PCFG of Example4.70:
   nullability
   (i) 0 P is not final and (ii) it does not have any nullable path to a final state, thus
   P is non-nullable; 0 S is similar, since its outgoing P-arc is non-nullable, thus S
   is non-nullable as well
   prospect sets
   S is not invoked anywhere, thus its prospect reduces to π S = { ? }; P is invoked
   on 0 S
   P
   − → 1 S , 2 S
   P
   − → 1 S and 1 P
   P
   − → 2 P , thus its prospect receives (i) terminals “,”
   and a from 1 S and 2 P , respectively, and (iv) prospect π S , since 1 S is final, thus it
   results π P = { a, ‘,’,? }
   guide sets
   that of call 0 S ??? 0 P receives (i) only the initials a and c of P, since P is
   nonnullable, and similarly for calls 2 S ??? 0 P and 1 P ??? 0 P , thus it results { a, c }
   for the three calls
   See Fig.4.47 for the PCFG of Example4.71:
   nullability
   (i) 0 E is final, thus E is nullable; (i) 0 T is non-nullable and (ii) does not have any
   nullable path to a final state, thus T is non-nullable
   initials
   those of T are (i) only terminals a and “(” from 0 T , since 0 T does not have any
   nullable outgoing arc or path; those of E are (ii) only the initials a and “(” of T,
   since 0 E has a non-nullable outgoing T-arc
   prospect sets
   E (axiom) is invoked on 1 T
   E
   − → 2 T , and its prospect receives ? and (i) only the
   initial “)” of 2 T , since 2 T does not have any nullable outgoing arc or path,
   thus it results π E = { ‘)’, ? }; T is invoked on both 0 E
   T
   − → 1 E and 1 E
   T
   − → 1 E ,
   and its prospect receives (ii) the initials a and “(” of T itself, since 1 E has
   an outgoing T-arc, and (iv) the prospect π E , since 1 E is final, thus it results
   π T = { a, ‘(’, ‘)’, ? }
   guide sets
   that of call 0 E ??? 0 T receives (i) only the initials a and “(” of T, since T is
   non-nullable, and similarly for call 1 E ??? 0 T , thus it results { a, ‘(’ }; that of
   call 1 T ??? 0 E receives (i) the initials a and “(” of E, (ii) the initial “)” of 2 T ,

<a id="P319"></a>

   since E is nullable, and (iv) the guide set of the following call 0 E ??? 0 T , already
   computed, thus it results { a, ‘(’, ‘)’ } ?
   The next three examples focus on the details (and pitfalls) left uncovered by
   Example4.76. None of them turns out to be deterministic ELL(1).

   Example 4.77 (graphical approach to guide sets—I) See the PCFG in Fig.4.49.
   Axiom S is non-nullable. Nonterminal B is nullable since state 0 B is final. Axiom S
   isinvokedonarc0 B
   S
   − → 1 B andtheinitialsofstate1 B areterminalb,thustheprospect
   set of S is { b, ? }. Nonterminal B is invoked on arcs 1 S
   B
   − → 2 S and 2 B
   B
   − → 1 B , and
   the initials of states 2 S and 1 B are terminals c and b, respectively, thus the prospect
   set of B is { b, c }.

   Theinitialsofaxiom S,i.e.,ofstate0 S ,areterminala.Theinitialsofnonterminal
   B, i.e., of state 0 B , are (recursively) those of S, i.e., a, and nothing else as S is
   non-nullable. The initials of B do not include the prospect set of B, despite (pitfall)
   state 0 B is final, as terminals c and b are not generated by the invoked machine M B
   as its first symbols, i.e., starting from 0 B . Instead, they are, respectively, generated
   by the invoking machines M S and (recursively) M B , the latter yet starting from state
   1 B (not from 0 B ).

   The guide set on the call 0 B ??? 0 S associated with 0 B
   S
   − → 1 B is (i) the initials
   a of S since S is non-nullable, thus it is { a }. The guide set on the call 1 S ??? 0 B
   associated with 1 S
   B
   − → 2 S includes (i) the initials a of B, (ii) the initials c of 2 S since
   B is nullable, and (iv) the guide { a } on the concatenated call 0 B ??? 0 S (to the
   same effect as (i)), thus it is { a, c }. Similarly, the guide set on the call 2 B ??? 0 B
   associated with 2 B
   B
   − → 1 B includes (i) again the initials a of B, (ii) the initials b of
   1 B since B is nullable, and (iv) again the guide { a } on call 0 B ??? 0 S , thus it is
   { a, b }.

   The guide set on call 1 S
   a, c
   − → 0 B does not include terminal b, despite (pitfall) b
   occurs on exit 0 B
   b, c
   −−→, as such an occurrence is due to the other call 2 B
   a, b
   − → 0 B ,
   and it includes terminal c because B is nullable and c follows the call to B (not
   because c is on the exit from 0 B ). Similarly, call 2 B
   a, b
   − → 0 B does not include c in
   its guide set, and it includes b instead. ?
   Example 4.78 (graphical approach to guide sets—II) See the PCFG in Fig.4.50.
   Nonterminals A and B are nullable. Axiom S is non-nullable. The initials of A are a
   (pitfall—not b and c), those of B are b (pitfall—not c and ?), and those of S are a,
   Ini(A), i.e., again a, and (iii) Ini(1 S ), i.e., c, since A is nullable, thus it is a and c.
   The prospect set of S is just { ? }, as S is not invoked anywhere. The prospect
   set of A includes b and c, since A is invoked on 1 A
   A
   − → 2 A followed by b and on
   0 S
   A
   − → 1 S followed by c, thus it is { b, c }. The prospect set of B includes c and ?,

<a id="P320"></a>

   0 S 1 S 2 S
   0 B 1 B 2 B
   S →
   ↓
   { b,
   a
   B
   c
   B → → { b, c }
   ↓
   { b, c }
   S
   b
   B
   { a, c }
   { a }
   { a, b }
   Fig.4.49 PCFG for Example4.77—state 2 B has a conflict on terminal b
   0 A 1 A 2 A 3 A A → → { b, c }
   ↓
   { b, c }
   a A b
   1 S
   0 S 3 S
   2 S
   S → →
   A
   a
   c
   B
   0 B 1 B 2 B 3 B B → → { c,
   ↓
   { c,
   b B c
   { a, c }
   { a, b }
   { b,
   { b, c }
   Fig.4.50 PCFG for Example4.78—state 0 S has a conflict on terminal a
   since B is invoked on 1 B
   B
   − → 2 B followed by c and (iv) on 2 S
   B
   − → 3 S , and state 3 S is
   final with prospect ?, thus it is { c, ? }.

   The guide set on call 1 A ??? 0 A includes Ini(A), i.e., a, and (iii) Ini(2 A ), i.e., b,
   since A is nullable, thus it is { a, b }. The guide set on call 0 S ??? 0 A includes a,
   Ini(A), i.e., again a, and (iii) Ini(1 S ), i.e., c, since A is nullable, thus it is { a, c }.
   The guide set on call 2 S ??? 0 B includes Ini(B), i.e., b, and (iv) the prospect set of
   S since state 3 S is final with prospect ?, thus it is { b, ? }. The guide set on call

<a id="P321"></a>

   1 B ??? 0 B includes Ini(B), i.e., b, and (iii) Ini(2 B ), i.e., c, since B is nullable, thus
   it is { b, c }.

   Notice this subtle difference: symbol ? is on call 2 S
   b,?
   − → 0 B , since that call to
   B returns to state 3 S , final with ? as prospect; instead, it is not on call 1 B
   b,c
   − → 0 B
   (despite it is on the exit from 0 B ), as state 2 B is not final and does not go to a final
   one through a nullable path. We see that other guide sets on calls do not necessarily
   include all the symbols on a chained exit, e.g., 1 A
   a,b
   − → 0 A and 0 A
   b,c
   −−→ with c not
   on call but on exit, for the reason that a guide set on a call is specific to that call site
   only (see also Example4.77). ?
   Example 4.79 (graphical approach to guide sets—III) See the PCFG in Fig.4.51.
   Nonterminal X isnullable.NonterminalY hasanullablepathoflength1tofinalstate
   1 Y ,thus(ii)itisnullable.Axiom S isnon-nullable.Mostsetsareeasytocomputeand
   here we do not detail all of them. The exit guide set of X includes b, since an X-call
   enters 1 Y (final) with prospect b, and includes c and e since X-calls are followed by
   such terminals (e is on a self-loop). The guide set on call 0 Y ??? 0 X includes b since
   X is nullable and enters 1 Y (final) with prospect b, includes c since X is nullable
   and the call is followed by it, and includes d since it is the initial of X. This guide
   set propagates upstream to call 2 S ??? 0 Y .

   Notice that terminal e is on call 1 X
   b, c, d, e
   − − → 0 X , because X is nullable and after
   returning from that call the computation may find e (on self-loop 2 X
   e
   − → 2 X ), not
   because it is also in the prospect of X. Furthermore, terminal e is on exit 0 X
   b, c, e
   −−−→,
   0 S 1 S 2 S 3 S S →
   ↓
   a S
   b
   Y
   0 Y 1 Y 2 Y Y → → {b}
   ↓
   {b}
   X c
   0 X 1 X 2 X X →
   ↓
   {b, c, e}
   → {b, c, e}
   d X
   e
   {a}
   {b, c, d}
   {b, c, d}
   {b, c, d, e}
   Fig.4.51 PCFG for Example4.79—state 2 X has a conflict on terminal e

<a id="P322"></a>

   yet it does not propagate upstream onto call path 2 S
   b, c, d
   − → 0 Y
   b, c, d
   − → 0 X , as it is
   visibly unrelated to those calls. ?

### 4.7.4.2 Relation Between Prospect Sets and Pilot Look-Ahead Sets

   Returning to the look-ahead computation in the ELR pilot, already solved through
   the closure function (4.8) and Algorithm4.35, we notice that a graphical approach
   reusing part of what said for the PCFG may help here as well:
   Each nonterminal arc q X
   A
   − → r X in the net causes, in every pilot m-state I 1 that
   contains a candidate ?q X , ρ?, the insertion by closure of a new initial candidate
   ?0 A , π ? with a look-ahead set π computed by the rule above.

   network pilot
   ...

   X
   q X r X s X
   f X →
   ... A
   a
   B ...

   nullable
   path
   q X ρ
   ... ...

   0 A π
   ... ...

   r X ρ
   ... ...

   ... ...

   I 1
   I 2
   A
   look-ahead set π = { a } ∪ Ini(B) ∪ Ini(s X )
   if B is nullable
   and so on as far as a
   nullable path follows
   ∪ ρ
   if path
   r X → f X
   is nullable
   part identical to the prospect set rule on p. 316
   part different
   Notice the similarity with the rule that computes the contribution of q X
   A
   − → r X to
   the prospect set π A of nonterminal A (on p. 316), but here the last term is ρ, i.e., the
   look-ahead inherited from the state q X invoking A (not the prospect set π X ); the two
   rule parts are framed. However, the prospect set π A receives all the look-ahead sets
   of all the final candidates of A.

   A simple case: in Fig.4.49, computing the closure of candidates ?2 B , ...? and
   ?1 S , ...?, respectively, yields ?0 B , b? and ?0 B , c?. Since nonterminal B has only
   these two call sites, the union of the two look-ahead singletons b and c yields the
   prospect set π B = { b, c }.


## 4.7.5 Increasing Look-Ahead inTop-Down Parsers

   A pragmatic approach for obtaining a deterministic top-down parser when the
   grammardoesnotcomplywithconditionELL(1)istolook-aheadofthecurrentcharacter
   and examine the following ones. This often suffices to make deterministic the choice
   at bifurcation points and has the advantage of not requiring annoying modifications
   of the original grammar.

   Algorithm4.73 on p. 311 (or its recursive descent version) has to be slightly
   modified, in order to examine in a bifurcation state the input characters located at

<a id="P323"></a>

   k > 1 positions ahead of the current one, before deciding the move. If such a test
   succeeds in reducing the choices to one, we say the state satisfies condition ELL(k).
   A grammar has the ELL(k) property if there exists an integer k ≥ 1 such that, for
   everynetmachineandforeverystate,atmostonechoiceamongtheoutgoingarrows
   is compatible with the characters that may occur ahead of the current character, at a
   distance less than or equal to k.

   For brevity, we prefer not to formalize the definition of the guide set of order k,
   since it is a quite natural extension of the basic case, and we directly proceed to
   exemplify the computation of a few guide sets of length k = 2.

   Example 4.80 (conflict between instruction labels and variable names) A small
   fragment of a programming language includes lists of instructions (variable 
   assignments, for statements, etc.), with or without a label. Both labels and variable names
   are identifiers. The EBNF grammar of this language fragment, just sketched for the
   example, is as follows (axiom progr) 26 :
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   ?progr? →
   ?
   ?label? ‘:’
   ?
   ?stat?
   ?
   ‘;’ ?stat?
   ? ∗
   ?stat? → ?assign_stat? | ?for_stat? | ...

   ?assign_stat? → id ‘=’ ?expr?
   ?for_stat? → for id ...

   ?label? → id
   ?expr? → ...

   The net machines are depicted in Fig.4.52 with the relevant guide sets. To avoid
   clogging, only two call arcs are represented with their guide sets of order k = 1, 2,
   the latter framed. The other guide sets are on the shift arcs and final darts exiting the
   furcationnodes.Thestatesaresimplyenumerated,andthehooksaroundnonterminal
   names are omitted. Clearly, state 0 is not ELL(1): the guide sets of length k = 1 (not
   framed)onthetwocallarcsthatleavestate0,sharingtheterminalid,arenotdisjoint.
   By refining the analysis, we observe that if the identifier is an instruction label, i.e.,
   arc 0
   label
   −−→ 1, then it is followed by a colon “:” character. On the other hand, in the
   case of arc 0
   stat
   −−→ 3, the identifier is the left variable of an assignment statement and
   is followed by an equal “=” character. 27 We have thus ascertained that inspecting
   the second next character suffices to deterministically resolve the choice of the move
   from state 0, which therefore satisfies condition ELL(2). In the PCFG, the call arcs
   leaving state 0 are labeled with the guide sets of length k = 2 (framed), which are
   disjoint. Such pre-computed sets will be used at run-time by the parser. On the other
   hand, in the bifurcation states 3 and 4, a look-ahead length k = 1 suffices because
   26 The symbols within brackets ? ? are nonterminals, all the others are terminals or are assumed to
   be terminals; and the square brackets mean their contents are optional.

   27 Inthecaseoftheforstatement,forsimplicityweassumethatthelexemeforisareservedkeyword,
   which may not be used as a variable or label identifier.


<a id="P324"></a>

   partial PCFG
   4 5 stat →
   →
   assign stat
   { id }
   { for }
   for stat
   0 1 2 3
   progr →
   label
   stat
   ‘:’
   stat
   ‘;’
   { ‘;’ }
   13 14 label →
   →
   id
   6 7 8 9 assign stat →
   →
   id ‘=’
   expr
   10 11 12 for stat →
   −→ ...

   for id
   { id, for } (k = 1)
   { id ‘=’, for id } (k = 2)
   { id } (k = 1)
   { id ‘:’ } (k = 2)
   Fig. 4.52 Machine net and the relevant parts of the parser control-flow graph of Example4.80.
   With look-ahead length k = 2 the graph is deterministic
   the guide sets on the outgoing shift arcs and on the final dart are disjoint. It would
   be wasteful to use the maximal look-ahead length everywhere in the parser. ?
   In practice, parsers use different look-ahead lengths with an obvious economy
   criterion: in each bifurcation state, the minimum length m ≤ k needed to arbitrate the
   choice of the branches should be used.

   Some top-down parsers that are more powerful, though computationally less
   efficient, have also been designed. A well-known example is the ANTLR parser [28],
   whichusesalook-aheadofvariableandunboundedlength,dependingonthecurrent
   parser state. Other parsers, if the choice in a bifurcation state remains uncertain, take
   other cues into consideration. One possibility is to compute some semantic 
   condition, an idea to be worked out in the last chapter. Of course, all such parsers can no
   longer be considered as deterministic pushdown automata, because they use other
   information pieces in addition to the stack contents and, in the case of unlimited
   look-ahead, they are able to perform multiple scans on the input.


<a id="P325"></a>

4.8 Operator-Precedence Grammars and Parallel Parsing
=====================================================

   Inmanyapplicationareas,algorithmsthatoriginallyhadbeendesignedforsequential
   execution later have been transformed into parallel algorithms, to take advantage of
   the available multiple processors on current computers for reducing running time.
   For language parsing, a straightforward approach is to split the source text into as
   many segments (substrings) as the number of available processors, to parse each
   segment on a different processor by using a sequential parser, and then to recombine
   the resulting partial parse trees into the complete tree. In principle the speed-up
   obtained with respect to the original sequential algorithm scales with the number of
   processors, but there are conceptual and practical obstacles on this way that limit
   scalability.

   First and foremost, neither the LR(1) nor the LL(1) parsing algorithms can be
   easily transformed into parallel programs because they need to scan the text 
   sequentially from left to right and their decisions, whether to shift or to reduce, may depend
   on distant past decisions. Therefore, the parser of, say, the second segment may be
   unable to proceed (deterministically) until the parser of segment one has finished,
   which thus defeats the benefit of parallelization. To illustrate, observe the 
   deterministic (LR(1) and also LL(1)) language below:
   L =
   L 1
   ?
   c ∗ oc ∗ a n b n | n ≥ 1
   ?
   ∪
   L 2
   ?
   c ∗ t c ∗ a n b 2n | n ≥ 1
   ?
   such that the presence of letter o (respectively t) announces that each later letter a
   will be followed by one letter b (respectively two letters b). Imagine to parse the
   segmented text shown below:
   segment 1
   cc ... ct cc ... c
   segment 2
   a a ... a bb ... b
   Since the second parser does not know whether the first one has encountered a letter o
   ort,itfacesthetaskofanalyzinglanguage{ a n b n | n ≥ 1 } ∪
   ?
   a n b 2n | n ≥ 1
   ? ,
   which is nondeterministic (as explained on p. 216 for a similar case). Notice that an
   arbitrary distance separates the positions of substrings o (or t) and a bb, where the
   choice of the reduction a b ? ... or a bb ? ... is due. This example shows that, if
   our goal is to deterministically parse separate text segments, we must exclude any
   deterministic language that exhibits such a long-distance dependence between
   operations performed by independent parsers. In other words, to be suitable for parallel
   parsing, a grammar, besides being LR(1), should not require parsing information
   to be carried to another substring over a long stretch of text. We loosely call local
   parsability such a grammar property.

   We describe a very efficient sequential parsing algorithm that bases its parsing
   decisionsonthelocalpropertiesofthetext,therebyitlendsitselftoparallelexecution.

<a id="P326"></a>


## 4.8.1 Floyd Operator-Precedence Grammars and Parsers

   Floyd[29,30],apioneerofcompilertechniques,tookinspirationfromthetraditional
   notion of precedence between arithmetic operators, in order to define a class of
   languages such that the shape of the parse tree is solely determined by a binary
   relation between terminals that are consecutive, or that become consecutive after
   a few bottom-up reduction steps. The Floyd approach assumes that the grammar
   has been normalized in the operator form (see Sect. 2.5.13.7 on p. 74); we recall
   that a rule is in the operator form if any two nonterminals that occur in the right
   part of the rule are separated by at least one terminal character. Many practical
   grammars are already in the operator form, e.g., the grammar of the Dyck language:
   S → b S e S | b S e | be. Otherwise, we have seen in Chap. 2 how to construct an
   equivalent grammar in the operator form.

   All the grammars in this section are purely BNF in the operator form and free
   from empty rules.

   Precedence Relations
   We introduce the concept of precedence relation: this term actually means three
   partial binary relations over the terminal alphabet, named:
   ? yields precedence
   ? takes precedence
   ˙ = equal in precedence
   For a given grammar and any pair of terminal characters a, b ∈ Σ, the precedence
   relation may take one or more of the above three values, or may be undefined. But
   for the operator-precedence grammars to be soon defined, for every character pair a,
   b, the relation, if it is defined, is unique.

   To grasp the intended meaning of the relations, it helps to start from the language
   of the arithmetic expressions with the operators plus “+” (addition) and times “×”
   (multiplication). Since traditionally times take precedence over plus, the following
   operator-precedencerelationshold:× ? +and+ ? ×.Theformerrelationmeans
   that in an expression such as 5 × 3 + 7, the first reduction to apply is 5 × 3 ? ···.
   Similarly, the fact that plus yields precedence to times, says that in an expression
   such as 5 + 3 × 7, the first reduction to apply is 3 × 7 ? ···.

   We are going to extend this traditional sense of precedence from the arithmetic
   operators to any terminal characters. Before that, we have to explain the meaning
   of the “equal in precedence” relation a ˙ =b. It means that a grammar rule contains a
   substring a Ab where A is a nonterminal or is null, i.e., between a and b there is not
   another terminal. Thus in the grammar for parenthesized arithmetic expressions, the
   leftandtherightparenthesesareintherelation“(” ˙ = “)”duetotherule E → ( F ).
   Awordofwarning:noneofthethreerelationsenjoysthemathematicalproperties
   of reflexivity, symmetry and transitivity, of the numerical order relations <, > and
   =, notwithstanding the typographical similarity.


<a id="P327"></a>

   We observe that for an operator grammar, every string β that derives from a
   nonterminal has the property that any two nonterminals are separated by one or
   more terminal characters. More precisely we have:
   if A
   ∗
   = ⇒ β then β ∈ Σ ∗
   ?
   V N Σ +
   ? ∗
   ∪ Σ ∗
   ?
   V N Σ +
   ? ∗
   V N
   We also say that such strings β are in the operator form. More explicitly, a string in
   the operator form takes one of the forms below:
   a ∈ Σ or A ∈ V or B σ C (4.23)
   where B and C are nonterminals possibly null, and the string σ is in operator form
   and starts and ends with a terminal character.

   To construct a precedence parser for a given grammar, we have to compute the
   precedence relations and to check that they do not conflict in a sense to be explained.
   We need to define two sets of terminal characters, which are similar to the sets of
   initial and final characters of a string, but differ because the nonterminal symbols
   are considered to be always nullable.

   Definition 4.81 (left/right terminal sets) The left and right terminal sets of a string
   in the operator form, respectively, denoted L and R, are recursively defined by:
   L(a) = { a } R(a) = { a } for a ∈ Σ
   L(A) =
   ?
   ∀ rule A→α
   L(α) R(A) =
   ?
   ∀ rule A→α
   R(α)
   L(B σC) = Ini (σ) ∪ L(B) R(B σC) = Fin (σ) ∪ R(C)
   (4.24)
   where B, C and σ are as in (4.23) above, and Ini and Fin denote the initial and final
   characters of a string. ?
   Example 4.82 (operator-precedence grammar of arithmetic expressions) A
   grammar G for certain arithmetic expressions is shown in Fig.4.53, left. The left/right
   terminal sets of the strings that occur in the grammar as left- or right-hand sides,
   computed by Eq. (4.24), are listed in Table4.8. ?
   To compute the precedence relations we need the following definition.

   Definition 4.83 (precedence relations) Given an operator grammar, let B be a
   nonterminal symbol, and let a and b be terminal characters. The grammar 
   operatorprecedence (OP) relations over Σ × Σ are:

<a id="P328"></a>

   grammar G operator precedence matrix M
   S → A | B
   A → A + B | B + B
   B → B × n | n
   n + ×
   n
   +
   ×
   ˙ =
   Fig. 4.53 Operator-precedence grammar for certain arithmetic expressions and its 
   operatorprecedence matrix (read a matrix entry as row rel. col, e.g., n ? +, etc)
   Table 4.8 Some left and right terminal sets for the grammar in Fig.4.53
   String Left terminal set L Right terminal set R
   n n n
   B × n { n, × } n
   B { n, × } n
   B + B { n, ×, + } { n, + }
   A + B { n, ×, + } { n, + }
   A { n, ×, + } { n, + }
   S { n, ×, + } { n, + }
   takes precedence a ? b iff some rule contains a substring B b and a ∈ R(B)
   yields precedence a ? b iff some rule contains a substring a B and b ∈ L(B)
   equal in precedence a
   .

   = b iff some rule contains a substring a B b or a b
   The operator-precedence matrix (OPM), M, of a grammar is a square array of size
   |Σ |thatassociatestoeachorderedpair(a, b)theset M a,b ofOP relationsholding
   between the terminals a and b.

   A grammar has the operator-precedence property (we also say it is an 
   operatorprecedence grammar) if, and only if, each matrix entry contains at most one relation.
   In that case, the matrix qualifies as conflict-free. ?
   We refer the reader to Fig.4.54 for a schematic illustration of the meaning of 
   precedence relations.

   As said, the precedence relations are not necessarily symmetric, in the sense that
   relation a ? b does not imply that relation b ? a holds; similarly relation a ˙ =b
   does not imply that relation b ˙ =a holds.

   For the current Example4.82, we compute the OPM in Fig.4.53, right, by using
   the left/right terminal sets listed in Table4.8, and by inspecting the substrings that

<a id="P329"></a>

   A
   A
   A + B
   + B
   n
   + +
   + n
   B
   B
   B × n
   × n
   n ×
   × ˙ =n
   Fig.4.54 Precedence relations made apparent in the derivations of Example4.82
   occur in the following rules:
   rule terminal set precedence relations
   ··· → A + ... R(A) = { n, + } n ? + and + ?+
   ··· → B + ... R(B) = { n } n ? +
   ··· → ... + B L(B) = { n, ×} + ? n and + ?×
   ··· → ... × n × ˙ =n
   The grammar has the OP property since its OPM is conflict-free.

   Wecommentthemeaningofafewrelations.TheOPM entries+ ? n andn ? +
   indicate that, when parsing a string containing the pattern ... + n + ..., the right
   part n of rule B → n has to be reduced to nonterminal B. The OPM entry (n, n ) is
   emptybecauseinavalidstring,terminaln isneveradjacenttoanothern,orseparated
   by a nonterminal as in the patterns ... n An ... or ... n B n ...; in other words no
   sentential form may contain such patterns. Similarly, there is no relation between
   the operators × and +, because none of the patterns ... × + ..., ... × A + ...
   and ... × B + ... ever occurs in a sentential form. We observe that relation ˙ =
   is connected with an important grammar parameter, namely the length of the rule
   right-handsides.Clearlyarule A → A 1 a 1 ... A t a t A t+1 ,whereeachsymbol A i is
   a possibly missing nonterminal, is associated with the relations a 1 ˙ =a 2 ˙ = ··· ˙ =a t .
   The next example (Example4.84) refers to a real language, which turns out to
   have a syntax ideal for precedence parsing.

   Example 4.84 (grammar and OP matrix of JSON) The JavaScript Object Notation
   (JSON language) has been introduced in Chap.2 in Example2.43 on p. 48, where its
   officialgrammarislisted.WereproduceinFig.4.55thegrammarwithaminorchange
   needed for the operator form; in the same figure we show the operator-precedence
   matrix,whichisconflict-free.Theverylargesizeofthetypicaldatafilesrepresented
   in JSON, and the fact that the official grammar is essentially in the operator form
   and without precedence conflicts, make parallel parsing attractive. Notice that in the
   OPM a majority of cases is void. This is typical of practical grammars, whose OPM
   matrix is frequently quite sparse. ?

<a id="P330"></a>

   S → OBJECT
   OBJECT → { } | { MEMBERS }
   MEMBERS → PAIR | PAIR , MEMBERS
   PAIR → STRING : VALUE
   VALUE → STRING | OBJECT | ARRAY | num | bool
   STRING → | CHARS
   ARRAY → [ ] | [ ELEMENTS ]
   ELEMENTS → VALUE | VALUE, ELEMENTS
   CHARS → char | char CHARS
   operator precedence matrix
   { } , : num bool char [ ]
   { ˙ =
   }
   ,
   :
   num
   bool
   ˙ =
   char
   [ ˙ =
   ]
   Fig.4.55 OperatorgrammarandOP matrixofJSON.Uppercaseandlowercasewordsdenotenon-
   terminals and terminals, respectively. To cast the grammar into operator form, only the underlined
   rule has been changed with respect to the official JSON grammar (listed in Example2.43 on p. 48)

## 4.8.2 SequentialOperator-Precedence Parser

   Precedence relations precisely control whether a substring that matches the right
   part of a rule (also referred to as the handle) has to be reduced to the nonterminal on
   the rule left part. This control is very efficient and, unlike the conditions examined
   by LR(1) parsers, it does not rely on state information. A minor difficulty remains
   however,ifthegrammarincludestworulessuchas A → x and B → x withthesame
   right part: how to choose between nonterminals A and B when string x is reduced.
   A possibility is to represent such uncertainty as a set of nonterminals, i.e., { A, B },
   and to propagate it across the following reductions, until just one choice remains

<a id="P331"></a>

   (otherwise the grammar would be ambiguous). But, to simplify matters, we assume
   that the grammar is free from repeated right parts, i.e., it is invertible, a property that
   can be always satisfied by the transformation presented in Chap. 2 on p. 69.
   Operator-precedence grammars are an ideal choice for parallel parsing. For
   sequential parsing, they are occasionally adopted, in spite of their limitations with
   respecttoELR(1)grammars,becausetheparsersaresimplertoconstructandequally
   efficient. As the parallel algorithm stems directly from the sequential one, we start
   from the latter.

   The key idea driving the algorithm is simple. The algorithm scans the input, and
   wherever a series of ˙ = precedence relations enclosed by the relation pair ?, ? is
   foundbetweenconsecutiveterminals,theenclosedsubstringisareductionhandle.To
   findhandles,theparserusestheOPM andstoresthecurrentinputandtheprecedence
   relations on a pushdown stack S, until the next ? relation occurs. Then the handle
   is the portion of stack encompassed by the most recent ? and the top.

   For convenience, the input string is enclosed by the special character #, acting as
   end-markers. This is tantamount to adding the new axiom S 0 and the axiomatic rule
   S 0 → # S #. Therefore, character # yields precedence to every character in the left
   terminal set of S and every character in the right terminal set of S takes precedence
   over #.

   To prepare for its parallel use, we enable the parsing algorithm to analyze also
   the strings that may contain nonterminal characters. Such strings begin and end with
   terminal characters (or with character #) and are in the operator form. This setting is
   only needed when the parallel algorithm is called to parse internal segments of the
   input text.

   EachstacksymbolcontainedinS isapairoftheform(x, p),wherex ∈ Σ ∪ V
   and p is one of the precedence relations { ?, ˙ =, ? } or is undefined (denoted
   by ⊥). Component p is used to encode the precedence relation existing between
   two consecutive symbols; conventionally, we set p = ⊥ for nonterminals. In the
   following representations, we assume that the stack grows rightward. 28
   Algorithm 4.85 (operator-precedence sequential parser)
   OPparser ( S, u, head, end )
   S: stack of couples u: string to analyze head, end: pointers in u
   conventions and Settings
   denote u = u 1 u 2 ... u m and s = u 2 ... u m−1 with m > 2
   denote u 1 = a and u m = b where a and b are terminal
   for sequential use only:
   denote the initial stack contents as S = (a,⊥)
   set the initial pointers head := 2 and end := m
   28 See [31] for further reading on this algorithm, and [18] for the simpler version, which is however
   unfit for parallel execution.


<a id="P332"></a>

   begin
   1. let x = u head be the symbol currently pointed by head and consider the 
   precedence relation between x and the topmost terminal y in the stack S
   2. if x is a nonterminal then
   push (x,⊥) head + +
   3. else if y ? x then
   push (x,?) head + +
   4. else if y
   .

   = x then
   push (x,
   .

   =) head + +
   5. else if y ? x then
   a. if stack S does not contain ? then
   push (x,?) head + +
   b. else
   let the stack S be (z 0 , p 0 ) ... (z i−1 , p i−1 )(z i ,?) ... (z n , p n ) where z i
   (more precisely p i ) is the topmost ? relation, i.e., for every stack element
   on the right of z i the precedence relation is ˙ = or ⊥
   i. if z i−1 is a nonterminal and ∃ rule A → z i−1 z i ... z n then
   replace (z i−1 , p i−1 )(z i ,?) ... (z n , p n ) in S with (A,⊥)
   ii. else if z i−1 is a terminal or # and ∃ rule A → z i ... z n then
   replace (z i ,?) ... (z n , p n ) in S with (A,⊥)
   iii. else
   start an error recovery procedure 29
   end if
   end if
   end if
   6. if head < end or
   ? head = end and S ?= (a,⊥)(B,⊥) ?
   for any nonterminal B
   then goto step (1) else return S end if
   end ?
   29 An error is detected either when the handle, i.e., the string included between relations ? and ?,
   does not match any right-hand side, or when no precedence relation holds between two consecutive
   terminal characters.


<a id="P333"></a>

   stack input string relation or reduction step
      (#,⊥) #
      head
      n + n × n
      end
      # # n 1
      (#,⊥)(n, )
      head
      + n × n# n + n B 2
      (#,⊥)(B,⊥)
      head
      + n × n# # + 3
      (#,⊥)(B,⊥)(+, )
      head
      n ×n# + n 4
      (#,⊥)(B,⊥)(+, )(n, )
      head
      × n# n × n B 5
      (#,⊥)(B,⊥)(+, )(B,⊥)
      head
      × n# + × 6
      (#,⊥)(B,⊥)(+, )(B,⊥)(×, )
      head
      n # ×=n 7
      (#,⊥)(B,⊥)(+, )(B,⊥)(×, )(n, ) =˙
      head
      # n # B × n B 8
      (#,⊥)(B,⊥)(+, )(B,⊥)
      head
      # + # B + B A 9
      (#,⊥)(A,⊥)
      head
      end
      0 1 #
      ˙
   Fig.4.56 Steps of the operator-precedence sequential parser for the input a + a × a (grammar
   and OPM in Fig. 4.53)
   Ifnoerroroccurs,thealgorithmterminatesinaconfigurationthatcannotbechanged
   either by shift or by reduce actions, and we say that it is irreducible.

   When used sequentially and not as a part of the parallel version to be next
   presented,thealgorithmiscalledwithparametersu = #s #andS = (#, ⊥)(soa = #),
   it never performs step 2, never pushes (x, ?) onto the stack, and accepts the input
   string only if the stack is S = (#, ⊥)( S, ⊥).

   We comment a few steps of the parsing trace shown in Fig.4.56. Step 1 shifts
   token n on the stack, then step 2 detects the pattern ?n ? and reduces the handle
   n to B. At step 3, the topmost terminal on stack is #, which yields precedence to
   the current token + that is shifted. Skipping to step 7, the stack top × equals in
   precedence token n, which is therefore shifted. Step 8 identifies the handle B × n
   in the pattern ? B × ˙ =n ?; notice that the handle includes a nonterminal B, which
   occurs between the terminal characters related by + ? ×.


## 4.8.3 Comparisons and Closure Properties

   Let OP denote the family of languages defined by OP grammars. We start with the
   obviousremarkthatfamilyOPisincludedintothefamilyDET,sinceAlgorithm4.85
   clearly implements a deterministic pushdown machine.


<a id="P334"></a>

   The following examples permit to compare the languages of OP and those of
   LL(1) and LR(1) grammars.

   A language that is deterministic but is known not to be in OP is
   ?
   a n b a n | n ≥ 0
   ? ,
   generated by the grammar:
   S → a S a | b with matrix
   a b
   a ? ? ˙ = ?
   b ?
   There are conflicts in the matrix entry [a, a ], yet the grammar meets the LL(1) and
   therefore also the LR(1) condition.

   On the other hand there exist OP languages that are not in the LL(1) family. An
   exampleisthelanguage{ a ∗ a n b n | n ≥ 1 }(seeFig.4.35onp.285).Itisgenerated
   by the conflict-free OP rules S → a S | B and B → a B b | a b.

   Each OP grammar has the peculiar property of symmetry with respect to mirror
   reversal, in the sense that the grammar obtained by reversing the right parts of the
   rules is also an OP grammar. Its precedence matrix simply interchanges the “yield”
   and “take” relations of the original grammar, and reverses the “equal in precedence”
   relation. To illustrate we list another grammar G:
   G
   ⎧
   ⎨
   ⎩
   S → X b
   X → a X b | a b
   with OPM
   a b
   a ? ˙ =
   b ?
   The reversed language ( L (G) ) R is defined by the “reversed” OP grammar G R :
   G R
   ⎧
   ⎨
   ⎩
   S → b X
   X → b X a | ba
   with OPM
   a b
   a ?
   b ˙ = ?
   On the other hand, in general, neither an LL(1) nor an LR(1) grammar remains such
   when it is reversed. For instance, grammar G above has the LL(1) property, but the
   reversed grammar G R does not, though it remains LR(1). The case of a language L
   that ceases to be deterministic upon reversal is illustrated by:
   L =
   ?
   oa n b n | n ≥ 1
   ?
   ∪
   ?
   t a 2n b n | n ≥ 1
   ?
   Here the first character (o or t) determines whether the string contains one letter a
   per b or two letters a per b. The reversed language L R contains sentences of two
   types, say, b 4 a 4 o and b 4 a 8 t. A pushdown machine cannot deterministically decide
   when to pop a stack symbol standing for a letter b upon reading an a.

   We collect the preceding findings into the next property, which also deals with
   the families of regular and of input-driven languages (Definition4.20 on p. 225).

<a id="P335"></a>

   Property 4.86 (Comparison of OP and other deterministic families)
   1. Family OP is strictly included in the family DET of deterministic languages, i.e.,
   the family of LR(1) languages, see Property4.93 on p. 348.

   2. The families of OP languages and of LL(1) languages are not comparable.
   3. Family OP strictly includes the family of input-driven languages, therefore also
   the family of regular languages.

   4. The reversal L R of every OP language L has the OP property. More precisely, if
   L is generated by the grammar G then L R is generated by the reversed grammar
   G R . The OPM of G R interchanges the “yield” and “take” relations of G, and
   reverses the “equal in precedence” relation. ?
   Sincewehavealreadycommentedstatements1,2 and4,andtheinclusionofregular
   languages within input-driven ones has been discussed in Sect. 4.3.4.2 on p. 227, it
   remainstodiscussstatement3,i.e.,thateverylanguagerecognizedbyaninput-driven
   PDA is also an OP language.

   We start from the elementary case that R is a regular language over alphabet
   Σ internal , and weassume, without any loss of generality, thatit is defined bya strictly
   left-linear grammar G (Table4.4 on p. 227), which clearly is in the operator form.
   Each rule of G has one of the forms (disregard for simplicity the end-markers #)
   A → B a and A → a, where A and B are nonterminal symbols and a is terminal.
   By applying Definition4.83, we easily discover that grammar G cannot have any
   “yield” or “equal” relation, thus leaving us with the “takes” precedence relation. It
   followsthattheprecedencematrixofanystrictlyleft-lineargrammarisconflict-free.
   Notice that the same property holds for a strictly right-linear grammar, except that
   the only possible relations are “yield” precedence.

   Aftershowingthe(obviouslystrict)inclusionofthelanguagefamiliesREG ⊂ OP,
   we move to the general case of an ID language L over the 3-partitioned alphabet
   Σ call ∪ Σ return ∪ Σ internal . We are going to rely in part on the reader intuition 30 in
   thenextpresentationofthecorrespondencebetweentheinput-drivensubalphabetsof
   two terminal characters and the precedence relations that may occur in an equivalent
   OP grammar.

   Foreachinput-drivenlanguage L withalphabetΣ call ∪ Σ return ∪ Σ internal there
   existsanOP grammarthatgenerateslanguage L andhasanOPM withthefollowing
   structure:
   c ∈ Σ call i ∈ Σ internal r ∈ Σ return
   c ∈ Σ call ? ? ˙ =
   i ∈ Σ internal ? ? ?
   r ∈ Σ return ? ? ?
   (4.25)
   The OPM encodes the following constraints:
   30 We refer to [32] for a complete proof.


<a id="P336"></a>

   1. for each c 1 , c 2 ∈ Σ call , the only possible relation is c 1 ? c 2
   2. for each c ∈ Σ call and i ∈ Σ internal , the only possible relation is c ? i
   3. for each c ∈ Σ call and r ∈ Σ return , the only possible relation is c ˙ =r
   4. for each i ∈ Σ internal and c ∈ Σ call , the only possible relation is i ? c
   5. for each i 1 , i 2 ∈ Σ internal , the only possible relation is i 1 ? i 2
   6. for each i ∈ Σ internal and r ∈ Σ return , the only possible relation is i ? r
   7. for each r ∈ Σ return and c ∈ Σ call , the only possible relation is r ? c
   8. for each r ∈ Σ return and i ∈ Σ internal , the only possible relation is r ? i
   9. for each r 1 , r 2 ∈ Σ return , the only possible relation is r 1 ? r 2
   Tounderstandthereasonofsuchconstraints,ithelpstomentallyequatethecharacters
   inΣ call andΣ return toopenandclosedparentheses,respectively.Therefore,thecases
   c ? c,c ˙ =r andr ? r shouldbeclearbycomparisonwiththestructureofparentheses
   languages. Now consider the relation i ? i: it says that a string of internal characters
   has to be parsed using a left-linear grammar, as we just did to show the inclusion of
   regular languages into OP.

   The other cases of matrix (4.25) can be justified by observing in Fig.4.57 the
   parsing of a string over a 3-partitioned alphabet. Thus, relations c ? i and i ? c say
   that a substring over Σ internal , bordered on the left and on the right by c, must be
   handled as before for a regular language string bordered by #. For brevity, we do not
   discuss the remaining cases of matrix (4.25).

   To end the proof of statement 3 of Property4.86 we show an OP language that is
   not input-driven. The language is:
   L XYZ =
   ?
   a n b n | n ≥ 1
   ?
   ∪
   ?
   c n d n | n ≥ 1
   ?
   ∪
   ?
   e n (a c) n | n ≥ 1
   ?
   The straightforward operator-precedence grammar is in Example4.88 on p. 338. On
   the other hand, the language cannot be input-driven for the following reasons. The
   strings of type a n b n impose that a is a call and b a return; for similar reasons, c must
   be a call and d a return. But the strings of type e n (a c) n impose that at least one of
   a and c must be a return, which is a contradiction for a 3-partitioned alphabet.
   Fig.4.57 Parsing the string of an input-driven language using the OP matrix (4.25). The symbols
   of the form c, i and r, respectively, denote arbitrary call/internal/return characters

<a id="P337"></a>

   It would be possible to show that, if an OP grammar G has a partition of the
   terminal alphabet into three sets, i.e., Σ = Σ 1 ∪ Σ 2 ∪ Σ 3 , and its precedence
   matrix has the same structure as matrix (4.25) has, then the language L (G) is 
   inputdriven. More precisely, its 3-partition is Σ call = Σ 1 , Σ internal = Σ 2 and Σ return =
   Σ 3 .

   To sum up, the family of input-driven languages coincides with the family of
   languages generated by operator-precedence grammars having an OPM of the
   3partitioned form (4.25).


### 4.8.3.1 Closure Properties of Compatible OP Languages

   We say that two OP grammars G ? and G ?? are compatible if the union of their OP
   matrices, i.e., OPM(G ? ) ∪ OPM(G ?? ), is conflict-free. Notice that if the terminal
   alphabets of G ? and G ?? differ, the two OP matrices are normalized by considering
   the union of the two alphabets.

   Within the family of all the OP grammars, let us fix a particular OP matrix M.
   Then we focus on the family of OP grammars and corresponding languages that are
   compatible with matrix M, to be denoted by OP(M), by means of the following
   definition:
   OP(M) = { G | G is an OP grammar and OPM (G) ⊆ M}
   Clearly, every such family of grammars and corresponding languages is a subset of
   OP, with OP(M) ⊂ OP.

   Weobservethatiftherelation ˙ =in M iscircular,i.e.,ifmatrix M containsachain
   a 1 ˙ =a 2 ˙ = ... ˙ =a t ˙ =a 1 wheret ≥ 1,thentheOP(M)familyincludessomeunusual
   grammars. Such grammars would have rules with right parts of unbounded length,
   which are strings or substrings contained in the regular language (a 1 a 2 ... a t ) + .
   For both practical and mathematical reasons, we prefer to exclude such situation by
   assuming that matrix M does not contain any cyclic ˙ = chain.

   Foranymatrix M,thecorrespondinglanguagefamilyhastheremarkableproperty
   that the fundamental operations (union, intersection, set difference and 
   concatenation) that, when they are applied to general OP languages, may produce a language
   that loses the OP property, now produce a language that remains in the same family
   OP(M).

   Property 4.87 (closurepropertiesofcompatibleOPlanguages) ForeachOPmatrix
   M (assumed to be ˙ =-acyclic), the family of compatible languages, i.e., OP(M), is
   closed under the following operations: union, set difference and therefore 
   intersection.

   Moreover, for any compatible languages L ? = L (G ? ) and L ?? = L (G ?? ), the
   concatenation L = L (G ? ) · L (G ?? )andthestar
   ?
   L (G ? )
   ? ∗
   aregeneratedby 
   OPgrammars, respectively, G (·) and G (∗) , such that:
   OPM
   ? G
   (·)
   ?
   ⊇ OPM(G ? ) and OPM
   ? G
   (∗)
   ?
   ⊇ OPM(G ? ) (4.26)
   ?

<a id="P338"></a>

   The proof of the above properties, including the constructions of the grammars, is in
   [32], and we just show an example.

   Example 4.88 (operations on compatible grammars) The following languages L X ,
   L Y and L Z :
   L X =
   ?
   a n b n | n ≥ 1
   ?
   L Y =
   ?
   c n d n | n ≥ 1
   ?
   L Z =
   ?
   e n (a c) n | n ≥ 1
   ?
   are, respectively, defined by the OP grammars:
   X → a X b | a b Y → cY d | cd Z → e Z a c | ea c
   The three grammars are conflict-free and compatible as we see from the three
   individual OP matrices and from the resulting one M shown below:
   M X
   a b
   a ? ˙ =
   b ?
   c d
   c ? ˙ =
   d ?
   a c e
   a ˙ =
   c ?
   e ˙ = ?
   M =
   a b c d e
   a ? ˙ = ˙ =
   b ?
   c ? ? ˙ =
   d ?
   e ˙ = ?
   Therefore, if we apply the operations listed in Property4.87 to the above three
   languages, the languages that we obtain are compatible with matrix M. We show some
   cases.

   Language L X ∪ L Y ∪ L Z has the grammar S → X | Y | Z, plus the
   preceding rules for the nonterminal symbols X, Y and Z. The matrix is M, shown
   above.

   The concatenation L X L X contains strings of type a m b m a n b n , with m, n ≥ 1,
   where the character b can be followed by a. Since in the matrix M the case (b, a)
   is void, we can assign any relation to it. We discard the choice b ˙ =a that would
   create the circularity b ˙ =a ˙ =b, and we arbitrarily choose b ? a (if the case already
   contained a relation, we would have to stick to the same). In accordance with the
   chosen relation, the grammar and OPM for L X L X are:
   ?
   S → X a X b | X a b
   X → as above
   M X ∪ { b ? a }

<a id="P339"></a>

   The last operation we illustrate is (L X ) + . This time we arbitrarily assign to the case
   (b, a) the relation b ? a, thus obtaining the grammar and matrix:
   ?
   S → a X b S | a b S
   X → as above
   M X ∪ { b ? a } ?
   Returning to the input-driven languages, we may ask what effect the operations
   consideredinProperty4.87haveonsuchlanguages,butwealreadyknowtheanswer
   fromTable4.4onp.227:theinput-drivenfamilyisclosedunderthesameoperations.

   This is not surprising since we have just seen that an input-driven language is an OP
   language that has a particular form of precedence matrix, and by applying, say, the
   union operation to two input-driven languages with the same 3-partition, we obtain
   an OP language that has an OPM fitting with such a particular form.


## 4.8.4 Parallel Parsing Algorithm

   Here we describe how to use the sequential parsing algorithm to efficiently parse
   large texts in parallel. More information on the algorithm, its implementation and
   some experimental results can be found in [31].

   At the level of abstraction that is usually adopted in compiling, the input text
   consistsofastreamofsymbols(alsoknownaslexemesortokens),whichareterminal
   elements of the grammar. As we know, the sequential parser invokes the lexical
   analyzer (scanner) to obtain the next input token. To accommodate multiple parsers
   that operate in parallel, such an organization, which relies on a scanning subroutine
   invoked by the parser, has to be slightly revised in order to avoid that a token, say
   CYCLE27, stretching across consecutive text segments assigned to different parsers,
   may be split into two tokens, say CYCL and E27. Since this problem is marginal for
   the parallel parsing method we are interested in, we assume that the input text has
   been entirely scanned into tokens before parallel parsing starts.

   Then, our data-parallel algorithm splits the load among workers by dividing the
   text into k ≥ 1 segments, where k is the number of physical processors. How the
   text is split is irrelevant, provided that the substring lengths are approximately equal
   to ensure load balancing. In contrast, other parallel parsers have been proposed
   that perform a language dependent split, and require the text to be segmented at
   specificpositionswhereatokenoccurs(suchasbegin),whichpartsalargesyntactic
   constituent of the language.

   We apply Algorithm4.85 to each segment, by using a separate stack per worker.
   Each worker returns a stack, which is guaranteed to be correct independently of the
   stacks of the other workers. In practice, the reductions operated by each parser make
   a final fragment of the parse tree. The local parsability property of OP grammars
   manifests itself in that each parser bases its decisions on a look-ahead/look-back
   of length one, to evaluate the precedence relations between consecutive terminal
   characters. Therefore, in the split text, we have to overlap by one token any two

<a id="P340"></a>

   tree 1 tree 2 tree 3
   A
   B B
   n + n +
   B
   B
   B
   + n × n × n + n
   B
   n × n + n
   (#,⊥)(A,⊥)(+, ) (+,⊥)(B,⊥)(+, )(n, ) (n,⊥)(×, )(n,
   .

   =)(+, )(B,⊥)(#, )
   S 1 S 2 S 3
   Fig.4.58 Partial trees and corresponding stacks after the first parsing phase of text n + n + n ×
   n × n + n × n + n by using three workers
   consecutive segments, i.e., the last terminal of a segment coincides with the first
   terminal of the next segment.

   Example 4.89 (parallel parsing) Reconsider the grammar of Example4.82 on p.
   327, and the source text below:
      # n + n + n × n × n + n × n + n #
      Assume there are k = 3 workers and the segmented text is as below:
      1
      n + n +
      2
      n × n × n + n
      3
      × n + n
   wherethemarks#areleftimplicit,andthesymbols+andn notembracedareshared
   by the two adjacent segments.

   After each sequential parser has processed its segment, the partial trees and the
   stacks are shown in Fig.4.58. Notice that stack S 1 contains the shortest possible
   string: a nonterminal symbol embraced by the two terminals that are always present
   on the stack ends. Such a stack is called quasi-empty. Stacks S 2 and S 3 are not
   quasi-empty since they have also terminal symbols in their inner part. ?
   Stack Combination
   To complete parsing, we need an algorithm that joins the k existing stacks and
   launchesoneormoresequentialparsersonthecombinedstack(s).Differentstrategies
   are possible:
   serial
   Onenewstackisproducedbyjoiningallthestacks(andbytakingcareofcanceling
   one of the duplicated symbols at each overlapping point). The next phase uses
   just one worker, i.e., it is sequential.


<a id="P341"></a>

   dichotomic
   A new stack is created by joining every pair of consecutive stacks at positions
   1–2, 3–4, etc., so that the number of stacks of phase one is halved, as well as the
   number of workers.

   opportunistic
   Twoormoreconsecutivestacksarejoined,dependingontheirlength.Inparticular
   aquasi-emptystackisalwaysjoinedtoaneighboringstack.Thenumberofstacks
   is guaranteed to decrease by one, or more rapidly if more stacks are quasi-empty.
   Iftheoutputofphasetwoconsistsofmorethanonestack,thenthelasttwostrategies
   require further phases, which can employ the serial or again a parallel strategy. Of
   course, at the end all the strategies compute the same syntax tree and they only differ
   with respect to their computation speed.

   The serial strategy is convenient if the stacks after phase one are short, so that
   it would be wasteful to parallelize phase two. We do not need to describe phase
   two since it can reuse the sequential parsing algorithm, which we had the foresight
   to design to process also nonterminal symbols. The dichotomic strategy fixes the
   number of workers of phase two without considering the remaining amount of work,
   which is approximately measured by the length of each combined stack; this may
   cause useless overhead when a worker is assigned a short or quasi-empty stack to
   analyze. The opportunistic approach, to be described, offers more flexibility in the
   combination of stacks and their assignment to the workers of phase two.

   We recall that Algorithm4.85 has two main parameters: stack S and input string
   u. Therefore, we describe how to initialize these two parameters for each worker of
   phase two (no matter the strategy). Each stack S returned by phase one, if it is not
   quasi-empty, is split into a left and a right part, respectively, denoted as S L and S R ,
   suchthatS L doesnotcontainany?relationandS R doesnotcontainany?relation;
   of course, ˙ = relations may occur in both parts.

   Thus, stack S 2 is divided into S L
   2
   = (+,⊥)(B,⊥)(+,?) and S R
   2
   = (n,?). If
   several cut points satisfy the condition, then we arbitrarily choose one of them.
   Notice that the sequential parser can maintain a pointer to the stack cut point, with a
   negligible overhead, so we can assume that the stack returned by Algorithm4.85 is
   bipartite.

   To initialize the two parameters (stack and input string) for a worker, the parallel
   algorithm combines two consecutive bipartite stacks, as next explained. Notice that
   the input string thus obtained contains terminal and nonterminal symbols (computed
   inthereductionsofphaseone).LetW andW ? beconsecutiveworkersoftheprevious
   phase, and let their bipartite stacks be S L S R and S ? L S ? R , respectively. The initial
   configuration for a worker of the next phase is obtained as shown in Fig.4.59. More
   precisely we define the following functions:
   1. Stack initialization:
   S combine (S L , S R ) = (a,⊥) S R where a is the top symbol of S L

<a id="P342"></a>

   adjacent stacks produced by phase one
   no relations here no relations here
   S L S R
   X ...... X (a, r.) Y ...... Y (b, r.)
   (b, ⊥) W ...... W Z ...... Z
   S
   L
   S
   R
   no relations here no rel.s here
   initial configuration for phase two
   (a, ⊥) Y ...... Y (b, r.)
   W ...... W
   S combine (S L , S R ) u comb. (S
   L )
   S u
   Fig.4.59 Scheme of the initial configuration of a worker of the next phase: the stack is S and the
   input string is u ?
   Notice that the precedence value listed with a becomes undefined because in the
   new stack the a is not preceded by a terminal symbol.

   2. Input string initialization. The function u combine (S ? L ) returns the string u ?
   obtained from the stack part S ? L by dropping the precedence signs and 
   deleting the first element (which is redundant as it is on top of S R ).

   There are a few simple special but important cases. When S ? contains only ? and
   .

   =
   precedencerelations,thenumberofstackscanbereduced.Sinceintheneighbouring
   stack, part S R too contains only relations ? and ˙ =, we concatenate three pieces to
   obtain the new stack: S R S ? R , and the left part, denoted S ?? L , of the worker that

<a id="P343"></a>

   comes after W ? . As said, we need to keep only one of the overlapping elements: the
   last symbol of S R , but not the identical first symbol of S ? .

   Similarly, if S = (a,⊥)(A,⊥)(b,?) or S = (a,⊥)(A,⊥)(b, ˙ =) is a 
   quasiempty stack, the next stack part S ? L is appended to it. Notice that the case of a
   quasi-empty stack (a,⊥)(A,⊥)(b,?) belongs to the previous discussion.

   The outline of the parallel parsing algorithm comes next. Notice that for brevity
   the special cases discussed above are not encoded in this outline.

   Algorithm 4.90 (operator-precedence parallel parser)
   1. split the input string u into k substrings: #u 1 u 2 ... u k #
   2. launch k instances of Algorithm4.85 where for each index i with 1 ≤ i ≤ k the
   parameters of the i-th instance are as below:
   denote as a the last symbol of u i−1
   denote as b the first symbol of u i+1
   denote u 0 = u k+1 = #
   S := (a,⊥)
   u := u i b
   head := |u 1 u 2 ... u i−1 |
   end := |u 1 u 2 ... u i | + 1
   the result of this phase are k bipartite stacks S L
   i
   S R
   i
   3. repeat
   for each non-quasi-empty bipartite stacks S L
   i
   S R
   i
   and S L
   i+1
   S R
   i+1
   do
   launch an instance of Algorithm4.85 with these parameters:
   S := S combine (S L
   i
   , S R
   i
   )
   s := u combine (S L
   i+1 )
   head := 1
   end := |u|
   end for
   until
   ?
   there is only one stack S and the configuration is irreducible
   ?
   or an error
   is detected (triggering recovery actions not specified here)
   4. return stack S ?

<a id="P344"></a>

   Example 4.91 (parallel parsing steps) We resume from the configuration shown in
   Fig.4.58 where three bipartite stacks, computed by three workers, are present:
   (#,⊥) (A,⊥) (+,?) S 1
   (+,⊥) (B,⊥) (+,?)
   S R
   2
   ? ?? ?
   (n,?) S 2
   (n,⊥) (×,?) (n,
   .

   =) (+,?) (B,⊥) (#,?) S 3 = S L
   3
   Notice that stack S 1 is quasi-empty. In reality, the stacks are so short that the serial
   strategyshouldbeapplied,whichsimplyconcatenatesthethreestacksandtakescare
   of the overlaps. For the sake of illustrating the other strategies, we combine stacks
   2 and 3. We observe that the stack part S R
   3
   is void, hence S 3 ≡ S L
   3
   holds. Step 3,
   i.e., repeat, launches Algorithm4.85 on every initial configuration constructed as
   shown in Fig.4.59. Here there is just one instance, namely the parser that combines
   the outcomes of workers 2 and 3, starting from the initial configuration:
   S combine
   ? S L
   2 , S
   R
   2
   ?
   (+,⊥)(n,?)
   u combine (S L
   3 )
   × n + B #
   In the order, the following reductions are performed:
   n ? B B × n ? B
   thus obtaining the stack: S = (+,⊥)(B,⊥)(+,?)(B,⊥)(#,?). The situation
   before the final round is below:
   (#,⊥) (A,⊥) (+,?) (B,⊥) (+,?) S 1 S L
   2
   (+,⊥) (B,⊥) (+,?) (B,⊥) (#,?) S
   Theparsercanuseasinputstringsimplya# andthestackobtainedbyconcatenating
   the existing stacks:
   (#,⊥) (A,⊥) (+,?) (B,⊥) (+,?) (B,⊥) (+,?) (B,⊥)
   Three successive reductions A + B ? A, followed by A ? S, complete the
   analysis. ?
   We mention a possible improvement to the strategy, which mixes the serial and
   parallel approaches. At step 3, if the sum of the lengths of a series of consecutive
   stacks is deemed short enough, then the algorithm can join all of them at once into
   onestack,thusapplyingtheserialstrategytothatportionoftheremainingwork.This
   strategy is advantageous when the remaining work needed to join the stacks and to
   synchronize the workers outbalances the useful work effectively spent to complete
   the construction of the syntax tree.


<a id="P345"></a>


### 4.8.4.1 Parsing Complexity

   Parallel parsing techniques are mainly motivated by possible speed gains and electric
   power saving over sequential algorithms. We briefly analyze the performance of the
   parallelparser,assumingthat,afterthefirstphaseofsegmentparsing,alltheresulting
   stacks are joined at once into one stack (as discussed above).

   Intermsofasymptoticcomplexity,wearegoingtoarguethattheparallelalgorithm
   achieves these requirements:
   1. a best-case linear speedup with respect to the number of processors
   2. a worst-case performance that does not exceed the complexity of a complete
   sequential parsing
   To meet these requirements, it is essential that the combination of stacks S i and
   S i+1 inside step 3 of Algorithm4.90 takes O(1) time, hence O(k) overall time for k
   workers. This goal is easily achieved by maintaining at run-time a marker that keeps
   track of the separation between the two parts S L and S R of each bipartite stack. The
   marker is initialized at the position where the first relation ? is detected and then
   updated every time a reduction is applied and a new element is shifted on the stack
   as a consequence of a new relation ?. For instance in the case of S 2 in Fig.4.58, the
   marker is initialized at the position of the first + symbol and there remains after the
   reductions n ? B, B × n ? B and B × n ? B, because + ? n and + ? × hold.

   Then, when the second + within this segment is shifted (the first + remains because
   the ? between the two + is not matched by a corresponding ? at its left), the marker
   is advanced to the position of the second +, because + ? n holds. In such a position
   the marker indicates the beginning of S R
   2
   .

   These operations require a time O(1), irrespectively of whether we implement
   the stacks by means of arrays or of more flexible linked lists. It follows that the
   overhead due to stack combination (linearly) depends on k but not on the source text
   length, thus proving that the worst-case asymptotic complexity is the same as in the
   sequential algorithm.

   Truly, such analysis neglects the synchronization and communication costs
   between workers, which may deteriorate the performances of parallel algorithms.
   Such costs tend to outbalance the benefit of parallelization when the load assigned
   to a worker becomes too small. In our case, this happens when the text, i.e., the input
   stackassignedtoaworker,istoosmall.Thecostalsodependsontheimplementation
   techniquesusedforsynchronizationandcommunication,andontheirsupportbythe
   hardware operations provided by the computer. Experiments [31] indicate that
   parsing large texts in parallel on the current multiprocessor and multi-core computers,
   achieves a significant speed-up.


<a id="P346"></a>

   context-free gram.

   unambiguous context-free gram.

   ... LR(2) gram. LR(1) gram.

   ... LL(2) gram. LL(1) gram. OP gram.

   simple deterministic gram.

   Fig.4.60 Inclusion relationships between families of grammars

4.9 Deterministic Language Families:A Comparison
================================================

   The family DET of deterministic (context-free) languages, characterized by the
   deterministic pushdown automaton as their recognizer, was defined in Sect. 4.3 on
   p. 215, as well as their subclasses: simple deterministic (p. 223) and input-driven (p.
   223), the latter including parenthesis languages. Next, to complete the theoretical
   picture of deterministic families, we include in the comparison the families of 
   deterministic bottom-up and top-down parsable languages, and the operator-precedence
   languages. In this section we do not use EBNF grammars and machine nets
   systematically, because in the literature language theoretic properties have been mostly
   investigated for the basic BNF grammar model. Notice however that, since the
   families ELR and ELL of extended grammars strictly include those of non-extended
   grammars (LR and LL), some properties for the latter immediately extend to the
   former.

   Asweproceed,wehavetoclarifywhatweactuallycompare:grammarorlanguage
   families? An instance of the first case is the statement that every LL(1) grammar is
   also an LR(1) grammar; for the second case, an instance is that the family of the
   languages generated by LL(1) grammars is included in the DET family.

   The inclusion relationships between grammar families and between language
   families are depicted in Figs.4.60 and 4.61, respectively, and are now discussed case
   by case. Notice that some facts already known from previous sections and chapters
   are not repeated here.

   We briefly comment the inclusions between grammar families.

   Inthesimpledeterministiccase,foreverynonterminal Athegrammaralternatives
   must start with distinct terminals, i.e., they are A → bβ and A → cγ with b ?= c.
   Therefore, the guide sets at the corresponding bifurcation from the initial state of
   machine M A are disjoint, thus proving that the grammar is LL(1).


<a id="P347"></a>

   CF, context-free lang.

   unambiguous context-free lang.

   deterministic context-free, DET ≡
   ≡ LR(1) lang. ≡ LR(2) lang. ≡ ...

   ... LL(2) lang. LL(1) lang. OP lang.

   input-driven lang.

   REG, regular lang.

   (2)
   (3)
   (2)
   incomparable , (6)
   incomparable , (6)
   (5)
   (4)
   (1)
   (7)
   Fig.4.61 Inclusionrelationshipsbetweenlanguagefamilies;allinclusionsarestrict;numbersrefer
   to the comments
   The fact that every LL(1) grammar is also LR(1) is obvious, since the former
   condition is a restriction of the latter. Moreover, the inclusion is strict: it suffices to
   recall that a grammar with left-recursive rules surely cannot be LL(1), but it may be
   LR(1). It is also obvious that every LR(1) grammar is non-ambiguous.

   It remains to comment the inclusions between the subfamilies LL(k) and LR(k),
   withk ≥ 1.Althoughtheyhavenotbeenformallydefinedinthisbook,itisenoughto
   thinkofthemasobtainedbyextendingthelengthoflook-aheadfrom1tokcharacters,
   as we did in Sects. 4.6.6 and 4.7.5 for the LR(2) and the LL(2) case, respectively.
   In this way, we step up from the original conditions for k = 1, to the augmented
   conditions that we name LR(k) and LL(k) (dropping the “E” of ELR/ELL since the
   grammarsarenotextended).ThefactthateveryLR(k)(respectivelyLL(k))grammar
   is also LR(k + 1) (respectively LL(k + 1)) is trivial; moreover, the grammar in
   Fig.4.52 on p. 324, witnesses the strict inclusion ELL(1) ⊂ ELL(2), and after the
   natural conversion to BNF, also LL(1) ⊂ LL(2).

   We state without justification (the books [33,34] cover such properties in depth)
   that the inclusion of LL(k) within LR(k), which we already know to hold for k = 1,
   is valid for any value of k.


<a id="P348"></a>

   Next we exhibit examples to support the fact that every OP grammar meets the
   LR(1) condition. 31 Let G be an operator grammar (without copy rules and ε-rules)
   such that G presents an LR(1) conflict; does G also have OP conflicts? The answer
   is positive, as suggested by two old examples, respectively, exhibiting a shift-reduce
   and a reduce-reduce conflict.

   Example 4.92 (fromLR(1)conflictstoOPconflicts)Figure4.18onp.256detailsthe
   shift-reduceconflictinthegrammar: S → Abc | a band A → a.TheOPrelations
   contain the conflict: a ˙ =b and a ? b.

   For a reduce-reduce conflict, see the grammar in Fig.4.19 on p. 257:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S → A | B
   A → a b | a Ab
   B → a a b | a a B b
   Notice that this language is not deterministic. The OP matrix contains the conflict:
   a ˙ =a and a ? a. ?
   Language Family Comparisons
   WecompareseverallanguagefamiliesinFig.4.61(disregardingthemarginalsimple
   deterministic family, which fits between families REG and LL(1)). For two generic
   familiesX andY,whicharecharacterizedbycertaingrammarorautomatonmodels,
   the proof that family X is included in Y, consists of showing that every grammar (or
   automaton) of type X can be converted to a grammar (or automaton) of type Y. For
   a broad introduction to such comparison methods, we refer to [1].

   Before we analyze the relationships among language families, we have to state
   more precisely a fundamental property of deterministic languages, which we have
   vaguely anticipated.

   Property 4.93 (LR and DET families) The family DET of deterministic 
   contextfree languages coincides with the family of the languages generated by LR(1)
   grammars. ?
   This important statement (due to Knuth [16]) essentially says that the shift-reduce
   parsing approach with look-ahead length k = 1 perfectly captures the idea of 
   deterministic recognition, using as automaton a machine with a finite memory and an
   unbounded stack. Obviously, this does not say that every grammar generating a
   deterministic language enjoys the LR(1) property. For instance, the grammar might
   be ambiguous or require a look-ahead longer than one. But for sure there exists an
   equivalent grammar that is LR(1). Notice that, since the grammar family LR(1) is
   31 A formal comparison of precedence and LR(1) grammars is in Aho and Ullman [35]. Fischer
   [36] shows that any OP grammar is also a simple precedence grammar, which is a grammar type,
   not considered in this book, included in the LR(1) type.


<a id="P349"></a>

   included in the grammar family ELR(1), the latter too coincides with DET. On the
   other hand, any context-free language that is not deterministic, as the cases in Sect.
   4.3.2 on p. 218, cannot be generated by an LR(1) grammar.

   Consider now extending the look-ahead length, as we did in Sect. 4.6.6. Trusting
   the reader’s intuition (or going back to Sect. 4.6.6 for a few enlightening examples),
   we talk about ELR(k) grammars and parsers without formally defining them. Such
   a parser is driven by a pilot with look-ahead sets that contain strings of length k > 1.
   Clearly,thepilotmayhavemorem-statesthantheLR(1)pilot,becausesomem-states
   of the latter pilot are split, due to the look-ahead sets with k > 1 becoming different.
   However, since after all an ELR(k) parser is nothing more than a deterministic
   pushdown automaton, no matter how large parameter k is, from Property4.93 we
   have the following one:
   Property 4.94 (ELR families) For every integer k > 1, the family of the languages
   generated by ELR(k) grammars coincides with the language family generated by
   ELR(1) grammars, hence also with the DET family. ?
   Now a question arises: since every deterministic language can be defined by
   means of an LR(1) grammar, what is the use of taking higher values of the length
   parameter k? The answer comes from the consideration of the families of grammars
   in Fig.4.60, where we see that there exist grammars having the LR(2) property,
   but not the LR(1) one, and more generally that there exists an infinite inclusion
   hierarchy between the LR(k) and LR(k + 1) grammar families (k ≥ 1). Although
   Property4.94 ensures that any LR(k) grammar, with k > 1, can be replaced by an
   equivalent LR(1) grammar, the latter grammar is sometimes less natural or simply
   bigger, as seen in Sect. 4.6.6 where a few grammar transformations from LR(2)
   down to LR(1) are illustrated. The following comments are keyed to the numbered
   relationships among languages, shown in Fig.4.61:
   1. Since every regular language is recognized by a DFA, consider its state-transition
   graph and normalize it so that no arcs enter the initial state: such a machine only
   has terminal shifts and is clearly ELL(1). To show that it is LL(1), it suffices to
   take the grammar that encodes each arc p
   a
   − → q as a right-linear rule p → a q,
   and each final state f as f → ε. Such a grammar is easily LL(1) because every
   guide set coincides with the arc label, except the guide set of a final state, which
   is the end-marker ?.

   2. Every LL(k) language with k ≥ 1 is deterministic, hence also LR(1) by
   Property 4.93. Of course, this is consistent with the fact that the top-down parser of an
   LL(k) language is a deterministic pushdown automaton. It remains to argue that
   the inclusion is strict, i.e., for every value of k there exist deterministic languages
   that cannot be defined with an LL(k) grammar. A good example is the 
   deterministic language { a ∗ a n b n | n ≥ 0 }, studied in Fig.4.38 on p. 289, where the
   grammar with rules S → a ∗ N and N → a N b | ε, and the equivalent machine
   net, have been found to be ELR(1), but to violate the ELL(1) condition. The
   reasonisthat,intheinitialstateofthenetwithcharactera ascurrenttoken,theparser

<a id="P350"></a>

   is unable to decide whether to simply scan character a or to invoke machine M N .
   Quiteclearly,weseethatlengtheningthelook-aheaddoesnotsolvethedilemma,
   as witnessed by the sentences a ... a a n b n where the balanced part of the string
   is preceded by unboundedly many letters a. A natural but more difficult question
   is: can we find an equivalent ELL(k) grammar for this language? The answer,
   proved in [37], is negative.

   3. ThecollectionoflanguagefamiliesdefinedbyLL(k)grammars,fork = 1, 2, ...,
   formsaninfiniteinclusionhierarchy,inthesensethat,foranyintegerk ≥ 1,there
   exists a language that meets the LL(k + 1) condition, but not the LL(k) one (we
   refer again to [33,34,37]). Notice the contrast with the LR(k) collection of
   language families, which all collapse into DET for every value of the parameter k.
   4. See Property4.86 on p. 335.

   5. An OP parser is a deterministic pushdown machine, see Property4.86 on p. 335.
   6. It follows from Property4.86 on p. 335 and the strict containment of ID within
   OP.

   7. See the remark after Definition4.20 of input-driven automata on p. 225.
   To finish with Fig.4.61, the fact that a deterministic language is unambiguous is
   stated in Property4.17 on p. 220.

   For completeness, we end with a negative statement concerning decidability of
   the LR(k) property.

   Property 4.95 (undecidability) For a generic context-free grammar, it is 
   undecidable if there exists an integer k ≥ 1 such that the grammar has the LR(k) property.
   Foragenericnondeterministicpushdownautomaton,itisundecidableifthereexists
   an equivalent deterministic pushdown automaton; this is the same as saying that it
   is undecidable if a context-free language (presented by means of a generic grammar
   or pushdown automaton) is deterministic. ?
   However, if the look-ahead length k is fixed, then it is possible to check whether
   the grammar is LR(k) by constructing the pilot and verifying that there are not any
   conflicts: this is what we have done for k = 1 in a number of cases. Furthermore, it
   is not excluded that for some specific nondeterministic pushdown automaton, it can
   be decided if there exists a deterministic one accepting the same language; or (which
   is the same thing) that for some particular context-free language, it can be decided
   if it is deterministic; but general algorithms for answering these decision problems
   in all cases do not exist.

   4.10 Discussion of Parsing Methods
   We briefly discuss the practical implications of choosing top-down or bottom-up
   deterministic methods, then we raise the question of what to do if the grammar is
   nondeterministic and neither parsing method is adequate.


<a id="P351"></a>

   For the primary technical languages, compilers exist that use both top-down and
   bottom-up deterministic algorithms, which proves that the choice between the two
   is not really critical. In particular, the speed differences between the two algorithms
   are small and often negligible. The following considerations provide a few hints for
   choosing one or the other technique.

   We know from Sect. 4.9 (p. 346 and following) that the family DET of 
   deterministic languages, i.e., the LR(1) ones, is larger than the family of LL(k) languages,
   for any k ≥ 1. Therefore there are LR(1) grammars that cannot be converted into
   LL(k) grammars, no matter how large the guide set length k may be. In the practice
   this case is unfrequent. Truly, quite often the grammar listed in the official language
   reference manual violates the ELL(1) condition because of the presence of
   leftrecursive derivations, or in other ways, and choosing top-down parsing forces the
   designer to use a longer look-ahead, or to transform the grammar. The first option
   is less disturbing, if it is applicable. Otherwise, some typical transformations
   frequently suffice to obtain an LL(k) grammar: moving left recursions to the right and
   refactoring the alternative grammar rules, so that the guide sets become disjoint. For
   instance, the rules X → A | B, A → a | cC and B → b | c D are refactored as
   X → a | b | c (C | D ), thus eliminating the overlap between the guide sets of
   thealternativesof X.Unfortunatelytheresultinggrammaristypicallyquitedifferent
   from the original one and often less readable. Add to it that having two grammar
   versions to manage carries higher maintenance costs when the language evolves over
   time.

   Another relevant difference is that a software tool (such as Bison) is needed to
   constructashift-reduceparser,whereasrecursivedescentparserscanbeeasilycoded
   by hand, since the layout of each syntax procedure essentially mirrors the graph of
   the corresponding machine. For this reason the code of such compilers is easier to
   understand and may be preferred by the non-specialist. Actually, tools have been
   developed (but less commonly used) for helping in designing ELL (1) parsers: they
   compute the guide sets and generate the stubs of the recursive syntactic procedures.
   With respect to the form of the grammar, EBNF or pure BNF, we have presented
   parser generation methods that handle extended BNF rules, not just for top-down
   as done since many years, but also for bottom-up algorithms. Parser generator tools
   for ELR(1) grammars exist [19], but are less consolidated than those for pure BNF
   grammars, such as Bison.

   Aparserisnotanisolatedapplication,butitisalwaysinterfacedwithatranslation
   algorithm (or semantic analyzer) to be described in the next chapter. Anticipating
   some discussion, top-down and bottom-up parsers differ with respect to the type of
   translation they can support. A formal property of the syntax-directed translation
   methodology, to be presented later, makes top-down parsers more convenient for
   designing simple translation procedures, obtained by inserting output actions into
   the body of parsing procedures. Such one-pass translators are less expressive and
   less convenient to implement on top of bottom-up parsers.

   If the reference grammar of the source language violates the ELR (hence also the
   ELL) condition, the compiler designer has several choices:

<a id="P352"></a>

   1. to modify the grammar
   2. to adopt a more general parsing algorithm
   3. or to leverage on semantic information
   Actually this situation is more common for natural language processing (compu-
   tational linguistic) than for programming and technical languages, because natural
   languages are much more ambiguous than the artificial ones. So skipping case 1, we
   discuss the remaining ones.

   In case 2, the parsing problem for general context-free grammars (including the
   ambiguous case) has attracted much attention, and well-known algorithms exist that
   are able to parse any string in cubic time. The leading algorithms are named CYK
   (from its inventors Cocke–Younger–Kasami) and Earley (see for instance [18]); the
   latter is the object of the next section.

   We should mention that other parsing algorithms exist that fall in between 
   deterministicparsersandgeneralparserswithrespecttogeneralityandcomputationalper-
   formance. Derived from the ELL(k) top-down algorithms, parser generators exist,
   like ANTLR, that produce more general parsers: they may perform unbounded
   lookahead in order to decide the next move. For bottom-up parsing, a practical almost
   deterministic algorithm existing in several variants is due to Tomita [38]. The idea is
   tocarryoninparallelonlyaboundednumberofalternativeparsingattempts.Butthe
   designer should be aware that any general parser more general than the deterministic
   ones will be slower and more memory demanding.

   Incase3,sometimesanentirelydifferentstrategy forcurtailingthecombinatorial
   explosion of nondeterministic attempts is followed, especially when the syntax is
   highly ambiguous, but only one semantic interpretation is possible. In that case,
   it would be wasteful to construct a number of syntax trees, just to delete, at a later
   time,allbutoneofthemusingsemanticcriteria.Itispreferabletoanticipatesemantic
   checks during parsing, and thus to prevent meaningless syntactic derivations to be
   carried on. Such a strategy, called semantics-directed parsing, is presented in the
   next chapter within the semantic model named attribute grammars.

   4.11 A General Parsing Algorithm
   The LR and LL parsing methods are inadequate for dealing with ambiguous and
   nondeterministic grammars. In his seminal work [39], J. Earley introduced an
   algorithm for recognizing the strings of arbitrary context-free grammars, which has a
   cubic time complexity in the worst case. His original algorithm does not construct
   the (potentially numerous) syntax trees of the source string, but later certain
   efficient representations of parse forests have been invented, which avoid to duplicate
   the common subtrees and thus achieve a polynomial time complexity for the tree
   construction problem [40]. Until very recently [41], the Earley parsers computed the
   syntax tree in a second pass, after string recognition [18].


<a id="P353"></a>

   Concerning the use of EBNF grammars, J. Earley already gave some hints about
   how to do, and later a few parser generators have been implemented.

   Anotherlong-standingdiscussionconcernstheprosandconsofusinglook-ahead
   to filter parser moves. Since experiments have indicated that the look-ahead-less
   algorithms can be faster, at least for programming languages [42], we decided to
   presentthesimplerversionthatdoesnotuselook-aheadatall.Thisisinlinewiththe
   classical theoretical presentations of the Earley parsers for non-extended grammars
   (BNF), e.g., in [18,43].

   We present a version of the Earley parser for machine networks representing
   arbitrary EBNF grammars, including the nondeterministic and ambiguous ones. But for
   thecomputationofthesyntaxtree,werestrictourpresentationtounambiguousgram-
   mars, because this book focuses on programming languages, which are well-defined
   unambiguous formal notations, unlike the natural languages. Thus, our procedure
   for building syntax trees works for nondeterministic unambiguous grammars, and it
   does not deal with multiple trees or parse forests.

   Thesectioncontinueswithanintroductorypresentationbyexample,thenitdefines
   and illustrates the pure recognizer algorithm, and it finishes with the algorithm for
   building the syntax tree. At the section end, we mention some extensions of the
   Earley algorithm and other related issues.


## 4.11.1 Introductory Presentation

   The data structures and operations of the Earley and ELR(1) parsers are in many
   ways similar. In particular, the vector stack used by the parser implementation in
   Sect.4.6.5 on p. 271 is a good analogy to the Earley vector to be next introduced.
   Whenanalyzingastringx = x 1 ... x n orx = εoflengthn ≥ 0,thealgorithmusesa
   vector E [0 ... n]ofn + 1elements.Everyelement E [i]isasetofpairs,? q X , j ?,
   where q X is a state of machine M X , and j (0 ≤ j ≤ i ≤ n) is an integer pointer that
   indicates an element E [j], preceding or coinciding with E [i]; such E [j] contains
   a pair ? 0 X , j ?, with the same machine M X and the same integer j. The intended
   meaning of this pointer j is to mark the position ↑ in the input string x from where
   the current derivation of nonterminal X has started, as shown below:
   x 1 ... x j ↑ x j+1 ... x i
   ? ?? ?
   X
   ... x n
   The pointer thus stretches from character x j , excluded, to character x i , included. If
   the current derivation of X is ε, we have the case j = i.

   Thus, a pair has the form ?q X , j ? and is named initial or final depending on the
   state q X being, respectively, initial or final; a pair is called axiomatic if nonterminal
   X is the grammar axiom.

   An intuitive example familiarizes us with the Earley method. It embodies the
   essential concepts leaving out details to be later formalized.


<a id="P354"></a>

   Example 4.96 (introduction to the Earley method) The language:
   ?
   a n b n | n ≥ 1
   ?
   ∪
   ?
   a 2n b n | n ≥ 1
   ?
   is nondeterministic and is defined by the grammar and machine net in Fig.4.62,
   which violate the ELR(k) condition, as any other equivalent grammar would do
   (the ELR(1) pilot is in Fig.4.19 on p. 257. Suppose the string to be analyzed is
   x = x 1 x 2 x 3 x 4 = a a bb with length n = 4. We prepare the Earley vector E, with
   the elements E [0], …, E [4], all initialized to empty, except E [0] = ?0 S , 0?. In
   this pair, the state is set to 0 S (the initial state of machine M S ), and the pointer is set
   to 0, which is the position that precedes the first character x 1 of string x. As parsing
   proceeds, when the current character is x i , the current element E [i] will be filled
   with one or more pairs. The final vector E [0]... E [4] is shown in Table4.9.
   In broad terms, operations of three types, to be next explained on the example,
   can be applied on the current element E [i]: closure, terminal shift and nonterminal
   shift; their names are remindful of similar ELR operations.

   Closure The operation applies to a pair such that from its state an arc with
   nonterminal label X originates. Let the pair ? p, j ? be in the current element E [i] and
   suppose that the net has an arc p
   X
   − → q with nonterminal label X; the target state q
   is not relevant. The operation adds to the current element E [i] a new pair ? 0 X , i ?
   such that: the state is the initial one 0 X of the machine M X of nonterminal X; the
   pointer has value i, to indicate that the pair was created at step i from a pair already
   present in E [i].

   The effect of closure is to add to element E [i] all the pairs with the initial states
   of the machines that can recognize a substring starting from the next character x i+1 .
   grammar machine net
   S → A | B
   0 S 1 S
   S →
   →
   A, B
   A → aAb | ab
   0 A 1 A 2 A 3 A
   A →
   →
   a A b
   b
   B → aaB b | aab
   0 B 1 B 2 B 3 B 4 B B → →
   a a B b
   b
   Fig.4.62 Grammar and machine net of Example4.96

<a id="P355"></a>

   Table 4.9 Earley parsing trace of string a a bb for Example4.96
   Earley vector E [0...4]
   E (0) x 1 E (1) x 2 E (2) x 3 E (3) x 4 E (4)
   q j a q j a q j b q j b q j
   0 S 0 1 A 0 2 B 0 4 B 0 3 A 0
   0 A 0 1 B 0 1 A 1 3 A 1 1 S 0
   0 B 0 0 A 1 0 B 2 1 S 0
   0 A 2 2 A 0
   In our example, the closure writes into E [0] the pairs:
   ? 0 A , 0 ? ? 0 B , 0 ?
   Since a further application of the closure to the new pairs does not create new pairs,
   the algorithm moves to the second operation.

   Terminal shift The operation applies to a pair with a state from where a terminal
   shift arc originates. Suppose that the pair ? p, j ? is in element E [i − 1] and that
   the net has the arc p
   x i
   −→ q labeled by the current token x i . The operation writes into
   element E [i] the pair ? q, j ?, where the state is the destination of the arc and the
   pointer equals that of the pair in E [i − 1], to which terminal shift applies. Having
   scanned x i , the current token becomes x i+1 .

   Here,withi = 1and x 1 = a,theterminalshiftfromelement E [0]putsinto E [1]
   the new pairs:
   ? 1 A , 0 ? ? 1 B , 0 ?
   Then, closure applies again to the new pairs, with the effect of adding to E [1] the
   pair (notice the pointer has value 1):
   ? 0 A , 1 ?
   Ifthecurrentelement E [i]doesnotcontainanypairtowhichterminalshift,underthe
   current token, applies, the next element E [i + 1] remains empty, and the algorithm
   stops and rejects the string (in the example this never happens.).

   Continuing from element E [1] scanning character x 2 = a, a terminal shift puts
   into E [2] the pairs:
   ? 2 B , 0 ? ? 1 A , 1 ?

<a id="P356"></a>

   Through their closure, we add to E [2] the pairs:
   ? 0 B , 2 ? ? 0 A , 2 ?
   Scanning character x 3 = b, a terminal shift writes into E [3] the pairs:
   ? 4 B , 0 ? ? 3 A , 1 ?
   Now, the third type of operation, which is more complex, comes into play.

   Nonterminal shift The operation is triggered by the presence in E [i] of a pair
   ? f X , j ?,where f X isafinalstateofmachine M X :suchpairisqualifiedasenabling.
   Then,tolocatethepairstobeshifted,theoperationjumpstotheelement E [j]pointed
   to by the pointer of the enabling pair. In this example, the pointer always directs to
   an element at a preceding position j < i. However, if the grammar contains nullable
   nonterminals, the pointer j may also direct to the same position E [i], as we will see
   in other examples.

   In the element E [j], the nonterminal shift operation searches for a pair ? p, l ?
   such that the net contains an arc p
   X
   − → q, where label X matches the machine M X
   of state f X (in the enabling pair). The pointer l may range in the interval 0 ≤ l ≤ j.
   It can be proved that the search will find at least one such pair, and the nonterminal
   shift applies to it. The operation writes the pair ? q, l ? into E [i]; notice that state q
   is the target of the arc and the pointer value l is copied. If the search in E [j] finds
   two or more pairs that fit, then the operation is applied to each one, in any order.
   Here, the final pairs ? 4 B , 0 ? and ? 3 A , 1 ? in E [3] enable the nonterminal shifts
   onsymbols B and A,respectively.Thepointerinpair? 4 B , 0 ?linkstoelement E [0],
   which, intuitively, is the point where the parser began to recognize nonterminal B.
   In E [0] we find pair ? 0 S , 0 ?, and we see that the net has arc 0 S
   B
   − → 1 S from state
   0 S with nonterminal label B. Then, we add to element E [3] the pair:
   ? 1 S , 0 ?
   Similarly, for the other enabling pair ? 3 A , 1 ?, we find the pair ? 1 A , 0 ? in the
   element E [1] pointed to by 1; since the net has arc 1 A
   A
   − → 2 A , we write into E [3]
   the pair:
   ? 2 A , 0 ?
   We pause to compare terminal and nonterminal shift: the former works from E [i]
   to E [i + 1], two adjacent elements, and needs an arc p
   x i+1
   −−→ q, which scans token
   x i+1 ; the latter works from element E [j] to E [i], which can be separated by a long
   distance, and uses arc p
   X
   − → q, which corresponds to analyze the string derived from
   nonterminal X, i.e., all the tokens between the two elements.

   We resume the example from the point we stopped. If string x ended with x 3 , then
   itsprefixa a bscannedhithertowouldbeaccepted:acceptanceisdemonstratedbythe
   presence in element E [3] of the final axiomatic pair ? 1 S , 0 ?, with the pointer set to

<a id="P357"></a>

   zero.Inreality,onemorecharacter x 4 = b hastobescanned,andfromelement E [3]
   theterminalshiftwritesintoelement E [4]thepair? 3 A , 0?.Sincethepairisfinal,it
   enables a nonterminal shift that goes back to element E [0], finds pair ? 0 S , 0 ? such
   that the net has arc 0 S
   A
   − → 1 S , and finally puts the pair ? 1 S , 0 ? into element E [4].

   Thispaircausesthewholestring x = a a bb tobeaccepted.Thewholecomputation
   traceisinTable4.9.Thedividerbetweenvectorelementsseparatesthepairsobtained
   from the preceding element through terminal shift (top group), from those obtained
   through closure and nonterminal shift (bottom group); such a partitioning is just for
   visualization (an element is an unordered set); of course in the initial element E (0)
   the top group is always missing. The final states are highlighted as visual help. ?

## 4.11.2 Earley Algorithm

   The Earley algorithm for machine networks representing EBNF grammars uses two
   procedures,CompletionandTerminalShift.Theinputstringisdenotedx = x 1 ... x n
   or x = ε, with |x| = n ≥ 0. Here is Completion:
   Completion(E, i) - - with index 0 ≤ i ≤ n
   do
   - - loop that computes the closure operation
   - - for each pair that launches machine M X
   for
   ?
   each pair ? p, j ? ∈ E [i] and X, q ∈ V, Q s.t. p
   X
   − → q
   ?
   do
   add pair ? 0 X , i ? to element E [i]
   end for
   - - nested loops that compute the nonterminal shift operation
   - - for each final pair that enables a shift on nonterminal X
   for
   ? each pair ? f, j ? ∈ E [i] and X ∈ V such that f ∈ F
   X
   ?
   do
   - - for each pair that shifts on nonterminal X
   for
   ?
   each pair ? p, l ? ∈ E [j] and q ∈ Q s.t. p
   X
   − → q
   ?
   do
   add pair ? q, l ? to element E [i]
   end for
   end for
   while
   ?
   some pair has been added
   ?
   The Completion procedure adds new pairs to the current vector element E [i], by
   applying one or more times closure and nonterminal shift as long as pairs can be
   added to it. The outer loop (do-while) runs once at least, exits after the first run

<a id="P358"></a>

   if it cannot add any pair, or more generally exits after a few runs when closure
   and nonterminal shift cease to produce effect. Notice that Completion processes the
   nullable nonterminals by applying to them a combination of closures and nonterminal
   shifts. Here is TerminalShift:
   TerminalShift(E, i) - - with index 1 ≤ i ≤ n
   - - loop that computes the terminal shift operation
   - - for each preceding pair that shifts on terminal x i
   for
   ?
   each pair ? p, j ? ∈ E [i − 1] and q ∈ Q s.t. p
   x i
   −→ q
   ?
   do
   add pair ? q, j ? to element E [i]
   end for
   TheTerminalShift procedureaddstothecurrentelement E [i],thenewpairsobtained
   from the previous element E [i − 1] through a terminal shift scanning token x i
   (1 ≤ i ≤ n).

   Remarks. TerminalShift may fail to add any pair to the element, which so remains
   empty. A nonterminal that exclusively generates the empty string ε never undergoes
   terminal shift (it is processed only by completion).

   Noticethatbothprocedurescorrectlyworkalsowhentheelement E [i]or,respec-
   tively, E [i − 1], is empty, in which case the procedures do nothing.

   Algorithm 4.97 (Earley syntactic analysis)
   - - analyze the terminal string x for possible acceptance
   - - define the Earley vector E [0...n] with |x| = n ≥ 0
   E [0] := { ? 0 S , 0 ? } - - initialize the first elem. E [0]
   for i := 1 to n do - - initialize all elem.s E [1...n]
   E [i] := ∅
   end for
   Completion(E, 0) - - complete the first elem. E [0]
   i := 1
   - - while the vector is not finished and the previous elem. is not empty
   while
   ?
   i ≤ n ∧ E [i − 1] ?= ∅
   ?
   do
   TerminalShift(E, i) - - put into the current elem. E [i]
   Completion(E, i) - - complete the current elem. E [i]
   i + +
   end while ?

<a id="P359"></a>

   At start, the algorithm initializes E [0] with the axiomatic pair ? 0 S , 0 ? and sets the
   other elements (if any) to empty. Soon after the algorithm completes element E [0].
   Then, if n ≥ 1, i.e., string x is not empty, the algorithm loops on each subsequent
   elementfrom E [1]to E [n],putspairs(ifany)intothecurrentelement E [i]through
   TerminalShift, and finishes element E [i] through Completion (1 ≤ i ≤ n). If 
   TerminalShift fails to put any pairs into E [i], then Completion runs idle on it. Usually,
   the loop iterates as far as the last element E [n], but it may terminate prematurely if
   an element was left empty in a loop iteration; then it leaves empty all the following
   elements. We do not comment the case n = 0, which is simpler.

   Notice the analogy between Algorithm4.35 on p. 252, which constructs the pilot
   graphandthepresentalgorithm.Theformerbuildsapilotm-state:itshiftscandidates
   into the base and computes closure. The latter builds a vector element: it shifts pairs
   into the element and computes completion, which includes closure. The analogy is
   partialandtherearedifferences,too:acandidatehasalook-aheadandapairdoesnot;
   a pair has a pointer and a candidate does not; and processing nonterminals through
   shift is not identical either. But the major difference is conceptual: Algorithm4.35 is
   a tool that constructs the data structures which will control the parser, whereas the
   present algorithm constructs similar data structures during parsing. Therefore, such
   construction time does not impact on the ELR parser performance, but is the reason
   of the higher computational complexity of the Earley parser.

   We specify how Algorithm4.97 decides whether the input string x is legal for
   language L (G). If the algorithm terminates prematurely, before filling the whole
   vector E, then string x is obviously rejected. Otherwise, the acceptance condition
   below applies.

   Property 4.98 (Earley acceptance mode) When the Earley algorithm
   (Algorithm4.97) terminates, the string x is accepted if, and only if, the last
   element E [n] of vector E contains a final axiomatic pair with zero pointer, i.e., a pair
   of the form ? f S , 0 ? where it holds f S ∈ F S . ?
   We have seen a complete run of the algorithm in Table4.9, but that grammar is too
   simpletoillustrateotherinterestingpoints:iterationsintheEBNF grammar(circuits
   in the net) and nullable nonterminals.

   Example 4.99 (grammar with iterative operators and nullable nonterminal)
   Figure4.63 lists an unambiguous nondeterministic grammar (EBNF) with the
   corresponding machine net. The string a a bba a is analyzed and its Earley vector is
   shown in Fig.4.64. Since the last element, E [6], contains the axiomatic final pair
   ? 4 S , 0 ?, with pointer 0, the string is accepted as from Property4.98. The arcs
   representedinFig.4.64permittofollowtheoperationsthatleadtoacceptance.Adotted
   arc indicates a closure, e.g., ? 2 S , 0 ? → ? 0 B , 3 ?. A solid arc can indicate a 
   terminal shift, e.g., ? 0 B , 3 ? → ? 1 B , 3 ? on b. A dashed arc and a solid arc that join into
   the same destination together indicate a nonterminal shift: the source of the dashed
   arc is the enabling pair, that of the solid arc is the pair to be shifted, and the
   common destination of the two arcs is the operation result; e.g., ? 3 B , 3 ? → ? 3 S , 0 ?

<a id="P360"></a>

   ext. grammar (EBNF) machine network
   S → a + (bB a) ∗ | A +
   5 S 0 S 1 S 2 S 3 S 4 S
   S
   ↓
   ↓
   → ←
   a A
   a
   b
   B
   a
   b
   A
   A → aAb | ab 0 A 1 A 2 A 3 A A → →
   a
   A b
   b
   B → bB a | c | ε
   0 B 1 B 2 B 3 B B →
   ↓
   →
   b B
   a
   c
   Fig.4.63 Nondeterministic EBNF grammar G and network of Example4.99
   (dashed) along with ? 2 S , 0 ? → ? 3 S , 0 ? on B (solid). The label on each solid arc
   identifies the shift type, terminal or nonterminal.

   An arc path starting with a (dotted) closure arc, continued by one or more (solid)
   shift arcs (terminal and non), and ended by a (dashed) nonterminal arc, represents
   E [0] a E [1] a E [2] b E [3] b E [4] a E [5] a E [6]
   0 S 0 1 S 0 1 S 0 2 S 0 1 B 3 3 B 3 4 S 0
   0 A 0 1 A 0 1 A 1 3 A 1 3 A 0 1 A 4 1 A 5
   0 A 1 0 A 2 0 B 3 0 B 4 3 S 0 0 A 6
   closure 2 A 0 5 S 0 0 A 5
   nonterm. shift 3 S 0 2 B 3
   (term. & nonterm.) shift 0 A 4
   a a b B
   a
   b
   B
   a
   ε
   Fig.4.64 Parse trace of string a a bba a with the machine net in Fig. 4.63

<a id="P361"></a>

   a complete recognizing path within the machine of the nonterminal. The solid
   nonterminal shift arc that has the same origin and destination as the path cuts short the
   whole derivation. See for instance:
   ?2 S , 0?
   closure
   −→
   (dotted)
   ?0 B , 3?
   b
   →
   (sl.)
   ?1 B , 3?
   B
   →
   (sl.)
   ?2 B , 3?
   a
   →
   (sl.)
   ?3 B , 3?
   n.t. shif t
   −→
   (dashed)
   ?3 S , 0?
   ?2 S , 0?
   (solid)
   −→
   nonterminal shif t on B
   ?3 S , 0?
   Notice the self-loop on pair ? 0 B , 4 ? with label ε, as state 0 B is both initial and
   final or equivalently nonterminal B is nullable. It means that here machine M B
   starts and immediately finishes the computation. Equivalently, it indicates that the
   correspondinginstanceof B immediatelygeneratestheemptystring,likeforinstance
   the inner B in the arc above does.

   A (solid) shift arc labeled by a nonterminal that generates the empty string,
   ultimately,i.e.,immediatelyorafterafewderivationsteps,hasitssourceanddestination
   in the same vector element. See for instance in E [4]:
   ?1 B , 3?
   closure
   −→
   (dotted)
   ?0 B , 4?
   ε
   −→
   (solid)
   ?0 B , 4?
   n.t. shif t
   −→
   (dashed)
   ?2 B , 3?
   ?1 B , 3?
   (solid)
   −→
   nonterminal shif t on B
   ?2 B , 3?
   In fact, if the nonterminal shift arc ?1 B , 3?
   B
   − → ?2 B , 3? spanned a few vector
   elements, then nonterminal B would generate the terminals in between, not the empty
   string. In practice, the complete thread of arcs in Fig.4.64 represents the syntax tree
   of the string, which is explicitly shown in Fig.4.65. The nodes of the top tree are
   the machine transitions that correspond to the solid shift arcs (terminal and non-),
   while the bottom tree is the projection of the top one over the total alphabet of the
   grammar (terminals and nonterminals) and is the familiar syntax tree of the string
   for the given grammar. ?
   Correctness and Completeness of Earley Algorithm
   Property4.100,below,establishesaconnectionbetweenthepresenceofcertainpairs
   in the Earley vector, and the existence of a leftmost derivation for the string prefix
   analyzedsofar.AlongwiththeconsequentialProperty4.101,itstatesthecorrectness
   ofthemethod:ifastringisacceptedbyAlgorithm4.97foranEBNF grammarG,then
   it belongs to language L (G). Both properties are simple and it suffices to illustrate
   them by short examples.

   Property 4.100 (Earley—I) If it holds ? q, j ? ∈ E [i], which implies inequality
   j ≤ i, with q ∈ Q X , i.e., state q belongs to the machine M X of nonterminal X, then

<a id="P362"></a>

   representation with shift arcs or net transitions
   S
   0 S
   a
   − → 1 S 1 S
   a
   − → 1 S 1 S
   b
   − → 2 S 2 S
   B
   −→ 3 S
   0 B
   b
   − → 1 B 1 B
   B
   −→ 2 B
   0 B
   ε
   − → 0 B
   2 B
   a
   − → 3 B
   3 S
   a
   − → 4 S
   projection over the total alphabet of the grammar
   S
   a a b B
   b B
   ε
   a
   a
   Fig.4.65 Syntax tree for the string a a bba a of Example4.99
   it holds ? 0 X , j ? ∈ E [j] and the right-linearized grammar
   ˆ
   G (see Sect. 4.5.2.1
   on p.239) admits a leftmost derivation 0 X
   ∗
   = ⇒ x j+1 ... x i q if j < i or 0 X
   ∗
   = ⇒ q
   if j = i. ?
   For instance: in Example4.96 it holds ? 3 A , 1 ? ∈ E [3] with q = 3 A and 1 = j <
   i = 3, then it holds ? 0 A , 1 ? ∈ E [1] and grammar
   ˆ
   G admits a derivation 0 A ⇒
   a 1 A ⇒ a b3 A ; in Example4.99 it holds ? 0 B , 4 ? ∈ E [4] with q = 0 B and j =
   i = 4, then the property holds because state q itself is initial already.

   Property 4.101 (Earley—II) If the Earley acceptance condition (Property4.98) is
   satisfied, i.e., ? f, 0 ? ∈ E [n] with f ∈ F S , then the EBNF grammar G admits a
   derivation S
   +
   = ⇒ x, i.e., x ∈ L (S), and string x belongs to language L (G). ?
   For instance, again in Example4.96, it holds ? 1 S , 0 ? ∈ E [4] (n = 4) with f =
   1 S ∈ F S , then the right-linearized and EBNF grammars
   ˆ
   G and G admit derivations

<a id="P363"></a>

   0 S
   ∗
   = ⇒ a a bb1 S and S
   ∗
   = ⇒ a a bb, respectively, hence it holds a a bb ∈ L (S) and
   string a a bb belongs to language L (G). In fact, condition ? 1 S , 0 ? ∈ E [4] is the
   Earley acceptance one, as 1 S is a final state for the axiomatic machine M S . This
   concludes the proof that Algorithm4.97 is correct.

   Property4.102, below, contains two points 1 and 2 that are the converse of
   Properties4.100 and 4.101, respectively, and states the completeness of the method:
   givenanEBNF grammar G,ifastringbelongstolanguage L (G),thenitisaccepted
   by Algorithm4.97 executed for G.

   Property 4.102 (Earley—III) TakeanEBNFgrammarG andastringx = x 1 ... x n
   of length n that belongs to language L (G). In the right-linearized grammar
   ˆ
   G,
   consider any leftmost derivation d of a prefix x 1 ... x i (i ≤ n) of x, that is:
   d: 0 S
   +
   = ⇒ x 1 ... x i q W
   with q ∈ Q X and W ∈ Q ∗ , where Q is the state set of the whole net. The two points
   below apply:
   1. ifitholds W ?= ε,i.e., W = r Z forsomer ∈ Q Y ,thenitholds∃ j 0 ≤ j ≤ i and
   ∃ p ∈ Q Y such that the machine net has an arc p
   X
   − → r and grammar
   ˆ
   G admits
   two leftmost derivations d 1 : 0 S
   +
   = ⇒ x 1 ... x j p Z and d 2 : 0 X
   +
   = ⇒ x j+1 ... x i q,
   so that derivation d decomposes as follows:
   d: 0 S
   d 1
   = ⇒ x 1 ... x j p Z
   p→0 X r
   ====⇒ x 1 ... x j 0 X r Z
   d 2
   = ⇒ x 1 ... x j x j+1 ... x i q r Z = x 1 ... x i q W
   as an arc p
   X
   − → r in the net maps to a rule p → 0 X r in grammar
   ˆ
   G
   2. this point is split into two steps, the second being the crucial one:
   a. if it holds W = ε, then it holds X = S, i.e., nonterminal X is the axiom,
   q ∈ Q S and ? q, 0? ∈ E [i]
   b. if it also holds x 1 ...x i ∈ L (G), i.e., the prefix also belongs to language
   L (G), then it holds q ∈ F S , i.e., state q is final for the axiomatic machine
   M S , and the prefix is accepted (Property4.98) by the Earley algorithm
   Limit cases: if it holds i = 0 then it holds x 1 ... x i = ε; if it holds j = i then it
   holds x j+1 ... x i = ε; and if it holds x = ε (so n = 0) then both cases hold, i.e.,
   j = i = 0. We skip over Property4.102 as entirely evident.

   If the prefix coincides with the whole string x, i.e.,i = n, then step 2b implies that
   string x, which by hypothesis belongs to language L (G), is accepted by the Earley
   algorithm, which therefore is complete.

   We have already observed that the Earley algorithm unavoidably accepts also all
   the prefixes that belong to the language, whenever in an element E [i] before the

<a id="P364"></a>

   last one E [n] (i < n) the acceptance condition of Property 4.98 is satisfied. For
   instance in Example4.99 it holds ? 5 S , 0 ? ∈ E [4] with state 5 S final, so the 
   corresponding prefix a a bb of length 4 is accepted and it actually belongs to language
   L (G).

   Inconclusion,accordingtoProperties4.100–4.102,Algorithm4.97isbothcorrect
   and complete. Hence it performs the syntax analysis of EBNF grammars with no
   exceptions. ?
   Computational Complexity
   We have seen that the Earley parser of Algorithm4.97 does much more work than
   the deterministic top-down and bottom-up parsers do, which we know to have a time
   complexity linear with respect to the length of the input string. But how much more
   does it work? It is not difficult to compute the asymptotic time complexity in the
   worst case. We stepwise examine the algorithm.

   For the Earley parser, the chosen grammar is fixed and the input to analyze is
   a string. For a generic string of length n ≥ 1, we estimate the numbers of pairs
   ? state, pointer ? in the Earley vector E and of basic operations performed on
   them to fill the vector. A basic operation consists of checking if a pair has a certain
   state or pointer, or of adding a new pair. We can assume all the basic operations to
   have a constant computational cost, i.e., O(1). Then:
   1. A vector element E [i] (0 ≤ i ≤ n) contains a number of pairs ? q, j ? that is
   linear in i, as the number of states in the machine net is a constant and it holds
   j ≤ i. Since it also holds i ≤ n, we conservatively assume the number of pairs
   in E [i] is linearly bounded by n, i.e., O(n).

   2. For a pair ? p, j ? checked in the element E [i − 1], the terminal shift operation
   adds one pair to E [i]. So, for the whole E [i − 1], the TerminalShift procedure
   needs no more than O(n) × O(1) = O(n) basic operations.

   3. TheCompletionprocedureiteratestheoperationsofclosureandnonterminalshift
   as long as they can add some new pair, that is until the element saturates. So, to
   account for all loop iterations, we examine the two operations on the whole set
   E [i]:
   a. For a pair ? q, j ? checked in E [i], the closure adds to E [i] no more pairs
   than the number | Q | of states in the machine net, i.e., O(1). So, for the
   whole E [i], the closure needs no more than O(n) × O(1) = O(n) basic
   operations.

   b. For a final pair ? f, j ? checked in E [i], the nonterminal shift first searches
   pairs ? p, l ? with certain states p through E [j], which has a size O(n),
   and then adds to E [i] as many pairs as it has found, which are no more than
   the size of E [j], i.e., O(n). So, for the whole E [i], it needs no more than
   O(n) × O(n) + O(n) = O(n 2 ) basic operations.

   Therefore, for the whole set E [i], the Completion procedure in total needs no
   more than O(n) + O(n 2 ) = O(n 2 ) basic operations.


<a id="P365"></a>

   4. By summing up the numbers of basic operations performed by the procedures
   TerminalShift and Completion for i from 0 to n, we obtain:
   Earley cost = TerminalShift × n + Completion × (n + 1)
   = O(n) × n + O(n 2 ) × (n + 1) = O(n 3 )
   The time cost estimate is also conservative as the algorithm may stop, before
   completely filling the vector, when it rejects the string.

   The limit case of analyzing the empty string ε, i.e., n = 0, is trivial. The Earley
   algorithmjustcallstheCompletionprocedureonce,whichonlyworksontheelement
   E [0]. Since all the pairs in E [0] have a pointer of value 0, their number is no more
   than | Q |. So the cost of the whole algorithm only depends on the network size.
   Property 4.103 (Earley complexity) The asymptotic time complexity of the Earley
   algorithm in the worst case is cubic, i.e., O(n 3 ), where n is the length of the string
   analyzed. ?

### 4.11.2.1 Remarks on Complexity and Optimizations

   In practice, the Earley algorithm (Algorithm4.97) performs faster in some relevant
   situations.Forthenon-extended(BNF)grammarsthatarenotambiguous,theworst-
   case time complexity was proved to be quadratic, i.e., O(n 2 ), by Earley himself
   [39]. For the non-extended (BNF) grammars that are deterministic, the complexity
   approaches a linear time, i.e., O(n), in most cases [18].

   It is worth hinting how the non-ambiguous case works. If the grammar is not
   ambiguous, then a pair ? q, j ? is never added twice or more times to a vector
   element E [i] (0 ≤ j ≤ i ≤ n). In fact, if it happened, the grammar would have
   different ways to generate a string and it would be ambiguous. Then an optimization
   is possible. Suppose to rearrange each element and merge all the pairs contained in
   it that have the same state q, into a single tuple of t + 1 items: a state q and a series
   of t ≥ 1 pointers, i.e., ? q, j 1 , ..., j t ?. Since while the element builds, each pair
   adds only once to it, a tuple may not have any identical pointers and therefore the
   inequality t ≤ n + 1 holds. A shifting operation now simply means to search and
   find a tuple in an element, to update the tuple state and to copy the modified tuple
   into a subsequent element. If the destination element already has a tuple with the
   same state, both pointer series must be concatenated but, as explained before, the
   resulting tuple will not contain any duplicated pointers; the pointer series may be
   unordered, but this does not matter. So the rearrangement keeps and makes a sense.
   Now, we can estimate the complexity. Say constant s = | Q | is the number of
   states in the machine net. Since in a vector element each tuple has a different state,
   the number of tuples in there is bounded by s, i.e., a constant. Thus, the number of
   pointers in an element is bounded by s × t ≤ s × (n + 1), i.e., linear. The terminal
   shift and closure operations take a linear time per element, as before. But the 
   nonterminal shift operation gets lighter. Suppose to process an element: searching tuples

<a id="P366"></a>

   in the previous elements needs a number of state checks bounded by s 2 × (n + 1),
   i.e., linear; and copying the modified tuples cannot add to the element more than a
   linear number of pointers, for else duplication occurs. So, the three operations
   altogether take a linear time per element, and quadratic for the vector. Hence, the Earley
   algorithm for non-ambiguous non-extended (BNF) grammars has a worst-case time
   complexity quadratic in the string length, while, without optimization, it was cubic
   (Property4.103).

   32
   We also mention the issue of nullable nonterminals. As explained before, in the
   Earley algorithm these are processed through a cascade of closure and nonterminal
   shift operations in procedure Completion. We can always put an EBNF grammar
   into non-nullable form and get rid of this issue, yet the grammar may get larger
   and less readable; see Sect. 2.5.13.3. But an optimized algorithm can be designed to
   process the nullables of BNF grammars in only one step; see [42]. The same authors
   also define optimized procedures for building the syntax tree with nullables, which
   are adjustable for our version of the Earley algorithm and could work with EBNF
   grammars as well.


## 4.11.3 SyntaxTree Construction

   The next function BuildTree BT builds the syntax tree of an accepted string, using
   the vector E computed by Earley algorithm. For simplicity, BT works under the
   assumption that the grammar is unambiguous, and returns the unique syntax tree.
   The tree is represented as a parenthesized string, where two matching parentheses
   delimit a subtree rooted at some nonterminal node (such representation is described
   in Chap. 2 on p. 47).

   Take an EBNF grammar G = (V, Σ, P, S) and machine net M. Consider a
   string x = x 1 ... x n or x = ε of length n ≥ 0 that belongs to language L (G), and
   suppose its Earley vector E with n + 1 ≥ 1 elements is available.

   Function BT is recursive and has four formal parameters: nonterminal X ∈ V,
   state f , and two non-negative indices j and i. Nonterminal X is the root of the
   (sub)tree to be built. State f is final for machine M X ; it is the end of the computation
   path in M X that corresponds to analyzing the substring generated by X. Indices j
   and i satisfy the inequality 0 ≤ j ≤ i ≤ n; they, respectively, specify the left and
   right ends of the substring generated by X:
   X
   +
   =⇒
   G
   x j+1 ... x i if j < i X
   +
   =⇒
   G
   ε if j = i
   Grammar G admits derivation S
   +
   = ⇒ x 1 ... x n or S
   +
   = ⇒ ε, and the Earley algorithm
   accepts string x. Thus, element E [n] contains the final axiomatic pair ? f, 0?. To
   build the tree of string x with root node S, function BT is called with parameters
   32 Since this analysis is independent of the grammar being non-extended, it reasonably holds for
   non-ambiguous extended (EBNF) grammars as well. But remember that the element contents need
   a suitable rearrangement.


<a id="P367"></a>

   BuildTree ( S, f, 0, n ); then, the function will recursively build all the subtrees
   and will assemble them in the final tree. The commented code follows.

   Algorithm 4.104 (Earley syntax tree construction)
   BuildTree ( X, f, j, i )
   - - X is a nonterminal, f is a final state of M X and 0 ≤ j ≤ i ≤ n
   - - return as parenthesized string the syntax tree rooted at node X
   - - node X will have a list C of terminal and nonterminal child nodes
   - - either list C will remain empty or it will be filled from right to left
   C := ε - - set to ε the list C of child nodes of X
   q := f - - set to f the state q in machine M X
   k := i - - set to i the index k of vector E
   - - walk back the sequence of term. & nonterm. shift oper.s in M X
   while ( q ?= 0 X ) do - - while current state q is not initial
   - - try to backwards recover a terminal shift move p
   x k
   −→ q, i.e.,
   - - check if node X has terminal x k as its current child leaf
   (a) if
   ?
   ∃h = k − 1 ≥ j ∃ p ∈ Q X such that
   ? p, j ? ∈ E [h] ∧ net has p
   x k
   −→ q
   ?
   then
   C := x k · C - - concatenate leaf x k to list C
   end if
   - - try to backwards recover a nonterm. shift oper. p
   Y
   − → q, i.e.,
   - - check if node X has nonterm. Y as its current child node
   (b) if
   ?
   ∃Y ∈ V ∃e ∈ F Y ∃h j ≤ h ≤ k ≤ i ∃ p ∈ Q X s.t.

   ? e, h ? ∈ E [k] ∧ ? p, j ? ∈ E [h] ∧ net has p
   Y
   − → q
   ?
   then
   - - recursively build the subtree of the derivation:
   - - Y
   +
   ⇒
   G
   x h+1 ... x k if h < k or Y
   +
   ⇒
   G
   ε if h = k
   - - and concatenate to list C the subtree of node Y
   C := BuildTree ( Y, e, h, k ) · C
   end if
   q := p - - shift the current state q back to p
   k := h - - drag the current index k back to h
   end while
   return ( C ) X - - return the tree rooted at node X ?

<a id="P368"></a>

   Essentially, BT walks back on a computation path in machine M X and jointly scans
   back the vector from E [n] to E [0]. During the traversal, BT recovers the terminal
   andnonterminalshift operationstoidentifythechildren(terminalleavesandinternal
   nonterminalnodes)ofthesameparentnode X.Inthisway,BT reconstructsinreverse
   order the shift operations performed by the Earley algorithm (Algorithm4.97).
   ThewhileloopisthekerneloffunctionBT:itrunszeroormoretimesandrecovers
   exactly one shift operation per iteration. Conditions (a) and (b) in the loop body,
   respectively, recover a terminal and nonterminal shift. For a terminal shift (case (a)),
   thefunctionappendstherelatedleaf.Foranonterminalshift(case(b)),itrecursively
   calls itself and thus builds the related node and subtree. The actual parameters in the
   call are as prescribed: state e is final for machine M Y , and inequality 0 ≤ h ≤ k ≤ n
   holds.

   A limit case: if the parent nonterminal X immediately generates the empty string,
   then leaf ε is the only child, and the loop is skipped from start (see also Fig.4.67).
   Function BT uses two local variables in the while loop: the current state q of
   the machine M X ; and the index k of the current Earley vector element E [k]; both
   variables are updated at each iteration. In the first iteration, state q is the final state
   f of M X . When the loop terminates, state q is the initial state 0 X of M X . At each
   iteration, state q shifts back from state to state, along a computation path of M X .
   Similarly,indexk isdraggedbackfromelementi to j fromloopentrytoexit,through
   a few backward long or short hops. Sometimes however, index k may stay on the
   same element for a few iterations; this happens if, and only if, the function processes
   a series of nonterminals that ultimately generate the empty string.

   Function BT creates the list C, initially set to empty, which stores the children of
   X, from right to left. At each iteration, the while loop concatenates (on the left) one
   new child to C. A child can be a leaf (point (a)) or a node with its subtree recursively
   built(point(b)).Atloopexit,thefunctionreturnslistC encapsulatedbetweenlabeled
   parentheses: ( C ) X (see Fig.4.67). In the limit case when the loop is skipped, the
   function returns the empty pair ( ε ) X , as the only child of X is leaf ε.
   Our hypothesis that the grammar is non-ambiguous has the important 
   consequence that the conditional statements (a) and (b) are mutually exclusive at each
   loop iteration. Otherwise, if the grammar were ambiguous, function BT, as encoded
   in Algorithm4.104, would be nondeterministic.

   Two Examples
   We show how function BuildTree constructs the syntax trees for two grammars and
   we discuss in detail the first case. The second case differs in that the grammar has
   nullable nonterminals.

   Example 4.105 (construction of syntax tree from Earley vector) We reproduce the
   machinenetofExample4.96inFig.4.66,topleft.Forinputstringx 1 ... x 4 = a a bb
   the figure displays, from top to bottom, the syntax tree, the call tree of the
   invocations of function BuildTree, and the vector E returned by Earley algorithm

<a id="P369"></a>

   (Algorithm 4.97). The Earley vector (the same as in Table4.9) is commented with
   three boxes, which contain the conditions of the two nonterminal shifts, and the
   conditions of one (out of four) terminal shift. The arrows are reversed with respect
   to those in Fig.4.64, and trace the terminal and nonterminal shifts. The string is
   accepted thanks to pair ? 1 S , 0 ? in element E [4] (Property4.98). The initial call to
   function BuildTree is BT ( S, 1 S , 0, 4), which builds the tree for the whole string
   x 1 ... x 4 = a a bb.

   The axiomatic machine M S has an arc 0 S
   A
   − → 1 S . Vector E contains the two pairs:
   ? 3 A , 0 ? ∈ E [4],whichhasafinalstateofmachine M A andsoenablesashiftopera-
   tiononnonterminal A;and? 0 S , 0 ? ∈ E [0],whichhasastatewithanarcdirectedto
   state1 S andlabeledwithnonterminal A.Thus,condition(b)inthewhileloopofAlgo-
   rithm4.104 is satisfied. Therefore, the first recursive call BT ( A, 3 A , 0, 4) is
   executed, which builds the subtree for the (still whole) (sub)string x 1 ... x 4 = a a bb.
   This invocation of function BuildTree walks back vector E from element E [4]
   to E [1] and recovers the terminal shift on b, the nonterminal shift on A and the
   terminal shift on a. When processing the nonterminal shift on A with reference
   to element E [3], another recursive call occurs because vector E contains the two
   pairs? 3 A , 1 ? ∈ E [3]and? 1 A , 0 ? ∈ E [1],andmachine M A hasanarc1 A
   A
   − → 2 A .

   Therefore, condition (b) in the while loop is satisfied and the second recursive call
   BT ( A, 3 A , 1, 3) is executed, which builds the subtree for substring x 2 x 3 = a b.
   The two boxes at the bottom of Fig.4.66 explain the nonterminal shifts (point (b)
   inAlgorithm4.104),pointedtobyadoublearrow.Indetail,theformalparameters X
   and j (upperside)havetheactualvaluesofthecurrentinvocationofBuildTree.Local
   variables q and k (lower side) have the values of the current iteration of the while
   loop. Inside a box we find in order: the values assumed by nonterminal Y, final state
   e, index h and state p; second, some logical conditions on pairs and arc; and, last,
   the call to BT with its actual parameters. Each box so justifies one iteration, where
   condition (b) is satisfied, whence a call to BT occurs and a subtree is constructed.
   For terminal shifts the situation is simpler: consider the comment box in Fig.4.66
   andcompareitwiththeif-thenstatement(a)inAlgorithm4.104.Threemoreterminal
   shifts take place, but we do not describe them. ?
   Example 4.106 (syntax tree with nullable nonterminals) Figure4.67 shows the call
   tree of function BuildTree of Example4.99. The corresponding parse trace is in
   Fig.4.64 on p. 360: to see how function BT walks back the Earley vector and
   constructs the syntax tree, imagine to reverse the direction of solid and dashed arcs (the
   dotted arcs of closure are not relevant). Figure4.67 also shows the parenthesized
   subtrees returned by each call to BT.

   The innermost call to function BuildTree in Fig.4.67 is BT ( B, 0 B , 4, 4), which
   constructsthesubtreeofaninstanceofnonterminal B thatimmediatelygeneratesthe
   empty string ε. In fact, the two indices j and i are identical to 4 and their difference
   is zero; this means that the call is on something nullable inserted between terminals
   b 4 and a 5 . ?

<a id="P370"></a>

   0 S 1 S S → →
   A, B
   0 A 1 A 2 A 3 A A → →
   a A b
   b
   0 B 1 B 2 B 3 B 4 B B → →
   a a B b
   b
   BT (S, 1 S , 0, 4)
   BT (A, 3 A , 0, 4)
   a 1 BT (A, 3 A , 1, 3)
   a 2 b 3
   b 4
   call tree of function BT
   S
   A
   a A
   a b
   b
   syntax tree
   machine network
   Earley vector and how function BT walks it back for constructing the syntax
   tree above
   E [0] a 1 E [1] a 2 E [2] b 3 E [3] b 4 E [4]
   0 S 0
   0 A 0
   0 B 0
   1 A 0
   1 B 0
   0 A 1
   2 B 0
   1 A 1
   0 B 2
   0 A 2
   4 B 0
   3 A 1
   1 S 0
   2 A 0
   3 A 0
   1 S 0
   a
   a
   b
   b
   A
   A
   while loop iteration
   recursive call to BT
   destination of shift
   h = 0 p = 0 A
   0 A 0 ∈ E [0]
   net has 0 A
   a
   − → 1 A
   ⇒ leaf a is child of A
   X = A j = 0
   q = 1 A k = 1
   term. shift
   0 A
   a
   − → 1 A
   Y = A e = 3 A
   h = 1 p = 1 A
   3 A 1 ∈ E [3]
   1 A 0 ∈ E [1]
   net has 1 A
   A
   −→ 2 A
   ⇒ BT (A, 3 A , 1, 3)
   X = A j = 0
   q = 2 A k = 3
   nonterm. shift 1 A
   A
   −→ 2 A
   Y = A e = 3 A
   h = 0 p = 0 S
   3 A 0 ∈ E [4]
   0 S 0 ∈ E [0]
   net has 0 S
   A
   −→ 1 S
   ⇒ BT (A, 3 A , 0, 4)
   X = S
   j = 0
   q = 1 S k = 4
   nonterm. shift 0 S
   A
   −→ 1 S
   Fig.4.66 Actions and calls of function BuildTree on the shifts for Example4.96

<a id="P371"></a>

   BT (S, 4 S , 0, 6)
   = a a b b ( ε ) B a
   B
   a
   S
   a 1 a 2 b 3
   BT (B, 3 B , 3, 5)
   = b ( ε ) B a
   B
   b 4
   BT (B, 0 B , 4, 4)
   = ( ε ) B
   ε
   a 5
   a 6
   Fig.4.67 Calls and return values of BuildTree for Example4.99
   Computational Complexity
   The worst-case computational complexity of function BuildTree can be estimated
   by computing the size of the constructed syntax tree, the total number of nodes and
   leaves. Such total is directly related to the number of terminal and nonterminal shifts
   that function BT has to recover.

   The tree to be built is only one for the string accepted, as the grammar is
   nonambiguous. Since the grammar is also clean and free from circular derivations, for
   a string of length n the number of tree nodes is linearly bounded by n, i.e., O(n).
   The basic operations are those of checking the state or pointer of a pair, and of
   concatenating one leaf or node (i.e., its parenthesized representation) to the child
   list; both operations take a constant time, for a suitable parenthesized representation
   of the tree. The following analysis mirrors the complexity analysis of the Earley
   algorithm (function BT uses k as current index to vector E):
   1. A vector element E [k] (0 ≤ k ≤ n) contains a number of pairs of magnitude
   O(n).

   2. Checking the condition of the if-then statement (a) of Algorithm4.104 takes a
   constant time, i.e., O(1). The possible enlisting of one leaf takes a constant time
   as well. So, processing the whole E [k − 1] takes a time of magnitude O(n) ×
   O(1) + O(1) = O(n).

   3. Checkingthecondition(b)needstoidentifythepairs:(1)? e, h ?in E [k];and(2)
   ? p, j ? in E [h]. For each potential candidate pair (1) in E [k], the whole E [h]
   has to be searched to find the corresponding candidate pair (2) (if any); the search
   takesatimeofmagnitudeO(n).Thepossibleenlistingofone(nonterminal)node
   takes a constant time, i.e., O(1). So, processing the whole E [k] takes a time of
   magnitude O(n) × O(n) + O(1) = O(n 2 ).


<a id="P372"></a>

   4. Since the total number of terminal plus nonterminal shifts (a) and (b) to be 
   recovered is bounded by the number of tree nodes, i.e., O(n), the total time cost of
   function BT is no more than:
   BTcost =
   ?
   if-then (a) + if-then (b)
   ?
   × #of tree nodes
   = ?
   O(n) + O(n 2 )
   ?
   × O(n) = O(n 3 )
   Property 4.107 (treebuildingcomplexity) Theasymptotictimecomplexityoffunc-
   tion BuildTree to construct a syntax tree for a non-ambiguous grammar (Algo-
   rithm4.104), is cubic in the worst case, i.e., O(n 3 ), where n is the length of the
   string accepted. ?
   If we consider that function BuildTree (Algorithm4.104) just reads the Earley vector
   E and does not write it, the above complexity can be lowered by pre-ordering the
   vector. Suppose every element E [k] is independently ordered: pairs ? q, j ? are
   ranked by the value of pointer j (0 ≤ j ≤ n); ranking by state q does not matter as
   the number of states is a constant. Ordering each element takes a time O(n logn),
   e.g.,bytheQuickSortalgorithm.So,orderingalltheelementstakesatime(n + 1) ×
   O(n logn) = O(n 2 logn). After ordering the elements, function BT can regularly
   run as before.

   The benefit of pre-ordering is mainly for the time cost of the if-then statement
   (b) in function BuildTree. Now, finding a pair with some final state e and pointer
   h in E [k] takes a time O(n); and consequently searching the related pair with a
   fixed pointer j in E [h] takes a time O(logn), e.g., by the Binary Search algorithm.
   So, statement (b) takes a time O(n logn). Similarly, in the if-then statement (a) we
   can use Binary Search for searching a pair with fixed pointer j in E [h]| h=k−1 , so
   statement (a) takes a time O(logn). Therefore, the total time cost of function BT is
   no more than as below:
   BT cost = pre-ordering E +
   ?
   if-then (a) + if-then (b)
   ?
   × #of tree nodes
   = O(n 2 logn) +
   ?
   O(logn) + O(n logn)
   ?
   × O(n) = O(n 2 logn)
   This new complexity of BuildTree, for non-ambiguous grammars, is lower than the
   previous cubic one without pre-ordering (Property4.107), and it approaches that of
   the Earley algorithm rearranged for non-ambiguous grammars, too.

   We conclude by warning again that the version of function BuildTree presented
   here is not designed to properly work with ambiguous grammars. Anyway, since the
   grammars of programming languages are usually non-ambiguous, such a limitation
   is not relevant for compilation. However, ambiguous grammars are unavoidable in
   naturallanguageprocessing.There,themaindifficultyishowtocompactlyrepresent
   all the syntax trees the string may have, the number of which may be exponential—
   and even unbounded if there are nullable nonterminals—in the string length. This
   can be done by using a so-called Shared Packed Parse Forest (SPPF), which is a
   graph type more general than a tree but that still takes a worst-case cubic time for
   building; see [40] for how to proceed in such more general cases.


<a id="P373"></a>

   A practical parser for a programming language usually works on a syntax tree
   represented as a graph, with variables that model nodes and leaves, and pointers
   that connect them; not on a tree represented as a parenthesized string. The function
   BuildTree in Algorithm4.104 can be easily adapted to build such a tree, without
   changingitsalgorithmicstructure.Hereishowtorecodethethreecrucialstatements
   of BT (the rest is unmodified):
   original encoding recoded for a pointer-connected tree
   C := x k · C create leaf x k and attach it to node X
   C := BuildTree ( Y, e, h, k ) · C
   create node Y, call BuildTree (Y, ...)
   and attach node Y to node X
   return ( C ) X return the pointer to node X
   If upon return node X does not have any child nodes or leaves yet, leaf ε has to
   be attached to it. It is not difficult to encode the instructions above by means of a
   library for managing of graph structures, as many are available. The worst-case time
   complexity of this variant of BuildTree is still cubic. See also [18] for an overview
   of a few parsing methods and practical variants.

   4.12 Managing Syntactic Errors and Changes
   In this section we address two additional features that are requested from a parser:
   the capability to manage user errors in a text, and the ability to efficiently reparse
   the modified text when the errors are fixed, or more generally when some changes
   have been made.


## 4.12.1 Errors

   In human communication the hearer usually tries to correct the errors that quite
   frequentlyoccurinutterances,toassignalikelymeaningtothesentence;ifhefails,he
   mayprompttheuttererforacorrection.Ontheotherhand,forartificiallanguagesthe
   communication partners can be human beings and machines, and it helps to separate
   three cases: machine-to-human (as in automatic answering systems), machine-to-
   machine (e.g., XML) and human-to-machine, where programming languages are the
   typical case. Since “errare humanum est”, but machines are supposed not to make
   mistakes,wehavetodealonlywiththelastcase:ahandwrittendocumentorprogram
   is likely to contain errors and the machine has to be capable to suitably react.
   The simplest reaction is to stop processing the text as soon as the machine detects
   anerror,andtodisplayadiagnosticmessagefortheauthor.Amorecompletereaction
   involves recovery from error, i.e., the capability to resume processing the text. Yet
   more ambitious but harder to obtain is the capability to automatically correct the
   error.


<a id="P374"></a>

   Historically, in the old times when a central computer slowly processed batches
   of texts submitted by many users, the ability to detect all or most errors in a single
   compilationwasamust,inordertoavoidtime-consumingdelayscausedbyrepeated
   compilationsafterasingleerrorhadbeencorrectedbytheuser.Nowadays,compilers
   are interactive and faster, and a treatment of all the errors at once is no longer
   requested, while it remains important to provide an accurate diagnostic for helping
   the user to fix the error.

   In the following, first we classify the error types, then we outline some simple
   strategies for error recovery in parsers.


### 4.12.1.1 ErrorTypes

   The errors a person makes in writing can be subjective or objective. In a subjective
   error, the text remains formally correct both syntactically and semantically, but it
   does not reflect the author’s intended meaning. For instance, the typo x − y + z
   instead of x × y + z is a subjective error, which will alter the program behavior at
   run-time in rather unpredictable ways, yet it cannot be catched at compile time. A
   human error is objective if it causes a violation of the language syntax or semantic
   specifications.

   Sometimes,undetectederrorscausethecompiledprogramtoraisefaultconditions
   atrun-time,suchasmemoryerrors(segmentation,overflow,etc.)ornontermination.
   In such cases we say the program violates the dynamic (or run-time) semantic of
   the language. On the other hand, the errors that can be detected by the compiler or
   by the static semantic analyzer (described in the next chapter) are called static. It
   pertains to the run-time support (interpreter, virtual machine or operating system) of
   thelanguagetodetectandrecoverfromdynamicerrors,perhapsbyinvokingsuitable
   exception handlers. The methods used to detect subjective errors and to manage the
   dynamic errors they induce belong to software engineering, to operating system and
   virtual machine design and are out of scope for this book.

   Therefore,weonlydealwithobjectivestaticerrors,whichcanbefurtherclassified
   as syntactic and semantic. A syntactic error violates the context-free grammar that
   defines the language structure, or the regular expression that defines the lexicon. A
   semanticerrorviolatessomeprescriptionincludedinthelanguagereferencemanual,
   but not formulated as a grammar rule. Typically, such prescriptions are specified in
   English; less frequently, they are formalized using a formal or semi-formal notation
   (the attribute grammars of Chap. 5 are an example). For languages such as Java,
   the so-called type violations are a primary kind of semantic error: e.g., using as
   real a variable that was declared to be boolean; or using three subscripts to access
   a one-dimensional array; and many others. To detect and manage static semantic
   errors, the compiler builds certain data structures, called symbol tables that encode
   the properties of all the objects that are declared or used in a program.

   Sincethischapterdealswithparsing,wepresentsomebasicideasforthedetection,
   diagnosis and recovery of syntactic errors. A general discussion of error processing
   in parsers is in the book [18].


<a id="P375"></a>


### 4.12.1.2 Syntactic Errors

   We examine how a parser detects a syntactic error. To keep our analysis general
   enough, we avoid implementation details and we represent the parser as a pushdown
   automaton, which scans the input from left to right until either the end-of-text is
   reached or the move is not defined in the current configuration, to be then called
   erroneous. The current token, for which the move is undefined, is named the error
   token.Fromasubjectivepointofview,themistakeortypotheauthormadeinwriting
   thetextmayhaveoccurredatamuchearliermoment.Anegativeconsequenceisthat
   the parser diagnosis of the erroneous configuration may have little resemblance with
   the author’s subjective mistake. In fact, the input string ending with the token that
   the human erroneously typed, though different from the one the author had in mind,
   may be the initial part (prefix) of a legal sentence of the language, as next illustrated.
   Example 4.108 (human error and error token) In the FORTRAN language a loop
   such as DO 110 J = 9,20 is defined by the following rule:
   ?iterative instruction? → DO ?label? ‘=’ ?initial value? ‘,’ ?step?
   Suppose the human error is to omit the blank spaces around the label and so to
   write DO110J = 9,20; now token DO110J is a legal variable identifier and the string
   DO110J = 9 hitherto analyzed is a legal assignment of value 9 to it. The parser will
   detect the error on reading the comma. The delay between the first human error and
   the recognized error token is: DO 110J = 9
   delay
   , 20.

   A more abstract example from the regular language below:
   L = d b ∗ a a ∗ ∪ eb ∗ cc ∗
   happens with the illegal string x = d bb ... b
   ↓
   cccccc, where the first error token
   is marked by the arrow, whereas it is more likely that the human error was to type
   d for e, because for the two ways to correct x into x ? = e bb ... b
   delay
   cccccc or
   x ?? = d bb ... ba a a a a a, string x ? requires a single change, but string x ?? needs
   as many as six changes. Assuming the more likely correction, the delay between
   human error and error token is unbounded. ?
   The second example has introduced the idea of measuring the differences between
   the illegal string and the presumably correct one, and thus of obtaining a metric
   called editing or Levenshtein distance. In the example, the distance of x ? from x is
   one and of x ?? from x is six. Moreover, we say that string x ? is the interpretation at
   minimal distance from x, as no other sentence of language L has distance one from
   x. To give a precise definition of editing distance, one has to indicate which editing
   operations are considered.


<a id="P376"></a>

   A typical choice for typed texts includes the following operations, which 
   correspond to frequent typing mistakes: substituting a character for another; 33 erasing a
   character; and inserting a character. After detecting an erroneous token, it would be
   desirable to compute a minimal distance interpretation, which is a legal sentence as
   similar as possible to the scanned illegal string. Such a sentence is a reconstruction
   of what the typist intended to write, in accordance with a model that assigns a lower
   probability to the mutations involving many editing changes.

   Unfortunately, the problem of computing a minimal distance interpretation for
   context-free languages is computationally complex because of the unlimited spatial
   distance between the error token and the position where the editing corrections are
   due.Afurtherdifficultycomesfromthatitisnotknownhowthestringwillcontinue,
   thus what is a minimal distance interpretation so far, may turn out to necessitate too
   many editing changes when the input is further scanned. Therefore the typical error
   recovery techniques used by compilers, renounce to the minimal distance goal, yet
   they attempt to stay away from the opposite case necessitating as many corrections
   as the string length. 34
   Error Messages and Error Recovery
   In Fig.4.68 we represent the parser as a deterministic PDA. By definition of error
   token, the parser cannot move ahead, i.e., the transition function is undefined for
   the input b 0 and the top symbol A n . A straightforward (but not very informative)
   diagnostic message is: “token b 0 is not acceptable at line r”. This message can be
   made more precise by reporting the parser configuration, as we next explain for the
   case of a deterministic top-down parser.

   As we know, each symbol A i in the stack carries two pieces of information: a
   nonterminal symbol of the grammar (i.e., a net machine or a recursive procedure);
   and an internal state of that machine (i.e., the label of a procedure statement). We
   assume that the active machine is associated with A n . For the current instruction
   two cases are possible: the instruction scans a token e; or the instruction invokes a
   input: ... c 2 c 1 b 0 b 1 ... b m
   ←− w −→ error token ←− y −→
   stack top: → A n
   A n−1
   ...

   A 1
   Fig. 4.68 PDA parser configuration when an error is detected. The input string is z = wb 0 y ?,
   character b 0 is the error token, string w is the already scanned prefix, and string y = b 1 ... b m is
   the suffix still to be scanned. The pushdown stack contains α = A 1 A 2 ... A n , where A n is the top
   symbol
   33 The probability of a substitution error can be made more accurate by taking into account the key
   distances on the keyboard: two adjacent keys are more likely than two separated keys to be pressed
   one for the other.

   34 An example of minimum distance method for error handling is in [44].


<a id="P377"></a>

   machine/nonterminal E with guide set Gui(E). Then the message says: “at line r I
   expectedtoken e (or the tokens in Gui (E) in the latter case) insteadoftoken b 0 ”.
   Even with this improvement, such error messages can be misleading. Imagine
   that the subjective error omitted the first begin token in a string made of nested
   begin ... endblocks.Thefirsterrorconfigurationoftheparseroccurswhensymbol
   b 0 is the last end token of the series, while the parser expects to be outside of any
   block. The previous message tells only that token end is out of place, but gives
   no cue for finding the subjective error: the missing token begin could be in many
   possiblepositionsinthetext.Toproduceamoreusefuldiagnostichelp,someparsers
   incorporatealanguage-specificknowledge,suchasstatisticsaboutthemostfrequent
   errors.

   Error Recovery
   To complete this brief discussion, we outline a simple technique for error recovery.
   To resume parsing after an error, we need to intervene on one at least of the
   arguments (input token and top-of-stack symbol) of the PDA transition function, which
   is undefined in the current configuration. Notice that parsers of different types may
   haveasthirdargumenttheinternalstateofthePDA,whichisirrelevantinthecasewe
   are considering. It is easy to modify such arguments to make the next move possible,
   but it is more difficult to ensure that the following moves do not sink the parser into a
   newerrorconfiguration,orworseintoaseriesoferrorsandmeaninglessdiagnostics.
   A most elementary recovery technique, named panic mode, keeps skipping the
   next tokens b 1 , b 2 , ..., ?, until for some token b j the transition function becomes
   defined for arguments b j and A n , as this technique does not modify the stack. The
   intervening tokens from b 1 to b j−1 are ignored, thus causing the compiler to skip
   the analysis of potentially large text sections.

   Example 4.109 (panic mode) Consider a toy language containing two statement
   types:assignmentssuchasi = i + i + i,wheresymboli denotesavariableorproce-
   dureidentifier;andprocedurecallswithactualparameters,suchascall i ( i + i, i,
   i + i + i ).Weillustratethepanicmodeerrorrecoveryforthefollowing(erroneous)
   text:
   scanned
   text
   call i
   panic—skipped text
   error
   = i + i ... + i ? (4.27)
   The error is detected after correctly scanning two tokens, call and i. The parser
   reacts to the error event in this way: first it writes the diagnostic “I expected a left
   parenthesis and found an equal sign =”; then it panics and skips over a substring
   of unbounded length (including the error token), possibly generating useless
   diagnostics, as far as the very end of the text. So it can eventually recover a procedure
   call without parameters, call i, which may be the author’s intended meaning or not.
   However, skipping has the annoying effect that long text portions, which may
   contain more errors, are not analyzed; discovering such errors would require repeated
   compilation.


<a id="P378"></a>

   Notice that the text admits legal interpretations at small editing distance: deleting
   token call yields the assignment i = i + i ··· + i; and substituting token ‘=’
   with ‘(’ and then inserting token ‘)’ at the end before ‘?’ yields the invocation
   call i (i + i ··· + i ) of a procedure with one parameter. ?
   To suggest possible improvements over the panic technique, we observe that to
   exit the error configuration shown in (4.27), it suffices to let the parser insert tokens,
   accordingtoitsgrammaticalexpectations.Infact,startingagainfromtext (4.27),the
   parser is processing a statement belonging to the syntactic class of procedure calls,
   since it has already correctly scanned the call andi tokens when it hits the erroneous
   equal sign ‘=’. Inserting before the error token a left parenthesis token ‘(’, which
   is what the parser expects at this point instead of the equal sign ‘=’, yields:
   call i
   inserted
   ( = i + i ··· + i ?
   and thus allows one more parsing step, which scans the inserted token. Then the
   parser hits again the equal sign ‘=’, which is the error token:
   call i (
   error
   = i + i ··· + i ?
   Now the parser, having already inserted and not wanting to alter the original text
   too much, resorts to the panic technique, skips over the equal sign ‘=’ (or simply
   deletes it) and correctly parses the following piece of text i + ··· + i since this
   piece fits as actual parameter for the procedure, without any further errors as far as
   the end-of-text ‘?’, which becomes the new error token:
   call i (
   skipped
   (deleted)
   = i + i ··· + i
   error
   ?
   The last action is to insert a right parenthesis ‘)’ before the end-of-text ‘?’, as
   expected (since now the syntactic class is that of a parameter list):
   call i (i + i ··· + i
   inserted
   ) ?
   Then the analysis concludes correctly and recovers a procedure call that has an
   expressionasitsactualparameter:calli (i + i ··· + i ).Intotaltheparserperforms
   two insertions and one deletion, and the loss of text is limited. We leave to the reader
   to imagine the suitable diagnostic messages to be produced each time the parser
   intervenes on the error configuration.

   We have seen that a careful combination of token insertions and skips (or
   deletions), and possibly also of token substitutions, can reduce the substrings that are
   skipped and produce more accurate diagnostics. An interesting method for error
   recovery suitable to LL and LR parsing is in [45].


<a id="P379"></a>


## 4.12.2 Incremental Parsing

   Insomesituationsanothercapabilityisrequestedfromaparser(moregenerallyfrom
   acompiler):tobeabletoincrementallyprocessthegrammarortheinputstring.Asa
   matter of fact, the concept of incremental compilation takes two different meanings.
   incrementality with respect to the grammar This situation is not common and
   occurs when the source language is subjected to change, implying the grammar
   is not fixed. When changes are maybe minor but frequent, it is rather annoying or
   even impossible to create a new parser after each change. Such is the case of the
   so-called extensible languages, where the language user may modify the syntax
   of some constructs or introduce new instructions. It is then mandatory to be able
   to automatically construct the new parser, or better to incrementally modify the
   existing one after each syntax change. 35
   incrementality with respect to the source string A more common requirement is
   to quickly reconstruct the syntax tree after some change or correction of the input
   string has taken place.

   A good program construction environment should interact with the user and allow
   himtoeditthesourcetextandtoquicklyrecompileit,sominimizingtimeandeffort.

   In order to reduce the recompilation time after a change, incremental compilation
   methods have been developed that are based on special algorithms for syntax and
   semantic analysis. Focusing on the former, suppose the parser has analyzed a text,
   identified some kind of error and produced an error message. Then the author has
   made some corrections, typically in a few points of the text.

   A parser qualifies as incremental if the time it takes for analyzing the corrected text
   is much shorter than the time for parsing it the first time. To this end, the algorithm
   has to save the result of the previous analysis in such a form that updates can be
   just made in the few spots affected by changes. In practice, the algorithm saves the
   configurations traversed by the pushdown automaton in recognizing the previous
   text and rolls back to the most recent configuration that has not been affected by the
   changes to the text. From there the algorithm resumes parsing. 36 We notice that the
   local parsability property of operator-precedence grammars (exploited in Sect. 4.8.1
   for parallel parsing) is also valuable for obtaining incremental parsers.

   References
   1. Harrison M (1978) Introduction to formal language theory. Addison Wesley, Reading
   2. Hopcroft J, Ullman J (1969) Formal languages and their relation to automata. Addison-Wesley,
   Reading
   35 Some valuable methods for the incremental generation of parsers are in [46].
   36 The main ideas and algorithms for incremental parsing can be found in [47,48].

<a id="P380"></a>

   3. Hopcroft J, Ullman J (1979) Introduction to automata theory, languages, and computation.
   Addison-Wesley, Reading
   4. Salomaa A (1973) Formal languages. Academic, New York
   5. Salomaa A (1996) A direct complement construction for LR(1) grammars. Acta Inf 33(8):781–
   797
   6. SenizerguesG(2002)L(A)=L(B)?Asimplifieddecidabilityproof.TheorComputSci281:555–
   608
   7. Floyd RW, Beigel R (1994) The language of machines: an introduction to computability and
   formal languages. Computer Science Press, New York
   8. Simovici D, Tenney R (1999) Theory of formal languages with applications. World Scientific,
   Singapore
   9. Alur R, Madhusudan P (2004) Visibly pushdown languages, STOC ’04. ACM, New York. pp
   202–211
   10. OkhotinA,SalomaaK(2014)Complexityofinput-drivenpushdown automata.SIGACTNews
   45(2):47–67
   11. Okhotin A, Salomaa K (2009) Adding nesting structure to words. J ACM 56(3):16:1–16:43
   12. Baier C, Katoen JP (2008) Principles of model checking. MIT Press, Cambridge
   13. Clarke EM Jr, Grumberg O, Peled DA (1999) Model checking. MIT Press, Cambridge
   14. Berstel J, Boasson L (2002) Formal properties of XML grammars and languages. Acta Inf
   38:115–125
   15. Murata M, Lee D, Mani M, Kawaguchi K (2005) Taxonomy of XML schema languages using
   formal language theory. ACM TIT 5(4):660–704
   16. Knuth DE (1965) On the translation of languages from left to right. Inf Control 8:607–639
   17. AhoA,LamM,SethiR,UllmanJ(2006)Compilers:principles,techniquesandtools.Prentice-
   Hall, Englewoof Cliffs
   18. Grune D, Jacobs C (2009) Parsing techniques: a practical guide, 2nd edn. Springer, London
   19. Borsotti A, Breveglieri L, Crespi Reghizzi S, Morzenti A (2017) Fast deterministic parsers for
   transition networks. Acta Inf 1–28
   20. Heilbrunner S (1979) On the definition of ELR(k) and ELL(k) grammars. Acta Inf 11:169–176
   21. Chapman NP (1987) LR parsing: theory and practice. Cambridge University Press, Cambridge
   22. Rosenkrantz DJ, Stearns RE (1970) Properties of deterministic top-down parsing. ic 17(3):
   226–256
   23. Rosenkrantz DJ, Stearns RE (1971) Top-down syntax analysis. Acta Inf 1:79–110
   24. Wirth N (1975) Algorithms + data structures = programs. Prentice-Hall, Englewood Cliffs
   25. Lewi J, De Vlaminck K, Huens J, Huybrechts M (1979) A programming methodology in
   compiler construction, I and II. North-Holland, Amsterdam
   26. Breveglieri L, Crespi Reghizzi S, Morzenti A (2013) Parsing methods streamline, pp 1–64.
   CoRR abs/ arXiv:1309.7584
   27. Beatty JC (1982) On the relationship between the LL(1) and LR(1) grammars. J ACM
   29(4):1007–1022
   28. QuongR,ParrT(1995)ANTLR:Apredicated-LL(k)parsergenerator.SoftwPractExp25:789–
   810
   29. Floyd RW (1963) Syntactic analysis and operator precedence. J ACM 10(3):316–333
   30. CrespiReghizziS,MandrioliD,MartinDF(1978)Algebraicpropertiesofoperatorprecedence
   languages. Inf Control 37:(2):115–133
   31. Barenghi A, Crespi Reghizzi S, Mandrioli D, Panella F, Pradella M (2015) Parallel parsing
   made practical. Sci Comput Prog 112:195–226
   32. CrespiReghizziS,MandrioliD(2012)Operatorprecedenceandthevisiblypushdownproperty.
   J Comput Syst Sci 78(6):1837–1867
   33. SippuS,Soisalon-SoininenE(1988)Parsingtheory,volume1:languagesandparsing.Springer,
   Berlin
   References 381
   34. Sippu S, Soisalon-Soininen E (1990) Parsing theory, volume 2: LR(k) and LL(k). Springer,
   Berlin
   35. Aho A, Ullman J (1972) The theory of parsing, translation, and compiling, volume 1: parsing.
   Prentice-Hall, Englewoof Cliffs
   36. Fischer MJ (1969) Some properties of precedence languages. In: Fischer PC, Ginsburg S,
   Harrison MA (eds) Proceedings of the 1st ACM symposium on theory of computing, pp 181–
   190
   37. Beatty JC (1980) Two iteration theorems for the LL(k) languages. Theor Comput Sci 12:193–
   228
   38. Tomita M (1986) Efficient parsing for natural language: a fast algorithm for practical systems.
   Kluwer, Boston
   39. Earley J (1970) An efficient context-free parsing algorithm. Commun ACM 13(2):94–102
   40. Scott E (2008) SPPF-style parsing from Earley recognizers. Electr Notes Theor Comput Sci
   203(2):53–67
   41. Aycock J, Borsotti A (2009) Early action in an Earley parser. Acta Inf 46(8):549–559
   42. Aycock J, Horspool R (2002) Practical Earley parsing. Comput J 45(6):620–630
   43. Révész G (1991) Introduction to formal languages. Dover, New York
   44. Dain JA (1994) A practical minimum distance method for syntax error handling. Comput Lang
   20(4):239–252
   45. Burke MG, Fisher GA (1987) A practical method for LR and LL syntactic error diagnosis.
   TOPLAS 9(2):164–197
   46. Heering J, Klint P, Rekers J (1990) Incremental generation of parsers. IEEE TSE 16(12):1344–
   1351
   47. Ghezzi C, Mandrioli D (1979) Incremental parsing. ACM TOPLAS 1(1):58–70
   48. Larchevêque J (1995) Optimal incremental parsing. ACM TOPLAS 17(1):1–15

<a id="P383"></a>

Chapter 5 Translation Semantics and Static Analysis
===================================================

5.1 Introduction
================

   Inadditiontorecognitionandparsing,mostlanguageprocessingtasksperformsome
   kind of transformation of the original sentence. For instance, a compiler translates a
   program from a high-level programming language, e.g., Java, to the machine code
   of some microprocessor. This chapter presents a progression of translation models
   and methods.

   A translation is a function or more generally a mapping from the strings of the
   source language to the strings of the target language. As for string recognition,
   two approaches are possible. The generative point of view relies on two coupled
   grammars, termed a syntactic translation scheme, to generate a pair of strings that
   correspond to each other in the translation. The other approach uses a transducer,
   which is similar to are cognizer automaton but differs from it by its capability to emit
   a target string.

   Such methods may be called purely syntactic translations. They extend and
   complete the language definition and the parsing methods of previous chapters, but we
   hasten to say they are not adequate for implementing the rather involved 
   translations required for compilers. What is missing in the purely syntactic methods is the
   concern for the meaning or semantics of the language. For that, we shall present
   the attribute grammar model, which is a valuable software engineering approach for
   designingwell-structuredtranslatorsbytakingadvantageofthesyntacticmodularity
   of grammar rules.

   We attempt to clarify the distinction between the syntax and semantics of a
   language. The etymology of the two terms says rather vaguely that the former has to
   do with the structure and the latter with the meaning of the message or phrase to
   be communicated. In linguistics, the two terms have often been taken as emblems
   pointing to forms and contents, respectively; yet this reference to the studies in the
   human sciences does not make the distinction any more precise or formal.

   © Springer Nature Switzerland AG 2019
   S. Crespi Reghizzi et al., Formal Languages and Compilation,
   Texts in Computer Science, https://doi.org/10.1007/978-3-030-04879-2_5

<a id="P384"></a> 

   In the case of computer languages, there is a sort of consensus on a demarcation
   line between syntactic and semantic methods. The first difference comes from the
   domains of the entities and operations that are permitted by syntax versus semantics.
   Syntax uses the concepts and operations of formal language theory and represents
   algorithmsasautomata.Theentitiesarealphabets,stringsandsyntaxtrees;theoper-
   ations are concatenation, morphisms on characters and strings, and tree construction
   primitives. On the negative side, the concepts of number and arithmetic operation
   (sum, product, etc.) are extraneous to syntax. On the other hand, in semantics the
   entities are not limited as in syntax: also numbers, and any type of data structures
   available to programmers (such as tables or linked lists), may be defined and used as
   needed by semantic algorithms. These can take advantage of the syntactic structure
   as a skeleton for orderly processing the data associated with language components.
   The second difference is the higher computational complexity of the semantic
   algorithms with respect to the syntactic ones. We recall that the formal languages
   of concern for compilers belong almost exclusively to the regular and deterministic
   context-freefamilies.Stringrecognition,parsingandsyntactictranslationaretypical
   syntactic tasks that can be performed in a linear time, i.e., in a time proportional to
   the length of the source text. But such very efficient algorithms fall short of all the
   controls required to check program correctness with respect to the language reference
   manual. For instance, with a parser it is impossible to check that an object used in
   a Java expression has been consistently defined in a declaration. As a matter of
   fact, such a control cannot be done in linear time. This and similar operations are
   performed by a compiler subsystem usually referred to as a semantic analyzer.
   Continuingthecomparison,thedistinctionbetweensyntacticandsemanticmodels
   is imposed by pragmatic considerations. In fact, it is well known from computation
   theory that any computable function, such as deciding whether a source string is a
   valid Java program, in principle can be realized by a Turing machine. This is for sure
   a syntactic formalism, since it operates just on strings and uses the basic operations
   of formal language theory. But, in practice, a Turing machine is too complicated to
   be programmed for any realistic problem, let alone compiler design.

   Years of attempts at inventing grammars or automata that would allow some of the 
   basic semantic controls to be performed by the parser have shown that the legibility
   and convenience of syntactic methods rapidly decay, as soon as the model goes
   beyond the context-free languages and enters the domain of the context-dependent
   languages (p.105). In the state of the art, the practical syntactic methods are limited
   to the context-free domain, as we have argued on p.110.

   Chapter Outline
   The word translation signifies a correspondence between two texts that have the
   same meaning, but are written in two different languages. Many translation cases
   occur with artificial languages: the compilation of a programming language into
   machine code; the transformation of an HTML document into the PDF format used
   for portable documents; etc. The given text and language are qualified as the source,
   and the other text and language are the target. We may also include under translation

<a id="P385"></a>

   thecaseoftwotextsinthesamelanguage:anexampleistheprogramtransformation
   from, say, Java code to Java code, performed by certain software engineering tools.
   Inourpresentation,thefirstandmostabstractdefinitionoftranslationtobeconsid-
   eredwillbeamappingbetweentwoformallanguages.Thesecondkindoftranslation
   isthatobtainedbyapplyinglocaltransformationstothesourcetext,suchasreplacing
   a character with a string in accordance with a transliteration table. Then, two further,
   purely syntactic methods of defining a translation, will be presented: the translation
   grammar and the translation regular expression. Such translation models are also
   characterized by the abstract machines that compute them, respectively: the
   pushdown transducer, which can be implemented on top of a parsing algorithm; and the
   finite transducer, which is a finite automaton enriched with an output function.
   We repeat that the purely syntactic translation models fall short of the requirements
   of compilation, since they cannot express various typical text transformations which
   are necessary. Nevertheless, they are important as a conceptual foundation and a
   scaffolding for the actual methods used in compilation. Moreover, they have another
   use as methods for abstracting from the concrete syntax in order to expose similarities
   between languages.

   It is enlightening to show a structural analogy between the theories of the
   previous chapters and of the present one. At the level of set-theoretical definition the
   set of the sentences of the source language becomes the set of the matching pairs
   (source string and target string) of the translation relation. At the level of generative
   definition the language grammar becomes a translation grammar that generates pairs
   of source/target strings. Finally, at the level of operational definition the finite or
   pushdown automaton or parser that recognizes a language becomes a translator that
   computes the transformation. Such conceptual correspondences will clearly surface
   in this chapter.

   Thefifthandlastconceptualmodeltobepresentedisthesyntax-directedsemantic
   translation, a semiformal approach based on the previous models. This makes a
   convenient engineering method for designing well-structured modular translators.
   Its presentation relies on attribute grammars, which consist of a combination of
   syntax rules and semantic functions.

   Severaltypicalexamples willbepresented.Alexicalanalyzerorscannerisspeci-
   fied by the addition of simple semantic attributes and functions to a finite transducer.
   Other important examples are: type checking in assignments and expressions; 
   translation of conditional instructions into jumps; and semantics-directed parsing.
   The last part of the chapter presents another central method used to compile
   programming languages, namely static program analysis. This analysis applies to
   executable programs rather than to generic technical languages. The flowchart or
   control-flow graph of the program to be analyzed is viewed as a finite automaton.
   Static analysis detects on this automaton various properties of the program, which
   are related to its correctness, or needed to perform program optimizations. This
   final topic completes the well-balanced exposition of the elementary compilation
   methods.


<a id="P386"></a>

5.2 Translation Relation and Function
=====================================

   We introduce some notions from the mathematical theory of translations, 1 which
   suffice for the scope of the book. Let the source and target alphabets be denoted
   by Σ and Δ, respectively. A translation is a correspondence between source and
   target strings, which we formalize as a binary relation between the source and the
   target universal languages Σ ∗ and Δ ∗ ; that is, a translation relation is a subset of the
   Cartesian product Σ ∗ × Δ ∗ .

   A translation relation ρ is a set of pairs of strings (x, y ), with x ∈ Σ ∗ and
   y ∈ Δ ∗ , in formula:
   ρ = { (x, y ), ... } ⊆ Σ ∗ × Δ ∗
   We say that the target string y is the image or translation (or sometimes destination)
   of the source string x and that the two strings correspond to each other in the 
   translation. Given a translation relation ρ, the source language L 1 and target language
   L 2 are, respectively, defined as the projections of the relation on the first and second
   component, as follows:
   L 1 =
   ?
   x ∈ Σ ∗ | there exist a string y such that (x, y ) ∈ ρ
   ?
   L 2 =
   ?
   y ∈ Δ ∗ | there exist a string x such that (x, y ) ∈ ρ
   ?
   Alternatively, a translation can be formalized by taking the set of all the images of a
   source string. Then a translation is modeled as a function τ:
   τ : Σ ∗ → ℘
   ?
   Δ ∗
   ?
   τ(x) =
   ?
   y ∈ Δ ∗ | (x, y ) ∈ ρ
   ?
   where symbol ρ is a translation relation. 2 Function τ maps each source string on the
   set of the corresponding images, that is, on a language.

   Notice that the application of the translation function to every string of the source
   language L 1 produces a set of languages, and their union yields the target language
   L 2 , as follows:
   L 2 =
   ?
   x ∈ L 1
   τ (x)
   Ingeneral,atranslationfunctionispartiallydefined:forsomestringsoverthesource
   alphabet the function may be undefined. A simple expedient for making the function
   total is to posit that, where the application of function τ to string x is undefined, it is
   assigned the special value error.

   A particular but practically important case occurs when every source string has
   no more than one image, and in this case the translation function is τ : Σ ∗ → Δ ∗ .
   1 A rigorous presentation can be found in Berstel [1] and in Sakarovich [2].
   2 Remember that given a set X, symbol ℘ (X) denotes the power set of X, i.e., the set of all the
   subsets of X.


<a id="P387"></a>

   The inverse translation τ −1 is a function that maps a target string on the set of
   the corresponding source strings 3 :
   τ −1 : Δ ∗ → ℘
   ?
   Σ ∗
   ?
   τ −1 (y) =
   ?
   x ∈ Σ ∗ | y ∈ τ (x)
   ?
   Depending on the mathematical properties of the function, the following cases arise
   for a translation:
   total every source string has one or more images
   partial one or more source strings do not have any image
   single-valued no string has two distinct images
   multi-valued one or more source strings have more than one
   image
   injective distinctsourcestringshavedistinctimages,ordif-
   ferently stated, any target string corresponds to
   at most one source string; only in this case the
   inverse translation is single-valued
   surjective the image of the translation coincides with the
   range; i.e., every string over the target alphabet is
   theimageofatleastonesourcestring;onlyinthis
   case the inverse translation is total
   bijective (or simply one to one) the correspondence between the source and the
   target strings is both injective and surjective; i.e.,
   it is one to one; only in this case the inverse 
   translation is bijective as well
   To illustrate, consider a high-level source program, say in Java, and its image in
   the code of a certain machine. Clearly the translation is total because any valid
   program can be compiled into machine code and any incorrect program has the
   value error, i.e., a diagnostic, for image. Such translation is multi-valued because
   usually the same Java statement admits several different machine code realizations.
   The translation is not injective because two source programs may have the same
   machine code image: just think of two while and for loops translated to the same
   code that uses conditional and unconditional jumps. The translation is not surjective
   since some machine programs that operate on special hardware registers cannot be
   expressed by Java programs.

   On the other hand, if we consider a particular compiler from Java into machine
   code, the translation is totally defined, as before, and in addition it is single-valued,
   3 Given a translation relation ρ, the inverse translation relation ρ −1
   is obtained by swapping the
   corresponding source and target strings, i.e., ρ −1 = {( y, x ) | (x, y ) ∈ ρ }; it is equivalent to
   the inverse translation function τ −1 .


<a id="P388"></a>

   because the compiler chooses exactly one out of the many possible machine 
   implementationsofthesourceprogram.Thetranslationisnotnecessarilyinjective(forthe
   same reasons as above) and certainly it is not surjective, because a typical compiler
   does not use all the instructions of a machine.

   A decompiler reconstructs a source program from a given machine program.

   Notice that this translation is not the reverse translation of the compilation, because
   compiler and decompiler are algorithms independently designed, and they are
   unlikely to make the same design decisions for their mappings. A trivial
   example: given a machine program τ (x) produced by the compiler τ, the decompiler δ
   will output a Java program δ (τ (x)) that almost certainly differs from program x
   with respect to the presence of blank spaces!
   Sincecompilationisamappingbetweentwolanguagesthatarenotfinite,itcannot
   bespecifiedbytheexhaustiveenumerationofthecorrespondingpairsofsource/target
   strings. The chapter continues with a gradual presentation of the methods to specify
   and implement such infinite translations.

5.3 Transliteration
===================

   A naif way to transform a text is to apply a local mapping in each position of the
   source string. The simplest transformation is the transliteration or alphabetic
   homomorphism, introduced in Chap.2 on p.97. Each source character is transliterated to
   a target character or more generally to a string.

   Let us read Example2.89 on p.97 anew. The translation defined by an alphabetic
   homomorphism is clearly single-valued, whereas the inverse translation may or may
   not be single-valued. In that example the little square ? is the image of any Greek
   letter; hence the inverse translation is multi-valued:
   h −1 (?) = { α, ..., ω }
   If the homomorphism erases a letter, i.e., maps the letter to the empty string, as it
   happens with characters start-text and end-text, the inverse translation is 
   multivalued because any string made of erasable characters can be inserted in any text
   position.

   If the inverse function too is single-valued, the source/target mapping is a
   oneto-one or bijective function, and it is possible to reconstruct the source string from
   a given target string. This situation occurs when encryption is applied to a text. A
   historical example defined by transliteration is Julius Caesar’s encryption method,
   which replaces each letter of the Latin alphabet, having position i, 1 ≤ i ≤ 26, in
   the lexicographic ordering, by the letter in the position (i + k) mod 26 (i.e., at
   alphabetic distance k), where constant k, 1 ≤ k ≤ 25, is the ciphering secret key.

<a id="P389"></a>

   Tofinish,westressthattransliterationtransformsaletterintoanotheronewithout
   any regard to the occurrence context. It goes without saying that such a process falls
   short of the needs of compilation.

5.4 Purely SyntacticTranslation
===============================

   When the source language is defined by a grammar, a typical situation for 
   programming languages, it is natural to consider a translation model where each syntactic
   component, i.e., a subtree, is individually mapped on a target subtree. The latter are
   then assembled into the target syntax tree that represents the translation. Such 
   structural translation is now formalized as a mapping scheme, by relating the source and
   target grammar rules.

   Definition 5.1 (translation grammar) A translation grammar G τ = (V, Σ, Δ,
   P, S ) is a context-free grammar that has as terminal alphabet a set C ⊆ Σ ∗ × Δ ∗
   of pairs (u, v ) of source/target strings, also written as a fraction
   u
   v .

   The translation relation ρ G defined by grammar G τ is the following:
   ρ G τ = { (x, y ) | ∃z ∈ L (G τ ) ∧ x = h Σ (z) ∧ y = h Δ (z) }
   where h Σ : C → Σ and h Δ : C → Δaretheprojectionsfromthegrammarterminal
   alphabet to the source and target alphabets, respectively. ?
   Intuitively a pair of corresponding source/target strings is obtained by taking a
   sentence z generatedby G τ andbyprojectingitonthetwoalphabets.Suchatranslation
   is termed context-free or algebraic. 4
   The syntactic translation scheme associated with the translation grammaris the set
   of pairs of source and target syntax rules, obtained by, respectively, canceling from
   the rules of G τ the characters of the target alphabet or of the source alphabet. The set
   of source/target rules comprises the source grammar G 1 and the target grammar G 2
   of the translation scheme. A translation grammar and a translation scheme are just
   notational variations of the same conceptual model. Example5.2 shows a translation
   grammar and the equivalent translation scheme.

   Example 5.2 (translation grammar for string reversal) Given string a a b, its 
   translation is the mirror string ba a. The translation grammar G τ for string reversal is as
   follows:
   G τ : S →
   a
   ε
   S
   ε
   a
   |
   b
   ε
   S
   ε
   b
   |
   ε
   ε
   4 Another historical name for such translations is simple syntax-directed translations.

<a id="P390"></a>

   Equivalently, the translation grammar can be replaced by the following translation
   scheme (G 1 , G 2 ):
   source grammar G 1 target grammar G 2
   S → a S S → S a
   S → b S S → S b
   S → ε S → ε
   The two columns list the source and target grammars, and each row contains two
   corresponding rules. For instance, the second row is obtained by the source/target
   projections h Σ and h Δ (see Definition5.1), as follows:
   h Σ
   ?
   S →
   b
   ε
   S
   ε
   b
   ?
   = S → b S h Δ
   ?
   S →
   b
   ε
   S
   ε
   b
   ?
   = S → S b
   To obtain a pair of strings that correspond in the translation relation, we construct a
   derivation, like:
   S ⇒
   a
   ε
   S
   ε
   a
   ⇒
   a
   ε
   a
   ε
   S
   ε
   a
   ε
   a
   ⇒
   a
   ε
   a
   ε
   b
   ε
   S
   ε
   b
   ε
   a
   ε
   a
   ⇒
   a
   ε
   a
   ε
   b
   ε
   ε
   ε
   ε
   b
   ε
   a
   ε
   a
   = z
   and then we project the sentence z of L (G τ ) on the two alphabets, obtaining:
   h Σ (z) = a a b h Δ (z) = ba a
   Differently, using the translation scheme, we generate a source string by a derivation
   of G 1 and its image by a derivation of G 2 , where we pay attention to use two
   corresponding rules at each step. ?
   The reader may have noticed that the preceding translation grammar G τ of
   Example5.2 is almost identical to the familiar grammar of even-length palindromes. By
   marking with a prime the characters in the second half of a string, the palindrome
   grammar becomes the following grammar:
   G p : S → a S a ? | b S b ? | ε (5.1)
   Recoding strings
   a
   ε
   as a,
   b
   ε
   as b,
   ε
   a
   as a ? and
   ε
   b
   as b ? , the two grammars G τ and G p
   coincide. This remark leads to the next property.

   Property 5.3 (context-free language and translation) The following conditions are
   equivalent:
   1. The translation relation ρ G τ ⊆ Σ ∗ × Δ ∗ is defined by a translation grammar
   G τ .


<a id="P391"></a>

   2. There exists an alphabet Ω, a context-free language L over alphabet Ω and two
   alphabetichomomorphisms(transliterations)h 1 : Ω → Σ ∪ { ε }andh 2 : Ω →
   Δ ∪ { ε }, such that:
   ρ G τ = { (h 1 (z), h 2 (z)) | z ∈ L } ?
   Notice that the two homomorphisms may also erase a character of alphabet Ω. We
   reuse the translation from a string to its reversal, to illustrate the second statement
   of Property5.3.

   Example 5.4 (string reversal, continued from Example5.2) From part 2 of
   Property5.3, to express the translation ρ G τ of Example5.2 we use the alphabet Ω =
   ?
   a, b, a ? , b ?
   ?
   and the following context-free language L
   L =
   ?
   u (u R ) ? | u ∈ (a | b) ∗
   ? = ?
   ε, a a ? , ..., a bbb ? b ? a ? , ...

   ?
   where string (v) ? is the primed copy of string v. Notice that language L is generated
   bythegrammaratline(5.1)above.Thehomomorphismsh 1 andh 2 arethefollowing:
   Σ h 1 h 2
   a a ε
   b b ε
   a ? ε a
   b ? ε b
   Then, the string a bb ? a ? ∈ L is transliterated to the two strings below:
   ?
   h 1
   ? a bb ?
   a ? ? , h 2
   ? a bb ?
   a ? ?? = (a b, ba )
   that belong to the translation relation ρ G τ . ?

## 5.4.1 Infix and Polish Notation

   Arelevantapplicationofcontext-freetranslationistoconvertbackandforthbetween
   various representations of arithmetic (or logical) expressions, which differ by the
   relative positions of their operands and signs, and by the use of parentheses or other
   delimiters.

   We define the degree of an operator as the number of arguments or operands it
   may have. The degree can be fixed or variable, and in the latter case the operator
   is called variadic. Operators of degree two are very common and are called binary
   operators. For instance, the comparisons, such as equality “=” and difference “?=”,
   are binary operators. Arithmetic addition has a degree ≥ 2, but in a typical machine

<a id="P392"></a>

   language the add instruction is just binary, since it adds two registers. Moreover,
   sinceadditionisusuallyassumedtosatisfytheassociativeproperty,amany-operand
   addition can be decomposed into a series of binary additions to be performed, say,
   from left to right.

   Arithmeticsubtractionprovidesanexampleofnon-commutativebinaryoperation,
   whereas a change of sign (as in −x) is an example of unary operation. If the same
   symbol “−” is used to denote both operations, the operator becomes variadic with
   degree one or two.

   Examining now the relative positions of operators and operands, we have the
   following cases. An operator is prefix if it precedes its arguments, and it is postfix if
   it follows them. A binary operator is infix if it is placed between its arguments.
   One may also generalize the property of being infix, to operators with higher
   degree. An operator of degree n ≥ 2 is mixfix if its representation can be segmented
   into n + 1 parts, as follows:
   o 0 arg 1 o 1 arg 2 ... o n−1 arg n o n
   thatis,iftheargumentliststartswithanopeningmarko 0 ,thenitisfollowedbyn − 1
   (possibly different) separators o i and terminates with a closing mark o n . Sometimes
   the opening and closing marks are missing.

   Forinstance,theconditionaloperatorif ofmanyprogramminglanguagesismixfix
   with degree two or three if the else clause is present:
   if arg 1 then arg 2
   ?
   else arg 3
   ?
   Because of the varying degree, this representation is ambiguous if the second
   argument can be in turn a conditional operator (as seen on p.60). To remove ambiguity
   (see also Sect.2.5.11.6 on p.60), in certain languages the conditional construct is
   terminated by a closing mark end_if, e.g., in ADA.

   In machine language the binary conditional operator is usually represented in
   prefix form by an instruction such as:
   jump_if_false arg 1 , arg 2
   Actually, in machine language operations, every machine instruction is of the prefix
   type as it begins with an operation code. The degree is fixed for each code, and it
   typically ranges from zero, e.g., in a nop instruction, to three, e.g., in a three-register
   addition instruction add r1,r2,r3.

   A representation is called polish 5 if it does not use parentheses and if all the
   operators are either prefix or postfix (not mixed). The elementary grammar of polish
   expressions is printed on p.56.

   5 From the nationality of the logician Jan Lukasiewicz, who proposed its use for compacting and
   normalizing logical formulas.


<a id="P393"></a>

   Weillustrateafrequenttransformationperformedbycompilerstoeliminateparen-
   theses,by converting anarithmetic expression frominfix to polishnotation. For
   simplicity, we prefer to use disjoint source/target alphabets, in order to avoid the need
   of fractions in the rules of the translation grammar.

   Example 5.5 ( from infix to prefix operators) The source language comprises 
   arithmeticexpressionswith(infix)additionandmultiplication,parenthesesandthetermi-
   nali thatdenotesavariableidentifier.Thetranslationistopolishprefix:operatorsare
   moved to the prefix position, parentheses disappear, and identifiers are transcribed
   to i ? .

   Source and target alphabets:
   Σ = { +, ×, ‘(’, ‘)’, i } Δ =
   ?
   add, mult, i ?
   ?
   Translation grammar (axiom E):
   G τ
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   E → add T + E | T
   T → mult F × T | F
   F → ‘(’ E ‘)’ | i i ?
   Notice that rule E → add T + E abridges rule E →
   ε
   add
   T
   +
   ε
   E, at no danger of
   confusion because the source/target alphabets are disjoint.

   The equivalent translation scheme follows:
   source grammar G 1 target grammar G 2
   E → T + E E → add T E
   E → T E → T
   T → F × T T → mult F T
   T → F T → F
   F → ‘(’ E ‘)’ F → E
   F → i F → i ?
   (5.2)
   An example of translation is shown in the combined source/target syntax tree in
   Fig.5.1. Imagine to erase the dashed part of the tree: then the source syntax tree of
   expression (i + i) × i shows up, as the parser would construct it by using the source
   grammar G 1 . Conversely, erasing from the tree the leaves of the source alphabet and
   their edges, we would see the tree generated by the target grammar for the image
   string mult add i ? i ? i ? . ?
   Construction of Target Syntax Tree
   We observe that in a translation scheme, such as (5.2) above, derived from a 
   translation grammar according to Definition5.1, the following conditions are always met:

<a id="P394"></a>

   E
   T
   mult F
   ( E
   add T
   F
   i i
   + E
   T
   F
   i i
   )
   × T
   F
   i i
   Fig.5.1 Source/target tree generated by the translation grammar of Example5.5
   1. The rules of the two grammars are in one-to-one correspondence.

   2. Anytwocorrespondingruleshaveidenticalleftparts,andtheirnonterminalsym-
   bols occur in the same order in their right parts.

   Given a translation grammar, we explain how to compute the image of a source
   sentence x. First, we parse the string using source grammar G 1 and we construct the
   source syntax tree t x of x, which is unique if the sentence is unambiguous. Then, we
   visit tree t x , say, in left-to-right pre-order.

   At each visited node, if the current node of tree t x is the left part symbol of a
   certain rule of grammar G 1 , then we apply the corresponding rule of target grammar
   G 2 and thus we append some children nodes to the target tree. At the end of visit,
   the target tree is complete.

   Abstract Syntax Tree
   Syntactic translations are a convenient method for trimming and transforming source 
   syntax trees, in order to remove the elements that are irrelevant to the later stages of
   compilation,andtoreformatthetreeasneeded.Thistransformationisaninstanceof
   language abstraction (p.26). A case has already been considered: the elimination of
   parentheses from arithmetic expressions. One can easily imagine other cases, such
   as the elimination or recoding of separators between the elements of a list, or of the
   mixfix keywords of the conditional instructions like if …then …else …end_if. The
   result of such transformations is often called an abstract syntax tree.


<a id="P395"></a>


## 5.4.2 Ambiguity of Source Grammar andTranslation

   We have already observed that most applications are concerned with single-valued
   translations. However, if the source grammar is ambiguous, a sentence admits two
   different syntax trees, each one corresponding to a target syntax tree. Therefore the
   sentence will have two images, generally different.

   The next example (Example5.6) shows the case of a translation that is 
   multivalued because of the ambiguity of its source component.

   Example 5.6 (redundant parentheses) A case of multi-valued translation is the
   conversion from prefix polish to infix notation, the latter with parentheses. It is 
   immediate to write the translation scheme, as it is the inverse translation of Example5.5 on
   p.393. Therefore it suffices to interchange source and target grammars, to obtain:
   source grammar G 1 target grammar G 2
   E → add T E E → T + E
   E → T E → T
   T → mult F T T → F × T
   T → F T → F
   F → E F → ‘(’ E ‘)’
   F → i ? F → i
   Here the source grammar G 1 has an ambiguity of unbounded degree, which comes
   from the following circular derivation (made of copy rules):
   E ⇒ T ⇒ F ⇒ E
   For instance, the following derivations of source string i ? are possible:
   E ⇒ T ⇒ F ⇒ i ? E ⇒ T ⇒ F ⇒ E ⇒ T ⇒ F ⇒ i ? ...

   and each of them has a distinct image:
   E ⇒ T ⇒ F ⇒ i E ⇒ T ⇒ F ⇒ ( E ) ⇒ ( T ) ⇒ ( F ) ⇒ ( i ) ...

   The fact is scarcely surprising since in the conversion from prefix to infix, one can
   insert as many pairs of parentheses as he wishes, but the translation scheme does not
   prescribe their number and allows the insertion of redundant parentheses. ?
   On the other hand, suppose now that the source grammar is unambiguous, so that
   each sentence has only one syntax tree. Yet, it may happen that the translation is

<a id="P396"></a>

   multi-valued, if in the translation scheme different target rules correspond to the
   same source rule. An example is the next translation grammar:
   S →
   a
   b
   S S →
   a
   c
   S S →
   a
   d
   where the source grammar S → a S | a is unambiguous, yet the translation τ (a a)
   = { bd, cd } is not single-valued, because the first two rules of the translation
   grammar correspond to the same source rule S → a S.

   The next statement (Property5.7) gives sufficient conditions for avoiding
   ambiguity in a translation grammar.

   Property 5.7 (unambiguity conditions for translation) Let G τ = (G 1 , G 2 ) be a
   translation grammar such that:
   1. The source grammar G 1 is unambiguous.

   2. No two rules of the target grammar G 2 correspond to the same rule of G 1 .
   Then the translation specified by grammar G τ is single-valued and defines a 
   translation function. ?
   In the previous discussion on translation ambiguity, we have considered the
   ambiguity of the source and target grammars separately, but not the ambiguity of the
   translation grammar G τ itself, because it is not relevant to ensure the single-valued
   translation property. The next example (Example5.8) shows that a translation
   grammar, though unambiguous, may have an ambiguous source grammar which causes
   the translation to be multi-valued.

   Example 5.8 (end-mark in conditional instruction) The translation mandatorily
   places an end-mark end_if after the conditional instruction if … then … ? else
   ? .

   The translation grammar below:
   S →
   if
   if
   c
   c
   then
   then
   S
   ε
   end_if
   |
   if
   if
   c
   c
   then
   then
   S
   else
   else
   S
   ε
   end_if
   | a
   is unambiguous. Yet, the underlying source grammar, which defines the conditional
   instruction without end-mark, is a typical case of ambiguity (p.60). The translation
   produces two images of the same source string, as follows:
   if c then if c then a
   end_if
   ↓ else a
   end_if
   ↓ ↑
   end_if end_if
   which are obtained by inserting the end-marks in the positions indicated by the
   vertical darts either over or under the line. ?

<a id="P397"></a>

   Compiler designers ought to pay attention to avoid translation grammars that make
   the translation multi-valued. Parser construction tools help because they routinely
   check the source grammar for determinism, which excludes ambiguity.


## 5.4.3 Translation Grammar and PushdownTransducer

   Weshiftfocusfromtranslationgrammarstotheabstractmachines,calledtransducers
   or input–output automata, that compute such translations. Much as the recognizer of
   a context-free language, the transducer implementing a translation grammar needs a
   pushdown or LIFO store.

   Apushdowntransducer also knownasIO-automaton islikeapushdown 
   automaton (defined on p.205), enriched with the capability to output zero or more 
   characters at each move. More precisely, eight items have to be specified to define such a
   machine:
   Q set of states
   Σ source alphabet
   Γ pushdown stack alphabet
   Δ target alphabet
   δ state transition and output function
   q 0 ∈ Q initial state
   Z 0 ∈ Γ initial symbol on the stack
   F ⊆ Q set of final states
   The function δ is defined in the domain Q × (Σ ∪ { ε }) × Γ and has the set
   Q × Γ
   ∗
   × Δ ∗ as range. 6 The meaning of the function is the following: if the current
   state, input character and stack top, respectively, are q ? , a and Z, and if it holds
   δ (q ? , a, Z) =
   ?
   q ?? , γ, y
   ? , then the machine reads character a from the input and
   symbol Z from the stack top, enters the next state q ?? and writes sequence γ on top
   of the stack and character y to the output. If the automaton recognizes the source
   string by empty stack, then the set of final states coincides with Q. The meaning of
   function δ is schematized in Fig.5.2. The subjacent automaton of the translator is
   obtained by erasing the target alphabet symbols and the output string actions from
   the definitions.

   The formalization of the translation computed by a pushdown translator follows
   the same lines as the definition of acceptance by the subjacent pushdown automaton,
   6 Alternatively, it is possible to specify the output by a separate output function, as we did for the
   BSP finite-state transducer in Definition3.54 on p.178. We skip the formalization of the case of
   nondeterministic transducer, just saying that the range of function δ would become the powerset ℘
   of the preceding Cartesian product.


<a id="P398"></a>

   Fig.5.2 Scheme of the
   move of a pushdown
   transducer
   δ q , a, Z = q , γ, y
   next state
   string written on stack
   string written to output
   current state
   current char.

   stack top
   as stated in Sect.4.2.2 on p.205. The instantaneous configuration of the pushdown
   transducerisdefinedasa4-tuple(q, y, η, z ) ∈ ( Q × Σ ∗ × Γ
   ∗
   × Δ ∗ ),including:
   q current state
   y remaining portion (suffix) of the source string x to be read
   η stack contents
   z string written to the output tape, up to the current configuration
   When a move is executed, a transition from a configuration to the next one occurs,
   denotedas(q, x, η, w) ?→ ( p, y, λ, z ).Acomputationisachainofzeroormore
   transitions, denoted by
   ∗
   ?− →. Depending on the kind of the performed move (reading
   or spontaneous), the following two kinds of transition are possible.

   current conf. next conf. applied move
   (q, a x, η Z, z ) ( p, x, η γ, z y )
   reading move
   δ (q, a, Z) = ( p, γ, y )
   (q, a x, η Z, z ) ( p, a x, η γ, z y )
   spontaneous move
   δ (q, ε, Z) = ( p, γ, y )

<a id="P399"></a>

   Initial configuration and string acceptance are defined as in Sect.4.2.2, and the
   computed translation τ is defined as follows (notice that acceptance by final state is
   assumed):
   τ (x) = z ⇐⇒ (q 0 , x, Z 0 , ε)
   ∗
   ?− → (q, ε, λ, z ) with q ∈ F and λ ∈ Γ
   ∗

### 5.4.3.1 FromTranslation Grammar to PushdownTransducer

   Translation schemes and pushdown transducers are two ways of representing
   language transformations: the former is a generative model suitable for specification,
   and the latter is procedural and helps in compilation. Their equivalence is stated in
   the next property.

   Property 5.9 (equivalence of translation grammar and pushdown transducer) A
   translation relation is defined by a translation grammar or scheme if, and only
   if, it is computed by a (nondeterministic) pushdown transducer. ?
   Forbrevity,weonlydescribetheconversionfromagrammartoatransducer,because
   the other direction is less pertinent to compilation.

   Consider a translation grammar G τ . A first way of deriving the equivalent
   pushdowntranslator T istoapplyessentiallythesamealgorithmthatweusedtoconstruct
   the pushdown recognizer of language L (G τ ) (Algorithm4.1 on p.202). Then, this
   machine is transformed into a transducer through a small change of the operations on
   the target characters. After pushing a target symbol onto the stack, when that symbol
   surfaces anew on the stack top, it is written to the output; but a target symbol is not
   matched against the current input character, unlike source characters.

   Normalization of Translation Rules
   To simplify the construction, it helps to reorganize without loss of generality the
   source and target strings that occur in a grammar rule, in such a way that the initial
   character is a source one, where possible. More precisely, we make the following
   hypotheses in the form of the source/target pairs
   u
   v
   that occur in the rules, 7 where
   u ∈ Σ ∗ and v ∈ Δ ∗ :
   1. For any pair
   u
   v
   it holds |u | ≤ 1; i.e., the source u is a single character a ∈ Σ or
   the empty string. Clearly this is not a limitation because if the pair
   a 1 a 2
   v
   occurs
   in a rule, it can be replaced by the pairs
   a 1
   v
   a 2
   ε
   without affecting the translation.

   2. No rule may contain the following substrings:
   ε
   v 1
   a
   v 2
   or
   ε
   v 1
   ε
   v 2
   where v 1 , v 2 ∈ Δ ∗
   7 For completeness, we mention that in some publications on formal languages, the source/target
   fractions
   u
   v
   in the grammar rules are denoted as u { v }, e.g., X → αu { v } β instead of X →
   α
   u
   v
   β; but such a notation is not used in this book.


<a id="P400"></a>

   Should such combinations be present in a rule, they can be respectively replaced
   by the equivalent pairs
   a
   v 1 v 2
   or
   ε
   v 1 v 2 .

   We are ready to describe the correspondence between the rules of grammar G τ =
   (V, Σ, Δ, P, S ) and the moves of the so-called predictive transducers.

   Algorithm 5.10 (construction ofthe (nondeterministic) predictive pushdown 
   transducer) Let C be the set of the pairs of type
   ε
   v
   with v ∈ Δ + , and of type
   b
   w
   with
   b ∈ Σ and w ∈ Δ ∗ , occurring in some grammar rule. The rules for constructing the
   transducer moves are in Table5.1. Rows 1, 2, 3, 4 and 5 apply when the stack top
   is a nonterminal symbol. In case 2 the right part begins with a source terminal and
   the move is conditioned by its presence in the input. Rows 1, 3, 4 and 5 give rise to
   spontaneous moves, which do not shift the reading head. Rows 6 and 7 apply when
   a pair surfaces on top of stack. If the pair contains a source character (row 7), it must
   coincide with the current input character; if it contains a target string (rows 6 and
   7), the latter is output. Initially, the stack contains the axiom S, and the reading head
   is positioned on the first character of the source string. At each step, the automaton
   (nondeterministically) chooses an applicable rule and executes the corresponding
   move. Finally, row 8 accepts the string if the stack is empty and the current character
   marks the end of text. ?
   Notice the automaton does not make use of states; i.e., the stack is the only memory
   used. As we did for recognizers, we will later enrich the machine with states in order
   to obtain a more efficient deterministic algorithm.

   The next example (Example5.11) shows a simple translation computed by a
   nondeterministic pushdown transducer.

   Example 5.11 (nondeterministic pushdown transducer) Consider the source
   language L below:
   L =
   ?
   a ∗ a m b m | m > 0
   ?
   and define the following translation τ on L:
   τ (a k a m b m ) = d m c k where k ≥ 0 and m > 0
   The translation τ first changes letter b to letter d, and then it transcribes to letter c
   any letter a that exceeds the number of b’s. The moves of the transducer are listed
   in Table5.2 next to the corresponding rules of the translation grammar. The choice
   of moves 1 or 2 in Table5.2 is not deterministic, and so is the choice of moves 3
   or 4. Move 5 outputs a target character that had been pushed by move 1. Move 6
   just scans an input character, and move 7 is for acceptance. The subjacent pushdown
   automaton is not deterministic. ?
   The following example (Example5.12) shows that not all context-free translations
   can be computed by a pushdown transducer of the deterministic kind. A similar
   property holding for regular translations and finite transducers will be illustrated in
   Sect.5.5.2.


<a id="P401"></a>

   Table5.1 Correspondence between translation grammar rules and pushdown translator moves (if
   n = 0, then the sequence A 1 ... A n is absent)
      # Rule
      Move Comment
      1
      A →
      ε
      v
      B A 1 ... A n
      n ≥ 0
      v ∈ Δ + B ∈ V
      A i ∈ (C ∪ V )
   If top = A then write ( v ); pop; push
   ( A n ... A 1 B )
   Emit the target string v and push
   on stack the prediction string
   B A 1 ... A n
   2
   A →
   b
   w
   A 1 ... A n
   n ≥ 0
   b ∈ Σ w ∈ Δ ∗
   A i ∈ (C ∪ V )
   If cc = b ∧ top = A then write ( w );
   pop; push ( A n ... A 1 ); advance the
   reading head
   Char b was the next expected and has
   been read; emit the target string w ;
   push the prediction string A 1 ... A n
   3
   A → B A 1 ... A n
   n ≥ 0 B ∈ V
   A i ∈ (C ∪ V )
   If top = A then pop; push
   ( A n ... A 1 B )
   Push the prediction string
   B A 1 ... A n
   4
   A →
   ε
   v
   v ∈ Δ + If top = A then write ( v ); pop Emit the target string v
   5 A → ε
   If top = A then pop
   6
   For every pair
   ε
   v
   ∈ C
   If top =
   ε
   v
   then write ( v ); pop
   The past prediction
   ε
   v
   is now
   completed by writing v
   7
   For every pair
   b
   w
   ∈ C
   If cc = b ∧ top =
   b
   w
   then write ( w );
   pop; advance the reading head
   The past prediction
   b
   w
   is now
   completed by reading b and writing w
   8 - - - - - - - -
   If cc =? ∧ stack is empty then accept;
   halt
   The source string has been entirely
   scanned and no goal is present in the
   stack
   Example 5.12 (context-free nondeterministic translation) The translation function
   τ below:
   τ (u) = u R u where u ∈ { a, b } ∗
   maps every string into a reversed copy followed by the same string. Function τ is
   easily specified by the following scheme:
   translation gram. G τ source gram. G 1 target gram. G 2
   S →
   ε
   a
   S
   a
   a
   S → S a S → a S a
   S →
   ε
   b
   S
   b
   b
   S → S b S → b S b
   S →
   ε
   ε
   S → ε S → ε

<a id="P402"></a>

   Table 5.2 Moves of a nondeterministic pushdown transducer
      # Rule Move
      1 S →
      a
      ε
      S
      ε
      c
   If cc = a ∧ top = S, then pop; push ( ε
   c
   S); advance the reading head
   2 S → A If top = S, then pop; push (A)
   3 A →
   a
   d
   A
   b
   ε
   If cc = a ∧ top = A, then pop; write (d); push ( b
   ε
   A); advance the
   reading head
   4 A →
   a
   d
   b
   ε
   If cc = a ∧ top = A, then pop; write (d); push ( b
   ε ); advance the reading
   head
   5 - - - If top =
   ε
   c , then pop; write (c)
   6 - - - If cc = b ∧ top =
   b
   ε , then pop; advance the reading head
   7 - - - If cc =? ∧ stack is empty, then accept; halt
   The translation cannot be deterministically computed by a pushdown machine. The
   reason is that such a machine should output the mirror copy of the input before the
   direct copy of the input. 8 The only way to reverse a string by means of a pushdown
   device is to store the string in the stack and then to pop and output each character.
   But popping destroys the stored string and makes it unavailable for later copying to
   the output. ?
   Nondeterministicalgorithms,suchasinExample5.11,areusedinfrequentlyincom-
   pilation. In the next section we develop translator construction methods suitable for
   use in combination with the widespread deterministic parsers.


## 5.4.4 Syntax Analysis with OnlineTranslation

   Givenacontext-freetranslationscheme,theconstructionofAlgorithm5.10produces
   a pushdown transducer that is often nondeterministic and generally unsuitable for use
   inacompiler.Toconstructanefficientandwell-engineeredtranslator,itisconvenient
   to resume from the point reached in Chap.4 with the construction of deterministic
   parsers, and to enrich them with output actions. Given a context-free translation
   grammar or scheme, we make the assumption that the source grammar is suitable
   for deterministic parsing. In this case, to compute the image of the source string, the
   parser emits the translation piece by piece, as it completes the parsing of a syntactic
   subtree.

   8 For a formal proof see [3,4].


<a id="P403"></a>

   We know that bottom-up and top-down parsers differ in the construction order
   of the syntax tree. A question to be clarified is how the order interferes with the
   possibility of correctly producing the translation. We anticipate the main result: the
   top-downparsingorderisfullycompatiblewiththetranslationsdefinedbytranslation
   grammars, whereas the bottom-up order places some restrictions on the translation.
   We recall there are two techniques for building efficient top-down deterministic
   parsers:aspushdownautomataandasrecursivedescentprocedures.Bothtechniques
   are simple to extend toward the construction of translators. Our initial example is the
   translation that, for a given input string, outputs its leftmost derivation represented
   as a list of source grammar rules. The following approach works for any LL(k)
   grammar, and we illustrate it with the grammar of a Dyck language.

   Example 5.13 (leftmost derivation of Dyck language) The source grammar G 1
   (below left) has the following rules, numbered for reference:
   G 1
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   1 : S → a S a ? S
   2 : S → b S b ? S
   3 : S → ε
   G t
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   S →
   a
   1
   S a ? S
   S →
   b
   2
   S b ? S
   S →
   ε
   3
   The target alphabet is {1, 2, 3}, and each rule of the translation grammar G t (above
   right) just plugs the rule label in the first position. The translation thus computed,
   for a string, say, ba a ? b ? ∈ L (G 1 ), is 21333, which is the sequence of rules in the
   order they are applied in the leftmost derivation S
   +
   = ⇒ ba a ? b ? . Clearly, the DPDA
   that implements the top-down parser can be enriched with a write instruction that
   emits the rule label, as soon as the current rule has been selected. Compare also with
   the similar example in Chap.4 on p.311.


## 5.4.5 Top-Down DeterministicTranslationby Recursive Procedures

   We next focus on the approach based on the recursive descent translator. In order
   to streamline the design of syntactic translation algorithms for grammars extended
   with regular expressions, it is convenient to represent the source grammar G 1 by
   means of a recursive network of finite machines, as we did in Chap.4. Assuming the
   source grammar is ELL(k) with k = 1 or larger, we recall the organization of the
   recursive descent parser (p.313). For each nonterminal symbol, a procedure has the
   task of recognizing the (sub)strings derived from it. The procedure body blueprint is
   identical to the state-transition graph of the corresponding machine in the network
   that represents the grammar. For computing the translation, we simply insert a write
   transitioninthemachineandacorrespondingwriteinstructionintheprocedurebody.
   An example (Example5.14) should be enough to explain such a straightforward
   modification of a recursive descent parser.


<a id="P404"></a>

   machine net machine net with write actions
   2 T
   0 T 1 T
   3 T
   E → → { ‘)’,
   T
   + { + }
   − { − }
   T
   T
   2 T 2 T
   0 T 1 T
   3 T 3 T
   E →
   →
   T
   +
   −
   T
   ε
   write (add)
   T
   ε
   write (sub)
   Fig. 5.3 Machine M E that represents the EBNF axiomatic rule E → T
   ?
   + T | −T
   ? ∗
   of the
   source grammar in Example5.14 (left), and the same machine M E augmented with write actions
   and states where necessary (right)
   Example 5.14 (recursive descent translator from infix to postfix) The source
   language consists of arithmetic (infix) expressions with two levels of operators and
   with parentheses, and the translation converts such expressions to the postfix polish
   notation as exemplified below:
   v × (v + v) ⇒ v v v add mult
   The source language is defined by the extended BNF grammar G 1 below (axiom E):
   G 1
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   E → T
   ?
   + T | − T
   ? ∗
   T → F
   ?
   × F | ÷ F
   ? ∗
   F → v | ‘(’ E ‘)’
   Figure5.3 (left) represents the machine for nonterminal E: the machine meets the
   ELL(1) condition (p.306), and the relevant guide sets are written in braces.
   The recursive descent procedure for nonterminal E is shown in Fig.5.4 (left); we
   recall that function next returns the current input token; for simplicity, the check of
   error cases is omitted. Notice that the procedure could be refined so that at loop exit
   it checks if the current character is in the guide set { ‘)’, ? }, and thus anticipates
   error detection. Similar procedures for nonterminals T and F can be derived.
   In Fig.5.3 (right) we add to the machine for nonterminal E, a few suitable states
   and transitions that encode the write actions to output the operators in postfix
   position. From this augmented machine, the next recursive procedure including write
   instructions is easily derived and is also shown in Fig.5.4 (right). For brevity we
   omit the similar procedures for nonterminals T and F.


<a id="P405"></a>

   partial rec. des. parser partial parser with write instructions
   procedure E
   call T
   while cc ∈ { ‘+’, ‘−’ } do
   case cc of
   ‘+’ :begin
   cc := next
   call T
   end
   ‘−’ :begin
   cc := next
   call T
   end
   end case
   end while
   end procedure
   procedure E
   call T
   while cc ∈ { ‘+’, ‘−’ } do
   case cc of
   ‘+’ :begin
   cc := next
   call T
   write (“add”)
   end
   ‘−’ :begin
   cc := next
   call T
   write (“sub”)
   end
   end case
   end while
   end procedure
   Fig.5.4 Recursive descent syntactic procedures for nonterminal E, without (left) and with (right)
   write instructions, which are framed
   The machines and procedures that compute the complete translation correspond
   to the following extended BNF translation grammar G τ (axiom E):
   G τ
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   E → T
   ?
   +
   ε
   T
   ε
   add
   |
   −
   ε
   T
   ε
   sub
   ? ∗
   T → F
   ?
   ×
   ε
   F
   ε
   mult
   |
   ÷
   ε
   F
   ε
   div
   ? ∗
   F →
   v
   v
   |
   ‘(’
   ε
   E
   ‘)’
   ε
   In the grammar rules, the elements of the target string, i.e., the characters of alphabet
   Δ, appear in the fraction denominators. The source and target alphabets Σ and Δ
   are as follows:
   Σ = { +, −, ×, ÷, v, ‘(’, ‘)’ }
   Δ = { add, sub, mult, div, v }
   and the round parentheses disappear from the target alphabet Δ, since they are
   unnecessary in the postfix polish form. ?

<a id="P406"></a>

   Notice that we have not formally defined the concept of extended BNF translation
   grammar, but we rely upon the reader’s intuition for it.


## 5.4.6 Bottom-Up DeterministicTranslation

   Now, consider a context-free translation scheme and assume the source grammar
   is suitable for bottom-up deterministic parsing, by the ELR(1) condition on p.254
   (actuallybytheLR(1)conditionsincethistimethegrammarisnotextended).Unlike
   the top-down case, it is not always possible to extend the parser with the write
   actions that compute the translation, without jeopardizing determinism. Intuitively,
   the impediment is simple to understand. We know the parser works by shifts and
   reductions: a shift pushes on stack a macro-state of the pilot finite automaton, i.e., a
   set of states of some machines. Of course, when a shift is performed, the parser does
   not know which rule will be used in the future for a reduction, and conservatively
   keeps all the candidates open. Imagine that two distinct machine states occur as
   candidates in the current macro-state, and make the likely hypothesis that different
   output actions are associated with them. Then, when the parser shifts, the translator
   should perform two different and contradictory write actions, which is impossible
   for a deterministic transducer. Thus, the reasoning above explains the impossibility
   to emit the output during a shift move of the translator.

   On the other hand, when a reduction takes place, exactly one source grammar
   rule has been recognized, which is identified by the final state of the corresponding
   machine.Sincethemappingbetweensourceandtargetrulesinthetranslationscheme
   is a total function, a reduction identifies exactly one target rule and can safely output
   the associated target string.

   We present a case (Example5.15) where the translation grammar has rules that
   containoutputsymbolsexclusivelylocalizedattheruleend,sothatthepilotautoma-
   ton can be enriched with write actions in the reduction m-states.

   Example 5.15 (translation of expressions to postfix notation) The BNF translation
   grammar below, which specifies certain formulas that use two infix signs 9 to be
   translated to postfix operators, is represented as a machine network (axiom E) in
   Fig.5.5:
   E → E
   +
   ε
   T
   ε
   add
   | T T → T
   × a
   ε
   ε
   a mult
   |
   a
   ε
   ε
   a
   Notice that care has been taken to position all the target characters as suffix at the
   ruleend;incidentally,rule E → T doesnotneedtargetcharacters.TheELR(1)pilot
   graph can be easily upgraded for translation by inserting the output actions in the
   reductions, as shown in Fig.5.6. Of course, the parser will execute a write action
   when it performs the associated reduction. ?
   9 More precisely, two-level infix arithmetic expressions of type sum of product, with variables a but
   without parentheses.


<a id="P407"></a>

   0 E 1 E 2 E 3 E
   4 E
   E →
   →
   ε
   add
   →
   E
   +
   ε
   T
   T
   0 T 1 T 2 T 3 T
   4 T
   T →
   →
   ε
   a mult
   →
   ε
   a
   T
   a
   ε
   ×
   ε
   a
   ε
   machine net with output
   Fig.5.5 Machine net of the BNF translation grammar of Example5.15
   4 T + × write(“a”)
   2 E +
   0 T +
   0 T ×
   pilot graph with write actions
   0 E
   0 E +
   0 T
   0 T +
   0 T ×
   1 E +
   3 E + write(“add”)
   1 T + ×
   4 E +
   1 T + ×
   2 T + × 3 T + × write(“a mult”)
   →
   I 0
   I 1
   I 2
   I 3
   I 4
   I 5
   I 6
   I 7
   E
   +
   T
   a
   T
   a
   ×
   a
   ×
   Fig.5.6 Pilot of the translator of Example5.15 with write actions in the reduction m-states. (The
   closure of m-states I 0 and I 2 is shown step by step.)
   On the contrary, in the next case (Example5.16) the presence of write actions on
   shift moves makes the translation incorrect.

   Example 5.16 (translation of parenthesized expressions) The BNF grammar G τ
   below specifies a translation of a language similar to the Dyck one, where every
   pair of matching “parentheses” a, c is translated to the pair b, e, but with one 
   exception: no translation is performed for the innermost pair of symbols a, c, which is just
   erased.


<a id="P408"></a>

   0 S 1 S 2 S 3 S 4 S
   5 S
   S →
   →
   →
   a S c S
   c
   machine net (source gram.)
   tentative partial pilot graph with write actions
   5 S
   does not
   write b e
   5 S c
   does not
   write b e
   ...

   0 S
   1 S
   early to
   write b
   0 S c
   1 S c
   0 S c
   4 S
   late to
   write b
   3 S
   should write e but b
   was not written yet
   0 S
   2 S
   ↓
   I 0
   I 1
   I 2
   I 3
   I 4
   I 5
   I 6 I 7
   a a
   S
   c
   c
   c
   a
   S
   a
   S
   Fig.5.7 Machine net and tentative partial pilot with write actions (Example5.16)
   G τ : S →
   a
   b
   S
   c
   e
   S |
   a
   ε
   c
   ε
   τ (a a c c a c) = b e
   τ (a c) = ε
   Figure5.7 shows the machine net of the source grammar G 1 of G τ . The final states
   are separate in order to identify which alternative rule has been analyzed, as each
   rule has a distinct output.

   In Fig.5.7 an attempt is made to obtain an ELR(1) pilot with write actions; for
   brevity the construction is partial. The attempt fails almost immediately: either the
   write actions are premature, as in the m-state I 1 where it is unknown whether the
   letter a that has just been shifted belongs to an innermost pair or not, or they are too
   late, as in the m-state I 5 . ?

### 5.4.6.1 Postfix Normal Form

   In practice, when specifying a context-free translation intended for bottom-up
   parsing, it is preferable to put the translation grammar into a form (Definition5.17) that
   confines write actions within reduction moves.


<a id="P409"></a>

   Definition 5.17 (postfix translation grammar) A translation grammar or scheme is
   inthepostfixnormalformifeverytargetgrammarrulehastheform A → γ w,where
   γ ∈ V ∗ and w ∈ Δ ∗ . ?
   Differently stated, no target string may occur as an inner substring of a rule, but only
   as a suffix. Example5.2 (p.389) is in the postfix form, while Examples5.12 (p.401)
   and 5.16 (p.407) are not.

   Onemaywonderwhatlossorworsening,ifany,iscausedbythepostfixcondition.

   From the standpoint of the family of translation relations that can be specified, it
   is easy to show that postfix grammars have the same expressivity as the general
   context-free translation grammars do, sometimes at the cost of some obscurity. In
   Algorithm5.18 we explain the transformation of a generic translation grammar into
   postfix normal form.

   Algorithm 5.18 (convertingatranslationgrammartopostfixform)Considerinturn
   each rule A → α of the given translation grammar. If the rule violates the postfix
   condition,findthemaximal(longest)targetstringv ∈ Δ + thatoccursintherightmost
   position in α, and transcribe the rule as:
   A → γ
   ε
   v
   η
   whereγ isanystringandη isanon-emptystringdevoidoftargetcharacters.Replace
   this rule with the next ones:
   A → γ Y η Y →
   ε
   v
   where symbol Y is a new nonterminal. The second rule complies with the postfix
   condition; if the first rule does not (yet), find anew the maximal rightmost target
   string within string γ, and repeat the transformation above. Eventually, all the target
   elements that occur in the middle of a rule will have been moved to suffix positions,
   and so the resulting grammar will be in the postfix normal form. ?
   The next example (Example5.19) illustrates the transformation to postfix, and it
   should convince the reader that the original and transformed grammars define the
   same translation.

   Example 5.19 (grammar transformation to postfix normal form) Examining
   the translation grammar G τ : S →
   a
   b
   S
   c
   e
   S | a c of the previous example (Exam-
   ple5.16), we notice that in the target grammar G 2 two letters (pointed) violate the
   postfix form:
   ↓ ↓
   S → b S e S

<a id="P410"></a>

   To normalize the grammar, we apply Algorithm5.18, thus obtaining the following
   grammar, which contains two new nonterminals:
   G τ : S → A S C S | a c A →
   a
   b
   C →
   c
   e
   G 2 postfix : S → A S C S | ε A → b C → e
   Notice that A stands for the left parenthesis b and B for the right one e. It is easy
   to check that the original and the postfix target grammars G 2 and G 2 postfix define
   the same translation. The machine net in Fig.5.8 represents the translation grammar
   normalized to postfix form. Since the grammar is BNF and the two alternative rules
   of the axiom S do not have any output, the two recognizing paths of machine M S
   end in the same final state, 4 S . The pilot of the postfix grammar, partially shown
   in Fig.5.8, writes the output only at reduction time. Now, the write actions in the
   pilot automaton are, respectively, associated with the reduction states 1 A and 1 C of
   nonterminals A and C, which correspond to parenthesis pairs that are not innermost.
   Also, notice that there is not any write action in the m-state I 5 , which includes the
   final state 4 S and hence specifies a reduction to nonterminal S, because the symbol
   c just shifted to reach m-state I 5 certainly belongs to an innermost pair. ?
   We summarize the previous discussion on bottom-up translators with a sufficient
   condition (Property5.20) for being deterministic.

   Property 5.20 (determinism of bottom-up transducers) A translation defined by
   a BNF (not extended) translation grammar in the postfix normal form, such that
   the source grammar satisfies condition LR(k) with k ≥ 1, can be computed by a
   deterministic bottom-up parser, which writes on output at reduction moves 
   exclusively. ?
   Inessence,thepostfixformallowstheparsertodeferitswriteactionsuntilitreaches
   a state where the action is uniquely identified.

   However,thisnormalizationmethodhassomedrawbacks.Theintroductionofnew
   nonterminal symbols, such as A and B in Example5.19, makes the new grammar
   less readable. Another nuisance may come from rule normalization, when empty
   rules such as Y → ε are added to the source grammar. Empty rules tend to increase
   the length k of the look-ahead needed for parsing; in some cases the LR(k) property
   may be lost, as the next example (Example5.21) shows.

   Example 5.21 (loss of LR(1) property caused by normalization) The translation of
   aninfixexpression 10 toprefixformisspecifiedbythegrammarG τ below.Bylooking
   10 The expression is simply a summation a + a + ··· + a, of one or more terms.

<a id="P411"></a>

   BNF translation grammar in postfix form
   G τ : S → A S C S | a c A →
   a
   b
   C →
   c
   e
   machine net with output
   5 S
   0 S 1 S 2 S 3 S 4 S
   S →
   →
   A S C S
   a
   ε
   c
   ε
   0 A 1 A 0 C 1 C A → → C → →
   a
   b
   c
   e
   partial pilot graph with write actions
   0 S
   0 A a
   1 S
   0 S c
   0 A a
   2 S
   0 C a
   etc.

   5 S
   1 A a write b
   5 S c
   1 A a write b
   1 C a write e
   4 S do not write e
   (omitted) some m-states and transitions
   →
   I 0
   I 1
   I 2
   I 3
   I 4
   I 5
   a
   A S
   a c
   c
   Fig. 5.8 Translation grammar G τ in the postfix form, machine net with output and partial pilot
   with write actions (Example5.19)
   at the target grammar G 2 , we ascertain that the first rule of G τ is not in the postfix
   form.

   G τ original G 1 G 2
   E →
   ε
   add
   E
   + a
   a
   E → E + a E → add E a
   E →
   a
   a
   E → a E → a

<a id="P412"></a>

   0 E 1 E 2 E 3 E 4 E
   5 E 0 Y
   E →
   →
   ε
   a
   →
   ε
   a
   Y →
   →
   ε
   add
   Y
   a
   ε
   E
   +
   ε
   a
   ε
   Fig. 5.9 Machine net with output of a BNF translation grammar normalized in the postfix form
   (Example5.21)
   Here is the postfix form G ? τ of grammar G τ , obtained by normalization (Algo-
   rithm5.18):
   G ? τ postfix G ? 1 G ? 2
   E → Y E
   + a
   ε
   ε
   a
   E → Y E + a E → Y E a
   E →
   a
   ε
   ε
   a
   E → a E → a
   Y →
   ε
   add
   Y → ε Y → add
   ThemachinenetofgrammarG ? τ isinFig.5.9.ItiseasytocheckthatgrammarG ? τ and
   the original G τ define the same translation. Unfortunately, while the original source
   grammar G 1 satisfies the LR(1) condition, the new rule Y → ε of the postfix source
   grammar G ? 1 makes state 0 Y both initial and final, and causes an LR(1) shift-reduce
   conflict in the macro-states I 0 , I 1 and I 6 of the pilot shown in Fig.5.10. ?
   Fortunately,inmanypracticalsituationsgrammarnormalizationtopostfixformdoes
   not hinder deterministic parsing.


### 5.4.6.2 SyntaxTree asTranslation

   A common application of syntactic translation (both top-down and bottom-up) is
   to construct the syntax tree of the source text. Programs usually represent a tree
   as a linked data structure, but to construct such a representation we need semantic
   actions, to be discussed in later sections. Here, instead of producing a linked list,
   which would be impossible to do with a purely syntactic translation, we are content
   withoutputtingthesequenceoflabelsofthesourcerules,andintheordertheyoccur
   in the series of reduction steps executed by the shift-reduce parser. This is similar
   to Example5.13 above and only differs in the label order, which corresponds to a
   reversedrightderivationinsteadofaleftone;seealsothetreevisitordersinFig.4.10
   on p.232.


<a id="P413"></a>

   1 E
   0 E +
   0 Y a add
   2 E 3 E 4 E a
   5 E + a
   0 E
   0 Y a add
   5 E a
   1 E +
   0 E +
   0 Y a add
   2 E + 3 E + 4 E + a
   ↓
   I 0
   I 1
   I 2 I 3
   I 4
   I 5
   I 6
   I 7 I 8
   I 9
   I 10
   Y
   E
   a
   a
   + a
   Y
   a
   Y
   E
   +
   a
   Fig. 5.10 Pilot LR(1) of the translation grammar of Fig.5.9. Correctly, write actions are only in
   reduction m-states, but there are shift-reduce conflicts in m-states I 0 , I 1 and I 6
   Given a source grammar with rules labeled for reference, the next postfix 
   translation scheme produces the label sequence in the order of execution of the reduction
   steps:
   label source grammar rule translation grammar rule
   r i A → α A → α
   ε
   r i
   It is not difficult to see that the image produced by the above translation is the
   reversal of the sequence of rule labels occurring in the rightmost derivation of the
   input string. Since by hypothesis the source grammar is LR(1) and the scheme is
   postfix, it suffices to enrich the parser with write actions to compute this translation.

### 5.4.6.3 Comparison ofTop-Down and Bottom-Up Approaches

   We recapitulate the main considerations about upgrading parsers to translators. The
   main argument in favor of the top-down methods is that they do not suffer any
   limitation as they allow the implementation of any syntactic translation scheme,
   provided of course that the source grammar meets the suitable condition for parsing.
   Moreover, a recursive descent translator can be easily constructed by hand, and the
   resulting program is easy to understand and maintain.


<a id="P414"></a>

   Ontheotherhand,forthebottom-upmethodsthelimitationimposedbythepostfix
   normal form of the translation grammar may be compensated by the superiority of
   the LR(k) grammars over the LL(k) ones, for the definition of the source language.
   In conclusion neither method, top-down versus bottom-up, is entirely superior to the
   other one.

5.5 RegularTranslation
======================

   Just like context-free grammars have as a special case the right-linear grammars,
   which in turn have regular expressions and finite automata as their natural 
   counterpart, in a similar way translation grammars include as a special case the right-linear
   translation grammars, which define translations that can be characterized in terms of
   regular translation expressions and finite transducers.

   Consider for instance the translation τ below which converts a letter a to b or c,
   depending on the number of letters being even or odd:
   τ
   ⎧
   ⎨
   ⎩
   a 2n
   τ
   ?→ b 2n n ≥ 0
   a 2n+1
   τ
   ?→ c 2n+1 n ≥ 0
   (5.3)
   Translation τ is specified by the following right-linear translation grammar G τ
   (axiom A 0 ):
   G τ
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   A 0 →
   a
   c
   A 1 |
   a
   c
   |
   a
   b
   A 3 | ε
   A 1 →
   a
   c
   A 2 | ε
   A 2 →
   a
   c
   A 1
   A 3 →
   a
   b
   A 4
   A 4 →
   a
   b
   A 3 | ε
   The well-known equivalence between right-linear grammars and finite automata is
   now extended by defining regular translation expressions.

   First,regularexpressionscanbemodifiedinordertospecifyatranslationrelation:
   the arguments of the expression are pairs of source/target strings, instead of being
   characters as customary. Therefore, a sentence generated by such an r.e. model is
   a sequence of pairs. By separating the source component of each pair from the
   target one, we obtain two strings, which can be interpreted as a pair belonging to
   the translation relation. Thus, such an r.e. model defines a translation relation to be
   called regular or rational.

   By this approach, the translation relation τ in (5.3) is defined by the following
   regular translation expression e τ :
   e τ =
   ?
   a 2
   b 2
   ? ∗
   ∪
   a
   c
   ?
   a 2
   c 2
   ? ∗

<a id="P415"></a>

   The string of fractions below:

      a
      c
      ·
      ?
      a 2
      c 2
      ? 2
      =
      a
      c
      ·
      a 2
      c 2
      ·
      a 2
      c 2
      =
      a 5
      c 5
      ∈ L (e τ )
   then corresponds to the pair
   ?
   a 5 , c 5
   ?
   in the translation relation ρ τ defined by the
   expression e τ .

   Example 5.22 (consistent transliteration of an operator) The source text is a list of
   numbers separated by a division sign “/”. The translation may replace the sign by
   either one of the signs “:” or “÷”, but it must consistently choose the same sign
   throughout. For simplicity we assume the numbers to be unary. The source/target
   alphabets are as follows:
   Σ = { 1, ‘/’ } Δ = { 1, ‘ ÷ ’, ‘ : ’ }
   The source strings have the form c (/c) ∗ , where c stands for any unary number
   denoted by 1 + . Two valid translations are the following:
   (3/5/2, 3 : 5 : 2) (3/5/2, 3 ÷ 5 ÷ 2)
   Onthecontrary,transliteration(3/5/2, 3 : 5 ÷ 2)iswrong,becausethedivision
   signs are differently transliterated.

   Notice this translation cannot be expressed by a homomorphism, as the image of
   the division sign is not single-valued; on the other hand, the inverse translation is an
   alphabetic homomorphism. The translation is defined by the r.e. below:

      (1, 1) +
      ?
      (‘/’, ‘ : ’) (1, 1) +
      ? ∗
      ∪ (1, 1) +
      ?
      (‘/’, ‘ ÷ ’) (1, 1) +
      ? ∗

   or better by the more readable fractional notation below:

      ?
      1
      1
      ? +
      ?
      /
      :
      ?
      1
      1
      ? + ? ∗
      ∪
      ?
      1
      1
      ? +
      ?
      /
      ÷
      ?
      1
      1
      ? + ? ∗
   
   Thetermsproducedbyapplyingaderivationarestringsoffractions,i.e.,stringpairs.
   Consider the following derived string:

      ?
      1
      1
      ? 1
      ?
      /
      ÷
      ?
      1
      1
      ? 2 ? 1
      =
      1
      1
      /
      ÷
      1
      1
      1
      1

   projectitonthetopandbottomcomponents,andthusobtainthepairofsource/target
   strings (1/11, 1 ÷ 11). ?
   We summarize the previous discussion and examples (Example5.22) about regular
   translations, in the next definition.


<a id="P416"></a>

   Definition 5.23 (regular (or rational) translation) A regular or rational translation
   expression, shortly r.t.e., is a regular expression with union, concatenation and star
   (and cross) operators that has as arguments some string pairs (u, v ), also written as
   u
   v , where terms u and v are possibly empty strings, respectively, on the source and
   on the target alphabet.

   Let C ⊂ Σ ∗ × Δ ∗ be the set of the pairs (u, v ) occurring in the expression. The
   regular or rational translation relation defined by the r.t.e. e τ consists of the pairs
   (x, y ) of source/target strings such that:
   • There exists a string z ∈ C ∗ in the regular set defined by the r.t.e. e τ .
   • Strings x and y are the projections of string z on the first and second components,
   respectively. ?
   It is straightforward to see that the set of source strings defined by an r.t.e. (as well
   as the set of target strings) is a regular language. Yet notice that not every translation
   relation that has two regular sets as its source and target languages can be defined
   with an r.t.e.: an example to be discussed later is the relation that maps each string
   to its mirror string.


## 5.5.1 Two-Input Automaton

   SincethesetC ofpairsoccurringinanr.t.e.canbeviewedasanewterminalalphabet,
   the regular language over C can be recognized by a finite automaton, as illustrated
   in the coming example.

   Example 5.24 (consistent transliteration of an operator (Example5.22) continued)
   The recognizer of the regular translation relation is shown in Fig.5.11.

   Thisautomatoncanbeviewedasamachinewithtworead-onlyinputtapes,inshort
   a 2I-machine, each one with an independent reading head, respectively, containing
   thesourcestringx andthetargetstring y.Initiallytheheadsarepositionedonthefirst
   characters and the machine is in the start state. The machine performs as specified
   by the state-transition graph: e.g., in state q 1 , on reading a slash “/” from the source
   tape and a sign “÷” from the target tape, the automaton moves to state q 4 and shifts
   both heads by one position. If the machine reaches a final state and both tapes have
   been entirely scanned, the pair (x, y ) belongs to the translation relation. 11
   This automaton can check that two strings, such as:
   (11‘/’1, 1‘ ÷ ’1) ≡
   11/1
   1 ÷ 1
   11 The model is known as a Rabin and Scott machine. For greater generality such a machine may be
   equipped with three or more tapes, in order to define a relation between more than two languages;
   see Sakarovitch [2] and Berstel [1] for this and similar models.


<a id="P417"></a>

   q 2 q 3
   q 0 q 1
   q 4 q 5
   →
   ↓
   ↓
   ↑
   1
   1
   1
   1
   /
   :
   1
   1
   1
   1
   /
   :
   /
   ÷
   1
   1
   1
   1
   /
   ÷
   Fig.5.11 2I-automaton of the r.t.e. of Examples5.22 and 5.24
   do not correspond in the translation, because the following computation:
   q 0
   1
   1
   −→ q 1
   does not admit any continuation with the next pair, i.e., fraction
   1
   ÷ .

   ?
   It is sometimes convenient to assume that each tape is delimited on the right by a
   reserved character marking the tape end.

   Atafirstglance,regulartranslationexpressionsandtwo-inputmachinesmayseem
   to be wrong idealizations for modeling a compiler, because in compilation the target
   stringisnotgivenandmustbecomputedbythetranslator.Yetthisconceptualization
   is valuable for specifying some simple translations and also as a rigorous method for
   studying translation functions.

   Whendesigningatwo-inputrecognizer,wecanassumewithoutlossofgenerality
   that each move reads one character or nothing from both the source and the target
   tapes; the following definition (Definition5.25) formalizes the concept.

   Definition 5.25 (two-inputautomatonor2I-automaton)Afiniteautomatonwithtwo
   inputs or 2I-automaton is defined by a set of states Q, the initial state q 0 ∈ Q and a
   set F ⊆ Q of final states. The transition function δ is defined as follows:
   δ: Q × (Σ ∪ { ε }) × (Δ ∪ { ε }) → ℘ (Q)
   An automaton move includes the following actions:
   • The automaton enters state q ? .

   • If q ? ∈ δ (q, a, b), the automaton reads characters a from the source tape and b
   fromthetargettape;ifq ? ∈ δ (q, ε, b),theautomatondoesnotreadthesourcetape
   and reads character b from the target tape; similarly for the case q ? ∈ δ (q, a, ε);
   if q ? ∈ δ (q, ε, ε), the automaton reads neither the source nor the target tape.

<a id="P418"></a>

   The automaton recognizes the source and target strings if the computation ends in a
   final state after both tapes have been entirely scanned. ?
   Imaginenowtoprojectthearclabelsofa2I-automatonontothefirstcomponent.The
   resulting automaton has just one input tape with symbols from the source alphabet
   Σ and termed the input automaton subjacent to the original machine; it recognizes
   the source language.

   Sometimes another normal form of a 2I-automaton is used, characterized by the
   fact that each move reads exactly one character either from the source tape or from
   the target tape, but not from both. More precisely the arc labels are of the following
   two types:
   label
   a
   ε
   with a ∈ Σ, i.e., read one character from source
   label
   ε
   b
   with b ∈ Δ, i.e., read one character from target
   This means that a machine in such a normal form shifts only one head per move
   and one position at a time. It is to be expected that such a normalization will often
   increase the number of states because a non-normalized move, like the following
   one (left), is replaced by the cascade of normalized moves (right):
   q
   r
   q
   ?q, r ? r
   a
   b
   a
   ε
   ε
   b
   where the state ?q, r ? is new.

   On the other hand, in order to make the model more expressive and concise,
   it is convenient to allow regular translation expressions as arc labels. As for finite
   automata, this generalization does not change the computational power, but it helps
   in hiding the details of complicated examples.

   Finally, a short notation: in an arc label we usually drop a component that is the
   empty string. Thus we may write:
   a + b
   d
   |
   a + c
   e
   in place of the following:
   a +
   ε
   b
   d
   |
   a +
   ε
   c
   e
   This r.t.e. says that a sequence of letters a, if followed by a letter b, is translated to
   a letter d; if it is followed by c, it is translated to e.


<a id="P419"></a>


### 5.5.1.1 Equivalence of Models

   In accordance with the well-known equivalence of regular expressions and finite
   automata, we state a similar property for translations.

   Property 5.26 (equivalence of translation models) The families of translations
   defined by regular translation expressions and by finite (nondeterministic)
   2Iautomata coincide. ?
   We recall that an r.t.e. defines a regular language R over an alphabet that consists of
   a finite set of pairs of strings (u, v ) ≡
   u
   v , where u ∈ Σ
   ∗
   and v ∈ Δ ∗ . By separately
   extracting from each string in R the source and target elements, we obtain a crisp
   formulation of the relation between source and target languages.

   Property 5.27 (Nivat theorem) The following four conditions are equivalent:
   1. The translation relation ρ τ is defined by a right-linear (or left-linear) translation
   grammar G τ .

   2. The translation relation ρ τ is defined by a 2I-automaton.

   3. The translation relation ρ τ ⊆ Σ ∗ × Δ ∗ is regular.

   4. There exist an alphabet Ω, a regular language R over Ω, and two alphabetic
   homomorphisms h 1 : Ω → Σ ∪ { ε } and h 2 : Ω → Δ ∪ { ε }, such that:
   ρ τ = { (h 1 (z), h 2 (z)) | z ∈ R } ?
   Example 5.28 (division by two) The image string is the halved source string. The
   translation relation
   ? ?
   a 2n , a n
   ?
   | n ≥ 1
   ?
   is defined by the following r.t.e.:
   ?
   a a
   a
   ? +
   An equivalent 2I-automaton A is shown below:
   q 0 q 1 q 2 → →
   a
   ε
   a
   a
   a
   ε
   To apply the Nivat theorem (clause4 of Property5.27), we derive from automaton A
   the following r.t.e.:
   ? a
   ε
   a
   a
   ? +
   and we rename for clarity the pairs below:
   a
   ε
   = c
   a
   a
   = d

<a id="P420"></a>

   Next, consider the alphabet Ω = { c, d }. The r.t.e. defines the regular language
   R = (cd ) + , obtained replacing each fraction with a character of the new alphabet.
   The following alphabetic homomorphisms h 1 and h 2 :
   Ω h 1 h 2
   c a ε
   d a a
   producetheintendedtranslationrelation.Thusfor z = cd cd ∈ R wehave h 1 (z) =
   a a a a and h 2 (z) = a a.

   To illustrate Nivat theorem (clause1 of Property5.27), we apply the well-known
   equivalence of finite automata and right-linear grammars (p.129), to obtain the
   following equivalent right-linear translation grammar:
   S →
   a
   ε
   Q 1 Q 1 →
   a
   a
   Q 2 Q 2 →
   a
   ε
   Q 1 | ε
   Each grammar rule corresponds to a move of the 2I-automaton. Rule Q 2 → ε is a
   short notation for Q 2 →
   ε
   ε .

   ?
   We recall from Sect.5.4 that for the syntactic translations that have a context-free
   grammarastheirsupport,thenotationbasedonatranslationgrammarhastobeused,
   because such translations require a pushdown stack and cannot be described by a
   finite memory device, such as a 2I-automaton or an r.t.e.

   Several, but not all, properties of regular languages have a counterpart for regular
   translation relations. 12 Thus, the union of regular translation relations still yields
   a regular translation relation, but it is not always so for their intersection and set
   difference. For some translation relations, it is possible to formulate a pumping
   lemma similar to the one for the regular languages on p.89.

   Non-regular Translation of Regular Languages
   Asalreadynoticed,noteverytranslationrelationwithtworegularlanguagesassource
   and target is necessarily regular; i.e., it can be defined through a 2I-finite automaton
   or an r.t.e. Consider for instance the following languages L 1 , L 2 and translation τ:
   L 1 = (a | b) ∗ L 2 = (a | b) ∗ τ (x) = x R with x ∈ L 1
   This translation τ cannot be defined by a 2I-automaton, as a finite set of states does
   not suffice to check that the string on the second tape is the reversal of the one on
   the first tape.

   12 See the already cited books [1,2] for a comprehensive presentation.


<a id="P421"></a>


## 5.5.2 Translation Function and FiniteTransducer

   We leave the static perspective of a translation as a relation between two strings,
   and we focus instead on the translation process, where an automaton is viewed
   as an algorithmic implementation of a translation function. We introduce the finite
   transducer or IO-automaton. This machine reads the source string from the input
   tape and writes the image on the output tape. We shall mostly study single-valued
   translationsandespeciallythosethatarecomputedbydeterministicmachines,butwe
   introducethemodelbymeansofanondeterministicexample,forthesametranslation
   relation τ on p.414.

   Example 5.29 (nondeterministic translation) It is required to translate a string a n to
   the image b n if n is even, or to the image c n if n is odd. The translation relation ρ τ
   below:
   ρ τ =
   ? ?
   a 2n , b 2n
   ?
   | n ≥ 0
   ?
   ∪
   ? ?
   a 2n+1 , c 2n+1
   ?
   | n ≥ 0
   ?
   defines the following translation function τ:
   τ (a n ) =
   ⎧
   ⎨
   ⎩
   b n for n ≥ 0 even (n = 0 is even)
   c n for n ≥ 1 odd
   The r.t.e. e τ is the following:
   e τ =
   ?
   a 2
   b 2
   ? ∗
   ∪
   a
   c
   ?
   a 2
   c 2
   ? ∗
   A simple deterministic two-input automaton recognizes this relation ρ τ :
   q 2 q 1 q 0 q 3 q 4
   ↓
   ↓ ↓ ↓
   a
   b
   a
   c
   a
   b
   a
   b
   a
   c
   a
   c
   Tocheckfordeterminism,observethatonlytheinitialstateq 0 hastwooutgoingarcs,
   but that the source labels of such arcs are different. 13 Therefore the 2I-machine can
   deterministically decide if two strings memorized on the two tapes, such as
   a a a a
   bbbb ,
   correspond to each other in the translation relation.

   The transducer or IO-automaton has the same state-transition graph as the
   2Iautomaton above, but the meaning of an arc, e.g., q 0
   a
   b
   − → q 1 , is entirely different:
   13 If the 2I-automaton has some moves that do not read from either tape, the determinism condition
   has to be formulated more carefully; e.g., see [2].


<a id="P422"></a>

   in the state q 0 it reads character a from the input, writes character b to the output
   and enters state q 1 . Yet the other arc q 0
   a
   c
   − → q 3 instructs the machine to perform a
   different action, while reading the same character a in the same state q 0 . In other
   words, the choice between the two moves is not deterministic. As a consequence,
   two computations are possible for the input string a a:
   q 0 → q 1 → q 2 q 0 → q 3 → q 4
   but only the former succeeds in reaching a final state, namely q 2 , and thus only
   its output is considered: τ (a a) = bb. Notice that the presence of nondeterministic
   moves shows up also in the input automaton subjacent to the transducer, which is
   clearly nondeterministic.

   Moreover, it should be intuitively clear that the requested translation τ cannot be
   computed by any deterministic finite transducer, because the choice of the character
   to emit can only be made when the input tape has been entirely scanned; but then it
   is too late to decide how many characters have to be output. ?
   Tosumupthefindingsofthisexample(Example5.29),therearesingle-valuedregular
   translations that cannot be computed by a deterministic finite IO-automaton. This is
   a striking difference with respect to the well-known equivalence of the deterministic
   and nondeterministic models of finite automata (Property3.24 on p.147).


### 5.5.2.1 SequentialTransducer

   Insomeapplicationsitisnecessarytoefficientlycomputethetranslationinrealtime,
   becausethetranslatormustproducetheoutputwhilescanningtheinput.Atlast,when
   the input is finished, the automaton may append to the output a finite piece of text
   that depends on the final state reached. This is the behavior of a type of deterministic
   machine called a sequential transducer. 14 The next definition formalizes the concept
   (Definition5.30).

   Definition 5.30 (sequential transducer) A sequential transducer or IO-automaton
   T is a deterministic machine defined by a set Q of states, a source alphabet Σ and
   a target alphabet Δ, an initial state q 0 and a set F ⊆ Q of final states. Furthermore
   there are three single-valued functions:
   1. The state-transition function δ computes the next state.

   2. The output function η computes the string to be emitted by a move.

   3. The final function ϕ computes the last suffix to be appended to the target string
   at termination.

   14 Thisistheterminologyof[2];but[1]callssubsequentialthesamemodel.Inelectricalengineering
   a sequential machine is a quite similar logical device that computes a binary output, which is a
   function of the sequence of input bits.


<a id="P423"></a>

   The domains and images of these three functions are the following (symbol ? is the
   end-marker of the input):
   δ: Q × Σ → Q η: Q × Σ → Δ ∗ ϕ: F × { ? } → Δ ∗ ?
   Inthegraphicalpresentationasastate-transitiongraph,thetwofunctionsδ (q, a) =
   r and η (q, a) = u are represented by the arc q
   a
   u
   −→ r, which means: in the state
   q, when reading character a, emit string u and move to the next state r. The final
   function ϕ(r, ?) = v means: when the source string has been entirely scanned, if
   the final state is r, then write string v.

   For a source string x, the translation τ (x) computed by the sequential transducer
   T is the concatenation of two strings, produced by the output function and by the
   final one:
   τ (x) =
   ?
   y z ∈ Δ ∗
   ∃ a labeled computation
   x
   y
   that ends in the
   state r ∈ F and it holds z = ϕ(r, ?)
   ?
   Themachineisdeterministicbecausetheinputautomaton? Q, Σ, δ, q 0 , F ?subja-
   centtoT isdeterministic,andtheoutputandfinalfunctionsηandϕaresingle-valued.
   However, the condition that the subjacent input automaton is deterministic does
   not ensure by itself that the translation is single-valued, because between two states
   of the sequential transducer T there may be two arcs labeled
   a
   b
   and
   a
   c , which cause
   the output not to be unique.

   We call sequential a translation function that is computable by a sequential 
   transducer, as of Definition5.30. The next example (Example5.31) shows a simple use
   of a sequential translation, inspired to arithmetic.

   Example 5.31 (meaningless zeroes) The source text is a list of binary integers, i.e.,
   a list of bit sequences, separated by a blank represented by symbol “?”. The 
   singlevalued translation deletes all the meaningless zeros, i.e., the leading ones; an integer
   ofallzeroesistranslatedtoonlyonezero.Thistranslationisdefinedbythefollowing
   r.t.e.:
   ??
   0 +
   0
   ?
   0
   ε
   ? ∗
   1
   1
   ?
   0
   0
   1
   1
   ? ∗ ?
   ?
   ?
   ? ∗ ?
   0 +
   0
   ?
   0
   ε
   ? ∗
   1
   1
   ?
   0
   0
   1
   1
   ? ∗ ?
   ?
   ε
   The equivalent sequential transducer is shown in Fig.5.12. The final function ϕ
   does not write anything in the final state q 1 , whereas it writes a character 0 in
   the final state q 2 . For instance, when translating the source string 00 ? 01, the
   machine traverses the state sequence q 0 q 2 q 2 q 0 q 2 q 1 and writes the target string
   ε · ε · 0 · ? · ε · 1 · ε = 0 ? 1. For the source string 00, the machine traverses the
   state sequence q 0 q 2 q 2 , writes the target string ε · ε and at last writes the target
   character 0. ?

<a id="P424"></a>

   q 0
   q 1 q 2
   ↓
   ↓
   ε
   ↓
   0
   1
   1
   0
   ε
   0
   0 ,
   1
   1
   0
   1
   1
   0
   ε
   Fig.5.12 Sequentialtransducer(IO-machine)ofExample5.31.Noticethegraphicalrepresentation
   of the final function ϕ by means of the fractions
   ?
   ε
   and
   ?
   0
   that label the final darts on states q 1 and
   q 2 , respectively
   We show a second example of sequential transducer that makes an essential use of
   the final function ϕ for computing a translation τ specified as follows:
   τ (a n ) =
   ?
   e for n ≥ 0 even
   o for n ≥ 1 odd
   Thesequentialtransducerhasonlytwostates,bothfinal,thatcorrespondtotheparity
   classes, even and odd, of the source string. The machine does not write anything,
   while it switches from one state to the other one; at the end, depending on the final
   state, it writes character either e or o for either even or odd, respectively.
   To conclude, we mention a practically relevant property: the composition of two
   sequential functions is still a sequential function. This means that the cascade of two
   sequential transducers can be replaced by just one (typically larger) transducer of
   the same kind.


### 5.5.2.2 Two Opposite Passes

   Given a single-valued translation specified by a regular translation expression or
   by a 2I-automaton, we have seen that it is not always possible to implement the
   translation by means of a sequential transducer, i.e., a deterministic IO-automaton.
   On the other hand, even in such cases the translation can always be implemented by
   twocascaded(deterministic)sequentialpasses,eachonebeingonewaybutscanning
   the string in opposite directions. In the first pass a sequential transducer scans from
   left to right and converts the source string into an intermediate string. Then in the
   second pass another sequential transducer scans the intermediate string from right
   to left and produces the specified target string. We show a simple case.


<a id="P425"></a>

   Example 5.32 (regular translation by two one-way passes in opposite directions)
   We recall from Example5.29 on p.421 that the translation of string a n into string b n
   ifn isevenorintoc n ifn isodd(withn ≥ 0)cannotbedeterministicallycomputedby
   an IO-automaton. The reason is that by a one-way scan, the parity class of the string
   would be known just at the end; but then it is too late for writing the output, because
   number n is unbounded and exceeds the finite memory capacity of the machine.
   We show an implementation by two cascaded sequential transducers that scan their
   respective input in opposite directions.

   The first sequential machine scans the input from left to right and computes the
   intermediate translation τ 1 , represented by the r.t.e. e τ 1 below:
   e τ 1 =
   ?
   a
   a ?
   a
   a ??
   ? ∗ ?
   a
   a ?
   ?
   which maps to a ? (resp. to a ?? ) a character a occurring at odd (resp. at even) position
   in the input string. The last term
   a
   a ?
   may be missing.

   The second transducer scans the intermediate text from right to left and computes
   the final translation τ 2 , represented by the r.t.e. e τ 2 below:
   e τ 2 =
   ?
   a ??
   b
   a ?
   b
   ? ∗
   ∪
   ?
   a ?
   c
   a ??
   c
   ? ∗
   a ?
   c
   where the choice between the two sides of the union is controlled by the first character
   being a ? or a ?? in the reversed intermediate string produced by τ 1 . Thus, for instance,
   for the source string a a we have the following:
   τ 2
   ?
   (τ 1 (a a)) R
   ?
   = τ 2
   ?
   (a ? a ?? ) R
   ?
   = τ 2 (a ?? a ? ) = bb
   In fact, a cascade of two one-way sequential transducers that scan their input in
   opposite directions is equivalent to a one-way transducer equipped with a pushdown
   stack, which is a more powerful automaton model. ?
   In several applications the computing capacity of the sequential translator model is
   adequate to the intended job. In practice, the sequential transducer is often enriched
   with the capability to look-ahead on the input string, in order to anticipate the choice
   oftheoutputtobeemitted.Thisenhancementissimilartowhathasbeenextensively
   discussed for parsing in Chap.4. The sequential translator model with look-ahead is
   implemented by compiler construction tools of widespread application, such as Lex
   and Flex. 15
   15 For a formalization of look-ahead sequential transducers, we refer to Yang [5].

<a id="P426"></a>


## 5.5.3 Closure Properties ofTranslation

   Tofinishwiththepurelysyntactictranslations,wefocusnowontheformallanguage
   families that are induced by such transformations. Given a language L belonging to
   a certain family, imagine to apply a translator that computes a translation function
   τ. Then consider the image language generated by τ, below:
   τ (L) =
   ?
   y ∈ Δ ∗ | y = τ (x) ∧ x ∈ L
   ?
   The question is: what is the language family of the image language τ (L)? For
   instance, if the language L is context-free and the translator is a pushdown
   IOautomaton, is the target language context-free as well?
   One should not confuse the language L and the source language L 1 of the 
   transducer,thoughbothoneshavethesamealphabetΣ.Thesourcelanguage L 1 includes
   all and only the strings recognized by the automaton subjacent to the translator, i.e.,
   the sentences of the source grammar G 1 of the translation scheme G τ . The 
   transducer converts a string of language L to a target string, provided the string is also a
   sentence of the source language L 1 of the transducer; otherwise an error occurs and
   the transducer does not produce anything.

   The essential closure properties of translations are in the next table:
      # language finite transducer pushdown transducer
      1 L ∈ REG τ (L) ∈ REG
      2 L ∈ REG τ (L) ∈ CF
      3 L ∈ CF τ (L) ∈ CF
      4 L ∈ CF τ (L) not always ∈ CF
   Cases 1 and 3 descend from the Nivat theorem (Property5.27 on p.419) and from
   the fact that both language families REG and CF are closed under intersection with
   regular languages (p.213). In more detail, the recognizer of language L can be
   combined with the finite transducer, thus obtaining a new transducer that has the
   intersection L ∩ L 1 as source language. The new transducer model is the same as
   that of the recognizer of L: a pushdown machine if language L is context-free; a
   finite machine if it is regular. To complete the proof of cases 1 and 3, it suffices to
   convertthenewtransducerintoarecognizerofthetargetlanguageτ (L),bydeleting
   fromthemovesallthesourcecharacterswhilepreservingallthetargetones.Clearly,
   the machine thus obtained is of the same model as the recognizer of L.

   For case 2 essentially the same reasoning applies, but now the recognizer of the
   target language needs in general a pushdown memory. An example of case 2 is
   the translation (of course by means of a pushdown transducer) of a string u ∈ L =
   { a, b } ∗ to the palindromic image u u R , which clearly belongs to a context-free
   language.

   Case 4 is different because, as we know from Table2.9 on p.95, the intersection
   of two context-free languages L and L 1 is not always in the family CF. Therefore,

<a id="P427"></a>

   it is not certain that a pushdown automaton will be able to recognize the image of
   L computed by a pushdown translator. The next example (Example5.33) shows a
   situation of this kind.

   Example 5.33 (pushdown translation of a context-free language) To illustrate case
   4 of the translation closure table above, consider the translation τ of the following
   context-free language L:

      L =
      ?
      a n b n c ∗ | n ≥ 0
      ?
   to the language with three powers shown below (Example2.86 on p.93):

      τ (L) =
      ?
      a n b n c n | n ≥ 0
      ?

   which is not context-free, as we know. The image of translation τ is defined by
   the following translation grammar G τ , for brevity presented in EBNF but easily
   convertible to BNF:

      G τ : S →
      ?
      a
      a
      ? ∗
      X X →
      b
      b
      X
      c
      c
      | ε

   which constrains the numbers of characters b and c in a target string to be equal,
   whereas the equality of the number of a’s and b’s is externally imposed by the fact
   that any source string has to be in the language L. ?

5.6 SemanticTranslation
=======================

   None of the previous purely syntactic translation models is able to compute but
   the simplest transformations, because they rely on too elementary devices: finite
   and pushdown IO-automata. On the other hand, most compilation tasks need more
   involved translation functions.

   A first elementary example is the conversion of a binary number to decimal.
   Another typical case is the compilation of data structures to addresses: for example,
   a record declaration as
   BOOK : record
   AUT: char(8); TIT: char(20); PRICE: real; QUANT: int;
   end
   is converted to a table describing each symbol: type, dimensions in bytes, offset of
   eachfieldrelativetoabaseaddress.Assumingthebaseaddressoftherecordisfixed,
   say, at 3401, the translation is:

<a id="P428"></a>

   symbol type dimension address
   BOOK record 34 3401
   AUT string 8 3401
   TIT string 20 3409
   PRICE real 4 3429
   QUANT int 2 3433
   In both examples, to compute the translation we need some arithmetic functions
   which are beyond the capacity of pushdown transducers. Certainly it would not be
   viable to use more powerful automata, such as Turing machines or context-sensitive
   translation grammars, as we have argued (Chap.2 on p.105) that such models are
   alreadytoointricateforlanguagedefinition,nottomentionforspecifyingtranslation
   functions.

   A pragmatic solution is to encode the translation function in some programming
   languageorinamorerelaxedpseudo-code,asusedinsoftwareengineering.Toavoid
   confusion, such language is called compiler language or semantic metalanguage.
   The translator is then a program implementing the desired translation function.
   The implementation of a complex translation function would produce an intricate
   program,unlesscareistakentomodularizeit,inaccordancewiththesyntaxstructure
   of the language to be translated. This approach to compiler design has been used
   for years with varying degrees of formalization under the title of syntax-directed
   translation.Noticetheterm“directed”marksthedifferencefromthepurelysyntactic
   methods of previous sections.

   The leap from syntactic to semantic methods occurs when the compiler includes
   tree-walking procedures, which move along the syntax tree and compute some
   variables, called semantic attributes. The attribute values, computed for a given source
   text, compose the translation or, as it is customary to say, the meaning or semantics.
   A syntax-directed translator is not a formal model, because attribute computing
   procedures are not formalized. It is better classified as a software design method,
   based on syntactic concepts and specialized for designing input–output functions,
   such as the translation function of a compiler.

   We mention that formalized semantic methods exist, which can accurately 
   represent the meaning of programming languages, using logical and mathematical
   functions. Their study is beyond the scope of this book. 16
   A syntax-directed compiler performs two cascaded phases:
   1. Parsing or syntax analysis
   2. Semantic evaluation or analysis
   16 Formal semantic methods are needed if one has to prove that a compiler is correct; i.e., for any
   source text the corresponding image expresses the intended meaning. For an introduction to formal
   semantics, see for instance [6,7].


<a id="P429"></a>

   Phase 1 is well known: it computes a syntax tree, usually condensed into a so-called
   abstract syntax tree, containing just the essential information for the next phase. In
   particular, most source language delimiters are deleted from the tree.

   The semantic phase consists of the application of certain semantic functions, on
   each node of the syntax tree until all attributes have been evaluated. The set of
   evaluated attribute values is the meaning or translation.

   A benefit of decoupling syntax and semantic phases is that the designer has greater
   freedom in writing the concrete and abstract syntaxes. The former must comply with
   theofficial languagereference manual. Onthe other hand, theabstract syntax should
   beassimpleaspossible,provideditpreservestheessentialinformationforcomputing
   meaning. It may even be ambiguous: ambiguity does not jeopardize the single-value
   property of translation, because in any case the parser passes just one abstract syntax
   tree per sentence to the semantic evaluator.

   The above organization, termed two-pass compilation, is most common, but
   simpler compilers may unite the two phases. In that case there is just one syntax, the
   one defining the official language.


## 5.6.1 Attribute Grammars

   We need to explain more precisely how the meaning is superimposed on a 
   contextfree language. The meaning of a sentence is a set of attribute values, computed by
   the so-called semantic functions and assigned to the nodes of the syntax tree. The
   syntax-directedtranslatorcontainsthedefinitionofthesemanticfunctions,whichare
   associatedwiththegrammarrules.Thesetofgrammarrulesandassociatedsemantic
   functions is called an attribute grammar.

   To avoid confusion, in this part of the book a context-free grammar will be called
   syntax, reserving the term grammar to attribute grammars. For the same reason the
   syntactic rules will be called productions. In agreement with standard pratice, the
   context-free productions are in pure BNF.


### 5.6.1.1 IntroductoryExample

   Attribute grammar concepts are now introduced on a running example.

   Example 5.34 (converting a fractional binary number to base 10 (Knuth 17 )) The
   source language L, defined by the following regular expression:
   L = { 0, 1 } + • { 0, 1 } +
   is interpreted as the set of fractional base 2 numbers, with the point separating the
   integer and fractional parts. Thus the meaning of string 1101 • 01 is the number
   2 3 + 2 2 + 2 0 + 2 −2 = 8 + 4 + 1 +
   1
   4
   = 13.25 in base ten. The attribute grammar
   17 This historical example by D. E. Knuth introduced [8] attribute grammars as a systematization of
   compiler design techniques used by practitioners.


<a id="P430"></a>

   Table 5.3 Attribute grammar of Example5.34
      # Syntax Semantic
      functions
      Comment
      1 N → D • D v 0 :=
      v 1 + v 2 × 2 −l 2
   Add integer value
   to fractional value
   divided by weight
   2 l 2
   2 D → D B v 0 :=
   2 × v 1 + v 2
   l 0 := l 1 + 1 Compute value
   and length
   3 D → B v 0 := v 1 l 0 := 1
   4 B → 0 v 0 := 0 Value
   initialization
   5 B → 1 v 0 := 1
   is in Table5.3. The syntax is listed in column two. The axiom is N, nonterminal D
   stands for a binary string (integer or fractional part), and nonterminal B stands for a
   bit. In the third column we see the semantic functions or rules, which compute the
   following attributes:
   attribute meaning domain
   nonterminals that
   possess the attribute
   v value decimal number N, D, B
   l length integer D
   Asemanticfunctionneedsthesupport ofaproduction,andseveralfunctionsmaybe
   supportedbythesameproduction.Productions1,4and5supportonefunction,while
   productions2and3supporttwofunctions.Observenowthesubscriptoftheattribute
   instances, such as v 0 , v 1 , v 2 and l 2 , on the first row of the grammar. A subscript
   cross-references the grammar symbol possessing that attribute, in accordance with
   the following numbering convention 18 :
   N
   ????
   0
   → D
   ????
   1
   • D
   ????
   2
   stating that instance v 0 is associated with the left part N of the production rule,
   instancev 1 withthefirstnonterminaloftherightpart,etc.However,ifinaproduction
   a nonterminal symbol occurs exactly once, as N in the first production, the more
   expressive notation v N can be used instead of v 0 , without confusion.

   18 Alternatively,a more verbose styleisusedinothertexts,e.g., forthe firstfunction: v of N instead
   of v 0 .


<a id="P431"></a>

   N
   D • D
   D B D B
   B 0 B 1
   1 0
   v = 2.25
   v = 2
   l = 2
   v = 1
   l = 2
   v = 1
   l = 1
   v = 0
   v = 0
   l = 1
   v = 1
   v = 1 v = 0
   Fig.5.13 Decorated syntax tree of Example5.34
   Thefirstsemantic ruleassignstoattributev 0 avalue computedbythe expressions
   containing the attributes v 1 , v 2 , l 2 , which are the function arguments. We can write
   in functional form:
   v 0 := f (v 1 , v 2 , l 2 )
   We explain how to compute the meaning of a given source string. First we construct
   its syntax tree; then for each node we apply each function supported by the 
   corresponding production. The semantic functions are first applied to the nodes where the
   arguments of the functions are available. The computation terminates when all the
   attributes have been evaluated.

   The tree is then said to be decorated with attribute values. The decorated tree,
   which represents the translation or semantics of the source text 10 • 01, is shown
   in Fig.5.13. There are several possible orders or schedules for attribute evaluation:
   for a schedule to be valid, it must satisfy the condition that no function f is applied
   before the functions that return the arguments of f .

   In this example the final result of the semantic analysis is the attribute of the root,
   i.e., v = 2.25. The other attributes act as intermediate results. The root attribute is
   then the meaning of the source text 10 • 01. ?

## 5.6.2 Left and Right Attributes

   In the grammar of Example5.34, attribute computation essentially flows from
   bottom to top because an attribute of the left part (father) of a production rule is defined
   by a function having as arguments some attributes of the right part (children).

<a id="P432"></a>

   However in general, considering the relative positions of the symbols in the
   supporting production, the result and arguments of a semantic function may occur in
   various positions, to be discussed.

   Consider a function supported by a production and assigning a value to an attribute
   (result).Wenameleft (orsynthesized)theattributeifitisassociatedwiththeleftpart
   of the production. Otherwise, if the result is associated with a symbol in the right
   part of the production, we say it is a right (or inherited 19 ) attribute. By the previous
   definition, also the arguments of a function can be classified as left or right, with
   respect to the supporting production.

   To illustrate the classification, first we return to the grammar in Table5.3: both
   attributes v and l are left. Then, we show a grammar featuring both left and right
   attributes.

   Example 5.35 (breaking a text into lines—Reps [9]) A text has to be segmented into
   lines. The syntax generates a series of words separated by a space (written ⊥). The
   text has to be displayed in a window having a width of W ≥ 1 characters and an
   unbounded height, in such a way that each line contains a maximum number of
   leftaligned words and no word is split across lines. By hypothesis no word has length
   greater than W. Assume the columns are numbered from 1 to W.

   The grammar computes the attribute last, which identifies the column number
   of the last character of each word. For instance, the text “no doubt he calls me an
   outlaw to catch”with window width W = 13 is displayed as follows:
   1 2 3 4 5 6 7 8 9 10 11 12 13
   n o d o u b t h e
   c a l l s m e a n
   o u t l a w t o
   c a t c h
   Variable last takes value 2 for word no, 8 for doubt, 11 for he, …, and 5 for
   catch.

   The syntax generates lists of words separated by a blank space. The terminal
   symbol c represents any character. To compute the text layout, we use the following
   attributes:
   length the length of a word (left attribute)
   prec the column of the last character of the preceding word (right attribute)
   last the column of the last character of the current word (left attribute)
   To compute attribute last for a word, we must first know the column of the last
   character of the preceding word, denoted by attribute prec. For the first word of the
   text, the value of prec is set to −1.

   19 The word inherited is used by object-oriented languages in a totally unrelated sense.

<a id="P433"></a>

   Table 5.4 Attribute grammar of Example5.35
      # Syntax Right attributes Left attributes
      1 S 0 → T 1 prec 1 := −1
      2 T 0 → T 1 ⊥ T 2 prec 1 := prec 0
      prec 2 := last 1
      last 0 := last 2
      3 T 0 → V 1 last 0 := if (prec 0 + 1 + length 1 ) ≤ W
      then (prec 0 + 1 + length 1 )
      else length 1
      end if
      4 V 0 → c V 1 length 0 := length 1 + 1
      5 V 0 → c length 0 := 1
   Attribute computation is expressed by the rules of the attribute grammar in
   Table5.4. Two remarks on the syntax are: first, the subscripts added to 
   nonterminal symbols act as reference to the semantic functions, but they do not differentiate
   thesyntacticclasses;i.e.,theproductionswithandwithoutsubscriptsareequivalent.
   Second, the syntax has an ambiguity caused by production rule T → T ⊥T, which
   is bilaterally recursive (see Chap.2 on p.55). But the drawbacks an ambiguous
   syntax has for parsing do not concern us here, because the semantic evaluator receives
   exactly one parse tree to work on. The ambiguous syntax is more concise, and this
   reduces also the number of semantic rules. The length of a word V is assigned to the
   left attribute length in the rules associated with the last two productions. Attribute
   prec is a right one, because the value is assigned to a symbol in the right part of the
   first two productions. Attribute last is a left one; its value decorates the nodes with
   label T of a syntax tree and provides the final result in the root of the tree.
   To choose a feasible attribute evaluation schedule for a given syntax tree, we have
   toexaminethedependenciesbetweentheassignmentstatements.InFig.5.14theleft
   and right attributes are, respectively, placed to the left and the right of a node; the
   nodes are numbered for reference. To simplify drawing, the subtrees of nonterminal
   V are omitted, but attribute length, which is the relevant information, is present with
   its value.

   The attributes of a decorated tree can be viewed as the nodes of another directed
   graph, the (data) dependence graph. For instance, observe arc last(2) → prec(4)
   from last of T 2 to prec of T 4 : it represents a dependence of the latter attribute from
   the former, induced by function prec 2 := last 1 , which is supported by production 2.
   A function result has as many dependence arcs as it has arguments. Notice the arcs
   interconnect only attributes pertaining to the same production.

   Tocomputetheattributes,theassignmentsmustbeexecutedinanyordersatisfying
   theprecedencesexpressedbythedependencegraph.Attheend,thetreeiscompletely
   decorated with all the attribute values. ?

<a id="P434"></a>

   S 0
   last = 5 T 1 prec = −1
   last = 2 T 2 prec = −1 last = 5 T 4 prec = 2
   length = 2 V 3 last = 8 T 5 prec = 2 last = 5 T 7 prec = 8
   length = 5 V 6 last = 11 T 8 prec = 8 last = 5 T 10 prec = 11
   length = 2 V 9 length = 5 V 11
   no
   doubt
   he calls
   ←−
   Fig.5.14 Decorated tree with dependence graph for Example5.35
   An important quality of the attribute evaluation process is that the result is
   independent of the application order of functions. This property holds for grammars
   complying with certain conditions, to be considered soon.

   Usefulness of Right Attributes
   The grammar in Table5.4 uses both left and right attributes, so the questions arise:
   can we define the same semantics without using right attributes (as we did in the
   introductory Example5.3)? And then, is it convenient to do so?
   The first answer is yes in general, and we show it for Example5.35. The position
   ofthelastletterofawordcanbecomputedbyadifferentapproach.Initiallycompute
   the left attribute length, and then construct a new left attribute list, having as domain
   a list of integers representing word lengthes. For the tree in Fig.5.14, node T 7 , which
   covers the text he calls, would have the attribute list = ?2,5?. After processing all
   the nodes, the value list in the subroot T 1 of the tree is available, list = ?2, 5, 2, 5?.
   It is then straightforward, knowing the page width W, to compute the position of the
   last character of each word.

   But this solution is fundamentally bad, because the computation by the semantic
   function at the root of the tree has essentially the same complexity as the original text
   breaking problem: nothing has been gained by the syntax-directed approach, since
   the original problem has not been decomposed into simpler subproblems.

   Another drawback is that information is now concentrated in the root, rather than
   distributed on all the nodes by means of the right attribute last, as it was in the
   grammar in Table5.4.


<a id="P435"></a>

   Finally,tocounterbalancethesuppressionofrightattributes,itisoftennecessaryto
   introducenon-scalarattributes,suchaslistsorsets,orothercomplexdatastructures.
   Inconclusion,whendesigninganattributegrammarthemostelegantandeffective
   design is often obtained relying on both left and right attributes.


## 5.6.3 Definition of Attribute Grammar

   It is time to formalize the concepts introduced by previous examples. This is done
   in the next definition.

   Definition 5.36 (attribute grammar) An attribute grammar is defined as follows.
   1. A context-free syntax G = (V, Σ, P, S ), where V and Σ are the terminal and
   nonterminal sets, P the production rule set and S the axiom. It is convenient to
   avoid the presence of the axiom in the right parts of productions.

   2. A set of symbols, the (semantic) attributes, associated with nonterminal and
   terminal syntax symbols. The set of the attributes associated with symbol, e.g.,
   D, is denoted attr(D).

   The attribute set of a grammar is partitioned into two disjoint subsets, the left
   attributes and the right attributes.

   3. Each attribute σ has a domain, the set of values it may take.

   4. A set of semantic functions (or rules). Each function is associated with a 
   production rule:
   p: D 0 → D 1 D 2 ... D r r ≥ 0
   where D 0 is a nonterminal and the other symbols can be terminal or nonterminal.
   The production p is the syntactic support of the function. In general, several
   functions may have the same support.

   Notation: the attribute σ associated with a symbol D k is denoted by σ k or also by
   σ D if the syntactic symbol occurs exactly once in production p.

   A semantic function has the form:
   σ k := f (attr ({ D 0 , D 1 , ..., D r }) \ { σ k })
   where 0 ≤ k ≤ r; function f assigns to the attribute σ of symbol D k , the value
   computed by the function body; the arguments of f can be any attributes of the
   same production p, excluding the result of the function.

   Usually, the semantic functions are total functions in their domains. They are
   written in a suitable notation, termedsemantic metalanguage, such as a 
   programming language or a higher-level specification language, which can be formal or
   informal as a pseudo-code.

   Afunctionσ 0 := f (...)definesanattribute,qualifiedasleft,ofthenonterminal
   D 0 , which is the left part (or father or parent) of the production.


<a id="P436"></a>

   A function σ k := f (...) with k ≥ 1 defines an attribute, qualified as right, of a
   symbol (sibling or child) D k occurring in the right part.

   It is forbidden (as stated in 2.) for the same attribute to be left in a function and
   right in another one.

   Notice that since terminal characters never occur in the left part, their attributes
   cannot be of the left type. 20
   5. Consider the set fun(p) of all the functions supported by production p. They
   must satisfy the following conditions:
   a. For each left attribute σ 0 of D 0 , there exists in fun(p) exactly one function
   defining the attribute.

   b. For each right attribute δ 0 of D 0 , no function exists in fun(p) defining the
   attribute.

   c. For each left attribute σ i , where i ≥ 1, no function exists in fun(p) defining
   the attribute.

   d. Foreachattributeδ i ,wherei ≥ 1,thereexistsinfun(p)exactlyonefunction
   defining the attribute.

   The left attributes σ 0 and the right ones δ i with i ≥ 1 are termed internal for
   production p, because they are defined by functions supported by p.

   The right attributes δ 0 and left attributes σ i with i ≥ 1, are termed external for
   production p, because they are defined by functions supported by other 
   productions.

   6. Some attributes can be initialized with constant values or with values computed
   by external functions. This is often the case for the so-called lexical attributes,
   thoseassociatedwithterminalsymbols.Forsuchattributesthegrammardoesnot
   specify a computation rule. ?
   Example 5.37 (Example5.35 on p.432 continued) We refer again to the grammar in
   Table5.4 on p.433, where the attributes are classified as follows:
   left attributes length and last
   right attributes prec
   internal/external forproduction2theinternalattributesareprec 1 ,prec 2 andlast 0 ;
   the external ones are prec 0 and last 2 (attribute length is not
   pertinent to production 2)
   Then we have:
   attr(T) = { prec, last } attr(V) = { length } attr(S) = ∅
   20 In practice, the attributes of terminal symbols are often not defined by the semantic functions of
   thegrammar,butareinitializedwithvaluescomputedduringlexicalanalysis,whichisthescanning
   process that precedes parsing and semantic analysis.


<a id="P437"></a>

   We warn the reader against misuse of not local attributes. Item 4 of Definition5.36
   expressesasortofprincipleoflocalityofsemanticfunctions:itisanerrortodesignate
   as argument or result of a semantic function supported by p, an attribute which is
   not pertinent to production p. An instance of such error occurs in the modified rule
   2 below:
      # syntax semantic functions
      1 S 0 → T 1 ...

      2 T 0 → T 1 ⊥ T 2 prec 1 := prec 0 + length 0
      non-pertinent
      attribute
      3 … …
   Here the principle of locality is violated because length / ∈ attr(T): any attribute of
   a node, other than the father or a sibling, is out of scope. ?
   The rationale of the condition that left and right attributes are disjoint sets is discussed
   next. Each attribute of a node of the syntax tree must be defined by exactly one
   assignment; otherwise it would take two or more different values depending on the
   order of evaluation, and the meaning of the tree would not be unique. To prevent
   this, the same attribute may not be left and right, because in that case there would be
   two assignments, as shown in the fragment:
      # support semantic functions
      1 A → B C σ C := f 1 (attr(A, B))
      2 C → D E σ C := f 2 (attr(D, E))
      A
      B C σ =?
      D E
   Clearly variable σ C , internal for both productions, is a right attribute in the former,
   a left one in the latter. Therefore the final value it takes will depend on the order of
   function applications. Then the semantics loses the most desirable property of being
   independent of the implementation of the evaluator.


## 5.6.4 Dependence Graph and Attribute Evaluation

   An advantage of a grammar as a specification of a translation is that it abstracts from
   thedetailsoftree-traversingprocedures.Infact,theattributeevaluationprogramcan
   be automatically constructed from the functional dependencies between attributes,
   of course supposing that the bodies of the semantic functions are given.

   Toprepareforsuchconstruction,weformalizethefunctionaldependencies,within
   three progressively more comprehensive scopes, by means of the following directed
   graphs:

<a id="P438"></a>

   dependence graph of a semantic function
   the nodes of this graph are the arguments and result of the function
   considered, and there is an arc from each argument to the result
   dependence graph of a production p
   this graph, denoted by dep p , collects the dependence graphs for all the
   functions supported by the production considered
   dependence graph of a decorated syntax tree
   this graph already introduced (see Fig.5.14 on p.434) is obtained by
   pasting together the graphs of the individual productions that are used in the
   tree nodes.

   The next example (Example5.38) shows a case.

   Example 5.38 (dependence graphs) We reproduce from p.433 into Fig.5.15 the
   grammar of Example5.35. For clarity we lay each graph over the supporting
   production (dotted edges), to evidence the association between attributes and syntactic
   components. Production 2 is the most complex, with three semantic functions, each
   one with just one argument, hence one arc (visually differentiated by the style) of
   the graph. Notice, in the dependence graph of each production, that a node with
   (respectively without) incoming arcs is an attribute of type internal (respectively
   external).

   Thedependencegraphofa(decorated)syntaxtreewasalreadyshowninFig.5.14
   on p.434. ?

### 5.6.4.1 AttributeValues as Solution of Equations

   We expect each sentence of a technical language to have exactly one meaning, i.e.,
   a unique set of values assigned to the semantic attributes; otherwise we would be
   faced with an undesirable case of semantic ambiguity.

   We know that the values are computed by assignments and that there is exactly
   oneassignmentperattributeinstanceinthetree.Wemayviewthesetofassignments
   as a system of simultaneous equations, where the unknowns are the attribute values.
   From this perspective, the solution of the system is the meaning of the sentence.
   For a sentence, consider now the attribute dependence graph of the tree, and
   suppose it contains a directed path:
   σ 1 → σ 2 → ... → σ j−1 → σ j with j > 1
   whereeachσ k standsforsomeattributeinstance.Thenamesoftheattributeinstances
   can be the same or different. The corresponding equations are as follows:
   σ j = f j
   ?
   ..., σ j−1 , ...

   ?
   σ j−1 = f j−1
   ?
   ..., σ j−2 , ...

   ?
   ...

   σ 2 = f 2 (..., σ 1 , ...)

<a id="P439"></a>

   syntax support and semantic functions
      # syntax right attributes left attributes
      1 S 0 → T 1 prec 1 := −1
      2 T 0 → T 1 ⊥ T 2 prec 1 := prec 0
      prec 2 := last 1 last 0 := last 2
      3 T 0 → V 1 last 0 := if (prec 0 + 1 + length 1 ) ≤ W
      then (prec 0 + 1 + length 1 )
      else length 1
      end if
      4 V 0 → c V 1 length 0 := length 1 + 1
      5 V 0 → c length 0 := 1
      dependence graph dep 2 of production 2
      last T 0 prec
      last T 1 prec last T 2 prec
   dependence graphs of the remaining productions
   S 0
   last T 1 prec
   ↓
   last T 0 prec
   length V 1
   length V 0
   c length V 1
   length V 0
   c
   ↑
   Fig.5.15 Grammar of Example5.35 and dependence graphs of productions
   since the result of a function is an argument of the next one.

   For instance, in Fig.5.14 on p.434 one such path is the following:
   prec(T 1 ) → prec(T 2 ) → last(T 2 ) → prec(T 4 ) → prec(T 5 ) → ...


<a id="P440"></a>

   Revisiting the previous examples of decorated trees, it would be easy to verify that,
   for any sentence and syntax tree, no path of the dependence graph ever makes a
   circuit, to be formalized next.

   A grammar is acyclic if, for each sentence, the dependence graph of the tree 21 is
   acyclic. The next property (Property5.39) states when a grammar can be considered
   correct, of course disregarding any errors in the function bodies.

   Property 5.39 (correct attribute grammar) Given an attribute grammar satisfying
   theconditionsofDefinition5.36,consideranysyntaxtree.Iftheattributedependence
   graph of the tree is acyclic, the system of equations corresponding to the semantic
   functions has exactly one solution. ?
   Toprovetheproperty,weshowthat,undertheacyclicitycondition,theequationscan
   be ordered in such a way that each semantic function is applied after the functions that 
   computeitsarguments.Thisproducesavalueforthesolution,sincethefunctionsare
   total. The solution is clearly unique, as in a classical system of simultaneous linear
   equations.

   Let G = (V, E )beanacyclicdirectedgraph,andidentifythenodesbynumbers
   V = { 1, 2, ..., |V | }.Thenextalgorithm(Algorithm5.40)computesatotalorder
   ofnodes,calledtopological.Theresultord [1 ... |V |]isthevectorofsortednodes:
   it gives the identifier of the node that has been assigned to the ith position (1 ≤ i ≤
   |V |) in the ordering.

   Algorithm 5.40 (topological sorting)
   begin
   i := 0 - - initialize node counter i
   while V ?= ∅ do
   n := any node of set V without incoming arcs
   - - notice: such a node exists as graph G is acyclic
   i + + - - increment node counter i
   ord[i] := n - - put node n into vect. ord at pos. i
   V := V \ { n } - - remove node n from set V
   - - remove the dangling arcs from set E
   E := E \ { arcs going out from node n }
   end while
   end ?
   21 We assume as usual that the parser returns exactly one syntax tree per sentence.

<a id="P441"></a>

   In general, many different topological orders are possible, because the dependence
   graph typically does not enforce a total order relation.

   Example 5.41 (topologicalsorting)ApplyingthealgorithmtothegraphofFig.5.14
   on p.434, we obtain a topological order:
   length 3 , length 6 , length 9 , length 11 , prec 1 , prec 2 , last 2 , prec 4 ,
   prec 5 , last 5 , prec 7 , prec 8 , last 8 , prec 10 , last 10 , last 7 , last 4 , last 1
   ?
   Next, we apply the semantic functions in topological order. Pick the first node; its
   equation is necessarily constant; i.e., it initializes the result attribute. Then proceed
   by applying the next equations in the order, which guarantees availability of all
   arguments. Since all the functions are total, a result is always computed. The tree
   is thus progressively decorated with a unique set of values. Therefore for an acyclic
   grammar the meaning of a sentence is a single-valued function.

   Actually the above evaluation algorithm is not very efficient, because on one
   hand it requires computing the topological sort, and on the other hand it may require
   multiple visits of the same node of the syntax tree. We are going to consider more
   efficient algorithms, although less general, which operate under the assumption of a
   fixed order of visit (scheduling) of the tree nodes.

   Now consider what happens if the dependence graph of a tree contains a path that
   makes a circuit, implying that in the chain:
   σ 1 → σ 2 → ... → σ j−1 → σ j with j > 1
   twoelementsi andk with1 ≤ i < k ≤ j areidentical,i.e.,σ i = σ k .Thenthesystem
   ofequationsmayhavemorethanonesolution,andthegrammarmaybesemantically
   ambiguous.

   A remaining problem is how to check whether a given grammar is acyclic: how
   can we be sure that no decorated syntax tree will ever present a closed dependence
   path? Since the source language is usually infinite, the acyclicity test cannot be
   performed by the exhaustive enumeration of the trees. An algorithm to decide if an
   attribute grammar is acyclic exists but is complex 22 and not used in practice. It is
   more convenient to test certain sufficient conditions, which not only guarantee the
   acyclicity of a given grammar, but also permit constructing the attribute evaluation
   schedule, to be used by the semantic analyzer. Some simple yet practical conditions
   are described next.

   22 See Knuth [8,10]. The asymptotic time complexity is NP-complete with respect to the size of the
   attribute grammar.


<a id="P442"></a>


## 5.6.5 One-Sweep Semantic Evaluation

   A fast evaluator should compute the attributes of each tree node with a single visit
   or at worst with a small number of visits of the nodes. A well-known visit order of
   a tree is the depth-first traversal, which in many cases permits the evaluation of the
   attributes with just one sweep over the tree.

   Let N be a node of a tree and N 1 ,..., N r its child nodes; denote by t i the subtree
   rooted in node N i .

   A depth-first algorithm first visits the tree root. Then, in order to visit the generic
   subtreet N rootedinanode N,itrecursivelyproceedsasfollows.Itperformsadepth-
   first visit of the subtrees t 1 ,...,t r , in an order corresponding to some permutation
   of 1, 2,...,r, which not necessarily coincides with the natural order 1,2,...,r.
   This semantic evaluation algorithm, termed one-sweep, computes the attributes
   according to the following principles:
   • Before entering and evaluating a subtree t N , it computes the right attributes of
   node N (the root of the subtree).

   • At the end of visit of subtree t N , it computes the left attributes of N.
   Wehastentosaythatnotallthegrammarsarecompatiblewiththisalgorithm,because
   more intricate functional dependencies may require several visits of the same node.
   Theappealofthismethodisthatitisveryfast,andthatpractical,sufficientconditions
   for one-sweep evaluation are simple to state and to check on the dependence graph
   dep p of each production p.

   Experience with grammar design indicates it is often possible to satisfy the
   onesweep conditions, sometimes with minor changes to the original semantic functions.

### 5.6.5.1 One-Sweep Grammar

   Foreachproduction p: D 0 → D 1 D 2 ... D r ,withr ≥ 0,weneedtodefineabinary
   relationbetweenthesyntacticsymbolsoftherightpart,toberepresentedinadirected
   graph, called the sibling graph, denoted sibl p . The idea is to summarize the 
   dependenciesbetweentheattributesofthesemanticfunctionssupportedbytheproduction.
   The nodes of the sibling graph are the symbols { D 1 , D 2 , ..., D r } of the 
   production. The sibling graph has an arc:
   D i → D j with i ?= j and i, j ≥ 1
   if in the dependence graph dep p there is an arc σ i → δ j from an attribute of symbol
   D i to an attribute of symbol D j . We stress that the nodes of the sibling graph are
   not the same as those of the dependence graph of the production: the former are
   syntactical symbols, and the latter are attributes. Clearly, all the nodes (attributes)
   of dep p having the same subscript j are coalesced into one node D j of sibl p : in
   mathematical terms, the sibling graph is related to the dependence graph by a node
   homomorphism.


<a id="P443"></a>

   The next definition states the conditions that make a grammar suitable for
   onesweep semantic evaluation.

   Definition 5.42 (one-sweepgrammar)Agrammarsatisfiestheone-sweepcondition
   if, for each production p: D 0 → D 1 D 2 ... D r (with r ≥ 0) that has a dependence
   graph dep p , the following clauses hold:
   1. Graph dep p contains no circuit.

   2. Graph dep p does not contain a path:
   λ i → ... → ρ i with i ≥ 1
   that goes from a left attribute λ i to a right attribute ρ i of the same symbol D i ,
   where D i is a sibling.

   3. Graphdep p containsnoarcλ 0 → ρ i ,withi ≥ 1,fromaleftattributeofthefather
   node D 0 to a right attribute of a sibling node D i .

   4. The sibling graph sibl p contains no circuit. ?
   We orderly explain each item:
   1. This condition is necessary for the grammar to be acyclic (a requirement for
   ensuring existence and uniqueness of meaning).

   2. If we had a path λ i → ... → ρ i , with i ≥ 1, it would be impossible to compute
   therightattributeρ i beforevisitingsubtreet i ,becausethevalueoftheleftattribute
   λ i is available only after the visit of the subtree. This contravenes the depth-first
   visit order we have opted for.

   3. As in the preceding item, the value of attribute ρ i would not be available when
   we start visiting the subtree t i .

   4. This condition permits to topologically sort the child nodes, i.e., the subtrees
   t 1 ,...,t r , and to schedule their visit in an order consistent with the precedences
   expressed by graph dep p . If the sibling graph had a circuit, there would be
   conflicting precedence requirements on the order of visiting the sibling subtrees. In
   that case it would be impossible to find a schedule valid for all the attributes of
   the right part of production p.

   Algorithm 5.43 (construction of one-sweep evaluator) We write a semantic 
   procedure for each nonterminal symbol, having as arguments the subtree to be decorated
   and the right attributes of its root. The procedure visits the subtrees, and computes
   and returns the left attributes of the root (of the subtree).

   For each production p: D 0 → D 1 D 2 ... D r with r ≥ 0:
   1. Choose a topological order, denoted TOS, of the nonterminals D 1 , D 2 ,..., D r
   with respect to the sibling graph sibl p .

   2. For each symbol D i , with 1 ≤ i ≤ r, choose a topological order, denoted TOR,
   of the right attributes of symbol D i with respect to the dependence graph dep p .

<a id="P444"></a>

   λ μ D ρ
   λ A ρ λ B ρ λ C ρ σ
   f 1
   f 4
   f 2
   f 4 f 3
   f 5
   f 6
   f 5
   f 5
   Fig. 5.16 Dependence graph dep of rule D → A B C for Example5.44. The semantic function
   names are placed on dependence arcs
   3. Choose a topological order, denoted TOL, of the left attributes of symbol D 0 ,
   with respect to the dependence graph dep p .

   The three orders TOS, TOR and TOL together prescribe how to arrange the 
   instructions in the body of the semantic procedure, to be illustrated in the coming example
   (Example5.44). ?
   Example 5.44 (one-sweep semantic procedure) For brevity we consider a grammar
   fragment containing just one production and we leave the bodies of the semantic
   functionsunspecified.Production D → A B C hasthedependencegraphdepshown
   in Fig.5.16. It is straightforward to check that the graph satisfies conditions 1, 2 and
   3 of Definition5.42, because:
   1. there are neither circuits
   2. nor any path from a left attribute λ A , λ B or λ C to a right attribute, such as ρ B , of
   the same node
   3. nor any arc from a left attribute λ D or μ D to a right one of A, B or C
   4. the sibling graph sibl, below, is acyclic:
   A B C

<a id="P445"></a>

   We explain where its arcs come from: A → C comes from dependence λ A → ρ C
   and C → B from dependence ρ C → ρ B . Next, we compute the topological orders:
   • Sibling graph: TOS = A, C, B.

   • Right attributes of each sibling: since A and B have only one right attribute, the
   topological sorting is trivial; for C we have TOR = ρ, σ.

   • Left attributes of D: the topological order is TOL = λ, μ.

   To complete the design, it remains to list the instructions of the semantic procedure
   of this production, in an order compatible with the chosen topological orders. More
   precisely, the order of left attribute assignments is TOL, the order of procedure
   invocations(toevaluatesubtrees)isTOS,andtheorderofrightattributeassignments
   is TOR.

   The final semantic procedure follows:
   procedure D (in t, ρ D ; out λ D , μ D )
   - - t root of subtree to be decorated
   ρ A := f 1 (ρ D )
   - - abstract functions are denoted f 1 , f 2 , etc.

   A(t A , ρ A ; λ A )
   - - invocation of A to decorate subtree t A
   ρ C := f 2 (λ A )
   σ C := f 3 (ρ C )
   C (t C , ρ C , σ C ; λ C )
   - - invocation of C to decorate subtree t C
   ρ B := f 4 (ρ D , ρ C )
   B (t B , ρ B ; λ B )
   - - invocation of B to decorate subtree t C
   λ D := f 5 (ρ D , λ B , λ C )
   μ D := f 6 (λ D )
   end procedure ?
   To conclude, this method is very useful for designing an efficient recursive semantic
   evaluator, provided the grammar satisfies the one-sweep condition.


<a id="P446"></a>


## 5.6.6 Other Evaluation Methods

   One-sweep evaluation is practical, but some grammars have complicated 
   dependencies that prevent its use. More general classes of evaluators and corresponding
   grammar conditions are available, which we do not discuss. 23 To expand the scope
   of the one-sweep evaluation methods, we develop a rather intuitive idea. The
   evaluation process is decomposed into a cascade of two or more phases, each one of
   the one-sweep type, which operate on the same syntax tree, but on distinct sets of
   attributes.

   We describe the method focusing on two phases, but generalization is straightfor-
   ward. The attribute set Attr of the grammar is partitioned by the designer into two
   disjoint sets Attr 1 ∪ Attr 2 = Attr, to be, respectively, evaluated in phase one and
   two. Each attribute set, together with the corresponding semantic functions, can be
   viewed as an attribute subgrammar.

   Next, we have to check that the first subgrammar satisfies the general conditions
   of Definition5.36 (p.435) as well as the one-sweep condition of Definition5.42
   (p.443). In particular, the general condition imposes that every attribute is defined
   by some semantic function; as a consequence no attribute of set Attr 1 may depend
   on any attribute of set Attr 2 ; otherwise it would be impossible to evaluate the former
   attribute in phase one.

   Then, we construct the one-sweep semantic procedures for phase one, exactly as
   we would have done for a one-sweep grammar (as in Example5.44 above). After the
   execution of phase one of the semantic analyzer, all the attributes in set Attr 1 have a
   value, and it remains to evaluate the attributes of the second set.

   For phase two, we have again to check whether the second subgrammar meets
   the same conditions. Notice, however, that for the second evaluator, the attributes of
   set Attr 1 are considered as initialized constants. This means that the dependencies
   between two elements of Attr 1 , and between an element of Attr 1 and an element of
   Attr 2 , are disregarded when checking the conditions. In other words, only the 
   dependencies inside set Attr 2 need to be considered. The phase two evaluator operates
   on a tree decorated with the attributes of the first set and computes the remaining
   attributes in one sweep.

   The crucial point for multi-sweep evaluation to work is to find a good partition
   of the attributes into two (or more) sets. Then, the construction works exactly as in
   one sweep. Notice that not all the attribute values computed in phase one have to be
   storedinthedecoratedtreeproducedbyphaseone,butonlythoseusedasarguments
   by semantic functions are applied in the second sweep.

   As a matter of fact, designing a semantic evaluator for a rich technical language is
   a complex task, and it is often desirable to modularize the project in order to master
   the difficulty; some forms of modularization are discussed in [13]. Partitioning the
   23 Amongthemwementiontheevaluatorsbasedonmultiplevisits,ontheorderedattributegrammar
   (OAG) condition and on the absolute acyclicity condition. A survey of evaluation methods and
   corresponding grammar conditions is in Engelfriet [11,12].


<a id="P447"></a>

   global attribute set into subsets associated with evaluation phases offers a precious
   help for modularization. In practice, in many compilers the semantic analyzer is
   subdivided into phases of smaller complexity. For instance, the first stage analyzes
   the declarations of the various program entities (variables, types, classes, etc.) and
   the second stage processes the executable instructions of the programming language
   to be compiled.


## 5.6.7 Combined Syntax and Semantic Analysis

   Forfasterprocessing,itissometimespossibleandconvenienttocombinesyntaxtree
   construction and attribute computation, trusting the parser with the duty to invoke
   the semantic functions.

   In the following discussion we use pure BNF syntax for the support, because the
   use of EBNF productions makes it more complicate to specify the correspondence
   between syntax symbols and attributes.

   There are three typical situations for consideration, depending on the nature of
   the source language:
   • The source language is regular: lexical analysis with lexical attributes.
   • The source syntax is LL(k): recursive descent parser with attributes.

   • The source syntax is LR(k): shift-reduce parser with attributes.

   Next we discuss the enabling conditions for such combined syntax–semantic
   processors.


### 5.6.7.1 Lexical Analysis with Attribute Evaluation

   The task of a lexical analyzer (or scanner) is to segment the source text into the
   lexical elements, called lexemes or tokens, such as identifiers, integer or real
   constants and comments. Lexemes are the smallest substrings that can be invested with
   some semantic property. For instance, in many languages the keyword begin has
   the property of opening a compound statement, whereas its substring egin has no
   meaning.

   Each technical language uses a finite collection of lexical classes, as the ones just
   mentioned.Alexicalclassisaregularformallanguage:atypicalexampleistheclass
   of identifiers, defined by the regular expression of Example2.27 on p.29. A lexeme
   of an identifier class is a sentence belonging to the corresponding regular language.
   Inlanguagereferencemanuals,wefindtwolevelsofsyntacticspecifications:from
   lower to higher, the lexical and syntactic levels. The former defines the form of the
   lexemes. The latter assumes the lexemes are given in the text, and considers them
   to be the characters of the terminal alphabet of its syntax. Moreover, some lexemes
   may carry a meaning, i.e., a semantic attribute, which is computed by the scanner.

<a id="P448"></a>

   Lexical Classes
   Focusing on typical lexicons, we notice that some lexical classes, viewed as formal
   languages, have a finite cardinality. Thus, the reserved keywords of a programming
   language make a finite or closed class. An example is the class of keywords, 
   including:
   { begin, end, if, then, else, do, ..., while }
   Similarly, the number of arithmetic, boolean and relational operation signs is finite.
   Onthecontrary,identifiers,integerconstantsandcommentsarecasesofopenlexical
   classes, having unbounded cardinality.

   A scanner is essentially a finite transducer or IO-automaton (see Sect.5.5.2 on
   p.421). Its task is to divide the source text into lexemes and to assign an encoding to
   each one. In the source text the lexemes are separated, depending on their classes, by
   blank spaces or delimiters such as new-line. The transducer returns the encoding
   of each lexeme and removes the delimiters.

   More precisely, the scanner transcribes each lexeme into the target string as a
   pair of elements: the name, i.e., the encoding of the lexeme, and another semantic
   attribute, termed lexical.

   Lexical attributes change from a class to another and are altogether missing from
   certain classes.

   Some typical cases are:
   integer const the attribute is the value of the constant in base ten
   identifier the attribute is a key, to be used by the compiler for quickly locating
   the identifier in a symbol table
   comment a comment has no attribute, if the compilation framework does not
   manage program documentation, and throws away source program
   comments; if the compiler keeps and classifies comments, their
   lexical attribute is instrumental to retrieve them
   keyword has no semantic attribute, just a code for identification
   Unique Segmentation
   In a well-designed technical language, lexical definitions should ensure that, for any
   source text, the segmentation into lexemes is unique. A word of caution is necessary,
   for the concatenation of two or more lexical classes may introduce ambiguity. For
   instance, string beta237 can be divided into many ways into valid lexemes: a lexeme
   betaofclassidentifier followedby237ofclassinteger,oranidentifierbeta2followed
   by the integer 37, and so on.

   Inpractice,thissortofconcatenationambiguity(p.58)isoftencuredbyimposing
   to the scanner the longest prefix rule. The rule tells the scanner to segment a string
   x = u v into the lexemes, say, u ∈ identifier and v ∈ integer, in such a way that u is
   the longest prefix of x belonging to class identifier. In the example, the rule assigns
   the whole string beta237 to class identifier.


<a id="P449"></a>

   By this prescription, the translation is made single-valued and can be computed
   by a finite deterministic transducer, augmented with the actions needed to evaluate
   the lexical semantic attributes.

   Lexical Attributes
   We have observed that some lexical classes carry a semantic attribute and 
   different classes usually have different attribute domains. Therefore, at a first glance it
   would seem necessary to differentiate the semantic functions for each lexical class.
   However, it is often preferable to unify the treatment of lexical attributes as far as
   possible, in order to streamline the scanner organization: remember that a scanner
   has to be efficient because it is the innermost loop of the compiler. To this end, each
   lexical class is assigned the same attribute of type string, named ss, which contains
   the substring recognized as lexeme by the scanner.

   For instance, the attribute ss of identifier beta237 is just a string “beta237” (or a
   pointer thereto). Then the finite transducer returns the translation:
   ?class = identifier, ss = ‘beta237’?
   upon recognizing lexeme beta237. This pair clearly contains sufficient information,
   to pass as argument to a later invocation of an identifier-specific semantic function.
   The latter looks up string ss in the symbol table of the program under compilation;
   if it is not present, it inserts the string into the table and returns its position as a
   semantic attribute. Notice such identifier-specific semantic function is better viewed
   as a part of the attribute grammar of the syntax-directed translator, rather than of the
   scanner.


### 5.6.7.2 Attributed Recursive DescentTranslator

   Assume the syntax is suitable for deterministic top-down parsing. Attribute
   evaluation can proceed in lockstep with parsing if the functional dependencies of the
   grammar obey certain additional conditions beyond the one-sweep ones.

   We recall that the one-sweep algorithm (Definition5.42 on p.443) visits in 
   depthfirstorderthesyntaxtree,traversingthesubtreest 1 ,...,t r ,forthecurrentproduction
   D 0 → D 1 ... D r in an order that may be different from the natural one. The order
   is a topological sorting, consistent with the dependencies between the attributes of
   nodes 1,...,r.

   On the other hand, we know that a top-down parser constructs the tree in the
   natural order; i.e., subtree t j is constructed after subtrees t 1 ,...,t j−1 . It follows
   that, to combine the two processes, we must exclude any functional dependence that
   would enforce an attribute evaluation order other than the natural one, as stated next
   (Definition5.45).


<a id="P450"></a>

   Definition 5.45 (L-condition)] A grammar satisfies the condition named L 24 if, for
   each production p: D 0 → D 1 ... D r , it holds:
   1. The one-sweep condition 5.42 (p.443) is satisfied.

   2. The sibling graph sibl p contains no arc D j → D i with j > i ≥ 1. ?
   Noticethesecondclausepreventsarightattributeofnode D i todependonany(leftor
   right) attribute of a node D j placed to its right in the production. As a consequence,
   the natural order 1, …, r is a topological order of the sibling graph and can be
   applied to visit the sibling subtrees. The next property relates the L-condition and
   deterministic parsing.

   Property 5.46 (attributegrammaranddeterministicparsing)Letagrammarbesuch
   that:
   • ThesyntaxsatisfiestheLL(1)or,moregenerally,theLL(k)condition(seep.322).

   • The semantic rules satisfy the L-condition.

   Then, it is possible to construct a top-down deterministic parser with attribute
   evaluation, to compute the attributes at parsing time. ?
   Theconstructionofthesemanticevaluator,presentedinthecomingexample(Exam-
   ple5.47), is a straightforward combination of a recursive descent parser and an
   onesweep recursive evaluator.

   Example 5.47 (recursive descent parser with attribute evaluation) Revising
   Example5.34 (p.429), we write a grammar to convert a fractional number smaller than 1,
   from base two to base ten. The source language is defined by the regular expression
   below:
   L = • (0 | 1) ∗
   Themeaningofastring,say,•01isthedecimalnumber0.25.Thegrammarislisted
   in Table5.5. Notice that the value of a bit is weighted by a negative exponent, equal
   to its distance from the fractional point.

   The syntax is deterministic LL(2), as it can be checked. Next we verify condition
   L, production by production.

   N → • D The dependence graph has one arc v 1 → v 0 , hence:
   – There are no circuits in the graph.

   – There is no path from left attribute v to right attribute l of the same sibling.
   – In the graph there is no arc from attribute v of father to a right attribute l of a
   sibling.

   – The sibling graph sibl has no arcs.

   24 The letter L stands for left to right.


<a id="P451"></a>

   Table 5.5 Grammar for converting fractional numbers (Example5.47)
   Grammar
   Syntax Left attributes Right attributes
   N 0 → • D 1 v 0 := v 1 l 1 := 1
   D 0 → B 1 D 2 v 0 := v 1 + v 2 l 1 := l 0 l 2 := l 0 + 1
   D 0 → B 1 v 0 := v 1 l 1 := l 0
   B 0 → 0 v 0 := 0
   B 0 → 1 v 0 := 2 −l 0
   Attributes
   Attribute Meaning Domain Type Assoc. symbols
   v Value Real Left N, D, B
   l Length Integer Right D, B
   D → B D The dependence graph:
   v D l
   v B l v D l
   – has no circuit.

   – has no path from left attribute v to right attribute l of the same sibling.
   – has no arc from the left attribute v of the father to the right attribute v of a
   sibling.

   – The sibling graph has no arcs.

   D → B Same as above.

   B → 0 The dependence graph has no arcs.

   B → 1 The dependence graph has one arcl 0 → v 0 , which is compatible with one
   sweep, and there are not any brothers.

   Similar to a parser, the program comprises three procedures N, D and B, having
   as their arguments the left attributes of the father node. To implement parser
   lookahead, a procedure uses two variables to store the current character cc1 and the next

<a id="P452"></a>

   one cc2. Function read updates both variables. Variable cc2 determines the choice
   between the syntactic alternatives of D.

   procedure N (in ∅; out v 0 )
   if cc1 = ‘•’ then
   read
   else
   error
   end if
   l 1 := 1 - - initialize a local var. with right attribute of D
   D (l 1 , v 0 ) - - call D to construct a subtree and compute v 0
   end procedure
   procedure D (in l 0 ; out v 0 )
   case cc2 of
   ‘0’, ‘1’ : begin - - case of rule D → B D
   B (l 0 , v 1 )
   l 2 := l 0 + 1
   D (l 2 , v 2 )
   v 0 := v 1 + v 2
   end
   ‘?’ : begin - - case of rule D → B
   B (l 0 , v 1 )
   v 0 := v 1
   end
   otherwise error
   end case
   end procedure
   procedure B (in l 0 ; out v 0 )
   case cc1 of
   ‘0’ : v 0 := 0 - - case of rule B → 0
   ‘1’ : v 0 := 2 −l 0 - - case of rule B → 1
   otherwise error
   end case
   end procedure
   To activate the analyzer, the compiler invokes the axiom procedure. ?

<a id="P453"></a>

   Clearly, a skilled programmer could improve in several ways the previous schematic
   implementation.


### 5.6.7.3 Attributed Bottom-Up Parser

   Supposing the syntax meets the LR(1) condition, we want to combine 
   bottomup syntax tree construction with attribute evaluation. Some problems have to be
   addressed: how to ensure that precedences on semantic function calls induced by
   attribute dependencies are consistent with the order of tree construction; when to
   compute the attributes; and where to store their values.

   Considering first the problem of when semantic functions should be invoked, it
   turns out that right attributes cannot be evaluated during parsing, even assuming the
   grammar complies with the L-condition of Definition5.45 (that was sufficient for
   top-down evaluation in one sweep). The reason is that a shift-reduce parser defers
   the choice of the production until it has to perform a reduction, when the parser is in
   a macro-state containing a final state of a machine (or, what is the same, the marked
   production is D 0 → D 1 ... D r •). This is the earliest time the parser can choose the
   semantic functions to be invoked.

   The next problem comes from attribute dependencies. Just before reduction, the
   parserstackcontainsr elementsfromtop,whichcorrespondtothesyntacticsymbols
   of the production right part. Assuming the values of all the attributes of D 1 ... D r
   are available, the algorithm can invoke the functions and return the values of the left
   attributes of D 0 .

   But there is a difficulty for the evaluation of the right attributes of D 1 ... D r .
   Imagine that the algorithm is about to construct and decorate the subtree of D 1 . To
   be in accordance with the one-sweep evaluation, every right attribute ρ D 1 should be
   available before evaluating the subtree rooted at D 1 . But attribute ρ D 1 may depend
   on some right attribute ρ 0 of the father D 0 , which is unavailable, because the syntax
   treedoesnotyetcontaintheupperpart,includingthenodeassociatedwiththefather.
   The simplest way to circumvent this obstacle is to assume the grammar does not use
   right attributes. This ensures that the left attributes of a node will only depend on the
   left attributes of the children, which are available at reduction time.

   Coming to the question of memorization, the attributes can be stored in the stack,
   next to the items (m-states of the pilot machine) used by the parser. Thus, each
   stack element is a record, made of a syntactic field and one or more semantic fields
   containing attribute values (or perhaps pointers to values stored elsewhere); see the
   next example (Example5.48).

   Example 5.48 (calculating machine without right attributes) The syntax for certain
   arithmeticexpressionsisshownasanetworkinFig.5.17,anditspilotgraphisshown
   in Fig.5.18. In machine M E the final states of the alternative rule paths E → T and
   E → E + T areunifiedandsimilarlyin M T .ThissyntaxisBNF,andthepilotmeets

<a id="P454"></a>

   0 E 1 E 2 E 3 E E → →
   E +
   T
   T
   0 T 1 T 2 T 3 T
   T →
   →
   T
   a
   × a
   Fig.5.17 Machine net { M E , M T } for the arithmetic expressions (Example5.48)
   the LR(1) condition. 25 The following attribute grammar computes the expression
   value v or sets to true a predicate o in the case of overflow. Constant maxint is the
   largest integer the calculator can represent. A character a has an initialized attribute
   v with the value of the integer constant a. Both attributes are left:
   syntax semantic functions
   E 0 → E 1 + T 2
   o 0 := o 1 or (v 1 + v 2 > maxint) v 0 :=
   if o 0 then nil else (v 1 + v 2 )
   E 0 → T 1 o 0 := o 1 v 0 := v 1
   T 0 → T 1 × a
   o 0 := o 1 or (v 1 × value(a) > maxint)
   v 0 := if o 0 then nil else (v 1 × value(a))
   T 0 → a o 0 := false v 0 := value(a)
   We trace in Fig.5.19 a computation of the pushdown machine, extended with the
   semantic fields. The source sentence is a 3 + a 5 , where the subscript of a constant
   is its value. When the parser terminates, the stack contains attributes v and o of the
   root of the syntax tree. ?
   Right Attributes Independent of Father
   Actually, prohibition to use right attributes may badly complicate the task of writing
   anattributegrammar.Attributedomainsandsemanticfunctionsmayturnlesssimple
   and natural, although in principle we know any translation can be specified without
   using right attributes.

   25 See also Example5.15 on p.406, where the source syntax is the same, but the machine graphs are
   drawn as trees by keeping separate the final states of each alternative; the pilot in Fig.5.6 on p.407
   has more m-states but recognizes the same language as here.


<a id="P455"></a>

   syntax semantic functions
   E 0 → E 1 + T 2
   o 0 := o 1 or (v 1 + v 2 > maxint)
   v 0 := if o 0 then nil else (v 1 + v 2 )
   E 0 → T 1
   o 0 := o 1
   v 0 := v 1
   T 0 → T 1 × a
   o 0 := o 1 or (v 1 × value(a) > maxint)
   v 0 := if o 0 then nil else (v 1 × value(a))
   T 0 → a
   o 0 := false
   v 0 := value(a)
   3 T + ×
   2 E +
   0 T + ×
   2 T + ×
   0 E +
   0 T + ×
   1 E +
   3 E +
   1 T + ×
   →
   I 0
   I 1
   I 2
   I 3
   I 4
   I 5
   E
   +
   T
   a
   T
   a
   a
   ×
   Fig.5.18 Pilot graph of the arithmetic expressions (Example5.48 and Fig.5.17)
   Grammar expressivity improves if right attributes are readmitted, although with
   the following limitation on their dependencies (Definition5.49).

   Definition 5.49 (A-condition 26 for bottom-up evaluation) For each production
   p: D 0 → D 1 ... D r , the following must hold:
   1. The L-condition (p.450) for top-down evaluation is satisfied.

   2. No right attribute ρ D k of a sibling depends on a right attribute σ D 0 of the father,
   with 1 ≤ k ≤ r ?
   26 The letter A stands for ascending order.


<a id="P456"></a>

   stack string
   I 0 a 3 + a 5
   I 0 a 3 I 4 + a 5
   I 0
   T
   v = 3
   o = false
   I 3 + a 5
   I 0
   E
   v = 3
   o = false
   I 1 + a 5
   I 0
   E
   v = 3
   o = false
   I 1 + I 2 a 5
   I 0
   E
   v = 3
   o = false
   I 1 + I 2 a 5 I 4
   I 0
   E
   v = 3
   o = false
   I 1 + I 2
   T
   v = 5
   o = false
   I 3
   I 0
   E
   v = 3 + 5
   = 8
   o = false
   Fig.5.19 Shift-reduce parsing trace with attribute evaluation (Example5.48)
   Positivelystated,thesameconditionbecomes:arightattributeρ D k mayonlydepend
   on the right or left attributes of symbols D 1 ... D k−1 , with 1 ≤ k ≤ r.
   If a grammar meets the A-condition, the left attributes of nonterminal symbols
   D 1 , ..., D r areavailablewhenareductionisexecuted.Thustheremainingattributes
   can be computed in the following order:
   1. Right attributes of the same nonterminal symbols, in the order 1,2,...,r
   2. Left attributes of father D 0
   Notice this order differs from the scheduling of top-down evaluation, in that right
   attributes are computed later, during reduction.

   Finally,weobservethatthisdelayedevaluationgivesmorefreedomtocomputethe
   rightattributesinanorderotherthanthenaturalleft-to-rightordernecessarilyapplied
   by top-down parsers. This would allow to deal with more involved dependencies
   betweenthenodesofthesiblinggraph(p.443),similarlytotheone-sweepevaluators.

<a id="P457"></a>


## 5.6.8 Typical Applications of AttributeGrammar

   Syntax-directedtranslationiswidelyappliedincompilerdesign.Attributegrammars
   provideaconvenientmodularnotationforspecifyingthelargenumberoflocaloper-
   ations performed by the compiler, without actually getting into the implementation
   of the compiler itself. Since it would be too long to describe with some degree of
   completenessthesemanticanalysisoperationsforaprogramminglanguage,wesim-
   ply present a few typical interesting parts in a schematic manner. Actually, it is the
   case that the semantic analysis of programming languages comprises rather repetitive
   parts, but spelling them out in detail would not add to a conceptual understanding of
   compilation.

   We selected for presentation the following: semantic checks, code generation and
   the use of semantic information for making parsing deterministic.


### 5.6.8.1 Semantic Check

   The formal language L F defined by the syntax is just a gross approximation by
   excess to the actual programming (or more generally technical) language L T to be
   compiled;thatis,thesetinclusion L F ⊃ L T holds.Theleftmemberisacontext-free
   language,whiletherightoneisinformallydefinedbythelanguagereferencemanual.

   Formally speaking, language L T belongs to a more complex language family, the
   context-sensitive one. Without repeating the reasons presented at the end of Chap.3,
   a context-sensitive syntax cannot be used in practice, and formalization must be
   contented with the context-free approximation.

   Totouchthenatureofsuchapproximations,imagineaprogramminglanguage L T .

   The sentences of L F are syntactically correct, yet they may violate many 
   prescriptions of the language manual, such as type compatibility between the operands of an
   expression, agreement between the actual and the formal parameters of a procedure,
   and consistence between a variable declaration and its use in an instruction.
   A good way to check such prescriptions is by means of semantic rules that return
   boolean attributes called semantic predicates. A given source text violates a 
   semantic prescription if the corresponding semantic predicate turns out to be false after
   evaluating the attributes. Then the compiler reports a corresponding error, referred
   to as a static semantic error.

   Ingeneral,thesemanticpredicatesfunctionallydependonotherattributesthatrep-
   resent various program properties. For an example, consider the agreement between
   a variable declaration and its use in an assignment statement. In a typical program,
   declaration and use are arbitrarily distant substrings; therefore the compiler must
   store the type of the declared variable in an attribute, called symbol table or 
   environment. This attribute will be propagated through the syntax tree, to reach any node
   where the variable is used in an assignment or in another statement. However such
   a propagation is just a fiction, because if it were performed by copying the table,
   then it would be too inefficient. In practice the environment is implemented by a
   global data structure (or object), which is in the scope of all the concerned semantic
   functions.


<a id="P458"></a>

   The next attribute grammar (Example5.50) schematizes the creation of a symbol
   table and its use for checking the variables that are present in the assignments.
   Example 5.50 (symboltableandtypechecking)Theexamplecoversthedeclarations
   of scalar and vector variables to be used in the assignments. For the sake of the
   example, we assume the following semantic prescriptions have to be enforced:
   1. A variable may not be doubly declared.

   2. A variable may not be used before declaration.

   3. The only valid assignments are between scalar variables and between vector
   variables of identical dimension.

   The attribute grammar is shown in Table5.6. The syntax is in a quite abstract form
   and distinguishes the variable declarations from their uses. Attributes n and v are
   the name of a variable and the value of a constant, respectively. The symbol table
   is searched using as key the name n of a variable. For each declared variable, the
   symbol table contains a descriptor descr with the variable type (scalar or vector) and
   the vector dimension, if applicable. During its construction, the table is hosted by
   attribute t. Predicate dd denounces a double declaration. Predicate ai denounces a
   type incompatibility between the left and the right parts of an assignment. Attribute
   t, the symbol table, is propagated to the whole tree for the necessary local controls
   to take place.

   A summary of the attributes follows:
   attribute meaning domain type assoc. symbols
   n variable name string left D, id
   v constant value number left const
   dd double declaration boolean left D
   ai left/right parts incompatible boolean left A
   descr variable descriptor record left D, L, R
   t symbol table array of records right A, D, L, P, R
   The semantic analyzer processes a declaration D, and if the declared variable is
   alreadypresentinthesymboltable,itsetspredicatedd totrue.Otherwise,thevariable
   descriptorisconstructedandpassedtothefathernode,alongwiththevariablename.

   The left and right parts L and R of an assignment A have an attribute descr
   (descriptor) that specifies the type of each part: variable (indexed or not) or constant.
   If a name does not exist in the symbol table, the descriptor is assigned an error code.
   The semantic rules control the type compatibility of the assignment and return
   predicate ai. The control that the left and right parts of an assignment are compatible
   is specified in pseudo-code: the error conditions listed in items 2 and 3. make the
   predicate true.


<a id="P459"></a>

   Table 5.6 Grammar for checking the declaration of variables versus their use in the assignment
   statements (Example5.50)
   Syntax Semantic Functions Comment
   S → P t 1 := ∅ Initialize tab. to empty
   P → D P
   t 1 := t 0
   t 2 := insert (t 0 , n 1 , descr 1 )
   Propag. tab. to subtree
   add description to tab.

   P → A P
   t 1 := t 0
   t 2 := t 0
   Propag. tab. to subtrees
   P → ε No functions used here
   D → id
   dd 0 := present (t 0 , n id )
   if ¬dd 0 then
   descr 0 := ‘sca’
   end if
   n 0 := n id
   Declare scalar variable
   D → id [const]
   dd 0 := present (t 0 , n id )
   if ¬dd 0 then
   descr 0 := (‘vect’, v const )
   end if
   n 0 := n id
   Declare vector variable
   A → L := R
   t 1 := t 0
   t 2 := t 0
   ai 0 := ¬?descr 1 is compatible with descr 2 ?
   Propag. tab. to subtrees
   L → id descr 0 := ?type of n id in t 0 ?
   L → id
   ?
   id
   ?
   if
   ⎛
   ⎝

   type of n id 1 in t 0
   !
   = ‘vect’ and

   type of n id 2 in t 0
   !
   = ‘sca’
   ⎞
   ⎠ then
   descr 0 := ‘sca’
   else
   error
   end if
   Use indexed variable
   R → id descr 0 := ?type of n id in t 0 ? Use sca./vect. variable
   R → const descr 0 := ‘sca’ Use constant
   R → id
   ?
   id
   ?
   if
   ⎛
   ⎝

   type of n id 1 in t 0
   !
   = ‘vect’ and

   type of n id 2 in t 0
   !
   = ‘sca’
   ⎞
   ⎠ then
   descr 0 := ‘sca’
   else
   error
   end if
   Use indexed variable

<a id="P460"></a>

   For instance, in the syntactically correct text below:
   D 1
   a [10]
   D 2
   i
   D 3
   b
   A 4
   i := 4
   A 5 : ai=true
   c := a [i]
   D 6
   c[30]
   D 7 : dd=true
   i
   A 8 : ai=true
   a := c
   afewsemanticerrorshavebeendetectedinassignments A 5 , A 8 andinthedeclaration
   D 7 . ?
   Many improvements and additions would be needed for a real compiler, and we
   mention a few of them. First, to make diagnostic more accurate, it is preferable to
   separate various error classes, e.g., undefined variable, incompatible type and wrong
   dimension.

   Second, the compiler must tell the programmer the position (e.g., the line
   number) of each error occurrence. By enriching the grammar with other attributes and
   functions, it is not difficult to improve on diagnostic and error identification. In
   particular any semantic predicate, when it returns true in some tree position, can be
   propagated toward the tree root together with a node coordinate. Then in the root
   another semantic function will be in charge of writing comprehensive and readable
   error messages.

   Third, there are other semantic errors that are not covered by this example: for
   instancethecheckthateachvariableisinitializedbeforeitsfirstuseinanexpression,
   or that, if it is assigned a value, then it is used in some other statement. For such
   controls, compilers adopt a more convenient method instead of attribute grammars,
   called static program analysis, to be described at the end of this chapter.
   Finally,aprogramthathaspassedallthesemanticcontrolsincompilationmaystill
   produce dynamic or run-time errors when executed. See for instance the following
   program fragment:
   array a [10]; ... read(i); a [i] := ...

   The read instruction may assign to variable i a value that falls out of the interval
   1 ... 10, a condition clearly undetectable at compilation time.


### 5.6.8.2 Code Generation

   Since the final product of compilation is to translate a source program to a sequence
   of target instructions, their selection is an essential part of the process. The problem
   occurs in different settings and connotations, depending on the nature of the source
   and target languages, and on the distance between them. If the differences between
   the two languages are small, the translation can be directly produced by the parser,
   aswehaveseeninSect.5.4.1(p.391)fortheconversionfrominfixtopolishnotation
   of arithmetic expressions.

   On the other hand, it is much harder to translate a high-level language, say Java,
   to a machine language, and the large distance between the two makes it convenient
   to subdivide the translation process into a cascade of simpler phases. Each phase

<a id="P461"></a>

   translates an intermediate language or representation to another one. The first stage
   takes Java as source language, and the last phase produces machine code as target
   language.Compilershaveusedquiteavarietyofintermediaterepresentations:textual
   representations in polish form, trees or graphs, representations similar to assembly
   language, etc.

   An equally important goal of decomposition is to achieve portability with respect
   tothetargetandsourcelanguage.Inthefirstcase,portability(alsocalledretargeting)
   means the ease of modifying an existing compiler, when it is required to generate
   code for a different machine. In the second case, the modification comes from a
   change in the source language, say, from Java to FORTRAN. In both cases some
   phases of a multi-phase compiler are independent of the source or target languages,
   and can be reused at no cost.

   The first phase is a syntax-directed translator, guided by the syntax of, say, Java.
   The following phases select machine instructions and transform the program, in
   ordertomaximizetheexecutionspeedofthetargetprogram,tominimizeitsmemory
   occupationor,insomecases,toreducetheelectricenergyconsumption. 27 Noticethe
   first phase or phases of a compiler are essentially independent of the characteristic
   of the target machine; they comprise the so-called front-end compiler.

   The last phases are machine dependent and are called the back-end compiler. The
   back-end actually contains several subsystems, including at least a machine code
   selection module and a machine register allocation one. The same front-end
   compiler is usually interfaced to several back-end compilers, each one oriented toward a
   specific target machine.

   The next examples offer a taste of the techniques involved in translating from
   high-level to machine-level instructions, in the very simple case of control 
   instructions. In a programming language, control statements prescribe the order and choice
   of the instructions to be executed. Constructs like if then else and while do are 
   translated by the compiler to conditional and unconditional jumps. We assume the target
   languageoffersaconditionalinstructionjump-if-falsewithtwoarguments:aregister
   rc containing the test condition and the label of the instruction to jump to.
   In the syntax, the nonterminal L stands for a list of instructions. Clearly, each
   jump instruction may require a fresh label that differs from the already used labels:
   therefore, the translator needs an unbounded supply of labels. To create such new
   labels when needed, the compiler invokes a function fresh that at each invocation
   assigns a new label to the attribute n.

   The translation of a construct is accumulated in attribute tr, by concatenating
   (sign •) the translations of the constituents and by inserting jump instructions with
   newly created labels. Such labels have the form e_397, f_397, i_23, …, where the
   integer suffix is the number returned by function fresh. Register rc is designated by
   the homonymous attribute of nonterminal cond.

   27 Codeselectingphasesareoftendesignedbyusingspecializedalgorithmsbasedontherecognition
   of patterns on an intermediate tree representation. For an introduction to such methods, see, e.g.,
   the textbooks [14–16].


<a id="P462"></a>

   Table 5.7 Grammar for translating conditional if then else instructions to conditional jumps with
   labels (Example5.51)
   Syntax Semantic functions
   F → I n 1 := fresh
   I → if (cond)
   then
   L 1
   else
   L 2
   end if
   tr 0 := tr cond •
   jump-if-false rc cond ,e_n 0 •
   tr L 1 • jump f_n 0 •
   e_n 0 : •
   tr L 2 •
   f_n 0 :
   In the next examples (Examples5.51 and 5.52), we respectively illustrate these
   issues with conditional and iterative instructions.

   Example 5.51 (conditional instruction) The grammar of the conditional statement
   if then else, generated by nonterminal I, is shown in Table5.7. For brevity we omit
   the translation of a boolean condition cond and of the other language constructs. A
   complete compiler should include the grammar rules for all of them.

   We exhibit the translation of a program fragment, by assuming the label counter
   is set to n = 7:
   if (a > b) tr(a > b)
   then jump-if-false rc,e_7
   a := a − 1 tr(a := a − 1) jump f_7
   else e_7:
   a := b tr(a := b)
   end if f_7:
   … … – rest of the program
   Remember that tr(...) is the machine language translation of a construct. Register
   rc is not chosen here, but when the compiler translates expression a > b and selects
   a register to put the expression result into. ?
   Next, we show the translation of a loop with initial condition.


<a id="P463"></a>

   Table5.8 Grammar for translating iterative while do instructions to conditional jumps with labels
   (Example5.52).

   Syntax Semantic functions
   F → W n 1 := fresh
   W → while(cond)
   do
   L
   end while
   tr 0 := i_n 0 : • tr cond •
   jump-if-false rc cond ,f_n 0 •
   tr L • jump i_n 0 •
   f_n 0 :
   Example 5.52 (iterativeinstruction)Thegrammarforawhiledostatement,shownin
   Table5.8,isquitesimilartothatofExample5.51anddoesnotneedfurthercomments.

   Itsufficestodisplaythetranslationofaprogramfragment(assumingthatfunction
   fresh returns value 8):
   while (a > b) i_8: tr(a > b)
   do jump-if-false rc,f_8
   a := a − 1 tr(a := a − 1) jump i_8
   end while f_8:
   … … – rest of the program
   Other conditional and iterative statements, e.g., multi-way conditionals and loops
   with final conditions, require similar sets of compiler rules. ?
   However, the straightforward translations obtained are often inefficient and need
   improvement, which is done by the optimizing phases of the compiler. 28 A trivial
   example is the condensation of a chain of unconditional jumps into a single jump
   instruction.


### 5.6.8.3 Semantics-Directed Parsing

   In a standard compilation process, we know that parsing comes before semantic
   analysis: the latter operates on the syntax tree constructed by the former. However,
   in some circumstances, syntax is ambiguous and parsing cannot be successfully
   executed on its own, because it would produce too many trees and thus would puzzle
   28 The optimizer is by far the most complex and expensive part of a modern compiler; the reader is
   referred to, e.g., the textbooks [14–17].


<a id="P464"></a>

   thesemanticevaluator.Actually,thisdangerconcernsjustafewtechnicallanguages,
   because the majority is designed so that their syntax is deterministic. But in natural
   language processing the change of perspective is dramatic, because the syntax of
   human languages by itself is very ambiguous.

   We are here considering the case when the reference syntax of the source language
   is indeterministic or altogether ambiguous, so that a deterministic look-ahead parser
   cannot produce a unique parse of the given text. A synergic organization of syntax
   and semantic analysis overcomes this difficulty, as explained next.

   Focusingonartificialratherthannaturallanguages,areasonableassumptionisthat
   nosentenceissemanticallyambiguous;i.e.,everyvalidsentencehasauniquemean-
   ing. Of course, this does not exclude a sentence from being syntactically ambiguous.
   Butthentheuncertaintybetweendifferentsyntaxtreescanbesolvedatparsingtime,
   by collecting and using semantic information as soon as it is available.

   For top-down parsing, we recall that the critical decision is the choice between
   alternativeproductions,whentheirELL(1)guidesets(p.299)overlaponthecurrent
   input character. Now we propose to help the parser solve the dilemma by testing
   a semantic attribute termed a guide predicate, which supplements the insufficient
   syntactic information. Such a predicate has to be computed by the parser, which is
   enhanced with the capability to evaluate the relevant attributes.

   Notice this organization resembles the multi-sweep attribute evaluation method
   described on p.446. The whole set of semantic attributes is divided into two parts
   assignedforevaluationtocascadedphases.Thefirstsetincludestheguidepredicates
   and the attributes they depend on; this set must be evaluated in the first phase, during
   parsing. The remaining attributes may be evaluated in the second phase, after the,
   by now unique, syntax tree has been passed to the phase two evaluator.

   We recall the requirements for the first set of attributes to be computable at
   parsing time: the attributes must satisfy the L-condition (p.450). Consequently,
   the guide predicate will be available when it is needed for selecting one of the
   alternative productions, for expanding a nonterminal D i in the production D 0 →
   D 1 ... D i ... D r , with 1 ≤ i ≤ r. Since the parser works depth-first from left to
   right, the part of the syntax tree from the root down to the subtrees D 1 ... D i−1 is
   then available.

   Following the L-condition, the guide predicate may only depend on the right
   attributes of D 0 and on other (left or right) attributes of any symbol, which in the
   right part of the production precedes the root of the subtree D i under construction.
   The next example illustrates the use of guide predicates.

   Example 5.53 (a language without punctuation marks) The syntax of the historical
   Pascal-like language PLZ-SYS 29 did without commas and any punctuation marks,
   thus causing many syntactic ambiguities, in particular in the parameter list of a
   procedure. In this language a parameter is declared with a type and more parameters
   may be grouped together by type.

   29 Designed in the 1970’s for an eight-bit microprocessor with minimal memory resources.

<a id="P465"></a>

   Procedure P containsfiveidentifiersinitsparameterlist,whichcanbeinterpreted
   in three ways:
   P proc ( X Y T1 Z T2)
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   1. X has type Y and T1, Z have type T2
   2. X, Y have type T1 and Z has type T2
   3. X, Y, T1, Z have type T2
   Insightfully, the language designers prescribed that type declarations must come
   beforeproceduredeclarations.Forinstance,ifthetypedeclarationsoccurringbefore
   the declaration of procedure P are the following:
   type T1 = record ... end type T2 = record ... end
   then case 1 is excluded, because Y is not a type and T1 is not a variable. Similarly
   case 3 is excluded, and therefore the ambiguity is solved.

   It remains to be seen how the knowledge of the preceding type declarations can be
   incorporated into the parser, to direct the choice between the several possible cases.
   Within the declarative section D of language PLZ-SYS, we need to consider just two
   parts of the syntax: the type declarations T and the procedure heading I (we do not
   need to concern us here with procedure bodies). Semantic rules for type declarations
   will insert type descriptors into a symbol table t, managed as a left attribute. As in
   earlier examples, attribute n is the name or key of an identifier.

   Uponterminatingtheanalysisoftypedeclarations,thesymboltableisdispatched
   toward the subsequent parts of the program and, in particular, to the procedure
   heading declarations. For downward and rightward propagation, the left attribute t is
   (virtually)copiedintoarightattributetd.Thenthedescriptordescr ofeachidentifier
   allows the parser to choose the correct production rule.

   To keep the example small, we make drastic simplifications: the scope (or
   visibility) of the declared entities is global to the entire program; every type is declared
   as a record not further specified; we omit the control of double declarations; we do
   not insert in the symbol table the descriptors for the declared procedures and their
   arguments.

   ThegrammarfragmentislistedinTable5.9.Sincetypeandvariableidentifiersare
   syntactically undistinguishable, as both the type_id and var_id nonterminals expand
   to a generic identifier id, both the alternative rules of nonterminal V start with the
   string id id, because:
   V → var_id V - - starts with var_id followed by var_id
   V → var_id - - starts with var_id followed by type_id
   Therefore these alternative rules violate the LL(2) condition. Next the parser is
   enhanced with a semantic test, and consequently it is allowed to choose the correct
   alternative.


<a id="P466"></a>

   Table 5.9 Grammar for using type declaration for disambiguation (Example5.53)
   Syntax Semantic functions
   – declarative part – symbol table is copied from T to I
   D → T I td I := t T
   – type declaration – descriptor is inserted in table
   T → type id = record ... end T t 0 := insert(t 2 , n id , ‘type’)
   T → ε t 0 := ∅
   – procedure heading – table is passed to L and to I ≡ 3
   I → id proc ( L ) I td L := td 0 td 3 := td 0
   I → ε
   – parameter list – table is passed to V and to L ≡ 3
   L → V type_id L td V := td 0 td 3 := td 0
   L → ε
   – variable list (of same type) – table is passed to var_id and to V ≡ 2
   V → var_id V td 1 := td 0 td 2 := td 0
   V → var_id td 1 := td 0
   type_id → id
   var_id → id
   Let cc1 and cc2, respectively, be the current terminal character (or rather lexeme)
   and the next one. The guide predicates for each alternative rule are listed below:
      #
      production
      guide predicate
      1
      1’
      V → var_id V
      ?the descr. of cc2 in table td 0 ? ?= ‘type’ and
      ?the descr. of cc1 in table td 0 ? ?= ‘type’
      2
      2’
      V → var_id
      ?the descr. of cc2 in table td 0 ? = ‘type’ and
      ?the descr. of cc1 in table td 0 ? ?= ‘type’

<a id="P467"></a>

   The mutually exclusive clauses1 and 2 act as guide predicates, to select one 
   alternative out of 1 and 2. Clauses 1’ and 2’ are a semantic predicate controlling that the
   identifier associated with var_id is not a type identifier.

   Furthermore, it would be possible to add a semantic predicate to production L →
   V type_id L,in order to check whether thesort of type_id ≡ cc2 inthe table is equal
   to “type”. ?
   With such help from the values of the available semantic attributes, the top-down
   parser deterministically constructs the tree.

5.7 Static Program Analysis
===========================

   In this last part of the book we describe a technique for program analysis and
   optimization, used by all the compilers translating a programming language and also by
   many software engineering tools.

   Imagine that the front-end compiler has translated a program to an intermediate
   representation closer to the machine or the assembly language. The intermediate
   program is then analyzed by other compiler phases, the purpose and functionality of
   which differ depending on these circumstances:
   verification to further examine the correctness of the program
   optimization to transform the program into a more efficient
   version, for instance by optimally assigning machine
   registers to program variables
   scheduling and parallelizing to change the instruction order, for a better 
   exploitation of processor pipelines and multiple functional
   units,andforavoidingthatsuchresourcesbeattimes
   idle and at times overcommitted
   Although very different, such cases use a common representation of the program,
   called a control-flow graph, similar to a program flowchart. It is convenient to view
   this graph as describing the state-transition function of a finite automaton. Here
   our standpoint is entirely different from syntax-directed translation, because the
   automaton is not used to formally specify a programming language, but just for the
   givenprogramonwhichattentionisfocused.Astringrecognizedbythecontrol-flow
   automaton denotes an execution trace of that program, i.e., a sequence of machine
   operations.

   Static analysis consists of the study of certain properties of control-flow graphs,
   by using various methods that come from logic, automata theory and statistic. In our
   concise presentation we mainly consider the logical approach.


<a id="P468"></a>

   5.7.1 A Program as an Automaton
   Inaprogramcontrol-flowgraph,eachnodeisaninstruction.Atthisleveltheinstruc-
   tionsareusuallysimplerthaninahigh-levelprogramminglanguage,sincetheyarea
   convenient intermediate representation produced by the front-end compiler. Further
   simplifying matters, we assume the instruction operands are simple variables and
   constants; that is, there are not any aggregate data types. Typical instructions are
   assignments to variables, and elementary arithmetic, relational and boolean 
   expressions, usually with at most one operator.

   In this book we only consider intraprocedural analysis, meaning that the 
   controlflowgraphdescribesonesubprogramatatime.Moreadvancedstudies 30 areinterpro-
   cedural: they analyze the properties of a full program involving multiple procedures
   and their invocations.

   If the execution of an instruction p can be immediately followed by the execution
   of an instruction q, the graph has an arc directed from p to q. Thus an arc represents
   the immediate precedence relation between instructions: p is the predecessor, and
   q is the successor.

   The first instruction a program executes is the entry point, represented by the
   initial node of the graph. For convenience, we assume the initial instruction does not
   have any predecessors. On the other hand, an instruction having no successors is a
   program exit point or final node of the graph.

   Unconditional instructions have at most one successor. Conditional instructions
   havetwosuccessors(andmorethantwoforinstructionssuchastheswitch statement
   of the C language). An instruction with two or more predecessors is a confluence of
   as many arcs of the graph.

   A control-flow graph is not a faithful program representation, but just an 
   abstraction: it suffices for extracting the properties of interest, but some information is
   missing, as explained next.

   • The true/false value determining the successor of a conditional instruction such as
   if then else is typically not represented.

   • An unconditional go to instruction is not represented as a node, but simply as the
   arc to the successor instruction.

   • An operation (arithmetic, read, write, etc.) performed by an instruction is replaced
   by the following abstraction:
   – Avalueassignmenttoavariable,bymeansofastatementsuchasanassignment
   or a read instruction, is said to define that variable.

   – If a variable occurs in an expression, namely in the right part of an assignment
   statement, or in a boolean expression of a conditional, or in the argument list
   30 The already cited books on compiler techniques cover also static analysis, and a more specific
   reference is [18].


<a id="P469"></a>

   of a write instruction, we say the statement uses (or makes reference to) that
   variable.

   – Therefore, a node representing a statement p in the graph is associated with two
   sets: the set def(p) of defined variables and the set use(p) of used variables.
   Notice that in this model the actual operations performed by a statement, say,
   multiplication versus addition, are typically overlooked.

   Consider for instance a statement p: a := a ⊕ b, where ⊕ is an unspecified binary
   operator. The instruction is represented in the control-flow graph by a node carrying
   the following information:
   def(p) = { a } use(p) = { a, b }
   In this abstract model the statements read(a) and a := 7 are undistinguishable, as
   they carry the same associate information: def = { a } and use = ∅.

   In order to clarify the concepts and to describe some applications of the method,
   we present a more complete example (Example5.54).

   Example 5.54 (flowchart and control-flow graph) In Fig.5.20 we see a subprogram
   with its flowchart and its control-flow graph. In the abstract control-flow graph we
   need not list the actual instructions, but just the sets of defined and used variables.
   Instruction1hasnopredecessorsandisthesubprogramentryorinitialnode.Instruc-
   tion 6 has no successors and is the program exit or final node. Node 5 has two
   successors, whereas node 2 is at the confluence of two predecessors. The sets use(1)
   and def(5) are empty. ?
   Language of Control-Flow Graph
   We consider the finite automaton A, represented by a control-flow graph. Its 
   terminal alphabet is the set I of program instructions, each one schematized by a triple
   ?label,defined variables, used variables? such as:
   ?2, def(2) = { b }, use(2) = { a }?
   For the sake of brevity, we often denote such instruction by the first component
   only, i.e., the instruction label or number. Notice that two nodes containing the same
   instruction, say x := y + 5, are made distinct by their labels.

   We observe that the arcs are unlabeled and all the information is attached to the
   nodes. This should remind us of the local automata studied on p.152, where the
   terminal characters of the alphabet are written inside the states. Clearly, all the arcs
   entering the same node “read” the same character. Since each state is identified by
   the instruction label, we do not have to assign it an explicit name; the same as in the
   syntax charts on p.237.

   The initial state is marked by an entering arrow (dart). The final states are those
   without a successor.


<a id="P470"></a>

   program
   a := 1
   e 1: b := a + 2
   c := b + c
   a := b × 3
   if a < m goto e 1
   return c
   flowchart program control-flow graph A
   a := 1
   b := a + 2
   c := b + c
   a := b × 3
   a < m
   return c
   ↓
   1
   2
   3
   4
   5
   6
   false
   true
   def(1) = { a }
   def(2) = { b } use(2) = { a }
   def(3) = { c } use(3) = { b, c }
   def(4) = { a } use(4) = { b }
   use(5) = { a, m }
   use(6) = { c }
   ↓
   1
   2
   3
   4
   5
   6
   Fig.5.20 Program, flowchart and abstract control-flow graph of Example5.54
   Theformallanguage L (A)recognizedbytheautomatoncontainsthestringsover
   alphabet I that label a path from the entry node to an exit node. Such path denotes a
   sequence of instructions, which may be executed when the program is run.

   Obviously,eachnodenumberisdistinctsinceitcorrespondstoadifferentinstruc-
   tionlabel.Thisconfirmsthattheformallanguage L (A)belongstothefamilyoflocal
   languages (Definition3.28 on p.151), which are a rather restricted subset of the
   regular language family REG.

   In the previous example, the alphabet is I = { 1, 2, 3, 4, 5, 6 }. A recognized
   path is the following:
   1 → 2 → 3 → 4 → 5 → 2 → 3 → 4 → 5 → 6 ≡ 1234523456

<a id="P471"></a>

   The set of such recognized paths is the language L (A) = 1 (2345) + 6.

   Conservative Approximation
   Actually,theautomatonspecifiesonlyanapproximationofthevalidexecutionpaths
   of a program. Not all the recognized paths are really executable by the program,
   because our model disregards the boolean condition that selects a successor node of
   a conditional statement. A trivial example is the following program fragment:
   1: if a ∗ ∗2 ≥ 0 then 2: istr 2 else 3: istr 3
   wheretheformallanguageacceptedbytheautomatoncontainstwopaths{ 12, 13 },
   but path 13 is not executable because a square is never negative.

   As a consequence of such an approximation, static analysis may sometimes reach
   pessimisticconclusions:inparticular,itmaydiscovererrorsinanever-executedpath.
   Of course, it is in general undecidable whether a path of a control-flow graph will
   ever be executed, because this would be equivalent to deciding whether there exists
   a value assignment to the input variables that causes the execution of that path. The
   latter problem can be reduced to the halting problem of a Turing machine, which is
   undecidable.

   As it is generally impossible to know which program paths are executable or not,
   it would be much worse if the static analysis erred by disregarding some path that
   turns out to be executable, because then it may fail to detect some real errors.
   In conclusion, the decision to examine all the recognized paths (from the initial
   node to a final one) is a conservative approximation to program analysis, which may
   causethediagnosisofnon-existingerrorsortheprudentialassignmentofunnecessary
   resources, but it never misses real error conditions or real resource requirements.
   A usual hypothesis in static analysis is that the automaton is clean (p.124); i.e.,
   each instruction is on a path from the initial node to a final one. Otherwise one or
   more of the following anomalies may occur in the program: some executions never
   terminate, or some instructions are never executed (the program contains so-called
   unreachable code).


## 5.7.2 Liveness Intervals of aVariable

   A professional compiler performs several passes of analysis over the intermediate
   representationsofaprograminordertoimproveit.Averyinterestinganalysis,which
   allows a variety of profitable optimizations, is the study of the liveness intervals of
   program variables.

   Definition 5.55 (variable liveness) A variable a is live on the exit from a program
   node p, if in the program control-flow graph there exists a path from p to a node q.
   not necessarily distinct from p, such that:
   • The path does not traverse an instruction r, with r ?= q, that defines variable a,
   i.e., such that a ∈ def(r).


<a id="P472"></a>

   • Instruction q uses variable a, i.e., a ∈ use(q). ?
   For brevity we say that the variable is live-out of node p. In other words, a variable
   is live-out of a certain node if some instruction that may be successively executed
   makes use of the value the variable has in the former node.

   Tograspthepurposeofthisdefinition,imaginethatinstruction pistheassignment
   a := b ⊕ c,andsupposewewanttoknowifsomeinstructionmakesuseofthevalue
   assignedtothevariablea in p.Thequestioncanberephrasedas:isvariablea live-out
   ofnode p?Ifnot,theassignment p isuselessandcanbedeletedwithoutaffectingthe
   program semantics. Furthermore, if none of the variables used by p is used in some
   otherinstruction,alltheinstructions assigningavaluetosuch variablesmaybecome
   useless after deleting instruction p. The next example (Example5.56) illustrates the
   situation.

   Example 5.56 (Example5.54 continued) For the example of Fig.5.20 on p.470, we
   reproduce in Fig.5.21 the program control-flow graph with the live variable sets for
   each arc, also referred to as a program point. Observe in the picture the variables
   that are live-in each program point. Thus variable c is live on the entrance to node 1,
   because there exists path 123 such that c ∈ use(3), and neither node 1 nor node 2
   defines c.

   It is customary to say that variable a is live-in the intervals (i.e., paths) 12 and
   452; it is not live-in the intervals 234 and 56, and so on.

   Fig.5.21 Control-flow
   graph of the program with
   the sets of variables live-out
   of the nodes (Example5.54)
   a := 1
   b := a + 2
   c := b + c
   a := b × 3
   a < m
   return c
   { c, m }
   ↓
   1
   2
   3
   4
   5
   6
   { a, c, m }
   { b, c, m }
   { b, c, m }
   { a, c, m }
   { c }
   { a, c, m }

<a id="P473"></a>

   More precisely, we say that a variable is live-out for a node if it is live on any arc
   outgoing from the node. Similarly, a variable is live-in for a node if it is live on some
   arc entering the node. For instance, the variables { a, c, m } ∪ { c } are live-out for
   node 5. ?

### 5.7.2.1 Computing Liveness Intervals

   Let I be the instruction set. Let D (a) ⊆ I and U (a) ⊆ I, respectively, be the sets
   of the instructions that define and use some variable a. For instance, in the running
   example (Fig.5.20 on p.470) it is D (b) = { 2 } and U (b) = { 3, 4 }. The liveness
   condition will be first expressed in terms of formal language operations and then by
   means of a more expressive set-theoretical notation.

   It is not difficult to see that variable a is live-out for node p if, and only if,
   for the language L (A) accepted by the automaton, the following condition holds:
   language L (A) contains a sentence x = u pv q w, where u and w are (possibly
   empty) arbitrary instruction sequences, p is any instruction, v is a possibly empty
   instruction sequence not containing a definition of a and instruction q uses variable
   a. The above conditions are formalized as follows:
   u, w ∈ I ∗ ∧ p ∈ I ∧ v ∈
   ?
   I \ D (a)
   ? ∗
   ∧ q ∈ U (a) (5.4)
   Observe again that the set difference contains all the instructions that do not define
   variable a, whereas instruction q uses a.

   The set of all the strings x that meet condition (5.4), denoted as L p , is a subset
   of the language L (A) recognized by A, i.e., L p ⊆ L (A). Moreover language L p is
   regular, because it can be defined by the following intersection:
   L p = L (A) ∩ R p (5.5)
   wherelanguage R p isregularandisdefinedbytheextended(i.e.,withasetdifference)
   regular expression R p below:
   R p = I ∗ p
   ?
   I \ D (a)
   ? ∗
   U (a) I ∗ (5.6)
   Formulas (5.5) and (5.6) prescribe that letter p must be followed by a letter q taken
   from set U (a), and that all the letters (if any) intervening between p and q must not
   belong to set D (a).

   It follows that in order to decide whether variable a is live-out of node p, one has
   to check that language L p is not empty. We know one way of doing it: we construct
   therecognizeroflanguage L p ,thatis,theproductmachinefortheintersection (5.5),
   as explained in Chap.3 on p.190. If this machine does not contain any path from the
   initial state to a final one, then language L p is empty.

   Anyway, such a procedure is not practical, when taking into account the large
   dimension of the real programs to be analyzed. Therefore we introduce another
   specialized method, which not only performs more efficiently, but computes at once all

<a id="P474"></a>

   the live variables in all the program points. The new method systematically
   examines all the paths from the current program point to some instruction that uses some
   variable.

   The liveness computation will be expressed as a system of data-flow equations.
   Consider a node p of a control-flow graph or program A. A first equation expresses
   the relation between the variables live-out live out (p) and those live-in live in (p). A
   second equation expresses the relation between the variables live-out of a node and
   those live-in for its successors.

   We denote by succ(p) the set of the (immediate) successors of node p and by
   var(A) the set of all the variables of program A.

   Data-Flow Equations
   For each final (exit) node p:
   live out (p) = ∅ (5.7)
   For any other node p:
   live in (p) = use(p) ∪
   ?
   live out (p) \ def(p)
   ?
   (5.8)
   live out (p) =
   ?
   ∀q ∈succ(p)
   live in (q) (5.9)
   Comments:
   • In Eq.(5.7) no variable is live-out of any node of the (sub)program graph. Here
   we have disregarded the output parameters (if any) of the subprogram, which
   are typically used after exiting the subprogram and thus can be considered to be
   live-out of the final node, though in another subprogram.

   • For Eq.(5.8) a variable is live-in for p if it is used in p or if it is live-out for p but
   not defined by p. Consider instruction 4: a := b × 3 (Fig.5.20 on p.470). Out of
   4,variablesa,m andc arelive,becauseforeachonethereexistsapaththatreaches
   a use of that variable without traversing a node that defines the same variable. On
   entering node 4, the following variables are live: b because it is used in 4; and c
   andm becausetheyarelive-outfor4andnotdefinedin4.Onthecontrary,variable
   a is not live-in for 4 because it is defined in 4, though it is live-out for 4.
   • For Eq.(5.9), node 5 has successors 2 and 6; then the variables live-out for 5 are
   those (namely a, c and m) live-in for 2 and the one (namely c) live-in for 6.

### 5.7.2.2 Solution of Data-FlowEquations

   Given a control-flow graph, it is straightforward to write the two Eqs.(5.8) and
   (5.9) for each instruction. For a graph with | I | = n ≥ 1 nodes, the resulting system
   has 2 × n equations with 2 × n unknowns, i.e., live in (p) and live out (p) for each
   instruction p ∈ I.Eachunknownisasetofvariables,andthesolutiontobecomputed
   is a pair of vectors, each one containing n sets.


<a id="P475"></a>

   Tosolvetheequationsystemweuseiteration,bytakingtheemptysetastheinitial
   approximation (with i = 0) for every unknown:
   ∀ p ∈ I live in (p) = ∅ live out (p) = ∅
   Let i ≥ 0 be the current iteration. In each equation of the system (5.8), (5.9), we
   replace the unknowns occurring in the right-hand sides with the values of the current
   iteration,andthusweobtainthevaluesofnextiterationi + 1.Ifatleastoneunknown
   differs from the previous iteration, then we execute one more iteration; otherwise we
   terminate and the last vector pair computed is a solution of the equation system.
   This solution is termed the least fixed point of the transformation that computes
   a new vector from one of the preceding iterations.

   To see why a finite number of iterations always suffices to converge to the least
   fixed-point solution, observe the following:
   • The cardinality of every set live in (p) and live out (p) is bounded by the number of
   program variables |var(A)|.

   • Everyiterationmayonlyaddsomevariablestosomesetsorleavethemunchanged,
   but it never removes any variable from a set; in other words, the transformation is
   monotonic nondecreasing with respect to set inclusion.

   • If an iteration does not change any set, the algorithm terminates.

   We illustrate the algorithm on the running example.

   Example 5.57 (running example (Example5.54) continued: iterative computation
   of live variables) First, we compute by inspection the sets of instructions that define
   (D) and use (U) program variables:
   var. D U
   a 1, 4 2, 5
   b 2 3, 4
   c 3 3, 6
   m ∅ 5
   Next, the equations for the program (Fig.5.20 on p.470) are written in Table5.10.
   The names of the unknowns are shortened to in(p) and out(p) instead of live in (p)
   and live out (p), respectively. Then we compute and tabulate the successive 
   approximations, starting from the empty sets; at each iteration we first compute thein values
   and then the out values. The least fixed point is reached after five iterations: it would
   be easy to verify that one more iteration would not change the last result. ?
   It is important to note that the convergence speed to the fixed point is very sensitive
   to the order, although the solution does not depend on the processing order of nodes.

<a id="P476"></a>

   Table 5.10 Liveness equations of the program in Fig.5.20 (Example5.57)
   Equations
   1 in(1) = out(1) \ {a } out(1) = in(2)
   2 in(2) = {a } ∪ (out(2) \ {b}) out(2) = in(3)
   3 in(3) = {b, c} ∪ (out(3) \ {c}) out(3) = in(4)
   4 in(4) = {b} ∪ (out(4) \ {a }) out(4) = in(5)
   5 in(5) = {a, m } ∪ out(5) out(5) = in(2) ∪ in(6)
   6 in(6) = {c} out(6) = ∅
   Unknowns computed at each iteration
   in = out in out in out in out in out in out
   1 ∅ ∅ a ∅ a,c c a,c c a,c,m c,m a,c,m
   2 ∅ a b,c a,c b,c a,c b,c,m a,c,m b,c,m a,c,m b,c,m
   3 ∅ b,c b b,c b,m b,c,m b,c,m b,c,m b,c,m b,c,m b,c,m
   4 ∅ b a,m b,m a,c,m b,c,m a,c,m b,c,m a,c,m b,c,m a,c,m
   5 ∅ a,m a,c a,c,m a,c a,c,m a,c a,c,m a,c,m a,c,m a,c,m
   6 ∅ c ∅ c ∅ c ∅ c ∅ c ∅
   We mention the time complexity of the iterative algorithm. 31 The worst-case
   complexity is O (n 4 ), where n is the number of nodes, i.e., of instructions of the
   subprogram. In practice, for many realistic programs the computational complexity
   is close to linear in time.


### 5.7.2.3 Application of Liveness Analysis

   We show two classical widespread applications of the previous analysis: memory
   allocation for variables and detection of useless instructions.

   Memory Allocation
   Liveness analysis is best applied to decide if two variables can reside in the same
   memory cell (or in the same machine register). It is evident that if two variables are
   live-in the same program point, both values must be present in the memory when
   execution reaches that point, because they may have future uses. Therefore their
   values cannot reside in the same cell: we then say the two variables interfere.
   Conversely, if two variables do not interfere, that is, they are never live-in the
   same program point, then the same memory cell or register can be used to keep their
   values.

   31 For a proof refer for instance to any of [14,15,17].


<a id="P477"></a>

   Example 5.58 (interference and register assignment) In the control-flow graph of
   Fig.5.21 on p.472, we see that variables a, c and m occur in the same set live in (2);
   therefore the three variables pairwise interfere. Similarly, the variable pairs (b, c),
   (b, m) and (c, m) interfere in the set live in (3). On the other hand, no set contains
   variables a and b, which therefore do not interfere.

   As stated before, two interfering variables must reside in different memory cells.
   It follows that each one of the variables c and m needs a separate cell, while both
   variables a and b may reside in the same cell, which has to be different from the
   previous two cells because variable a interferes with c and m. In conclusion we have
   found that three cells suffice to store the values of four program variables. ?
   Currentcompilersoptimallyassignregisterstoprogramvariablesbymeansofheuris-
   tic methods, by relying on the interference relation.

   Useless Definition
   An instruction defining a variable is useless if the value assigned to the variable
   is never used by any instruction. This is tantamount to saying that the value is not
   live-out for the defining instruction. Therefore, to verify that a definition of variable
   a by an instruction p is not useless, we have to check whether variable a is present
   in the set live out (p).

   TheprogramofFig.5.20onp.470doesnothaveanyuselessdefinitions,incontrast
   with the next example.

   Example 5.59 (useless variable definition) Consider the program in Fig.5.22. The
   picture lists the live variables in and out of each instruction. Variable c is not
   liveout for node 3; hence instruction 3 is useless. Useless instructions can be erased by
   the compiler. The elimination of instruction 3 brings two benefits: the program is
   shorter and faster to execute, and variable c disappears from the sets in(1), in(2),
   in(3) and out(5). This reduces the interferences between variables and may bring a
   reduction in the number of registers needed, which is often a bottleneck for program
   performance. ?
   This is an example of the frequently occurring phenomenon of chain reaction
   optimizations triggered by a simple program transformation.


## 5.7.3 Reaching Definition

   Another basic and widely applied type of static analysis is the search for variable
   definitions that reach some program point.

   To introduce the idea by an application, consider an instruction that assigns a
   constant value to variable a. The compiler examines the program to see if the same
   constant can be replaced for the variable in the instructions using a. The benefit
   of the replacement is manyfold. First, a machine instruction having a constant as
   operand (a so-called immediate operand) is often faster. Second, substituting with a

<a id="P478"></a>

   Fig.5.22 Control-flow
   graph with live sets applied
   to detect the useless
   definitions in the program
   (Example5.59)
   a := m
   b := a + 2
   c := b + c
   c := b × 3
   a < m + c
   return b
   { c, m }
   ↓
   1
   2
   3
   4
   5
   6
   { a, c, m }
   { a, b, c, m }
   { a, b, m }
   { a, b, c, m }
   { b }
   { a, c, m }
   constant a variable occurring in an expression may produce an expression where all
   the operands are constant. Then, the expression value can be computed at compile
   time, with no need to generate any machine code for it. Lastly, since the replacement
   eliminates one or more uses of a, it shortens the liveness intervals and reduces
   the interferences between variables; thus the pressure on the processor registers is
   consequently reduced, too.

   The above transformation is termed constant propagation. In order to develop
   it, we need a few conceptual definitions, which are also useful for other program
   optimizations and verifications.

   Consider an instruction p: a := b ⊕ c that defines variable a. For brevity, we
   denote such a variable definition as a p , while D (a) denotes the set of all the
   definitions of the same variable a in the subprogram under analysis. The following
   definition formalizes the concept.

   Definition 5.60 (reaching definition) We say that the defn of a variable a in an
   instruction q, i.e., a q , reaches the entrance of an instruction p, if there exists a
   path from q to p such that it does not traverse a node (distinct from q) that defines
   variable a. ?
   When this happens, instruction p may use the value of variable a computed in the
   instruction q.

   Referring to automaton A, i.e., to the control-flow graph of the subprogram, the
   condition above (Definition5.60) can be restated more precisely as follows.
   Defi5.7 Static Program Analysis 479
   nition a q reaches instruction p if language L (A) contains a sentence of the form
   x = u q v pw,whereu andware(possiblyempty)arbitraryinstructionsequences, p
   is any instruction, v is a possibly empty instruction sequence not containing any
   definitionofa andinstructionq definesvariablea.Theaboveconditionsareformalized
   in this way:
   u, w ∈ I ∗ ∧ q ∈ D (a) ∧ v ∈
   ?
   I \ D (a)
   ? ∗
   ∧ p ∈ I (5.10)
   Notice that the instructions p and q may coincide.

   Looking again at the program on p.470, reproduced in Fig.5.23, we find that
   definition a 1 reaches the entrance of instructions 2, 3 and 4, but not the entrance of
   instruction 5. Definition a 4 reaches the entrance of instructions 5, 6, 2, 3 and 4.
   Data-Flow Equations for Reaching Definition
   To compute the reaching definitions in all the program points, we set up a system of
   equations similar to those for liveness analysis.

   If node p defines variable a, we say that any other definition a q of the same
   variable in another node q, with q ?= p, is suppressed by p. Formally, the set of
   definitions suppressed by instruction p is the following:
   ⎧
   ⎪
   ⎨
   ⎪
   ⎩
   sup(p) =
   ?
   a q
   q ∈ I ∧ q ?= p ∧
   a ∈ def(q) ∧ a ∈ def(p)
   ?
   if def(p) ?= ∅
   sup(p) = ∅ if def(p) = ∅
   Notice the set def(p) may contain more than one name if instruction p defines
   multiple variables, such as the statement read(a, b, c).

   Thesetsofdefinitionsreachingtheentrancetoandexitfromanode p aredenoted
   as in(p) and out(p), respectively. The set of the (immediate) predecessor nodes of
   p is denoted as pred(p).

   Data-Flow Equations
   For the initial node 1:
   in(1) = ∅ (5.11)
   For any other node p ∈ I:
   out(p) = def(p) ∪
   ?
   in(p) \ sup(p)
   ?
   (5.12)
   in(p) =
   ?
   ∀q ∈pred(p)
   out(q) (5.13)
   Comments:
   • Equation(5.11)assumesforsimplicitythatnovariablesarepassedasinputparam-
   eters to the subprogram. Otherwise, more accurately, set in(1) should contain all
   the definitions, external to the subprogram, of the input parameters.


<a id="P480"></a>

   a := 1
   b := a + 2
   c := b + c
   a := b × 3
   a < m
   return c
   { c ? , m ? }
   ↓
   1
   2
   3
   4
   5
   6
   { a 1 , c ? , m ? }
   { a 1 , a 4 , b 2 , c ? , c 3 , m ? }
   { a 1 , a 4 , b 2 , c 3 , m ? }
   { a 4 , b 2 , c 3 , m ? }
   { a 4 , b 2 , c 3 , m ? }
   { a 4 , b 2 , c 3 , m ? }
   Fig.5.23 Control-flow graph with reaching definitions (Example5.61)
   • Equation(5.12) inserts into the exit from p all the local definitions of p and the
   definitions reaching the entrance to p, provided the latter are not suppressed by p.
   • Equation(5.13) states that any definition reaching the exit of some predecessor
   node reaches also the entrance to p.

   Similar to the liveness equations, the reaching definition system can be solved by
   iteration until the computed solution converges to the first fixed point. In the starting
   iteration all the unknown sets are empty.

   We illustrate the situation in the next example (Example5.61) with a program
   containing a loop.

   Example 5.61 (reaching definition) Observe in Fig.5.23 the same program 
   controlflow graph on p.470, with the reaching definition sets computed by solving the
   equation system listed in Table5.11.

   Variablescandm aretheinputparametersofthesubprogram,andwemayassume
   they are externally defined in the calling subprogram at some unknown points denoted
   c ? and m ? . For instance, notice that the external definition c ? of variable c does not
   reach the entrance of instruction 4, since it is suppressed by instruction 3.

<a id="P481"></a>

   Table 5.11 Data-flow equations for the reaching definitions in the program (Example5.61 in
   Fig.5.23).

   in(1) = { c ? , m ? }
   out(1) = { a 1 } ∪ (in (1) \ { a 4 })
   in(2) = out(1) ∪ out(5)
   out(2) = { b 2 } ∪ (in(2) \ ∅) = { b 2 } ∪ in(2)
   in(3) = out(2)
   out(3) = { c 3 } ∪ (in(3) \ { c ? })
   in(4) = out(3)
   out(4) = { a 4 } ∪ (in(4) \ { a 1 })
   in(5) = out(4)
   out(5) = ∅ ∪ (in(5) \ ∅) = in(5)
   in(6) = out(5)
   out(6) = ∅ ∪ (in(6) \ ∅) = in(6)
   We list the constant terms occurring in the equations:
   node instruction def sup
   1 a := 1 a 1 a 4
   2 b := a + 2 b 2 ∅
   2 c := b + c c 3 c ?
   4 a := b × 3 a 4 a 1
   5 a < m ∅ ∅
   6 return c ∅ ∅
   At iteration 0 all the sets are empty. After a few iterations, the unknown values
   converge to the sets shown in Table5.11. ?

### 5.7.3.1 Constant Propagation

   Carrying further the previous example (Example5.61 and Fig.5.23), we look for
   opportunities to replace a variable by a constant value. An instance of the problem
   is the question: can we replace the variable a in the instruction 2 with the constant
   1 assigned by instruction 1 (i.e., definition a 1 )? The answer is negative because the
   set in(2) of reaching definitions contains another definition of a, namely a 4 , which
   implies that some computation may use for a the value defined in the instruction
   4. Therefore the program containing instruction b := 1 + 2 instead of b := a + 2

<a id="P482"></a>

   would not be equivalent to the original one, which is quite evident since a run can
   execute the loop body.

   Generalizing this reasoning, it is easy to state a condition: in the instruction p, it
   is safe to replace with a constant k a variable a used in p if the following conditions
   hold:
   1. There exists an instruction q: a := k that assigns constant k to variable a, such
   that definition a q reaches the entrance of p.

   2. No other definition a r of variable a reaches the entrance of p, with r ?= q.
   Inthenextexample(Example5.62),weshowsomeprogramimprovementsproduced
   by a constant propagation and by the induced simplifications.

   Example 5.62 (optimizationfollowingconstantpropagation)Fig.5.24showsasim-
   ple program control-flow graph and lists the reaching definition sets and the live
   v := 4
   v × 8 ≥ 0
   w := 7 w := 12
   ...

   code using w but not v
   ...

   code using w but not v
   ↓
   ∅ live in = ∅
   1
   2
   3 4
   5
   6
   { v 1 } live in = { v }
   { v 1 }
   live in = ∅
   { v 1 }
   live in = ∅
   { v 1 , w 3 }
   live in = { w }
   { v 1 , w 4 }
   live in = { w }
   { v 1 , w 3 , w 4 } live in = { w }
   Fig.5.24 A control-flow graph with reaching definitions and live variables before constant 
   propagation (Example5.62)

<a id="P483"></a>

   Fig.5.25 Program
   control-flow graph of
   Fig.5.24 after the
   optimizations induced by
   constant propagation
   w := 7
   ...

   code using w but not v
   ...

   code using w but not v
   ↓
   ∅ live in = ∅
   3
   5
   6
   { w 3 }
   live in = { w }
   { w 3 } live in = { w }
   variablesetsintherelevantpointsoftheprogram.Observethattheonlydefinitionof
   variable v reaching the entrance of node 2 is v 1 . By the previous condition, it is legal
   toreplacevariablev withconstant4intheconditionalinstruction2,whichafterward
   becomes the constant boolean expression 4 × 8 ≥ 0. Now, variable v ceases to be
   live-out for assignment 1, which becomes useless and can be deleted. But program
   simplificationdoesnotendhere.Thecompilercancomputethevalueoftheconstant
   expression 32 8 × 4 = 32 ≥ 0 = true, thus determining which one of the successor
   legs of statement 2 will be taken, say the one to the left. Then the right leg will never
   be taken and can be deleted. We say that instruction 4 becomes unreachable or dead
   code, since no computation starting in the program entry will ever reach it. Now,
   the conditional instruction 2 is redundant and can be eliminated. After these 
   transformations, the simplified program is shown in Fig.5.25. Now the analysis could
   proceed to determine if constant w = 7 can be safely propagated to the rest of the
   program. ?

### 5.7.3.2 Availability ofVariables and Initializations

   A basic correctness check a compiler should perform is to control that the variables
   are initialized before their first use. More generally, a variable used in some 
   instruction must, on entrance to the instruction, have a value computed by means of a valid
   assignment (or by another statement type that can define variables). Otherwise, we
   say that the variable is not available, and a compiler-time error occurs.

   Coming back to the program control-flow graph of Fig.5.23 on p.480, observe
   thatnode3usesvariablec,butonthepath123noinstructionexecutedbeforenode3
   32 Anticipating a computation to compiler time is termed constant folding.

<a id="P484"></a>

   assignsavaluetoc.Thisisnotnecessarilyanerror:ifvariablecisaninputparameter
   of the subprogram, its value is supplied by the subprogram invocation statement. In
   such a case, variable c has a value on the entrance to node 3 and no error occurs. The
   same discussion applies to variable m.

   Variable b is available on the entrance to node 3 because its value was defined in
   node 2 by an assignment, which uses variable a; the latter is in turn available on the
   entrance to 2, following the initialization in the node 1.

   This sort of reasoning becomes intricate, and we need to clarify the concept of
   availability in the next definition (Definition5.63). For simplicity we assume the
   subprogram does not have any input parameters.

   Definition 5.63 (variable availability) A variable a is available on the entrance to
   node p, i.e., just before the execution of instruction p, if in the program control-flow
   graph every path from the initial node 1 to the entrance of p contains a statement
   that defines variable a. ?
   By comparing this notion with the concept of reaching definition introduced on
   p.478, we notice a difference in the quantification over paths. If the definition a q of
   variable a reaches the entrance of node p, there necessarily exists a path from node
   1 to node p that traverses the defining instruction q. But this does not guarantee that
   variable a is available on the entrance to p, because we cannot exclude that there
   exists another path from 1 to p, which avoids node q as well as any other node that
   defines variable a.

   It follows that the condition of a variable definition being available on a node
   entrance is more constraining than that of the variable definition reaching the same
   node.

   Tocomputethevariablesavailableontheentrancetonode p,letusexaminemore
   closely the set of definitions reaching the exits of the predecessors of p. If for every
   node q that is a predecessor of p, the set of reaching definitions out(q) on the exit
   from q contains (at least) one definition of variable a, then variable a is available on
   the entrance to p. In that case we also say that some definition of a always reaches
   node p.

   It is simple to convert the latter condition into an effective test that the variables
   are correctly initialized. For an instruction q, the new notation out ? (q) denotes, as
   out(q) did, the set of the variable definitions reaching the exit from q, but with their
   subscriptsdeleted.Forinstance,forout(q) = { a 1 , a 4 , b 3 , c 6 }wehaveout ? (q) =
   { a, b, c }.

   Badly Initialized Variable
   An instruction p is not well initialized if the following predicate holds:
   ∃q ∈ pred(p) such that use(p) ? out ? (q) (5.14)
   The condition says that there exists a node q predecessor of p, such that the set of
   definitions reaching its exit does not include all the variables used in p. Therefore,

<a id="P485"></a>

   Fig.5.26 Control-flow
   graph with the available
   variables for Example5.64
   a := 1
   ...... a ≤ 10
   b := c + 1
   a := b × 3
   ↓
   ∅
   1
   2
   3
   4
   5
   { a 1 }
   { a 1 , a 4 , b 3 }
   { a 1 , a 4 , b 3 }
   { a 4 , b 3 }
   when the program execution runs on a path through q, one or more variables used
   in p do not have a value. The next example (Example5.64) illustrates the situation.
   Example 5.64 (detecting uninitialized variables) Observe the program control-flow
   graphinFig.5.26,completedwiththesetsofreachingdefinitions.Condition(5.14)is
   false in the node 2, since for every predecessor (nodes 1 and 4) the set out ? contains a
   definition of a, which is the only variable used in 2. On the other hand, the condition
   is true in the node 3 because no definition of variable c reaches the exit from 2.
   Our analysis has thus detected a program error: instruction 3 uses an uninitialized
   variable, namely c.

   To find all the remaining initialization errors, we can proceed as follows. We
   replace any erroneous instructions discovered so far, such as node 3, by a dummy
   no-operation instruction. Then we update the computation of the reaching definition
   sets and we evaluate condition (5.14) again. By so doing, we would discover that
   instruction 4 is not well initialized because definition b 3 is not really available,
   althoughitispresentinthesetout(3),becauseinstruction3hasalreadybeenmarked
   as ineffective. Then also instruction 4 becomes ineffective. Continuing in the same
   manner, no other errors would be discovered. ?
   The previous analysis allows to catch at compile time many errors that had gone
   unnoticed during the preceding phases of parsing and semantic analysis, with the
   benefitthattheywillnotcausehard-to-understandrun-timeerrorsorraiseexceptions
   during program execution.


<a id="P486"></a>

   To conclude, static analysis 33 encompasses many more conditions and properties
   than the cases of liveness and reaching definitions we have been able to present. It
   is a powerful general method for analyzing programs before execution, in order to
   optimize them or to verify their correctness.

   References
   1. Berstel J (1979) Transductions and context-free languages. Teubner, Stuttgart
   2. Sakarovitch J (2009) Elements of automata theory. Cambridge University Press, Cambridge,
   England
   3. Aho A, Ullman J (1972) The theory of parsing, translation, and compiling, volume 1: parsing.
   Prentice-Hall, Englewood Cliffs, NJ
   4. AhoA,UllmanJ(1973)Thetheoryofparsing,translationandcompiling,volume2:compiling.
   Prentice-Hall, Englewood Cliffs, NJ
   5. YangW(1996)Mealymachinesareabettermodeloflexicalanalyzers.ComputLang22:27–38
   6. Tennent RD (1991) Semantics of programming languages. Prentice-Hall, Englewood Cliffs
   7. Winskel G (1993) The formal semantics of programming languages. MIT Press, Cambridge,
   MA
   8. Winskel G (1968) Semantics of context-free languages. Math Syst Theory 2(2):127–145
   9. Reps TW, Teitelbaum T, Demers AJ (1983) Incremental context-dependent analysis for
   language-based editors. ACM TOPLAS 5(3):449–477
   10. Knuth DE (1971) Semantics of context-free languages (errata corrige). Math Syst Theory
   5(2):95–99
   11. EngelfrietJ(1984)Attributegrammars:attributeevaluationmethods.In:LorhoB(ed)Methods
   and tools for compiler construction. Cambridge University Press, Cambridge, UK, pp 103–138
   12. Engelfriet J, Filé G (1989) Passes, sweeps, and visits in attribute grammars. J ACM 36(4):841–
   869
   13. Crespi Reghizzi S, Psaila G (1998) Grammar partitioning and modular deterministic parsing.
   Comput Lang 24(4):197–227
   14. AhoA,LamM,SethiR,UllmanJ(2006)Compilers:principles,techniquesandtools.Prentice-
   Hall, Englewoof Cliffs, NJ
   15. Appel A (2002) Modern compiler implementation in Java. Cambridge University Press,
   Cambridge, UK
   16. Srikant YN, Shankar P (2002) Compiler design handbook: optimizations and machine code
   generation. CRC Press Inc, Boca Raton, FL, USA
   17. Muchnick S (1997) Advanced compiler design and implementation. Morgan Kaufmann, San
   Francisco
   18. Nielson F, Nielson HR, Hankin C (2010) Principles of program analysis. Springer, Berlin
   33 Abookonthetheoryofstaticprogramanalysisis[18].Forasurveyofapplicationsincompilation,
   see, e.g., [14,15,17].

Index
=====

   Symbols
   2I-automaton, 416, 417
   ε-move, 130
   A
   Abstract syntax tree, 45, 394, 429
   Acceptance
   by empty stack, 202
   with final state, 207
   Accepting mode of pushdown machine, 208
   Accessible
   state, 124, 246
   subsets, 144
   A-condition for attribute evaluation, 455
   Acyclic attribute grammar, 440, 441
   Acyclicity test, 441
   Acyclic LST definition, 185
   Algol 60, 60
   Algorithm
   Berry-Sethi
   algorithm, 161–163
   parser, 172, 179
   bottom-up parser construction, see
   ELR(1) parser construction
   cleaning
   of a finite automaton, 124
   of a grammar, 39
   composition of local automata, 154
   conversionoftranslationgrammartopost-
   fix form, 409
   deterministic automaton from a regular
   expression construction, 158
   determinization of finite automaton, 163
   Earley parser, 358
   elimination
   of copy rules, 68
   of nullable nonterminals, 67
   ofrepeatedrightpartsingrammarrules,
   70
   of spontaneous moves, 143
   ELR(1) parser construction, 260
   pointerless, 294
   with vector stack, 272
   finite automaton complementation, 189
   finite automaton determinization, see
   powerset construction
   from grammar to nondeterministic
   pushdown automaton, 202
   from regular expression to deterministic
   finite automaton, see Berry-Sethi
   algorithm
   from regular expression to deterministic
   finite parser, see Berry-Sethi parser
   from regular expression to grammar, 82
   from unilineargrammar to regular 
   expression, 87
   local recognizer construction, 152
   merge of kernel-equivalent m-states, 291
   minimization of finite deterministic
   automaton, 125
   normalization of a grammar
   into Chomsky form, 72
   into Greibach form, 79
   into operator form, 74
   © Springer Nature Switzerland AG 2019
   S. Crespi Reghizzi et al., Formal Languages and Compilation,
   Texts in Computer Science, https://doi.org/10.1007/978-3-030-04879-2
   487
   488 Index
   into position restricted form, 80
   into real-time form, 78
   one-sweep evaluator construction, 443
   operator precedence parallel parser
   costruction, 343
   operator precedence sequential parser
   construction, 331
   pilot graph construction, 252
   powerset construction, 145
   predictive parser construction
   as DPDA, 311
   by recursive procedures, 313
   predictive pushdown transducer 
   construction, 400
   reduction
   of a grammar, see cleaning of a
   grammar
   ofanautomaton,seecleaningofafinite
   automaton
   syntax tree construction, 367
   top-down parser construction, see 
   predictive parser construction
   topological sorting, 440
   transformation of grammar recursion
   immediate left recursion, 76
   non-immediate left recursion, 77
   Alphabet, 8
   unary, 92
   Alphabetic homomorphism, 97, 388
   Alternative, 19, 34
   Ambiguity, 52
   conditional instruction, 60, 392, 396
   decision in r.e., 186
   degree, 54
   EBNF, 104
   inherent, 62, 147, 221
   in r.e., 170, 182
   of automaton, 137, 221
   of bilateral recursion, 55
   of circular derivation, 395
   of concatenation, 58
   of regular expression, 22, 84, 165
   of translation, 395
   of union, 56
   source grammar, 396
   versus nondeterminism, 221
   Ambiguity degree
   of r.e., 170
   ANTLR, 324, 352
   Aperiodic language, 193
   Arden identity, 88
   Arithmeticexpression,42,43,53,56,63,65,
   69, 103
   calculating machine, 453
   infix, 404
   machine net, 235
   parenthesis-free, 86
   polish, 393
   syntax chart, 238
   Artificial language, 5
   Attribute, 428, 430
   inherited, see right attribute
   left, see left attribute
   lexical, see lexical attribute
   right, see right attribute
   synthesized, see left attribute
   Attribute grammar, 429, 457
   A-condition, 455
   acyclic, 440
   applications, 457
   definition, 435
   L-condition, 450
   multi-sweep, 446, 464
   one-sweep, 442, 443
   Attribute subgrammar, 446
   Automaton
   ambiguous, 137, 221
   clean, 124
   configuration, 118
   equivalent, 119
   finite, 120
   finite deterministic, 122
   for local language, 152
   from regular expression, 147, 148, 150
   generalized, 139
   local, 152
   minimization, 126
   nondeterministic, 133
   product, 190
   real-time, 212
   to regular expression, 139
   two-way, 424
   unidirectional, 118
   with spontaneous moves, 135
   Available variables, 483
   Axiom, 33, 34
   B
   Back-end compiler, 461
   Base of macro-state, 252
   Berry, 161
   Berry-Sethi, 147, 161
   Index 489
   algorithm, 161–163
   parser, 172, 179
   Berstel, 150, 386
   Binary operator, 391
   Block structure, 31
   BMC, 139
   BNF grammar, 34, 277
   Boolean grammar, 109
   Bottom-up
   attribute evaluation, 455
   deterministic translation, 406
   parser with attribute evaluator, 453
   syntax analysis, 231
   Brzozowski and McCluskey, 139
   BS, see Berry-Sethi algorithm
   BSP, see Berry-Sethi parser
   BuildTree (BT), 366
   C
   Cancellation rule of Dyck, 49
   Candidate, 243
   identifier, 259
   Cardinality of language, 8
   Cartesian product of automata, 190
   CF family, 84, 95, 99, 101, 209
   closure properties, 51
   Choice of a regular expression, 20
   Chomsky
   hierarchy, 36, 105
   normal form, 36, 72
   Circular derivation, 40, 395
   Clean automaton, 124, 471
   Cleaning
   of a finite automaton, 124
   of a grammar, 39
   Cleaveland, 109
   Closing mark, 27
   Closure
   function, 243
   in Earley parser, 354
   of macro-state, 252
   Closure properties
   of REG, 25
   of CF, 51, 95
   of CF and REG, 95
   of DET, 216
   of input-driven L., 227
   under substitution, 99
   under translation, 426
   under transliteration, 99
   Code generation, 460
   Codes and ambiguity, 58
   Coke-Younger-Kasami algorithm, 352
   Compaction of pilot, 290
   Compiler
   decomposition, 446
   language, 428
   Complement
   automaton, 188
   of language, 13
   of regular language, 188
   Completion in Earley parser, 356
   Computation, 133, 206
   label, 122, 133
   length, 133
   of nondeterministic automaton, 133
   of Turing machine, 119
   Concatenation, 9
   of context-free languages, 51, 96
   of Dyck languages, 58
   of languages, 12
   Condensed skeleton tree, 45
   Conditional instruction, 60, 392, 396
   ambiguity, 60, 392, 396
   Configuration
   of automaton, 118
   of pushdown automaton, 201, 206
   of Turing machine, 119
   Conflict
   convergence, 251
   reduce-reduce, 251
   shift-reduce, 251
   Conjunctive grammar, 109
   Conservative approximation, 471
   Constant
   folding, 483
   propagation, 478, 481
   Context-dependent language, 105
   Context-free
   deterministic, 215, 216
   grammar
   definition, 34
   from pushdown automaton, 210
   introduction, 32
   translation, 389
   Context-sensitive language, 105
   Control-flow graph, 467, 469
   automaton, 469
   language, 469
   Control instruction translation, 461
   Convergence conflict, 251, 253, 255
   Convergent transitions, 253
   490 Index
   Copy-free normal form, 67
   Copy rule, 67
   Correspondence between finite automaton
   and unilinear grammar, 137
   Cross operation, 16
   D
   Dart, 306
   Data-flow equations, 474, 479
   liveness, 474
   reaching definitions, 479
   solution, 474
   Dead code, 483
   Decidable language, 119
   Decimal constant, 121, 123, 124, 135, 137
   Decision algorithm, 116
   Decompiler, 388
   Decorated tree, 431, 433
   Degree of operator, 391
   Dependence graph, 433, 438, 442
   of decorated tree, 433, 438
   of semantic function, 438
   Derivation, 37
   circular, 40
   EBNF, 103
   immediate, 21
   left, 46
   of regular expression, 21
   right, 46
   self-nesting, 91
   Deterministic
   finite automaton, 122
   language, 215, 216
   unambiguity, 220
   language families comparison, 346
   pushdown automaton, 215
   subclasses, 221
   simple grammar, 223
   Deterministic Finite Automaton (DFA), 122
   Deterministic Pushdown Automaton
   (DPDA), 215
   Determinizationofautomaton,142,162,163
   DET family, 215, 216, 348
   Dictionary, 8
   Difference of languages, 13
   Digrams, 150
   Directed Acyclic Graph (DAG), 178, 181
   Distance of strings, 375
   Distinctly parenthesized grammar, 50, 223
   Distinguishability of states, 126
   Document Type Definition (DTD), 228
   Double Greibach normal form, 80
   Double service lemma, 218
   Dyck
   cancellation rule, 49
   language, 49, 84, 96
   concatenation, 58
   translation, 403
   Dynamic error, 460
   E
   Earley
   algorithm, 352, 357
   closure, 354
   introductory example, 353
   nonterminal shift, 356
   parser, 357
   completeness, 361
   completion algorithm, 357
   computational complexity, 364
   correctness, 361
   function BuildTree, 367
   grammar ambiguity, 372
   grammar unambiguity, 373
   nullable nonterminals, 369
   optimization, 373
   syntax analysis algorithm, 358
   syntax tree construction, 366
   syntax tree construction complexity,
   371
   terminal shift algorithm, 358
   vector, 357
   terminal shift, 355
   vector, 357
   Early scanning, 277, 278
   EBNF grammar, 100, 233
   ambiguity, 104
   derivation, 103
   translation, 405
   Editing distance, 375
   ELL(1)
   condition, 286
   direct, 306
   violation, 309
   parser
   direct construction, 305
   step by step construction, 288
   parsing, 282
   PCFG, 298
   violation remedy, 351
   ELL(k)
   condition, 323
   Index 491
   parser, 324
   ELR(1)
   condition, 254
   grammar, 349
   language, 349
   parser
   pointerless, 294
   vector-stack, 271
   parser construction, 252
   ELR(k)
   grammar, 349
   language, 349
   relation with ELR(1), 349
   Empty string, 10
   Encryption, 388
   End-mark in conditional instructions, 396
   Engelfriet, 446
   Environment, 457
   Epsilon move, 130
   Equations of unilinear grammar, 87, 88
   Equivalence
   of finite automata, 123, 128
   of grammars
   generalized structural, 65
   strong, 65
   weak, 64
   Equivalent regular expressions, 22
   Error
   dynamic, 374
   objective, 374
   recovery, 376, 377
   panic mode, 377
   panic mode with token insertion, 378
   reporting, 376
   semantic, 374
   state, 123
   static, 374
   subjective, 374
   syntactic, 374, 375
   treatment, 373
   type, 374
   Expansion of nonterminal, 66
   Extended regular expression, 188, 191
   External attribute, 436
   F
   Finals, 150
   Finding a word in text, 134
   FIN family, 20
   Finite
   automaton, 120
   deterministic, 122
   left-linear grammar, 138
   transducer, 421, 422
   opposite passes, 424
   with look-ahead, 425
   transducer for BSP, 178
   Finite-state family, 136
   Flex, 425
   Floyd, 219
   operator precedence languages, 335, 337
   Followers, see set of followers
   Follow set, see set of followers
   Formal language, 6
   Free monoid, 15
   Front-end compiler, 461
   FSA, 120
   G
   General
   automaton, 117
   parser, 352
   Generalized automaton, 139
   Goal-oriented, see predictive
   Grammar
   ambiguity, 53
   from ambiguous r.e., 84
   attribute, 429
   BNF, 34, 277
   Boolean, 109
   Chomsky classification, 105
   clean, 39
   cleaning, 39
   conjunctive, 109
   context-free, 32
   context-sensitive, 105
   EBNF, 100, 233
   equivalent, 38
   errors, 39
   extended context-free, 100, 233
   homogeneous, 36, 77
   invertible, 69
   left-linear, 85
   linear, 84
   marked BNF grammar rule, 236
   non-left-recursive, 76
   normal form, 37, 65
   of regular language, 82
   of Van Wijngarten, 109
   operator, 74
   parenthesized, 50, 223
   representations, 35
   492 Index
   right-linear, 85, 129
   right-linearized, 239
   simple deterministic, 223, 346
   strictly unilinear, 86
   target, 389
   translation, 389, 399
   type 0, 105
   type 1, 105
   type 2, 105
   type 3, 85, 105
   unilinear, 85
   Graph
   context-sensitive, 106
   dependence, 433
   local automaton, 152
   parser control-flow, 298
   pilot, 252
   program control-flow, 468
   reachability, 39
   sibling, 442
   state-transition, 120
   syntax chart, 237
   syntax tree, 43
   Graphical computation for ELL
   guide set, 317
   prospect set, 316
   set of initials, 315
   Graphical method for ELL, 314
   examples, 318
   pilot look-ahead, 322
   Greibach normal form, 36, 78
   Guide
   predicate, 464
   set, 299, 306
   graphical computation, 317
   graphical method, 314
   H
   Handle, 245, 259
   Heilbrunner grammar complement, 217
   Hierarchical list, 29
   Hierarchy of Chomsky, 105
   Homomorphism
   alphabetic, 97
   nonalphabetical, 98
   HTML, 228
   I
   ID family, 227
   Incremental
   compilation, 379
   parser, 379
   Infinite ambiguity degree
   of r.e., 170
   Infix
   operator, 392
   to postfix translation, 404
   Inherent ambiguity, 62, 147, 221
   Inherited attribute, see right attribute
   Initial
   of a string, 10
   state uniqueness, 136
   Initials, see set of initials
   Input-driven
   family, 227
   language, 223, 225, 227
   Instruction scheduling, 467
   Interference between variables, 476
   Intermediate
   language, 461
   representation, 461, 467
   Internal attribute, 436
   Interprocedural static analysis, 468
   Intersection
   of context-free and regular language, 213
   of context-free languages, 96
   of regular languages, 188, 190
   Intraprocedural static analysis, 468
   Inverse translation, 387
   Invertible grammar, 69, 223
   IO-automaton, 421
   sequential, 422
   Item, see candidate
   Iteration operator, 16
   J
   JavaScriptObjectNotation(JSON),48,228,
   329
   input-driven model, 228
   Jumps, 461
   K
   Kernel
   equivalent macro-states, 253
   of macro-state, 252, 253
   Knuth, 244, 267, 282, 348
   L
   Language, 8
   abstraction, 26
   artificial, 5
   complement, 13
   Index 493
   conjunctive, 110
   context-free, 32, 38, 105, 209
   context-sensitive, 105, 106
   decidable, 119
   Dyck, 49
   empty, 8
   equation, 87
   finite, 8
   formal, 6
   formalized, 5
   generated, 37
   infinite, 8
   recursive derivations, 41
   input-driven, 223, 225
   local, 151
   locally testable, 151
   nullable, 10
   recursive, 119
   recursively enumerable, 119
   regular, 20
   replica, 94, 110
   with center, 108, 110
   source, 386
   substitution of, 28
   target, 99, 386
   unary alphabet, 92
   universal, 13
   visibly pushdown, 223, 225
   with three equal powers, 93, 106, 110
   with two equal powers, 90
   L-condition for attribute evaluation, 450
   Least fixed point, 475
   Left
   attribute, 432, 435
   derivation, 46
   quotient, 17, 281
   recursion, 36
   elimination, 77
   immediate, 76
   LR(1), 282
   non-immediate, 77
   recursive grammar
   top-down parsing, 285
   recursive rule, 76
   elimination, 76
   Left-linear grammar, 85, 138
   Leftmost derivation, see left derivation
   Left/right terminal set, 327
   Length of string, 9
   Levenshtein distance, 375
   Lex, 425
   Lexeme, 447
   Lexical
   analysis, 447
   attribute, 436, 448, 449
   class, 447, 448
   closed and open, 448
   finite and non-, 448
   level, 447
   segmentation, 448
   Linear
   grammar, 84
   language equation, 87
   regular expression, 156
   syntax tree segments, 176, 177
   Linearized Syntax Trees (LST), 168, 169,
   172, 181
   construction, 181
   Linguistic abstraction, 26
   List
   abstract, 27
   concrete, 27
   hierarchical, 29
   with precedence, 29
   with separators, 27
   Live-in, 473
   Liveness, 471, 474, 476
   equations, 474
   interval, 471, 473
   Live-out, 473
   Live variable, 471, 483
   LL(1)
   grammar, 346
   relation with LR(1), 347
   LL(2) example, 323
   LL(k)
   grammar, 347
   relation with LR(k), 347
   Local
   automaton, 152
   language, 151, 193, 469
   automaton, 152
   composition of, 153
   normalized automaton, 153
   set, 156
   regular expression, 156
   testability, 151
   Locally testable language, 193
   Longest prefix rule, 448
   Look-ahead
   extending, 275
   increasing, 322
   494 Index
   set, 243
   graphical method, 322
   LR(1)
   family, 348
   grammar, 348
   transformation, 275, 279
   parser
   relation with ELR(1), 267
   superflous features, 270
   relation with DET, 348
   LR(2) to LR(1), 277, 279
   LR(k)
   early scanning, 277
   grammar, 275
   left quotient, 281
   transformation, 282
   Lukasiewicz, 392
   M
   M.r.e., 166
   Machine net, 233
   Macro-state, 250
   base, 252
   closure, 252
   kernel, 252
   Marked
   abstract syntax tree, 167
   BNF grammar rule, 236, 277, 279
   regular expression, 166
   syntax tree, 168
   Marked Abstract Syntax Tree (MAST), 167
   Marked Syntax Tree (MST), 168
   McNaughton, 194
   Meaning, 428
   Memory allocation, 476
   Metagrammar, 34
   Metalanguage of regular expression, 33
   Minimal automaton, 125
   Minimization of finite automaton, 126
   Mirror reflection, 10
   Mixfix operator, 392
   M-state, see macro-state
   Multiple transition property, 253
   Multi-sweep semantic evaluator, 446
   N
   Nerode relation, 126
   Nested structure, 31
   Network of finite machines, 233
   Nivat theorem, 419
   Noncounting language, 193
   Nondeterminism motivation, 131
   Nondeterministic
   automaton conversion to deterministic,
   142, 162, 163
   finite automaton, 130, 133, 136
   pushdown automaton, 202, 203
   union, 219
   Nondeterministic Finite Automata (NFA),
   133
   Non-left-recursive grammar, 76
   Non-LR(k), 282
   Non-nullable normal form, 66, 67
   Nonterminal, 32
   alphabet, 34
   expansion, 66
   nullable
   graphical method, 315
   shift, 248
   in Earley parser, 356
   Normal form
   Chomsky, 72
   copy-free, 67
   Greibach, 78
   non-nullable, 66, 67
   real-time, 78
   without repeated right parts, 69
   Nullable
   language, 10
   nonterminal, 66
   regular expression, 157
   Numbered
   input alphabet, 158
   regular expression, 156
   Numerical constant, 121, 123, 124, 135, 137
   O
   One-sweep
   attribute evaluation, 442
   attribute grammar, 442, 443
   semantic evaluator construction, 443
   Opening mark, 27
   Operator
   binary, 391
   degree, 391
   infix, 392
   mixfix, 392
   postfix, 392
   prefix, 392
   unary, 392
   variadic, 391
   Operator grammar, 74
   Index 495
   normal form, 36, 74
   Operator precedence
   grammar, 326
   languages, 335, 337
   relation, 326, 327
   sequential parser, 330, 331
   Opposite passes, 424
   Optimizationofprogram,385,467,477,481
   Ordered Attribute Grammar (OAG), 446
   P
   Palindrome, 32, 50, 390
   nondeterminism, 220
   pushdown machine, 207
   Panic mode error recovery, 377
   Papert, 194
   Parallel parser, 325, 343
   Parenthesis
   language, 47, 223
   redundant, 395
   Parenthesized
   expression, 45
   grammar, 50, 223
   tree, 45, 366
   Parser, 230
   Berry-Sethi, 172, 179
   choice criteria, 350
   Coke-Younger-Kasami, 352
   Earley, 357
   for regular expressions, see Berry-Sethi
   general, 352
   local, 325, 326
   parallel, 325, 339, 343
   predictive, 310, 311
   recursive descent, 313
   semantics-directed, 352, 463
   top-down and bottom-up, 231
   top-down deterministic, 311
   with attribute evaluation, 447
   with attribute evaluator, 450
   with translation, 402
   Parser Control-Flow Graph (PCFG), 290,
   298
   direct construction, 305
   Pilot
   compaction, 290
   construction, 252
   graph, 252
   machine of translator, 406
   Pin, 150
   PLZ-SYS, 464
   Pointerless ELR(1) parser, 294
   Polish notation, 56, 391, 392
   Portability of compiler, 461
   Position restricted normal form, 80
   Postaccessible state, 124
   Postfix
   normal form, 408
   operator, 392
   Power
   of language, 12
   of string, 11
   Powerset construction, 145
   Precedence
   of operators, 11
   relation, 326
   Prediction in Earley parser, 354
   Predictive
   parser, 310
   direct construction, 305
   parser automaton, 311
   pushdown automaton, 202
   pushdown transducer, 400
   Prefix, 10
   operator, 392
   Prefix-free language, 12
   Problematic regular expression, 171, 182
   Product
   machine, 190
   of automata, 190
   Production, 34
   Program
   analysis, 467
   optimization, 385, 467, 477, 481
   Projection, 98
   Prospect set, 299, 306
   graphical computation, 316
   Pumping property, 89
   Pure syntactic translation, 389
   Pushdown automaton
   accepting modes, 208
   conversion to grammar, 210
   definition, 205
   determinism, 215
   deterministic subclasses, 221
   forms of nondeterminism, 215
   on-line, 212
   real-time, 206, 212, 222
   time complexity, 204
   PushdownAutomaton(PDA),120,200,202,
   205, 209
   Pushdown IO-automaton, 397
   496 Index
   Pushdown transducer, 397, 399, 400
   from translation grammar, 399
   nondeterministic, 400
   Q
   Quotient
   of grammars, 281
   of languages, 17
   R
   Rabin and Scott, 416
   machine, 416
   Rational translation, see regular translation
   Reachable
   nonterminal, 39
   state, 124
   Reaching definition, 477, 483
   always, 484
   Real-time
   normal form, 78
   property, 206, 212
   pushdown automaton, 222
   Recognition algorithm, 116
   Recognize, see accept
   Recursion bilateral, 55
   Recursive
   derivation, 41
   descent, 449
   parser, 284, 313
   parser with attributes, 449
   translator, 403
   machine net, 233
   semantic evaluator, 445
   Recursive descent translator, 404
   Reduce move, 245
   Reduce-reduce conflict, 251, 255, 277
   Reduce-shift conflict, see shift-reduce
   conflict
   Reflection, 10
   REG family, 20, 84, 95, 100
   closure properties, 25
   included in CF, 84
   Register assignment, 477
   Regular expression, 18
   ambiguity, 22, 84, 85, 170
   Berry-Sethi method, 147
   extended, 24, 191
   from automaton, 139
   language defined, 22
   linear, 156
   metalanguage, 33
   nullability, 157
   numbered, 23, 156
   problematic, 171
   Thompson method, 147, 148
   with complement, 188
   with intersection, 188
   Regular expression to automaton, 147
   Berry-Sethi, 161
   structural method, 148
   Thompson, 148
   Regular language, 20
   intersection, 188
   pumping property, 89
   Regular translation, 414
   expression, 416
   Repeated right parts, 69
   Replica, 94, 108, 110
   Reps, 432
   Retargeting, 461
   Reversal, 10
   as translation, 389
   of context-free language, 52, 96
   of language, 11
   recognizer of, 132
   Right
   attribute, 432, 434, 435, 453
   derivation, 46
   recursion, 36
   LR(1), 282
   Right-linear grammar, 85, 129
   Right-linearized grammar, 269, 312
   Rightmost derivation, see right derivation
   Rule
   Chomsky normal, 36
   copy, 36, 67
   empty, 36
   Greibach normal, 36
   homogeneous, 36
   homogeneous binary, 72
   left-linear, 36
   left-recursive, 36
   linear, 36
   recursive, 36
   right-linear, 36
   right-recursive, 36
   self-embedding, 36
   self-nesting, 36
   subcategorization, 36
   terminal, 36
   with operators, 36
   Run-time error, 460
   Index 497
   S
   Sakarovitch, 141, 191, 386
   Scan in Earley parser, 355
   Scanner, 447
   Scheduling of instructions, 467
   Segments of linear syntax tree, 176, 177
   Self-nesting, 47
   derivation, 91
   Semantic, 5, 383, 427, 428
   analysis, 428
   attribute, 428, 431
   lexical, 448
   check, 457
   error, 457
   evaluator, 428, 442
   multi-sweep, 446
   one-sweep, 442
   recursive, 445
   function, 430, 435
   interpretation, 64
   metalanguage, 428, 435
   predicate, 457
   procedure, 445
   rule, 430
   translation, 427
   Semantics-directed parser, 352, 463
   Sentence, 8
   ambiguous, 23
   Sentential form, 38
   Sequential
   function, 423
   transducer, 422
   Set
   guide, 317
   look-ahead, 322
   nullable nonterminals, 315
   of followers, 161, 241
   of initials, 150, 242, 315
   graphical computation, 315
   prospect, 316
   Sethi, 161
   Set operations, 13
   Shift
   move, 244
   terminal or nonterminal, 251
   Shift-reduce
   conflict, 251, 255, 279
   parser with attribute evaluator, 453
   translator, 406
   Sibling graph, 442
   Simple deterministic grammar, 223, 346
   Single-transition property, 286, 290
   Single-valued translation, 396
   Sink state, see trap state
   Skeleton tree, 45
   condensed, 45
   Slice, 175
   Source
   grammar, 389
   ambiguity, 396
   language, 386
   Spontaneous move, 130, 135, 205
   elimination, 142, 143
   Stack
   candidate, 259
   macro-state, 259
   m-state, 259
   Stack m-states (sms), 259
   Star
   of context-free language, 51, 96
   operation, 14
   properties, 15
   Star-free language, 193
   State
   accessible, 124
   distinguishable, 125
   postaccessible, 124
   reachable, 124
   trap, 123
   useful, 124
   useless, 124
   State-transition diagram, 120
   pushdown machine, 207
   Static error, 457
   Static program analysis, 460, 467, 486
   interprocedural, 468
   intraprocedural, 468
   STP, see single-transition property
   Strictly unilinear grammar, 86
   String, 8
   empty, 10
   form, 37
   matching, 165
   Strong equivalence of grammars, 65
   Structural adequacy, 64, 86
   Subcategorization rule, 67
   Subjacent automaton, 418, 422, 423, 426
   pushdown, 397
   Substitution, 28, 98
   closure property, 99
   Substring, 10
   Suffix, 10
   498 Index
   Symbol table, 374, 457
   Syntactic
   analysis, 230
   error, 375
   level, 447
   procedure, 283, 313
   semantic analyzer, 450
   support, 430, 435
   translation, 389
   scheme, 389
   Syntactic-semantic analysis, 447
   Syntax, 5, 6
   abstract, 26
   analysis, 230
   bottom-up and top-down, 231
   chart, 237, 469
   tree, 43
   abstract, 394, 429
   as translation, 412
   Syntax-directed
   compiler, 428
   translation, 148, 389, 428
   Synthesized attribute, see left attribute
   T
   Target
   grammar, 389
   language, 99, 386
   Terminal
   alphabet, 34
   shift in Earley parser, 355
   symbol, 8
   Thompson, 147, 148
   Token, 447
   TOL, 444
   Tomita, 352
   Top-down
   deterministic translation, 403
   parsing, 282
   syntax analysis, 231
   Topological sort, 440
   TOR, 443
   TOS, 443
   Transducer, see translator
   Transduction, see translation
   Transition networks, 237
   Translation
   closure properties, 426
   function, 386, 387, 423
   sequential, 423
   grammar, 389, 399, 420
   EBNF, 405
   postfix normal form, 408
   rule normalization, 399
   to pushdown transducer, 399
   infix to postfix, 404
   rational, see regular translation
   regular, see regular translation
   relation, 386
   scheme, 389
   single-valued, 396
   syntactic, 389
   Translator
   Berry-Sethi, 172, 179
   bottom-up, 406
   comparison, 413
   top-down, 403
   two-way, 424
   with recursive procedures, 403
   Transliteration, 97, 388
   to words, 98
   Trap state, 123
   Tree
   construction, 366
   decorated, 431
   of regular expression, 165
   of regular phrase, 165
   pattern matching, 461
   syntax, 43
   Turing machine, 105, 119
   Two-input automaton, 416, 417
   Two-pass compiler, 429
   Type checking, 458
   Type 0 grammar, 105
   Type 1 grammar, 105
   Type 2 grammar, 105
   Type 3 grammar, 85, 105
   U
   Unary
   alphabet, 92
   operator, 392
   Undecidability
   deterministic context-free language, 350
   deterministic pushdown automaton, 350
   LR(k), 350
   Undistinguishability of states, 126
   Unilinear grammar, 85, 136, 137
   equations, 87
   Uninitialized variable, 485
   Union of context-free languages, 51, 96
   Universal language, 13, 15
   Index 499
   Unreachable code, 471, 483
   Useful state, 124
   Useless
   assignment, 472
   state, 124
   elimination, 124
   variable definition, 477
   Uzgalis, 109
   V
   Van Wijngarten, 109
   Variable
   availability, 483
   definition, 468
   suppressed, 479
   initialization, 483, 484
   use, 469
   Variadic operator, 391
   Vector-stack parser, 271
   Vocabulary, 8
   VW grammar, 109
   W
   Weak equivalence of grammars, 64
   Well-defined nonterminal, 39
   Word, 8
   X
   XML, 47, 228
   Y
   Yang, 425
