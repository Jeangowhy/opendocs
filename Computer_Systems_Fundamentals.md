/Computer Systems Fundamentals
===================================================================

![OpenCSF Logo](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_static/CSF-Logo.png) 

Chapter 0   Preface[¶]

*   [0.1. About OpenCSF](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/About.html)
*   [0.2. How to Use this System](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Intro.html)

Chapter 1   Introduction to Computer Systems[¶]

*   [1.1. Introduction to Concurrent Systems](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/IntroConcSysOverview.html)
*   [1.2. Systems and Models](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/)1_2_
    *   [1.2.1. Models as Representations](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#1_2_models-as-representations)
    *   [1.2.2. From Models to Implementations](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#1_2_from-models-to-implementations)
*   [1.3. Themes and Guiding Principles](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Themes.html)
    *   [1.3.1. Systems as Foundations of Computing](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#1_3_systems-as-foundations-of-computing)
    *   [1.3.2. Systems and Complexity](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#1_3_systems-and-complexity)
    *   [1.3.3. The Semiotics of Computer Systems](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#1_3_the-semiotics-of-computer-systems)
*   [1.4. System Architectures](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Architectures.html)
    *   [1.4.1. Client/Server Architectures](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#1_4_client-server-architectures)
    *   [1.4.2. Peer-to-peer (P2P) Architectures](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#1_4_peer-to-peer-p2p-architectures)
    *   [1.4.3. Layered Architectures](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#1_4_layered-architectures)
    *   [1.4.4. Pipe-and-filter Architectures](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#1_4_pipe-and-filter-architectures)
    *   [1.4.5. Event-driven Architectures](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#1_4_event-driven-architectures)
    *   [1.4.6. Hybrid Architectures](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#1_4_hybrid-architectures)
*   [1.5. State Models in UML](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/StateModels.html)
    *   [1.5.1. State Space Explosion](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#1_5_state-space-explosion)
    *   [1.5.2. Implementing Finite State Machines](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#1_5_implementing-finite-state-machines)
*   [1.6. Sequence Models in UML](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/SequenceModels.html)
    *   [1.6.1. Summary Questions](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#1_5_summary-questions)
*   [1.7. Extended Example: Text Parser State Machine](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/StateModelImplementation.html)

Chapter 2   Processes and OS Basics[¶]

*   [2.1. Processes and OS Basics](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ProcessesOverview.html)
*   [2.2. Processes and Multiprogramming](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/)2_2_
    *   [2.2.1. Uniprogramming and Utilization](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#2_2_uniprogramming-and-utilization)
    *   [2.2.2. Multiprogramming and Concurrency](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#2_2_multiprogramming-and-concurrency)
    *   [2.2.3. Context Switches and Overhead Costs](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/#2_2_context-switches-and-overhead-costs)
*   [2.3. Kernel Mechanics](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/KernelMechanics.html)
    *   [2.3.1. Kernel Memory Structure and Protections](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/KernelMechanics.html#kernel-memory-structure-and-protections)
    *   [2.3.2. The Boot Procedure](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/KernelMechanics.html#the-boot-procedure)
    *   [2.3.3. Kernel Invocation](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/KernelMechanics.html#kernel-invocation)
    *   [2.3.4. Mode Switches and Privileged Instructions](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/KernelMechanics.html#mode-switches-and-privileged-instructions)
*   [2.4. System Call Interface](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Syscall.html)
    *   [2.4.1. System Calls vs. Function Calls](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Syscall.html#system-calls-vs-function-calls)
    *   [2.4.2. Linux System Calls](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Syscall.html#linux-system-calls)
    *   [2.4.3. Calling System Calls in Assembly](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Syscall.html#calling-system-calls-in-assembly)
    *   [2.4.4. Calling System Calls with syscall()](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Syscall.html#calling-system-calls-with-syscall)
*   [2.5. Process Life Cycle](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ProcessCycle.html)
    *   [2.5.1. Process Creation](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ProcessCycle.html#process-creation)
    *   [2.5.2. Switching Program Code](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ProcessCycle.html#switching-program-code)
    *   [2.5.3. POSIX Spawn Interface](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ProcessCycle.html#posix-spawn-interface)
    *   [2.5.4. Process Destruction](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ProcessCycle.html#process-destruction)
*   [2.6. The UNIX File Abstraction](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/UnixFile.html)
    *   [2.6.1. Basic File Access](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/UnixFile.html#basic-file-access)
    *   [2.6.2. Working with Files](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/UnixFile.html#working-with-files)
    *   [2.6.3. Accessing File Metadata](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/UnixFile.html#accessing-file-metadata)
*   [2.7. Events and Signals](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/EventsSignals.html)
    *   [2.7.1. Sending Process Signals](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/EventsSignals.html#sending-process-signals)
    *   [2.7.2. Custom Signal Handlers](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/EventsSignals.html#custom-signal-handlers)
*   [2.8. Extended Example: Listing Files with Processes](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Extended2Processes.html)

Chapter 3   Concurrency with IPC[¶]

*   [3.1. Concurrency with IPC](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/IPCOverview.html)
*   [3.2. IPC Models](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/IPCModels.html)
    *   [3.2.1. Advantages and Disadvantages](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/IPCModels.html#advantages-and-disadvantages)
    *   [3.2.2. An IPC Taxonomy](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/IPCModels.html#an-ipc-taxonomy)
*   [3.3. Pipes and FIFOs](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Pipes.html)
    *   [3.3.1. Basic Pipes](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Pipes.html#basic-pipes)
    *   [3.3.2. Pipes and Shell Commands](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Pipes.html#pipes-and-shell-commands)
    *   [3.3.3. FIFOs](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Pipes.html#fifos)
*   [3.4. Shared Memory With Memory-mapped Files](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/MMap.html)
    *   [3.4.1. Memory-mapped Files](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/MMap.html#memory-mapped-files)
    *   [3.4.2. Region Protections and Privacy](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/MMap.html#region-protections-and-privacy)
*   [3.5. POSIX vs. System V IPC](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/POSIXvSysV.html)
    *   [3.5.1. POSIX IPC Fundamentals](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/POSIXvSysV.html#posix-ipc-fundamentals)
*   [3.6. Message Passing With Message Queues](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/MQueues.html)
    *   [3.6.1. POSIX Message Queues](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/MQueues.html#posix-message-queues)
*   [3.7. Shared Memory](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ShMem.html)
    *   [3.7.1. POSIX Shared Memory](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ShMem.html#posix-shared-memory)
*   [3.8. Semaphores](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/IPCSems.html)
    *   [3.8.1. POSIX vs. System V Semaphores](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/IPCSems.html#posix-vs-system-v-semaphores)
    *   [3.8.2. POSIX Named Semaphores](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/IPCSems.html#posix-named-semaphores)
    *   [3.8.3. POSIX Unnamed Semaphores](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/IPCSems.html#posix-unnamed-semaphores)
*   [3.9. Extended Example: Bash-lite: A Simple Command-line Shell](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Extended3Bash.html)

Chapter 4   Networked Concurrency[¶]

*   [4.1. Networked Concurrency](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/SocketsOverview.html)
*   [4.2. The TCP/IP Internet Model](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/FiveLayer.html)
    *   [4.2.1. Internet Model](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/FiveLayer.html#internet-model)
    *   [4.2.2. Packet Encapsulation and Nomenclature](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/FiveLayer.html#packet-encapsulation-and-nomenclature)
*   [4.3. Network Applications and Protocols](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/NetApps.html)
    *   [4.3.1. Naming and Addressing](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/NetApps.html#naming-and-addressing)
    *   [4.3.2. Protocol Specifications and RFCs](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/NetApps.html#protocol-specifications-and-rfcs)
*   [4.4. The Socket Interface](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Sockets.html)
    *   [4.4.1. Networking Data Structures](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Sockets.html#networking-data-structures)
    *   [4.4.2. Client Socket Interface](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Sockets.html#client-socket-interface)
    *   [4.4.3. Server Socket Interface](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Sockets.html#server-socket-interface)
    *   [4.4.4. Socket Communication](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Sockets.html#socket-communication)
*   [4.5. TCP Socket Programming: HTTP](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/TCPSockets.html)
    *   [4.5.1. Hypertext Transfer Protocol (HTTP)](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/TCPSockets.html#hypertext-transfer-protocol-http)
    *   [4.5.2. BNF Protocol Specification](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/TCPSockets.html#bnf-protocol-specification)
    *   [4.5.3. HTTP/1.1 Persistent Connections](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/TCPSockets.html#http-1-1-persistent-connections)
    *   [4.5.4. Processing HTTP Headers](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/TCPSockets.html#processing-http-headers)
    *   [4.5.5. Persistent State with Cookies](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/TCPSockets.html#persistent-state-with-cookies)
*   [4.6. UDP Socket Programming: DNS](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/UDPSockets.html)
    *   [4.6.1. Resolving DNS Queries](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/UDPSockets.html#resolving-dns-queries)
    *   [4.6.2. DNS Resource Record Structure](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/UDPSockets.html#dns-resource-record-structure)
    *   [4.6.3. DNS Protocol Messages](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/UDPSockets.html#dns-protocol-messages)
    *   [4.6.4. Constructing DNS Queries with Sockets](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/UDPSockets.html#constructing-dns-queries-with-sockets)
    *   [4.6.5. Processing DNS Query Responses](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/UDPSockets.html#processing-dns-query-responses)
*   [4.7. Application-Layer Broadcasting: DHCP](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/AppBroadcast.html)
    *   [4.7.1. DHCP Overview](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/AppBroadcast.html#dhcp-overview)
    *   [4.7.2. DHCP Protocol Messages](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/AppBroadcast.html#dhcp-protocol-messages)
*   [4.8. Extended Example: CGI Web Server](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Extended4CGI.html)

Chapter 5   The Internet and Connectivity[¶]

*   [5.1. The Internet and Connectivity](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/InternetOverview.html)
*   [5.2. Application Layer: Overlay Networks](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/AppLayer.html)
    *   [5.2.1. Characteristics of Peer-to-Peer Networks](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/AppLayer.html#characteristics-of-peer-to-peer-networks)
*   [5.3. Transport Layer](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/TransLayer.html)
    *   [5.3.1. Unreliable Transport: UDP](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/TransLayer.html#unreliable-transport-udp)
    *   [5.3.2. Reliable Transport: TCP](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/TransLayer.html#reliable-transport-tcp)
    *   [5.3.3. TCP Handshake and Connections](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/TransLayer.html#tcp-handshake-and-connections)
    *   [5.3.4. TCP Timeout and Packet Loss](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/TransLayer.html#tcp-timeout-and-packet-loss)
*   [5.4. Network Security Fundamentals](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/NetSec.html)
    *   [5.4.1. Symmetric Key Encryption](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/NetSec.html#symmetric-key-encryption)
    *   [5.4.2. Public Key Encryption](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/NetSec.html#public-key-encryption)
    *   [5.4.3. Cryptographic Hash Functions](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/NetSec.html#cryptographic-hash-functions)
    *   [5.4.4. Transport-Layer Security (TLS)](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/NetSec.html#transport-layer-security-tls)
    *   [5.4.5. TLS in Practice: HTTPS](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/NetSec.html#tls-in-practice-https)
*   [5.5. Internet Layer: IP](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/NetLayer.html)
    *   [5.5.1. IP Addresses and Subnets](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/NetLayer.html#ip-addresses-and-subnets)
    *   [5.5.2. IP Packet Formats](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/NetLayer.html#ip-packet-formats)
    *   [5.5.3. Network Routing Protocols](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/NetLayer.html#network-routing-protocols)
*   [5.6. Link Layer](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/LinkLayer.html)
    *   [5.6.1. LAN Packet Transmission: Ethernet](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/LinkLayer.html#lan-packet-transmission-ethernet)
    *   [5.6.2. LAN Packet Transmission: ARP](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/LinkLayer.html#lan-packet-transmission-arp)
    *   [5.6.3. What Lies Beneath: Carrier Signals](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/LinkLayer.html#what-lies-beneath-carrier-signals)
*   [5.7. Wireless Connectivity: Wi-Fi, Bluetooth, and Zigbee](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Wireless.html)
    *   [5.7.1. Wireless Protocol Stacks and Uses](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Wireless.html#wireless-protocol-stacks-and-uses)
*   [5.8. Extended Example: DNS Client](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Extended5DNS.html)

Chapter 6   Concurrency with Multithreading[¶]

*   [6.1. Concurrency with Multithreading](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ThreadsOverview.html)
*   [6.2. Processes vs. Threads](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ProcVThreads.html)
    *   [6.2.1. Multithreading](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ProcVThreads.html#multithreading)
    *   [6.2.2. Advantages and Disadvantages of Threads](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ProcVThreads.html#advantages-and-disadvantages-of-threads)
    *   [6.2.3. Thread Models](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ProcVThreads.html#thread-models)
*   [6.3. Race Conditions and Critical Sections](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/RaceConditions.html)
    *   [6.3.1. Race Conditions](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/RaceConditions.html#race-conditions)
    *   [6.3.2. Critical Sections](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/RaceConditions.html#critical-sections)
    *   [6.3.3. Thread Safety and Reentrancy](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/RaceConditions.html#thread-safety-and-reentrancy)
*   [6.4. POSIX Thread Library](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/POSIXThreads.html)
    *   [6.4.1. Creating and Joining Threads](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/POSIXThreads.html#creating-and-joining-threads)
    *   [6.4.2. Attached and Detached Threads](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/POSIXThreads.html#attached-and-detached-threads)
*   [6.5. Thread Arguments and Return Values](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ThreadArgs.html)
    *   [6.5.1. Passing a Single Argument to Threads](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ThreadArgs.html#passing-a-single-argument-to-threads)
    *   [6.5.2. Passing Multiple Arguments to Threads](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ThreadArgs.html#passing-multiple-arguments-to-threads)
    *   [6.5.3. Returning Values from Threads](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ThreadArgs.html#returning-values-from-threads)
*   [6.6. Implicit Threading and Language-based Threads](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ImplicitThreads.html)
    *   [6.6.1. Implicit Threading with OpenMP](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ImplicitThreads.html#implicit-threading-with-openmp)
    *   [6.6.2. Threads as Objects](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ImplicitThreads.html#threads-as-objects)
        *   [6.6.2.1. Java Threads](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ImplicitThreads.html#java-threads)
        *   [6.6.2.2. Python Threads](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ImplicitThreads.html#python-threads)
    *   [6.6.3. Concurrency as Language Design](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ImplicitThreads.html#concurrency-as-language-design)
        *   [6.6.3.1. Goroutines](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ImplicitThreads.html#goroutines)
        *   [6.6.3.2. Rust Concurrency](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ImplicitThreads.html#rust-concurrency)
*   [6.7. Extended Example: Keyboard Input Listener](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Extended6Input.html)
*   [6.8. Extended Example: Concurrent Prime Number Search](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Extended6Primes.html)

Chapter 7   Synchronization Primitives[¶]

*   [7.1. Synchronization Primitives](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/SynchOverview.html)
*   [7.2. Critical Sections and Peterson’s Solution](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/CritSect.html)
    *   [7.2.1. Peterson’s Solution](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/CritSect.html#peterson-s-solution)
    *   [7.2.2. Synchronization Properties](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/CritSect.html#synchronization-properties)
    *   [7.2.3. Peterson’s Solution and Modern Hardware](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/CritSect.html#peterson-s-solution-and-modern-hardware)
*   [7.3. Locks](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Locks.html)
    *   [7.3.1. POSIX Mutexes](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Locks.html#posix-mutexes)
    *   [7.3.2. Spinlocks](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Locks.html#spinlocks)
    *   [7.3.3. Defining Critical Sections](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Locks.html#defining-critical-sections)
*   [7.4. Semaphores](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Semaphores.html)
    *   [7.4.1. Semaphores as Signaling](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Semaphores.html#semaphores-as-signaling)
    *   [7.4.2. Mutual Exclusion with Semaphores](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Semaphores.html#mutual-exclusion-with-semaphores)
    *   [7.4.3. Multiplexing with Semaphores](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Semaphores.html#multiplexing-with-semaphores)
*   [7.5. Barriers](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Barriers.html)
    *   [7.5.1. Concurrent Calculations with Barriers](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Barriers.html#concurrent-calculations-with-barriers)
*   [7.6. Condition Variables](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Condvars.html)
    *   [7.6.1. Condition Variables vs. Semaphores](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Condvars.html#condition-variables-vs-semaphores)
    *   [7.6.2. How to Use a Condition Variable](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Condvars.html#how-to-use-a-condition-variable)
    *   [7.6.3. A Condition Variable Example](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Condvars.html#a-condition-variable-example)
    *   [7.6.4. Monitors and Synchronized Methods](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Condvars.html#monitors-and-synchronized-methods)
*   [7.7. Deadlock](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Deadlock.html)
    *   [7.7.1. Necessary Conditions](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Deadlock.html#necessary-conditions)
    *   [7.7.2. Livelock and False Solutions](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Deadlock.html#livelock-and-false-solutions)
    *   [7.7.3. Avoiding Deadlock](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Deadlock.html#avoiding-deadlock)
*   [7.8. Extended Example: Event Log File](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Extended7Events.html)

Chapter 8   Synchronization Patterns and Problems[¶]

*   [8.1. Synchronization Patterns and Problems](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/SynchProblemsOverview.html)
*   [8.2. Basic Synchronization Design Patterns](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/SynchDesign.html)
    *   [8.2.1. Signaling](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/SynchDesign.html#signaling)
    *   [8.2.2. Turnstiles](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/SynchDesign.html#turnstiles)
    *   [8.2.3. Rendezvous](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/SynchDesign.html#rendezvous)
    *   [8.2.4. Multiplexing](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/SynchDesign.html#multiplexing)
    *   [8.2.5. Lightswitches](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/SynchDesign.html#lightswitches)
*   [8.3. Producer-Consumer Problem](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ProdCons.html)
    *   [8.3.1. Producer-Consumer with Unbounded Queue](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ProdCons.html#producer-consumer-with-unbounded-queue)
    *   [8.3.2. Single Producer-Single Consumer Solution Using a Bounded Queue](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ProdCons.html#single-producer-single-consumer-solution-using-a-bounded-queue)
    *   [8.3.3. Multiple Producers Solution Using a Bounded Queue](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ProdCons.html#multiple-producers-solution-using-a-bounded-queue)
*   [8.4. Readers-Writers Problem](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ReadWrite.html)
    *   [8.4.1. A Solution Using Lightswitches](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ReadWrite.html#a-solution-using-lightswitches)
    *   [8.4.2. Fairness to Writers](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ReadWrite.html#fairness-to-writers)
    *   [8.4.3. Search-Insert-Delete Problem](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ReadWrite.html#search-insert-delete-problem)
*   [8.5. Dining Philosophers Problem and Deadlock](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/DiningPhil.html)
    *   [8.5.1. Solution of Limiting Accesses](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/DiningPhil.html#solution-of-limiting-accesses)
    *   [8.5.2. Solution by Breaking Hold-and-wait](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/DiningPhil.html#solution-by-breaking-hold-and-wait)
    *   [8.5.3. Solution by Imposing Order](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/DiningPhil.html#solution-by-imposing-order)
*   [8.6. Cigarette Smokers Problem and the Limits of Semaphores and Locks](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/CigSmokers.html)
    *   [8.6.1. Implications of the Cigarette Smokers Problem](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/CigSmokers.html#implications-of-the-cigarette-smokers-problem)
    *   [8.6.2. On Cigarette Smokers and Dining Philosophers](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/CigSmokers.html#on-cigarette-smokers-and-dining-philosophers)
*   [8.7. Extended Example: Parallel Modular Exponentiation](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Extended8ModExp.html)

Chapter 9   Parallel and Distributed Systems[¶]

*   [9.1. Parallel and Distributed Systems](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParallelDistributedOverview.html)
*   [9.2. Parallelism vs. Concurrency](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParVConc.html)
    *   [9.2.1. Multiprocessing Systems](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParVConc.html#multiprocessing-systems)
*   [9.3. Parallel Design Patterns](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParallelDesign.html)
    *   [9.3.1. Algorithmic Strategy Patterns](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParallelDesign.html#algorithmic-strategy-patterns)
    *   [9.3.2. Implementation Strategy Patterns](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParallelDesign.html#implementation-strategy-patterns)
    *   [9.3.3. Parallel Execution Patterns](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParallelDesign.html#parallel-execution-patterns)
*   [9.4. Limits of Parallelism and Scaling](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Scaling.html)
    *   [9.4.1. Amdahl’s Law and Strong Scaling](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Scaling.html#amdahl-s-law-and-strong-scaling)
    *   [9.4.2. Gustafson’s Law and Weak Scaling](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Scaling.html#gustafson-s-law-and-weak-scaling)
*   [9.5. Timing in Distributed Environments](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/DistTiming.html)
    *   [9.5.1. Clock Synchronization](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/DistTiming.html#clock-synchronization)
    *   [9.5.2. Logical Clocks and Lamport Timestamps](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/DistTiming.html#logical-clocks-and-lamport-timestamps)
    *   [9.5.3. Vector Clocks](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/DistTiming.html#vector-clocks)
*   [9.6. Reliable Data Storage and Location](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/DistDataStorage.html)
    *   [9.6.1. Google File System](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/DistDataStorage.html#google-file-system)
    *   [9.6.2. Distributed Hash Tables](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/DistDataStorage.html#distributed-hash-tables)
*   [9.7. Consensus in Distributed Systems](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/DistConsensus.html)
    *   [9.7.1. Byzantine Generals Problem](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/DistConsensus.html#byzantine-generals-problem)
    *   [9.7.2. Limits on Consensus](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/DistConsensus.html#limits-on-consensus)
    *   [9.7.3. Practical Byzantine Fault Tolerance](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/DistConsensus.html#practical-byzantine-fault-tolerance)
*   [9.8. Extended Example: Blockchain Proof-of-Work](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Extended9Blockchain.html)

Appendix A[¶]

*   [10.1. C Language Reintroduction](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/CLangOverview.html)
*   [10.2. Documentation and Debugging](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Debugging.html)
    *   [10.2.1. Man Pages](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Debugging.html#man-pages)
    *   [10.2.2. Debugging with GDB](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Debugging.html#debugging-with-gdb)
        *   [10.2.2.1. Breakpoints and Watchpoints](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Debugging.html#breakpoints-and-watchpoints)
        *   [10.2.2.2. Backtrace](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Debugging.html#backtrace)
        *   [10.2.2.3. Tracing multiple processes](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Debugging.html#tracing-multiple-processes)
*   [10.3. Basic Types and Pointers](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/BasicTypes.html)
    *   [10.3.1. C99 Fixed-width Types](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/BasicTypes.html#c99-fixed-width-types)
    *   [10.3.2. Pointer Basics](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/BasicTypes.html#pointer-basics)
*   [10.4. Arrays, Structs, Enums, and Type Definitions](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Arrays.html)
    *   [10.4.1. Two-dimensional Arrays](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Arrays.html#two-dimensional-arrays)
    *   [10.4.2. Structs and Packing](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Arrays.html#structs-and-packing)
    *   [10.4.3. Enums and Type Definitions](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Arrays.html#enums-and-type-definitions)
*   [10.5. Functions and Scope](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Functions.html)
    *   [10.5.1. Function Parameters and Return Values](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Functions.html#function-parameters-and-return-values)
    *   [10.5.2. Call-by-Reference Parameters](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Functions.html#call-by-reference-parameters)
    *   [10.5.3. Arrays as Parameters](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Functions.html#arrays-as-parameters)
*   [10.6. Pointers and Dynamic Allocation](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Pointers.html)
*   [10.7. Strings](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Strings.html)
    *   [10.7.1. Investigating String Contents](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Strings.html#investigating-string-contents)
    *   [10.7.2. Common String Manipulations](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Strings.html#common-string-manipulations)
    *   [10.7.3. Converting Between Strings and Integers](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Strings.html#converting-between-strings-and-integers)
*   [10.8. Function Pointers](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/FunctionPointers.html)
    *   [10.8.1. Passing Function Pointers as Arguments](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/FunctionPointers.html#passing-function-pointers-as-arguments)
    *   [10.8.2. Function Pointer Lookup Tables](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/FunctionPointers.html#function-pointer-lookup-tables)
*   [10.9. Files](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Files.html)
    *   [10.9.1. File Permissions and Ownership](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Files.html#file-permissions-and-ownership)
    *   [10.9.2. Persistent Storage](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Files.html#persistent-storage)
    *   [10.9.3. Directories and Links](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Files.html#directories-and-links)
    *   [10.9.4. Advisory Locks](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Files.html#advisory-locks)

Appendix B

*   [11.1. Glossary](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Glossary.html)
*   [11.2. Bibliography](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Bibliography.html)

*   [Index](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/genindex.html)

[Contents](#)   ::   [0.1. About OpenCSF](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/About.html)  »

[Contact Us](mailto:webmaster@opencsf.org) [License](https://opencsf.org/lib/license.html)


/Computer Systems Fundamentals - reStructuredText
===================================================================

Chapter 0   Preface[¶]

*   [0.1. About OpenCSF](https://opencsf.org/Books/csf/source/About.rst)
*   [0.2. How to Use this System](https://opencsf.org/Books/csf/source/Intro.rst)

Chapter 1   Introduction to Computer Systems[¶]

*   [1.1. Introduction to Concurrent Systems](https://opencsf.org/Books/csf/source/IntroConcSysOverview.rst)
*   [1.2. Systems and Models](https://opencsf.org/Books/csf/source/SysAndModels.rst)
*   [1.3. Themes and Guiding Principles](https://opencsf.org/Books/csf/source/Themes.rst)
*   [1.4. System Architectures](https://opencsf.org/Books/csf/source/Architectures.rst)
*   [1.5. State Models in UML](https://opencsf.org/Books/csf/source/StateModels.rst)
*   [1.6. Sequence Models in UML](https://opencsf.org/Books/csf/source/SequenceModels.rst)
*   [1.7. Extended Example: Text Parser State Machine](https://opencsf.org/Books/csf/source/StateModelImplementation.rst)

Chapter 2   Processes and OS Basics[¶]

*   [2.1. Processes and OS Basics](https://opencsf.org/Books/csf/source/ProcessesOverview.rst)
*   [2.2. Processes and Multiprogramming](https://opencsf.org/Books/csf/source/Multiprogramming.rst)
*   [2.3. Kernel Mechanics](https://opencsf.org/Books/csf/source/KernelMechanics.rst)
*   [2.4. System Call Interface](https://opencsf.org/Books/csf/source/Syscall.rst)
*   [2.5. Process Life Cycle](https://opencsf.org/Books/csf/source/ProcessCycle.rst)
*   [2.6. The UNIX File Abstraction](https://opencsf.org/Books/csf/source/UnixFile.rst)
*   [2.7. Events and Signals](https://opencsf.org/Books/csf/source/EventsSignals.rst)
*   [2.8. Extended Example: Listing Files with Processes](https://opencsf.org/Books/csf/source/Extended2Processes.rst)

Chapter 3   Concurrency with IPC[¶]

*   [3.1. Concurrency with IPC](https://opencsf.org/Books/csf/source/IPCOverview.rst)
*   [3.2. IPC Models](https://opencsf.org/Books/csf/source/IPCModels.rst)
*   [3.3. Pipes and FIFOs](https://opencsf.org/Books/csf/source/Pipes.rst)
*   [3.4. Shared Memory With Memory-mapped Files](https://opencsf.org/Books/csf/source/MMap.rst)
*   [3.5. POSIX vs. System V IPC](https://opencsf.org/Books/csf/source/POSIXvSysV.rst)
*   [3.6. Message Passing With Message Queues](https://opencsf.org/Books/csf/source/MQueues.rst)
*   [3.7. Shared Memory](https://opencsf.org/Books/csf/source/ShMem.rst)
*   [3.8. Semaphores](https://opencsf.org/Books/csf/source/IPCSems.rst)
*   [3.9. Extended Example: Bash-lite: A Simple Command-line Shell](https://opencsf.org/Books/csf/source/Extended3Bash.rst)

Chapter 4   Networked Concurrency[¶]

*   [4.1. Networked Concurrency](https://opencsf.org/Books/csf/source/SocketsOverview.rst)
*   [4.2. The TCP/IP Internet Model](https://opencsf.org/Books/csf/source/FiveLayer.rst)
*   [4.3. Network Applications and Protocols](https://opencsf.org/Books/csf/source/NetApps.rst)
*   [4.4. The Socket Interface](https://opencsf.org/Books/csf/source/Sockets.rst)
*   [4.5. TCP Socket Programming: HTTP](https://opencsf.org/Books/csf/source/TCPSockets.rst)
*   [4.6. UDP Socket Programming: DNS](https://opencsf.org/Books/csf/source/UDPSockets.rst)
*   [4.7. Application-Layer Broadcasting: DHCP](https://opencsf.org/Books/csf/source/AppBroadcast.rst)
*   [4.8. Extended Example: CGI Web Server](https://opencsf.org/Books/csf/source/Extended4CGI.rst)

Chapter 5   The Internet and Connectivity[¶]

*   [5.1. The Internet and Connectivity](https://opencsf.org/Books/csf/source/InternetOverview.rst)
*   [5.2. Application Layer: Overlay Networks](https://opencsf.org/Books/csf/source/AppLayer.rst)
*   [5.3. Transport Layer](https://opencsf.org/Books/csf/source/TransLayer.rst)
*   [5.4. Network Security Fundamentals](https://opencsf.org/Books/csf/source/NetSec.rst)
*   [5.5. Internet Layer: IP](https://opencsf.org/Books/csf/source/NetLayer.rst)
*   [5.6. Link Layer](https://opencsf.org/Books/csf/source/LinkLayer.rst)
*   [5.7. Wireless Connectivity: Wi-Fi, Bluetooth, and Zigbee](https://opencsf.org/Books/csf/source/Wireless.rst)
*   [5.8. Extended Example: DNS Client](https://opencsf.org/Books/csf/source/Extended5DNS.rst)

Chapter 6   Concurrency with Multithreading[¶]

*   [6.1. Concurrency with Multithreading](https://opencsf.org/Books/csf/source/ThreadsOverview.rst)
*   [6.2. Processes vs. Threads](https://opencsf.org/Books/csf/source/ProcVThreads.rst)
*   [6.3. Race Conditions and Critical Sections](https://opencsf.org/Books/csf/source/RaceConditions.rst)
*   [6.4. POSIX Thread Library](https://opencsf.org/Books/csf/source/POSIXThreads.rst)
*   [6.5. Thread Arguments and Return Values](https://opencsf.org/Books/csf/source/ThreadArgs.rst)
*   [6.6. Implicit Threading and Language-based Threads](https://opencsf.org/Books/csf/source/ImplicitThreads.rst)
*   [6.7. Extended Example: Keyboard Input Listener](https://opencsf.org/Books/csf/source/Extended6Input.rst)
*   [6.8. Extended Example: Concurrent Prime Number Search](https://opencsf.org/Books/csf/source/Extended6Primes.rst)

Chapter 7   Synchronization Primitives[¶]

*   [7.1. Synchronization Primitives](https://opencsf.org/Books/csf/source/SynchOverview.rst)
*   [7.2. Critical Sections and Peterson’s Solution](https://opencsf.org/Books/csf/source/CritSect.rst)
*   [7.3. Locks](https://opencsf.org/Books/csf/source/Locks.rst)
*   [7.4. Semaphores](https://opencsf.org/Books/csf/source/Semaphores.rst)
*   [7.5. Barriers](https://opencsf.org/Books/csf/source/Barriers.rst)
*   [7.6. Condition Variables](https://opencsf.org/Books/csf/source/Condvars.rst)
*   [7.7. Deadlock](https://opencsf.org/Books/csf/source/Deadlock.rst)
*   [7.8. Extended Example: Event Log File](https://opencsf.org/Books/csf/source/Extended7Events.rst)

Chapter 8   Synchronization Patterns and Problems[¶]

*   [8.1. Synchronization Patterns and Problems](https://opencsf.org/Books/csf/source/SynchProblemsOverview.rst)
*   [8.2. Basic Synchronization Design Patterns](https://opencsf.org/Books/csf/source/SynchDesign.rst)
*   [8.3. Producer-Consumer Problem](https://opencsf.org/Books/csf/source/ProdCons.rst)
*   [8.4. Readers-Writers Problem](https://opencsf.org/Books/csf/source/ReadWrite.rst)
*   [8.5. Dining Philosophers Problem and Deadlock](https://opencsf.org/Books/csf/source/DiningPhil.rst)
*   [8.6. Cigarette Smokers Problem and the Limits of Semaphores and Locks](https://opencsf.org/Books/csf/source/CigSmokers.rst)
*   [8.7. Extended Example: Parallel Modular Exponentiation](https://opencsf.org/Books/csf/source/Extended8ModExp.rst)

Chapter 9   Parallel and Distributed Systems[¶]

*   [9.1. Parallel and Distributed Systems](https://opencsf.org/Books/csf/source/ParallelDistributedOverview.rst)
*   [9.2. Parallelism vs. Concurrency](https://opencsf.org/Books/csf/source/ParVConc.rst)
*   [9.3. Parallel Design Patterns](https://opencsf.org/Books/csf/source/ParallelDesign.rst)
*   [9.4. Limits of Parallelism and Scaling](https://opencsf.org/Books/csf/source/Scaling.rst)
*   [9.5. Timing in Distributed Environments](https://opencsf.org/Books/csf/source/DistTiming.rst)
*   [9.6. Reliable Data Storage and Location](https://opencsf.org/Books/csf/source/DistDataStorage.rst)
*   [9.7. Consensus in Distributed Systems](https://opencsf.org/Books/csf/source/DistConsensus.rst)
*   [9.8. Extended Example: Blockchain Proof-of-Work](https://opencsf.org/Books/csf/source/Extended9Blockchain.rst)

Appendix A[¶]

*   [10.1. C Language Reintroduction](https://opencsf.org/Books/csf/source/CLangOverview.rst)
*   [10.2. Documentation and Debugging](https://opencsf.org/Books/csf/source/Debugging.rst)
*   [10.3. Basic Types and Pointers](https://opencsf.org/Books/csf/source/BasicTypes.rst)
*   [10.4. Arrays, Structs, Enums, and Type Definitions](https://opencsf.org/Books/csf/source/Arrays.rst)
*   [10.5. Functions and Scope](https://opencsf.org/Books/csf/source/Functions.rst)
*   [10.6. Pointers and Dynamic Allocation](https://opencsf.org/Books/csf/source/Pointers.rst)
*   [10.7. Strings](https://opencsf.org/Books/csf/source/Strings.rst)
*   [10.8. Function Pointers](https://opencsf.org/Books/csf/source/FunctionPointers.rst)
*   [10.9. Files](https://opencsf.org/Books/csf/source/Files.rst)

Appendix B

*   [11.1. Glossary](https://opencsf.org/Books/csf/source/Glossary.rst)
*   [11.2. Bibliography](https://opencsf.org/Books/csf/source/Bibliography.rst)


/Michael S. Kirkpatrick - James Madison University
==================================================

[JMU Kirkpams JMU kirkpams](https://w3.cs.jmu.edu/kirkpams/kirkpams/)

*   [CALENDAR](https://w3.cs.jmu.edu/kirkpams/kirkpams/calendar.shtml)

*   TEACHING
    
    - [CS 330 - Spring 2024](https://w3.cs.jmu.edu/kirkpams/kirkpams/330)
    - [CS 361 - Spring 2024](https://w3.cs.jmu.edu/kirkpams/kirkpams/361)
    - [Teaching Interests](https://w3.cs.jmu.edu/kirkpams/kirkpams/teaching.shtml)
    - [OpenCSF](https://opencsf.org/)
    
*   RESEARCH
    
    - [Interests & Publications](https://w3.cs.jmu.edu/kirkpams/kirkpams/research.shtml)
    - [Recent Talks](https://w3.cs.jmu.edu/kirkpams/kirkpams/talks.shtml)
    
*   SERVICE
    
    - [ACM COPE](https://w3.cs.jmu.edu/kirkpams/kirkpams/cope.shtml)
    - [ACM E & P](https://w3.cs.jmu.edu/kirkpams/kirkpams/ep.shtml)
    - [JMU CFI](https://w3.cs.jmu.edu/kirkpams/kirkpams/cfi.shtml)
    

Michael S. Kirkpatrick
======================

* Associate Professor, [Department of Computer Science](https://jmu.edu/cs/)  
* [James Madison University](https://jmu.edu/)  
* E-mail: kirkpams@jmu.edu  
* Twitter: [@kirkpams](https://twitter.com/kirkpams/)  
* Phone: 540-568-3371  
* Office: CS/ISAT 223  
* [C.V.](https://w3.cs.jmu.edu/kirkpams/Kirkpatrick-CV.pdf)

![Portrait of Michael S. Kirkpatrick in his office. Photo taken Fall 2019.](https://w3.cs.jmu.edu/kirkpams/images/jmu-faculty-expert-s20.jpg)

Current and future teaching
---------------------------

*   [CS 343 - Application Development](https://w3.cs.jmu.edu/kirkpams/kirkpams/343)
*   [CS 361 - Computer Systems II](https://w3.cs.jmu.edu/kirkpams/kirkpams/361)

Interests and professional affiliations
---------------------------------------

My professional interests revolve around one central question: _What is the most effective way to teach students how to design and implement secure and robust computer systems that will benefit society?_

*   Chair of [ACM Ethics & Plagiarism Committee](https://www.acm.org/publications/publications-board-committees)
*   Member of [ACM Publications Board](https://www.acm.org/publications/publications-board-committees)
*   Member and Officer of [ACM Committee on Professional Ethics (COPE)](https://ethics.acm.org/)
*   Member of [Electronic Frontier Foundation (EFF)](https://eff.org/)
*   Faculty Affiliate of [JMU Center for Faculty Innovation (CFI)](http://www.jmu.edu/cfi)
*   Member of [ACM Computer Science Education (SIGCSE)](http://www.sigcse.org/), [ACM Computers & Society (SIGCAS)](http://www.sigcas.org/), [National Center for Women & Information Technology (NCWIT)](https://ncwit.org/), and [ACM-W](http://women.acm.org/)

![Association for Computing Machinery is the computer science professional society](https://w3.cs.jmu.edu/kirkpams/images/acm_logo_tablet.svg)

![ACM Committee on Professional Ethics supports the ACM Code of Ethics and Professional Conduct](https://w3.cs.jmu.edu/kirkpams/images/cope.png)

![Electronic Frontier Foundation is a non-profit digital civil liberties organization](https://w3.cs.jmu.edu/kirkpams/images/eff-2023-member-member.png)

Education
---------

Ph.D., Computer Science, Purdue University, 2011

*   Dissertation: "Trusted Enforcement of Contextual Access Control"
*   Adviser: Elisa Bertino
*   Committee: Mikhail Atallah, Ninghui Li, and Dongyan Xu

M.S., Computer Science & Engineering, Michigan State University, 2007

*   Thesis: "Canary Bit: Extending Secure Bit for Data Pointer Protection from Buffer Overflow Attacks"
*   Adviser: Richard Enbody
*   Committee: Anthony Wojcik and Wayne Dyksen

B.A., Mathematics & Computer Science, Indiana University, 2001

*   Minor: Religious Studies


![James Madison University logo](https://w3.cs.jmu.edu/kirkpams/kirkpams/images/JMU-Logo-RGB-horiz-purple.png)


© 2011-2024 Michael S. Kirkpatrick.  
This work is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).


/Chapter 0   Preface[¶]
=======================

//0.1. About OpenCSF[¶]
=======================

> “Computers are useless. They can only give you answers.”
> 
> Pablo Picasso

Why this book is needed
-----------------------

As the field of Computer Science continues to evolve and expand, there is an increasing amount of pressure to increase the size of the discipline at the undergraduate level. While traditional systems areas, such as Computer Organization, Operating Systems, and Computer Networks, remain critical foundations, there are growing calls to include newer topics, such as Parallel Computing and Distributed Systems. In addition, existing systems-related areas such as Computer and Information Security continue to grow in both scope and importance.

To compensate for this increase in systems-related demand, the ACM Computing Curriculum 2013 (CC2013) adopted a flexible two-tier system for defining what constituted the core of the discipline. Under this system, Computer Science departments are advised to cover 100% of the topics identified as Core Tier 1, while also striving for 80% of Core Tier 2 topics. As part of this change, CC2013 introduced the System Fundamentals knowledge area as Core Tier 1 material. This knowledge area was intended to cover common design principles, themes, and concepts that span multiple of the traditional systems areas.

The aim of this book is to provide a breadth-first overview of concurrent systems architectures and programming. Specifically, this book aims to cover 100% of the Core Tier 1 material for the areas of System Fundamentals, Operating Systems, Network-centric Computing, and Parallel and Distributed Computing. In achieving this coverage, this book provides a flexible foundation for undergraduate Computer Science programs to achieve Core Tier 1 coverage while customizing their curriculum for Core Tier 2 as appropriate for their students. Furthermore, this approach provides a foundational scaffold for additional systems courses that can apply these principles and concepts with more in-depth study of specific areas.

Assumed background knowledge and coding style
---------------------------------------------

This book relies on a working knowledge of Computer Organization and C programming. Specifically, this book assumes that readers are familiar with the C memory model, including memory addresses and pointers, as well as the relationship between high-level languages and assembly language. A specific knowledge of x86 assembly language is not necessary, as most references to it rely almost exclusively on instructions that are similar to other assembly languages.

In our C programming examples, our aim is to provide cross-platform support as much as possible. That is, as much as possible, our code samples will work identically on any UNIX (such as macOS) or Linux system. We use the [GNU coding style](https://www.gnu.org/prep/standards/standards.html), adhering to the C99 and [POSIX.1-2017](https://pubs.opengroup.org/onlinepubs/9699919799/) standards. The primary exception to this approach is the chapter on Interprocess Communication (IPC), which is inherently platform dependent. For instance, macOS employs System V IPC semantics and only supports a limited subset of POSIX IPC features; on the other hand, POSIX is the preferred interface for Linux. As such, avoiding the interface conflicts is unavoidable. To reduce the impact of this conflict, however, we will focus on the POSIX interface in the main text

Guiding educational philosophy principles
-----------------------------------------

While the focus in this book is obviously on concurrent computing systems, we approach this topic from a pragmatic stance rooted in active learning. Specifically, throughout the text, our emphasis is to focus on the application of systems concepts and integrating this material to the readers’ prior knowledge. Our aim is to provide a solid foundation of core computer systems ideas to all students, regardless of whether they go on to pursue advanced study in the systems area or other domains.

Throughout the book, we strive to illustrate concepts based on the notion of worked examples. Within the context of this book, we apply this idea by providing working code wherever feasible, with comments and the surrounding text documenting the code to identify the key steps that explain why the code works. For some particularly advanced concepts, providing succinct illustrative code for an undergraduate context is simply not feasible. However, we deliberately try to explain concepts with code as much as possible to keep the discussion concrete and practical.

Text conventions
----------------

Throughout this text, we use a number of text boxes and structures to highlight key ideas and concepts. As noted previously, this book places a heavy emphasis on providing working code samples. To achieve this goal, we highlight key C functions in boxes structured as below. These boxes start by identifying the required header file (unistd.h in this case) to include. Each function listed includes the function prototype and a brief description.

![Decorative C library image](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images-Library.png)
C library functions – <unistd.h>

* * *

`pid_t fork(void);` Create a new process.

Code listings provide minimally functional code to illustrate these functions’ uses. Each listing uses line numbers so that the surrounding text can more easily emphasize key lines. Each listing is numbered as N.M, with N being the chapter number and M specifying a numerical order within the chapter. All code listings are available for download at [https://csftext.org/](https://csftext.org/). Each chapter ends with an Extended Example that is not explicitly numbered as a code listing. These Extended Examples are complete stand-alone programs that provide a full implementation of how to apply the chapter’s concepts.

```cpp
/* Code Listing N.M:
   Sample code to illustrate a point
 */

pid_t child = fork ();
```

We use three types of text boxes to emphasize certain points throughout the text. These boxes use the icons and title structures shown here.

![Decorative bug warning](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images-BugWarning.png)
Bug Warning

* * *

The text in this box describes common coding errors, some of which can lead to serious security vulnerabilities and system crashes.

![Decorative example icon](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images-Example.png)
Example 0.1.1

* * *

These examples provide a concrete instance to illustrate a general concept. These examples often focus on mathematical calculations using formulas recently described or provide a specific example of the binary data that makes up a message header or contents. These examples are numbered (again, with N as the chapter number and M increasing numerically) so that the surrounding text can compare and contrast examples as needed.

![Decorative note icon](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images-Note.png)
Note

* * *

These boxes highlight related points of interest to the topic being discussed. These boxes also emphasize cross-platform issues that can arise.

Structure and historical ordering
---------------------------------

With the exception of the introductory chapter, the rest of the book is generally structured to show the chronological development (with slight variations) of technologies and abstractions related to concurrent systems. Chapter 2 covers the abstractions of processes and files, which date back to the earliest computer systems. J. Presper Eckert and John Mauchly used the file abstraction to describe stored programs in the UNIVAC (1951); earlier references in the 1940s also used this concept. Processes were key to the earliest multiprogramming systems, such as the Atlas Supervisor (1962) and CTSS (1961).

As soon as computers were able to run multiple processes concurrently, the natural next step was to make the processes communicate with each other. Chapters 3, 4, and 5 focus on this phase of historical development. IPC and networking technologies emerged in the next two decades to support this goal. Douglas McIlroy, who had previously contributed to the Multics OS, introduced the concept of pipes in UNIX (1973). Joel M. Winett defined the socket in RFC 147 (1971) as unique identifiers for transmitting data to the ARPA network. Vint Cerf introduced the term Internet in RFC 675 as part of the first specification of TCP. The TCP/IP protocol suite was later standardized in 1982, with the Internet emerging as the successor of ARPANET.

Chapters 6, 7, and 8 can be characterized as an example of the quotation, “Plus ça change, plus c’est la même chose.” Or put another way, everything old is new again. IBM OS/360 introduced threads in 1967. Edsger Dijkstra’s THE OS, which introduced semaphores as a software construct, was released the following year. Dijkstra introduced the Dining Philosophers problem in 1965. Although these techniques date back to these early years of computing, their resurgence in the 1990s and 2000s is instrumental in modern computing.

In 1985, Tony Hoare revisited the Dining Philosophers problem as part of his work on defining the Communicating Sequential Processes (CSP) language. The POSIX thread and synchronization standards were defined in POSIX.1c-1995. There were many factors driving the interest in multithreading, including the development of Linux and its use for cluster systems, as well as the impact of the “power wall” on Moore’s law and integrated circuit design.

Chapter 9 closes with an introduction to parallel and distributed computing. As with multithreading, the general concepts of distributed computing are decades old. The Internet, for instance, can be characterized as a distributed computing system. Leslie Lamport introduced distributed timestamps in 1978. However, the past two decades have brought these techniques to new heights with massively parallel supercomputers (such as Cray Titan or IBM BlueGene) and grid computing (Berkeley Open Infrastructure for Network Computing (BOINC), [Folding@home](mailto:Folding%40home), Great Internet Mersenne Prime Search (GIMPS)). Furthermore, these topics are frequently studied in stand-alone or graduate courses. As such, this chapter aims to provide a teaser or introduction to advanced study in these topics.

Chapters 2 – 9 all begin with a variant on the following diagram, which roughly summarizes the historical timeline described above. As described previously, the time frames illustrated here correspond to when the critical nature of these technologies came to the forefront of computing and computer science research, not necessarily when they were first created. Synchronization dates back to the 1960s and the foundations of much of the distributed systems field can be traced back to the 1970s and 1980s.

![Timeline of the key technologies developed in computer systems](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Timeline.png)

This chronological structure is not the only possible ordering for these topics. For instance, one could choose to cover threads immediately after processes. An advantage of this approach is the ability to highlight their differences as units of execution. One difficulty with this approach is the tight coupling of multithreading with the need for synchronization, due to the shared address space. A second challenge is that adding more material between the discussion of processes and IPC might make the latter seem less relevant to modern system design; why bother with processes and message queues when synchronized threads (particularly using Linux tasks) facilitate a more lightweight form of data exchange?

Another approach would be to move the networking concepts later, after threads and synchronization. This structure provides an elegant split between single-system (processes, IPC, threads) concurrency and multi-system (sockets, networks, distributed systems) concurrency. However, this approach then lessens the emphasis of sockets as a form of IPC, as sockets, (particularly kernel sockets) can be used within a single system. It also decouples the discussion of multithreading from parallelism.

In sum, there are multiple paths that these topics could be addressed in a book or a course. Readers or instructors can select which path they feel is most appropriate, bearing these tradeoffs in mind.

Funding support
---------------

This online text was supported through the [VIVA Course Redesign Grant Program](https://vivalib.org/c.php?g=836990&p=6638954).



//0.2. How to Use this System[¶]
================================

**Welcome to OpenCSF!** OpenCSF is an open source project whose goal is to provide a freely available textbook for Computer Systems Fundamentals. OpenCSF is built on the OpenDSA platform, created at Virginia Tech. The automated build process uses the OpenDSA scripts, but with some custom modifications to specify formatting features and a more custom style. In this initial release, OpenCSF contains the text of Computer Systems Fundamentals, along with interactive question sets. These question sets are not linked to any particular learning management system.

**Question Sets:** Most modules finish with a collection of multiple choice, True/False, or type-a-number questions. To get credit for a question set, you will have to answer some number of the questions correctly (the exact number required can be different for each question set). Once you have credit, the interface should indicate this. You can still get more questions at that point if you would like more practice. Above the question on the right-hand side is a counter to indicate your current number of questions correct out of the total number needed to complete the exercise. If you answer a question wrong, your progress toward the completion threshold will go backwards by one point. (And you will still have to answer that question before you can continue!) Note that once you have been given completion credit for the question set, you cannot lose that credit by answering more questions, even if you then get some wrong.

The question sets work by randomly selecting from the questions available to that set. Typically, once you correctly answer a question, you will not see it again (or at least not with the same inputs). But if you answer it incorrectly (and then clear it with the correct answer), it might appear again.

The questions will have “hints” that you can use to help you figure out the answer. If you take a hint, you will not get credit for that question toward completing the exercise. However, you will not lose a point on that question, either.

**Enlarging Equations:** Math is rendered using the MathJAX library, which gives you a lot of options on how you can see things. Most importantly, if you right click on any math equation, you will get a context menu that includes “Math Settings”. This in turn has a sub-menu named “Zoom Trigger”. With that, you can set zoom to “hover” or “click”. From then on, hovering or clicking (if you had selected one) on any equation will make it larger. This can really help with reading some of the equations.


/Chapter 1   Introduction to Computer Systems[¶]
================================================

//1.1. Introduction to Concurrent Systems[¶]
============================================

> “The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform.”
> 
> Ada Lovelace

[Concurrency](#term-concurrency) is hard. When your program is performing dozens of tasks at the same time, a single crash in any of them can cause all of them to stop. Perhaps one of the tasks was supposed to wait for some critical calculation to complete, but it proceeded too early. Perhaps one task corrupted a pointer that another was using, leading to a mysterious segmentation fault. Perhaps two tasks overwrote each other’s intermediate calculations, leading to incorrect final results. Once you begin writing software that goes beyond a single coherent algorithm, you quickly learn that you are entering a world with a completely new set of programming errors that you never imagined. Each line of code can interact and interfere with other parts of the system in new, unintended, and unpredictable ways. In other words, concurrency is hard. This book aims to make it less so.

🚀 Chapter Objectives

* * *

In this chapter, we will address the following instructional objectives:

*   We will summarize the major themes and guiding principles this book addresses.
*   We will describe the relationship between the notions of systems and models.
*   We will characterize the key features of common system architectures.
*   We will introduce UML state and sequence models, which will be used throughout the book.

Our focus in this book is to establish a foundation of key principles and techniques that are be used to implement concurrent systems software. This class of software ranges from operating systems (OS) and web servers to databases and scientific modeling platforms. What these types of programs have in common is that they subdivide complicated tasks into smaller ones that can be run at the same time. In doing so, these approaches introduce [nondeterminism](#term-nondeterminism) and complexity, as the programmer must give up some control over what steps and calculations will be performed in which particular order. If the programmer writes code that says tasks A, B, and C can be executed in any order, they will quickly learn whether or not that claim is actually true.

At this point, some readers may object: they do not plan to write systems software, so this material is not relevant for them. While the first part of that claim may be true (or it may not), the latter is certainly not the case. The principles used to build systems and infrastructure can be applied to more traditional applications; if your application uses event handlers to detect mouse clicks or key presses, you have a concurrent program. If your code is structured to use a web-based application programming interface (API), you have a concurrent program. If your application performs graphical calculations or applies artificial intelligence, you have a concurrent program (or you should, as it would run faster).

In short, concurrency is everywhere. Mastering the concepts and principles of concurrent programming can help you build better software, whether the end result is a new application or a platform for applications. In this chapter, we will introduce the major themes and guiding principles that we will use throughout the book. We will also define some key terminology and notation that are helpful in characterizing concurrent systems. Without further delay, let us begin our exploration.



//1.2. Systems and Models[¶]
============================

Throughout this book, we are considering the idea of computer systems from a perspective that strives to apply insights from [systems theory](#term-systems-theory). From this view, a [system](#term-system) is an integrated collection of entities and their interactions. The computer itself is a system. Computers are assembled from disparate hardware components that include a central processing unit (CPU), storage devices, input and output devices, random access memory (RAM) cards, and a printed circuit board (PCB) that links them all together. A software OS executes on top of these components, thus integrating their independent functionality into a more complex entity. To fully understand the nature of the computer as a whole, it’s important to understand the components’ independent functionality as well as how they work together.

![A computer is a system of interacting hardware and software entities](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.1.1.png)

Figure 1.2.1: A computer is a system of interacting hardware and software entities

The system is made more complex by adding application software to the mix. Each application is constructed from code modules, variables, and other run-time memory structures that are compiled and integrated to provide a single higher-level function. But software does not run itself; the software must be executed within the greater context of computing, as a sequence of steps performed by a physical computer with the help of an OS. Some applications run in isolation from one another, but they still require the OS and hardware. Others communicate with services that provide information or perform complimentary tasks, allowing the developer to build more powerful software without having to start from scratch. [Figure 1.2.1](#1_2_sysinteractfig) summarizes this view of the computer as a system of interacting components.

In short, all of computing is a system. As computer scientists continue to explore the power of computing, we can take this claim even further to suggest that computing is made up of a system of systems. In order to stream a movie over the Internet, the application software must integrate network connectivity and local file storage, probably with the help of a graphics processing unit (GPU) that improves the visual quality of the experience. Each of these hardware and software components are systems, and the application is coordinating their interaction to serve the user. As another example, consider large-scale scientific projects that model changes in the climate or advanced quantum mechanics phenomena. This type of work involves linking thousands of parallel computing nodes that coordinate their calculations and exchange data as needed, creating truly complicated systems and interactions. Our goal is to understand the common principles and strategies that make these systems work.

///1.2.1. Models as Representations[¶]
--------------------------------------

A first step toward understanding a system is constructing a [model](#term-model) – a simplified representation – of the system. [Figure 1.2.1](#1_2_sysinteractfig) above is a model of a computer as a system. As a simplification, models necessarily omit details that are present in the system itself. Some models have visual representations, facilitating human interpretation of the system. That is, [visual models](#term-visual-model) are very good tools for people to make sense of the entities and their interactions. System designers, developers, and users can use visual models to avoid mistakes. Other models are [formal](#term-formal-model), written in a mathematical specification language. One way to characterize the differences between models is to describe their [level of abstraction](#term-level-of-abstraction). Visual models may have a relatively low level of abstraction, including details that are aesthetically pleasing but not important. Formal models have a relatively high level of abstraction, omitting everything except the bare minimum.

Consider the models shown in [Figure 1.2.2](#1_2_sysphysmodel). The model on the left has the lowest level of abstraction, as it includes details that show a yellow box on a grey ramp. This scenario is then converted into a free-body diagram in the middle. Free-body diagrams are used in physics to illustrate the forces acting on an object, while removing unneeded details such as the drawing of the box and the ramp. The equation on the right is the formal model that captures this information. The net force F is the sum of the forces created by gravity (mg), the normal force N, and the force of friction \(F_f\). All three models convey the same information, but they provide different levels of abstraction.

![Three models for the same physical phenomenon](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.1.2.png)

Figure 1.2.2: Three models for the same physical phenomenon

A [static (or structural) model](#term-static-model) describes the fixed, unchanging features of a system, while omitting explanations of how the system or entities change. Readers with a background in object-oriented programming may be familiar with class diagrams created using [unified modeling language (UML)](#term-unified-modeling-language). These models describe features like class inheritance and sub-typing, while omitting characteristics like the values of an object’s variables or the messages it uses to communicate with other objects. Throughout this text, we will focus repeatedly on one particular type of static model, the system architecture.

In contrast, a [dynamic (or behavioral) model](#term-dynamic-model) focuses on changes to the system. These models are particularly important to understanding systems, because many of their key features are [emergent properties](#term-emergent-property). An emergent property is one that results from the dynamic features of the system and is not intuitive from static models alone. We will be using two dynamic UML models throughout this book. A [state model](#term-state-model) describes the system’s possible states – defined by meaningful combinations of system parameters – and the possible transitions between them. For instance, a streaming video service may characterize different states to represent “requesting the video,” “buffering received data,” “pausing the video,” and “playing the video.” A [sequence model](#term-sequence-model) illustrates the order of messages and key events that a system experiences over time. One use of a sequence model is to describe a specific order in which state transitions occur. These models provide a clear illustration of network communication protocols and messages to coordinate parallel and distributed computation.

///1.2.2. From Models to Implementations[¶]
-------------------------------------------

Models are useful tools for analyzing and illustrating the behavior of a system. Well-constructed models of the natural world can help scientists develop insight into and explain real-world phenomena, such as the chemical reactions that release energy stored in food, the wave-like behavior of electrons in an atom, or the fractal designs in crystals. User manuals for appliances, vehicles, children’s toys, etc., use illustrations as models to convey information to consumers who are not necessarily experts in the field.

In contrast to many other fields, the field of computer systems places a strong emphasis on turning models into implementations. That is, our goal is not simply to build and interpret models, but to turn these models into working artifacts. This implementation process can be quite challenging and creates many opportunities for errors; mistakes that cause the system to behave differently from the model can render the model useless or, even worse, increase confusion. As such, much of our focus throughout this book is on successfully turning models into concrete, executable implementations.



//1.3. Themes and Guiding Principles[¶]
=======================================

When describing systems and their behavior, there are three central themes that emerge. The first is that _computer systems create a platform for applications_, acting as a foundation for advanced work. Using that foundation effectively requires understanding that _systems have semiotics_, which means that all communication – whether between system components or between the system and the user – exist within a context that influences how messages are interpreted. Lastly, because system designers cannot predict how the system will be used – particularly when the number of users increases – _systems entail complexity_, leading to unintended properties that may be beneficial or detrimental. The remainder of this book is devoted to addressing these themes, while identifying guiding principles for designing and implementing systems correctly and efficiently.

///1.3.1. Systems as Foundations of Computing[¶]
------------------------------------------------

Computer systems are not built to exist in isolation. Rather, computer systems are platforms intended to create a foundation for advanced applications. As shown in [Figure 1.3.1](#1_3_themessysapps), applications rely on the OS to provide common services, such as access to the file system or network. Some OS components, such as the scheduler, rely solely on the hardware as a platform. Others rely on subsystems within the OS, such as the generic file interface [[1]](#f1). These subsystems, in turn, rely on the interfaces to hardware components, such as storage devices, memory units, or network cards. In all cases, a higher-level software component is relying on a reactive system that provides service upon request.

![Applications rely on OS services, built on top of other OS services and hardware](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.1.3.png)

Figure 1.3.1: Applications rely on OS services, built on top of other OS services and hardware

One key factor to consider when designing concurrent systems software is the [scarcity of resources](#term-scarcity-of-resources). Individual computers have a limited number of CPU cores that can execute software at the same time. Similarly, applications must share access to a finite amount of physical memory resources. Consequently, systems software must explicitly manage and control access to shared resources, so as to prevent applications from interfering with each other or corrupting each other’s work.

In order to manage the scarcity, system designers must address the inherent tradeoffs based on the available resources. There are several common tradeoffs that should be considered. The [space/time tradeoff](#term-space-time-tradeoff) describes the phenomenon that using more resources for a short period of time can lead to faster execution. One example of this is the creation of a buffer for exchanging data. A small buffer would lead to more messages send back and forth, leading to a larger total delay; a large buffer could allow all data to be sent in a single message, but that reduces the memory (space) available for other software running.

A second tradeoff to consider is the [interface abstraction](#term-interface-abstraction). Consider the system design in [Figure 1.3.1](#1_3_themessysapps), described previously. In this OS design, the network interface, file system, and memory management all rely on a common interface abstraction that everything is a file. That is, all of these services should be treated as a sequence of bytes, with an integer file descriptor used to identify the data stream. This approach allows all of these subsystems to use common functions `read()` and `write()` to interact with the system. However, this generic interface obscures some key differences between network communications and data stored on a hard drive. For instance, once a message is retrieved from a network card, the message is removed from the network card’s internal memory, as if it has been destroyed; that is, an application cannot rely on the network card to store the message persistently for reference whenever it is needed. On the other hand, that is exactly the service that files on a hard drive provide. Consequently, abstracting away these differences provides a common interface, but eliminates the possibility of optimizing the interface based on the specific hardware component.

Another tradeoff that commonly arises involves [security vs. usability](#term-security-vs-usability). A hard drive can be made perfectly secure by smashing it with a hammer; no malicious adversary would then be able to steal your data, but you would no longer be able to use it either. In a significantly less extreme example, network messages might employ security techniques to ensure all of the data is transferred and only the intended recipient reads it. The downside is that this level of security may be unnecessary and, even worse, the verification procedures involved are too slow for the application to use.

All computer system designs must entail compromises. The scarcity of resources makes it necessary to balance the factors that influence these tradeoffs in order to support higher-level applications effectively. System architects – especially those building systems intended for general-purpose use – must predict what applications will be deployed and how best to balance these factors for the specific context of those applications.

///1.3.2. Systems and Complexity[¶]
-----------------------------------

Computer systems are [complex](#term-complexity). This does not simply mean that they are complicated, though that is certainly the case. Taking a system design from specification to implementation requires close attention to details that may not be immediately obvious. Changing the order of two lines of code, using a different compiler option, or inadvertently freeing a pointer at the wrong time can all lead to catastrophic system failures. These are examples of how implementing computer systems is a complicated task that requires special care.

Complexity, on the other hand, means something more than just being complicated. Complexity implies that the system exhibits [emergent properties](#term-emergent-property) that are not intuitive or obvious from the design. Interactions between entities in a system leads to unintended combinations of behaviors and system configurations. One example of an such unanticipated behavior in an OS is known as Bélády’s anomaly, which describes the unusual scenario where adding more memory resources to a computer actually leads to slower performance. Other emergent properties arise when entities compete for access to shared resources. In deadlock, two or more processes simultaneously block each other’s access in a permanent and unresolvable stalemate; process A has something that process B needs and vice versa, but neither will give up what they possess. In priority inversion, a high-priority task may be prevented from running by a low-priority task.

Networked computers often experience attacks as emergent properties that exploit non-obvious vulnerabilities. One example of such an attack is a syn flood, in which thousands or millions of coordinated nodes send requests for service to the same server. As the server allocates resources to process these requests, it quickly runs out of memory and crashes. Another example is a replay attack on a security protocol. In a replay attack, a malicious adversary snoops on network traffic and observes that a user was successfully logged in after sending a particular message; the adversary does not actually know what the message says, but they send it anyway and are also granted access.

In all of these cases, complexity is the result of unanticipated sequences of actions that lead to new and unusual configurations. As a result, computer system designers must accept that **reliability is elusive**. When a system is designed and implemented, the developers routinely perform a number of tests to convince themselves that the system behavior is correct. However, these testing procedures are inherently limited and cannot provide strong assurances. Given this challenge, many systems are designed to use redundancy to overcome probabilistic errors.

📜 Example 1.3.1

* * *

To illustrate how redundancy provides reliability, consider a system component that has a 1% probability (p = 0.01) of failing within a particular time frame. Adding a second copy as a back-up, decreases the probability of that component failing to 0.01% (p = 0.01 * 0.01 = 0.0001), as an overall failure requires both copies to fail _at the same time_. Adding a third copy as backup decreases the probability to 0.0001% (p = 0.01 * 0.01 * 0.01).

📜 Example 1.3.2

* * *

As another example of reliability through redundancy, consider the TCP network protocol. In TCP, a device sends a [segment](#term-segment) of information to another device, waiting for a response. If the response does not arrive within a pre-designated time frame, TCP resends the message but increases the waiting time limit. Note that increasing the wait time has the effect of reducing the probability of failing to get the response back in time. For instance, maybe the first time limit was 10 ms, but the response actually took 12 ms. For the second message, the time limit was increased to 15 ms, so the 12 ms response time is acceptable. Sending redundant messages, especially when the time limit for receiving a response is increased, improves the overall reliability of the protocol.

Computer systems designers and developers must also accept that **system complexity breeds misunderstanding**. In many cases, the root causes of an error or failure are hidden by the complexity. Since errors and other emergent properties arise from unanticipated sequences of actions, anyone trying to debug the system afterward might not have enough information to determine what went wrong; the system simply may not have been logging the most relevant information to reconstruct the cause. As an example, consider the following line of C code:

```sh
printf ("hello, %s", user);
```

Assume that the system has crashed, and the developer is using this line as a hint for debugging. After examining the output, the developer observes no mention of this line of code and concludes that the error occurred before this point. Further investigation reveals that the `user` pointer was initialized correctly, so this line of code did not cause the segmentation fault.

Without closer investigation, it is possible that any of those claims were wrong. The `user` variable may have been initialized correctly, but another part of the code may have corrupted the pointer, making it null. Alternatively, the null-byte at the end of the string may have been modified, converting the `user` string “Alice” to turn into “Alice&76as87d92f00d9f8…” If that new string is long enough (before encountering another null-byte, the parameter would overflow the stack in an internal library function.

Even worse for debugging, _this line may have been executed successfully without error_. Observe that the printed string does not end with a `'\n'`. When this happens, `printf()` does not necessarily cause the message to be printed immediately. Rather, the message gets placed into a buffer for printing when the buffer gets flushed. If the program crashes before the buffer is flushed, the message is simply lost. Consequently, when debugging complex systems, it is critically important to use the correct tools; relying on print statements is no longer sufficient, as they may cause you to look in the wrong place.

///1.3.3. The Semiotics of Computer Systems[¶]
----------------------------------------------

The term [semiotics](#term-semiotics) [[2]](#f2) describes the use and interpretation of symbols and signs. That is, semiotics focuses on how communication should be interpreted based on the language used and the context of the messages. Understanding the semiotics of a message involves considering the relevant [syntax](#term-syntax), [semantics](#term-semantics), and [pragmatics](#term-pragmatics). The syntax defines to the rules that control how symbols can be combined to create a message. The semantics define the meaning of the individual symbols. The pragmatics characterize the relationship between the message and the entity making sense of the message. The semiotics of computer systems can be explored along three dimensions: machine-to-machine, machine-to-human, and machine-to-world.

The first dimension – and the most well understood – focuses on the communication between two computer systems. That is, when two systems need to exchange information, what data structures, communication protocols, and applications will be involved? In _Computer Systems: A Programmer’s Perspective_ [[Bryant2015]](Bibliography.html#bryant2015), Bryant and O’Hallaron capture this idea with the statement that **information = bits + context**. That is, computers store data in binary form (bits) that conveys no meaning without applying the relevant context. The semiotics view of systems expands Bryant and O’Hallaron’s statement by distinguishing the semantics (what the symbols mean) from the pragmatics (what the computer does in response).

![The same sequence of 8 bits can have several possible semantics](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.1.4.png)

Figure 1.3.4: The same sequence of 8 bits can have several possible semantics

The syntax dimension of semiotics imposes a structure on the bits; a simple example of this structure is the fact that binary data is typically read as bytes – groups of eight bits. Consider the bits shown in [Figure 1.3.4](#1_3_themesbits). The semantics of _byte_ indicate that the least-significant bit is placed on the right, yielding the unsigned integer value 174 or the signed integer -82. Applying the semantics of characters would yield the value ‘®’. An _octet_ is another numeric semantic interpretation group of eight bits, but with the least-significant bit on the left; this octet has the decimal value 117. The pragmatics of the context implies whether or not the program in question can read that kind of file.

📜 Example 1.3.3

* * *

To make these terms more concrete within the context of computing, consider the outputs of calling the `hexdump` command-line utility on two files (for the latter, only selected lines are shown):

```sh
$ hexdump -C data.xml
00000000  3c 6d 65 6e 75 3e 3c 6f  70 74 69 6f 6e 3e 4f 70  |<menu><option>Op|
00000010  65 6e 20 46 69 6c 65 3c  2f 6f 70 74 69 6f 6e 3e  |en File</option>|
00000020  3c 2f 6d 65 6e 75 3e 0a                           |</menu>.|
00000028
$ hexdump -C data.mp3 | head -n 150 | tail -n 3
000024d0  03 11 00 3f 00 fe 9d 6f  f4 d3 3e 8b a8 4c 23 8a  |...?...o..>..L#.|
000024e0  e6 2b d9 8c 37 70 18 8a  23 45 6a 03 a2 bc 6e 11  |.+..7p..#Ej...n.|
000024f0  dd 60 31 99 50 4c 1e de  44 93 7b 38 b7 12 35 77  |.\`1.PL..D.{8..5w|
```

Different programs will read and interpret these files in distinct ways, starting with the syntax. The syntax declares how the file contents must be structured. If the program is expecting XML, it would expect the human-readable text structure as shown in the first file. A program expecting XML would not be able to interpret the latter file successfully. A media player program, on the other hand, would know how to interpet the binary data structures of the second file.

The semantics of the system then define what the file contents mean. For instance, the XML data is intended to indicate that a user should be shown a menu that allows a user to open a file; it is unlikely that the creator of the file intended this XML code to express how much they liked a movie they saw. The semantics of the symbol `"<menu>"` has an intended meaning that the message is conveying. As for the pragmatics, consider what would happen if you opened the MP3 file in an audio player as compared to a text editor. There is a relationship between the MP3 file (the message) and the audio player (the entity interpreting the message) that does not exist between the MP3 file and the text editor.

The second way to consider the semiotics of computer systems is to examine how computer systems and humans interact. In applications programming, this dimension includes usability concerns. In the systems realm, understanding the syntax and the semantics entails learning a variety of functions that define the API for interacting with the OS and the hardware. Computer systems provide many services that can be used to exchange information between applications and coordinate the intended work. Much of this book is devoted to explaining and documenting these APIs and how to use them.

At the same time, it is vital to address the pragmatics of computer systems-to-human interactions, as **human cognition and automated computation differ**. Some concepts, such as multitasking (“doing multiple things at the same time”), mean different things to humans and to computer systems. To illustrate this concept, consider the following two lines of C code:

```sh
x = x + 1;					x = x - 1;
```

Assuming the variable x had an initial value of 5, what would be the result of executing these two lines of code “at the same time”? Our intuition is that these lines of code are not literally executed at the same physical moment, as they contradict one another. Instead, “at the same time” to us means “at approximately the same moment” rather than the literal interpretation. The next question we could ask, then, would be what final value of x should be expected. It may come as a surprise that, after executing both of these statements “at approximately the same moment”, x could end up with three possible final values: 4, 5, or 6. Understanding and addressing the differences between human cognition and computing requires acknowledging the pragmatics of machine-to-human interactions.

The third dimension, which is of particular interest (but not limited) to those who design and build embedded systems, is the relationship between the computer and the physical world. An embedded system is one in which computing is integrated into a larger system that is not obviously devoted to computation; examples of embedded systems include robotics, non-autonomous (i.e., not self-driving) automobiles, or entertainment systems. The key insight to this dimension of semiotics is that **the computer itself is a model**. That is, anything that can be achieved with the computer can only ever be a simplified approximation of the world.

Another way to describe this issue is to note that computing is built on a digital (or discrete) simulation of a world that is inherently analog (or continuous) in nature. One simple illustration of this fact is to consider floating-point representations of real numbers. There are more real numbers between 0 and 1 than we can store by combining all of the computing resources in the entire world. That is often not a problem, as 0.1999999999999999 is a close enough approximation of 0.2 for practical uses. [[3]](#f3)

This digital approximate of analog phenomena can be found in other aspects of computing. For instance, the very notion of a bit 0 or 1 is created by denoting whether or not an electrical device measures a voltage above or below a given threshold. Devices can also be placed into a high impedance state, in which the value of a bit is neither 0 nor 1. Similarly, time is a continuous physical phenomenon that is not measured identically. Many computers create a discrete approximation of time by measuring the vibrations produced by a crystal subjected to an electric charge. If we were to zoom in on the measurements, there would be slight variations that can neither be predicted nor controlled.

This last factor – time – is the most relevant to the discussion in this book. Concurrent computer systems are, by definition, focused on coordinating the execution of software that is intended to perform multiple tasks at more or less the same time. To accomplish this goal, the system designer must take steps to inject timing controls – synchronization – into the implementation. At the same time, the designer must learn to recognize that there are limits to what can be accomplished regarding time. As this book progresses, time will become a central theme, leading to some well-established but counterintuitive results that the challenge of time makes certain goals – such as getting concurrent systems to agree on what time it is – are impossible.

[[1]](#id1)

Our perspective of the OS structure derives from the UNIX and Linux design, in which all devices are files.

[[2]](#id2)

Semiotics are commonly studied in human-centered fields of study, such as sociology or anthropology. However, approaching the study of computer systems from this perspective allows one to examine nuanced features of this field that lead to mistakes and failures.

[[3]](#id4)

The choice of 0.2 was intentional here. The standard floating-point representation, IEEE 754 format, cannot represent the exact value 0.2, as it cannot be written as a finite summation of powers of two.



//1.4. System Architectures[¶]
==============================

As stated previously, a model is a simplified representation of a system. Models are created to illustrate particular features of the system, while omitting other details that are not relevant to the current discussion. In other words, if we characterize a system implementation—with all details, variables, and states set to have particular values—as exhibiting a low level of abstraction, a model would possess a higher level. We can construct a model from an implementation by removing details, or we can construct an implementation by adding details to a model. However, both the model and the implementation are focused on a particular application. For instance, an automobile’s cruise control system would be an implementation, created from a model that illustrates which components determine the vehicle’s velocity, which entities calculate the changes to acceleration needed, and which ones adjust the rotation of the wheels. Both the model and the implementation are focused on the specific issue of acceleration in an automobile.

We can also approach the question of models from an even higher level of abstraction by examining the system’s [architectural style](#term-architectural-style). The architectural style describes the relationship between entities in the system and how those entities can communicate. Selecting a particular architecture has significant overall impact on the system’s performance and design style. Each approach has its advantages and disadvantages, and these must be considered for the intended use of the system.

///1.4.1. Client/Server Architectures[¶]
----------------------------------------

![Multiple clients connect to a single server](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.1.5.png)

Figure 1.4.1: Multiple clients connect to a single server

[Figure 1.4.1](#1_4_archcliserv) shows the logical structure of the [client/server architecture](#term-client-server-architecture), a well-known and intuitive paradigm for building computer systems. The developer creates a centralized server that receives requests for service and responds accordingly. Web servers listen for requests created by a user’s web browser (the client) or an app running on a smartphone or tablet. Email servers allow you to send a message to someone else using a client application running within a web page or as part of a native email application. Network file servers can store large amounts of data, such as a collection of movies and music files; a streaming media client can then retrieve these files as indicated by the user.

When creating a client/server architecture, the first key question is to determine how the client knows how to locate the server. The [uniform resource identifier (URI)](#term-uniform-resource-identifier) approach is the most common, relying on standard Internet services to fill in the details. For instance, the URI `www.example.com/index.html` indicates that there is a file named `index.html` that can be accessed from the `www.example.com` web server. This answer is not entirely satisfying, because it leaves unanswered how you are supposed to know where to find `www.example.com`; the answer is you use a DNS client to contact a DNS server as a preliminary step toward using your web client to contact a web server. The main idea in this approach is that there is a single entity (the server) that provides a service to clients upon request.

Client/server architectures are common in user-focused applications, but they are also widely used in systems that do not provide visible services. As an example, several components of operating systems (OS) are designed according to a client/server architecture. For instance, the graphical user interface (GUI) is typically structured as a server that displays information in windows, manages the desktop, and directs keyboard or mouse input correctly; each application that is running acts as a client, sending requests to the GUI to update the visual display. Components that provide information about what applications are running, what the current time is, or whether a login request is allowed are all structured as servers. That is, anything that is typically described as an OS _service_ exhibits a client/server architecture.

One advantage of the client/server architecture is the simplicity of handling updates. If a file is renamed, its contents modified, or changed in any other way, this change only needs to occur once. After the file or resource is updated on the server, the new version can be made immediately available. Relatedly, the centralized structure makes it easy to detect security breaches or data corruptions. If a user reports a broken link or some other bad result, the problem only needs to be fixed in one location.

From a communication perspective, client/server architectures rely on communication [protocols](#term-protocol), standards that specify precisely how to request a service and interpret the response. Protocols are necessary, because there are no pre-defined assumptions regarding who can be a client. Returning to the previous examples from above, a web server defaults to responding to requests from any web browser connected to the Internet. The email server that you are using will accept incoming messages from anyone, including a long-lost friend attempting to reconnect or a criminal enterprise sending you spam with a virus attached. So long as the client and server adhere to the communication protocol, messages can be delivered to and from the server. Granted, after the server receives the message, it may use a spam filter or other security mechanism to discard it without your knowledge; however, the message was still delivered to the server itself.

Client/server architectures have a key feature that is both an advantage and disadvantage: centralization. Maintaining a single server makes it easy to process changes efficiently and provide consistent service. The downside of this is that the system has a _single point of failure_. If the web server crashes, then the entire service is down. The single access point also creates a _bottleneck_ when many clients try to request service at the same time. As an analogy, imagine going to a restaurant that only has one table. That is not a problem for most of the day when there are few (if any) customers arriving; however, the restaurant would quickly go out of business since it could not serve enough customers during the busy (and profitable) meal-time rushes. One way for client/server architectures to solve this problem is to use [replication](#term-343). Instead of using only one server, a few servers coordinate the work very closely, providing backup to each other. This would be analogous to adding more tables to the restaurant.

///1.4.2. Peer-to-peer (P2P) Architectures[¶]
---------------------------------------------

This idea of replication can be extended even further to the notion of [peer-to-peer (P2P) architectures](#term-peer-to-peer-architecture). [Figure 1.4.2](#1_4_archp2p) illustrates the logical structure of a P2P architecture. P2P architectures retain many of the key features of traditional client/server architectures, with the exception that every (or most) participating entities take turns acting as both clients and servers. Any node in the architecture can communicate with any other, requesting or providing service as needed.

![All P2P nodes are clients and servers](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.1.6.png)

Figure 1.4.2: All P2P nodes are clients and servers

For many readers, P2P architectures are probably associated with file-sharing services, such as BitTorrent. In that service, you may use your client to download a file provided by another user; in return, while you are connected to the network, other users can also download that same file (or pieces of it) from your client. P2P architectures go beyond just this single application, though. Another common example of a P2P architecture is DNS, the service that translates human-readable domain names (`www.example.com`) into a numeric IP address. (We will cover many of the details of DNS in Chapter 4.) Although your web browser interacts with DNS in a traditional client/server approach, DNS itself consists of a world-wide P2P network of systems that exchange information to support these translations.

Overall, P2P architectures extend the client/server paradigm with the benefit that they [scale](#term-scale) well by maintaining good (or even better) service as the number of users increases. For instance, in a system like DNS with P2P nodes distributed world-wide, your request can be routed to the server that is physically closest to you. Consequently, your request would be handled more efficiently than if the message had to be transmitted to the other side of the world.

The tradeoff for these scaling benefits is more difficulty in both administration and security. For instance, if one of the nodes is corrupted so that it serves spam or other malware, system administrators may struggle to identify which particular node is causing the problem. Another problem that arises from this tradeoff is that updates are not guaranteed to be immediately accessible. When a file is updated on one node, that change needs to be communicated to all of the other nodes, which takes time. While this change is propagating, that file may be designated as not available or only the old version can be accessed. The choice between traditional client/server and P2P architectures must account for all of these factors.

///1.4.3. Layered Architectures[¶]
----------------------------------

![Common layers for an information system application](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.1.7.png)

Figure 1.4.3: Common layers for an information system application

In a [layered architecture](#term-layered-architecture), the system structure is arranged in a hierarchical order of modular components. [Figure 1.4.3](#1_4_archlayer) shows a common layered architecture that is used in building information systems. In this design, the user interacts with the Presentation Layer, which is responsible for the user interface and other graphical components. This layer can interact only with a Business Layer that encodes domain-specific business logic, such as adding new employees to a human resources database. The Business Layer may interact with a Services Layer that defines common processes that are used for a variety of application. User authentication and security mechanisms would commonly be implemented as such a service. At the bottom level, the Persistence Layer contains the database and other mechanisms for storing data.

Client/server and P2P architectures support bidirectional communication between any entity that adheres to the specified protocol. In contrast, a layered architecture imposes a strict ordering to the allowable communication. Each layer is designed to interact only with the layer immediately above and the layer immediately below. For instance, in [Figure 1.4.3](#1_4_archlayer), the Presentation Layer should never interact directly with the Services or Persistence Layers. Instead, all data that the Presentation Layer receives for long-term storage would need to be passed to the Business Layer, which would forward the requests on to the lower layers.

One significant advantage of the layered architecture approach is that the layers can be independently modified and replaced with new components. Switching the Persistence Layer to use a different kind of database would not affect any of the other layers, assuming the interfaces are preserved and the layers are cleanly defined. Furthermore, new layers could be added as the application evolves, allowing for an extensible and modular approach.

On the other hand, the layers of many systems are hard to delineate cleanly in a single hierarchical ordering. Some systems have circular dependencies that cannot be easily resolved, while others have layers that are not truly independent. For instance, consider the relationship between a file system and the physical storage device driver in an OS. In theory, file systems could break files into pieces of any size; in practice, all file systems structure data in chunks that are multiples of 512 B, because that is the standard size imposed by the cables used with hard drives. Consequently, it does not seem efficient to ignore this reality when defining the file system layer. Similarly, there may be good reason to allow messages to bypass layers. For instance, the Business Layer may need to store a significant amount of temporary data until an approval (implemented in the Services Layer) is granted; thus, the system could benefit from granting the Business Layer direct access to the Persistence Layer.

A related problem with layered architectures is that there tends to be unnecessary redundancy and inefficient mechanisms. As an example of the former, consider the layers of the Internet protocol stack, which we will discuss in Chapter 5. The layers of this stack use the terms [segment](#term-segment), [packet](#term-packet), [datagram](#term-datagram), and [frame](#term-frame) to refer to a structured message that contains a header and a piece of data. Granted, there are subtle nuances between some of them, but the concept is very similar. Additionally, all of these layers include error checking mechanisms to ensure that none of the bits of the message have been corrupted; when a message arrives, multiple layers all perform the same calculation to check the message’s validity.

///1.4.4. Pipe-and-filter Architectures[¶]
------------------------------------------

In contrast to the previous architectures, [pipe-and-filter architectures](#term-pipe-and-filter-architecture) impose a unidirectional ordering on communication between system components. The data processing begins at the _source_ of the input and proceeds through a series of stages. The output of each stage becomes the input of the next, with the intention that each stage transforms the data in some way. The final result is referred to as the _sink_, which denotes the output. Readers who have some familiarity with working on the UNIX or Linux command line are likely to have encountered a pipe-and-filter architecture, created by the following sample command line:

$ sort foo.txt | grep -i error | head -n 10 > out.txt

In this sequence of commands, the contents of the file foo.txt are sorted and sent to the grep utility, which performs text matching (finding “error” in a case insensitive manner). The lines that contain “error” are then sent to the head command that grabs only the first 10 lines of output, writing the results in a new file called `out.txt`. [Figure 1.4.4](#1_4_archpipefilter) illustrates this sequence as a pipe-and-filter architecture.

![The pipe-and-filter system constructed from chaining command-line utilities](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.1.8.png)

Figure 1.4.4: The pipe-and-filter system constructed from chaining command-line utilities

In this example, the system was constructed dynamically by linking these command-line utilities together. That is one way for creating a pipe-and-filter architecture. If there are several independent modules that share a common interface (a stream of bytes as input and output), then a pipe-and-filter system can be constructed from any composition of these components. Other pipe-and-filter systems are not dynamic, and the sequence of stages cannot be re-ordered.

One example of such a pipe-and-filter system is a programming language compiler. The source code is passed through a lexical analysis stage to recognize keywords (e.g., “if” and “while”) and other language components. These tokens are passed to syntax analysis to check for errors, such as forgotten semicolons or other invalid code. Next, the semantic analysis stage checks for type mismatches (e.g., initializing an `int` variable with `"foo"`). These stages, which make up the _front end_ of the compiler, lead to the generation of intermediate representation. The processing continues with additional stages to optimize the code and generate the machine-language executable.

One significant disadvantage of a pipe-and-filter architecture is the unidirectional structure does not allow for error recovery. For instance, if you chain together 10 command-line utilities and the third one wipes out all of the data being processed, the later stages cannot recover the eliminated data. Furthermore, it may not be immediately evident which stage was the cause of the lost data. Despite this drawback, however, pipe-and-filter architectures are very good for serial data processing.

///1.4.5. Event-driven Architectures[¶]
---------------------------------------

The last major style that we will consider here is the [event-driven architecture](#term-event-driven-architecture). An [event](#term-event) refers to a meaningful change in the state of the system. For instance, in a GUI, keyboard presses and mouse clicks are instances of events. When either of these occur, the user is indicating a desire for the system to respond to this request in some way. [Figure 1.4.5](#1_4_archevent) illustrates the logical structure of an event-driven system. An _event generator_ is any input device that can trigger the creation of an event. A keyboard and mouse are event generators in a GUI; the server that a mobile app uses to connect to other users would also be an event generator. The events are sent through an _event channel_, such as an HTTP connection or a wire, where they are received by an _event processing_ component. This component detects the type of event that was triggered and invokes the corresponding _event handler_ to respond accordingly.

![The logical structure of an event-driven architecture](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.1.9.png)

Figure 1.4.5: The logical structure of an event-driven architecture

Event-driven architectures are effective designs for [reactive](#term-reactive) systems. By differentiating the event channel, event processing, and event handlers as distinct components, this approach creates a robust and extensible framework. To support a new type of event, the developer creates the event handler and makes the event processing layer aware of it. Similarly, the channel can be modified and upgraded without affecting the event handler operations.

One drawback of the event-driven architecture is how it complicates the timing of access to shared resources. Some event handlers are designed to have higher priority than others, and some event handlers may take a while to complete. Consequently, if a low-priority event handler is currently using a shared resource when a high-priority event arrives, the correct response is not immediately clear. The low-priority handler may need to finish its work to keep the system in a stable, consistent state because it has made some changes that cannot be undone. However, the high-priority handler is, by definition, supposed to run first. As such, correct handling of events can be complicated.

///1.4.6. Hybrid Architectures[¶]
---------------------------------

In practice, computer systems rarely fit into exactly one architectural style. Many systems are best described as hybrid architectures that can be viewed from different perspectives depending on the context. This hybrid structure is particularly common when systems are composed of subsystems that operate independently. Some subsystems are more naturally aligned with one architecture than another, implying that there is no single architecture for the whole system.

One example of this is an OS kernel, which is the central system that provides applications with managed access to the hardware. From one perspective, the kernel is an event-driven architecture, as it consists of a collection of _interrupt handlers_ that respond to signals (interrupts) from the hardware that something meaningful has occurred. Those events can include plugging in a USB drive, receiving data from a network card, detecting keyboard presses, being alerted to low battery power, and so on. From a different perspective, the kernel is also a client/server architecture. Applications that are running use the [system call interface](#term-system-call) to request access to a shared resource. These requests include sending and receiving pieces of data across the network, opening and closing files, sending audio data to the speakers, and so on. Furthermore, some aspects of the kernel are designed as layered architectures. File systems, for example, are often structured in layers that convert generic file operations (reading, writing) into operations that are specific to the particular organization of a specific device; that is, the device may use file systems such as FAT32, HFS+, ext4, or NTFS, and the kernel maps the generic operations as needed. Thus, there are systems, such as OS kernels, that can exhibit the features of more than one architecture based on which aspect of the system is being considered.



//1.5. State Models in UML[¶]
=============================

[States](#term-state) are unique and meaningful configurations of the system. In some cases, a state may be defined by a particular combination of values assigned to certain variables. In others, several sets of possible values are grouped together within a single state. [Transitions](#term-transition) denote changes from one state to another. Transitions are triggered by events and can have [effects](#term-effect) that produce some visible behavior.

![A UML state model for a streaming media player](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.1.10.png)

Figure 1.5.1: A UML state model for a streaming media player

UML [state models](#term-state-model) are one way to visualize the behavior of a system from the perspective of its states and transitions. [Figure 1.5.1](#1_5_statemodelsuml) shows an example of a state model for a streaming media player. The player has four states: `Connecting`, `Buffering`, `Playing`, and `Closing`. The arrows between states denote transitions, with the following events defined: `Connected`, `Ready`, `Suspended`, `Cancelled`, and `Finished`. These events correspond to both user input, as well as issues such as running out of buffered data (the infamous _buffering_ that frustrates users). Effects of transitions are denoted after the “/” in the label, so the `Connected` event causes the effect `Start` Loading to retrieve the first bytes to buffer.

The solid circle at the top indicates an _initial state_ where the system starts, where the outlined circle in the bottom right denotes a _final state_. Some models may have more than one initial state or more than one final state. Observe that, in this case, there is no event labeled for the transitions from the initial state or into the final state. These are considered to be _empty transitions_ and are not associated with any particular event.

State models are effective tools for illustrating the general flow between states and the overall structure of the system. As is true of any model, state models convey some information while omitting other details. Specifically, state models omit information relating to the sequence and the timing of the states or the events. [[1]](#f4) For instance, in the case of the `Closing` state, the system could remain in that state for 1 ms or 10 hours before proceeding to the final state; both interpretations would be valid. Additionally, one execution of the system could completely avoid the `Playing` state, while another execution switches between `Playing` and `Buffering` 100 times. If more precise information is needed about the timing or the sequence of transitions, then other models will need to be used.

///1.5.1. State Space Explosion[¶]
----------------------------------

Interpreting a well-defined state model should be intuitive; the states should be meaningfully different, and the transitions between them should be logical. However, constructing state models is not a straightforward process and requires a significant amount of practice to do well. One of the most intuitive challenges is how to deal with the [state space explosion](#term-state-space-explosion) problem, which arises when the number of states in the model increase so dramatically that the model is no longer useful.

📜 Example 1.5.1

* * *

One technique that can be used to evaluate a program for correctness is known as _model checking_, which analyzes a formal specification of a state model. For instance, based on that model, can we determine if the program will ever reach a state where a pointer variable is set to `NULL` and the pointer is dereferenced? If so, we can examine the sequence of steps to get to that state to debug the program.

To start this process, we need a model of the software program. Consider a naïve approach that creates a unique state for every possible value of every possible variable. If the system contained only a single 8-bit `char` (denoted `ch1`) the state model would consist of \(2^8 = 256\) states. Add a second `char` (denoted `ch2`) and the model increases to \(2^{16} = 16,536\) states. This fact arises from the observation that we must (without further knowledge of the program) consider all 256 possible values of `ch2` when `ch1` is 0, then another 256 values when `ch1` is 1, and so on. If we convert both variables to the 32-bit `int` type and the model has \(2^{64} = 1.84 * 10^{19}\) states, just to represent two variables.

This approach illustrates the dangers of the state space explosion problem. Because every additional bit of information doubles the number of possible states, state models can easily become unusably large. As such, it is critical to group configurations into meaningful states (such as positive and negative values for a key variable. However, determining what is meaningful is inherently application-specific and requires the judgment of someone with relevant expertise.

///1.5.2. Implementing Finite State Machines[¶]
-----------------------------------------------

There are multiple ways to turn a state model into an executable _finite state machine (FSM)_ [[2]](#f5) implementation. In fact, this is such a common practice that there are automated tools that will take a state model specification and generate executable code. The disadvantage with these types of tools is that the code is not necessarily readable. In this section, we will step through an example implementation that illustrates some of the key aspects of how these translations work.

[Code Listing 1.1](#cl1-1) shows the key declarations of a header file for a generic FSM. Event and state types are declared as integer types, while the action type is declared as a function pointer that takes a pointer to a FSM as an instance. The action type can be used for transition effects (as used in this example), as well as for state entry or exit activities. Lines 15 – 21 declare our FSM structure. Within the FSM, we will keep track of the current state, the number of events (used for error checking), and a pointer to a `transition` function; this function will take in the current state and event as arguments, then return the next state and the associated transition effect (if any) to the caller. (Note that many implementations of FSM do not use this kind of `struct`, using global variables for the current state and transition lookup tables. Our struct is meant to encapsulate this in a way that we support multiple concurrent FSM instances if needed.)

```cpp
/* Code Listing 1.1:
   Header file for a generic FSM handler
 */

/* States and events should just be integers */
typedef int state_t;
typedef int event_t;

/* Dummy struct for circular typedefs */
struct fsm;

/* Function pointer type declaration for effects */
typedef void (*action_t) (struct fsm *);

/* Generic FSM struct that could have other fields, as well */
typedef struct fsm {
  state_t state;  /* current state */
  size_t nevents; /* number of events for this FSM */
  /* pointer to the FSM's transition function */
  state_t (*transition) (struct fsm *, event_t, action_t *);
} fsm_t;

/* Invoke an event handler for a given FSM */
void handle_event (fsm_t *fsm, event_t event);
```

Lastly, there is a single `handle_event()` function declared that will serve as the interface between the FSM and the entity controlling it. [Code Listing 1.2](#cl1-2) shows the structure of this function. The function starts by confirming that the event number is valid. If so, `handle_event()` will call the FSM’s `transition` function to look up information about the current event. This function will return -1 if there is no transition defined for that particular state and event combination. I.e., events that invalid in the current state are ignored. Otherwise, the effect (if any) will be executed on line 21 and the FSM’s state will be updated (line 22).

```cpp
/* Code Listing 1.2:
   Body of handle_event() for a generic FSM handler
 */

void
handle_event (fsm_t *fsm, event_t event)
{
  /* Confirm the event is valid for the given FSM */
  if (event >= fsm->nevents)
    return;

  /* Use the FSM's lookup tables; if next is -1, the event is not
     defined for the current state */
  action_t effect = NULL;
  state_t next = fsm->transition (fsm, event, &effect);
  if (next == -1)
    return;

  /* Perform the effect (if defined) and change the state */
  if (effect != NULL)
    effect (fsm);
  fsm->state = next;
}
```

[Code Listing 1.1](#cl1-1) and [1.2](#cl1-2) provided a generic structure for any FSM. We now want to turn our attention to the state model that we used in [Figure 1.5.1](#1_5_statemodelsuml) to model a simple media player. The first step is to create a table form of the transitions between states. [Table 1.1](#tbl1-1) shows the table of the transitions for [Figure 1.5.1](#1_5_statemodelsuml). Each row of the table corresponds to a possible current state in the model. Each column within that row represents the next state based on a particular event, as well as any related transition effect. For instance, if the Suspend event occurs while the FSM is in the `Playing` state, the next state would be `Buffering` and the `pause_play()` effect would be; if the `Finish` event had occurred instead, the next state would be `Closing` and there would be no effect. Any of the boxes in this table that do not have entries indicate that there is no valid transition for that state and event combination.

|            |       Connect        |       Suspend        |     Ready      |  Finish |  Cancel |
|------------|----------------------|----------------------|----------------|---------|---------|
| Connecting | Buffering/start_load |                      |                |         |         |
| Buffering  |                      |                      | Playing/resume |         | Closing |
| Playing    |                      | Buffering/pause_play |                | Closing |         |
| Closing    |                      |                      |                |         |         |

Table 1.1: Representing the transitions between states in Figure 1.5.1

[Code Listing 1.3](#cl1-3) documents how to turn [Table 1.1](#tbl1-1) into executable code. To start, line 6 defines an enum type for the specific states for this FSM instance, and line 7 creates a preprocessor constant for the number of states. Recall from [Code Listing 1.1](#cl1-1) that states are, ultimately, integer types. The advantage of using an `enum` is that it allows us to refer to states by names (e.g., `CONN` for the `Connecting` state) instead of integers, which would be otherwise meaningless and likely to cause errors. Line 8 defines a constant for the number of events based on similar enum for events but defined elsewhere.

Lines 11 – 17 encode the transitions defined in [Table 1.1](#tbl1-1). In the enum defined on line 6, the last value (`NST`) is used to indicate _no state_, meaning that there is no transition defined. These values in this two-dimensional array correspond to the blanks in the table. Lines 20 – 26 define a similar lookup table, but for the transition effects. Since effects are actions that do something, [Code Listing 1.1](#cl1-1) defined the `action_t` type as a function pointer; as a result, the empty values in the table are `NULL` pointers.

```cpp
/* Code Listing 1.3:
   Internal state and lookup table definitions for Figure 1.5.1
 */

/* Internal type definition of states */
typedef enum { CONN, BUFF, PLAY, CLOS, NST } ms_t;
#define NUM_STATES (NST+1)
#define NUM_EVENTS (NIL+1)

/* Lookup table for transitions; row=state, column=event */ 
static ms_t const _transition[NUM_STATES][NUM_EVENTS] = {
  // Connect Suspend Ready   Finish Cancel
  {  BUFF,   NST,    NST,    NST,   NST  }, // Connecting
  {  NST,    NST,    PLAY,   NST,   CLOS }, // Buffering
  {  NST,    BUFF,   NST,    CLOS,  NST  }, // Playing
  {  NST,    NST,    NST,    NST,   NST  }  // Closing
};

/* Lookup table for effects; row=state, column=event */
static action_t const _effect[NUM_STATES][NUM_EVENTS] = {
  // Connect     Suspend     Ready   Finish Cancel
  {  start_load, NULL,       NULL,   NULL,  NULL }, // Connecting
  {  NULL,       NULL,       resume, NULL,  NULL }, // Buffering
  {  NULL,       pause_play, NULL,   NULL,  NULL }, // Playing
  {  NULL,       NULL,       NULL,   NULL,  NULL }  // Closing
};
```

Note that [Code Listing 1.3](#cl1-3) declared both `_transition` (the transition table) and `_effect` (the effects table) as `static`. This design choice helps to create a modular approach, as these tables will not be accessed directly outside of this file. [Code Listing 1.4](#cl1-4) shows how these tables will be accessed through the `media_transition()` function, also declared `static` in the same file. Given the FSM’s current state, if the entry in the `_transition` table for the specified event is not defined, return -1 to indicate no transition should be taken. Otherwise, set the `effect` call-by-reference parameter to the appropriate function and return the next state. Note that, just like the `_transition` and `_effect` tables, the `media_transition()` function cannot be accessed outside the current file. The `media_init()` function provides the link. When a new instance of this FSM is needed, the controller can use this function to get a new `fsm_t`, which contains a pointer to the `media_transition()` function.

```cpp
/* Code Listing 1.4:
   Connecting the specific FSM with the generic interface
 */

/* Given FSM instance and event, perform the table lookups */
static state_t
media_transition (fsm_t *fsm, event_t event, action_t *effect)
{
  /* If the state/event combination is bad, return -1 */
  if (fsm->state >= NST || event >= NIL || _transition[fsm->state][event] == NST)
    return -1;

  /* Look up the effect and transitions in the tables */
  *effect = _effect[fsm->state][event];
  return _transition[fsm->state][event];
}

/* Return an FSM that links to these internals */
fsm_t *
media_init (void)
{
  fsm_t *fsm = calloc (1, sizeof (fsm_t));
  fsm->nevents = NUM_EVENTS;
  fsm->state = CONN;
  fsm->transition = media_transition;
  return fsm;
}
```

Finally, [Code Listing 1.5](#cl1-5) shows how a controlling program would create a FSM instance and send it events. Since all of the information about the transitions and effects is encapsulated inside the FSM struct, the controller just needs to focus on the logic of which event to send to the FSM. Recall from [Code Listing 1.2](#cl1-2) that `handle_event()` passes the event through the FSM’s `fsm->transition` function pointer to determine the state change and effect function. Since `handle_event()` ignores invalid transitions, sending the wrong event (such as on line 7) cannot cause an error in the FSM.

```cpp
/* Code Listing 1.5:
   Creating an instance of the FSM and sending it events
 */

fsm_t *fsm = media_init ();
handle_event (fsm, Connect);
handle_event (fsm, Connect); /* no transition here ! */
handle_event (fsm, Ready); 
```

[[1]](#id1)

There are more advanced state models, known as timed automata, that do explicitly incorporate time within their representation. However, we will not be using those models in this book.

[[2]](#id2)

The term state model is often used synonymously with finite-state machine or finite-state automata. Here, we are using the terms separately just to distinguish between the non-executable model and its executable implementation.



//1.6. Sequence Models in UML[¶]
================================

![A UML sequence model for a streaming media player execution](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.1.11.png)

Figure 1.6.1: A UML sequence model for a streaming media player execution

As noted previously, state models omit information about timing or the sequence of events that a system encounters while it is running. UML [sequence models](#term-sequence-model) provide a different view of the system, by looking at the sequence of communication messages that are exchanged during an execution of the system. [Figure 1.6.1](#1_5_sequenceuml) shows an example of the messages that might be exchanged between the streaming media player (whose states are those in [Figure 1.5.1](#1_5_statemodelsuml)) and the server that is providing the data to play. The player starts by connecting to the server; the server’s response could denote the event that triggers the transition to the `Connected` state. The player then starts requesting data in chunks of 16 KB, buffering the data until it is ready to be played.

UML sequence models use _lifelines_ to denote one entity’s perspective over time; the start time would be at the top and later time is farther down. UML sequence models use a solid arrow head (such as the `Connect` message) to denote [synchronous](#term-synchronous) messages, with the open arrow head (such as the request for bytes 16384-32767) to denote [asynchronous](#term-asynchronous) messages. The distinction indicates whether or not the sender must wait while the message is sent or if it can do something else during that time. For instance, it would make sense for the `Connect` and the first request to be synchronous messages, because that connection and initial data are necessary to get started. Then, once the system has started playing the buffered data, additional requests for more data could be sent as asynchronous messages. Throughout this book, we will mostly be modeling communication as asynchronous messages, which is common for concurrent systems.

[Figure 1.6.1](#1_5_sequenceuml) illustrates three additional key features of UML sequence models. First, the dotted message lines indicate that a message is a _reply_. This notation is especially useful for synchronous messages to illustrate the linkage between the request and the response. The `Play Movie` message is a _self-message_, which indicates a meaningful event on that lifeline. That is, from that message, the movie is being played, as long as there is data available. While the movie is playing, additional asynchronous messages are sent and more data is received for buffering. Lastly, the message that ends at a circle between the lifelines denotes a _lost message_. This notation indicates that the media player sent the message, but the server did not receive it. This type of lost message is common in networked systems, as we will explore in Chapters 4 and 5.

Throughout this book, we will use both UML state models and sequence models to illustrate key ideas in relation to concurrent systems. Understanding how to read and interpret these models is an important skill in this area. Unlike state models, however, there is no particular translation for sequence models. Specifically, the type of message that would need to be sent depends on the underlying form of communication, such as [pipes](#term-pipe) or [sockets](#term-socket), which we will discuss in later chapters.

///1.6.1. Summary Questions[¶]
------------------------------

Empty here

//1.7. Extended Example: Text Parser State Machine[¶]
=====================================================

![A UML state model for a simple parser](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.1.12.png)

Figure 1.7.1: A UML state model for a simple parser

In this Extended Example, we will illustrate how to use a state machine as a parser. This is a very simplified version of how compilers read in and parse source code. [Figure 1.7.1](#1_7_stateparser) shows the state model that we will be using to implement the parser. This parser recognizes only the strings `"int"` and `"integer"`, rejecting everything else. Both strings are expected to end with a space, leading to the Accept final state.

To reduce the size of the model, the character-recognition events (`CHAR_*`) are fired when the corresponding character is read from the input in the correct order. For instance, parsing the string `"int"` would fire three events in sequence: `CHAR_i`, `CHAR_nt`, then `CHAR_nt`. On the other hand, parsing string `"itn"` would fire the events in sequence: `CHAR_i` then `CHAR_other`, because the `'t'` would not be in the correct order. A more complete model would use distinct events for each character, as well as separate states to represent the full ordering; that is, `'i'` would lead to a one state, then `'n'` would lead to a second, and `'t'` would to a third. However, such a model is more difficult to include in the space constraints of the page here.

First, we complete the generic FSM handler, which we described in [Code Listing 1.1](#cl1-1) and [1.2](#cl1-2). The key difference here is that there are some additional fields added to the `fsm_t` declaration. As each character of the input is processed, it is added to a buffer that keeps track of the string in progress. Also, to assist the controller in determining which events are valid due to the ordering of the characters, there are `next_valid` and `next_event` fields. For instance, when the FSM first enters the `Int_Or_Integer` state, only the character `'i'` has been processed. Consequently the `next_valid` character would be `'n'` and the `next_event` would be `CHAR_nt`. However, once the `'n'` is read, the `next_valid` would change to `'t'` accordingly. The `final` field is used by the controlling driver program to determine if the string was read completely.

```cpp
#ifndef __statemodel_h__
#define __statemodel_h__

#include <stdbool.h>
#include <sys/types.h>

/* States and events should just be integers */
typedef int state_t;
typedef int event_t;

/* Dummy struct for circular typedefs */
struct fsm;

/* Function pointer type declaration for effects */
typedef void (*action_t) (struct fsm *);

#define BUFFER_LENGTH 10

/* Generic FSM struct that could have other fields, as well */
typedef struct fsm {
  state_t state;  /* current state */
  size_t nevents; /* number of events for this FSM */

  /* pointer to the FSM's transition function */
  state_t (*transition) (struct fsm *, event_t, action_t *, action_t *);

  /* helper fields to keep track of the current input and the
     string built so far */
  char input;
  char buffer[BUFFER_LENGTH];
  size_t length;

  bool final; /* set on accept or reject */

  /* special handling: since there are self-transitions that are
     valid a limited number of times, use these fields to keep
     track of the next event to fire based on expected input */
  char next_valid;
  event_t next_event;
} fsm_t;

/* Invoke an event handler for a given FSM */
void handle_event (fsm_t *fsm, event_t event);

#endif
```

Next, we extend [Code Listing 1.2](#cl1-2) so that it can also perform entry activities on state changes.

```cpp
#include <stdio.h>
#include <stdlib.h>

#include "statemodel.h"

void
handle_event (fsm_t *fsm, event_t event)
{
  /* Confirm the event is valid for the given FSM */
  if (event >= fsm->nevents)
    return;

  /* Use the FSM's lookup tables; if next is -1, the event is not
     Defined for the current state */
  action_t effect = NULL;
  action_t entry = NULL;
  state_t next = fsm->transition (fsm, event, &effect, &entry);
  if (next == -1)
    return;

  /* Perform the effect (if defined) */
  if (effect != NULL)
    effect (fsm);

  /* Perform the new state's entry action (if defined) */
  if (entry != NULL)
    entry (fsm);

  /* Change the state */
  fsm->state = next;
}
```

The `"parse.h"` header file defines the interface to this specific FSM. To control the parser FSM, a driver program would need access to the event names (defined by the `parse_event_t` type) and the name of the initialization function.

```cpp
#ifndef __parse_h__
#define __parse_h__

#include "statemodel.h"

/* Valid events for this type of FSM */
typedef enum {
  CHAR_i, CHAR_nt, CHAR_e, CHAR_ger, CHAR_other, SPACE, NIL
} parse_event_t;

fsm_t *parse_init (void);

#endif
```

The next piece of code extends [Code Listing 1.3](#cl1-3) with the information needed for the parser structure. As before, there’s an initialization function (`parse_init()`) that sets up the internals of the `fsm_t`. The `append_character()` function keeps track of how many times each particular state has been visited and what character is expected next. Again, a more verbose FSM would not need this information. Finally, the `accept()` and `reject()` functions implement the transition effects.

```cpp
#include <assert.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#include "statemodel.h"
#include "parser.h"

/* Internal type definition of states */
typedef enum { ENTER, IOI, IO, ACCEPT, FAIL, NST } parse_t;
#define NUM_STATES (NST+1)
#define NUM_EVENTS (NIL+1)

/* Prototypes for actions/effects and transition function */
static void append_character (fsm_t *);
static void accept (fsm_t *);
static void reject (fsm_t *);
static state_t parse_transition (fsm_t *, event_t, action_t *, action_t *);

/* Return an FSM that links to these internals */
fsm_t *
parse_init (void)
{
  fsm_t *fsm = calloc (1, sizeof (fsm_t));
  fsm->nevents = NUM_EVENTS;
  fsm->state = ENTER;
  fsm->transition = parse_transition;

  /* Set up internal fields for keeping track of characters */
  fsm->input = 0;
  memset (fsm->buffer, 0, sizeof (fsm->buffer));
  fsm->length = 0;

  fsm->next_valid = 'i';
  fsm->next_event = CHAR_i;
  fsm->final = false;

  return fsm;
}

/* Lookup table for transitions; row=state, column=event */
static parse_t const _transition[NUM_STATES][NUM_EVENTS] = {
  // C_i C_nt C_e  C_ger C_oth SPACE
  { IOI, NST, NST, NST,  FAIL, NST    }, // Enter
  { NST, IOI, IO,  NST,  FAIL, ACCEPT }, // Int_Or_Integer
  { NST, NST, NST, IO,   FAIL, ACCEPT }, // Integer_Only
  { NST, NST, NST, NST,  NST,  NST    }, // Accept
  { NST, NST, NST, NST,  NST,  NST    }, // Fail
};

/* Lookup table for effects; row=state, column=event */
static action_t const _effect[NUM_STATES][NUM_EVENTS] = {
  // C_i  C_nt  C_e   C_ger C_oth   SPACE
  { NULL, NULL, NULL, NULL, reject, NULL   }, // Enter
  { NULL, NULL, NULL, NULL, reject, accept }, // Int_Or_Integer
  { NULL, NULL, NULL, NULL, reject, accept }, // Integer_Only
  { NULL, NULL, NULL, NULL, NULL,   NULL   }, // Accept
  { NULL, NULL, NULL, NULL, NULL,   NULL   }, // Fail
};

/* Lookup table for state entry actions */
static action_t const _entry[NUM_STATES] = {
  NULL, append_character, append_character, NULL, NULL
};

/* Given FSM instance and event, perform the table lookups */
static state_t
parse_transition (fsm_t *fsm, event_t event, action_t *effect, action_t *entry)
{
  /* If the state/event combination is bad, return -1 */
  if (fsm->state >= NST || event >= NIL || _transition[fsm->state][event] == NST)
    return -1;

  /* Look up the effect and transitions in the tables */
  *effect = _effect[fsm->state][event];

  /* Look up the next state in the list of entry events */
  state_t next = _transition[fsm->state][event];
  if (next != NST)
    *entry = _entry[next];

  return next;
}

/* Appends the current character to the buffer */
static void
append_character (fsm_t *fsm)
{
  assert (fsm->length < BUFFER_LENGTH - 1);
  fsm->buffer[fsm->length++] = fsm->input;

  /* Helper inputs to keep track of what the next event will be */
  switch (fsm->input)
    {
    case 'i':
      fsm->next_valid = 'n';
      fsm->next_event = CHAR_nt;
      break;
    case 'n':
      fsm->next_valid = 't';
      fsm->next_event = CHAR_nt;
      break;
    case 't':
      fsm->next_valid = 'e';
      fsm->next_event = CHAR_e;
      break;
    case 'e':
      if (! strcmp (fsm->buffer, "inte"))
        fsm->next_valid = 'g';
      else
        fsm->next_valid = 'r';
      fsm->next_event = CHAR_ger;
      break;
    case 'g':
      fsm->next_valid = 'e';
      fsm->next_event = CHAR_ger;
      break;
    default:
      fsm->next_valid = '\0';
      fsm->next_event = NIL;
    }
}

/* Prints that the string is accepted */
static void
accept (fsm_t *fsm)
{
  printf ("ACCEPT: \"%s\"\n", fsm->buffer);
  fsm->final = true;
}

/* Prints that the string is rejected and at what character */
static void
reject (fsm_t *fsm)
{
  printf ("REJECT: \"%s\" at character '%c'\n", fsm->buffer, fsm->input);
  fsm->final = true;
}
```

The final piece of code for this parser is the main driver program. This program reads in the command-line arguments and attempts to parse each one as beginning with either `"int"` or `"integer"`. For each provided input, this controller will use `parse_init()` to create a new FSM instance. The code in lines 37 – 48 start by checking for a space (indicating the first token of the input has been read) or for the expected next character (based on the information stored in the FSM). Based on these checks, line 52 fires the event and the pointer moves on to the next character. If all characters have been read, there are three possibilities: 1) a space was found and the match was a success 2) the match already failed, or 3) the input has matched a prefix of `"int"` or `"integer"`, but there was no space to terminate the string. To resolve case 3, we just need to check the strings. [[1]](#f6)

```cpp
#include <stdio.h>
#include <string.h>

#include "statemodel.h"
#include "parser.h"

int
main (int argc, char *argv[])
{
  /* Inputs to parse. Matches will be exactly "int", "integer",
     or begin with "int " or "integer ". */
  if (argc < 2)
    {
      fprintf (stderr, "Must pass at least one string argument\n");
      fprintf (stderr, "Sample command line:\n");
      fprintf (stderr, "  ./ext int char \"i n t\" integ\n");
      return 1;
    }

  for (int i = 1; i < argc; i++)
    {
      printf ("Processing input: \"%s\"\n", argv[i]);

      /* Create a new parser instance for each input */
      fsm_t *fsm = parse_init ();
      char *walker = argv[i];
      event_t evt = NIL;

      /* Traverse through each character in the input */
      while (*walker != '\0')
        {
          fsm->input = *walker;

          /* If the input is a space, check if the string
             match is successful. Otherwise, check if the
             next character matches what is expected. */
          if (fsm->input == ' ')
            {
              if (! strcmp (fsm->buffer, "int") || ! strcmp (fsm->buffer, "integer"))
                evt = SPACE;
              else
                evt = CHAR_other;
            }
          else if (fsm->input == fsm->next_valid)
            evt = fsm->next_event;
          else
            evt = CHAR_other;

          /* The event should never be NIL because of the
             logic above, but this style is for consistency. */
          if (evt != NIL)
            handle_event (fsm, evt);

          /* If a string is accepted or rejected, we are done. */
          if (fsm->final)
            break;

          /* Move to the next input character. */
          walker++;
        }

      /* At the end of the input. Either we have matched exactly
         "int" or "integer", or we have matched a substring (such
         as "inte"). Substrings should be rejected. Use '$' to
         indicate an END-OF-STRING character. */
      if (!fsm->final)
        {
          fsm->input = '$';
          if (! strcmp (fsm->buffer, "int") || ! strcmp (fsm->buffer, "integer"))
            handle_event (fsm, SPACE);
          else
            handle_event (fsm, CHAR_other);
        }
      printf ("\n");
    }

  return 0;
}
```

[[1]](#id2)

One may object at this point and ask why we didn’t just compare the strings from the beginning and avoid all of the FSM complexity. The answer is that this example is one miniscule component of a much larger system: a compiler. The state models used for compilers are significantly larger and distinguish keywords (such as `"int"`) from programmer-defined variable names (such as `"interest_rate"`). Simple string comparison is not a feasible approach to such complex work.


/Chapter 2   Processes and OS Basics[¶]
=======================================

//2.1. Processes and OS Basics[¶]
=================================

![Timeline of major CSF topics with Processes and IPC highlighted](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Timeline.2.png)

> “Don’t wonder if some mishap may happen, but rather ask what one will do about it when it does occur.”
> 
> Fernando Corbató

The abstractions of processes, operating systems, and files date back to the earliest computer systems. J. Presper Eckert and John Mauchly used the file abstraction to describe stored programs in the UNIVAC (1951); earlier references in the 1940s also used this concept. Processes were key to the earliest multiprogramming operating systems, such as the Atlas Supervisor (1962) and CTSS (1961). This chapter examines the fundamental building blocks of concurrent systems.

🚀 Chapter Objectives

* * *

In this chapter, we will address the following instructional objectives:

*   We will consider how the kernel resides and operates within the context of a process.
*   We will explore how the kernel uses multiprogramming to interleave the execution of multiple processes.
*   We will compare and contrast the system call interface with function calls.
*   We will examine code relating to the life cycle of a process, including how custom signal handlers can respond to events.

When discussing programming languages like C, Java, or assembly language, we typically talk about writing software _programs_. Recall that a program is an implementation of an algorithm within a particular programming language. For instance, the function in [Code Listing 2.1](#cl2-1) is part of a program that documents its algorithm in the comments.

```cpp
/* Code Listing 2.1:
   A C-based implementation of a recursive factorial calculation algorithm
 */

/* factorial (n) : calculates n!
   if n = 1, then factorial (n) = 1
   otherwise, factorial (n) = n * factorial (n - 1) 
 */

int64_t
factorial (int64_t n)
{
  if (n <= 1)
    return 1;
  else
    return n * factorial (n - 1);
}
```

A [process](#term-process) is defined to be an instance of a program in execution. What this means is that a process is an active entity that is running through the instructions that are specified in the program. Whenever you run the software program that you have written and compiled into an executable file, you create a new process. Furthermore, if you run the same program several times on the same machine, you typically have multiple processes that are all executing the same code. (We will find that this is not always the case when we discuss [threads](#term-thread).)

![The major segments stored in a virtual memory instance](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.2.1.png)

Figure 2.1.3: The major segments stored in a virtual memory instance

A process corresponds to a single instance of [virtual memory](#term-virtual-memory) as shown in [Figure 2.1.3](#2_1_processvirtmem). When you run a program, the process is assigned a new virtual memory instance that includes the _code_, _data_, _heap_, and _stack_ segments to contain run-time information. The process executes the instructions as if it were the only piece of software that existed on that hardware. The fundamental goal of working with processes is to establish _isolation_ of programs. If an error occurs in one process (for instance, if your program triggers a segmentation fault), only that process would be affected; the data in other processes would not be harmed, as they would exist in other instances of virtual memory.

The [operating system](#term-operating-system) [[1]](#f7) (OS) or [kernel](#term-kernel) is a special program that exists simultaneously within every process. Just like any normal program, the kernel has code, data, heap, and stack segments in memory. Specifically, there is a portion of each process’s virtual memory that is reserved for use by the kernel. [[2]](#f8) This portion of memory can only be accessed when the CPU is set to be in [kernel mode](#term-kernel-mode). Normal programs run in [user mode](#term-user-mode), which restricts the type of code that can be run and prevents the program from directly accessing shared resources, such as I/O devices.

Although the kernel has many features that are common to normal software programs, it operates in a fundamentally different manner. Specifically, the kernel is designed to be [reactive](#term-reactive) in nature. While normal programs are loaded to perform a certain task from start to finish, the kernel exists in the context of other programs and only executes when it is needed. We say that the kernel is _invoked_ when the normal program requests access to a shared resource or some sort of exceptional condition occurs. When the kernel is invoked, it begins executing within the context of the current process. That is, the kernel behaves **as if it is a part of the program that is running**. However, the instructions being executed are fetched from the kernel’s code segment. Once the kernel has performed whatever action is was needed for, it hands control back to the user-mode program.

Understanding the relationship between user-mode programs and the kernel is critical to grasping how modern computer systems execute code. In this chapter, we will discuss what the kernel is and how it exists within the context of a process. We’ll also consider how user-mode code interacts with the kernel to take advantage of peripheral hardware components, such as the keyboard or hard drive. Finally, we’ll look at how processes are created and how they receive information about key events through the use of signals.

[[1]](#id1)

The term [operating system](#term-operating-system) is often used in common usage to refer to the collection of programs that provide the basic system environment of a computer. For instance, it is common to hear people talk about the desktop environments provided by the Windows or macOS operating systems. In the context of computer science, however, the term is normally reserved to describe the kernel. The reason for this is that most of the standard services that users and system administrators interact with run at a lower privilege level than the kernel, meaning they are restricted just like normal programs. Throughout this text, we adhere to this convention, that the term operating system is a synonym for the kernel.

[[2]](#id2)

In late 2017, security researchers discovered a new form of attack that allowed user-mode code to bypass these protections and access the kernel. To mitigate these attacks, known as Meltdown [Lipp et al., 2018] and Spectre [Kocher et al., 2018], kernel vendors created patches that moved most of the kernel to a separate process. However, the separation is not complete and part of the kernel must remain present in all processes. We omit these details for simplicity and retain the conventional model in which the kernel is co-resident in all processes.



//2.2. Processes and Multiprogramming[¶]
========================================

Early computer systems were used to run a single program at a time. Whenever a user wanted to perform a calculation with a computer, they would submit the [job](#term-job) to an administrator and receive the results later. Administrators quickly realized that they could save time by [batching](#term-batch) and submitting multiple jobs at the same time. Batch processing reduced the number of times the administrator had to load programs manually, adding and removing the [monitor](#term-monitor) code as needed. It also increased the amount of computing time that could be accomplished, as a new job could be started immediately after the previous job finished.

In short, batch processing reduced the amount of time wasted between the execution of multiple jobs. Eliminating wasted time like this was very important, as these early computers were very expensive. To justify the expense, designers and users needed to get as much work out of the computer as possible. While batch processing helped in this regard, early computing pioneers quickly realized that another source of wasted time remained: the CPU sat idle while the rest of the computer performed very slow I/O operations. Eliminating this wasted CPU time became the goal of [multiprogramming](#term-multiprogramming).

///2.2.1. Uniprogramming and Utilization[¶]
-------------------------------------------

Early batch processing systems used a strategy known as [uniprogramming](#term-uniprogramming). In uniprogramming, one program was started and run in full to completion; the next job would start immediately after the first one finished. The problem with this approach is that programs consisted of both CPU instructions and I/O operations. CPU instructions were very fast, as they consisted of electrical signals. I/O operations, however, were very slow. One example of an early I/O device was the _drum memory_, which was a large magnetic device that had to be mechanically turned for every data access. If a program required many I/O operations to be performed, that created a lot of wasted time where the CPU sat idle instead of executing instructions.

We can quantify this wasted time as the CPU utilization. In general terms, [utilization](#term-utilization) can be defined mathematically as the actual usage of a resource divided by the potential usage. Utilization is reported as a unit-less ratio, typically a percentage. For instance, if a system is only used for half of the time that it could be, we would say that it experienced 50% utilization. In regard to CPU time, utilization is calculated as the following ratio:

<!-- $\large \mbox{CPU utilization} = \displaystyle\frac{\mbox{total CPU time}}{\mbox{total real time}}$ -->

                        total CPU time
    CPU utilization = ------------------
                        total real time

📜 Example 2.2.1

* * *

Consider the following timeline illustration for three sequential uniprogramming processes.

![Process A executes at time 1, then the CPU sits idle for times 2 through 4. Process A continues for 5 through 8. Process B takes over and runs at time 9, then the CPU sits idle for times 10 through 15. B runs again at 16. C runs for 17 through 25, except for time 24.](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.2.0.1.png)

The green regions indicate times when the CPU is executing instructions in the program, while the yellow indicates that the times where the CPU is idle while waiting on an I/O operation to complete. The following table summarizes the time each process spends executing on the CPU or waiting for I/O:

| Process | CPU time | I/O time |
|---------|----------|----------|
| A       |        5 |        3 |
| B       |        2 |        6 |
| C       |        8 |        1 |
| Total   |       15 |       10 |

In this scenario, the CPU was used for a total of 15 out of 25 possible seconds, so the system experienced 60% CPU utilization when running these three jobs:

<!-- $\large \mbox{CPU utilization} = \displaystyle\frac{5 + 2 + 8}{5 + 2 + 8 + 3 + 6 + 1} = \frac{15}{25} = 60\%$ -->

                        5 + 2 + 8                   15
    CPU utilization = ------------------------ = -------- = 60%
                        5 + 2 + 8 + 3 + 6 + 1       25

As the preceding example illustrates, a significant amount of system resources can be wasted by waiting on I/O operations to complete. It should be noted that this problem still exists in modern systems. While modern I/O devices (such as solid-state drives) are significantly faster than memory drums, modern CPUs are also significantly faster than those in early machines. **I/O bottlenecks tend to be among the most significant barriers to high-speed performance**. Given that waiting on I/O operations to complete is slow and wasteful, system designers proposed a straightforward solution: switch to another process while an I/O operation is being performed.

///2.2.2. Multiprogramming and Concurrency[¶]
---------------------------------------------

In [multiprogramming](#term-multiprogramming) (also called [multitasking](#term-multitasking)), several processes are all loaded into memory and available to run. Whenever a process initiates an I/O operation, the kernel selects a different process to run on the CPU. This approach allows the kernel to keep the CPU active and performing work as much as possible, thereby reducing the amount of wasted time. By reducing this waste, multiprogramming allows all programs to finish sooner than they would otherwise.

📜 Example 2.2.2

* * *

Consider the following timeline illustration for the same three processes from [Example 2.2.1](#2_2_uniprogex), but in a multiprogramming environment.

![With interleaving, other processes run while one is waiting on I/O. In this case, Process A runs at times 1 and 5 through 8. While A is waiting, Process B runs at time 2 and C runs at times 3 and 4 when both A and B are waiting. After A's I/O finishes, B takes over at time 9. C runs for 10 through 14 and 16. At time 15, no process needs CPU time, so the CPU sits idle.](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.2.0.2.png)

As before, the green regions indicate CPU execution and the yellow indicates I/O operations. However, note that processes B and C can run while A is waiting on its I/O operation. Similarly, A and C execute while B is waiting on I/O operations. As a result, the CPU is only completely idle while C’s I/O operation is performed at time 15, because A and B have already run to completion.

In our revised CPU utilization calculation, the numerator does not change because the total amount of CPU execution time has not changed. Only the denominator changes, to account for the reduced time wasted waiting on A’s and B’s I/O operations.

<!-- $\large \mbox{CPU utilization} = \displaystyle\frac{5 + 2 + 8}{5 + 2 + 8 + 1} = \frac{15}{16} = 93.75\%$ -->

                         5 + 2 + 8          15
    CPU utilization = ----------------- = ------ = 93。75%
                        5 + 2 + 8 + 1       16

There are two forms of multiprogramming that have been implemented. The most common technique is [preemptive multitasking](#term-preemptive-multitasking), in which processes are given a maximum amount of time to run. This amount of time is called a [quantum](#term-quantum), typically measured in milliseconds. In preemptive multitasking, if a process issues an I/O request before its quantum has expired, the kernel will simply switch to another process early. However, if the quantum has expired (i.e., the time limit has been reached), the kernel will preempt the current process and switch to another. In contrast, with [cooperative multitasking](#term-cooperative-multitasking), a process can run for as long as it wants until it voluntarily relinquishes control of the CPU or initiates an I/O request.

Cooperative multitasking has a number of advantages, including its simplicity of design and implementation. Furthermore, if all processes are very interactive (meaning they perform many I/O operations), it can have low overhead costs. However, cooperative multitasking is vulnerable to rogue processes that dominate the CPU time. For instance, if a process goes into an infinite loop that does not perform any I/O operation, it will never surrender control of the CPU and all other processes are blocked from running. As a result, modern systems favor preemptive multitasking.

By providing a mechanism for increasing CPU utilization, multiprogramming creates the foundation for concurrent execution of software. [Concurrency](#term-concurrency), in this context, can be thought of as the appearance of simultaneous program execution. That is, concurrency through multiprogramming is what makes it possible to use a web browser while listening to music with an MP3 player at the same time. Throughout most of the history of computing, these applications were not actually executing code at the same time; rather, the kernel was just switching back and forth between them so quickly that users could not notice.

A simple way to illustrate multiprogramming in modern software is with the `sleep()` function. This function’s only argument is the number of seconds to pause the current process. During this time, the system will switch to other processes that need to run. In other words, calling `sleep()` can be interpreted as a form of cooperative multitasking.

📦 C library functions - `<unistd.h>`

* * *

`unsigned sleep(unsigned seconds);`

Suspend the current process for a number of seconds.

[Code Listing 2.2](#cl2-2) shows an example of using `sleep()` to introduce repeated pauses in a process’s execution. This code will print “Hello!” 10 times, pausing for one second between each print. By running this code along with other programs (such as a media player or web browser), you can clearly observe that your computer is still operating during these 10 seconds. In fact, this can be made even clearer by running the same program in multiple windows; each instance will take turns printing the message and sleeping along with the others.

```cpp
/* Code Listing 2.2:
   Repeatedly pausing the current function to illustrate multiprogramming
 */

for (int i = 0; i < 10; i++)
{
    printf ("Hello!\n");
    sleep (1);
}
```

///2.2.3. Context Switches and Overhead Costs[¶]
------------------------------------------------

[Context switches](#term-context-switch) form the basis of multiprogramming. A context switch is the change from one process’s virtual memory image to another. In a context switch, the kernel portion of virtual memory does not change. However, the user-mode code, data, heap, and stack segments that the CPU uses are changed. It’s important to note that both the old and new processes still reside in [physical memory](#term-physical-memory) – the hardware memory component – simultaneously. The difference is that the kernel has changed which portion of physical memory is actively being used for the virtual memory image.

![Timeline showing the major events in a context switch from process A to process B](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.2.3.png)

Figure 2.2.6: Timeline showing the major events in a context switch from process A to process B

Although context switches are critical for multiprogramming, they also introduce complexity and overhead costs in terms of wasted time. First, when the kernel determines that it needs to perform a context switch, it must decide which process it will now make active; this choice is performed by a [scheduling](#term-scheduling) routine that takes time to run. Second, context switches introduce delays related to the memory hierarchy. In early cache systems, a context switch required all cache data to be invalidated. As such, all cache data for the old process had to be flushed and replaced with the new process’s data. Newer systems allow different parts of the cache to be associated with different processes; however, this reduces the total amount of data that the cache can store, increasing the number of cache misses.

Because context switches introduce overhead costs, kernel designers strive to reduce their impact on performance. One way to do this is to increase the quantum given to each process, thereby letting it run longer before forcing a context switch. In fact, this is the main idea behind cooperative multitasking, which can be viewed as providing an infinite quantum to each process. The risk is that increasing the time quantum too much allows some processes to dominate the CPU time, effectively monopolizing the system resources. Empirical research on this trade-off has led to a common practice of 4 - 8 ms per quantum in modern systems.



//2.3. Kernel Mechanics[¶]
==========================

The [kernel](#term-kernel) is a program that runs with full access privileges to the entire computer. The kernel controls access to all shared system resources, including physical memory, the file system, and I/O devices. The kernel is also responsible for handling all exceptional system and software events, such as power disruption or the addition of new _plug-and-play_ peripheral components. To be precise, the kernel is primarily responsible for two functions. It acts as a [resource manager](#term-resource-manager), providing access to shared system hardware resources as needed. The kernel also acts as a [control program](#term-control-program), handling errors and access violations in a safe manner.

The origins of the kernel date back to the earliest days of mainframe computing, when it was known as the [monitor](#term-monitor). The monitor was a collection of software routines for standard operation that got included with every program when it was run; these included routines for clearing out memory and loading stored data. The term was later changed to [resident monitor](#term-resident-monitor) to reflect the fact that this code was always present (resident) in memory.

In the family of x86 architectures, the CPU’s operating mode is stored as a 2-bit value known as the [current privilege level](#term-current-privilege-level) (CPL), which is also called a [ring](#term-ring). Although these architectures make it possible to have four rings, only two are used in practice. When the system is in ring 3 ([user mode](#term-user-mode)), the set of instructions that are allowed is restricted and no instruction can access any part of memory owned by the kernel.

In ring 0 ([kernel mode](#term-kernel-mode)), all valid memory addresses can be accessed and an additional set of [privileged instructions](#term-privileged-instruction) can be performed. Examples of privileged instructions include `hlt`, which halts the CPU, and `invd`, which can be used to invalidate the CPU cache. In addition, some normal instructions behave differently in ring 0 than they do in ring 3; `popf` (pops a word from the stack into the status flag register) is one example, as some status bits are not updated in ring 3.

🔍 Note

* * *

To further illustrate the distinction between the kernel and the common usage of the term OS, consider the notion of OS versions or distributions. Windows users are probably familiar with names such as XP, Vista, or Windows 10. Similarly, Mac users might distinguish macOS Sierra or OS X El Capitan, just as Linux users may talk about Linux Mint, Ubuntu, or Red Hat Linux.

In all three of these cases, all of the versions and distributions share a common kernel. In the case of Windows, the kernel is known as the NT kernel, which was first released in 1993. macOS is the most recent name of the Mac OS X kernel, which was first released in 2001. All distributions of Linux use the Linux kernel, which was first released in 1991. The various OS distributions are primarily distinguished by the non-kernel programs and services that are included. Although the kernel may contain some updates, the internal structure and services have remained moderately consistent.

///2.3.1. Kernel Memory Structure and Protections[¶]
----------------------------------------------------

![Application code running in user-mode cannot access any part of kernel memory](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.2.2.png)

Figure 2.3.2: Application code running in user-mode cannot access any part of kernel memory

The kernel exists as a protected region of virtual memory within the context of every process. Just like a normal user-mode program, the kernel contains a code segment, global data, and a heap for dynamic memory allocation. Rather than having a single stack, however, the kernel contains many stacks; for each user-mode process, the kernel contains at least one stack.

In general, the kernel interacts with its memory regions the same way as normal programs interact with theirs. The CPU uses the `%rip` register to load the next instruction from the code segment. Dynamically allocated data structures are stored on the heap and local variables are stored on the stack that is currently in use.

Since the kernel contains information about all processes and system resources, user-mode programs must be prevented from tampering with it. For instance, you would not want a faulty program to reformat your hard drive or shut off power at random times; only the kernel should be able to perform these actions. To prevent tampering from other programs, the kernel configures the CPU to restrict access to the portions of physical memory that are storing the kernel’s virtual memory contents. As a result, if an instruction tries to access a memory location within the kernel while the CPU is set to user mode, the CPU itself will detect this invalid access and trigger an exception.

///2.3.2. The Boot Procedure[¶]
-------------------------------

The kernel is loaded and begins executing as a part of the [boot sequence](#term-boot-sequence). When a computer is first turned on, the CPU begins to execute [firmware](#term-firmware) code routines stored in non-volatile hardware storage. Examples of firmware include BIOS and UEFI. These firmware routines locate OS-specific [boot loaders](#term-boot-loader), such as GRUB (used by Linux), BOOTMGR (Windows), or BootX (macOS). If a system is configured to support [dual booting](#term-dual-booting) (the ability to choose from more than one OS), it is the boot loader that provides this capability.

Once the boot loader determines the kernel to load, it locates a file containing the kernel on a storage device. In many systems, the kernel is stored in a compressed format (such as a `tar` archive file compressed with gzip), so the boot loader must decompress it when loading it into memory. Once the kernel has been loaded, the boot loader calls the kernel’s `main()` entry point. [[1]](#f9) Calling the kernel’s `main()` is the last action performed by the boot loader, and the kernel then takes over full control of the system. The kernel begins by initializing its own data structures for managing the system and launching its initial system services as separate processes. These system services include the service that allows users to login to the computer.

///2.3.3. Kernel Invocation[¶]
------------------------------

The kernel can be invoked in two ways. First, the user-mode program may initiate a [system call](#term-system-call), which is a request for a specific service. As an example, calling the `printf()` function will eventually lead to a system call that makes a request for the kernel to write to a particular file, typically `stdout`. System calls invoke the kernel by executing a [trap instruction](#term-trap-instruction). On the x86 family of architectures, this instruction is known as `syscall`.

The second way that the kernel can be invoked is in response to either an [interrupt](#term-interrupt) or an [exception](#term-exception). An interrupt is an asynchronous notification from a hardware component that indicates service is needed. For example, every time you press a key on the keyboard or click a mouse button, that hardware device triggers an interrupt to make the kernel aware of the event. An exception (sometimes called a software interrupt) is a synchronous notification of a problem with the software. Exceptions include faults and aborts, such as segmentation faults, dividing by zero, or illegal memory values that may be the result of hardware failures.

///2.3.4. Mode Switches and Privileged Instructions[¶]
------------------------------------------------------

A [mode switch](#term-mode-switch) refers to a change in the CPL. System calls and interrupts both trigger a mode switch from ring 3 to ring 0. At the same time that the CPL changes, the `%rip` register is updated to begin reading from the kernel’s code segment. The address loaded into the `%rip` is determined by a data structure that the kernel sets up during the boot process. In addition to updating the CPL and the `%rip`, the CPU makes a copy of the user-mode program status (such as its `%rip` value).

One important aspect to note about a mode switch is how quickly it occurs. Specifically, mode switches occur within a single execution of the von Neumann instruction cycle. Once the `%rip` has been updated at the end of one instruction’s cycle, the CPU checks if an interrupt needs processing. If there is a pending interrupt, the CPU triggers a mode switch before fetching the next instruction; if not, then the next instruction is fetched.

After the system call or interrupt has been processed, the kernel forces a mode switch by executing the `iret` instruction. Just as `ret` updated the `%rip` to return to the portion of code that called a function, `iret` acts as a return from an interrupt to get back to the appropriate location in the user-mode program. The `iret` instruction restores the user-mode program’s status that it had stored previously and lowers the CPL back to ring 3.

[[1]](#id1)

More recent x86 processors have also added another bit to the CPL that is used by certain types of virtualization technologies. This additional bit is used to distinguish between “guest mode” and “host mode.” In these types of systems, multiple guest _virtual machines_ may be running as “guests” while a single _hypervisor_ manages them as the “host.” In these types of environments, ring 0 refers to kernel mode within a guest, whereas the hypervisor operates in “ring -1,” which is kernel mode within the host.



//2.4. System Call Interface[¶]
===============================

User-mode programs can execute standard CPU instructions that are focused on performing a calculation or implementing a logical control flow. However, user-mode programs have no direct access to any shared computing resource outside the CPU. For instance, user-mode software cannot read data from a hard drive, send data across a network interface, or even display information to the monitor screen. Instead, the user-mode program must execute a system call to request the kernel perform this action on its behalf.

///2.4.1. System Calls vs. Function Calls[¶]
--------------------------------------------

At the level of assembly language, a system call involves executing a trap instruction. In modern x86 code, the trap instruction is `syscall` [[1]](#f10) , which acts in a manner analogous to call. Instead of jumping to a function within the same program, though, `syscall` triggers a mode switch and jumps to a routine in the kernel portion of memory. The kernel validates the system call parameters and checks the process’s access permissions. For instance, if the system call is a request to write to a file, the kernel will determine whether the user running the program is allowed to perform this action. Once the kernel has finished performing the system call, it uses the `sysret` instruction, which performs a role similar to the standard `ret` instruction. The difference is that sysret also changes the privilege level, returning the system to user mode.

From a higher level perspective, system calls are often written to look like standard C functions. For instance, it is common to find references to the `write()` system call. This practice is simply a form of short-hand notation. In most cases, there is a C function that acts as a wrapper for the system call. That is, there is a C function called `write()` in the C standard library; this function will perform a few initial steps before triggering the `syscall` trap instruction. To be clear, there is a distinction between the `write()` C function and the system call, but this distinction is often blurred in practice.

///2.4.2. Linux System Calls[¶]
-------------------------------

The Linux source code repository contains the full list of Linux system calls. [[2]](#f11) This table identifies the mapping between the system call number (which actually specifies the system call), the name that is commonly used, and the entry point routine within the Linux kernel itself. For instance, system call 0 is the `read()` system call. When a user-mode program executes the `read()` system call, the system will trigger a mode switch and jump to the `sys_read()` function within the Linux kernel.

There are a couple of observations that can be made from this table. First, every system call has a unique number associated with it. As we will explain next, x86 system call mechanics only use this number. The name that associated with each number is just to give meaning to the programmer, just as we use function names instead of relying on memorization of hard-coded addresses. Second, the names of the entry point functions in Linux are the names of the system calls with `sys_` prepended; for instance, the `open()` system call will call the `sys_open()` function in the kernel, and `mmap()` will call `sys_mmap()`.

Lastly, note that the names of the system calls correspond to many common C standard library functions. For instance, `open()` and `close()` are the system calls that are used to establish connections to files, `socket()` is the system call to create a socket for network communication, and `exit()` can be used to terminate the current process. That is, many C functions are simply wrappers for system calls.

In contrast, many C functions are implemented to provide additional functionality on top of system calls. In the case of `printf()`, the code will eventually trigger the `write()` system call. The primary difference is that `write()` requires low-level details of how the system is being used that `printf()` abstracts away. In addition, calling `write()` requires exact knowledge of the length of the message to be printed, whereas `printf()` does not. In summary, many C standard library functions provide a thin wrapper for invoking system calls, while other functions do not.

[Table 2.1](#tbl2-1) lists a small sample of the more than 300 system calls available on 64-bit Linux systems. The full list of system calls can be found in the `syscalls(2)` man or in `<asm/unistd_64.h>`, which is included (through a nested sequence of headers) by `<sys/syscall.h>`. [[3]](#f12) Each system call is documented in a section 2 man page [[4]](#f13) (e.g., `man 2 read`).

|     Syscall    | Number | Purpose |
|----------------|--------|---------|
| `read`         |      0 | Read from a file descriptor
| `write`        |      1 | Write to a file descriptor
| `nanosleep`    |     35 | High-resolution sleep (units in seconds and nanoseconds)
| `exit`         |     60 | Terminate the current process
| `kill`         |     62 | Send a signal to a process
| `uname`        |     63 | Get information (name, release, etc.) about the current kernel
| `gettimeofday` |     96 | Get the system time (in seconds since 12:00 AM Jan. 1, 1970)
| `sysinfo`      |     99 | Get information about memory usage and CPU load average
| `ptrace`       |    101 | Trace another process's execution

Table 2.1: A sample of common Linux system calls

///2.4.3. Calling System Calls in Assembly[¶]
---------------------------------------------

In assembly language, a system call looks almost exactly like a function call. Arguments are passed to the system call using the general purpose registers and the stack as needed. The main difference is that the system call number is stored into the `%rax` register. As an example, we can write a standard “Hello, world” program in assembly language using two system calls.

In [Code Listing 2.3](#cl2-3), the four mov instructions (lines 9 – 12) set up the arguments for the `write()` system call, which expects three arguments: the file handle to write to, the address of the message to write, and the length of the message in bytes. As with a normal function, these are passed in the `%rdi`, `%rsi`, and `%rdx` registers. In a normal function call, the `call` instruction would specify the function to execute. However, `syscall` does not encode this information. Instead, on line 5, we moved the constant 1 into `%rax`, as this is the number for the `write()` system call. Similarly, lines 16 and 17 indicate that the `exit()` system call should be invoked with the value 0 as a parameter.

```sh
# Code Listing 2.3:
# An assembly-language “hello world” program with system calls

  .global _start	

  .text
_start:
  # write(1, message, 13)
  mov $1, %rax                # system call 1 is write
  mov $1, %rdi                # file handle 1 is stdout
  mov $message, %rsi          # address of string to output
  mov $13, %rdx               # number of bytes
  syscall                     # invoke OS to write to stdout

  # exit(0)
  mov $60, %rax               # system call 60 is exit
  xor %rdi, %rdi              # we want return code 0
  syscall                     # invoke OS to exit

  .data
message:
  .ascii "Hello, world\n"
```

Many system calls have return values that can be used to determine if an error occurred. As with standard functions, the kernel puts return values in the `%rax` register. Negative values in the range of -4095 to -1 indicate an error.

///2.4.4. Calling System Calls with syscall()[¶]
------------------------------------------------

Another method for invoking Linux system calls directly is to use `syscall()`. For instance, the program in [Code Listing 2.4](#cl2-4) shows the C equivalent of the assembly language code shown in Code Listing 2.3. As before, we can bypass the C standard library functions for `write()` and `exit()` by invoking the system call directly. Specifcally, lines 12 and 13 make two system calls, although they look like standard function calls. The C compiler will translate these into the sequence of instructions in lines 9 – 13 and 16 – 18 from [Code Listing 2.3](#cl2-3).

```cpp
/* Code Listing 2.4:
   Using syscall() in C to invoke Linux system calls for writing and exiting
 */

#include <unistd.h>

char *message = "Hello, world\n";

int
main (void)
{
  syscall (1, 1, message, 13);
  syscall (60, 0);

  /* should never reach here */
  return 0;
}
```

One aspect to note about the implementation of `syscall()` is that its parameters get passed in the wrong registers. Specifically, the compiler mostly treats `syscall()` as a regular function call, but it passes the first parameter in `%rdi` instead of the standard `%rax`, because the kernel expects the system call number to be in `%rdi`. [Code Listing 2.5](#cl2-5) shows how Linux implements `syscall()`, shifting the register values as needed (lines 9 – 13) and invoking the `syscall` instruction.

```sh
# Code Listing 2.5:
# The Linux implementation of the C syscall() function

# From sysdeps/unix/sysv/linux/x86_64/syscall.S

  .text
ENTRY (syscall)
  movq %rdi, %rax          # Syscall number -> rax.
  movq %rsi, %rdi          # shift arg1 - arg5.
  movq %rdx, %rsi
  movq %rcx, %rdx
  movq %r8, %r10
  movq %r9, %r8
  movq 8(%rsp),%r9         # arg6 is on the stack.
  syscall                  # Do the system call.
  cmpq $-4095, %rax        # Check %rax for error.
  jae SYSCALL_ERROR_LABEL  # Jump to error handler if error.

L(pseudo_end):
  ret                      # Return to caller.

PSEUDO_END (syscall)
```

[[1]](#id1)

The `syscall` instruction is the primary trap instruction in 64-bit x86 systems. Earlier x86 programs performed system calls by triggering an interrupt with the `int $0x80` instruction; the kernel would use `iret` to return from the interrupt. For performance reasons, this approach was replaced with the `sysenter` and `sysexit` instructions on 32-bit systems. `syscall` and `sysret` are the 64-bit equivalent of these faster system call instructions.

[[2]](#id2)

See [https://github.com/torvalds/linux/blob/v3.13/arch/x86/syscalls/syscall_64.tbl](https://github.com/torvalds/linux/blob/v3.13/arch/x86/syscalls/syscall_64.tbl) for example.

[[3]](#id3)

To prevent naming collisions, the names of the system calls are more complicated than shown in the table. Specifically, the `Read()` system call is listed in this table as `__NR_read`.

[[4]](#id4)

For readers new to man pages, documentation on this system can be found by typing `man man` on the command line. In brief, on Linux and UNIX systems, all C libraries are documented through this manual. The manual is divided into several sections, with section 2 used for system calls and section 3 used for the C standard library. The section of the manual for a function is noted in parentheses after the name. For instance, `exit(2)` documents on the exit system call, whereas `exit(3)` documents the standard C library `exit()` function. (Note there is a difference!)



//2.5. Process Life Cycle[¶]
============================

From a very high-level perspective, we can describe the life cycle of a process as a sequence of [events](#term-event). The first event is the creation of the process as a new virtual memory instance. The CPU will then begin executing the program code according to the _von Neumann instruction cycle_ until it reaches the `halt` instruction indicating the program is complete. At that point, the process is destroyed and any resources (such as physical memory) associated with it are reclaimed by the kernel.

///2.5.1. Process Creation[¶]
-----------------------------

There are generally two system calls that are associated with creating a new process: `fork()` and `exec()`. When a program calls `fork()`, a system call is triggered that asks the kernel to create a new virtual memory instance. In the immediate moment when this occurs, the contents of the new virtual memory instance is an exact duplicate of the process that was running. We refer to the two processes as the _parent_ and _child_. While handling the `fork()`, the kernel will allocate new internal data structures for the child, assign it a [process identifier](#term-process-identifier) (PID), and assign a small set of initial resources to it. A PID is an integer value that acts like a name that can be used to uniquely refer to a process.

📦 C library functions – `<unistd.h>`

* * *

`pid_t fork(void);`
    Creates a new process. Returns the child’s PID if successful.

`pid_t getpid(void);`
    Gets the current process’s PID.

`pid_t getppid(void);`
    Gets the PID of the current process’s parent.

[Code Listing 2.6](#cl2-6) demonstrates how to use `fork()`. Once both processes are running, the code can distinguish between the parent and child by looking at the value returned from `fork()`.The kernel returns 0 to the child, and it returns the child’s PID to the parent. If the kernel failed or refused to create a new process for some reason, it would return a negative value to the parent; the global variable `errno` (declared in the standard library file `errno.h`) would be set to an integer value indicating the error that occurred.

```cpp
/* Code Listing 2.6:
   Using fork() to create a child process and access its PID
 */

pid_t child_pid = fork ();

if (child_pid < 0)
  printf ("ERROR: No child process created\n");
else if (child_pid == 0)
  printf ("Hi, I am the child!\n");
else
  printf ("Parent just learned about child %d\n", child_pid);
```

The return values from `fork()` make sense when you consider that the C library defines two functions that allow the process to determine its own PID (`getpid()`) and to find out its parent’s PID (`getppid()`). The child can use these two functions to identify itself and its parent. However, the parent process may have already created dozens of child processes. The return value from `fork()` informs the parent of the PID for this new child process that was just created.

[Code Listing 2.7](#cl2-7) shows an example of what happens when the return code from `fork()` is not checked. On line 6, the original process creates a child. For clarity, let the original process of PID 1000 and this child has PID 1001. Since both processes continue to execute with the same code, both reach line 9 and create two additional processes (PIDs 1002 and 1003). As a result, there would be four processes that execute line 12, printing the message four times (with different PIDs each time).

```cpp
/* Code Listing 2.7:
   Running the same code in multiple processes
 */

/* Start by creating a second process */
pid_t first_fork = fork ();

/* BOTH the original parent and child create new processes */
pid_t second_fork = fork ();

/* This line will print four times */
printf ("Hello world from %d!\n", getpid ());
```

It may seem odd that `fork()` would create a new process running the exact same code as the parent. However, there are several times where this makes sense. Consider a web browser, such as Google Chrome. Whenever you open a new tab, Chrome will make a call to `fork()` to create a new process for that tab. Intuitively, all tabs in a web browser work the same way: they get a URL request from the user and retrieve the corresponding web page or file from a server somewhere. Consequently, it makes sense that all of the processes for Chrome would start with the same code segment. Making the tabs run in separate processes provides some security and reliability: If a plug-in crashes in one tab, only that tab is affected; the other tabs exist in other processes, so they are isolated from the crash.

🐞🐛🐌 Bug Warning

* * *

Once a process uses `fork()` to create a child process, the timing of the execution becomes nondeterministic from the programmer’s point of view. The programmer cannot predict or control when the child begins executing relative to the parent. In some systems, the child process may begin executing immediately before the parent process returns from the call to `fork()`. In others, the child may simply be created and start running at some later time. Consequently, if the order of the execution matters, the programmer must take extra steps to ensure correct behavior; one example of this is to use `wait()` as described below. We will examine more advanced techniques in later chapters.

///2.5.2. Switching Program Code[¶]
-----------------------------------

In many cases, after making a call to `fork()`, you want to switch to a different program. For instance, consider the `bash` terminal program. Within `bash`, some commands are considered _built-in_ and are executed by `bash` itself. One example of a built-in is the export command, which can be used to define or change an [environment variable](#term-environment-variable), such as `PATH` or `CLASSPATH`. Other commands are used to call a separate compiled program, such as `ls`, `gcc`, or `vim`. The code to execute these commands does not exist as a part of `bash`. Rather, these are distinct programs and the code needs to be loaded into the new process.

📦 C library functions – `<unistd.h>`

* * *

`int execl(const char *path, const char *arg0, ...  /*, (char *)0 */);`
    Executes the program identified by the exact path.

`int execle(const char *path, const char *arg0, ...  /*, (char *)0, char* const envp[]*/);`
    Executes the program at path and set environment variables.

`int execlp(const char *file, const char *arg0, ...  /*, (char *)0 */);`
    Looks up the program file in the current `PATH`.

`int execv(const char *path, char *const argv[]);`
    Like execl, but command-line arguments are in an array.

`int execve(const char *path, char *const argv[], char *const envp[]);`
    Like execle, but command-line arguments are in an array.

`int execvp(const char *file, char *const argv[]);`
    Like execlp, but command-line arguments are in an array.

`int fexecve(int fd, char *const argv[], char *const envp[]);`
    Executes the program stored in an open file handle fd.

Loading a new program is handled by the `exec()` system call, which can be invoked by a family of C functions. The functions differ by how you pass the parameters. The `execl()`, `execle()`, and `execlp()` functions take the command line arguments as additional function parameters, whereas execv(), execve(), and execvp() require you to create a single array that contains all of the arguments. The `execl()` and `execv()` functions expect that you provide the exact path to the executable file, while `execlp()` and `execvp()` will look up the path for you. The `execle()` and `execve()` functions allow you to specify a custom set of environment variables for the new program. Finally, `fexecve()` allows you to pass an open file descriptor for the program instead of the program’s name or path. [Code Listings 2.8](#cl2-8) and [2.9](#cl2-9) show two different ways to call the `ls` program to list files, one using `execl()` and the other using `execvp()`.

```cpp
/* Code Listing 2.8:
   Using execl() lists all parameters in one list and requires an exact path
 */

/* Create the child and run 'ls -l' */
pid_t child_pid = fork ();
if (child_pid < 0)
  exit (1); /* exit if fork() failed */

if (child_pid == 0)
  {
    int rc = execl ("/bin/ls", "ls", "-l", NULL);
    exit (1); /* exit if exec fails */
  }	

/* Make the parent wait for the first child */
wait (NULL);
```

```cpp
/* Code Listing 2.9:
   Using execvp() puts command-line arguments in a vector and does a path lookup
 */

/* Set up all parameters for 'ls -l' in a single vector */
char *const parameters[] = { "ls", "-l", NULL};
child_pid = fork ();
if (child_pid < 0)
  exit (1); /* exit if fork() failed */

if (child_pid == 0)
  {
    int rc = execvp ("ls", parameters);
    exit (1); /* exit if exec fails */
  }

/* Make the parent wait for the second child */
wait (NULL);
```

There are a couple of important points to note in this code. First, note that both calls to `ls` will have the same three arguments (`argv[0] = "ls"`, `argv[1] = "-l"`, and `argv[2] = NULL`). That is, the first parameter passed to `execl()` on line 12 of [Code Listing 2.8](#cl2-8) is not included in the argument list as the child process receives it. The convention that `argv[0]` is the name of the program arises from how programs like bash specify the remaining parameters to the `exec()` functions.

Second, observe that both versions use `NULL` to indicate the end of the arguments. In the case of `execl()`, function takes a variable-length parameter list, so it relies on `NULL` to know when it should stop looking for arguments. In the case of `execvp()`, the function expects exactly two arguments, with the second being an array of unknown length.

Third, note that the first parameter of `execl()` must be an exact path to the location of the executable file. If you call `execl()` with just `"ls"` instead of `"/bin/ls"`, then it will look for an executable file in the current directory that is named `ls`. If the file does not exist, the call to `execl()` will return a negative value to indicate an error. For the call to `execvp()`, bash will look through the directories specified by the `PATH` environment variable to try to find the executable file for you. If line 12 of [Code Listing 2.8](#cl2-8) used `execlp()` instead of `execl()`, `bash` would perform a `PATH` lookup there, as well.

Finally, note that no code after a successful call to any of the `exec()` functions will execute in the current process. In this case, [Code Listing 2.8](#cl2-8) line 13 and [Code Listing 2.9](#cl2-9) line 14 should never be reached, so neither of the calls to `exit()` should happen. The reason is that `exec()` **replaces the current process’s code, data, stack, and heap segments**. Assuming the call to `exec()` is successful, the child process will start over with a clean virtual memory instance; none of the parent process’s information is retained within the child process.

🐞🐛🐌 Bug Warning

* * *

Always make sure to check the return values for both `fork()` and `exec()`. Most OS enforce a limit on the number of processes that a single user can create; once you reach this limit, `fork()` will fail. Similarly, if you make a mistake in the name of the file that you want to run or you do not have permissions, `exec()` will fail and the child process will continue to execute the current program’s code. Failing to check these return values can lead to unexpected behavior.

///2.5.3. POSIX Spawn Interface[¶]
----------------------------------

In recent years, the use of `fork()` has been criticized for a variety of reasons. For instance, the implementation requirements of it are considered too slow and require too much power for very small embedded computing systems (e.g., think of the “computers” in a car that operate the cruise control or anti-lock breaking). Another problem is that calling `fork()` on a process with multiple threads (see Chapter 7) can lead to inconsistent copies of the parent process; that is, the thread that calls `fork()` might not have a completely accurate memory image, particularly in [multicore](#term-multicore) architectures. Lastly, the semantics of `fork()` have become very complicated, as there are many special cases that must be handled, such as how to deal with open files, timers, asynchronous I/O buffers, etc. [[1]](#f14)

To address some of these criticisms, POSIX includes a new function interface, `posix_spawn()`. This function takes multiple parameters, including the path to the executable and the `argv` and `envp` arrays that would be passed into `execv()` or `execve()` functions. In addition, `posix_spawn()` takes other parameters that can specify additional operations. These operations give the programmer explicit control over the special cases mentioned above.

📦 C library functions – `<spawn.h>`

* * *

`int posix_spawn(pid_t *pid, const char *path, const posix_spawn_file_actions_t *file_actions, const posix_spawnattr_t *attrp, char *const argv[], char *const envp[]);`
    Create a new process and load the code identified by the path in a single call.

[Code Listing 2.10](#cl2-10) shows how to use `posix_spawn()` to run an external program. In this case, the `cksum` program is run on a CSV file, as specified in the `path` and `argv` arguments. The child’s process ID is returned through the call-by-reference parameter `&child`. The `assert()` is used here to confirm that the process was successfully spawned; `posix_spawn()` returns a non-zero value if an error occurs at any point and the process was not successfully spawned.

```cpp
/* Code Listing 2.10:
   Using posix_spawn() to run a helper program instead of fork() and execv()
 */

pid_t child = -1;
char *path = "/usr/bin/cksum";
char *argv[] = { "cksum", "movies.csv", NULL };

/* Spawn and run an external program in one step */
assert (posix_spawn (&child, path, NULL, NULL, argv, NULL) == 0);

/* Make the parent wait for the child */
wait (NULL);
```

///2.5.4. Process Destruction[¶]
--------------------------------

A process can be terminated in a number of ways. Some methods of process termination are voluntary in nature, such as when the code makes a call to `exit()`, `abort()`, or returns from `main()`. Other methods are involuntary, such as when a process is killed after a segmentation fault or similar exception. Regardless of what causes the process termination, the same set of procedures are typically performed: open file handles are closed, the virtual memory space is destroyed, the associated physical memory is reclaimed by the system, and any relevant data structures in the kernel are cleaned up.

The preceding example also illustrates a key element about the timing of the destruction of processes. Once a process calls `fork()` and creates a child process, the relative scheduling between the parent and child is nondeterministic from the programmer’s perspective. The two processes are scheduled independently by the OS running on that particular machine, and the scheduling will be influenced by other processes running, as well. However, the parent can temporarily pause its own execution by making a call to `wait()` or `waitpid()`.

📦 C library functions – `<sys/wait.h>`

* * *

`pid_t wait(int *stat_loc);`
    Waits on all children, gets status information in stat_loc.

`pid_t waitpid(pid_t pid, int *stat_loc, int options);`
    Waits on a particular child process identified by pid.

The `wait()` system call will block the current process until all of its children have been terminated. A call to `waitpid()` is similar, but it only waits for a single, identified child to terminate. In the preceding example, the first child makes a call to `execl()` to run the `ls` program. At approximately the same time, the parent process calls `wait(NULL)`. The nondeterministic nature of scheduling means it is impossible to determine which happens first, but the result is the same: the parent process will not proceed to the second call to `fork()` until the first child process runs the `ls` program and terminates.

Note that there is no problem even if the child process terminates before the parent gets scheduled to run. If all children have been terminated by the time a process calls `wait()`, then the parent process will simply continue on without pausing. The only requirement for `wait()` is that the parent cannot continue if there is at least one child process still trying to run.

The convention with C programs is to return an integer value from `main()` to indicate whether the program completed successfully. The standard practice is to return 0 to indicate that no error occurred, whereas a non-zero value typically indicates an error occurred in some way. When working on the command-line, this value can be retrieved by printing the `bash` variable `$?`. In C programs, this value can be retrieved through the `int *` parameter passed to `wait()` as shown in [Code Listing 2.11](#cl2-11).

```cpp
/* Code Listing 2.11:
   Using wait() and WEXITSTATUS() to retrieve a process's return code
 */

/* Create the first child and run 'ls -l' */
pid_t child_pid = fork ();
if (child_pid < 0)
  exit (1); /* exit if fork() failed */
if (child_pid == 0)
  exit (23); /* child process exits */

/* Make the parent wait to retrieve the return code*/
int status = 0;
pid_t exited = wait (&status);

/* Make the parent print the child's exit code of 23 */
printf ("PID %d exited with code %d\n", exited, WEXITSTATUS(status));
```

🐞🐛🐌 Bug Warning

* * *

The return value of the child process is shifted to the left when it is retrieved using `wait()`, as the lower eight bits are used for other purposes. The `WEXITSTATUS()` macro corrects this bit shifting to receive the actual return code from the child process.

[[1]](#id4)

For a full description of the problems with `fork()`, see the paper: A. Baumann, J. Appavoo, O. Krieger, and T. Roscoe, “A `fork()` in the road.” In Workshop on Hot Topics in Operating Systems (HotOS ‘19). New York, NY: ACM, 2019.



//2.6. The UNIX File Abstraction[¶]
===================================

When multiple processes exist on a single machine, they rely on virtual memory to create the illusion that they have sole access to the CPU; the context switch mechanism prevents one process from accessing another process’s register values, stack, heap, etc. However, processes ultimately do not have sole access to the entire machine. There are many resources, such as a network interface, storage devices, user input devices, and so on, that must be shared with other processes on the same machine. As such, processes act as a unit of ownership for instances of access to these resources.

The [UNIX file abstraction](#term-unix-file-abstraction), which is widely used in modern OS design, provides a uniform interface to these various shared resources. This abstraction relies on two features: **a file is a sequence of bytes** and **everything is a file**. It is important to emphasize that this definition is different from the common usage of the term “file,” which is typically associated with persistent data storage. The key differences between this common usage and the UNIX file abstraction are as follows:

> *   Arbitrary or bidirectional access to a file is not necessarily possible. In some cases, once a byte has been read from the file, that byte no longer exists in the file; there is no way to seek to a previous position in such files. Similarly, sequential access of the bytes in order may be required, with no way to skip ahead.
> *   Files may not have names or persistent storage. Some files (such as those described in Chapter 3 for [interprocess communication](#term-interprocess-communication), commonly referred to as IPC) exist solely as in-memory constructs at run-time, identified only by an integer [file descriptor](#term-file-descriptor). Other files (such as `/dev/random` on UNIX and Linux systems) exist solely as an abstract interface to a hardware component or generate data at run-time on demand.
> *   Files do not necessarily have structure or typing. Readers are likely family with persistent files that can be distinguished by a file extension. For instance, a file with the `.pdf` extension has a different internal structure than one with a `.png` extension; programs that read or write these files must make sure that the bytes adhere to a pre-defined semantic structure. However, in the UNIX file abstraction, this pre-defined structure does not exist; a file is just a sequence of bytes.

By removing so much contextual information about files, this abstraction might seem to lose much of its meaning or utility. On the contrary, this abstraction greatly simplifies the work of dealing with a variety of resources; there are certain operations (creating, deleting, opening, closing, reading, writing) that are common to the lifecycle of all files. The UNIX file abstraction provides a single, consistent interface for these operations, thus eliminating much of the complexity of supporting many such resources.

///2.6.1. Basic File Access[¶]
------------------------------

The most basic operations for working with files are creating and opening them. For files that can be identified with named locations in the file system directory structure (such as `/dev/random`, `/usr/bin/cksum`, or `/home/csf/movies.csv`), we can use the `open()` function. The first parameter is the path to the file; this path can be an _absolute path_ (such as `/dev/random`) or a _relative path_ (such as `../src/main.c`) that describes the location relative to the current working directory. If the file is successfully opened, the return value from `open()` is the file descriptor, a non-negative integer value that other functions use to identify the file. This value should typically be greater than 2, as the default behavior is to open three files when a process is created: 0 (`STDIN_FILENO`) for standard input (such as reading from the command prompt), 1 (`STDOUT_FILENO`) for standard output (such as writing out to the screen), and 2 (`STDERR_FILENO`) for standard error (also writing out to the screen).

📦 C library functions – `<fcntl.h>`

* * *

`int open(const char *path, int oflag, ...);`

    Open or create a file for reading or writing.

The second parameter (`oflag`) specifies how the file will be accessed by the current process. [Table 2.2](#tbl2-2) shows the flags that may be passed as a bit-mask to `open()`. Note that these flags do not necessarily align with the common notion of file permissions; a file that is accessible for both reading and writing may be opened in read-only mode (`O_RDONLY`). However, if the file permissions do not allow the requested access, `open()` will return -1.

| Permission |                            Purpose                             |
|------------|----------------------------------------------------------------|
| O_RDONLY   | Open for reading only
| O_WRONLY   | Open for writing only
| O_RDWR     | Open for reading and writing
| O_NONBLOCK | Do not block on opening while waiting for data
| O_CREAT    | Create the file if it does not exist; requires passing `mode_t` 
| O_TRUNC    | Truncate to size 0
| O_EXCL     | Error if `O_CREAT` and the file exists

Table 2.2: Flags for opening files

For the common usage of the term “file,” the `O_NONBLOCK` flag is the least intuitive in [Table 2.2](#tbl2-2), as this flag is normally used for other purposes. Specifically, this flag plays an important role in IPC and network programming. When using a file to communicate with other processes (either on the same machine or across the network), the default behavior for reading is for processes to [block](#term-blocking-i-o) (pause) until the data has been received from the sender. The `O_NONBLOCK` flag changes this behavior so that reading will immediately fail and the process can move on to other work instead of waiting.

[Code Listing 2.12](#cl2-12) illustrates how the flags can be combined as a bitmask using the bitwise-or (`|`). In this case, the file is also being created (`O_CREAT`) with a size of 0 bytes initially (`O_TRUNC`) and the current process will have write-only access (`O_WRONLY`). This file will be persistent and stored in the file system with 644 permissions (6 = read and write for the owner of the file, 4 = read-only for the associated group and others); as such, the file could later be opened in read-write mode. Note that this third parameter (`mode`) is required when creating a new file, but is ignored at other times.

```cpp
/* Code Listing 2.12:
   Creating a new (empty) file that is ready for writing
 */

/* This will create an empty file */
char *path = "data.log";
mode_t mode = 0644;
int fd = open (path, O_CREAT | O_TRUNC | O_WRONLY, mode);
```

Once the file has been opened, it can be read from. The `read()` function takes three parameters: the file descriptor, the address of a buffer in memory to read the bytes into, and the maximum number of bytes to read. [[1]](#f15) The value returned from `read()` indicates the actual number of bytes successfully read, which may be fewer than the `nbyte` parameter. (Calling `read()` with `nbyte` set to 100 on a file that only contains 10 bytes of data will return 10, not 100.) Finally, when the process is finished working with a file, the `close()` function will release any associated resources in the kernel or the C library data that have been allocated for this process.

📦 C library functions – `<unistd.h>`

* * *

`ssize_t read(int fildes, void *buf, size_t nbyte);`
    Read up to nbyte bytes from a file into the buffer identified by buf.

`int close(int fildes);`
    Deletes a file descriptor.


🐞🐛🐌 Bug Warning

* * *

There are several key aspects of working with files that are easy to underestimate. First and foremost is the importance of using a correct value for the `nbyte` parameter of `read()`. This parameter always indicates the maximum number of bytes to read and it should never indicate more than the size of the allocated buffer pointer. [Buffer overflows](#term-buffer-overflow) are some of the most dangerous and persistent sources of software vulnerabilities, and passing an incorrect parameter to `read()` is a common culprit. Consider the following example:

```cpp
/* Allocate a buffer of 2 bytes */
char *buffer = calloc (2, sizeof (char));
/* WRONG: This reads MORE THAN 2 bytes into the buffer */
read (fd, buffer, sizeof (buffer));
```

The problem here is a misunderstanding of the `sizeof()` keyword, which returns the size of the specified parameter. The misunderstanding is that `sizeof(buffer)` returns the size of a pointer variable (8 bytes on a 64-bit system), not the size of the dynamically allocated buffer on the heap. (Contrast this with lines 5 and 6 in [Code Listing 2.13](#cl2-13) below.) As such, this code is trying to read up to 8 bytes of data into a buffer than can only hold 2 bytes. The result is that `read()` will simply copy the additional 6 bytes into the memory after the end of the buffer, potentially corrupting other data.

There are other frequent, though less serious, problems with using files. One (which is also in the example above) is to call `read()` without checking its return value; programmers often assume that the number of bytes read is the same as the number of bytes requested, which is not necessarily true. To illustrate this, consider the possibility of calling `read()` on a file that has been opened in `O_WRONLY` mode; `read()` would return -1 to indicate the operation failed. Another problem is failing to call `close()`; this causes memory leaks, as allocated data is not freed up appropriately. On the other hand, another problem can arise when a file descriptor is used after the file has been closed; this can cause future reads to fail or (potentially even worse) to read from the wrong file.

[Code Listing 2.13](#cl2-13) illustrates how to open, read from, and close a file. In this example, we are reading from a special file known as `/dev/random`. This file can be used to generate a sequence of random numbers one byte at a time; every time this code runs, the result should be different. Note that the file is closed on line 13, but the data is not used by the program until line 17. This is not a problem, as the data was read into the process’s memory; that is, the `read()` operation has made a copy of the data on the stack, so access to the file is no longer necessary.

```cpp
/* Code Listing 2.13:
   Reading 10 random numbers using the /dev/random file
 */

uint8_t buffer[10]; // space allocated automatically on the stack
memset (buffer, 0, sizeof (buffer));

/* /dev/random is a special device file that produces an
   unending stream of random numbers */
int fd = open ("/dev/random", O_RDONLY);
assert (fd > 0);
ssize_t bytes = read (fd, buffer, sizeof (buffer));
close (fd);

printf ("Read %zd bytes of random data:\n", bytes);
for (int i = 0; i < bytes; i++)
  printf ("  %02" PRIx8, buffer[i]);
printf ("\n");
```

Some files, particularly IPC and device interface files, require special handling when reading. Recall that the default behavior for open files is to block until data is ready; this behavior is undesirable when other productive work could be done. For instance, a web server that is blocking while trying to read data from one client could be missing out on connection requests from other clients. The `poll()` function provides a useful interface for avoiding this situation.

📦 C library functions – `<poll.h>`

* * *

`int poll(struct pollfd fds[], nfds_t nfds, int timeout);`
    Examine an array of file descriptors to determine if some are ready for I/O.


The first argument to `poll()` consists of an array of `struct pollfd` instances, the second parameter is the length of the array, and the `timeout` designates a maximum amount of time (measured in milliseconds) to wait for input to be ready. The fields of the `struct pollfd` are shown below. For each `struct` in the array, the `fd` field designates a file descriptor to monitor for input or output events, and the `events` field designates the events to wait for. Typically, `events` is set to the constant `POLLIN` to indicate a check for the presence of normal data that can be read without blocking. The `revents` field is set by the call to `poll()`.

```cpp
/* defined in poll.h */
struct pollfd {
  int    fd;       /* file descriptor */
  short  events;   /* events to look for */
  short  revents;  /* events returned */
};
```

[Code Listing 2.14](#cl2-14) shows how to use `poll()` to check for available data. If `poll()` returns 0, then the requested event (available input data) has not occurred before the timeout expired. The `revents` field would be set to a value to indicate why the `poll()` failed. For instance, `POLLHUP` indicates the device has been disconnected, `POLLNVAL` indicates the file descriptor is not open, and `POLLERR` indicates an error has occurred with the device.

```cpp
/* Code Listing 2.14:
   Checking a file descriptor for available input data
 */

/* Set up a single pollfd for the file descriptor fd */
struct pollfd fds[1];
fds[0].fd = fd;
fds[0].events = POLLIN; // Looking for input data

if (poll (fds, 1, 100) == 0) // wait for 100 ms
  {
    /* No data is available to be read */
    printf ("Poll failed: %d\n", fds[0].revents);

    /* Close and exit if appropriate */
    close (fd);
    exit (1);
  }
```

///2.6.2. Working with Files[¶]
-------------------------------

In addition to reading, programs typically need to write to a file. The arguments to `write()` are identical to those for `read()`. Unlike `read()`, there is not really a concern with buffer overflow with `write()`, as data is being sent away from the current process; the kernel buffers on the other end will prevent such errors. However, checking the return value from `write()` is as important as it is with `read()` to make sure that all of the intended data was written successfully; this is especially true when writing large pieces of data. [Code Listing 2.15](#cl2-15) illustrates how to write to a file. Note that writing to the end of a persistent file will cause it to grow. In this example, the file is created to be empty (`O_TRUNC`), but writing six bytes creates a file of size six (the last byte is the null terminator `'\0'`).

📦 C library functions – `<unistd.h>`

* * *

`ssize_t write(int fildes, const void *buf, size_t nbyte);`
    Write up to nbyte bytes from a buffer into the specified file.


```cpp
/* Code Listing 2.15:
   Creating an empty file and writing to it
 */

/* Create an empty file with read and write permissions */
int fd = open ("blank", O_CREAT | O_TRUNC, S_IRUSR | S_IWUSR);
close (fd);

/* Open the file for writing (write will append) */
fd = open ("blank", O_WRONLY);
assert (fd > 0);
assert (write (fd, "hello", 6) == 6);
close (fd);

/* Read in what we just wrote */
char buffer[6];
memset (buffer, 0, sizeof (buffer));
fd = open ("blank", O_RDONLY);
assert (read (fd, buffer, 6) == 6);
close (fd);
printf ("Contents: [%s]\n", buffer);
```

If the file supports arbitrary accesses, the `lseek()` function will change the file’s internal location information to a specified target. The `offset` can be specified as either a positive or negative value. The `whence` parameter, which takes a limited number of possible values, plays an important role in determining this location. If `whence` if set to `SEEK_SET`, then the `offset` argument is the exact number of bytes into the file to use as the location. Setting `whence` to `SEEK_CUR` will add the `offset` to the current location number; a negative `offset` will seek backwards, while a positive value seeks forward. Lastly, setting `whence` to `SEEK_END` will add the `offset` to the size of the file; using a negative offset moves the location to the number of bytes before the end of the file. Whichever value is passed, the final location must be positive. If the location is larger than the file size, performing a write at that point will increase the file size accordingly. Any gap between the existing end of the file and the new data will be filled with null bytes.

📦 C library functions – `<unistd.h>`

* * *

`off_t lseek(int fildes, off_t offset, int whence);`
    Reposition the offset of a file descriptor to a specified location.


```cpp
/* Code Listing 2.16:
   Growing a file with lseek() and write()
 */

/* Open and jump to offset 10 (4 bytes after the end) */
int fd = open ("blank", O_RDWR);
off_t offset = lseek (fd, 4, SEEK_END);
printf ("Offset is now %lld\n", offset);

/* Write additional bytes, appending to the file */
size_t bytes = write (fd, "goodbye", 8);
printf ("Wrote %zd additional bytes\n", bytes);

/* Now read all of the file into a buffer and print it.
   Note that the file only contains 18 bytes */
char buffer[20];
memset (buffer, 0, sizeof (buffer));
offset = lseek (fd, 0, SEEK_SET);
printf ("Offset is now %lld\n", offset);
assert (read (fd, buffer, 20) == 18);

printf ("Contents:\n");
for (int i = 0; i < sizeof (buffer); i++)
  printf ("%02x '%c'\n", buffer[i], buffer[i]);
close (fd);
```

[Code Listing 2.16](#cl2-16) shows the effect of using `lseek()` and `write()` on the file created by Code Listing 2.15. The file initially contained six bytes (`'h'`, `'e'`, `'l'`, `'l'`, `'o'`, `'\0'`). The seek on line 7 places the internal location to offset 10. The `write()` on line 11, then, extends the file size to include the new data, as well as the padding of null bytes; the new file size would then be 18 bytes. As such, the `read()` on line 20 requests 20 bytes but only gets 18. Printing the final file with `hexdump` shows the results:

    00000000  68 65 6c 6c 6f 00 00 00  00 00 67 6f 6f 64 62 79  |hello.....goodby|
    00000010  65 00                                             |e.|
    00000012

///2.6.3. Accessing File Metadata[¶]
------------------------------------

When working with files, it is often important to access [metadata](#term-metadata) – information about the file – rather than the contents about the file itself. For instance, when reading a persistent file into memory from storage, knowing the file’s size is necessary for allocating memory for the buffer. As another example, consider an _intrusion detection_ program that is responsible for monitoring a file system for security threats or attacks; this program might check for changes to the associated permissions or the user ID that is considered the owner of the file.

📦 C library functions – `<sys/stat.h>`

* * *

`int fstat(int fildes, struct stat *buf);`
    Get status information about a file given an open file descriptor.

`int stat(const char *path, struct stat *buf);`
    Get status information about a file.


The `fstat()` and `stat()` functions provide an interface for accessing file metadata. Note that `stat()` uses the path name of the file within the directory structure, which is appropriate for the common notion of a file as persistent storage; however, `fstat()` works on any file descriptor, which allows you to examine the metadata of any file, including unnamed IPC or device files. Both functions take a pointer to a `struct stat`, writing the file metadata into this buffer.

```cpp
/* defined in sys/stat.h */
struct stat {
  dev_t    st_dev;    /* device inode resides on */
  ino_t    st_ino;    /* inode's number */
  mode_t   st_mode;   /* inode protection mode */
  nlink_t  st_nlink;  /* number of hard links to the file */
  uid_t    st_uid;    /* user-id of owner */
  gid_t    st_gid;    /* group-id of owner */
  dev_t    st_rdev;   /* device type, for special file inode */ 
  off_t    st_size;   /* file size, in bytes */
  /* ... other fields depending on OS ... */
};
```

This `struct` definition contains additional fields based on the particular operating system, but the ones shown here are consistent across multiple platforms. A full discussion of all of these fields is beyond the scope of this book, but a few of them are particularly important. To start, consider the `st_ino` andtils literal notranslate">st_nlink fields. Each file stored on typical storage device (USB drive, hard drive, etc.) is uniquely identified by an [inode](#term-inode), an on-disk data structure that contains the metadata; each inode is uniquely identified by an inode number (`st_ino`). However, the file might have multiple human-readable names in the directory structure. These names – _links_ (also called _hard links_) – all point to the same file contents; the `st_nlink` field indicates the number of links that exist to a single file. With hard links, there is only one file; there are just multiple references to the same location. In contrast a _symbolic link_ is a distinct file that is not represented in the inode. See Appendix A for a longer discussion of inodes and links.

Another key field of the `struct stat` is the `st_mode` field. The most common use of this field is to set permissions for accessing the file. These permissions include combinations of read, write, and execute for the owner of the file (the user), the associated group, or everyone else. The `st_mode` field also stores additional permissions and information about the file; for instance, this field can be used to distinguish symbolic links, regular files, or directories. [Table 2.3](#tbl2-3) shows the standard list of bitmask values that can be combined in the `st_mode` field.

|   Name  | Bitmask |   Description   |
|---------|---------|-----------------|
| S_IRUSR |  000400 | Read (user)     |
| S_IWUSR |  000200 | Write (user)    |
| S_IXUSR |  000100 | Execute (user)  |
| S_IRGRP |  000040 | Read (group)    |
| S_IWGRP |  000020 | Write (group)   |
| S_IXGRP |  000010 | Execute (group) |
| S_IROTH |  000004 | Read (other)    |
| S_IWOTH |  000002 | Write (other)   |
| S_IXOTH |  000001 | Execute (other) |

|   Name   | Bitmask |         Description         |
|----------|---------|-----------------------------|
| S_IFIFO  |  010000 | Named pipe (IPC)            |
| S_IFCHR  |  020000 | Character device (terminal) |
| S_IFDIR  |  040000 | Directory file type         |
| S_IFBLK  |  006000 | Block device (disk drive)   |
| S_IFREG  |  100000 | Regular file type           |
| S_IFLNK  |  120000 | Symbolic link               |
| S_IFSOCK |  140000 | Socket (IPC, networks)      |
| S_ISUID  |  004000 | Setuid (SUID) bit           |
| S_ISGID  |  002000 | Setgid (SGID) bit           |
| S_ISVTX  |  001000 | Sticky bit                  |

Table 2.3: Bitmasks used in the st_mode field

For example, the hello.c file above would have the bitmask `100644` (displayed as `-rw-r--r--` by the `ls -l` command), as it is a regular file (`100000`) with read/write permissions for the user and read for group and others. The symlink.c would have `st_mode` `120755` (displayed as `lrwxr-xr-x`). Note that the first character in the displayed version indicates the type of file (`-` for `S_IFREG`, `l` for `S_IFLNK`, `d` for `S_IFDIR`, and so on).

🔍 Note

* * *

The `SUID`, `SGID`, and sticky bits have complex meanings and interpretations. One source of their complexity is that `SUID` only affects executable regular files, the sticky bit (which is mostly obsolete and has changed over time) only affects, and `SGID` affects both executables and directories! These meanings can be summarized as follows:

> *   `SUID`: Processes created with this executable will inherit the user ID of the file’s owner, rather than the user ID of the user executing the program.
> *   `SGID` (regular file): Processes created with this executable will inherit the group ID of the file’s group, rather than the group ID of the user executing the program.
> *   `SGID` (directory): Files and subdirectories created in this directory will inherit the group ID of this directory.
> *   Sticky bit (modern usage): Files in this directory can only be deleted by the user who is considered the owner of the file.

When these bits are set on a file, `ls -l` displays them by overlaying them on top of the execute bits in the permission field, using an `'s'` in the user field for `SUID`, `'s'` in the group field for `SGID`, and `'t'` in the other field for the sticky bit; if the corresponding `'x'` bit is present, a lower-case letter is used, while an upper-case letter indicates the `'x'` bit is absent. For instance, `rwsr-x---` would indicate both `S_IXUSR` and `S_ISUID` are set; `rw-r-Sr--` would mean that `S_IGUID` is set but `S_IXGRP` is not.

[Code Listing 2.17](#cl2-17) illustrates how to use `fstat()` to investigate a file’s metadata. The address of the local variable `info` is passed to `fstat()` to collect the metadata on line 9. Lines 13 – 15 are using the bitwise-and operator (`&`) to determine if certain permission bits are set; the result would be non-zero (true) if the bit is set but would be zero (false) if not. Lines 18 and 19 demonstrate a very common technique when reading in files. Line 19 uses the `st_size` field to allocate the exact amount of space needed to read in the full file contents, then line 20 reads in exactly that number of bytes. Once the file is read into memory, it can be accessed in a variety of ways. Since this file is an ASCII-formatted CSV file, it can be manipulated just like a normal string. Line 24 uses `strtok()` to split this string at the first instance of the newline character (`'\n'`); line 25 can then print just that line as a string. This change only affects the in-memory buffer, and it does not change the contents of the original file stored on disk.

```cpp
/* Code Listing 2.17:
   Using fstat() to determine the file size and read in the exact amount of data
 */

/* Open a CSV file and read its status information */
struct stat info;
int fd = open ("movies.csv", O_RDONLY);
assert (fd > 0);
assert (fstat (fd, &info) >= 0);

/* Check the file size and permissions */
printf ("File is %lld bytes in size\n", info.st_size);
printf ("Is file readable by user? %s\n", (info.st_mode & S_IRUSR ? "yes" : "no"));
printf ("Is file executable by user? %s\n", (info.st_mode & S_IXUSR ? "yes" : "no"));
printf ("Is this a directory? %s\n", (info.st_mode & S_IFDIR ? "yes" : "no"));

/* Create a buffer that is the exact size of the file and
   read in the contents */
char *buffer = calloc (info.st_size, sizeof (char));
ssize_t bytes = read (fd, buffer, info.st_size);
assert (bytes == info.st_size);
close (fd);

char *line = strtok (buffer, "\n");
printf ("Here is the first line:\n%s\n", line);
```

🔍 Note

* * *

The traditional UNIX permission structure—assigning permissions based only on the user, group, or other—is inflexible and not well suited for many applications. For example, consider two user that are collaborating on a project. These two users both need full permissions to read and write to a file, but they do not want to make the file publicly accessible otherwise. Under the traditional approach, a system administrator could create a group containing these two users; the users could then set permissions based on the group ID. The problem is that each user can only be assigned to a single group. If these users also have similar collaborations with different users on the same system, they cannot use the same approach.

To fix this problem, many modern systems support _access control lists_ (ACLs). Using ACLs, users can grant or revoke permissions to other users on an individual basis. In addition, ACLs allow the same user to be a member of multiple groups. Rather than using the traditional ls and chmod commands to view and change permissions, ACLs use the `getfacl` and `setfacl` commands. Consider the following example of these two commands.

```sh
$ getfacl team
# file: team
# owner: csf
# group: staff
user::rwx
user:alissa:rwx
user:marcos:rwx
group::r-x
group:csfadmin:r-x
mask::rwx
other::--- 
default:user::rwx
default:user:alissa:rwx
default:user:marcos:rwx
default:group::r-x
default:csfadmin:r-x
default:other::---
$ setfacl -m u:sergei:rwx
$ setfacl -m d:u:sergei:rwx
```

For each file and directory, there is an assigned owner and group, just like the traditional UNIX permissions, as indicated by the lines beginning with `#`. The other lines are individual permissions that have been set for the particular file. The user and group permissions contain three fields separated by a colon (`:`). The middle field indicates which user or group and the third field indicates the permissions (read, write, execute); if the user or group field is empty, the permission applies to the owner or group of the file. Directories can also have `default` permission lines; any time a file is created in this directory, the specified default permissions are automatically assigned to it. When using `setfacl` to add, change, or remove a permission entry, the terms `user`, `group`, and `default` can be abbreviated as simply `u`, `g`, or `d`.

[[1]](#id2)

To reiterate the notion of files as just a sequence of bytes, note that the file descriptor here was not necessarily the value returned from `open()` as described above. For files that do not correspond to named locations in the directory tree structure, the file descriptor may be created by a different function (such as `pipe()` or `socket()` as described in Chapters 3 and 4).



//2.7. Events and Signals[¶]
============================

In between the creation and destruction of the process, a number of other events can occur that may or may not be expected. For instance, the program may require user input from the keyboard, or it may try to read data from a file. When these requests are made, the process may be put into a [blocked state](#term-blocked-state) until the requested input data is available. When that input data is provided, the process will then resume. Similarly, an external user or the kernel itself might force the process to enter a [suspended state](#term-suspended-state) for an indefinite time period. For instance, maybe the machine is overloaded and slow, so a system administrator intervenes to pause some processes temporarily.

Blocking, suspending, and resuming processes all correspond to the abstract notion of a process life cycle event. Other events can occur that do not necessarily cause a change in the process state (e.g., the process can ignore the event and continue running). Here, we’ll look at code for creating processes and handling these events using [signals](#term-signal).

///2.7.1. Sending Process Signals[¶]
------------------------------------

If you have been working with the command line for a while, there is a good chance that you have used signals without realizing it. When a process gets stuck (e.g., in an infinite loop), you might have hit `Ctrl-c` to kill the process. Or if you knew the PID, you may have executed `kill -9` on the PID or even `killall` with the name of the program. In all of these cases, bash sent a signal to the killed process that caused the process to terminate.

Signals can be identified by either their name or their number. From the command line, you can send a signal using the `kill` utility with either the signal name or the associated number. For instance, the following command would send `SIGKILL` to process number 12345:

    $ kill -KILL 12345

This command is identical to calling `kill -9 12345`, because `SIGKILL` is signal 9 in POSIX systems. One thing to note about using the `kill` utility is that the `"SIG"` part of the name is omitted. You can observe this in the previous example, where we used the `-KILL` argument to specify the `SIGKILL` signal. [Table 2.4](#tbl2-4) describes some common signals. Note that the signal numbers shown are for x86 and ARM architectures (and most others). Some CPU architectures, such as Alpha and MIPS, use different numbers for some of these. Because of these variations, it is generally preferred to use the signal name rather than the number.


|    Name   | Number |                                Description                                |
|-----------|--------|---------------------------------------------------------------------------|
| `SIGINT`  |      2 | Interrupts the process, generally killing it. Typically sent with Ctrl-c. |
| `SIGKILL` |      9 | Kills the process. Cannot be ignored or overwritten.                      |
| `SIGSEGV` |     11 | Sent to a process when it experiences a segmentation fault.               |
| `SIGCHLD` |     17 | Sent to a parent when a child process finishes. Used by wait().           |
| `SIGSTOP` |     19 | Suspends the process. Cannot be ignored or overwritten.                   |
| `SIGTSTP` |     20 | Suspends the process. Typically sent with Ctrl-z.                         |
| `SIGCONT` |     18 | Resumes a suspended process. >                                            |


Table 2.4: Common POSIX signals, their numbers (on x86 and ARM architectures), and their effects

As described in the table, `SIGINT` and `SIGKILL` serve essentially the same purpose, as do `SIGSTOP` and `SIGTSTP`. The difference is that programs can implement custom handlers for `SIGINT` and `SIGTSTP`, possibly even ignoring them. The reason for this is that the application may be in the middle of some critical task that cannot wait. For example, the process may be half-way completed with writing data to a file; a custom signal handler may allow the process to finish writing the file before exiting. `SIGKILL` and `SIGSTOP` cannot be overwritten, and these signals will kill or suspend the process immediately.

The `SIGCONT` signal can be used to resume a process that has been suspended with either `SIGSTOP` or `SIGTSTP`. For instance, you may start a program on the command line, then wish to make it run in the background. You could use `Ctrl-z` to send `SIGTSTP`, suspending the process; then, use the `bg` command to send `SIGCONT`, resuming the process in the background. Similarly, `fg` would send `SIGCONT`, but resume the process in the foreground.

The default behavior for C programs is to respond to `SIGSEGV` by printing a message that a segmentation fault occurred and then exit. A program can include a custom signal handler to respond to segmentation faults differently. As an example, this is how debuggers (e.g., `gdb`) detect segmentation faults. Instead of killing the process, the debugger pauses it, allowing the user to run the `backtrace` utility to determine what line of code caused the fault.

📦 C library functions – `<signal.h>`

* * *

`int kill(pid_t pid, int sig);`
    Send a signal to another process.


The `SIGCHLD` signal gets sent to a process whenever any of its children terminate. The `waitpid()` library function includes a custom signal handler that checks if the terminated process matches the one that the parent was waiting on. If not, the parent would suspend itself again. Similarly, `wait()` includes a signal handler to determine if all of the process’s children have completed.

[Code Listing 2.18](#cl2-18) shows how to use `kill()` to send a signal from inside a program. Since `fork()` returns 0 to the child process, only the child will enter the infinite loop on line 15. The parent process would skip over this and execute the call to `kill()` on line 20. Note that the timing of the two processes does not matter here. Without the `sleep()` on line 18, the parent would kill the child before it had a chance to run; adding the `sleep()` gives both a chance to print the message on line 7. Since the child is killed while running on line 15, only the parent will print the message on line 21.

```cpp
/* Code listing 2.18:
   Using the kill() function to terminate a child process in an infinite loop
 */

/* Create a child process */
pid_t child_pid = fork ();
printf ("Process %d is running\n", getpid ());

/* Exit if the fork fails to create a process */
if (child_pid < 0)
  exit (1);

/* The child enters an infinite loop */
if (child_pid == 0)
  while (1) ;

/* Make the parent sleep so the child gets a chance to run */
sleep (1);
/* The parent sends the SIGKILL signal to kill the child */
kill (child_pid, SIGKILL);
printf ("Process %d is exiting\n", getpid ());
```


///2.7.2. Custom Signal Handlers[¶]
-----------------------------------

To overwrite a signal handler, you start by defining a function with the desired new behavior. In the simplest form, this function must take exactly one `int` parameter and have a `void` return type. The parameter received by this function will be the specific signal that was passed. As such, it is possible to define one signal handling function and use it to respond to multiple signals.

📦 C library functions – `<signal.h>`

* * *

`int sigaction(int sig, const struct sigaction *restrict act,  struct sigaction *restrict oact);`
    Use a custom function to respond to a standard signal.


[Code Listing 2.19](#cl2-19) shows how to use `sigaction()` to overwrite a signal handler. Lines 6 – 21 define the behavior of the signal handler. On line 29, this function name (i.e., the address of the function) is copied into the `sa_handler` field of a `struct sigaction`. Line 32 successfully overwrites the `SIGINT` handler, so using `Ctrl-C` on the command line will invoke the custom `sigint_handler()` function. Line 36 will fail to overwrite the `SIGKILL` handler, as this action is not permitted. When this program is run, it can still be killed with the `kill -9` command.


```cpp
/* Code Listing 2.19:
   Overwriting the SIGINT handler to overwrite Ctrl-C termination
 */

/* Create the function for responding to signals */
static void
sigint_handler (int signum)
{
  /* Length is hard-coded at 19; write signum into spaces */
  char buffer[] = "Received signal   \n";
  if (signum < 10)
    buffer[16] = '0' + signum;
  else
    {
      buffer[16] = '0' + (signum / 10);
      buffer[17] = '0' + (signum % 10);
    }
  /* Must use write(), because it is asynchronous-safe */
  write (STDOUT_FILENO, buffer, 19);
  exit (0);
}

int
main (int argc, char *argv[])
{
  /* Create a sigaction and link it to the handler */
  struct sigaction sa;
  memset (&sa, 0, sizeof (sa));
  sa.sa_handler = sigint_handler;

  /* Overwrite the SIGINT handler */
  if (sigaction (SIGINT, &sa, NULL) == -1)
    printf ("Failed to overwrite SIGINT.\n");

  /* Try (and fail) to overwrite SIGKILL */
  if (sigaction (SIGKILL, &sa, NULL) == -1)
    printf ("This should fail. SIGKILL cannot change.\n");

  /* Loop until SIGINT or SIGKILL is received */
  printf ("Entering loop\n");
  while (1) ;
  return 0;
}
```

The call to `exit()` on line 20 is important for this example. Without this call, using `Ctrl-c` would no longer terminate the program. Each time that the `SIGINT` would be raised, the signal handler would print the message and the function would return back to what it was doing before. That is, the function would return back to the infinite loop on line 41. **This is very dangerous and should not be done in regular practice**. We can do so here only because our infinite loop is doing nothing.

In more realistic code, the signal might interfere with the executing code. For instance, the interrupted code might have just set a number of registers to prepare for a function call. The signal handler might then change these values, leading to errors when the normal execution resumes. If the application must be able to return to normal execution after processing a signal, the solution is to create a non-local goto with `sigsetjmp()` and `siglongjmp()`.

📦 C library functions – `<sig/setjmp.h>`

* * *

`int sigsetjmp(sigjmp_buf env, int savemask);`
    Set jump point for a non-local goto.

`void siglongjmp(sigjmp_buf env, int val);`
    Non-local goto with signal handling.


[Code Listing 2.20](#cl2-20) illustrates the basic structure of non-local gotos. Assuming the signal handler is set up properly (as indicated by the comment on line 17), the call to `sigsetjmp()` on line 18 creates a safe place to resume execution after handling the signal. When this function is called directly, the return value will be 0 (i.e., false); as such, the body of the if-statement will not be executed when the process first runs. The process will enter the infinite loop at line 21. Then, when the signal occurs, the `sig_handler()` will run. The call to `siglongjmp()` on line 11 causes the flow of execution to return to line 18, where the target of the goto was set. However, critically, this call also changes the return value from `sigsetjmp()` to become the second value passed to `siglongjmp()`! As such, after the signal handler runs, the process will resume normal execution as if `sigsetjmp()` had returned 1 (true) and printing the message on line 19. At that point, the process would return to the infinite loop. Note that both `sigsetjmp()` and `siglongjmp()` require the same global `sigjmp_buf` variable to be connected.

```cpp
/* Code Listing 2.20:
   Using non-local gotos with signal handling
 */

sigjmp_buf context;

void
sig_handler (int signum)
{
  /* Code to process the signal is omitted */
  siglongjmp (context, 1);
}

int
main (int argc, char *argv[])
{
  /* Code to set up the signal handler is omitted */
  if (sigsetjmp (context, 0))
    printf ("Resuming execution\n");
  printf ("Entering loop\n");
  while (1) ;
  return 0;
}
```

The existing code has a problem, however. When a signal occurs, a bit gets set in the _signal mask_ to indicate that this signal has been raised. If the signal occurs again, **the signal handler will not get invoked again**. Rather, signal handlers will only run when the bit in the signal mask first gets set. Consequently, if [Code Listing 2.20](#cl2-20) was used to overwrite `SIGINT`, only the first `Ctrl-c` would invoke the signal handler. All additional `SIGINT`s would be ignored. The `sigprocmask()` function can fix this problem by lowering the bit in the signal mask.

📦 C library functions – `<setjmp.h>`

* * *

`int sigaddset(sigset_t *set, int signo);`
    Add a signal to a signal set.

`int sigemptyset(sigset_t *set);`
    Initialize an empty signal set.

`int sigprocmask(int how, const sigset_t *set, sigset_t *oset);`
    Examine and change blocked signals.


[Code Listing 2.21](#cl2-21) fixes the signal handler from [Code Listing 2.20](#cl2-20) by resetting the signal mask. Line 12 will clear the bit (`SIG_UNBLOCK`) in the signal mask for each signal in the set. Since lines 9 – 11 create a set that contains only the signal that was being processed (i.e., because the current signal number gets passed as the `signum` argument), only the current signal will get reset. [[1]](#f16) Consequently, when the normal execution would resume, the signal could be repeatedly raised.

```cpp
/* Code Listing 2.21:
   Resetting a signal in the signal mask
 */

void
sig_handler (int signum)
{
  /* Code to process the signal */
  sigset_t set;
  sigemptyset (&set);
  sigaddset (&set, signum);
  sigprocmask (SIG_UNBLOCK, &set, NULL);
  /* Now that the signal has been cleared, do the goto */
  siglongjmp (context, 1);
}
```

🐞🐛🐌 Bug Warning

* * *

The number of functions that can be safely called from within a signal handler is very limited. For instance, you should never call `printf()` from within a signal handler. The problem is that most C functions in the standard library are not guaranteed to have well-defined behavior in an asynchronous context. To avoid bugs that arise from undefined execution timing, you should only call functions that are asynchronous-safe. The list of functions and more information can be found in CERT Secure Coding Rule SIG30-C.

There are other variations that can be used for custom signal handlers beyond what is shown here. For further information, consult the POSIX documentation for `sigaction()`. Also, note that there is an older `signal()` function that can also be used for overwriting signals. However, use of `signal()` is discouraged in the POSIX specification, and newer applications should use `sigaction()` instead.

[[1]](#id3)

Note that other signals could be added to this set; if those other signals were currently waiting to be handled, the call to `sigprocmask()` would clear them and their handlers would not be invoked.



//2.8. Extended Example: Listing Files with Processes[¶]
========================================================

In this Extended Example, the main process starts by overwriting the `SIGINT` signal handler. The process then enters a loop (line 29) where it repeatedly retrieves user input in the `process_input()` function. This function (lines 72 – 102) provides a simplified shell that accepts two possible commands: `ls` and `exit`. If the user enters the `ls` command, the main process uses `fork()` and `exec()` to list files in the current directory (`list_files()` in lines 49 – 65), similar to how `bash` works. The `exit` command will terminate the program by returning false from `process_input()`. Finally, the overwritten `SIGINT` handler (lines 36 – 46) will also check for and `kill()` child processes that may have become stuck (e.g., if the directory contains thousands of files).

```cpp
#include <assert.h>
#include <errno.h>
#include <signal.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/wait.h>
#include <unistd.h>

pid_t child_pid = 0;

static void sigint_handler (int signum); /* SIGINT handler */
static bool list_files (void);   /* child that lists files */
static bool process_input (void);   /* retrieve user input */

int
main (int argc, char *argv[])
{
  /* Create a sigaction and link it to the handler */
  struct sigaction sa;
  sa.sa_handler = sigint_handler;

  /* Overwrite the SIGINT handler, exiting on failure */
  assert (sigaction (SIGINT, &sa, NULL) != -1);

  /* Keep running until the user requests the program stop */
  bool running = false;
  while ((running = process_input ())) ;

  return 0;
}

/* Shutdown with Ctrl-c. This will terminate the infinite
   loops */
static void
sigint_handler (int signum)
{
  /* Kill any lingering child process if it exists */
  if (child_pid != 0)
    {
      write (STDOUT_FILENO, "Killing file list process before exiting.\n", 43);
      kill (child_pid, SIGKILL);
    }
  exit (0);
}

/* Create new process and run the "ls" command in that process */
static bool
list_files (void)
{
  /* Create child process */
  child_pid = fork ();
  if (child_pid < 0)
    return false;

  /* Child process will print the files */
  if (child_pid == 0)
    assert ((execlp ("ls", "ls", "-1tr", NULL)) == 0);

  /* Parent waits on the child to finish listing files */
  waitpid (child_pid, NULL, WUNTRACED);
  child_pid = 0; /* Child no longer exists */
  return true;
}

/* Show a prompt, get up to 99 bytes of command input, and
   respond to the user's request. Possible commands are "ls"
   "loop" and "exit". This loop can be thought of as a simplified
   bash shell, as it illustrates the basic mechanics of how bash
   receives commands and runs them in new processes. */
static bool
process_input (void)
{
  char buffer[100];
  char *rc = NULL;

  printf ("$ ");
  /* Clear the buffer and read in up to 99 bytes */
  memset (buffer, 100, '\0');
  rc = fgets (buffer, 99, stdin);

  /* Check for errors reading from stdin */
  if (rc == NULL)
    return false;

  /* Get the first token as the command */
  char *token = strtok (buffer, " ");
  if (token == NULL)
    return true;

  /* ls command is supported */
  if (!strncmp (token, "ls", 2))
    return list_files ();

  /* also allow exiting with the exit keyword */
  if (!strncmp (token, "exit", 4))
    return false;

  printf ("Invalid command: %s\n", token);
  return true;
}
```


/Chapter 3   Concurrency with IPC[¶]
====================================

//3.1. Concurrency with IPC[¶]
==============================

![Timeline of major CSF topics with Processes and IPC highlighted](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Timeline.3.png)

> “The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point.”
> 
> Claude Shannon

As soon as computers were able to run multiple processes concurrently, the natural next step was to make the processes communicate with each other. This chapter focuses on interprocess communication technologies, which emerged in the 1960s and 1970s. For instance, Douglas McIlroy, who had previously contributed to the Multics OS in the late 1960s, introduced the concept of pipes in UNIX (1973).

🚀 Chapter Objectives

* * *

In this chapter, we will address the following instructional objectives:

*   We will distinguish the properties of the two basic models for performing IPC.
*   We will compare and contrast the mechanics and uses of message passing IPC.
*   We will explore how shared memory techniques blur the lines of what constitutes a process.
*   We will examine code illustrating practical applications of IPC.

Processes are a robust mechanism for isolating programs from one another. In a multiprogramming environment with hundreds of processes running concurrently, the kernel ensures that a segmentation fault in one process (say, a web browser media plugin) cannot affect another (your email client). Similarly, once a process uses `fork()` to create a child process, the parent and child have distinct copies of every variable; changing the value of the variable in the child will not affect the parent, and vice versa.

In some circumstances, however, the isolation imposed by separate processes can be undesirable. For instance, all of the tabs in a modern web browser run in different processes but share the same overall browser configuration settings. If you add a new bookmark in one tab, it would be tedious to have to add that bookmark manually to all other open tabs. Similarly, it would be a waste of system resources for all of those processes to have duplicate copies of the browser source code.

As another example of why you might need to break the isolation guarantees of processes, consider a printer queue. While the kernel itself is responsible for controlling and accessing the printer, there are certain devoted system processes that provide the user with an interface to work with. With this process, you can find out if the printer is low on toner, what documents are queued up to be printed, and what printer errors might be occurring. This queue is running in a separate process. When you want to print the document in your word processor, the data must be transferred from one process to another.

This chapter focuses on the mechanics and uses of [interprocess communication](#term-interprocess-communication) (IPC). IPC allows two or more processes to exchange data while maintaining most of the guarantees of isolation. Some IPC exchanges are transient and are established only at the time the data is needed; once the transfer is complete, the IPC connection is terminated and the processes return to complete isolation from one another. Other IPC exchanges involve a persistent connection that can last for long durations, even when the two processes are not explicitly communicating. In this case, since these are distinct processes, they retain some isolation guarantees; if one of the processes crashes, the other is unlikely to be affected, unless the crash actually affects the shared IPC connection. Our goal is to examine the different techniques that can be used and when each technique is most appropriate.



//3.2. IPC Models[¶]
====================

There are multiple ways that different forms of IPC can be classified. The first and most common distinction is to separate techniques that adhere to a [message passing](#term-message-passing) model from a [shared memory](#term-shared-memory) model. In message passing, whenever a piece of data is to be exchanged, the process will invoke the kernel with a request to send the information to the target process. That is, the user-mode process will copy the data into a buffer or other form of data structure, then issue a [system call](#term-system-call) to request the data be transferred. Once the kernel is invoked, it will copy the transferred data first into its own memory. At some later point, the target process will also issue a system call to retrieve the data. In short, message passing techniques require the kernel become involved in every distinct exchange of data.

Shared memory techniques work fundamentally differently than message passing. In shared memory, the processes initially set up a region of their virtual memory to use for the IPC. Once the region is established within the process, the process issues a system call to request that the kernel make the region shared. Other processes would do the same. After the initial system call to set up the shared memory, the processes can read from and write to the region just as it would access non-shared data on its own heap. That is, the process could write to the region by dereferencing a pointer to it. This data then appears within the context of the other process automatically. There is no explicit system call required to read the new data.

///3.2.1. Advantages and Disadvantages[¶]
-----------------------------------------

Message passing and shared memory both have advantages and disadvantages relative to each other. One key dimension for comparing their performance is the amount of time overhead required. In message passing, every piece of data exchanged requires two system calls: one to read and one to write. In addition, the transferred data must be copied twice: once into the kernel memory and once into the receiving process. For a single message, this time penalty is insignificant and is unlikely to affect the process’s performance. However, if the number of messages passed is extremely large, the cumulative effect of this penalty may be significant.

In contrast, shared memory techniques only require a one-time performance penalty during the set-up phase. Once the memory has been shared, there is no additional penalty, regardless of the amount of data transferred. The trade-off is that the penalty for setting up shared memory is significant. The kernel must perform several slow operations to link the shared region to all of the processes’ virtual memory spaces.

Overall, if the two processes will be exchanging a lot of data back and forth repeatedly, shared memory performs very well. While the work to set up the shared memory is expensive, the aggregate effect of the performance is reduced. However, if processes only need to exchange a single message of a few bytes, shared memory would perform very poorly. Message passing techniques impose significantly smaller overhead to set up a one-time data exchange.

Shared memory also has another disadvantage that message passing avoids, which is the problem of [synchronization](#term-synchronization). If both processes try to write to the shared memory region at the same time, the result would be unpredictable and could lead to errors in one or both processes. Consequently, the accesses must be synchronized, meaning that the timing is carefully controlled. Since all message passing exchanges go through the kernel, synchronization techniques are not necessary.

As an example of the synchronization problem, consider an example where the two processes are keeping track of money in your bank account. One process writes a record for a \$100 purchase that you made, while another records a \$100 deposit. If both are recorded correctly, your new balance should be the same as when the processes started. However, if the account record is in shared memory and the writes are not properly synchronized, the results may not be correct. Instead of your final balance being exactly what you started with, you may end up with \$100 more or less than your original balance. Either you or your bank would be very unhappy with this result. Synchronization is a complex topic and we discuss it in its own chapter.

///3.2.2. An IPC Taxonomy[¶]
----------------------------

There are several different IPC techniques that can be used for a variety of different purposes. In The Linux Programming Interface [[Kerrisk2010]](Bibliography.html#kerrisk2010) , Kerrisk provides a taxonomy of UNIX IPC that is useful for classifying these techniques along a number of different dimensions. These dimensions include which model (shared memory or message passing), the primary intended purpose (data exchange or synchronization), and the granularity of the communication (byte stream or structured messages). We could also add a distinction between network-based or local to a single machine. [Table 3.1](#tbl3-1) shows this taxonomy.

Before summarizing the taxonomy, we note that the term _shared memory_ has two meanings in IPC that can cause some confusion. As we described above, shared memory is a general model that refers to the fact that the IPC occurs in a region of memory that is co-present within the multiple processes. Shared memory also refers to a specific technique for creating these regions; this technique uses library functions that begin with shm, such as `shmat()` or `shm_open()`. There are other shared memory techniques that rely on other functions. As an example, memory-mapped files are a form of shared memory that also copies all data into persistently stored files; data shared with `shm` functions only exists in memory. To avoid this collision of terminology, we will use `shm()` when referring to the specific techniques while reserving the term _shared memory_ for the more general model.

|     Technique      |      Model      |     Purpose     | Granularity | Network |
|--------------------|-----------------|-----------------|-------------|---------|
| pipe/FIFO          | message passing | data exchange   | byte stream | local   |
| socket             | message passing | data exchange   | either      | either  |
| message queue      | message passing | data exchange   | structured  | local   |
| shm()              | shared memory   | data exchange   | none        | local   |
| memory-mapped file | shared memory   | data exchange   | none        | local   |
| signal             | message passing | synchronization | none        | local   |
| semaphore          | message passing | synchronization | none        | local   |

Table 3.1: Classifying standard IPC techniques based on several characteristics

Although [signals](#term-signal) are covered in another chapter, we include them in this taxonomy because they can be interpreted as a limited form of IPC. Signals do not include any data and they are received asynchronously by the destination process. As such, they are very limited in the information they are conveying; they simply alert another process of a particular type of pre-defined event. However, this can be interpreted as a form of IPC.

[Sockets](#term-socket) are a flexible form of message passing IPC that can be used in multiple ways. The most common way that sockets are used is to send data across a network connection. As such, we will not consider sockets in this chapter and will explore their uses when discussing networks.



//3.3. Pipes and FIFOs[¶]
=========================

[Pipes](#term-pipe) allow processes to communicate using a unidirectional byte stream, with two ends designated by distinct [file descriptors](#term-file-descriptor). A common visual analogy for a pipe is a real-world water pipe; water that is poured into one end of the pipe comes out the other end. In the case of IPC, the “water” is the sequence of bytes being sent between the processes; the bytes are written into one end of the pipe and read from the other end. Pipes have several important characteristics that shape their use:

> *   Pipes are _unidirectional_; one end must be designated as the reading end and the other as the writing end. Note that there is no restriction that different processes must read from and write to the pipe; rather, if one process writes to a pipe then immediately reads from it, the process will receive its own message. If two processes need to exchange messages back and forth, they should use two pipes.
> *   Pipes are _order preserving_; all data read from the receiving end of the pipe will match the order in which it was written into the pipe. There is no way to designate some data as higher priority to ensure it is read first.
> *   Pipes have a _limited capacity_ and they use [blocking I/O](#term-blocking-i-o); if a pipe is full, any additional writes to the pipe will block the process until some of the data has been read. As such, there is no concern that the messages will be dropped, but there may be performance delays, as the writing process has no control over when the bytes will be removed from the pipe.
> *   Pipes send data as unstructured _byte streams_. There are no pre-defined characteristics to the data exchanged, such as a predictable message length. The processes using the pipe must agree on a communication protocol and handle errors appropriately (such as if one of the processes terminates the communication early).
> *   Messages that are smaller than the size specified by `PIPE_BUF` are guaranteed to be sent [atomically](#term-atomic). As such, if two processes write to a pipe at the same time, both messages will be written correctly and they will not interfere with each other.

///3.3.1. Basic Pipes[¶]
------------------------

The simplest form of communication with pipes is to provide parent-child communication using the `pipe()` library function. This function takes an int array of length 2. (Recall that arrays are always passed by reference.) Assuming the kernel is able to create the pipe, the array will contain the file descriptors for the two ends of the pipe. If the pipe creation, the function returns -1.

📦 C library functions – `<unistd.h>`

* * *

`int pipe (int pipefd[2]);`
    Opens a pipe and returns the file descriptors in the array.


Once the pipe is opened, we can use the standard `read()` and `write()` functions with the file descriptors. [Code Listing 3.1](#cl3-1) show the standard convention of using the array index 1 for writing and 0 for reading. This practice aligns with the use of file descriptor 1 for `stdout` and 0 for `stdin`.

```cpp
/* Code Listing 3.1:
   Sending a simple message through a pipe to a child process
 */

int pipefd[2];
char buffer[10];
/* Clear the buffer */
memset (buffer, 0, sizeof (buffer));

/* Open the pipe */
if (pipe (pipefd) < 0)
  {
    printf ("ERROR: Failed to open pipe\n");
    exit (1);
  }

/* Create a child process */
pid_t child_pid = fork ();
assert (child_pid >= 0);
if (child_pid == 0)
  {
    /* Child closes write-end, then reads from the pipe */
    close (pipefd[1]);
    ssize_t bytes_read = read (pipefd[0], buffer, 10);
    if (bytes_read <= 0)
      exit (0);

    printf ("Child received: '%s'\n", buffer);
    exit (0);
  }

/* Parent closes the unused reading end */
close (pipefd[0]);

/* Parent sends 'hello' and waits */
strncpy (buffer, "hello", sizeof (buffer));
printf ("Parent is sending '%s'\n", buffer);
write (pipefd[1], buffer, sizeof (buffer));
wait (NULL);
printf ("Child should have printed the message\n");
```

This code illustrates several conventions with using pipes. First, for the parent and child to communicate, `pipe()` must be called before `fork()`. This ordering is required to give both processes access to the pipe. Second, it is customary for each process to close one end of the pipe immediately after the `fork()`. This practice helps to align the unidirectional nature of the pipe with its intended use. That is, the convention is that a pipe is reserved for sending data from one process to another; if the second process wants to respond, it should use a different pipe. Third, the file descriptor at index 1 is used for writing, while index 0 is for reading.

🐞🐛🐌 Bug Warning

* * *

It is very important to follow the convention of closing the unused end of the pipe immediately after the `fork()`. Failure to do so can cause programs to freeze unexpectedly. Consider the following example:


```cpp
int pipefd[2];
pipe (pipefd);

if (fork () == 0)
  exit (0); /* child exits without writing */

char buffer[10];
read (pipefd[0], buffer, sizeof (buffer));
```

On line 8, the parent process will try to read from the pipe. Instead of immediately returning, the process will block until an `EOF` (end-of-file) is written into the pipe. Since the child is the only other process that could write to the pipe and the child exits without writing anything, the parent will block indefinitely. This problem can be difficult to diagnose, such as when the child process is redirecting the standard output of an external program using `dup2()` (which we illustrate below) and the external program produces no output. To avoid this frustration, always close the unused end of the pipe.

![Pipes are unidirectional and should not be used to respond](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.3.1.png)

Figure 3.3.3: Pipes are unidirectional and should not be used to respond

[Figure 3.3.3](Pipes.html#ipcpipe) illustrates a key feature and common bug with pipes. Once a process has used `pipefd[0]` in a call to `read()`, the same process cannot turn around and `write()` to `pipefd[1]`; doing so will fail silently and no data will be sent. To allow for bidirectional communication, use two pipes as in [Code Listing 3.2](#cl3-2). After performing the `fork()`, either pipe can be designated to be used for parent-to-child or child-to-parent messages.

```cpp
/* Code Listing 3.2:
   Using two pipes for bidirectional communication between parent and child
 */

int p2cfd[2]; /* parent-to-child */
int c2pfd[2]; /* child-to-parent */
char buffer[10];
ssize_t bytes_read = 0;

/* Clear the buffer and open the pipe */
memset (buffer, 0, sizeof (buffer));
if ((pipe (p2cfd) < 0) || (pipe (c2pfd) < 0))
  {
    printf ("ERROR: Failed to open pipe\n");
    exit (1);
  }

/* Create a child process */
pid_t child_pid = fork ();
assert (child_pid >= 0);

if (child_pid == 0)
  {
    /* Child closes write end of p2c, read of c2p */
    close (p2cfd[1]);
    close (c2pfd[0]);
    bytes_read = read (p2cfd[0], buffer, 10);
    if (bytes_read <= 0)
      exit (0);
    printf ("Child received: '%s'\n", buffer);

    /* Child sends response of "goodbye" */
    strncpy (buffer, "goodbye", sizeof (buffer));
    write (c2pfd[1], buffer, 10);
    exit (0);
  }

/* Parent closes read end of p2c, write of c2p */
close (p2cfd[0]);
close (c2pfd[1]);

/* Parent sends 'hello' and waits */
strncpy (buffer, "hello", sizeof (buffer));
printf ("Parent is sending '%s'\n", buffer);
write (p2cfd[1], buffer, sizeof (buffer));

/* Parent reads response back from child */
bytes_read = read (c2pfd[0], buffer, 10);
if (bytes_read <= 0)
  exit (1); /* should receive response */
printf ("Parent received: '%s'\n", buffer);
```

[Figure 3.3.4](Pipes.html#ipctwopipe) illustrates the circular structure of the two pipes used in Code Listing 3.2. The parent uses `p2cfd` to send data to the child. Responses from the child use the other pipe, identified by `c2pfd`. In both cases, the calls to `write()` use index 1 and the `read()` calls use index 0.

![Structure of the two pipes used in Code Listing 3.2](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.3.2.png)

Figure 3.3.4: Structure of the two pipes used in Code Listing 3.2

///3.3.2. Pipes and Shell Commands[¶]
-------------------------------------

One of the most common use of pipes is to link together multiple commands on the command line. For instance, consider the following command line:

```sh
$ ls -l | sort -n -k 5 | tail -n 1 | awk '{print $NF}'
```

This command line creates four processes that are linked together. First, the `ls` command prints out the list of files along with their details. This list is sent as input to `sort`, which sorts numerically based on the 5th field (the file size). The `tail` process then grabs the last line, which is the line for the largest file. Finally, `awk` will print the last field of that line, which is the file name of whatever file is the largest.

![Chained logical structure of a sequence of bash commands connected with pipes](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.3.3.png)

Figure 3.3.5: Chained logical structure of a sequence of bash commands connected with pipes

[Figure 3.3.5](Pipes.html#ipcpipeline) illustrates the chained structure of these four processes. These four processes are created by bash using both `fork()` and `exec()`. Once the processes are created, `bash` links their standard input and output by setting up a pipe to connect each process with the one after it. (This is the reason the vertical bar (`|`) is referred to as a pipe.) However, there is an additional step: `bash` needs to link the pipe with each process’s standard input and output. The `dup2()` function accomplishes this task.

📦 C library functions – `<unistd.h>`

* * *

`int dup2 (int oldfd, int newfd);`

Closes newfd and replaces it with the file of oldfd

🐞🐛🐌 Bug Warning

* * *

The arguments for `dup2()` are easily confused because of the names given in the standard C documentation. The `newfd` is the file descriptor that you want to use after the call to `dup2()`. For instance, if you want to change your file descriptors so that subsequent calls to `printf()` write to the pipe instead of the standard output screen, the `newfd` argument should be `STDOUT_FILENO`. (The confusion seems to arise because the pipe was created after standard output, so new programmers often think of the pipe file descriptor as “new,” which is incorrect.)

[Code Listing 3.3](#cl3-3) illustrates the basic functionality of how `bash` uses `dup2()` with pipes to link commands together. Specifically, the command line would be `ls | sort`, so `bash` needs to create and link two processes. The `sort` process closes the write end of the pipe and links the read end to become its standard input.

Similarly, the `ls` process closes its read end of the pipe and links the write end to its standard output. Anything that `ls` writes to its standard output (using `printf()`), `sort` would read from its standard input; both processes are unaware that the pipe exists. In fact, even if this code used `exec()` to load new program code within these child processes, the processes would continue to use the pipe as `stdin` and `stdout` without any change to the program’s source code.


```cpp
/* Code Listing 3.3:
   Creating a bash-like linkage between two processes
 */

/* Parent is acting like 'bash' interpreting the command line:
     $ ls | sort
   This example assumes the variable declaration and pipe creation
   as shown in Code Listing 3.1. */

/* 'sort' child process */
assert ((child_pid = fork ()) >= 0);
if (child_pid == 0)
  {
    /* 'sort' closes unused write end of the pipe */
    close (pipefd[1]);
    /* ...and uses the read end as standard input */
    dup2 (pipefd[0], STDIN_FILENO);
    /* Reading from "stdin" now reads from the pipe */
    ssize_t bytes_read = read (STDIN_FILENO, buffer, sizeof (buffer));
    if (bytes_read <= 0)
      exit (0);
    /* Trim off the trailing newline character */
    char *token = strtok (buffer, "\n");
    printf ("'sort' process received '%s'\n", token);
    exit (0);
  }

/* 'ls' child process */
assert ((child_pid = fork ()) >= 0);
if (child_pid == 0)
  {
    /* 'ls' closes the read end of the pipe */
    close (pipefd[0]);
    /* ...and uses the write end as standard output */
    dup2 (pipefd[1], STDOUT_FILENO);

    /* printf() now writes to the pipe instead of the screen */
    printf ("list of files\n");
    exit (0);
  }
/* 'bash' parent closes both ends of the pipe within itself */
close (pipefd[0]);
close (pipefd[1]);
wait (NULL);
```

This example also illustrates one subtle aspect of the closing of pipes. When a process closes one end of the pipe, it is only closing its access to that end of the pipe. Other processes may still use that end of the pipe. For instance, in the previous example, observe that the parent process closes both `pipefd[0]` and `pipefd[1]`. This does not affect the pipe itself, and it does not prevent the two child processes from communicating via the pipe. All these two lines of code do are close the parent’s (`bash`’s) access to the pipe, preventing it from reading or writing to the pipe. The pipe will remain in existence until all processes with access either close both ends of the pipe or exit (which closes all open file connections).

///3.3.3. FIFOs[¶]
------------------

The pipes described above create a simple mechanism for parent and child processes to communicate, but they cannot be used for unrelated processes. Specifically, notice that the call to `pipe()` must happen within the same program that later calls `fork()`. That is, the pipe is first created within a single process and is only shared with processes created as children (or children of children, if the child also calls `fork()`). This approach will not work for two random processes that need to create an ad hoc communication session. [FIFOs](#term-fifo) (first-in, first-out) are a variation on a pipe that creates a more flexible communication structure.

FIFOs work by attaching a filename to the pipe. For this reason, FIFOs are also called [named pipes](#term-named-pipe) as opposed to the anonymous pipes discussed previously. FIFOs are created by one process that calls `mkfifo()`. Once created, any process (with correct access permissions) can access the FIFO by calling `open()` on the associated filename. Once the processes have opened the file, they can use the standard `read()` and `write()` functions to communicate.

📦 C library functions – `<sys/stat.h>`

* * *

`int mkfifo (const char *pathname, mode_t mode);`

Creates a new file identified by the pathname to use as a FIFO

A common use for FIFOs is to create client/server applications on the same machine. For example, consider an anti-virus server that runs in the background, scanning for corrupted files. When the system administrator wants to get a report on potentially bad files, they run a client application that uses a FIFO to initiate contact with the server. Both the server and the client application are distinct processes that are running separate programs. That is, neither process was created by either of them calling `fork()`. As such, an anonymous `pipe()` call would not work. Instead, both processes use the name attached to the FIFO to set up the communication.

As a simple scenario, consider a server that prints hello whenever a client writes a non-zero value to a file and shuts down when the client writes a zero. [Code Listing 3.4](#cl3-4) shows the structure of the server. The server starts by creating the FIFO with read and write permissions for the current user. Then, the server opens the FIFO in read-only mode and enters the listening loop. Once the server reads a value of 0 from the FIFO, it exits the loop, then closes and deletes the FIFO.

```cpp
/* Code Listing 3.4:
   The basic structure of a server process using a FIFO
 */

/* Create the FIFO or die trying */
const char *FIFO = "/tmp/MY_FIFO";
assert (mkfifo (FIFO, S_IRUSR | S_IWUSR) == 0);

/* Try to open the FIFO. Delete FIFO if open() fails */
int fifo = open (FIFO, O_RDONLY);
if (fifo == -1)
  {
    fprintf (stderr, "Failed to open FIFO\n");
    unlink (FIFO);
    return 1;
  }

/* Main server loop */
while (1)
  {
    int req = 0;
    if (read (fifo, &req, sizeof (int)) != sizeof (int))
      continue;

    /* If we read a 0, quit; otherwise print hello */
    if (req == 0)
      break;
    printf ("hello\n");
  }

/* Read a 0 from the FIFO, so close and delete the FIFO */
close (fifo);
printf ("Deleting FIFO\n");
unlink (FIFO); 
```

[Code Listing 3.5](#cl3-5) shows a sample client. This client opens the FIFO, then writes a sequence of integers (5 down to 0) into the FIFO. Note that anything the client writes after the 0 would be thrown away, as the server would delete the FIFO at that point.

```cpp
/* Code Listing 3.5:
   A client process that sends six messages to the server in Code Listing 3.4
 */

const char *FIFO = "/tmp/MY_FIFO";

/* Use the file name to open the FIFO for writing */
int fifo = open (FIFO, O_WRONLY);
assert (fifo != -1);

/* Open the FIFO 6 times, writing an int each time */
for (int index = 5; index >= 0; index--)
  {
    /* Write 5, 4, 3, 2, 1, 0 into the FIFO */
    int msg = index;
    write (fifo, &msg, sizeof (int));

    /* Add a slight delay each time */
    sleep (1);
  }

/* Close the FIFO */
close (fifo);
```

Although FIFOs use standard file I/O functions (e.g., `open()`, `read()`, and `write()`), it is important to note that they are not regular files. Specifically, once data has been read from a FIFO, the data is discarded and cannot be read again (just like an anonymous pipe). In contrast, with a regular file, multiple processes can read the same data from the same file. That is, regular files store data persistently, but FIFOs do not. Consequently, FIFOs cannot be used for broadcasting a single message to multiple recipients; only one process can read the data. Similarly, FIFOs (like pipes) are not suitable for bi-directional communication; if a process writes into the FIFO then immediately tries to read a response, **it may read its own message**!

Also similar to anonymous pipes, FIFOs use [blocking I/O](#term-blocking-i-o) until both ends are opened by at least one process. As such, there is no concern about a process writing into the FIFO too soon; if no process has opened the FIFO for reading, the writing process will block until a reader becomes available. This behavior may be problematic if the writing process needs to perform some other task. To resolve this problem, pass the `O_NONBLOCK` option during the call to `open()` to make the FIFO access non-blocking.a" data-frame-width="950" data-frame-height="550" data-external="false" data-points="1.0" data-required="True" data-showhide="show" data-threshold="5" data-type="ka" data-exer-id="">



//3.4. Shared Memory With Memory-mapped Files[¶]
================================================

A [memory mapping](#term-memory-mapped-file) is a region of the process’s virtual memory space that is mapped in a one-to-one correspondence with another entity. In this section, we will focus exclusively on [memory-mapped files](#term-memory-mapped-file), where the memory of region corresponds to a traditional file on disk. For example, assume that the address `0xf77b5000` is mapped to the first byte of a file. Then `0xf77b5001` maps to the second byte, `0xf77b5002` to the third, and so on.

When we say that the file is mapped to a particular region in memory, we mean that the process sets up a pointer to the beginning of that region. The process can the dereference that pointer for direct access to the contents of the file. Specifically, there is no need to use standard file access functions, such as `read()`, `write()`, or `fseek()`. Rather, the file can be accessed as if it has already been read into memory as an array of bytes. Memory-mapped files have several uses and advantages over traditional file access functions:

> *   Memory-mapped files allow for multiple processes to share read-only access to a common file. As a straightforward example, the C standard library (`glibc.so`) is mapped into all processes running C programs. As such, only one copy of the file needs to be loaded into physical memory, even if there are thousands of programs running.
> *   In some cases, memory-mapped files simplify the logic of a program by using memory-mapped I/O. Rather than using `fseek()` multiple times to jump to random file locations, the data can be accessed directly by using an index into an array.
> *   Memory-mapped files provide more efficient access for initial reads. When `read()` is used to access a file, the file contents are first copied from disk into the kernel’s [buffer cache](#term-buffer-cache). Then, the data must be copied again into the process’s user-mode memory for access. Memory-mapped files bypass the buffer cache, and the data is copied directly into the user-mode portion of memory.
> *   If the region is set up to be writable, memory-mapped files provide extremely fast IPC data exchange. That is, when one process writes to the region, that data is immediately accessible by the other process without having to invoke a system call. Note that setting up the regions in both processes is an expensive operation in terms of execution time; however, once the region is set up, data is exchanged immediately. [[1]](#f17)
> *   In contrast to message-passing forms of IPC (such as [pipes](#term-pipe)), memory-mapped files create persistent IPC. Once the data is written to the shared region, it can be repeatedly accessed by other processes. Moreover, the data will eventually be written back to the file on disk for long-term storage.

///3.4.1. Memory-mapped Files[¶]
--------------------------------

Three functions provide the basic functionality of memory-mapped files. The `mmap()` and `munmap()` functions are used to set up or remove a mapping, respectively. Both functions take a length parameter that specifies the size of the region. The `mmap()` function also includes parameters for the types of actions that can be performed (`prot`), whether the region is private or shared with other processes (`flags`), the file descriptor (`fd`), and the byte offset into the file that corresponds with the start of the region (`offset`). The `addr` parameter for mmap() is typically NULL, allowing the system to determine the address of the region. For `munmap()`, the `addr` must be the start of the memory-mapped region (which is the value returned by `mmap()`).

📦 C library functions – `<sys/mman.h>`

* * *

`void *mmap (void *addr, size_t length, int prot, int flags, int fd, off_t offset);`

Map a file identified by fd into memory at address addr.

`int munmap (void *addr, size_t length);`

Unmap a mapped region.

`int msync (void *addr, size_t length, int flags);`

Synchronize mapped region with its underlying file.

One key issue with memory-mapped files is the timing of when updates get copied back into the file on disk. For instance, if another process opens and reads the file using `read()`, will this other process have access to any updates that were written to the memory-mapped region? The answer is that it depends on a number of timing factors.

The first factor is the kernel itself. When a file is mapped into memory with `mmap()`, the kernel will occasionally trigger a write to copy updated portions back to disk. This write can occur for a number of reasons and cannot be predicted. A second factor is the file system of the underlying file. Some file systems do not commit changes to the file until the writing process has closed its connection to the file. If the writing process still has the file mapped into memory, its connection must still be open; as a result, no other process would be able to access any updates written to the memory-mapped file.

Processes can insert control over this issue by using the `msync()` function. This function takes a flags parameter that can initiate a synchronous, [blocking](#term-blocking-i-o) write (`MS_SYNC`) or an asynchronous, non-blocking one (`MS_ASYNC`). In the case of the asynchronous write, the data will get copied to disk at a later point; however, the updated data would be immediately available to any process that reads from the file with `read()`.

///3.4.2. Region Protections and Privacy[¶]
-------------------------------------------

When setting up a memory-mapped file, the process must specify the protections that will be associated with the region (`prot`). Note that these protections only apply to the current process. If another process maps the same file into its virtual memory space, that second process may set different protections. As such, it is possible that a region marked as read-only in one process may actually change while the process is running. [Table 3.2](#tbl3-2) identifies the protections that can be combined as a bit-mask.

Protection

Actions permitted

PROT_NONE

The region may not be accessed

PROT_READ

The contents of the region can be read

PROT_WRITE

The contents of the region can be modified

PROT_EXEC

The contents of the region can be executed

Table 3.2: Protection flags for memory-mapped files

Memory-mapped regions can also be designated as private (`MAP_PRIVATE`) or shared (`MAP_SHARED`). When calling `mmap()` exactly one of these two options must be specified for the flags parameter. If a region is designated as private, any updates will not be visible to other processes that have mapped the same file, and the updates will not be written back to the underlying file.

[Code Listing 3.6](#cl3-6) shows how to map and unmap a file into memory. In this example, we are opening the `/bin/bash` executable file on Linux. Linux executables are formatted using the _executable and linking format (ELF)_ specification. Part of this specification indicates that the first byte of the file must be `0x7f` and the next three bytes are the ASCII characters for `ELF`. This program snippet confirms that `bash` on Linux is a valid ELF file. (It should be.)

```cpp
/* Code Listing 3.6:
   Read the first bytes of the bash executable to confirm it is ELF format
 */

/* Open the bash ELF executable file on Linux */
int fd = open ("/bin/bash", O_RDONLY);
assert (fd != -1);

/* Get information about the file, including size */
struct stat file_info;
assert (fstat (fd, &file_info) != -1);

/* Create a private, read-only memory mapping */
char *mmap_addr = mmap (NULL, file_info.st_size, PROT_READ, MAP_PRIVATE, fd, 0);
assert (mmap_addr != MAP_FAILED);

/* ELF specification:
   Bytes 1 - 3 of the file must be 'E', 'L', 'F' */
assert (mmap_addr[1] == 'E');
assert (mmap_addr[2] == 'L');
assert (mmap_addr[3] == 'F');

/* Unmap the file and close it */
munmap (mmap_addr, file_info.st_size);
close (fd);
```

![Memory-mapped files use provide direct access to contents](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.3.4.png)

Figure 3.4.2: Memory-mapped files use provide direct access to contents

[Figure 3.4.2](MMap.html#ipcmap) illustrates the structure of the memory-mapped file in [Code Listing 3.6](#cl3-6). The file’s original contents are stored on the hard drive. When the file is mapped into memory, the process has a region of memory that corresponds to the exact structure of the file, creating the appearance that the file’s contents have been copied into memory. The call to `mmap()` returns a pointer to this region, which can be accessed as an array of bytes.

[[1]](#id1)

Bypassing the kernel’s buffer cache can also be a disadvantage if other processes access the file using the traditional `read()` function. Specifically, both processes (one with the memory map and one without) will cause the data to be copied from disk into memory. Making the second transfer from disk into memory is significantly slower than making a duplicate copy from the buffer cache (which is in memory). Consequently, both processes would take a performance hit for loading the file from disk, even though the process using the memory-mapped file would experience slightly less impact.



//3.5. POSIX vs. System V IPC[¶]
================================

Throughout this book, we try to focus on platform-independent concepts and code examples as much as possible. In particular, we tend to focus on the [POSIX interface specification](#term-portable-operating-system-interface), given its broad base of support across the major modern OS. IPC is the exception that proves the rule. Some OS (e.g., macOS in particular) provide little or no support for POSIX IPC; instead, these OS uses the [System V interface](#term-system-v) for IPC. In this section, we will continue to focus on POSIX and describe the relevant interfaces for [message queues](#term-message-queue), [semaphores](#term-semaphore), and [shared memory](#term-shared-memory). Appendix B describes the System V IPC interfaces for reference and comparison.

🐞🐛🐌 Bug Warning

* * *

POSIX IPC code will not work successfully on macOS. macOS does not provide the required header files for POSIX message queues, so the code will not successfully compile. Even worse, macOS provides the header files for POSIX unnamed semaphores (allowing the code to compile), but the implementation consists of empty stub functions. That is, the semaphore functions will simply return **without actually acting like semaphores**. To avoid these issues, code intended to run on macOS should use the System V IPC interface instead. Pre-processor directives can be used to switch between the code required at compile time.

///3.5.1. POSIX IPC Fundamentals[¶]
-----------------------------------

All POSIX IPC functions identify and refer to IPC objects with a string that adheres to a given format. The string must start with an initial slash, then have one or more non-slash characters. For instance, `/OpenCSF_mqueue` is a valid name, presumably for a message queue. Each object’s name must be unique, so application designers need to ensure that any name they choose is unlikely to conflict with other applications.

IPC objects are often represented as a file within a particular directory in the file system. In Linux, information related to the `/OpenCSF_mqueue` message queue would be stored in `/dev/mqueue/OpenCSF_mqueue`, for example. These files can be accessed using the standard file commands, such as ls, cat, or rm. However, manipulating these files with these standard utilities may cause unexpected behavior, as running processes may be relying on these files.

In addition to a name, all of the functions for opening a POSIX IPC connection accept at least two other bit mask parameters: `oflag` and `mode`. The `oflag` parameter specifies the requested access needed (`O_RDONLY`, `O_WRONLY`, `O_RDWR`); note that these are identical to the standard options for opening a file with `open()`. Similarly, the `mode` parameter uses the standard file permissions (e.g., `S_IRUSR`, `S_IWGRP`) to set the permissions on a newly created object.

If a new object is being created, the `O_CREAT` flag must be included in the oflag bit mask. If `O_CREAT` is omitted and the object does not exist, an error will be returned. In addition, the `O_EXCL` flag controls the behavior when attempting to create an object, but one with the specified name already exists. If `O_EXCL` is included and an object exists, the open fails and an error is returned. If `O_EXCL` is omitted, no object is created, but the open returns a connection to the existing object. Note that passing `O_EXCL` without `O_CREAT` is undefined in the POSIX specification and should not be used. The following examples illustrate how to use these options:

> *   `shm_open ("/shared_mem", O_CREAT | O_RDWR, S_IWUSR | S_IRGRP);`
>     
>     Create and/or open shared memory; if it doesn’t exist, set permissions to 0420 (user has write, group has read).
>     
> *   `shm_open ("/shared_mem", O_CREAT | O_EXCL, S_IWUSR | S_IRUSR);`
>     
>     Create but do not open shared memory with 0600 permissions. Fail if it already exists.
>     
> *   `shm_open ("/shared_mem", O_RDONLY);`
>     
>     Open a read-only connection to shared memory. If it does not exist, fail.
>     

🐞🐛🐌 Bug Warning

* * *

The POSIX IPC implementations are typically provided within the _POSIX Real-time Library_ (`librt`). As such, the `-lrt` flag must be passed to gcc when compiling and linking programs that use these mechanisms.



//3.6. Message Passing With Message Queues[¶]
=============================================

[Message queues](#term-message-queue) allow processes to communicate by exchanging structured messages. As with [pipes](#term-pipe) and [FIFOs](#term-fifo), processes can message queues follow a [message passing](#term-message-passing) IPC model for exchanging data in discrete messages. However, message queues differ from pipes in several important ways:

> *   While pipes send unstructured byte streams, messages are sent as distinct units. When a message is retrieved from the queue, the process receives exactly one message in its entirety; there is no way to retrieve part of a message and leave the rest in the queue.
> *   Message queues use special identifiers, rather than file descriptors. As a result, message queues require special functions for sending and receiving data, rather than the standard file interface functions.
> *   Message queues have associated metadata that allows processes to specify the order in which messages are received. That is, message queues do not require or guarantee a first-in, first-out ordering.
> *   Message queues have kernel-level persistence and use special functions or utilities for removing them. Killing the process will not remove the message queue.

🐞🐛🐌 Bug Warning

* * *

It is important to understand the difference between messages and byte streams. Assume that one process has made 100 calls to a function to transmit a single byte at a time. If pipes or FIFOs are used, all of this data can be retrieved with a single call to `read()` that requests 100 bytes. In the case of message queues, each byte is a distinct message that must be retrieved individually. There is no short-cut to retrieve all of the data at once.

Now, assume that the sending process sent all 100 bytes with one function call. If the processes are using a pipe or FIFO and the receiver requests only 50 bytes with `read()`, that process will retrieve exactly 50 bytes. However, if message queues are used, the bytes would be converted into a single message; the size of the message is determined based on the parameters used to set up the message queue. For instance, the default message size for POSIX message queues in Linux is 8192 bytes. If the receiving process requests less than this (such as requesting only 50 bytes), **it will receive nothing**. Ignoring the difference between a message and a byte stream can lead to unexpected behavior that causes unanticipated failures.

POSIX message queues also provide a number of key features that are not available in other interfaces, such as System V:

> *   A POSIX message queue is only removed once it is closed by all processes currently using it.
> *   POSIX message queues include an asynchronous notification feature that allows processes to alerted when a message is available.
> *   POSIX message metadata includes an associated priority level. The first message retrieved is always the one with the highest priority level.
> *   POSIX message queues allow application designers to specify attributes (such as message size or capacity of the queue) via optional parameters passed when opening the queue.

[Figure 3.6.2](MQueues.html#ipcprio) illustrates a key point about POSIX message queues. In this example, assume that the messages A, B, C, D, and E have been inserted (in that order). POSIX message queues use non-negative values for the priority, with 0 being lowest. As such, messages C and D are at the front of the queue because they have the highest priority. Within a single priority level, though, the queue uses a first-in, first-out ordering.

![POSIX message queues are priority-based, with 0 as the lowest priority](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.3.5.png)

Figure 3.6.2: POSIX message queues are priority-based, with 0 as the lowest priority

///3.6.1. POSIX Message Queues[¶]
---------------------------------

There are six functions typically used for setting up and using POSIX [message queues](#term-message-queue).

📦 C library functions – `<mqueue.h>`

* * *

`mqd_t mq_open (const char *name, int oflag, ... /* mode_t mode, struct mq_attr *attr */);`

Open (and possibly create) a POSIX message queue.

`int mq_getattr(mqd_t mqdes, struct mq_attr *attr);`

Get the attributes associated with a given message queue.

`int mq_close (mqd_t mqdes);`

Close a message queue.

`int mq_unlink (const char *name);`

Initiate deletion of a message queue.

`int mq_send (mqd_t mqdes, const char *msg_ptr, size_t msg_len, unsigned int msg_prio);`

Send the message pointed to by msg_ptr with priority msg_prio.

`ssize_t mq_receive (mqd_t mqdes, char *msg_ptr, size_t msg_len, unsigned int *msg_prio);`

Receive a message into a buffer pointed to by msg_ptr and get its priority msg_prio.

As with other POSIX IPC open functions, `mq_open()` includes both an `oflag` and `mode` parameter. In addition, the fourth parameter (`attr`) is used to specify attributes about the message queue, such as the message size or the capacity of the queue. Note that calls to `mq_open()` must have exactly 2 or 4 parameters. Both the `mode` and `attr` parameters must be included when creating a new message queue. When opening a connection to an existing message queue, both of these parameters are omitted. To use the system defaults when setting up a message queue, pass `NULL` as the fourth parameter to `mq_open()`. [Code Listing 3.7](#cl3-7) creates a message queue for writing and sends a simple message.


```cpp
/* Code Listing 3.7:
   Sending "HELLO" through a POSIX message queue
 */

/* Create and open a message queue for writing */
mqd_t mqd = mq_open ("/OpenCSF_MQ", O_CREAT | O_EXCL | O_WRONLY,  0600, NULL);

/* Ensure the creation was successful */
if (mqd == -1)
  {
    perror ("mq_open");
    exit (1);
  }

/* Send "HELLO" as a message with priority 10, then close the queue.
   Note the size is 6 to include the null byte '\0'. */
mq_send (mqd, "HELLO", 6, 10);
mq_close (mqd);
```

[Code Listing 3.8](#cl3-8) illustrates a standard approach for retrieving a message from the queue. The `msg_len` parameter for receiving messages requires special attention. The use of this parameter for `mq_send()` is intuitive and matches the behavior of functions like `write()` and `strncpy()`: `msg_len` specifies the maximum number of characters in the string identified by `msg_ptr` that will be sent. However, when receiving messages, `msg_len` must match the size of a message. As such, the standard approach is to use `mq_getattr()` and access the `mq_msgsize` field of the `struct mq_attr` returned.

```cpp
/* Code Listing 3.8:
   Opening a POSIX message queue and retrieving a message
 */

/* Open the message queue for reading */
mqd_t mqd = mq_open ("/OpenCSF_MQ", O_RDONLY);
assert (mqd != -1);

/* Get the message queue attributes */
struct mq_attr attr;
assert (mq_getattr (mqd, &attr) != -1);

char *buffer = calloc (attr.mq_msgsize, 1);
assert (buffer != NULL);

/* Retrieve message from the queue and get its priority level */
unsigned int priority = 0;
if ((mq_receive (mqd, buffer, attr.mq_msgsize, &priority)) == -1)
  printf ("Failed to receive message\n");
else
  printf ("Received [priority %u]: '%s'\n", priority, buffer);

/* Clean up the allocated memory and message queue */
free (buffer);
buffer = NULL;
mq_close (mqd);
```

🐞🐛🐌 Bug Warning

* * *

In many instances, it is common practice to create a statically sized buffer as a local variable (such as `char buffer[100];`) and use the buffer size when reading data from some other source. However, the following line of code **will not work** with message queues:

int bytes_read = mq_receive (mqid, buffer, 100, &prio);

The reason for this is that the `msg_len` parameter does not exactly match the message queue’s message size (a common default is 8192, but it depends on the system). As such, the message queue will interpret this as a request to read less than a full message and will return nothing.

Unlike pipes, message queues can be used to send `struct` instances, even if some of the fields contain the value 0. (Pipes treat a byte value of 0 as the null byte and stop at that point.) Consider the following trivial `struct` declaration:


```cpp
struct message {
  int x;
  char y;
  long z;
};
```

As shown in [Code Listing 3.9](#cl3-9), sending the message with POSIX message queues works almost exactly as shown previously with a `char` array. The only differences are the casting of the `msg_ptr` pointer and that the `msg_len` parameter is based on the size of the `struct`. Similarly, the only difference with reading is the casting of the `msg_ptr` to the `struct` type.

```cpp
/* Code Listing 3.9:
   Retrieving a structured POSIX message
 */

struct message msg;
msg.x = 0;
msg.y = 'q';
msg.z = -1;

/* Sending a struct works identically to a char array */
mq_send (mqd, (const char *)&msg, sizeof (struct message), 10);

/* When reading, use a char* buffer and explicitly cast */
if ((mq_receive (mqd, buffer, attr.mq_msgsize, &prio)) != -1)
  {
    struct message *msg = (struct message *)buffer;
    /* Retrieve message fields here */
  }
```

The default behavior for POSIX message queues is to perform [blocking I/O](#term-blocking-i-o) when writing to a full queue (or reading from an empty one). If this behavior is undesirable, there are three alternatives that can be used. The first is to include the `O_NONBLOCK` option in the `oflag` bit mask to open the queue in non-blocking mode. If the queue is ever full, `mq_send()` will return an error without blocking. The other option is to use `mq_timedsend()` and `mq_timedreceive()`, which use an additional parameter (`abs_timeout`) to specify the maximum amount of time to wait when blocked. Finally, rather than attempting to retrieve a message that may not have been sent, a process can use `mq_notify()` to request an asynchronous notification that a message has been sent. Interested readers should consult the language documentation for more information on these functions.

📦 C library functions – `<mqueue.h>`

* * *

`int mq_timedsend (mqd_t mqdes, const char *msg_ptr, size_t msg_len, unsigned int msg_prio, const struct timespec *abs_timeout);`

Try to send a message, but specify a maximum time limit (abs_timeout) for blocking.

`ssize_t mq_timedreceive (mqd_t mqdes, char *msg_ptr, size_t msg_len, unsigned int *msg_prio, const struct timespec *abs_timeout);`

Try to receive a message, but specify a maximum time limit (abs_timeout) for blocking.

`int mq_notify (mqd_t mqdes, const struct sigevent *notification);`

Request asynchronous notification when a message is placed in the queue.



//3.7. Shared Memory[¶]
=======================

![A shared memory region present in two processes](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.3.6.png)

Figure 3.7.1: A shared memory region present in two processes

Setting up [shared memory](#term-shared-memory) with the techniques in this section is very similar to using [memory-mapped files](#term-memory-mapped-file), with the exception that there is no pre-defined persistent file. That is, shared memory can be set up to allow for immediate data exchange between processes without having a persistent record of the communication. [Figure 3.7.1](ShMem.html#ipcshmem) illustrates the logical structure of a shared memory region that is mapped into two different processes.

For applications that exchange large amounts of data, shared memory is far superior to message-passing techniques like message queues, which require system calls for every data exchange. The major disadvantage of shared memory is that the processes must take extra precaution to synchronize access to the region. If process A writes into the shared region, that might cause unstable behavior in process B, or vice versa.

🐞🐛🐌 Bug Warning

* * *

When a shared memory region is established in two or more processes, there is no guarantee that the regions will be placed at the same base address. For instance, one process might have the shared region starting at address `0x40000000` while the other process uses `0x50008000`. It is critical to understand that these two addresses refer to the exact same piece of data. So storing the number 1 in the first process’s address `0x40000000` means the second process has the value of 1 at `0x50008000`. The two (different) addresses refer to the exact same location.

Given that the base addresses are different, all elements in the region must also have different addresses. The implication of this is that shared memory regions cannot use pointers to refer to other parts of the region. For instance, assume the shared memory contains a linked list with the first node at the beginning of the region. The next node appears 256 bytes later. Using the addresses above, in one process, `0x40000000` must contain a pointer to address `0x40000100`; in the other address, the pointer must point to `0x50008100`, given the different base address. This means that the beginning of the shared memory region must simultaneously store two different values, which is impossible.

The solution is to avoid the use of pointers within the shared memory region, using pointer arithmetic instead. That is, if `char *baseaddr` is declared to point to the base address of the region, then the two processes would need to use `*(baseaddr + 256)` to refer to the next node of the linked list. Or, as an alternative, avoid linked lists entirely and use a contiguous data structure instead.

///3.7.1. POSIX Shared Memory[¶]
--------------------------------

The POSIX interface for shared memory is very simple, consisting of two primary functions. The `shm_open()` takes the POSIX IPC object name, a bit-mask of flags (`oflag`) and a permission mode to apply new objects. [[1]](#f18) Similarly, `shm_unlink()` deletes the shared memory object.

📦 C library functions – `<sys/mman.h>`

* * *

`int shm_open (const char *name, int oflag, mode_t mode);`

Open a connection to a POSIX shared memory object.

`int shm_unlink (const char *name);`

Delete a POSIX shared memory object.

The following code sample uses a `struct` declared as follows:


```cpp
struct permission {
  int user;
  int group;
  int other;
};
```

[Code Listing 3.10](#cl3-10) sets up a POSIX shared memory object, which is identified by a given name. Note that the call to `shm_open()` creates the object but does not specify a size; instead, `ftruncate()` resizes the object to be large enough to store one instance of the `struct` permission. Next, the process maps the shared memory object into memory with `mmap()` before using `fork()` to create a child process (which inherits both the shared memory identifier and the memory mapped region). The child writes to the region before exiting; the parent waits until the child exits and reads the data.

```cpp
/* Code Listing 3.10:
   Using POSIX shared memory to exchange data between processes
 */

/* Create unsized shared memory object;
   return value is a file descriptor */
int shmfd = shm_open ("/OpenCSF_SHM", O_CREAT | O_EXCL | O_RDWR, S_IRUSR | S_IWUSR);
assert (shmfd != -1);

/* Resize the region to store 1 struct instance */
assert (ftruncate (shmfd, sizeof (struct permission)) != -1);

/* Map the object into memory so file operations aren't needed */
struct permission *perm = mmap (NULL, sizeof (struct permission),
                                PROT_READ | PROT_WRITE, MAP_SHARED, shmfd, 0);
assert (perm != MAP_FAILED);

/* Create a child process and write to the mapped/shared region */
pid_t child_pid = fork();
if (child_pid == 0)
  {
    perm->user = 6;
    perm->group = 4;
    perm->other = 0;

    /* Unmap and close the child's shared memory access */
    munmap (perm, sizeof (struct permission));
    close (shmfd);
    return 0;
  }

/* Make the parent wait until the child has exited */
wait (NULL);

/* Read from the mapped/shared memory region */
printf ("Permission bit-mask: 0%d%d%d\n", perm->user, perm->group, perm->other);

/* Unmap, close, and delete the shared memory object */
munmap (perm, sizeof (struct permission));
close (shmfd);
shm_unlink ("/OpenCSF_SHM");
```

Combining `shm_open()` and `mmap()` in this way is a common technique that often confuses novices. Specifically, it seems redundant to use both IPC mechanisms, because the same task could be accomplished with either `shm_open()` or `mmap()` (not both). While it is true that both are not required, combining them leads to certain advantages:

> *   The `shm_open()` parameters adhere to the standard POSIX IPC conventions. This allows the processes to use POSIX names to identify the objects, rather than creating files in arbitrary locations.
> *   Using `shm_unlink()` provides a safety check that is not guaranteed with creating and mapping arbitrary files. Specifically, `shm_unlink()` will not delete the object immediately if any other process also has an open connection; this delay allows decreases the likelihood of other processes experiencing random and unexpected failures. In contrast, if an arbitrary non-IPC file was used for the `mmap()`, the timing of the deletion would be determined by the underlying file system; this introduces unnecessary maintenance and possibly restricts the portability of the program.
> *   Using `mmap()` allows the developer to cast the shared memory object to a more meaningful data structure type and to avoid using file operations. Specifically, observe that the return type from `shm_open()` is a file descriptor, which does not provide any information about the type of data stored in the object. By mapping it into memory, we can cast the pointer returned and access the `user`, `group`, and `other` fields contained in the `struct`.

[[1]](#id1)

As a minor point of terminology, all forms of POSIX IPC, including shared memory, are referred to as _objects_. In contrast, System V shared memory is called a “segment” for historical purposes.



//3.8. Semaphores[¶]
====================

[Semaphores](#term-semaphore) are different from the other forms of IPC discussed in this chapter, because they do not allow for the general exchange of data. That is, you cannot use a semaphore to transmit application-specific information from one process to another. Rather, semaphores are used to synchronize access to shared resources; that is, semaphores **control the timing of shared accesses that may conflict**.

As a preliminary example, consider a multi-process database application that handles banking information. Assume that one process is accessing the file for your account to record a deposit that you’ve made. While this is happening, another process also opens your account file to record a purchase. You want to make sure that the transaction for the deposit is completely recorded before money is removed from your account. That is, you want the application to control the timing of these events.

[Synchronization](#term-synchronization) is a complex topic that will be covered in its own chapter. In this section, we will start with the simplest case of synchronization with semaphores: [signaling](#term-signaling-synchronization) to another process that an action has been performed. This concept is very similar to the signals used to control the life cycle of a process (e.g., `SIGKILL`). The form of signaling that we are now discussing is more generic and allows applications to define their own custom events. That is, semaphores let processes inform other processes that something has happened, and that something is a custom event that only matters to that application. This type of signaling can be considered to follow the message passing IPC model, as each signal sent requires a system call.

Fundamentally, a semaphore is a non-negative integer [[1]](#f19) with two operations: incrementing or decrementing the value by 1. For a variety of reasons, these two operations have many different names that can be used interchangeably. Incrementing a semaphore is also called _upping_, _signaling_, _posting_, and _V_ (from the Dutch word verhogen, “to raise”). Decrementing a semaphore is also called _downing_, _waiting_, or _P_ (from the Dutch proberen, “to test”). [[2]](#f20)

The key behavior is that any process that attempting to decrement a semaphore whose current value is 0 will become blocked until another process increments the value. If the current value is greater than 0 when a process decrements the value, the process can continue processing without waiting. When using semaphores, a simple heuristic to consider for the initial value is to ask how many processes should be given immediate access. Specifically, the initial value determines how many decrements can occur (without any corresponding increments) before processes start to get blocked. If a semaphore is initialized to 1, a single process can decrement the semaphore without waiting; a second process would be blocked. Initializing the semaphore to 10 would allow 10 processes through. This topic will be discussed in more detail in Chapters 8 and 9.

To illustrate the basic signaling pattern described above, consider two processes (call them A and B) that have access to a shared memory region. Assume that A is writing data to the region and B needs to know when the writing is completed. (For simplicity, this scenario is a one-time data exchange. Once A is finished writing, it will not write again.) To make the signaling happen, A and B share access to a semaphore initialized to 0. A will increment the semaphore after it has completed writing the data and B will decrement the semaphore before it attempts to read. Observe the timings that are possible with this sequence of events:

> *   In one ordering, B attempts to decrement the semaphore before A has completed writing. Since the semaphore’s value is 0, B becomes blocked and must wait until A finishes. A eventually completes the write and increments the semaphore, unblocking B.
> *   In a different ordering, A finishes writing and increments the semaphore before B tries to read. When B later goes to read the data, the semaphore’s value will be 1 (because A already incremented the value) and it can continue without blocking.

In either order, the semaphore guarantees that B cannot read any data until A has completed writing to the shared memory.

🐞🐛🐌 Bug Warning

* * *

In the scenario above, it is important to initialize the semaphore to 0 to achieve the proper timing. A common mistake here would be to initialize the semaphore to 1, thinking that we only want one of the two processes to access the shared memory. This practice is known as [mutual exclusion](#term-mutual-exclusion), and will be discussed as an important synchronization pattern. The problem in this scenario is that mutual exclusion does not ensure the proper timing. Specifically, if the semaphore is initialized to 1 and B decrements the semaphore first, A will be blocked. Consequently, A cannot even begin writing to the shared memory and both processes will end up waiting indefinitely.

///3.8.1. POSIX vs. System V Semaphores[¶]
------------------------------------------

[Semaphores](#term-semaphore), like message queues and shared memory, have both a POSIX and System V interface specification. Key differences between the two versions of semaphores include the following:

> *   POSIX semaphores are created and initialized in a single operation, one at a time. System V semaphores are created as a set, and each semaphore can be initialized independently later.
> *   POSIX semaphores can only be incremented or decremented by 1. System V semaphore operations allow processes to specify any unsigned integer that will be added to the semaphore’s value immediately.
> *   POSIX semaphores provide non-blocking operations that try to decrement the value. If the value is currently 0 (which would block the process), the attempt will fail and the process can react in other ways. System V semaphores do not provide this feature.
> *   System V semaphores are identified by `key_t` values. POSIX semaphores can be identified by valid POSIX names (begin with a “`/`”) or they can be unnamed.

///3.8.2. POSIX Named Semaphores[¶]
-----------------------------------

POSIX defines two types of semaphores: named and unnamed. [Named semaphores](#term-named-semaphore) are created using the standard POSIX arguments of `name`, `oflag`, and `mode`, along with an initial unsigned integer `value`. The `mode` and `value` parameters must both be included when creating a new semaphore, and both must be excluded when connecting to an existing semaphore. The `sem_wait()` and `sem_post()` functions decrement (wait) or increment (post) the semaphore’s value. If the value is currently 0, `sem_wait()` will block the current process until the value is changed by another process calling `sem_post()`. Note that `sem_post()` will only unblock one process at a time; if five processes are waiting on the semaphore, then five calls to `sem_post()` must be made to unblock them all.

📦 C library functions – `<semaphore.h>`

* * *

`sem_t *sem_open (const char *name, int oflag, ...  /* mode_t mode, unsigned int value */ );`

Returns (and optionally creates) a named semaphore.

`int sem_wait (sem_t *sem);`

Decrement the semaphore’s value; block if the value is currently 0.

`int sem_post (sem_t *sem);`

Increment the semaphore’s value; resume another process if the value is 0.

`int sem_close (sem_t *sem);`

Close a semaphore.

`int sem_unlink (const char *name);`

Delete a named semaphore.

[Code Listing 3.11](#cl3-11) illustrates how semaphores can be used to control the timing of two processes. In this example, the semaphore is used to guarantee the messages are printed in the correct order (“first” then “second” then “third”). Specifically, the semaphore blocks the child process until the parent prints “first” and ups the semaphore; then the parent calls `wait()`, which forces it to wait until the child exits. It is important to emphasize that named semaphores can be used by unrelated processes; they do not have to be parent and child as used in this example.


```cpp
/* Code Listing 3.11:
   Creating and using a POSIX semaphore to control the timing of parent/child execution
 */

/* Create and open the semaphore */
sem_t *sem = sem_open ("/OpenCSF_Sema", O_CREAT | O_EXCL, S_IRUSR | S_IWUSR, 0);
assert (sem != SEM_FAILED);

/* Fork to create the child process */
pid_t child_pid = fork();
assert (child_pid != -1);

/* Note the child inherits a copy of the semaphore connection */

/* Child process: wait for semaphore, print "second", then exit */
if (child_pid == 0)
  {
    sem_wait (sem);
    printf ("second\n");
    sem_close (sem);
    return 0;
  }

/* Parent prints then posts to the semaphore and waits on child */
printf ("first\n");
sem_post (sem);
wait (NULL);

/* Now the child has printed and exited */
printf ("third\n");
sem_close (sem);
sem_unlink ("/OpenCSF_Sema");
```

In many circumstances, a process may wish to check on the status of a semaphore without getting blocked indefinitely. For instance, assume one process is using a semaphore to signal that it has completed some important task; another process may want to check on whether the task has been completed, but still continue processing even if the event has not happened yet. Downing the semaphore with `sem_wait()` would not work, because it would block the process until the event occurs. Instead, POSIX provides two alternative functions for waiting on a semaphore. With `sem_trywait()`, a process can try to down the semaphore; however, if the semaphore’s current value is 0, then `sem_trywait()` returns an error instead of blocking. Alternatively, `sem_timedwait()` allows the process to specify a maximum amount of time to block; if no post occurs before the timeout arrives, then the process is unblocked and an error is returned.

📦 C library functions – `<semaphore.h>`

* * *

`int sem_trywait (sem_t *sem);`

Try to decrement the semaphore; return an error if doing so would block

`int sem_timedwait (sem_t *sem, const struct timespec *abs_timeout);`

Decrement the semaphore, but place a time limit on the blocking

///3.8.3. POSIX Unnamed Semaphores[¶]
-------------------------------------

POSIX [unnamed semaphores](#term-unnamed-semaphore) provide a lightweight approach for creating and using semaphores. Specifically, where `sem_open()` returns a pointer to a newly allocated semaphore, `sem_init()` takes a reference to a semaphore variable (declared as a `sem_t`) that has already been allocated within the current process’s memory space and sets the semaphore to the value passed. Once the semaphore is initialized, the other functions specified for named semaphores (e.g., `sem_post()` and `sem_wait()`) can be used.

📦 C library functions – `<semaphore.h>`

* * *

`int sem_init (sem_t *sem, int pshared, unsigned int value);`

Create and initialize a POSIX unnamed semaphore.

`int sem_destroy (sem_t *sem);`

Delete a POSIX unnamed semaphore.

POSIX unnamed semaphores have one very important difference from named semaphores: **unnamed semaphores must exist in shared memory**. That is, the preceding code example would not work with unnamed semaphores, because the parent and child processes have distinct memory spaces. As such, the `sem_wait()` and `sem_post()` calls in that code are meaningless, because they are operating on two different semaphores. However, if a shared memory space were set up with `shmat()`, the semaphore could be stored within that region and the code would work.

🐞🐛🐌 Bug Warning

* * *

macOS provides POSIX named semaphores, but **it does not support unnamed semaphores**. macOS programs written with unnamed semaphores will (unfortunately) compile, but the program will not behave correctly. The `sem_wait()` function will return an error and will not block the process. As noted previously, applications targeting macOS should generally use the System V IPC interface rather than POSIX.

The shared memory requirement for POSIX unnamed semaphores limits their usefulness in terms of IPC. They are much more commonly used with [multithreaded programs](#term-multithreading), since multiple threads in the same process automatically share the same memory space. Using unnamed semaphores in that case reduces the kernel system overhead, as the semaphore is stored within the process itself.

One scenario where POSIX unnamed semaphores are very helpful for IPC arises when creating a complex shared data structure. For instance, consider a binary search tree consisting of millions of nodes that is stored in a shared memory region. If the application needs to associate a distinct semaphore with each node, assigning and keeping track of millions of names would be a horrifying burden. Instead, each node’s `struct` declaration could include a `sem_t` field. This approach would simplify the management of the data structure, allowing each semaphore to be created or destroyed automatically with the tree node.

[[1]](#id1)

This section is describing the typical implementation of semaphores in most modern OS. The original definition of semaphores allowed the values to be negative, too. In the original version, decrementing the semaphore would first subtract one from the current value, then check if the result was negative; if so, the process would block. In modern implementations, the value is checked first; if the current value is zero, the process blocks and the subtraction does not occur until later. The difference in behavior has very subtle implications that go beyond the scope of this book. For our purposes, either implementation will work sufficiently.

[[2]](#id2)

`P()` and `V()` were the original names for the semaphore operations. Semaphores were proposed by the Dutch computer scientist, Edsger Dijkstra. These names are never used in modern systems, but they are included here for historical reference. Our general preference is to use decrement/increment, given that this pair clearly indicate how the value changes. We also use wait/post, however, in relation to POSIX semaphores to match the functions (`sem_wait()` and `sem_post()`) as named in the interface.



//3.9. Extended Example: Bash-lite: A Simple Command-line Shell[¶]
==================================================================

This Extended Example creates a minimal shell similar to the bash shell used in Linux and macOS. When this program runs, it will read a line of text at a time from the user. This line will be used as a command line, running in a separate process. The user can enter `quit` or `logout` to exit.

```cpp
#include <assert.h>
#include <fcntl.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/stat.h>
#include <unistd.h>

/* Set the maximum allowable length for a command line */
#define MAX_LINE_LENGTH 1024

/* Allow spaces, tabs, newline, and carriage return to match
   whitespace between tokens */
#define WHITESPACE " \t\n\r"

void run_child_process (char *, char **, char *);
char **tokenize_arguments (char *, char **);
char **get_out_name (char *, char **, char **);

int
main (void)
{
  char buffer[MAX_LINE_LENGTH + 1];
  memset (buffer, 0, MAX_LINE_LENGTH + 1);

  /* Main loop: iterate until user enters "quit" or "logout" */
  while (true)
    {
      /* Display a minimal prompt and read the command line */
      printf ("$ ");
      if (! fgets (buffer, MAX_LINE_LENGTH, stdin))
        break;

      /* Get the command for this line without any arguments */
      char *command = strtok (buffer, WHITESPACE);

      /* Check for reserved keywords to quit the shell */
      if (! strncmp (command, "quit", 5) || ! strncmp (command, "logout", 7))
        break;

      /* Get the array of arguments and determine the output file
         to use (if the line ends with "> output" redirection). */
      char *output = NULL;
      char **arg_list = tokenize_arguments (buffer, &output);

      /* Security precaution: This is a simple shell that should
         not be used for running commands in root. */
      if (! strncmp (command, "sudo", 5) || ! strncmp (command, "su", 3))
        {
          printf ("*WARNING* This implementation does not "
                  "support super-user commands\n");
          continue;
        }

      /* If something went wrong, skip this line */
      if (arg_list == NULL)
        {
          perror ("-bash-lite: syntax error\n");
          continue;
        }

      /* Copy the command name as the first argument */
      arg_list[0] = command;

      /* Create the child process and execute the command in it */
      pid_t child_pid = fork ();
      assert (child_pid >= 0);
      if (child_pid == 0)
        run_child_process (command, arg_list, output);

      /* Parent waits for the child, then frees up allocated memory
         for the argument list and moves on to the next line */
      wait (NULL);
      free (arg_list);
      memset (buffer, 0, MAX_LINE_LENGTH + 1);
    }

  return EXIT_SUCCESS;
}

/* Runs a command in an already created child process. The command
   string should already be copied as the first argument in the
   list. If the user typed an output redirection ("> out" or
   ">out"), then output_file is the name of the file to create.
   Otherwise, output_file is NULL. Should never return. */
void
run_child_process (char *command, char **arg_list, char *output_file)
{
  int out_fd = -1;

  /* If there is a specified output file, open it and redirect
     STDOUT to write to this file */
  if (output_file != NULL)
    {
      out_fd = open (output_file, O_RDWR | O_CREAT);
      if (out_fd < 0)
        {
          fprintf (stderr, "-bash-lite: failed to open file %s\n", output_file);
          exit (EXIT_FAILURE);
        }

      /* Make the file readable and redirect STDOUT */
      fchmod (out_fd, 0644);
      dup2 (out_fd, STDOUT_FILENO);
    }

  /* Use execvp, because we are not doing a PATH lookup and the
     arguments are in a dynamically allocated array */
  execvp (command, arg_list);

  /* Should never reach here. Print an error message, free up
     resources, and exit. */
  fprintf (stderr, "-bash-lite: %s: command not found\n", command);
  free (arg_list);
  if (out_fd >= 0)
    close (out_fd);
  exit (EXIT_FAILURE);
}

/* Given a command line (buffer), create the list of arguments to
   use for the child process. If the command line ends in an output
   redirection, update the output_file pointer to point to the name
   of the file to use. */
char **
tokenize_arguments (char *buffer, char **output_file)
{
  assert (buffer != NULL);
  assert (output_file != NULL);
  char *token = NULL;

  /* Allocate an initial array for 10 arguments; this can grow
     later if needed */
  size_t arg_list_capacity = 10;
  char **arguments = calloc (arg_list_capacity, sizeof (char *));
  assert (arguments != NULL);

  /* Leave the first space blank for the command name */
  size_t arg_list_length = 1;

  while ( (token = strtok (NULL, WHITESPACE)) != NULL)
    {
      /* If token starts with >, it is an output redirection. The
         rest of the line must be the file name. Need to pass both
         the rest of the token and the buffer, as there might not
         be a space before the file name. */
      if (token[0] == '>')
        return get_out_name (&token[1], output_file, arguments);

      /* If current argument array is full, double its capacity */
      if ((arg_list_length + 1) == arg_list_capacity)
        {
          arg_list_capacity *= 2;
          arguments = realloc (arguments, arg_list_capacity * sizeof (char *));
          assert (arguments != NULL);
        }

      /* Add the token to the end of the argument list */
      arguments[arg_list_length++] = token;
    }

  return arguments;
}

/* Determine the output file from either the token that begins with
   the '>' character (but that character was removed) or from the
   next token on the command line. Note that all tokens after the
   output file name will be ignored. */
char **
get_out_name (char *token, char **output_file, char **arguments)
{
  /* If token is not an empty string, it contains the output file
     name */
  if (strlen (token) != 0)
    {
      *output_file = token;
      return arguments;
    }

  /* Token is empty, so there was a space after the '>' symbol.
     There should be one token left that is the file name. */
  token = strtok (NULL, WHITESPACE);
  if (token == NULL)
    {
      /* This is an error, no file name was passed */
      free (arguments);
      return NULL;
    }

  /* The last token is the file name, so return it and the argument 
     list */
  *output_file = token;
  return arguments;
}
```


/Chapter 4   Networked Concurrency[¶]
=====================================

//4.1. Networked Concurrency[¶]
===============================

![Timeline of major CSF topics with Sockets and Networking highlighted](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Timeline.4.png)

> “The web is more a social creation than a technical one. I designed it for a social effect - to help people work together - and not as a technical toy.”
> 
> Tim Berners-Lee

Once processes on a single machine were able to exchange data, computer scientists turned their attention to bridging the gap between machines. The next step in the evolution of concurrency involved adapting the file abstraction to a network interface. Joel M. Winett defined the socket in RFC 147 (1971) as unique identifiers for transmitting data to the ARPA network, creating an instrumental abstraction that continues to serve as the basis of networked communications programming.

🚀 Chapter Objectives

* * *

In this chapter, we will address the following instructional objectives:

*   We will define the fundamental building blocks of the TCP/IP Internet model, including its structure as a layered architecture.
*   We will describe the key components and architectural structures of network applications.
*   We will examine how the socket API is used to implement network applications.
*   We will implement of basic features of three protocols (HTTP, DNS, DHCP) that demonstrate key features of TCP and UDP socket programming.

IPC provides a communication infrastructure that can be used in two very different ways. Architects of complex, interrelated applications can distribute the workload across processes to benefit from fault isolation guarantees, while maintaining a centralized means to coordinate the work. One example of this approach is a web browser that uses distinct processes for each tab opened; a plug-in crash in one tab would not affect any others. On the other hand, developers and users of simple applications may leverage IPC to chain these modules together to perform a more complex calculation. Linking multiple command-line processes together with a pipe illustrates this approach.

Both of these approaches benefit from a high degree of reliability, as all of the services are running on the same machine. For instance, if one process sends data through a message queue or pipe to another process, that data is unlikely to be lost by the kernel. Similarly, if two processes with similar privileges are both accessing a file on disk, it is unlikely that one of them will be able to read the file while the other cannot. Certainly, in these scenarios, misconfigurations or software errors make it possible for the communication to fail, but the system infrastructure itself is unlikely to cause such failures.

We can extend the goals and basic principles of IPC to networked communication between processes on different machines, but this extension does not maintain the same levels of reliability as local communication. That is, concurrent applications that are designed to run on top of _networked computer systems_—our focus is on the [Internet](#term-internet) in particular—commonly experience intermittent faults that cannot be attributed simply to misconfigurations or software errors. The network infrastructure itself introduces an element of volatility that applications must take into consideration. Fortunately, the network infrastructure provides services that can help to create reliable communication, if the application truly requires it.

In this chapter, we will introduce the general structure of the networked communication infrastructure used in the Internet. Our focus here is on the programming interface—the [socket](#term-socket) API—that software developers can use to communicate with processes running on different machines. We will also explore details of three common [protocols](#term-protocol) that highlight key features of socket programming. In the next chapter, we will then focus on the network as a system itself.



//4.2. The TCP/IP Internet Model[¶]
===================================

Modern computing systems rely on the [Internet](#term-internet) for communication. The Internet is not a single computer network. Rather, the Internet is a network of interconnected networks that communicate using a common set of [protocols](#term-protocol), specifications that define the structure and other requirements for messages between computers. The messages originate with [hosts](#term-host)—computers and similar connected devices—at the [network edge](#term-network-edge). The messages are then passed (or [forwarded](#term-forwarding)) between [routers](#term-router), devices that are connected in a way that creates the structure of the network. The [network core](#term-network-core) that serves as the basis of the Internet is the backbone, which refers to the connected networks that are controlled and maintained by a few telecommunications companies, such as AT&T and CenturyLink. These backbone providers sell access to [Tier 1 Internet Service Providers](#term-tier-1-internet-service-provider) (ISPs), including Level 3, Cogent, and GTT. Note that there is some overlap in these companies, as backbone providers can also be Tier 1 ISPs; these companies sell access to the communication lines to each other based on [peering agreements](#term-peering-agreement).

![Home and enterprise networks connect through local ISPs and the Internet backbone](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.1.png)

Figure 4.2.1: Home and enterprise networks connect through local ISPs and the Internet backbone

[Figure 4.2.1](FiveLayer.html#netbackbone) illustrates the basic structure of how computer systems connect through the Internet. A family may create a home network by setting up a Wi-Fi router that serves as an [access point](#term-access-point) for laptops, smartphones, gaming systems, smart TVs, or other such consumer devices. This Wi-Fi router is connected to a local ISP using a subscription service, such as cable modem access. The local ISP has an internal network that connects to other ISPs, eventually leading to a Tier 1 ISP. Similarly, companies can set up their own enterprise [local area network](#term-local-area-network) (LAN), allowing their computers to exchange data directly. Just like the home user, the company running the enterprise LAN would need to have a subscription for service from a local ISP in order for their employees to have Internet access. This company may also be acting as a [content provider](#term-content-provider) that creates services such as email, cloud computing data storage, or web sites.

///4.2.1. Internet Model[¶]
---------------------------

![The five layers of Internet communcation](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.2.png)

Figure 4.2.2: The five layers of Internet communcation

Network communication adheres to a layered architecture, which is commonly described using the [Internet model](#term-internet-model), as illustrated in [Figure 4.2.2](FiveLayer.html#netfivelayer). The [application layer](#term-application-layer) consists of the logical endpoints of the communication within applications; for instance, a web browser operating at the application layer at one end may establish a connection to a web server running in the application layer at the other endpoint. The applications at the two hosts cannot directly communicate. Instead, the application opens a [socket](#term-socket), the software endpoint for the connection. Sockets can be considered a form of message passing IPC that is primarily used for network communication; at either end, the processes write to and read from the socket using system calls.

The sockets provide the process with an interface to the [transport layer](#term-transport-layer), which is typically implemented in the OS. The transport layer provides some key services that make the network communication more usable to the application layer. The transport layer breaks application layer messages—which may be large—into fixed-size data [segments](#term-segment) for delivery. The transport layer then provides a [multiplexing](#term-multiplexing-networking) service, allowing all processes on a single host to share a single network connection. At the receiving end, the transport layer performs [demultiplexing](#term-demultiplexing), directing the data to the appropriate process. Multiplexing and demultiplexing are achieved through the use of a [port number](#term-port-number), which is an integer associated with a particular process; at each host, only a single process can be assigned to a particular port number at a time. Furthermore, packets can arrive at their destination out of order, so part of the demultiplexing service is to reassemble the application-layer messages in the expected order. For instance, when accessing a web page, the browser may send the application-layer message starting with `"GET /index.html HTTP/1.1"`. The demultiplexing service would ensure the data is read by the web server’s socket in that order, rather than `"ex.html HTTP/1.1GET /ind"`. It is very unlikely that messages this small would be split across multiple packets, but it is common for larger messages.

As the amount of data sent by applications increases, networks can experience [congestion](#term-congestion). When this happens, packets can be lost or significantly delayed. To address this problem, some transport layer protocols, such as [Transmission Control Protocol](#term-transmission-control-protocol) (TCP), also provide [flow control](#term-flow-control) and [reliable transport](#term-reliable-transport) services. When the network is experiencing a significant amount of congestion, the flow control will slow down the rate that the host sends new packets into the network; with enough hosts doing this, the congestion will gradually drop and the performance will increase. When packets are lost in transit, reliable transport will detect the lost data and send a request for the data to be resent. TCP is also a [connection-oriented protocol](#term-connection-oriented-protocol), which means that the two hosts store persistent state information between messages. In contrast, [User Datagram Protocol](#term-user-datagram-protocol) (UDP) is a [connectionless protocol](#term-connectionless-protocol) that does not maintain state between messages; each message that arrives at a server is treated as if it is from a new, unique client. UDP is also an [unreliable transport](#term-unreliable-transport) protocol, meaning that it does not attempt to correct errors from lost data.

When the transport layer needs to send or receive packets, the transport layer protocol contacts the [Internet layer](#term-internet-layer). The Internet layer provides the logical structure of the connections between hosts. That is, while the transport layer provides [end-to-end communication](#term-end-to-end-communication) for processes on the two hosts, the Internet layer provides _point-to-point_ communication between the hosts and routers, both within and across network boundaries. The [Internet Protocol](#term-internet-protocol) (IP) operates at this layer, associating a logical identifier—known as an IP address—with a host or router connected to the network. Network-layer protocols define the [routing](#term-routing) path that packets will traverse through the Internet. That is, when a local ISP router receives a packet from a subscriber, the router examines the intended destination IP address and sends the packet to another router based on its best guess for the shortest path to get there.

The lowest layers of this model are the [link](#term-link-layer) and [physical](#term-physical-layer) layers, which are typically designed and implemented together in the hardware devices. The reason for this co-design is that the protocol depends on the underlying physical transmission technique. A desktop or server may be using the [Ethernet](#term-ethernet) suite of protocols to send pulses of light over a fiber optic cable. A laptop communicating with a Wi-Fi access point may be using the [IEEE 802.11](#term-ieee-802-11) family of protocols that encode bits on radio waves. A smartphone, on the other hand, may be using a _5G_ link that uses a different type of radio that requires a different encoding technique.

The link and Internet layers serve a similar purpose, which is to create links between computer systems. The key difference is the layer of abstraction at which they operate. Link layer protocols can only send packets to other devices within the same network, as they are tied closely to the physical connection; for instance, the purpose of Ethernet is to transmit bits between two devices that are physically connected by a cable. The Internet layer protocols operate at a higher level of abstraction, as two consecutive routers may not be physically connected. Rather, one router may be connected to a [network switch](#term-switch) [[1]](#f21), a specialized device that provides high-speed packet forwarding within a LAN. That switch may be connected to another, which is connected to a third, and so on until the other router is reached. These individual point-to-point connections between switches may involve different link protocols or versions, such as alternating between the 100BASE-T (100 Mbit/s transmission speed) and 10GBASE-T (10 Gbit/s) versions of Ethernet. Thus, what looks like a single connection or [hop](#term-hop) between routers in the Internet layer may require traversing several point-to-point links between switches in the link layer.

🔍 Note

* * *

The Internet model that we have described here is mostly based on the official model described in RFC 1122, which specifies requirements for Internet hosts. To be precise, RFC 1122 does not include the physical layer in the model, but we find it is useful to include for completeness. Other books and resources also deviate from RFC 1122 in a variety of ways, such as slightly different naming conventions (_network layer_ instead of [Internet layer](#term-internet-layer), _data link layer_ or _network access_ instead of [link layer](#term-link-layer)). These details are not generally a major concern, but readers should be aware that these variations in terminology exist.

///4.2.2. Packet Encapsulation and Nomenclature[¶]
--------------------------------------------------

As the network data is passed down the protocol stack, each layer’s data needs to be preserved until it is ready to be used by the receiver. This is accomplished by encapsulating each layer’s data as the payload for the next layer, attaching a metadata header that contains information used by the lower layer. For instance, the transport-layer header would indicate if this packet is being sent using TCP or UDP, a protocol that does not provide reliable delivery. The headers also may contain information about the size of the payload or a checksum of its contents; this information can be used to determine if the data has been transmitted correctly or if some or all of it has been lost or corrupted.

![Data from each layer is encapsulated](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.3.png)

Figure 4.2.4: Data from each layer is encapsulated

A side effect of creating layered architectures like the Internet model is that the final composition often exhibits the frustrating characteristic of redundant nomenclature. As each layer is defined to be independent of the others, it is common for the protocols at each layer to use different terminology for the same idea. [Figure 4.2.4](FiveLayer.html#netencaps) illustrates the layer encapsulation for the Internet model. Application-layer becomes the payload for a transport-layer [segment](#term-segment) with a header that contains information about the end-to-end delivery between processes. The network-layer [packet](#term-packet) encapsulates a segment with information needed to route the message between hosts. The link-layer [frame](#term-frame) adds a header that facilitates the physical transmission of the information within a LAN.

While different layers use different terms for the same concept, there often is a nuanced difference between the two. For instance, a transport-layer segment generally varies considerably in size, containing the full contents of a single application-layer payload. On the other hand, network-layer packets may take a single segment and break it into multiple [fragments](#term-fragmentation) that are sent independently. Similarly, the term [datagram](#term-datagram) is often used as a synonym for packet. The subtle difference is that [packet](#term-packet) is generally associated just with the Internet layer, whereas [datagram](#term-datagram) has a looser connotation; [datagram](#term-datagram) can sometimes be used at other layers to refer to the presence of unreliable data transfers. Finally, the terms _byte_ and [octet](#term-octet) typically refer to a collection of eight bits. The difference here is that some architectures use byte to denote the basic information unit size, which is allowed to vary; thus, a byte is not always guaranteed to be eight bits, whereas an octet must be. Additionally, in some cases, an octet is read right-to-left instead of left-to-right; that is, the decimal value 5 may be denoted by the octet 10100000 instead of 00000101. Oftentimes, these subtle differences in terminology do not have a significant impact on the discussion, but it is important to note they exist.

[[1]](#id1)

Note the different terminology for similar devices. The term [router](#term-router) is used to describe a device that operates at the Internet layer; a [switch](#term-switch) only operates as high as the link layer and does not provide the higher layers of the protocol stack.



//4.3. Network Applications and Protocols[¶]
============================================

While the protocol stack adheres to a layered architecture, network applications typically adhere to the [client/server](#term-client-server-architecture) or [peer-to-peer architectures](#term-peer-to-peer-architecture). In both cases, one or more processes on a host creates a _server socket_ that listens for incoming connections; at a later time, a process on a different host creates a socket to connect to the server socket. In traditional client/server architectures, this client process is contacting the server to request access to a resource, such as a web page or a user’s email messages; clients are typically untrusted and less privileged than servers, requiring some form of authentication to gain access. In a peer-to-peer application, such as file sharing or multimedia streaming, both processes are equally privileged and exchange data in both directions; however, at some previous point, one of the two processes established a server socket that the other connected to.

In many ways, network applications using the socket interface behave like the message passing IPC techniques from the previous chapter. Socket communication can be structured like message queues, or they can use byte streams like pipes or FIFOs. In fact, [UNIX domain sockets](#term-unix-domain-socket) can be used specifically for this local communication. In addition, a Linux [netlink socket](#term-netlink-socket) provides a mechanism to pass messages between the kernel and a user-mode process. However, if a process needs to communicate with a process on a different host, however, then sockets are required; other forms of IPC do not provide this support.

///4.3.1. Naming and Addressing[¶]
----------------------------------

Unlike local IPC, sockets require extra information to set them up. Specifically, when setting up a socket to connect to a remote host, a process must provide information about which host and process to contact. This information is normally provided to the client process by the user in the form of a [uniform resource identifier](#term-uniform-resource-identifier) (URI). URIs adhere to a common structure:

URI = scheme:[//authority]path[?query][#fragment]

The _scheme_ component defines the application-layer protocol that is being used; common examples of schemes would be using http or https for web pages, ftp for file transfers, or ssh to connect to a remote terminal on another host. The _path_ component consists of a sequence of data fields, typically organized to create a hierarchy of resources, joined together by a delimiter. For example, the path in a web page URI defines the location of the requested file in that host’s file system, relative to the web server’s home directory; if a web server is configured to use the directory `/etc/apache` as its home directory, then the path `images/icon.gif` would be used to request a copy of the file /etc/apache/images/icon.gif. Note that some applications use a different delimiter for a path, such as a colon (`':'`).

The bracket notation in the URI indicates that the other components are optional, though some application protocols may require them. If these components are present, they begin with a particular identifying character sequence. The _query_ and _fragment_ components begin with the `'?'` and `'#'` characters, respectively, and they can be used to customize the data requested. For instance, in a web page that is dynamically constructed by the server, the query might be used to provide user input. One example of this would be an e-commerce application that uses a URI such as `http://mystore.com/itemLookup?itemID=53` to direct the request to a single server-side file (`itemLookup`) that performs a database query for item 53, showing its picture, price, and other information.

The fragment component, on the other hand, is typically used to direct the client application to customize the view in some way. For instance, a web browser using the fragment field may jump to a section identified in the HTML source with an anchor tag or an ID attribute, rather than displaying the default top of the page. The _authority_ component begins with a double slash (`'//'`) and adheres to the following structure:

authority = [userinfo@]host[:port]

The `host` field is either a domain name or an IP address that is used to locate the host within the network. The `port` number is used to identify the process that should receive the application data from the packet. Many protocols have a default port number, such as using port 80 for a web server communicating via HTTP. Application clients, such as web browsers, fill in this information automatically. For instance, entering `http://www.foo.com/` and `http://www.foo.com:80/` would yield the same results; both URIs contain the same scheme (`http`), the same host (`www.foo.com`), the same path (`'/'`), and the same port (80, implied in the former). The `userinfo` field is used if a user identity needs to be associated with the request, as when using SSH to log into a user’s account.

![The structure of URIs for three applications](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.4.png)

Figure 4.3.1: The structure of URIs for three applications

[Figure 4.3.1](NetApps.html#neturi) illustrates the structure of URIs for three example applications. The first URI shown illustrates one that may be used to access a page from a web server running on the user’s own machine (as indicated by the `localhost` host name in the authority field). Port 8080 is commonly used as the default port number for local instances of the Apache Tomcat server; so this URI may be used to access a Java servlet, providing the username as an input value, then jumping down to the section identified by the sec1 HTML ID attribute. The second example shows a URI as it may be used in an email application. Note that the URI definition does not impose strict requirements on the structure of a path, so an email address is acceptable for that application. The last example shows a URI for accessing a newsgroup.

///4.3.2. Protocol Specifications and RFCs[¶]
---------------------------------------------

The protocols that govern communication in the Internet are defined and maintained by the [Internet Engineering Task Force](#term-internet-engineering-task-force) (IETF), an international standards organization; IETF is a part of a larger non-profit organization, the [Internet Society](#term-internet-society) (ISOC), that is responsible for the leadership and development of the Internet. The aim of IETF is to publish standards to make the Internet work better, relying on an open and transparent process with a significant amount of volunteer and community effort. The IETF defines these standards in documents known as [requests for comment](#term-request-for-comment) (RFCs). [[1]](#f22) All RFCs are available for free download at [https://tools.ietf.org/rfc/](https://tools.ietf.org/rfc/).

[Table 4.1](#tbl4-1) identifies the RFC for several well-known protocols and components of the Internet. To be thorough, as shown in the table, there is even a specific RFC that defines the process for creating an RFC. Many protocols, such as TCP and IP, have a single core RFC but depend on several others beyond the main document. Other protocols spread their core definition over several RFCs; for example, the secure shell protocol (SSH) is defined in RFCs 4250 – 4254, with several other RFCs defining supporting components. Note that there are many protocols and services, such as the secure copy protocol (SCP), that are not defined in any RFC.

RFC

Purpose

768

UDP – unreliable transport layer protocol

791

IPv4 – network layer protocol (version 4)

793

TCP – reliable transport layer protocol

959

FTP – file transfer protocol

1034,1035

DNS – domain name translation database

2026

Defines the RFC process

2131

DHCP – dynamic IP addressing

2616

HTTP/1.1 – serving web page

3986

Structure and interpretation of a URI

8200

IPv6 – network layer protocol (version 6)

Table 4.1: RFCs for some well-known Internet services

RFCs for protocols typically include multiple sections, such as definitions of key terms, illustrations of the structure and timing of messages, explanations status and error messages, and security analyses. Some RFCs are very short, as short as a few pages in PDF format; others can be more than 100 pages. One key feature that is common is the use of :_ackus-Naur Form (BNF)_, a formal method for defining information about a language or a set of messages. Reading BNF is a key skill for understanding the protocol specifications in RFCs. As an example, consider the following portion of the HTTP/1.1 specification in RFC 2616:

    HTTP-message    = Request | Response
    Request         = Request-line
                      *(( general-header
                       | request-header
                       | entity-header ) CRLF)
                      CRLF
                      [ message-body ]
    Request-Line    = Method SP Request-URI SP HTTP-Version CRLF

These lines illustrate several key features of BNF. The text to the left of an equal sign (`'='`) declare a unique type of _entity_ that serves as a basic language feature; in this case, these lines define the structure of the entities `HTTP-message`, `Request`, and `Request-Line`. The vertical bar (`'|'`) denotes a choice operation that can be satisfied by either entity; for instance, an `HTTP-message` can be either a `Request` or a `Response`. Consecutive lines indicate multiple entities that are required in a particular sequence; the line breaks in BNF do not have a specific meaning and only serve the purpose of making the definition more readable. The parentheses are used for grouping entities, the _Kleene star_ character (`'*'`) indicates that an entity may appear zero or more times, and the brackets indicate an optional entity that may appear once or may be omitted. The `CRLF` entity denotes the character sequence “carriage return-line feed” that consists of ASCII character 13 (`'\r'`) followed by ASCII character 10 (`'\n'`), and the `SP` entity denotes a single space (ASCII code 32, `' '`).

📜 Example 4.3.1

* * *

To combine these BNF rules in an example, an HTTP Request must begin with exactly one `Request-line` followed by some sequence of entities (possibly an empty sequence) that must be either a `general-header`, `request-header`, or `entity-header`; there can be multiple instances of each of these headers and they can appear in any order. After all of the headers, there must be a single blank line ending in `CRLF`, followed by an optional message body. The `Request-line` that starts the `Request` must consist of a `Method` (such as `GET` or `POST`), a `Request-URI` (which is different but related to the general URI previously discussed), and an `HTTP-Version`, separated by spaces and ending with a `CRLF`. As an example, the following lines of code is a valid `HTTP-message`:

    GET index.html HTTP/1.0
    Accept-Charset: iso-8859-5, Unicode-1-1
    Date: Tue, 19 Nov 2018 08:12:31 GMT
    Accept-Encoding: *

This example illustrates a `GET` request using HTTP version 1.0 to request a copy of the file designated by the `Request-URI` `index.html`. The `Accept-Charset` and `Accept-Encoding` lines are examples of the `request-header` entity types, while the `Date` is a `general-header`. Each line ends with a `CRLF`, including the last line, which has no other text.

[[1]](#id1)

The term [request for comment](#term-request-for-comment) may seem to suggest that the document is an early draft that will be revised later after feedback from the community. This is, to an extent, actually true. The protocols themselves are frequently revised or extended, so many RFCs _obsolete_ others as the structure and functioning of the Internet evolves. Consequently, the name [RFC](#term-rfc) is appropriate because they serve as requests for the public to consider the current status of the Internet, proposing improvements where needed.



//4.4. The Socket Interface[¶]
==============================

The [socket](#term-socket) interface in C provides a mechanism for setting up a communication channel to another host system. For both clients and servers, the initial function call is the same. Processes call `socket()` to request a new socket instance from the OS. As with other forms of IPC such as pipes, sockets are treated as files, so the process receives a file descriptor from the return value. If the socket creation failed, the OS returns a negative value.

📦 C library functions – `<sys/socket.h>`

* * *

`int socket (int domain, int type, int protocol);`

Create a socket instance.

The `domain` field is used to declare the intended scope of routing needed; different values here indicate whether the socket will be used for IPv4, IPv6, or local communication. The `type` field determines whether the socket will read and write data as a byte stream, fixed-size messages, or as unprocessed (raw) data. The `protocol` field is typically unused and set to 0; one exception occurs when the client is acting as a [packet sniffer](#term-packet-sniffer), an application for capturing and examining packets sent by other processes on a host or network. These applications use [raw sockets](#term-raw-socket), as described below. For all three fields, the socket header file defines constant values that are used. [Table 4.2](#tbl4-2) identifies several common constants.

Field

Constant

Purpose

`domain`

`AF_INET`

Use IPv4 addresses

`AF_INET6`

Use IPv6 addresses

`AF_LOCAL`

Unix domain socket for IPC

`AF_NETLINK`

Netlink socket for kernel messages

`AF_PACKET`

Raw socket type

`type`

`SOCK_STREAM`

Byte-stream communication, used for TCP transport

`SOCK_DGRAM`

Fixed-size messages, used for UDP transport

`SOCK_RAW`

Raw data that is not processed by transport layer

`protocol`

`IPPROTO_RAW`

Receive IP datagrams without transport-layer processing

`ETH_P_ALL`

Receive Ethernet frames without network-layer processing

Table 4.2: Common arguments to the `socket()` function

The `domain` and `type` field constants are defined in the `sys/socket.h` header file. The `domain` fields listed here have another form that replaces the `AF` with `PF`. For example, there are also `PF_INET` and `PF_PACKET` constants. The original use of this different notation was to distinguish an _address family_ (`AF`) from a _protocol family_ (`PF`). In practice, these values tend to be identical, with the `AF` form more commonly used. For the `protocol` field, the `IPPROTO_RAW` and similar `IPPROTO_*` constants are defined in `netinet/in.h`. The `ETH_P_ALL` and similar constants are stored in `linux/if_ether.h` on Linux systems; other systems do not have these specific values.

Only certain combinations of these arguments make sense. For instance, a raw socket created with the domain `AF_PACKET` would use the `SOCK_RAW` type; a raw socket would not be set up to use the `SOCK_STREAM` type, which is commonly used for the stream-oriented TCP transport-layer protocol. Similarly, a socket created for IPv6 communication (`AF_INET6`) would not use the `SOCK_RAW` type, which would deliver the IP datagram payload directly to the process without the transport-layer processing; the process would then receive the full payload, including the TCP or UDP header fields. [Code Listing 4.1](#cl4-1) illustrates several common combinations of socket arguments.

```cpp
/* Code Listing 4.1:
   Common argument types for various sockets
 */

/* Create an IPv4 socket for TCP */
socketfd = socket (AF_INET, SOCK_STREAM, 0);

/* Create an IPv6 socket for TCP */
socketfd = socket (AF_INET6, SOCK_STREAM, 0);

/* Create an IPv4 socket for UDP */
socketfd = socket (AF_INET, SOCK_DGRAM, 0);

/* Create an IPv6 socket for UDP */
socketfd = socket (AF_INET6, SOCK_DGRAM, 0);

/* Create a raw socket for sniffing unprocessed Ethernet frames */
socketfd = socket (AF_PACKET, SOCK_RAW, htons (ETH_P_ALL));

/* htons is explained below */
```

Processes running on two separate hosts normally intend to exchange application-layer messages, relying on the lower layers just as a mechanism to deliver the data. (Refer back to [Figure 4.2.1](FiveLayer.html#netbackbone).) However, some processes are designed to gather information about the network itself. One example of such a process would be a packet sniffer that captures and inspects data sent through the network. Another example is the `traceroute` utility, which can gather information about the routers that are encountered on the way to a target host. Both of these applications can be used to monitor the health and performance of a network, and they also form the basis of security tools, such as a _network intrusion detection systems (network IDS)_.

Applications that gather information about the network rely on the use of raw sockets, which are sockets intended to break the layers of abstraction of the protocol stack. Using a raw socket, a process can examine a full IP datagram or TCP packet, including their respective headers. For instance, a raw socket with the protocol `IPPROTO_RAW` could be used to examine the TCP or UDP headers for a packet This reveals information about the processes that sent or are intended as the receiver of the data; however, this packet has still been processed by the Internet layer, so the IP headers would have been removed. To examine the IP headers, the `ETH_P_ALL` protocol field is needed.

Normal applications should not use raw sockets, because most of the information can be accessed in other ways. That is, when a client connects to a server, the client would already learn that host’s IP address and the port number associated with the server. Furthermore, using a raw socket would require the application programmer to understand details of the transport and network protocols to know how much data at the beginning of the message to ignore. Unless the application is specifically designed to gather information about the network itself, raw sockets are not needed in typical use.

///4.4.1. Networking Data Structures[¶]
---------------------------------------

Once the socket is created, the client and server processes use different functions to establish the network link between their sockets. These functions rely on a common `struct sockaddr` data structure. The basic form of this structure contains two fields:

```cpp
/* defined in sys/socket.h */
struct sockaddr {  /* generic socket address structure */
  sa_family_t sa_family;
  char sa_data[14];
};
```

The first field, `sa_family`, is two-bytes in size and identifies the domain of the socket that is being used. This field is set using `AF_INET`, `AF_PACKET`, and similar constants used to create the socket. The other field, `sa_data`, is an unstructured 14-byte sequence in the generic form. The interpretation of these bytes depends on the domain of the socket. For an IPv4 socket, the process would create a `struct sockaddr_in` [[1]](#f23) instead, which uses the following definition.


```cpp
/* defined in netinet/in.h */
struct sockaddr_in {
  sa_family_t sin_family;
  in_port_t sin_port;
  struct in_addr sin_addr;
  char sin_zero[8];
};

struct in_addr {
  in_addr_t s_addr; /* in_addr_t is a typedef alias for uint32_t */
};
```

The `sockaddr` and `sockaddr_in` structures are identical in size, allowing for straightforward casting between the two. Both begin with a `sa_family_t` field to indicate the domain. As the type is the same, both `struct`s use the same number of bytes for this information. The `sockaddr_in` breaks the rest of the bytes into three fields. The `sin_port` is a 16-bit field to designate the port number for the socket and the `sin_addr` contains the 32-bit IPv4 address. The `struct in_addr` contains a single field, `s_addr`, which is made an alias for a `uint32_t` by a `typedef` elsewhere in the C library. The `sin_zero` field of `struct sockaddr_in` is used to pad the size of the `struct` to match the size of the original `sockaddr`. [[2]](#f24)

📜 Example 4.4.1

* * *

To illustrate the internal structure of these address `struct`s, consider the following byte sequence:

`struct sockaddr`

`sa_family`

`sa_data`

`02`

`00`

`00`

`50`

`5d`

`b8`

`d8`

`22`

`00`

`00`

`00`

`00`

`00`

`00`

`00`

`00`

`sin_family`

`sin_port`

`sin_addr`

`sin_zero`

`struct sockaddr_in`

In both cases, the first two bytes denote the family, and both types of `struct`s interpret these bytes the same way. The value stored here is `AF_INET` (the constant 2), which indicates an IPv4 address. Code the checks this value as either type of struct would get the same answer for the family. If the address is interpreted as `struct sockaddr_in`, the three remaining fields (`sin_port`, `sin_addr`, and `sin_zero`) occupy 14 bytes, the same amount of space as the `sa_data` field in the `struct sockaddr` interpretation. In this particular example, the `sin_port` field contains the value 80 (`0x50`), whereas the `sin_addr` field contains the address 93.184.216.34 (`0x5db8d822`). Astute readers may notice there is a discrepancy in how the `sin_family` and `sin_port` fields are interpreted. This discrepancy is caused by the concept of _endianness_, which we will explain below.

IPv6 socket address `struct`s are similar in some respects. The `struct` is renamed `sockaddr_in6` and the fields are renamed to `sin6_family`, `sin6_port`, and `sin6_addr`. Two additional fields (`sin6_flowinfo` and `sin6_scope_id`) are also defined for behavior that exists in IPv6 but not in IPv4; these fields are used for specialized purposes that are beyond the scope of this book.

```cpp
/* included by netinet/in.h */
struct sockaddr_in6 {
  sa_fmily_t sin6_family;
  in_port_t sin6_port;
  uint32_t sin6_flowinfo;
  struct in6_addr sin6_addr;  /* IPv6 addresses are 128-bit */
  uint32_t sin6_scope_id;
};
```

Despite their similar naming and ordering of fields, IPv6 socket address structs are considerably larger in size. To be precise, consider the type of `sin_addr` compared with `sin6_addr`. For IPv4, the `struct in_addr` type is an alias for `uint32_t`—an unsigned 32-bit (4-byte) integer. For IPv6, the `struct in6_addr` contains a `union` of three different types. For readers unfamiliar with C `union`s, the types are not distinct fields. Rather, the `__u6_addr` field of the `struct` contains 16 bytes, but it can be interpreted in multiple ways. As such, the `sin6_addr` field alone is the size of the entire `sockaddr` or `sockaddr_in` `struct`s.

```cpp
/* included by netinet/in.h */
struct in6_addr {
  union {
    uint8_t  __u6_addr8[16];  /* aliased as s6_addr */
    uint16_t __u6_addr16[8];  /* aliased as s6_addr16 */
    uint32_t __u6_addr32[4];  /* aliased as s6_addr32 */
  } __u6_addr;
};
```

The difference in the `sin6_addr` field size is not a problem in practice. As the first field (`sin6_family`) remains a constant size of 16 bits, this field can be used to determine which type of `struct` is being used. The address field can then be accessed by explicitly casting to `struct sockaddr`, `struct sockaddr_in`, or `struct sockaddr_in6` as needed. In the code, the 16 bytes of the `sin6_addr` field can be viewed as an array of 16 `uint8_t` values, an array of eight `uint16_t` values, or an array of four `uint32_t` values; all three views refer to the same sequence of bytes, and they all require the same size. Given that the syntax of unions can be awkward, these fields are aliased as `s6_addr`, `s6_addr16`, and `s6_addr32`. [Code Listing 4.2](#cl4-2) shows how the alias makes the syntax cleaner to use.


```cpp
/* Code Listing 4.2:
   Accessing part of a union without and with an alias for the field
 */

struct in6_addr addr;
addr.__u6_addr.__u6_addr[0] = 5; /* set first byte to 5 */
addr.s6_addr[0] = 5; /* same thing, but using the alias */
```

📜 Example 4.4.2

* * *

The following sequence of bytes illustrates an example of an IPv6 address `struct`. The first observation to make is that this struct is 28 bytes in length, rather than the 16 bytes for IPv4 of the generic `struct sockaddr`. Due to this additional length, the bytes must be wrapped onto a second line to make the table readable.

`struct sockaddr_in6`

`sin6_family`

`sin6_port`

`sin6_flowinfo`

`sin6_addr`

`0a`

`00`

`00`

`50`

`00`

`00`

`00`

`00`

`26`

`06`

`28`

`00`

`02`

`20`

`00`

`01`

`sin6_addr` (continued)

`sin6_scope_id`

 

`02`

`48`

`18`

`93`

`25`

`c8`

`19`

`46`

`00`

`00`

`00`

`00`

 

As with `sockaddr` and `sockaddr_in`, the first two bytes can be read independently of the rest to determine the `sin6_family` field. The value 10 (`0x0a`) is the constant `AF_INET6`, which indicates code working with a pointer to this `struct` should case it as an IPv6 address. This example uses the same port (80) as [Example 4.4.1](Sockets.html#sockexampleipv4). The IP address shown in the `sin6_addr` field here would be `2606:2800:220:1:248:1893:25C8:1946`. Note that both of these addresses correspond to the same (as of this writing) URL: `www.example.com`. This server can be reached using either IPv4 or IPv6.

To illustrate the purpose of the `struct in6_addr union`, consider the following version of the IPv6 address from above. As before, we have written the address to wrap onto a second line for the purposes of the table structure.

`s6_addr32[0]`

`s6_addr32[1]`

`s6_addr16[0]`

`s6_addr16[1]`

`s6_addr16[2]`

`s6_addr16[3]`

`26`

`06`

`28`

`00`

`02`

`20`

`00`

`01`

`s6_addr32[2]`

`s6_addr32[3]`

`s6_addr16[4]`

`s6_addr16[5]`

`s6_addr16[6]`

`s6_addr16[7]`

`02`

`48`

`18`

`93`

`25`

`c8`

`19`

`46`

Each byte within the address can be accessed using the `s6_addr` array. If the address is stored in the variable addr as declared in [Code Listing 4.2](#cl4-2), `addr.s6_addr[0] = 0x26`, `addr.s6_addr[1] = 0x06`, and so on. However, the other types in this `union` allow the bytes to be grouped together as needed by the code. The groupings are shown by the labels for the `s6_addr16` and `s6_addr32` arrays above. That is, using the same variable declaration from [Code Listing 4.2](#cl4-2), `addr.s6_addr16[0] = 0x2606` and `addr.s6_addr16[1] = 0x2800`, whereas `addr.s6_addr32[0] = 0x26062800`. The `union` type allows the programmer to use whichever interpretation is convenient.

The internal representations of `in_addr` and `in6_addr` are different from the standard notation used for IP addresses. For instance, readers may be familiar with the _dotted decimal_ notation of IPv4 addresses, as illustrated by the [loopback address](#term-loopback-address) 127.0.0.1 that refers to the local host machine. This format is used for human readability, but the actual IP address is stored as a 32-bit value, with each byte corresponding to one of the dotted fields. In the case of the loopback, the IP address is `0x7f000001` (recall that `0x7f` is the same as the decimal number 127). Similarly, IPv6 addresses are typically formatted as a colon-delimited list of groups of four hexadecimal digits, such as `1122:3344:5566:7788:99aa:bbcc:ddee:ff00`. Leading zeros can be compressed, and consecutive sections of zeros can be replaced with a double colon. As such, the IPv6 loopback address can be written as `0:0:0:0:0:0:0:1`, or simply `::1`. However, the internal representation is the full 16-byte sequence of values.

In both IPv4 and IPv6 the sin_port and sin6_port fields require special handling, due to the issue of _endianness_. Recall that multi-byte numbers, such as a 16-bit unsigned integer, can be stored according to either _big endian_ or _little endian_ format depending on the CPU architecture. In a big endian architecture, the most significant byte (i.e., the _big end_ of the number) is placed at the lowest memory address; this relationship can be captured with the mnemonic, “big end at the bottom.” In contrast, a little endian architecture would place the least significant byte at the lowest address; here, the mnemonic is “little end at the lowest.” As an illustration, consider [Code Listing 4.3](#cl4-3). By casting the address of the `uint16_t` variable—which would actually be the address of its byte at the lowest numerical address—as a `uint8_t` pointer, the two bytes can be accessed individually.


```cpp
/* Code Listing 4.3:
   Casting and pointer arithmetic can illustrate endianness
 */

struct sockaddr_in address;
address.sin_port = htons (4096); /* see below */
/* Cast the address to access each byte individually */
uint8_t *as_array = (uint8_t *) &address.sin_port;
printf ("%p stores %" PRI8x "\n", &as_array[0], as_array[0]);
printf ("%p stores %" PRI8x "\n", &as_array[1], as_array[1]);
```

If we assume the `address.sin_port` field resides at address `0xbfff1234` (thus also occupying `0xbfff1235`), the variable layout in memory would vary based on the CPU architecture:

Big Endian

Address

Value

`bfff1235`

`00`

`bfff1234`

`10`

Little Endian

Address

Value

`bfff1235`

`10`

`bfff1234`

`00`

Table 4.3: Placement of bytes in memory depends on CPU endianness

While most modern CPU architectures use a little endian format, network protocols use big endian. Furthermore, since the sender and receiver hosts may have different CPU architectures, they may also differ in their endianness. To overcome this problem, multi-byte socket address fields that will be sent across the network require attention to endianness. The simplest way to achieve this is to use the C functions `htons()`, `htonl()`, `ntohs()`, and `ntohl()`. When the process is setting up the socket or sending data, use the `hton` (_host to network_) versions; as an example, refer back to the last line of [Code Listing 4.1](#cl4-1). At the other end, when a process reads data from the network, it will use the `ntoh` (_network to host_) version. Assuming both hosts follow this convention, neither host will require any advance knowledge of the other host’s CPU architecture endianness.

📦 C library functions – `<arpa/inet.h>`

* * *

`uint32_t htonl (uint32_t hostlong);`

Convert a 32-bit unsigned integer from host endian format to network endian format.

`uint16_t htons (uint16_t hostshort);`

Convert a 16-bit unsigned integer from host endian format to network endian format.

`uint32_t ntohl (uint32_t netlong);`

Convert a 32-bit unsigned integer from network endian format to host endian format.

`uint16_t ntohs (uint16_t netshort);`

Convert a 16-bit unsigned integer from network endian format to host endian format.

🐞🐛🐌 Bug Warning

* * *

Within the `sockaddr` and `sockaddr_in` structures, only fields that are intended to be sent across the network need to be formatted with the _host to network_ functions. Other fields must not be formatted in this way, as they are only used by the local host machine; flipping the byte order of these values would produce incorrect results. In practice, only the port field needs to be formatted with `htons()`; IP addresses are generally not hard-coded and helper functions format these values transparently.

///4.4.2. Client Socket Interface[¶]
------------------------------------

Once a client process has created a socket, the next step is to build the socket address structure and to establish a connection with the socket at the server host. The client is primarily concerned with specifying the IP address of the server and the associated port number. In the case of a connection-less protocol like UDP, this step does not actually involve contacting the server; rather, this step just involves configuring the socket’s peer IP address.

While there are standard port numbers for many applications, IP addresses should not be hard-coded, as they can change. Instead, `getaddrinfo()` provides an interface to look up an IP address by the standard text format used in URIs. This string is passed as the first argument, `nodename`. The `servname` parameter indicates a desired service, such as `"http"`. The `hints` parameter can be used to limit the list of results, such as restricting the domain to `AF_INET` (IPv4) or `AF_INET6` (IPv6), or limiting the type to `SOCK_STREAM` or `SOCK_DGRAM`. The final parameter, `res`, is a call-by-reference parameter that will be set to point to a linked list of address structures.

📦 C library functions – `<netdb.h>`

* * *

`int getaddrinfo (const char *nodename, const char *servname, const struct addrinfo *hints, struct addrinfo **res);`

Translate a human-readable hostname into an IP address, typically with the help of DNS.

`void freeaddrinfo (struct addrinfo *ai);`

Free all address information structures in the linked list beginning at ai.

Both the `hints` and `res` parameters use the `struct addrinfo` structure, defined in `netdb.h`. The `hints` argument initializes all fields to zero, setting the int fields as desired. For example, setting `hints.ai_family` to `AF_INET` would get results only for IPv4; to get all families, the value can be left as zero or explicitly set as `AF_UNSPEC`. Similarly, setting hints.ai_socktype to `SOCK_STREAM` would yield only byte stream sockets (e.g., those used in TCP). In the results list, the `ai_addr` field would point to a `struct sockaddr` as defined previously. Note that `getaddrinfo()` puts all values into the struct in the correct endianness needed for later functions, so no additional conversion is needed. Since `getaddrinfo()` dynamically allocates the results list, the `res` field should be passed to `freeaddrinfo()` to free the memory.


```cpp
/* defined in netdb.h */
struct addrinfo {
  int ai_flags;
  int ai_family;
  int ai_socktype;
  int ai_protocol;
  socklen_t ai_addrlen;
  char *ai_canonname;
  struct sockaddr *ai_addr;
  struct addrinfo *ai_next;
};
```

When building applications to connect to a server, the `ai_addr` field can be passed without casting. However, it is often beneficial to print the address in a readable format, such as when writing to a log file. The `inet_ntoa()` and `inet_ntop()` functions perform this formatting. Of these two, the `inet_ntoa()` function is older and only supports IPv4. This function takes a `struct in_addr` parameter, which would be the `sin_addr` field of a `struct sockaddr`. The return value is a pointer to a statically allocated location containing the string. As such, the pointer does not (and cannot) need to be freed later. On the other hand, `inet_ntop()` works with both IPv4 and IPv6, relying on the `af` parameter to distinguish between the two. The `src` argument points to the `sockaddr` to translate; note that this parameter uses a `void *` type, and the function will cast it based on the `af` argument. The `dst` argument is a pointer to a buffer to write the string into, with size indicating the length of the buffer. If the address can be translated properly, the function returns a pointer to the string, which should match the address of the buffer.

📦 C library functions – `<arpa/inet.h>`

* * *

`char *inet_ntoa (struct in_addr in);`

Convert an IPv4 address into a string using dotted decimal notation.

`const char *inet_ntop (int af, const void *src, char *dst, socklen_t size);`

Convert an IP address (either IPv4 or IPv6) into a string format.

[Code Listing 4.4](#cl4-4) illustrates how these functions can be used along with `getaddrinfo()` to translate a hostname (assumed to be declared with a value such as `"www.example.com"`) into an IPv6 readable address format. The `getaddrinfo()` internal implementation typically relies on the [Domain Name System](#term-domain-name-system) (DNS) protocol to look up the IP address. In this initial version, each of the results in the server list are confirmed to be an IPv6 address. The `ai_addr` field is then cast to the appropriate socket address `struct` and its `sin6_addr` field is passed to the `inet_ntop()` function for formatting. This implementation could be modified to print IPv4 addresses correctly by changing only the constants and field names as described previously; the buffer size would need to use the constant `INET_ADDRSTRLEN`, as well.


```cpp
/* Code Listing 4.4:
   Utility program that prints IPv6 addresses as a readable string
 */

struct addrinfo hints, *server_list = NULL, *server = NULL;
memset (&hints, 0, sizeof (hints));
hints.ai_family = AF_INET6;       /* change to AF_INET for IPv4 */
hints.ai_socktype = SOCK_STREAM;  /* limit to byte-streams */
hints.ai_protocol = IPPROTO_TCP;  /* create as a TCP socket */

/* Get a list of addresses at hostname that serve HTTP */
getaddrinfo (hostname, "http", &hints, &server_list);

/* Traverse through the linked list of results */
for (server = server_list; server != NULL; server = server->ai_next)
  {
    if (server->ai_family == AF_INET6)
      {
        /* Cast ai_addr to an IPv6 socket address */
        struct sockaddr_in6 *addr = (struct sockaddr_in6 *)server->ai_addr;

        /* Allocate a buffer to store the IPv6 string */
        char in6addr[INET6_ADDRSTRLEN];
        assert (inet_ntop (AF_INET6, &addr->sin6_addr, in6addr, sizeof (in6addr))
                != NULL);
        printf ("IPv6 address: %s\n\n", in6addr);
      }
  }
freeaddrinfo (server_list); /* Free allocated linked list data */
```

[Code Listing 4.5](#cl4-5) shows how the if-block from [Code Listing 4.4](#cl4-4) could use `inet_ntoa()` instead of `inet_ntop()`. For IPv4, the two functions produce identical output, so either one can be used.

```cpp
/* Code Listing 4.5:
   Additional code for printing out an IPv4 in dotted decimal format
 */

if (server->ai_family == AF_INET)
  {
    /* Cast ai_addr to an IPv4 socket address */
    struct sockaddr_in *addr = (struct sockaddr_in *)server->ai_addr;
    printf ("IPv4 address: %s\n", inet_ntoa (addr->sin_addr));
  }
```

[Code Listing 4.3](#cl4-3) showed how the port field could be explicitly set with a variable assignment. While it may not be apparent, [Code Listing 4.4](#cl4-4) illustrates a second, preferred way to handle this assignment. Specifically, `getaddrinfo()` puts the port number in socket address based on standard, [well-known ports](#term-well-known-port). In this case, the parameter `"http"` maps to port number 80. [Table 4.4](#tbl4-4) lists the port numbers for some common applications. Note that some services, such as FTP and Telnet, are discouraged because they offer no security guarantees; Telnet, for instance, allows anyone with a packet sniffer to discover a user’s password when logging in to a remote server. Instead, the secure approach is to run these protocols on top of SSH.

Port

Name

Service

21

FTP

Insecure file transfer

22

SSH

Secure shell

23

Telnet

Insecure remote access

25

SMTP

Email delivery

53

DNS

IP address lookup

67

DHCP

IP address assignment

68

DHCP

IP address assignment

80

HTTP

Web page

88

Kerberos

Authentication

Port

Name

Service

110

POP3

POP email access

123

NTP

Time synchronization

143

IMAP

IMAP email access

194

IRC

Internet chat service

389

LDAP

Authentication

443

HTTPS

Secure web page

530

RPC

Remote procedure call

631

IPP

Internet printing

993

IMAPS

Secure IMAP access

Table 4.4: List of common well-known ports

📜 Example 4.4.3

* * *

The relationship between the `struct`s can get confusing in code, due to the casting and pointer dereferences involved. To illustrate their connections, assume that `getaddrinfo()` returns a list with two nodes containing the addresses in [Example 4.4.1](Sockets.html#sockexampleipv4) and [Example 4.4.2](Sockets.html#sockstruct). This list could be visualized as follows:

![Visualization of a linked list of two struct addrinfo nodes](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.EX.png)

The key observertion is that the IP addresses (in the `struct sockaddr_in` and `struct sockaddr_in6`) are stored separately from the `struct addrinfo` nodes. Both nodes indicate the address is for a TCP server (indicated by `ai_socktype = SOCK_STREAM` and `ai_protocol = IPPROTO_TCP`). We can determine the type of address by checking `ai_family`, which is set to 2 (`AF_INET`) for IPv4 or 10 (`AF_INET6`) for IPv6. We could also determine this information indirectly by noting the `ai_addrlen` field that indicates the address length (4 for IPv4, 16 for IPv6).

In [Code Listing 4.5](#cl4-5), the `server` variable would point to the first `struct addrinfo`. Then the `addr` variable would point to the `struct sockaddr_in` shown just below it, allowing the call to `inet_ntoa (addr->sin_addr)` on the address. [Code Listing 4.6](#cl4-6) uses the next `struct addrinfo` containing the pointer to the IPv6 address. This pointer can then be passed to `inet_ntop()` to print the address.

[Example 4.4.1](Sockets.html#sockexampleipv4) and [Example 4.4.2](Sockets.html#sockstruct) detailed the byte contents of the `struct sockaddr_in` and `struct sockaddr_in6`, respectively. We can examime the first `struct addrinfo` as shown in the following table. In this case, we assume that the `struct sockaddr_in` instance is at memory location `0x5581f7459560` (as indicated by the `ai_addr` pointer), while the other `struct addrinfo` is stored at `0x5581f7453040` (as indicated by the `ai_next` pointer).

`struct addrinfo`

`ai_flags`

`ai_family`

`ai_socktype`

`ai_protocol`

`00`

`00`

`00`

`00`

`02`

`00`

`00`

`00`

`01`

`00`

`00`

`00`

`06`

`00`

`00`

`00`

`ai_flags`

`[PADDING]`

`ai_addr`

`10`

`00`

`00`

`00`

`76`

`65`

`72`

`0a`

`60`

`95`

`45`

`f7`

`81`

`55`

`00`

`00`

`ai_canonname`

`ai_next`

`00`

`00`

`00`

`00`

`00`

`00`

`00`

`00`

`40`

`30`

`45`

`f7`

`81`

`55`

`00`

`00`

Note that these `struct addrinfo` instances form a linked list of possible address responses (each of which point to external `struct sockaddr` instances). In this example, since `ai_next` is `NULL`, there are no more `struct addrinfo` entries in the list.

Once the address information is established, it can be passed to the `connect()` function to establish the initial connection to the socket at a server address, so long as it is accepting requests. If TCP is the transport layer protocol used, `connect()` will send an initial message to the server host process to initiate the TCP [3-way handshake](#term-tcp-handshake), making the server aware of the connection. If UDP or another connectionless protocol is used, `connect()` simply sets the IP address of the peer (i.e., the server) in the client host’s socket.

📦 C library functions – `<sys/socket.h>`

* * *

`int connect (int socket, const struct sockaddr *address, socklen_t address_len);`

Connect to a server or set the peer address of a connectionless server socket.

Building on the previous examples, [Code Listing 4.6](#cl4-6) shows how the results from `getaddrinfo()` can be used to connect to the server. In this example, we are creating a TCP connection over IPv4 to the designated `hostname`. Based on the hints fields and the second parameter to `getaddrinfo()`, every address returned in the `server_list` will be configured to connect to a web server running HTTP. When the for-loop ends, the process is either connected to the server to start an HTTP session or there is no socket connection available. In the latter case, the `socketfd` would be -1, and the client should recognize the failed connection.

```cpp
/* Code Listing 4.6:
   Client code that will connect to a web server for an HTTP session
 */

/* Declare an IPv4 socket for TCP */
int socketfd = -1;
struct addrinfo hints, *server_list = NULL, *server = NULL;
memset (&hints, 0, sizeof (hints));
hints.ai_family = AF_INET;       /* grab IPv4 only */
hints.ai_socktype = SOCK_STREAM; /* limit to byte streams */
hints.ai_protocol = IPPROTO_TCP; /* create as a TCP socket */

/* Get a list of addresses at hostname that serve HTTP */
getaddrinfo (hostname, "http", &hints, &server_list);

for (server = server_list; server != NULL; server = server->ai_next)
  {
    /* Attempt to create a TCP IPv4 socket */
    if ((socketfd = socket (server->ai_family, server->ai_socktype, 0)) < 0)
      continue;
    if (connect (socketfd, server->ai_addr, server->ai_addrlen) == 0)
      break;
    close (socketfd);
    socketfd = -1;
  }
freeaddrinfo (server_list);
if (socketfd < 0)
  exit (1);

/* ... begin HTTP session here ... */
```

///4.4.3. Server Socket Interface[¶]
------------------------------------

Setting up the server socket involves a different sequence of steps from the client process. As before, `getaddrinfo()` provides an interface for configuring the socket address information, but the arguments will be structured differently. For starters, we are not using `getaddrinfo()` to perform a DNS query on a remote host; instead, we are setting up the socket based on the current host’s existing IP address. In addition, if the server is part of a custom application, rather than a standard utility like a web server, the port number will not be identified as one of the well-known ports.

Code Listing 4.7 shows the differences in the parameters passed to `getaddrinfo()` for a server. First, the `hints.ai_flags` field is set to `AI_PASSIVE` and the `nodename` argument is set to `NULL`. This combination specifies that the socket will use the local host’s IP address and will be listening for incoming requests. Next, the `servname` parameter has been changed to `"8000"` to demonstrate how a custom port number can be used. Systems have existing processes set up for well-known ports, and each port number can only be assigned to a single process. Consequently, if we want to build our own web server, we would need to pick a random port number to use, 8000 in this case. To connect to this server once it is running, the client from Code Listing 4.6 would also need to be modified to pass `"8000"` to `getaddrinfo()` instead of `"http"` as the `servname` argument.

```cpp
/* Code Listing 4.7:
   Getting socket address information for a server
 */

struct addrinfo hints, *server_info = NULL;
memset (&hints, 0, sizeof (hints));
hints.ai_family = AF_INET;       /* grab IPv4 only */
hints.ai_socktype = SOCK_STREAM; /* specify byte-streaming */
hints.ai_flags = AI_PASSIVE;     /* use default IP address */
hints.ai_protocol = IPPROTO_TCP; /* create as a TCP socket */

/* Get a list of addresses at hostname that serve HTTP */
getaddrinfo (NULL, "8000", &hints, &server_info);
```

Once the socket address information has been configured, the process can then make a sequence of function calls to become a server. Typically (though not required), the first call is to `setsockopt()` to configure the socket with the `SO_REUSEADDR` option. This option avoids a common error during the next step, `bind()`. The `bind()` call links the port number with the current process. Sometimes when a port number is reused, a timing problem can cause the previous process (which is no longer running) to fail to release the port fully. Setting the `SO_REUSEADDR` option tells `bind()` to ignore this and forcefully replace the port association.

📦 C library functions – `<sys/socket.h>`

* * *

`int setsockopt (int socket, int level, int option_name, const void *option_value, socklen_t option_len);`

Configure internal settings for a socket.

`int bind (int socket, const struct sockaddr *address, socklen_t address_len);`

Assign a local sockaddr to a socket identifier; returns negative values if the bind cannot be done.

[Code Listing 4.8](#cl4-8) demonstrates using `setsockopt()` and `bind()` to extend the [Code Listing 4.7](#cl4-7) to set up a TCP server. From [Code Listing 4.7](#cl4-7), the port number requested for the server is 8000, which is the port number that would be stored in the `struct sockaddr` passed to `bind()`. The call to `setsockopt()` sets the `SO_REUSEADDR` option to true (1, stored in `socket_option`) at the socket level (`SOL_SOCKET`), meaning only this particular socket is affected. If the `bind()` is successful, the server is established. Another common option is `SO_RCVTIMEO`, which sets a time limit for blocking calls that read from the socket, allowing the process to close lost connections.

```cpp
/* Code Listing 4.8:
   Setting up a connection-oriented server and receiving connections
 */

/* Extending Code Listing 4.7 */
int socket_option = 1;
for (server = server_info; server != NULL; server = server->ai_next)
  {
    /* Attempt to create a TCP socket */
    if ((socketfd = socket (server->ai_family, server->ai_socktype, 0)) < 0)
      continue;

    /* Configure the socket to ignore bind reuse error */
    setsockopt (socketfd, SOL_SOCKET, SO_REUSEADDR, (const void *) &socket_option,
                sizeof (int));
    /* Set a 5-second timeout when waiting to receive */
    struct timeval timeout = { 5, 0 };
    setsockopt (socketfd, SOL_SOCKET, SO_RCVTIMEO, (const void *) &timeout,
                sizeof (timeout));

    /* Bind the TCP socket to the port number */
    if (bind (socketfd, server->ai_addr, server->ai_addrlen) == 0)
      break;
    close (socketfd);
    socketfd = -1;
  }

freeaddrinfo (server_list);
if (socketfd < 0)
  {
    perror ("ERROR: Failed to bind socket");
    exit (1);
  }
```

![Timing for a connection-less server that uses UDP](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.5.png)

Figure 4.4.12: Timing for a connection-less server that uses UDP

For connection-less protocols like UDP, no further action is needed to set up the server. The server can immediately begin waiting on incoming messages from the socket. [Figure 4.4.12](Sockets.html#sockudptiming) illustrates the flow of the client and server functions calls for this scenario. Observe that `connect()` does not transmit any message across the network and only updates local settings on the client. To exchange data, the two processes would call `sendto()` and `recvfrom()`, which are explained in the next section.

Connection-oriented TCP sockets require two additional function calls. The first, `listen()`, converts the socket to a connection-oriented server socket with a designated request queue. The second parameter, `backlog`, can be used to modify the maximum number of enqueued connection requests. Setting this value to 0 will use the system default size, which varies depending on the system implementation of the C library. There is also a maximum allowable listen queue size, defined by the constant `SOMAXCONN`. Once the process has converted its socket to a server socket, repeated calls to `accept()` establish connections with incoming requests. The `accept()` function is blocking, so the process will wait at that point until a new request comes in. When a new request arrives, `accept()` performs the server side of the 3-way handshake to establish the connection, storing information about the client in the `address` and `address_len` fields.

📦 C library functions – `<sys/socket.h>`

* * *

`int listen (int socket, int backlog);`

Convert the socket to a server socket that can accept incoming requests.

`int accept (int socket, struct sockaddr *address, socklen_t *address_len);`

Retrieve the first incoming connection, putting client’s information in the `sockaddr`.

![Timing for a connection-oriented server that uses UDP](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.6.png)

Figure 4.4.14: Timing for a connection-oriented server that uses UDP

[Figure 4.4.14](Sockets.html#socktcptiming) illustrates how the timing of the client and server functions relate. Both processes independently set up their sockets. The server then executes the sequence of calling `bind()`, `listen()`, and `accept()`. The call to `accept()` is blocking, so the server would then wait until a connection request arrives. When the request arrives, `accept()` would collect information about the client host and return. Contrast the functionality of `connect()` shown here with that shown in [Figure 4.4.12](Sockets.html#sockudptiming). With TCP sockets, the client call to `connect()` sends an initial message to the server to establish the connection.

[Code Listing 4.9](#cl4-9) shows the final steps of setting up a TCP connection-oriented server and receiving requests, beginning with the call to `listen()`. After converting the socket to a server socket, the process enters a loop waiting on incoming connection requests. When `accept()` returns with a connection, the client’s IP address and port number are copied into the struct sockaddr. Note that the client’s port number will not be 8000 in this case, which is the port number chosen for this server. Instead, client sockets are assigned an [ephemeral port](#term-ephemeral-port), which is a pseudo-randomly selected integer in the range 1024 - 65535, when they are being set up. In this example, the server uses the `inet_ntoa()` and `ntohs()` utilities to print the client’s IP address and port number, then immediately closes the connection and frees the resources. If the client tries to read from or write to the socket at that point, the operation would fail and the client would get an error message back.

```cpp
/* Code Listing 4.9:
   Receiving TCP connections
 */

/* Extending Code Listing 4.7 and 4.8 */

/* Convert to server socket */
listen (socketfd, 10);

/* Get the size of the sockaddr from getaddrinfo() results */
socklen_t addrlen = server->ai_addrlen;

while (1)
  {
    /* Allocate space for the incoming address info and get it */
    struct sockaddr *address = calloc (1, (size_t) addrlen);
    assert (address != NULL);

    int connection;
    if ((connection = accept (socketfd, address, &addrlen)) < 0)
      break;

    /* Print information about the connection and close it */
    struct sockaddr_in *addr = (struct sockaddr_in *)server->ai_addr;
    printf ("Incoming request from %s:%" PRI16d "\n", inet_ntoa (addr->sin_addr),
            ntohs (addr->sin_port));

    close (connection);
    free (address);
  }
close (socketfd);
```

///4.4.4. Socket Communication[¶]
---------------------------------

For TCP sockets, exchanging messages between the client and server can be done using the standard `read()` and `write()` operations, as with other forms of IPC. This works because sockets are treated like files, and the value returned from `socket()` behaves the same as any other file descriptor. UDP sockets require the use of `recvfrom()` and `sendto()` for data exchange. These functions use `struct sockaddr` parameters to determine the sender’s IP address when receiving and to specify the destination when sending. The `read()` and `write()` functions cannot serve this purpose, as the UDP socket identified by the file descriptor does not store this information. The `recvfrom()` and `sendto()` functions can also be used by TCP for consistency.

📦 C library functions – `<sys/socket.h>`

* * *

`ssize_t recvfrom (int socket, void *buffer, size_t length, int flags, struct sockaddr *address, socklen_t *address_len);`

Receive up to the given length in bytes from a socket.

`ssize_t sendto (int socket, const void *message, size_t length, int flags, const struct sockaddr *dest_addr, socklen_t dest_len);`

Send a message to another host through a socket.

As with other socket-related functions, `recvfrom()` and `sendto()` use the generic `struct sockaddr` type, relying on the `socklen_t` parameter to determine its length and, indirectly, its specific type. Both functions take a `void*` parameter and a size that define the location of the bytes to send or to write received bytes. The `flags` parameter for each can specify advanced usage options. [Code Listing 4.10](#cl4-10) illustrates the use of these functions, sending a simple HTTP request and reading the first part of the response.

```cpp
/* Code Listing 4.10:
   Sending to and receiving from an IPv6 socket
 */

/* Extending Code Listing 4.6 */

/* Create a message for a simple HTTP/1.0 request */
size_t buffer_len = 100;
char buffer[buffer_len];
memset (buffer, 0, buffer_len);
strncpy (buffer, "GET / HTTP/1.0\r\n\r\n", buffer_len);

/* When sending, you can check the number of bytes sent */
ssize_t bytes =
  sendto (socketfd, buffer, buffer_len, 0, server->ai_addr, server->ai_addrlen);

/* Copy the server IP address into a buffer */
char addr_buffer[INET6_ADDRSTRLEN];
inet_ntop (AF_INET6, &((struct sockaddr_in6 *)server->ai_addr)->sin6_addr,
           addr_buffer, sizeof (addr_buffer));
printf ("Sent %zd bytes to %s\n", bytes, addr_buffer);

/* Read all data into the buffer, keeping space for \0 at end */
while ((bytes = recvfrom (socketfd, buffer, buffer_len - 1, 0, server->ai_addr,
                          &server->ai_addrlen)) > 0)
  {
    printf ("%s", buffer);
    memset (buffer, 0, buffer_len);
  }
close (socketfd);
```

[Code Listing 4.10](#cl4-10) assumes that a client socket has already been created and has connected to a web server, as shown in [Code Listing 4.6](#cl4-6). For simplicity regarding the call to `inet_ntop()`, we are assuming this socket uses an IPv6; this assumption is only needed to print the IP address out. When receiving, since the client does not know the number of bytes total that will be sent, the client enters a loop that repeatedly requests enough data to fill up the buffer. Since `recvfrom()` is a blocking call, the process will wait if there are delays in the network. When the server is finished sending, `recvfrom()` will return zero and the client will exit the loop.

[[1]](#id1)

The `"_in"` part of `sockaddr_in` means “Internet,” not “input.” Relatedly, the field names have been changed from `sa_` (_socket address_) to `sin_` (_Internet socket address_). For IPv6, the name of the `struct` becomes `sockaddr_in6` and the field names begin with `sin6_`.

[[2]](#id2)

OS that are derived from BSD UNIX, including macOS, have an additional field in both `sockaddr_in` and `sockaddr_in6` to specify the length of the `struct`. This field is omitted from our discussion, as it is not universal.



//4.5. TCP Socket Programming: HTTP[¶]
======================================

Processes running at the application layer of the protocol stack are not fundamentally different from non-networked concurrent applications. The process has a virtual memory space, can exchange data through IPC channels, may interact with users through `STDIN` and `STDOUT`, and so on. The primary differences between such distributed application processes and non-networked processes are that the data is exchanged via an IPC channel based on a predefined communication [protocol](#term-protocol), and that channel has a significantly higher likelihood of intermittent communication failures. The peer process on the other host may be built by the same development team, it may be a customized open-source server, or it may be a proprietary network service. So long as both processes agree to abide by the protocol specification, writing distributed applications is not drastically different from other concurrent applications with IPC. In this section, we will demonstrate how to use TCP sockets to implement the basic functionality of HTTP, the protocol that underlies web-based technologies.

///4.5.1. Hypertext Transfer Protocol (HTTP)[¶]
-----------------------------------------------

![Basic request-response structure of HTTP running on top of TCP](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.7.png)

Figure 4.5.1: Basic request-response structure of HTTP running on top of TCP

HTTP is the protocol that defines communication for web browsers and servers. Readers who have built personal or professional web pages have relied on this protocol, even if they were unaware of the details of its operation. HTTP is a simple [request-response protocol](#term-request-response-protocol), defined in RFC 2616. To be precise, HTTP is a [stateless protocol](#term-stateless-protocol), in the sense that neither the client nor the server preserves any state information between requests; the server processes each request independently from those that arrived previously. HTTP applications use TCP connections for their transport layer, and [Figure 4.5.1](TCPSockets.html#tcphttp) shows the basic structure of HTTP in relation to the functions that establish the socket connection. The client—a web browser—sends an HTTP request to the server and receives a response.



📜 Example 4.5.1

* * *

Both HTTP requests and responses begin with a sequence of header lines, each ending in a two-character sequence denoted as `CRLF` (carriage return-line feed, or `"\r\n"` in C strings). The first line of requests must be a designated `Request` or `Response` line, which must adhere to a given structure. After the first line, all other headers are optional, but they provide the client and server with additional useful information. At the end of the header lines, there is a single blank line (consisting of only `CRLF`). The figure below shows a sample HTTP header for a GET request, which is the type of request that indicates the client is asking for a copy of a file; in contrast a POST request occurs when the client is writing data back to the server. In the figure below, the client is requesting `http://example.com/index.html`, based on a link from `https://link.from.com`.

![Sample HTTP headers for a GET request](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.8.png)

Figure 4.5.3: Sample HTTP headers for a GET request

The `netcat` tool is a useful way to explore the details of HTTP without a web browser. [[1]](#f25) Using `netcat`, you can interact directly with a remote HTTP server, typing the lines of the protocol itself. This tool is useful for text-based protocols like HTTP but cannot easily be used for protocols that use binary-formatted data. Consider the following example of a command-line session with `netcat`:

$ netcat -v example.com 80
Warning: Inverse name lookup failed for \`93.184.216.34'
example.com [93.184.216.34] 80 (http) open
GET / HTTP/1.1
Host: example.com
Connection: close

HTTP/1.1 200 OK 
[...more lines here, omitted for brevity...]

To use `netcat`, you specify the hostname (`example.com`) and the port number (80) to access. After the command prompt, the first two lines are printed by `netcat` (in verbose mode with the `-v` flag) to indicate that it has connected to the server. The next four lines (the `GET`, `Host`, `Connection`, and blank lines) were typed manually by the user to request the contents of `http://example.com/`. The `Host` is required for HTTP/1.1, as many web servers are operated by third-party providers. In the case of `example.com`, the web server is operated by a cloud service provider, `fastly.net`. That is, the server at 93.184.216.34 is not serving content exclusively for `example.com`; there are several other domains that can be accessed from the same IP address. The `Host` header, then, tells `fastly.net` which specific domain name you are trying to reach. The lines beginning with `HTTP/1.1 200 OK` are the response from the server. The structure of an HTTP response is explained below. We omit the full response here, as it consists of several lines of HTTP headers and HTML code that are not critical to the current discussion.

Writing the messages for an HTTP header is straightforward, as the headers are just concatenated text output. Code Listing 4.11 illustrates the general structure of this task. The client creates a buffer and copies the required `Request` line into the beginning. The string concatenation function, `strncat()`, appends the other lines to the buffer, and the buffer is written to the socket. Note that the `length` variable is used to keep track of how much available space is remaining in the buffer, which is always the capacity (500) minus the length of the existing string in the buffer.

```cpp
/* Code Listing 4.11:
   Constructing and sending an HTTP GET request
 */

size_t length = 500;
char buffer[length + 1];
memset (buffer, 0, sizeof (buffer));

/* Copy first line in and shrink the remaining length available */
strncpy (buffer, "GET /web/index.html HTTP/1.0\r\n", length);
length = 500 - strlen (buffer);

/* Concatenate each additional header line */
strncat (buffer, "Accept: text/html\r\n", length);
length = 500 - strlen (buffer);
/* Other lines are similar and omitted... */

write (socketfd, buffer, strlen (buffer));
```

🐞🐛🐌 Bug Warning

* * *

C’s string functions are notorious sources of buffer overflow vulnerabilities. One common way these vulnerabilities arise is with repeated calls to `strncat()`, such as the omitted lines in [Code Listing 4.11](#cl4-11). The problem is that each call reduces the amount of space left in the buffer. As this happens, the `length` parameter passed to `strncat()` each time must shrink to match only the remaining size of the buffer, not the original size. Using the original size each time would create the possibility that strings would be concatenated beyond the end of the buffer.

📜 Example 4.5.2

* * *

The figure below shows a sample response from a web server for the request in [Example 4.5.1](TCPSockets.html#getrequestex). The response begins with a required `Response` line that lets the client know the request was successful. The optional headers indicate that the body of the message (after the blank line) consists of 37 bytes of HTML text. (Note that the body of the message in [Example 4.5.1](TCPSockets.html#getrequestex) was empty, which is typical for `GET` requests.) The body of the response is the contents of the file `index.html` stored in the web server’s designated root directory. The newline character at the end of the HTML code is not required by the HTTP protocol; rather, it is simply a character stored in the file, as most text editors place a newline at the end of the file. From the perspective of HTTP, the body of the message is a meaningless stream of bytes; the content type only matters to the client (the web browser) so that the client knows how to handle the data. Specifically, the second line of the message body is an HTML header, demarcated with the `<head>...</head>` tag structure. This header has no meaning to HTTP itself.

![Sample HTTP response to the request from Figure 4.5.2](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.9.png)

Figure 4.5.6: Sample HTTP response to the request from [Example 4.5.1](TCPSockets.html#getrequestex)

///4.5.2. BNF Protocol Specification[¶]
---------------------------------------

The key features of the HTTP specification in RFC 2616 are structured as BNF declarations. To understand how these declarations structure the protocol, consider the required request and response lines. Every HTTP request must begin with a `Request-Line` and every response must begin with a `Status-Line`:

Request-Line = Method SP Request-URI SP HTTP-Version CRLF
Status-Line  = HTTP-Version SP Status-Code SP Reason-Phrase CRLF

The `SP` designates a space while `CRLF` designates the carriage return-line feed. Although whitespace is typically insignificant in HTML, it is significant when processing HTTP headers; spaces and `CRLF` characters are required in particular places to facilitate correct interpretations. For requests, there are several valid `Methods` that can be used, with `GET` and `POST` being the two most common. A `GET` request corresponds to reading a file to display in the web browser; the body of the request would be empty in that case. A `POST` request, on the other hand, occurs when the web browser is sending data back to the server, such as when a user enters data in a form and submits it; the message body after the blank line contain the data to send, which would contain the form contents.

Readers with experience writing HTML code may be familiar with _query strings_ and [cookies](#term-cookie). As described previously, a query is part of the standard URI structure that begins with a `'?'` and can provide information to the server about how to process the request. For instance, the URL `http://example.com/help.html?topic=login` indicates that the user is looking for help logging in. The `Request-URI` in this case is `/help.html?topic=login`, containing the query string. [[2]](#f26) Cookies, on the other hand, are another technology used to provide data to the server; for instance, a cookie may contain an authentication token or a username to keep track of the user from one request to the next. Cookies are stored in their own HTTP header, but they are not described in RFC 2616. Instead, there is a separate RFC 6265 that defines the structure of cookies and how to use them. Ultimately, though, from the perspective of HTTP describe above, cookies are simply small pieces of data stored in another optional header field.

///4.5.3. HTTP/1.1 Persistent Connections[¶]
--------------------------------------------

The last part of an HTTP `Request-Line` is the version, which corresponds to the first field of the `Status-Line` that begins the response. The examples in [Example 4.5.1](TCPSockets.html#getrequestex) and [Example GetResponseEx](TCPSockets.html) used the version HTTP/1.0, which is the basic `request-response` protocol we have discussed so far. HTTP/1.1 introduces [persistent connections](#term-persistent-connection), which are commonly used in modern web applications. With a standard HTTP/1.0 request, the TCP socket connection is closed when the server sends the response. As such, if a client needs to request more data, the client must establish a new connection and start over. With an HTTP/1.1 persistent connection, the TCP connection is only closed after the client sends a request that explicitly asks to close the connection.

```xml
<!-- Code Listing 4.12:
     HTML code that causes the sequence in Figure 4.6 -->

<html>
<head>
<script src="http://zoo.com/library.js" />
<script src="script.js" />
</head>
<body><img src="logo.png" /></body>
</html>
```

![Requesting four objects with HTTP/1.1 and a fourth object with HTTP/1.0 from a second server](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.10.png)

Figure 4.5.7: Requesting four objects with HTTP/1.1 and a fourth object with HTTP/1.0 from a second server

[Figure 4.5.7](TCPSockets.html#tcpobjects) illustrates a sample scenario that benefits from HTTP/1.1. The client connects to `foo.com` and requests the file `index.html`, which is shown in [Code Listing 4.12](#cl4-12). This file references two more files, `script.js` and `logo.png`, that are both stored on `foo.com`. These additional files are retrieved with separate HTTP requests. These requests are [asynchronous](#term-asynchronous), so the client can do other work while waiting for them. While waiting, the client connects to `zoo.com` with a request for `library.js`. Since this is the only file needed from that server, the client uses HTTP/1.0, which closes the socket immediately. Since the requests to foo.com used HTTP/1.1, the client must explicitly send a separate request to close the connection once all files have been received.

The asynchronous requests shown in [Figure 4.5.7](TCPSockets.html#tcpobjects) are common in modern web designs. These applications frequently augment HTML with JavaScript to create a dynamic and interactive web page; for instance, clicking on a button may produce a drop-down box of menu options to appear. In some cases, the JavaScript code may issue asynchronous HTTP requests. In truth, the web browser issues the request, as all client-side software (including JavaScript) runs as part of the browser process. These requests are standard HTTP requests as we have observed, with the exception that the browser uses multiple [threads of execution](#term-thread) to issue that request and wait for the response while doing other work. When the browser receives the HTTP response, it will invoke a JavaScript _callback_ routine that the script declared ahead of time to be responsible for handling the data. The web page’s URL does not appear to change, but it is possible to observe these requests as they happen. Most popular web browsers include a menu option, typically called something like “Developer Tools,” that can monitor and even modify these requests.

To be clear, the use of persistent TCP connections does not change the nature of HTTP as a stateless protocol. Although the TCP connection will remain open from one request to the next, the server does not maintain any local information about the specifics of the previous HTTP request or how it responded. The persistent connection is simply a performance improvement, and each request-response is considered a distinct, unrelated exchange.

///4.5.4. Processing HTTP Headers[¶]
------------------------------------

After the `HTTP-Version`, the remainder of the HTTP response `Status-Line` provides information to the client about whether or not the request was successful. The status is reported both as a number (`Status-Code`) and a text description (`Reason-Phrase`). [Table 4.5](#tbl4-5) describes several of the most common statuses. The `Reason-Phrase`s shown here are used by convention and they have no specific effect on processing the response; a web server could choose to break this convention with arbitrary `Reason-Phrase`s and the response would still be processed identically.

Status

Reason-Phrase

Purpose

200

OK

Request was successful

301

Moved Permanently

File has been moved to a new location

400

Bad Request

The HTTP request had incorrect syntax

401

Unauthorized

The request requires user authentication

403

Forbidden

Access to the resource is not allowed

404

Not Found

No file was found based on Request-URI

500

Internal Server Error

The server had an unexpected error or fault

503

Service Unavailable

The server is unavailable or not accepting new requests

Table 4.5: Common HTTP response status messages and their meanings

Some status codes are unrecoverable error messages. For instance, if a client receives 404 or 503 message, either the requested file does not exist or the server is down. There is nothing that the web browser can do to correct those situations, and the browser simply displays an error message. Other codes do allow the browser to respond automatically. If the server responds with a 301 status, the HTTP response should include a Location header that designates the new location; this may be a different URI on the same server, or it might be a full URL because the domain name is different. Web browsers can read this header and re-issue a new HTTP request based on this new location. As another example, if the server responds with a 401 status, the response must include a `WWW-Authenticate` header with a `challenge`. The browser may re-issue the request with an added `Authorization` header field that stores credentials to respond to the `challenge`.

Although writing HTTP headers—as shown in [Code Listing 4.11](#cl4-11)—is straightforward, reading them at the other end can be a challenge if not handled properly. The difficulty arises from the fact that header sizes vary, so the receiver does not know how many bytes to request from the socket at a time. To address this challenge, both clients and servers typically impose a maximum header size of 8 KB by convention. The initial read from the socket requests this much data. If a complete header is not found in this space, then the connection is terminated as invalid by clients; servers that receive such invalid headers return Status 413 to indicate `Entity Too Large`. A complete header must end with a blank line, creating the four-byte sequence `"\r\n\r\n"`. [Code Listing 4.13](#cl4-13) demonstrates how to perform this check by looking for this string. If it is found, the second `'\r'` is replaced with the null-byte `'\0'` to convert `buffer` to a complete string of the header, with each header line ending in `CRLF`.

```cpp
/* Code Listing 4.13:
   Checking for a complete HTTP response header
 */

#define HEADER_MAX 8192

/* Allocate a buffer to handle initial responses up to 8 KB */
char *buffer = calloc (HEADER_MAX + 1, sizeof (char));
ssize_t bytes = read (socketfd, buffer, HEADER_MAX);
assert (bytes > 0);

/* Look for the end-of-header (eoh) CRLF CRLF substring; if
   not found, then the header size is too large */
char *eoh = strnstr (buffer, "\r\n\r\n", HEADER_MAX);
if (eoh == NULL)
  {
    fprintf (stderr, "Header exceeds 8 KB maximum\n");
    close (socketfd);
    return EXIT_FAILURE;
  }

/* Replace the blank line of CRLF CRLF with \0 to split the
   header and body */
eoh[2] = '\0';
```

Once the header and body have been split, processing the header involves repeatedly breaking it at the `CRLF` locations. [Code Listing 4.14](#cl4-14) extends [Code Listing 4.13](#cl4-13) to demonstrate this processing, printing all header lines but looking specifically for the `Content-Length` header. Since [Code Listing 4.13](#cl4-13) ended by replacing the blank line of `CRLF CRLF` with the null byte, line 25 will set `eol` to `NULL` after the last header line is processed.

```cpp
/* Code Listing 4.14:
   Extending Code Listing 4.13 to read a header line at a time
 */

/* Print each header line, with eol indicating the location
   of CRLF at the end-of-line */
char *line = buffer;
char *eol = strstr (line, "\r\n");
size_t body_length = 0;
while (eol != NULL)
  {
    /* Null-terminate the line by replacing CRLF with \0 */
    *eol = '\0';
    printf ("HEADER LINE: %s\n", line);

    /* Get the intended body length (in bytes) */
    if (! strncmp (line, "Content-Length: ", 16))
      {
        char *len = strchr (line, ' ') + 1;
        body_length = strtol (len, NULL, 10);
      }

    /* Move the line pointer to the next line */
    line = eol + 2;
    eol = strstr (line, "\r\n");
  }
```

After [Code Listing 4.14](#cl4-14) establishes the Content-Length of the message (either request or response), this length can be compared with the length of the body that was already read. That is, the initial read from the socket received no more than 8 KB of data, which was the maximum size of the header. However, the body contents (particular for HTML data, images, and other objects returned as a response) are likely to exceed this 8 KB size limit. Consequently, an additional `read()` may be required to retrieve the rest of the body contents. [Code Listing 4.15](#cl4-15) completes the response processing by duplicating the body contents received so far and resizing it to read in the additional data. Note that line 12 will read from the socket into the space just after the existing body contents.

```cpp
/* Code Listing 4.15:
   Extending Code Listing 4.13 to read a header line at a time
 */

char *body = strdup (eoh + 4);
if (body_length > strlen (body)) // if false, all data received
  {
    /* Increase the body size and read additional data from the
       socket; the number of bytes to request is the Content-Length
       field minus the number of bytes already received */
    body = realloc (body, body_length);
    bytes = read (socketfd, body + strlen (body), body_length - strlen (body));
  }
```

In [Code Listings 4.13](#cl4-13), [4.14](#cl4-14), and [4.15](#cl4-15), we have been implicitly assuming the data was HTML for simplicity. The response header would declare this with the `Content-Type` header. However, the same principle ideas would apply regardless of the type of data requested. For instance, if the client issues a `GET` request for `/logo.png`, the response would start with the same headers we have used, but the `Content-Type` would inform the client that the body contains `image/png` instead of `text/html`. Given that images can contain the null-byte, [Code Listing 4.13](#cl4-13) and [Code Listing 4.15](#cl4-15) would need to be modified to avoid the use of `strlen()`. However, the code shown here could be adapted to support binary data objects, such as images.

///4.5.5. Persistent State with Cookies[¶]
------------------------------------------

Designing HTTP to be stateless worked well for its original purpose of sending and receiving documents. As the uses of web pages and the Internet have evolved, however, the lack of state information became a hindrance. Developers began to use HTTP as the foundation for web-based applications that required state persistence. As an example, consider a web-based email serice. The first request might allow a user to log into the system, then a second request would retrieve the messages in the user’s inbox. Additional requests would retrieve pages for composing new messages and sending them. Clearly, such an application design would benefit from storing persistent information on the server about the user.

In modern web development, there are multiple options for data persistence. HTTP [cookies](#term-cookie) are one of the oldest and most pervasive tools to accomplish this. A cookie is a short, text-based key-value pair that gets sent as an HTTP header field (similar to the `Content-Length` or `Connection` headers previously discussed). [Code Listing 4.16](#cl4-16) demonstrates one way to create a cookie in Javascript.

```cpp
/* Code Listing 4.16:
   Creating a new cookie in Javascript that expires in 15 minutes
 */

var date = new Date();
date.setTime(date.getTime() + 15 * 60 * 1000); // add 15 minutes
var newCookie = "username=julian;expires=" + date.toUTCString() +
  ";path=/;samesite=strict;secure";
document.cookie = newCookie;
```

The Javascript code in [Code Listing 4.16](#cl4-16) would run on the client side, causing the web browser to create a new cookie as the key-value pair `username=julian`. The rest of lines 7 and 8 control how the browser will use the cookie. The `expires` field indicates that the cookie is only valid for the next 15 minutes; it will be deleted after that time has passed. The `path=/` indicates that this cookie will be sent along with any requests for the current domain name, regardless of the web site’s directory structure. The `samesite=strict` prevents the cookie from being sent to third-party web sites; that is, the cookie will not be sent to other domains, such as advertising networks. Lastly, the `secure` restricts the cookie to transfer over HTTPS and will prevent its transfer over standard HTTP. Line 9 adds the new cookie to the browser’s`document.cookie` value, which is the concatenated list of all cookies. (This line is unintuitive, as Javascript uses the assignment operator for this purpose, but the line is actually appending the value to an existing string.)

Once the browser executes the code in [Code Listing 4.16](#cl4-16), future HTTPS requests that occur in the next 15 minutes will contain the HTTP header line `Cookie: username=julian\r\n`. Note that the header line does not contain information about the expiration date or the other fields, as these are only relevant to the browser. As an alternative, the server could also generate the cookie. The primary differences are that the HTTP header line would include all of the fields:

Set-Cookie: username=julian;expires=Sat, 15 May 2021 16:00:00 GMT;
path=/;samesite=strict;secure

To add persistent state to the HTTP exchange, the server would use a database that mapped the cookie `username=julian` to information about the user. As such, the server-side code would be able to connect a new request to previous requests, creating the history needed for the application. One common technique, even without a specific user login mechanism, is to use a _session cookie_, such as `session=182735927341`. Session cookies persist until the web browser is closed, allowing servers an easy way to link requests from the same web browser as likely to be related.

[[1]](#id1)

While `netcat` is useful for exploring protocols, it sends all data in unencrypted form and cannot be used for any form of secure communication.

[[2]](#id2)

Using query strings with standard HTML files is pointless, as HTML is a static formatting language and cannot respond to input. Other server-side technologies, such as Java servlets, PHP, or Apache Server-Side Includes (SSI) are required to handle the query dynamically.



//4.6. UDP Socket Programming: DNS[¶]
=====================================

![The DNS name space is organized as a hierarchy of zones of authority](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.11.png)

Figure 4.6.1: The DNS name space is organized as a hierarchy of zones of authority

The [Domain Name System](#term-domain-name-system) (DNS) is a distributed database that resolves human-readable URLs (such as `stuff.com` or `k12.county.edu`) into IP addresses. The basic structure and operation of DNS is defined in RFCs 1034 and 1035. DNS defines a hierarchical name space, illustrated in [Figure 4.6.1](UDPSockets.html#dnshier), that are controlled by multiple [name servers](#term-name-server). At the highest level is the [root name server](#term-root-name-server), which is denoted with a dot (`"."`). The [Internet Corporation for Assigned Names and Numbers](#term-internet-corporation-for-assigned-names-and-numbers) (ICANN) is a nonprofit organization that governs and maintains the root structures of the DNS hierarchy. The level just below the root is the set of [top-level domains](#term-top-level-domain) (TLDs), which provide structure based on the type of service that the registered organization provides. Each TLD is governed and maintained by a separate company or organization, such as Verisign. These organizations coordinate their work with ICANN to maintain the core of the DNS hierarchy.

Levels in the DNS hierarchy are indicated by dots within the domain name. For instance, an education institution (such as a university or school district) would register their domain names under the `.edu` TLD, establishing ownership of a domain name such as `university.edu`. Commercial enterprises and other businesses use the `.com` TLD, reserving domain names like `business.com`. Non-profit organizations register domain names under the `.org` TLD, establishing names like `charity.org`.

🔍 Note

* * *

The names `university.edu`, `business.com`, and `charity.org` are intended to illustrate the types of entities that might use these TLDs. These domains are actually registered to real organizations (CapStone University, a private registrant, and Global Impact, respectively). None of the addresses or descriptions in this section are intended to refer to these specific entities.

Organizations themselves can extend the DNS hierarchy based on their own needs and services. The organizations manage this by setting up and running their own authoritative name servers. For instance, `charity.org` might use the separate domain names `mail.charity.org` and `www.charity.org` to distinguish their email server from the server for their web page. These names are considered subdomains of the `charity.org` domain name.

🐞🐛🐌 Bug Warning

* * *

Once an organization has registered a domain name with the appropriate TLD, that organization has established control of all subdomains within their zone of authority. That is, the organization that has set up the authoritative name server for `charity.org` has administrative control over every domain name that ends with those fields. Note, though, that this control does not extend to similar-looking domain names. The key distinction is the presence of the `"."` immediately preceding `charity.org` in a domain name. That is, the owners of `charity.org` would have the authority for the domain names `mail.charity.org` and `www.charity.org`; they would not, however, have control over `mailcharity.org` or `wwwcharity.org`. Registering similar-looking domain names is a common technique for criminal or other malicious groups that are attempting to take advantage of users who make a mistake typing the URL or those who might overlook the missing `"."` as part of a spam email message in a phishing attack.

DNS is designed to be a resilient system for resolving addresses. As such, there is not actually a single root DNS server. As of this writing, there are currently 13 root servers operating world-wide. These 13 root servers communicate with each other to maintain a consistent database of IP addresses for the TLD servers. Again, as of this writing, there are currently over 1500 TLD extensions. These extensions include the original seven TLDs (`.com`, `.edu`, `.gov`, `.int`, `.mil`, `.net`, and `.org`). Other TLD extensions indicate country codes (such as `.uk` for the United Kingdom or `.ca` for Canada), though many companies repurposed country codes to make their domain names more readable. Some examples of this practice include `del.icio.us` and `bit.ly`, which used the country codes for the United State and Libya to create readable domain names; in the former case, a company registered the domain name `icio.us` with the US domain registrar, then created the `del.icio.us` entry within its own authoritative name server. In more recent years, ICANN has expanded the TLD extensions to include topical names, such as `.car`, `.hospital`, or `.restaurant`.

///4.6.1. Resolving DNS Queries[¶]
----------------------------------

To translate a domain name into an IP address, a user program (such as a web browser) contacts a local process known as a _resolver_. The resolver maintains a _master file_ that contains a local database of pre-defined addresses along with a cache of recently translated addresses. If the master file contains the address for the requested domain name, the resolver can return the answer immediately. For addresses that are not in the master file, the resolver would then consult the larger DNS structure. The pre-defined addresses in the master file include the addresses of the 13 root servers. For instance, the root name server `a.root-servers.net` has a persistent IP address of 198.41.0.4. These addresses serve as the entry point to the Internet’s DNS database.

DNS defines two strategies that resolvers can adopt. In the _iterative_ strategy, a DNS resolver will send repeated queries to different servers until it can resolve the request. For instance, a home user’s laptop may send a DNS request to their ISP or a public DNS service like OpenDNS. This DNS resolver had only the root server addresses, it would contact a root server to get the address of the TLD server; this same DNS resolver would then send a request to the TLD requesting the address of the authoritative name server, and so on. In the _recursive_ strategy, the DNS resolver would simply forward the request to a different resolver that would take control. This approach is illustrated by the same scenario, because the laptop itself has a DNS resolver; instead of iteratively contacting the name servers to resolve the request, the laptop sent a single request to the ISP DNS resolver or OpenDNS. The DNS specification requires all resolvers implement an iterative solution, while the recursive strategy is optional.

![Iterative sequence of DNS requests](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.12.png)

Figure 4.6.4: Iterative sequence of DNS requests

[Figure 4.6.4](UDPSockets.html#dnsiter) illustrates the iterative sequence of messages sent when a web browser tries to resolve `www.charity.org`. [[1]](#f27) The resolver finds the root server address and sends a query to 198.41.0.4 to look up the `.org` TLD address. The root name server responds with 199.19.56.1 as the address for the `.org` TLD name server. The resolver then contacts that server to get the address of the authoritative name server for `.charity.org`, which is at address 12.34.56.78. Finally, the resolver contacts the authoritative name server that indicates `www.charity.org` can be found at 12.34.56.80.

If every DNS query required the steps in [Figure 4.6.4](UDPSockets.html#dnsiter), the system would suffer from terrible performance. Every time someone accessed a web page, sent an email, or streamed a piece of music, the client would have to contact one of the 13 root servers; these servers would quickly crash from the strain of handling these requests. Instead, all levels of the DNS system employ an extensive amount of caching. As such, except under rare circumstances, the master file on the local machine already has the IP address for the `.org` TLD when the original request is received. Similarly, if the request for `www.charity.org` is followed by requests for `www-1.charity.org`, `mail.charity.org`, or any other subdomain, the resolver would not contact either the root or `.org` TLD name servers, as the resolver’s cache would already have the address of the needed authoritative name server. In addition to caching, the TLD and authoritative name servers also employ [replication](#term-343) across multiple IP addresses. That is, 199.19.56.1 is one of several IP addresses that correspond to the `.org` TLD. The resolver can contact any of these addresses and is likely to get the same results.

///4.6.2. DNS Resource Record Structure[¶]
------------------------------------------

The translation information for DNS queries are stored and sent in structures known as _resource records_. [Table 4.6](#tbl4-6) shows the generic structure of every resource record. The `NAME` and `RDATA` fields (indicated with the wavy lines in the table) are variable length; all other fields are exactly 16 bits wide. The `NAME` field (also called the owner) is the human-readable domain name of the record. The `TYPE` provides information about the resource under consideration, as described below. The `CLASS` designates the protocol stack in use, which is `IN` to indicate Internet. The `TTL` field indicates how many seconds the record should be considered valid in the local host’s cache; once the record expires, the resolver should repeat the query to check if the record has been updated. Finally, the `RDATA` field includes the actual data of the record, which is tied to the type, and the `RDLENGTH` indicates the length of `RDATA`.

0

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

`NAME`

`TYPE`

`CLASS`

`TTL`

`RDLENGTH`

`RDATA`

Table 4.6: Generic structure of a DNS resource record

There are several common types of resources records. The `A` type denotes a host address, so the `RDATA` field would contain the IP address for the domain name. The `CNAME` type denotes a [canonical name](#term-canonical-name) record that maps an alias to the definitive domain name. For instance, a company might create the subdomains `ftp.example.com` and `www.example.com`, though both of these addresses are handled by the server identified by the name `example.com`. Resource records for example.com would have an `A` type with the server’s IP address, while resource records for both `ftp.example.com` and `www.example.com` would have `CNAME` type, with `example.com` in the `RDATA` field. A third common resource record type is `NS`, which indicates the authoritative name server for the domain. For instance, the `RDATA` field for the `NS` record for `example.com` would contain the domain name of the authoritative server for the `example.com` domain. Finally, an `MX` record type is used to store information about mail exchange servers that are responsible for delivering email.

To illustrate the example, consider again the fictional scenario in [Figure 4.6.4](UDPSockets.html#dnsiter). When the resolver contacted the `.org` TLD name server at 199.19.56.1, this server might first reply with an NS resource record for the `charity.org` domain. This authoritative name server might have a domain name like `ns.charity.org`, which would be the contents of the `RDATA` field for the NS resource record. To resolve this address, a second response would contain a resource record with the A type containing the address of the name server 12.34.56.78. Complicating matters further, this authoritative name server might initially respond with a `CNAME` resource record to indicate that `www.charity.org` is an alias for `a0.web.charity.org`. A second response from the `charity.org` authoritative name server would then return an A resource record to indicate that `a0.web.charity.org` can be accessed at 12.34.56.80.

///4.6.3. DNS Protocol Messages[¶]
----------------------------------

Like HTTP/1.0, the DNS protocol is a simple request-response protocol with no persistent state between messages, but DNS uses UDP instead of TCP. That is, a DNS client can construct the datagram format specified by the RFC and send it to an arbitrary server as a UDP message with no prior connection. The server would then respond to the IP address in the UDP datagram header with the response. The DNS message itself contains five fields. The `header` field indicates if the message contains a query, a response, or another type of message. The question field contains the domain name that is being queried (the `QNAME`), along with the class (`QCLASS`) and type (`QTYPE`) of resource record requested. For a DNS response, the `answer` field would contain the resource record. The `authority` and `additional` fields provide additional information.

To illustrate the structure of a DNS query and response, consider a request to resolve the domain name `example.com`. [[2]](#f28) [Table 4.7](#tbl4-7) shows the interpretation of the bytes of the request. (Note that the exact structure of the UDP datagram consists of just the bytes shown, concatenated in order: `123401000001...`) The header starts with a 16-bit randomly chosen identifier denoted as XID (`1234` in our example), followed by a 16-bit value that serves as a bit mask. The structure of the bit mask is shown in [Table 4.8](#tbl4-8). The rest of the header after the bit-mask indicates how many entries are in each of the other fields, each as a 16-bit value.

Header

`1234`

`XID=0x1234 [random identifier]`

`0100`

`OPCODE=SQUERY`

`0001 0000 0000 0000`

`1 question field`

Question

`0765 7861 6d70 6c65 0363 6f6d 00`

`QNAME=EXAMPLE.COM.,`

`0001 0001`

`QCLASS=IN, QTYPE=A`

Answer

 

`<empty>`

Authority

 

`<empty>`

Additional

 

`<empty>`

Table 4.7: Sequence of bytes and their interpretation to query DNS for example.com

[Table 4.8](#tbl4-8) illustrates the structure of the 16-bit flag field that follows the XID field of a DNS header. In the message shown in [Table 4.7](#tbl4-7), the only bit set is the `Recursion Desired` (`RD`) bit. The `Opcode` field indicates that this is a standard query (`SQUERY = 0000`). In the response shown in [Table 4.9](#tbl4-9), the flag value is `0x8180`, which means that the Query Response (`QR`) bit has been set to indicate the message is a response, as well as the `Recursion Available` (`RA`) bit. The `RCODE` field is used to indicate if an error occurs, and all 0 bits there indicates there was no error processing the query. Information on the other fields is available in RFC 1035.

Bit Index

0

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

Meaning

`QR`

`Opcode`

`AA`

`TC`

`RD`

`RA`

`Z`

`RCODE`

Value

0

0

0

0

0

0

0

1

0

0

0

0

0

0

0

0

Hex Value

0

1

0

0

Table 4.8: Structure of the flags field in a DNS query header

The question field of the request starts with the fields of the domain name, with each field preceded by the number of bytes in the field. That is, the domain name in the question does not contain the standard dotted format for the name. In this example, the domain name (`QNAME` [[3]](#f29)) starts with 7 characters (`65 78 61 6d 70 6c 65 = "EXAMPLE"`) followed by a field with 3 characters (`63 6f 6d = "COM"`). The QNAME continues until the first 0-byte (`00`) is encountered, as this indicates a 0-length field. The remainder of the question indicate the class (`0001 = IN`) and the type of resource record sought (`0001 = A`).

[Table 4.9](#tbl4-9) shows the response for the query from [Table 4.7](#tbl4-7). In the response, the `header` is almost identical to that of the request. The randomly chosen identifier `XID` should match the original request; if the resolver has sent multiple requests, the `XID` field allows the resolver to determine which request is being answered. The bit mask has been modified to denote that this message is a response and the recursive resolution strategy is available. The `header` also indicates that a single answer has been provided. The `question` field is identical to the original request.

Header

`1234`

`XID=0x1234 [random identifier]`

`8180`

`OPCODE=SQUERY, RESPONSE, RA`

`0001 0001 0000 0000`

`1 question and 1 answer`

Question

`0765 7861 6d70 6c65 0363 6f6d 00`

`QNAME=EXAMPLE.COM.,`

`0001 0001`

`QCLASS=IN, QTYPE=A`

Answer

 

`<empty>`

`c00c`

`QNAME=EXAMPLE.COM. [compressed]`

`0001`

`QTYPE=A`

`0001`

`QCLASS=IN`

`0000 e949`

`TTL = 0xe949 = 59721`

`04`

`RDLENGTH = 4`

`5db8 d822`

`RDATA = 0x5db8d822 [93.184.216.34]`

Authority

 

`<empty>`

Additional

 

`<empty>`

Table 4.9: Sequence of bytes and their interpretation for the example.com DNS response

The answer field contains the resource record, which adheres to the general structure defined in [Table 4.6](#tbl4-6). The record is an `A`-type Internet (`IN` class) with a time-to-live of 59,721 seconds. The `RDLENGTH` indicates a length of four bytes for the `RDATA`, which contains an IPv4 address. Note that the address is simply a 32-bit number `0x4db8d822`; by interpreting each byte as a separate number, this denotes the dotted decimal address 93.184.216.34.

The `QNAME` field of the resource record is employing a compression technique to keep the message as small as possible. That is, since the question field already contains the domain name, there is no need to repeat the string in the answer. DNS indicates the compression is being used by setting the first two bits of the answer field to `11` (hence the first byte is `0xc`). Ignoring those two bits, the next 14 bits (`0x000c` after clearing out the two “`11`” bits) indicate the location of the name as a byte offset within the message. That is, the answer is pointing to the byte offset 12 (`0xc`) within the datagram, which is where the `07657861...` starts.

///4.6.4. Constructing DNS Queries with Sockets[¶]
--------------------------------------------------

To illustrate how to work with DNS in code, we start by declaring the following types for a DNS header and question. The `dns_header_t` and `dns_question_t` type definitions are those used in the macOS DNS implementation and are present in the `dns_util.h` header file. However, these are not part of the POSIX standard, so they do not exist on other systems. [[4]](#f30) We use them here for convenience to construct the query.

```cpp
typedef struct {
  uint16_t xid;      /* Randomly chosen identifier */
  uint16_t flags;    /* Bit-mask to indicate request/response */
  uint16_t qdcount;  /* Number of questions */
  uint16_t ancount;  /* Number of answers */
  uint16_t nscount;  /* Number of authority records */
  uint16_t arcount;  /* Number of additional records */
} dns_header_t;

typedef struct {
  char *name;        /* Pointer to the domain name in memory */
  uint16_t dnstype;  /* The QTYPE (1 = A) */
  uint16_t dnsclass; /* The QCLASS (1 = IN) */
} dns_question_t;
```

[Code Listing 4.17](#cl4-17) illustrates how to start creating a DNS query using the OpenDNS service. This same request could be sent to any DNS server, such as the DNS server operated by the reader’s ISP. [[5]](#f31) As with HTTP before, the code starts by creating a socket, but this socket uses the `SOCK_DGRAM` type to create a UDP socket. OpenDNS’s DNS server IPv4 address is available at 208.67.222.222, which is the hexadecimal value `0xd043dede`. DNS servers listen on port 53, so that value is also set. For the DNS header, we can randomly assign any value to the `XID` field, which has no inherent meaning to the server itself. The flag field is set to declare the message is a request (`Q=0`) and to indicate that recursion is desired (`RD=1`). Finally, we declare that we will be sending a single question in this request. Note that all of the numeric values are set using the `htons()` and `htonl()` standard C functions to ensure that the values in the datagram will be in the correct byte order.

```cpp
/* Code Listing 4.17:
   Creating a DNS header and question to send to OpenDNS
 */

int socketfd = socket (AF_INET, SOCK_DGRAM, 0);
struct sockaddr_in address;
address.sin_family = AF_INET;
/* OpenDNS is currently at 208.67.222.222 (0xd043dede) */
address.sin_addr.s_addr = htonl (0xd043dede);
/* DNS runs on port 53 */
address.sin_port = htons (53);

/* Set up the DNS header */
dns_header_t header;
memset (&header, 0, sizeof (dns_header_t));
header.xid= htons (0x1234);    /* Randomly chosen ID */
header.flags = htons (0x0100); /* Q=0, RD=1 */
header.qdcount = htons (1);    /* Sending 1 question */
```

[Code Listing 4.18](#cl4-18) illustrates the initial steps for setting up the question field. The length of this field is not fixed, as it depends on the length of the domain name being translated. As such, the `dns_question_t` type does not contain the full contents of the question itself, using a pointer to the name field within the program instead. For this scenario, we are only requesting an Internet address record, so we set the `QTYPE` to 1 (`A`) and `QCLASS` to 1 (`IN`).


```cpp
/* Code Listing 4.18:
   Creating a DNS header and question to send to OpenDNS
 */

/* Set up the DNS question */
dns_question_t question;
question.dnstype = htons (1);  /* QTYPE 1=A */
question.dnsclass = htons (1); /* QCLASS 1=IN */

/* DNS name format requires two bytes more than the length of the
   domain name as a string */
question.name = calloc (strlen (hostname) + 2, sizeof (char));
```

Recall the domain name formatting in [Table 4.7](#tbl4-7) and [Table 4.9](#tbl4-9). Given a human readable domain name, such as `www.charity.org`, the string is broken apart into distinct fields based on the dot; in this case, the three fields are `"www"`, `"charity"`, and `"org"`. Within the DNS question, each field is preceded by a one-byte value that indicates the length of the field. The name is considered terminated once the null-byte is used to indicate a zero-length field. [Code Listing 4.19](#cl4-19) shows an algorithm to convert a human readable name into the DNS question format. The code starts by copying the hostname into the second byte of the space allocated for the name; the reason for this is to leave one byte of space for the length of the first field (which will be 3 for `"www"`), which will be determined later. Throughout the rest of the algorithm the prev pointer is used to keep track of the location of the byte where the current field’s length will be stored. As such, prev is initialized to the first byte of the space for the name.

```cpp
/* Code Listing 4.19:
   Algorithm for converting a hostname string to DNS question fields
 */

/* Leave the first byte blank for the firstfield length */ memcpy (question.name + 1, hostname, strlen (hostname)); uint8_t *prev = (uint8_t *) question.name; uint8_t count = 0; /* Used to count the bytes in a field */ /* Traverse through the name, looking for the . locations */ for (size_t i = 0; i < strlen (hostname); i++) { /* A . indicates the end of a field */ if (hostname[i] == '.') { /* Copy the length to the byte before this field, then update prev to the location of the . */ *prev = count; prev = question.name + i + 1; count = 0; } else count++; } *prev = count;
```

When the hostname is copied into the space for the question, the string still contains the dot characters. In the DNS question format, these dots are replaced by the lengths of the field that follows. Returning to the example of `www.charity.org`, the first dot should be replaced by 7, indicating the length of the field `"charity"`. The for-loop in [Code Listing 4.19](#cl4-19) replaces the dots with the field name, by keeping prev pointing to the location of the dot preceding the current field. As such, once another dot is encountered, the code can update the byte where the previous dot is stored with the length of the field that just ended. The count variable is then reset to 0 (starting to count the length of a new field), and prev is updated to point to the new dot. When the loop ends, prev is still pointing to the location of the last dot, so its value can be modified with the length of the last field.

🐞🐛🐌 Bug Warning

* * *

The correctness of [Code Listing 4.19](#cl4-19) relies on correct handling of two common mistakes with pointers. First, it is important to distinguish between updating where in memory the prev pointer is pointing `(prev = query + i + 1)` and updating the value stored at that memory location `(*prev = count)`. Typos involving the `*` are notorious sources of bugs with pointers. The second critical dependency is the use of `calloc()` in [Code Listing 4.18](#cl4-18) instead of `malloc()`. Using `calloc()` initializes the space that question.name points to with all zeroes. Consequently, we do not need to explicitly null-terminate the string, because there is already a zero there. Since `malloc()` does not guarantee initialization of the allocated memory space, the byte that indicates the zero-length field might not actually store 0. This could lead to incorrect behavior in the DNS processing, including buffer overflows at either the client or server.

Once the header and question fields have been constructed, all that remains is to assemble these bytes into a packet and send the request through the UDP socket. [Code Listing 4.20](#cl4-20) illustrates this procedure. First, the total packet length needs to be determined. DNS headers are fixed size, but the questions are not. The length of the question is based on the extended length of the hostname (including the byte for the first field’s length and the final null-terminating byte). The question also contains two 16-bit values to indicate the `QTYPE` and `QCLASS`. Once the size is determined and the space is dynamically allocated, the code concatenates all fields as necessary. The header is copied in first, followed immediately by the `QNAME`, with the `QTYPE` and `QCLASS` at the end. Since DNS is based on UDP for transport, the code must use `sendto()` to deliver the message to the socket.

```cpp
/* Code Listing 4.20:
   Assembling the DNS header and question to send via a UDP packet
 */

/* Copy all fields into a single, concatenated packet */
size_t packetlen = sizeof (header) + strlen (hostname) + 2 +
  sizeof (question.dnstype) + sizeof (question.dnsclass);
uint8_t *packet = calloc (packetlen, sizeof (uint8_t));
uint8_t *p = (uint8_t *)packet;

/* Copy the header first */
memcpy (p, &header, sizeof (header));
p += sizeof (header);

/* Copy the question name, QTYPE, and QCLASS fields */
memcpy (p, question.name, strlen (hostname) + 1);
p += strlen (hostname) + 2; /* includes 0 octet for end */
memcpy (p, &question.dnstype, sizeof (question.dnstype));
p += sizeof (question.dnstype);
memcpy (p, &question.dnsclass, sizeof (question.dnsclass));

/* Send the packet to OpenDNS, then request the response */
sendto (socketfd, packet, packetlen, 0, (struct sockaddr *) &addr, 
        (socklen_t) sizeof (addr));
```

///4.6.5. Processing DNS Query Responses[¶]
-------------------------------------------

To receive the response from the DNS server, [Code Listing 4.21](#cl4-21) starts by allocating and clearing the contents of a 512-byte buffer in memory. The length of this buffer can be hard-coded in this way, as the DNS specification mandates a maximum of 512 bytes for all messages. The actual length of the received data is set when `recvfrom()` retrieves the response from the socket.

```cpp
/* Code Listing 4.21:
   Receiving a DNS header and confirming there were no errors
 */

socklen_t length = 0;
uint8_t response[512];
memset (&response, 0, 512);

/* Receive the response from OpenDNS into a local buffer */
ssize_t bytes = recvfrom (socketfd, response, 512, 0, (struct sockaddr *) &addr, &length);
```

The response from the server (assuming the request is successfully processed) would consist of the fixed-size header, a question field identical to that sent in the request, and an answer containing the information from a resource record. The structure of the answer depends on several factors, including the IP version (IPv4 or IPv6) and the type of record requested. That is, the responses for address (`A`), namespace (`NS`), or canonical name (`CNAME`) records vary in structure. In this scenario, we are requesting an IPv4 address, so the bytes in the request would match the following `struct` definition.


```cpp
/* Structure of the bytes for an IPv4 answer */
typedef struct {
  uint16_t compression;
  uint16_t type;
  uint16_t class;
  uint32_t ttl;
  uint16_t length;
  struct in_addr addr;
} __attribute__((packed)) dns_record_a_t;
```

🐞🐛🐌 Bug Warning

* * *

The use of `__attribute__((packed))` in this `struct` declaration is critical to tell the compiler not to re-order the fields of the `struct` within the program. When reading data from the network, the bytes will occur in a particular order. When we use a `struct` to impose a logical meaning on those bytes in a program, we would expect the interpretation to look like this:

![DNS structures are packed in a way that does not preserve word alignment](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.0.1.png)

However, compilers routinely re-order the fields in a `struct` to preserve _word alignment_, trying to group the bytes into chunks of 32 bits as much as possible. In this case, many compilers would swap the `ttl` and `length` fields, which would impose the wrong structure on the sequence of bytes received from the network:

![Failing to declare a DNS record ``struct`` as packed can mislabel bytes due to word alignment](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.0.2.png)

[Code Listing 4.22](#cl4-22) shows how the client can take the received response and interpret it correctly for an IPv4 A record. By casting the response as a `dns_header_t *` variable, the code can refer to the fields within the header based on the struct declaration. By applying the `0xf` bit mask, we can examine just the `RCODE` field of the flag to detect if an error occurs. If the `RCODE` is 0, then the request was processed correctly. Next, we need to traverse through the question field, which begins with the variable-length `QNAME`. The `start_of_name` pointer is created to keep track of where the name starts. Each iteration of the loop determines where the next field length byte will occur and replaces it with a dot, while calculating the total length of the name.


```cpp
/* Code Listing 4.22:
   Checking the header and question name of the DNS response
 */

dns_header_t *response_header = (dns_header_t *)response;
assert ((ntohs (response_header->flags) & 0xf) == 0);

/* Get a pointer to the start of the question name, and
   reconstruct it from the fields */
uint8_t *start_of_name = (uint8_t *) (response + sizeof (dns_header_t));
uint8_t total = 0;
uint8_t *field_length = start_of_name;
while (*field_length != 0)
  {
    /* Restore the dot in the name and advance to next length */
    total += *field_length + 1;
    *field_length = '.';
    field_length = start_of_name + total;
  }
```

Once we have determined the total length of the domain name in the question field, we can skip directly to the resource records in the answer. Immediately after the while loop in [Code Listing 4.22](#cl4-22), the `field_length` pointer will be pointing to the null byte at the end of the name. The records begin five bytes later, after the null byte, the `QTYPE`, and the `QCLASS` fields. Casting the remaining bytes as a `dns_record_a_t *` allows [Code Listing 4.23](#cl4-23) to treat this data as an array of records. The fields of these records can then be cast using the `struct` definition from above.


```cpp
/* Code Listing 4.23:
   Printing the DNS resource records returned
 */

/* Skip null byte, qtype, and qclass to get to first answer */
dns_record_a_t *records = (dns_record_a_t *) (field_length + 5);
for (int i = 0; i < ntohs (response_header->ancount); i++)
  {
    printf ("TYPE: %" PRId16 "\n", ntohs (records[i].type));
    printf ("CLASS: %" PRId16 "\n", ntohs (records[i].class));
    printf ("TTL: %" PRIx32 "\n", ntohl (records[i].ttl));
    printf ("IPv4: %08" PRIx32 "\n", ntohl (records[i].addr));
    printf ("IPv4: %s\n", inet_ntoa (records[i].addr));
  }
```

The Extended Example for Chapter 5 combines all of the preceding code segments, along with some additional statements for printing, into a single program to run as a basic DNS client. If this program is compiled into the current directory as an executable called `dns`, the output would look like the following when querying the address `example.com`. Note that this example only works with some domain names, as our basic client only supports a limited subset of the required functionality as defined in RFC 1034 and RFC 1035.

```sh
$ ./dns example.com
Lookup example.com
  1234 0100 0001 0000 0000 0000 0765 7861
  6d70 6c65 0363 6f6d 0000 0100 01
Received 45 bytes from 208.67.222.222:
  1234 8180 0001 0001 0000 0000 0765 7861
  6d70 6c65 0363 6f6d 0000 0100 01c0 0c00
  0100 0100 00e9 4900 045d b8d8 22
TYPE: 1
CLASS: 1
TTL: e949
IPv4: 5db8d822
IPv4: 93.184.216.34
```

[[1]](#id1)

The domain name and IP address for `www.charity.org` are fictional and provided for illustrative purposes only. The addresses 198.41.0.4 and 199.19.56.1 are the real addresses for the root and .org TLD name servers, however.

[[2]](#id2)

The Internet Assigned Name Authority (IANA) is one portion of ICANN. IANA maintains `example.com` specifically as a public resource for illustrating DNS functioning.

[[3]](#id5)

The DNS specification prepends a `"Q"` to the beginning of field names to indicate that the field is for a query, even though the distinction has no practical impact for basic queries. Hence, the reader should treat `NAME` and `QNAME `` as the same, likewise for ``CLASS` and `QCLASS`, and so on.

[[4]](#id9)

Linux contains similar structs in `<arpa/nameser.h>` and `<arpa/nameser_compat.h>`, but they are more complex than shown here. For instance, the Linux version contains names to access the individual bits of the `flags` field.

[[5]](#id10)

By using OpenDNS in this scenario, we can illustrate the full process of the network request, including setting up the UDP socket with an IP address that is functional as of this writing. OpenDNS also provides a number of other benefits, such as increased privacy and security services. For more information, consult their site at `https://www.opendns.com`.



//4.7. Application-Layer Broadcasting: DHCP[¶]
==============================================

DNS provides a mechanism that clients can use to determine the IP address for a server based on a human-readable domain name. In addition, server processes for common protocols listen for incoming requests on well-known ports. The client can use the address and port number to call `connect()` to establish a TCP connection for an HTTP request; for UDP services like DNS, the client uses the address and port number when it calls `sendto()` to deliver the datagram. Thus, in both cases, there are clear mechanisms for how the client determines the IP address and port number for the server. This raises the question of how the server determines the corresponding information to send the message back to the client.

One part of the answer to this question is that the TCP and UDP services provide this information along with the request. As we will note in the next chapter, transport-layer protocols attach a header to application-layer data containing the IP address and port numbers for both the sender and the receiver. However, this answer is not fully satisfying, because it does not explain how the client’s IP address is set. (Explaining the port number is easy: For ephemeral ports, the OS picks a random 16-bit number, typically above the value 4096.)

IP addresses can be either [static](#term-static-ip-address) or [dynamic](#term-dynamic-ip-address). With a static IP address, the ISP determines a constant value that gets persistently associated for a particular device; the device is configured so that it uses the same IP address every time it boots up. Static IP addresses are commonly used for servers; this practice reduces strain on the DNS, which would have to be updated every time the IP address for a server changed. For end-user client devices, such as laptops, tablets, mobile phones, or Internet-connected _smart home_ devices, IP addresses are normally determined by the [Dynamic Host Configuration Protocol](#term-dynamic-host-configuration-protocol) (DHCP), a protocol that assigns IP addresses dynamically when the device connects to a network. Note that this protocol is used in both wireless and wired networks, as DHCP will also run when a computer is connected via an Ethernet cable.

///4.7.1. DHCP Overview[¶]
--------------------------

The specification for DHCP is defined in RFC 2131. The full protocol is significantly more complex than either HTTP or DNS, because DHCP requires multiple steps rather than the basic request-response structure. DHCP message exchanges can include negotiation back and forth between the client and server based on the hardware and software requirements of each. Additionally, clients can abort the request before the protocol completes, and the server would need to recognize when this happens and close the connection to free up resources. Implementing the full specification requires building state machines for both the client and server. In this section, though, we limit the discussion to the most basic type of (successful) exchange because it illustrates a new socket programming technique not discussed previously: [broadcasting](#term-broadcast).

![Sequence of DHCP messages across a network](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.4.13.png)

Figure 4.7.1: Sequence of DHCP messages across a network

[Figure 4.7.1](AppBroadcast.html#dhcpbroadcast) illustrates the sequence of messages across a network for a DHCP request. To initiate a DHCP request, a new client (such as a laptop) uses UDP to broadcast a `DHCP discover` message, which declares that the sender is trying to find a DHCP server. In other words, the client sends a message using a reserved IP address that indicates the client does not know the destination. If the client is connected to a router that is running a DHCP server, the router would accept the message and would not forward it. However, if there is no intervening router (e.g., the client is connected via an Ethernet cable), then the message is sent to all hosts on the local network. Most devices on the network are not running DHCP servers, so they have no process listening at the well-known DHCP port number (67). For instance, a connected smart TV or network-accessible storage (NAS) hard drive would not be listening for incoming DHCP requests. For these devices, the OS simply ignores the message.

When the broadcast message is received by a DHCP server, the server replies by broadcasting a `DHCP offer` that informs the client of the server’s IP address, as well as proposing an IP address for the client to use. If there are multiple DHCP servers running on the network, then any that receives the `DHCP discover` message will respond with a `DHCP offer`. The client accepts the offer by sending a `DHCP request`. Although this message is broadcast, the body of the request contains information to designate which DHCP server’s offer is being accepted. That server confirms the configuration to all devices by broadcasting a final `DHCP ACK` (acknowledgement) that declares the new client has successfully been assigned to that particular IP address.

///4.7.2. DHCP Protocol Messages[¶]
-----------------------------------

0-7

8-15

16-23

24-31

`op`

`htype`

`hlen`

`hops`

`xid`

`secs`

`flags`

`ciaddr`

`yiaddr`

`siaddr`

`giaddr`

`chaddr (16 bytes)`

`sname (64 bytes)`

`file (128 bytes)`

`options (variable)`

Table 4.10: Structure of a DHCP message

RFC 2131 defines the core structure of DHCP messages, which is shown in [Table 4.10](#tbl4-10). In this illustration, each row of the table is 32 bits (four bytes) wide, and some fields would require multiple rows, as they are larger than four bytes in size. While this structure shows the relative ordering of the fields, the exact structure depends on the type of message. Depending on the type, some fields may be required, others are optional, and still more are not allowed. RFC 2132 specifies the allowable options.

For the four basic messages shown in [Figure 4.7.1](AppBroadcast.html#dhcpbroadcast), the important fields to consider are `op`, `xid`, `yiaddr`, and `siaddr`. The `op` field can be set to either `BOOTREQUEST` (1) or `BOOTREPLY` (2), indicating if the message is coming from the client or the server. As in DNS, the `xid` is a random integer to denote the transaction ID, connecting replies with the specific requests. The `yiaddr` is used to indicate _your_ (the client’s) IP address, while siaddr is the DHCP server’s address. The other fields, described in the RFC, are beyond the scope of this book. [Table 4.11](#tbl4-11) shows the values of these fields for a sample DHCP request. All messages are sent to the destination IP address 255.255.255.255, which is the reserved value that indicates a broadcast message. The client uses 0.0.0.0 as its source address for the UDP messages, indicating the sending host has no valid address, while the DHCP server can use its own address.

The client distinguishes itself from other new clients by picking a random value to use as the `xid`, which is 42 for the `DHCP discover` message in this case; after the server replies with the same `xid` value, the client and the server repeat the same value again in the later messages. A `DHCP discover` message from another new client might have 587 for its `xid`. By receiving a broadcast message with `xid` 42 from the server, the client that started with 42 has reasonable assurance that it is the intended recipient.

Message type

UDP addressing

DHCP contents

`DHCP discover`

`SRC: 0.0.0.0:68   DEST: 255.255.255.255:67`

`op: BOOTREQUEST   xid: 42   yiaddr: 0.0.0.0`

`DHCP offer`

`SRC: 192.168.1.1:67   DEST: 255.255.255.255:68`

`op: BOOTREPLY   xid: 42   yiaddr: 192.168.1.7   siaddr: 192.168.1.1   lease time: 86400`

`DHCP request`

`SRC: 0.0.0.0:68   DEST: 255.255.255.255:67`

`op: BOOTREQUEST   xid: 42   ciaddr: 192.168.1.7`

`DHCP ACK`

`SRC: 192.168.1.1:67   DEST: 255.255.255.255:68`

`op: BOOTREPLY   xid: 42   yiaddr: 192.168.1.7   siaddr: 192.168.1.1   lease time: 86400`

Table: 4.11: Structure of DHCP messages to set up an IP address

The other fields in the DHCP message establish the new client’s IP configuration. When the server sends the `DHCP offer`, the `yiaddr` field contains a proposed value for the client’s address. The client could reject the address, but here we assume that the offer is accepted. The client indicates that it accepts the offer by using the same value in its `DHCP request`, and the server confirms the acceptance in the `DHCP ACK`. In addition, these messages establish that the client knows the IP address of the DHCP server (`siaddr`), along with the duration of the _lease_—the time during which the client owns this IP address. In this case, the client will have a lease on the address 192.168.1.7 for a maximum of 86,400 seconds (24 hours). When the lease expires, the client would contact the server to renew the lease. At that point, the client would not need to repeat the full protocol; since the client knows the IP address of the server, it does not need to begin back at the `DHCP discover` stage.



//4.8. Extended Example: CGI Web Server[¶]
==========================================

This Extended Example creates a web server for processing requests using the _common gateway interface_ (CGI). In a standard web server, HTML files are stored on the file system and their contents are sent through the socket when requested. With CGI, the server can execute a compiled program. Whatever the program writes to its standard output gets redirected through the socket to the client. This technique supports dynamic server-side execution, such as generating HTML based on database query results.

```cpp
#include <arpa/inet.h>
#include <assert.h>
#include <netdb.h>
#include <netinet/in.h>
#include <signal.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdint.h>
#include <stdlib.h>
#include <string.h>
#include <sys/wait.h>
#include <unistd.h>

#define BUFFER_LENGTH 1000
#define WHITESPACE " \t\r\n"
#define CRLF "\r\n"

static void sigint_handler (int signum); /* SIGINT handler */
static bool process_request (int);
static int setup_server (uint16_t);

int serverfd = -1; /* Server socket file descriptor */

int
main (int argc, char *argv[])
{
  /* Create a sigaction and link it to the handler */
  struct sigaction sa;
  sa.sa_handler = sigint_handler;
  assert (sigaction (SIGINT, &sa, NULL) != -1);
  assert (sigaction (SIGTERM, &sa, NULL) != -1);

  /* Get the port number from the command line and set up the
     server. If the setup does not work, the socket file
     descriptor will be negative; exit if that happens. */
  if (argc != 2)
    return EXIT_FAILURE;
  uint16_t port = (uint16_t) strtoul (argv[1], NULL, 10);
  if ( (serverfd = setup_server (port)) < 0)
    {
      perror ("Failed to set up web server");
      return EXIT_FAILURE;
    }

  /* Enter a loop for processing web connections */
  bool running = true;
  while (running)
    {
      struct sockaddr_in address;
      memset (&address, 0, sizeof (address));
      socklen_t addrlen = 0;

      int connection = accept (serverfd, (struct sockaddr *)&address, &addrlen);
      if (connect < 0)
        break;

      running = process_request (connection);
    }

  /* After receiving a shutdown request, close the server
     socket and exit. Note that it would be good to include a
     signal handler (Chapter 2) to close the socket if the
     process is killed with SIGINT or another signal. */
  shutdown (serverfd, SHUT_RDWR);
  close (serverfd);
  serverfd = -1;
  return EXIT_SUCCESS;
}

/* Parse the incoming HTTP request and look for the requested
   file. It must be of the form /cgi-bin/foo.cgi or it must be
   shutdown. All other requests are ignored. Returning true
   from this function keeps the server running, while returning
   false will shut the server down. */
static bool
process_request (int connection)
{
  /* Read the HTTP request from the socket into a local buffer */
  char buffer[BUFFER_LENGTH];
  memset (buffer, 0, BUFFER_LENGTH);
  size_t bytes = read (connection, buffer, BUFFER_LENGTH - 1);
  if (bytes <= 0)
    {
      perror ("No data read from socket");
      shutdown (connection, SHUT_RDWR);
      close (connection);
      return true;
    }

  /* This minimal web server only supports requests of the
     following formats:

       GET /cgi-bin/hello.cgi HTTP/1.1 ...
       GET /cgi-bin/hello.cgi?username=me HTTP/1.1 ...
       GET /shutdown ...

   */

  /* Reject any request that doesn't start with "GET" */
  char *token = strtok (buffer, WHITESPACE);
  if (strncmp (token, "GET", 4))
    {
      perror ("Invalid HTTP request received");
      shutdown (connection, SHUT_RDWR);
      close (connection);
      return true;
    }

  /* Check for a shutdown request. Note that all URIs passed from
     the web browser will begin with a '/' character, so we are
     looking for "/shutdown". */
  token = strtok (NULL, WHITESPACE);
  if (!strncmp (token, "/shutdown", 10))
    {
      /* Send a message confirming the shutdown request, then
         return false to shut the server down */
      char *message = "HTTP/1.1 200 OK" CRLF
        "Connection: close" CRLF
        "Content-Type: text/html; charset=UTF-8" CRLF CRLF
        "<html><body><h2>Shutdown received</h2>"
        "<p>Goodbye</p></body></html>\n";

      write (connection, message, strlen (message));
      shutdown (connection, SHUT_RDWR);
      close (connection);
      return false;
    }

  /* For this example, we are only supporting CGI executable
     files and they must exist in a "cgi-bin" subdirectory of the
     server's working directory. Reject any request that does not
     begin with "/cgi-bin/". */
  if (strncmp (token, "/cgi-bin/", 9))
    {
      shutdown (connection, SHUT_RDWR);
      close (connection);
      return true;
    }

  /* Remove the leading '/' character, as we are looking for
     files based on an relative path, not an absolute path */
  char *cgi_file = token + 1;

  /* Search for a query string, which begins immediately following
     the '?' character if it exists. If one is found, keep track
     of where the query string starts and replace the '?' with a 
     null byte. This will ensure the file name is properly
     terminated. For example, "cgi-bin/hello.cgi?user=me" will be
     converted to the cgi_file "cgi-bin/hello.cgi" while the 
     question pointer will point to "user=me". */
  char *question = strchr (cgi_file, '?');
  if (question != NULL)
    {
      *question = '\0';
      question++;
    }
  /* Check for file read and execute permissions */
  if (access (cgi_file, R_OK | X_OK) != 0)
    {
      shutdown (connection, SHUT_RDWR);
      close (connection);
      return true;
    }

  /* Now we are ready to respond. Write back an HTTP/1.1 header
     to the web browser then run the CGI program. */
  char *message = "HTTP/1.1 200 OK" CRLF "Connection: close" CRLF
    "Content-Type: text/html; charset=UTF-8" CRLF CRLF;
  write (connection, message, strlen (message));

  pid_t child_pid = fork ();
  if (child_pid < 0)
    {
      shutdown (connection, SHUT_RDWR);
      close (connection);
      return false;
    }

  if (child_pid == 0)
    {
      /* If query string passed, set the environment variable */
      if (question != NULL)
        setenv ("QUERY_STRING", question, 1);

      /* Redirect the child process's STDOUT to write into the
         socket and execute the CGI program */
      dup2 (connection, STDOUT_FILENO);
      execlp (cgi_file, cgi_file, NULL);
      return true;
    }

  /* The parent waits until the child process runs (writing to the
     client over the socket), then closes the socket and continues
     with the next request */
  wait (NULL);
  shutdown (connection, SHUT_RDWR);
  close (connection);
  return true;
}

/* Set up a basic HTTP server on the localhost, using the port
   number passed on the command line. In this example, we are
   manually configuring the server to use 127.0.0.1 (localhost
   loopback) as the IP address. */
static int
setup_server (uint16_t port)
{
  /* Manually configure socket address and protocol information */
  struct sockaddr_in localhost;
  memset (&localhost, 0, sizeof (localhost));
  localhost.sin_family = AF_INET;                 /* IPv4 address */
  localhost.sin_port = htons (port);              /* port number from user */
  localhost.sin_addr.s_addr = htonl (0x7f000001); /* localhost loopback */

  struct addrinfo server;
  memset (&server, 0, sizeof (server));
  server.ai_family = AF_INET;         /* grab IPv4 only */
  server.ai_socktype = SOCK_STREAM;   /* specify byte-streaming */
  server.ai_flags = AI_PASSIVE;       /* use default IP address */
  server.ai_protocol = IPPROTO_TCP;   /* create as a TCP socket */
  server.ai_addrlen = INET_ADDRSTRLEN; /* IPv4 address length */
  server.ai_addr = (struct sockaddr *) &localhost;

  /* Create a TCP socket, then configure it to ignore bind reuse
     false errors and set a 5-second timeout for receiving */
  int socketfd = socket (server.ai_family, server.ai_socktype, 0);
  assert (socketfd >= 0);
  int sockopt = 1;
  setsockopt (socketfd, SOL_SOCKET, SO_REUSEADDR, (const void *) &sockopt,
              sizeof (int));
  struct timeval timeout = { 5, 0 };
  setsockopt (socketfd, SOL_SOCKET, SO_RCVTIMEO, (const void *) &timeout,
              sizeof (timeout));

  /* Bind the socket to the port number and convert it to a
     server socket */
  if (bind (socketfd, server.ai_addr, server.ai_addrlen) != 0) 
    {
      close (socketfd);
      return -1;
    }
  listen (socketfd, 10);
  return socketfd;
}

/* Interrupt handler in case we need to shut down with Ctrl-c.
   This helps to ensure the server socket is shutdown cleanly. */
static void
sigint_handler (int signum)
{
  if (serverfd >= 0)
    {
      shutdown (serverfd, SHUT_RDWR);
      close (serverfd);
    }
  exit (EXIT_SUCCESS);
}


/* This file needs to be compiled and the executable should be
   placed in a subdirectory of the directory where the server
   runs. If the server runs on port 8000 and this file is
   compiled into an executable named "hello.cgi", it can be
   accessed with a web browser at
     http://localhost:8000/cgi-bin/hello.cgi
 */
#include <stdio.h>
#include <stdlib.h>
/* CGI (common gateway interface) version of "Hello world".
   Programs like this can write to STDOUT. The web server that
   runs the executable form of this will redirect that output
   into a socket created for a web page access. */
int
main (void)
{
  printf ("<html>\n<body>\n");
  printf ("<h2>Hello world!</h2>\n");
  printf ("<p>Your CGI bin is set up properly</p>\n");
  /* Check to see if an HTTP query string was passed */
  char *query = getenv ("QUERY_STRING");
  if (query != NULL)
    printf ("<p>Here is your query string data: %s</p>\n", query);

  printf ("</body>\n</html>\n");
  return 0;
}
```


/Chapter 5   The Internet and Connectivity[¶]
=============================================

//5.1. The Internet and Connectivity[¶]
=======================================

![Timeline of major CSF topics with Sockets and Networking highlighted](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Timeline.5.png)

> “The stuff I design, if I’m successful, nobody will ever notice. Things will just work, and be self-managing.”
> 
> Radia Perlman

Early computer networks, such as ARPANET, were designed for government and military applications. As an increasing number of researchers worked on these technologies, there was a growing push to standardize the communication protocols and open access to academic and commercial interests. Vint Cerf introduced the term Internet in RFC 675 (1974) as part of the first specification of TCP, and the name stuck. The TCP/IP protocol suite was later standardized in 1982, with the Internet emerging as the public successor of ARPANET.

🚀 Chapter Objectives

* * *

In this chapter, we will address the following instructional objectives:

*   We will describe the principles of application-layer overlay networks.
*   We will compare and contrast the functioning of the UDP and TCP transport-layer protocols.
*   We will explore how cryptography can be used to provide secure network communication.
*   We will summarize how the network and link layers deliver messages through the Internet.
*   We will consider how wireless technologies different from traditional wired networks.

The previous chapter introduce the socket API as a mechanism that appears to behave like other forms of IPC. Using the socket API, the application can use traditional `read()` and `write()` functions or the equivalent `sendto()` and `recvfrom()` to exchange data with other processes. From this perspective, building networked applications bears many similarities to traditional single-host concurrent software. The key differences arise at the design stage.

Networked applications can face a dilemma that other forms of concurrent software do not. For example, assume that a process correctly formats a message and sends it into the socket. At the other end, the receiver has correctly prepared to receive the message, but the message never arrives. In other forms of IPC, the processes could check error codes provided by the OS to determine what went wrong. In the network, however, there is no OS. The sender believes it was successful at its end and hopes for the best for the message delivery. The receiver only knows the data did not arrive; it cannot tell if the data was lost in transit or if the sender itself failed.

Architects of networked applications must consider how to handle this scenario. Some socket configurations request the lower layers of the protocol stack fix the problem; the lower layers may still fail, but they will at least try. In other configurations, the application itself can accept responsibility for handling the fault, either by re-sending the request, displaying an error message, or downgrading the quality of the application service (such as showing a pixelated version of the image or pausing a video playback while more data is being buffered). This design choice entails selecting an appropriate transport-layer protocol. While it may seem that reliability is always required, that is not necessarily so. To properly design networked applications, it is important to understand how the layers of the network fit together.

In this chapter, we will explore the full protocol stack of the [Internet model](#term-internet-model). Starting at the top, we will start with the overall system architecture of the application, along with how the transport layer protocols influence the design. In addition, we will explore the capabilities and functioning of the lower protocol layers, which can influence application design choices. This overview includes an introduction to available security services, how hosts are located, and how data is actually transmitted. This chapter necessarily omits some details to create a foundational understanding of the key concepts of these layers. Interested readers should explore the details in the recommended resources.



//5.2. Application Layer: Overlay Networks[¶]
=============================================

HTTP and DNS are application-layer protocols that rely on a traditional client/server architecture. [[1]](#f32) In both cases, a client (web browser or DNS resolver) establishes a socket link to a service provider that responds with the requested data. In both cases, there is no expectation that the client will provide that data to another client at any point in the future. In contrast, many application-layer protocols are designed as [peer-to-peer](#term-peer-to-peer-architecture) (P2P) services. In a P2P architecture, processes at all nodes can act as both a client and a server as needed. The types of services that P2P networks provide vary a lot, as do their design and functionality. Some P2P applications are very complicated. RFCs 4981 and 5694 provide an overview of many of the underlying techniques and concepts for peer-to-peer applications.

For many readers, the topic of P2P applications is associated with file sharing, particularly file sharing networks that facilitate illegal distribution of music and movies. And while that is certainly one example of a P2P application, that is only one element of a much broader field of computing. [Table 5.1](#tbl5-1) summarizes four major application areas for P2P networks, along with examples of each. There are many areas of computing where a P2P architecture provides superior service than a centralized client/server architecture, including faster performance and improved reliability.

Application

Service Description

Examples

Content Distribution

Scalable approaches to sharing data with users across the Internet

*   File storage and sharing: Gnutella, Bittorrent, InterPlanetary File System (IPFS)
*   Content delivery networks (CDNs): Akamai, Limelight
*   Streaming media: Spotify, Sonos
*   Software update distribution: Linux, World of Warcraft

Distributed Computing

Delegating the work for an application across many computers

*   Privacy and censorship resistance: Tor, Freenet
*   Cryptocurrency: Bitcoin
*   Botnets and malware: Storm

Collaboration

Providing real-time human communication

*   Voice Over IP (VOIP): Skype (originally)
*   Instant Messaging: Tox

Platforms

Building applications

*   Java: JXTA

Table 5.1: Examples of peer-to-peer applications

As a more concrete example, [content delivery networks](#term-content-delivery-network) (CDNs) are companies that operate distributed caches of web pages and other files that would normally be transmitted over HTTP from a single server. Consider the release of a new movie trailer for a highly anticipated movie. Without CDNs, every potential viewer would have to access the movie studio’s website, potentially overloading and crashing the server. To prevent that from happening, the movie studio works with the CDN to distribute (legally) hundreds or thousands of copies to caches across the world. Then, when the trailer’s release is made public, potential viewers that go to the movie studio’s website get redirected to load the copy from the CDN cache that is closest in physical distance to the user. In that way, CDNs prevent a significant amount of unnecessary Internet traffic (making everyone’s data connections faster) while also decreasing the likelihood that the movie studio’s server crashes as a single point of failure. P2P content distribution is essential for the functioning of CDNs.

![An overlay network, with links between nodes denoted by dashed links](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.5.1.png)

Figure 5.2.1: An overlay network, with links between nodes denoted by dashed links

A key concept within the domain of P2P applications is the notion of an overlay network, as illustrated in [5.2.1](AppLayer.html#internetoverlay). In an overlay network, the participating nodes are processes running on different machines that are connected through the Internet. These nodes are arranged to have logical links—denoted by the dashed lines—that are independent of their underlying relationship across the Internet. In some cases, the overlay is designed specifically to hide these Internet relationships.

When nodes in the overlay network want to communicate, they establish socket links between their virtual neighbors. These neighbors may then contact their neighbors, and so on. For example, consider the structure shown in [5.2.1](AppLayer.html#internetoverlay). Assume that node A would like to request data from node B. Based on the overlay, node A would contact node C; node C would forward the request to node E; and node E would forward the request to node B. To accomplish this, node A would open a socket link to node C, sending data to its access point and the other routers based on the solid line links. When C forwards the request to E, C would be sending data **back along the same path of routers through the Internet**. E would then send the data through a socket link to B. As such, this request from A to B would pass through the routers at least 11 times (five from A to C, four from C to E, and two from E to B), **despite the fact that A and B are on the same local network**. Depending on the particular application, the response from B might traverse the same path, particularly if the system is designed to provide privacy guarantees that hide information about users. On the other hand, the request that B receives might have A’s IP address information included, allowing B to create the socket link directly with A instead of sending the data back through the overlay paths.

///5.2.1. Characteristics of Peer-to-Peer Networks[¶]
-----------------------------------------------------

Given the assumption of an overlay network, there are two interdependent design decisions that must be made regarding the network’s operation. The first element is whether the overlay is [structured](#term-structured-p2p-network) or [unstructured](#term-unstructured-p2p-network). Early public P2P networks, such as file sharing networks like Gnutella, employed an unstructured approach. When a user logged onto the system, their computer would contact an entry node that would then introduce this new node to the network. This type of network tends to experience a lot of [churn](#term-churn), as nodes frequently enter the network for a short time then leave. Churn means that the network is constantly changing, so there is never enough time for a consistent, logical structure to emerge.

In contrast, more recent designs focus on structured P2P networks that impose a logical framework on the overlay network. For instance, the nodes might be assigned identifiers and arranged in a virtual circle. Node 1234 would be aware of nodes 1233 and 1235, but no other node. Other structures, such as a CDN, might define the logical structure to align with the physical proximity of the nodes; consequently, a service request could be directly to the node that would provide the shortest (and fastest) path to exchange data.

The second—and closely related—design decision for overlay networks is how to identify objects (such as files) in the network. In an unstructured network, a common technique is to use [query flooding](#term-query-flooding), which essentially involves asking as many nodes as possible. As an illustration, assume node E in [5.2.1](AppLayer.html#internetoverlay) is requesting a file. E would send the request to its neighbors, B, C, and G. If any of those nodes had a copy of the file, they could provide it directly to E. If not, these nodes would forward the request to their neighbors, sending the request to nodes A and F. In query flooding, requests have a maximum number of [hops](#term-hop)—times that the request can be forwarded. Each time a node would forward the request, it decrements the hop count. Once the hop count reaches 0, the request is terminated and considered a failure.

Structured P2P networks provide more flexibility for identifying object locations. As these networks tend to have less churn (e.g., CDN companies tend to keep their cache servers constantly running with no downtime), the nodes are more likely to be in predictable locations. As such, these networks create an _index_ that maps objects to the nodes that store them. The index might be _local_, so that a node only knows about the data stored at it and some neighbors. Systems with local indexes can use query flooding as in unstructured networks. They can also use an algorithmic approach to traverse the network in a predictable way; in Chapter 9, we will describe how the Chord P2P network accomplishes this task efficiently. Alternatively, the system may provide a _centralized_ index, in which a single server maintains a global list of what data is stored at each node; any node requesting a file will first contact the index server to learn the location of the node to contact. Finally, the index may be _distributed_ among several index servers that try to—but do not guarantee—maintain a consistent mapping of object locations. This is the approach of the Google file system, also described in Chapter 9.

In summary, P2P networks provide scalable solutions for many applications that would experience bad performance in client/server architectures. P2P networks operate by imposing an overlay network that defines the logical connections between nodes, while relying on the Internet to perform the actual data delivery. In some cases, routing in the overlay network leads to a significant amount of Internet traffic, as the nodes may not have enough information to direct their requests efficiently. In others, the overlay network is designed to minimize the impact on Internet traffic, and in some cases to even reduce it.

[[1]](#id1)

DNS is sometimes described as a peer-to-peer architecture because of the distributed and replicated nature of the database. That is, each of the root and TLD name servers exchange data with each other as peers. However, DNS resolvers act solely as service requesters and do not provide a typical peer-to-peer service. As such, RFC 5694 characterizes DNS as client/server.



//5.3. Transport Layer[¶]
=========================

Up to this point, we have used application-layer socket programming essentially as another form of IPC for exchanging data between processes on different hosts. Communication at that level can follow a protocol that defines human-readable message formats, such as HTTP. Other applications, such as DNS or DHCP, exchange highly structured binary messages that are intended to be interpreted by peer processes. In either regard, the communication is specific to that application, and there is an assumption that “the network”—a mysterious, almost magical entity—transmits the data. That is, application-layer network programming can be easily conflated with other forms of IPC, replacing the OS with the network as the service provider. There is truth to this conflation, as some layers and protocols (including TCP and IP) are typically implemented within the OS. However, other layers are implemented in hardware, including network interface cards, cables between devices, and radios.

The [transport layer](#term-transport-layer) provides the first level of abstraction that application-layer programmers rely on. Specifically, the transport layer establishes the logical structure of a virtual [end-to-end communication](#term-end-to-end-communication) channel between processes. When you use a web browser to access `www.example.com`, you are intending to communicate with a particular process running on that other host, not just the machine in general; you would probably be surprised if your web browser suddenly began showing you binary DHCP or DNS responses instead of HTML-formatted information. The transport layer ensures that your request gets delivered to the process running the web server on the remote host, while returning the web page to your web browser process.

In contrast to a monolithic entity like an OS, the network is a layered architecture of distributed components that cooperate to exchange data. For a variety of reasons, the distributed nature of these components causes failed attempts at a significantly higher rate than an OS would ever encounter. As an example, consider the effect of someone tripping over the cord of a server and unplugging it; there is no possible way for your laptop to communicate with another machine that is turned off. As a less extreme example, the message might not actually be lost, but it may encounter a delay in the network that causes it to arrive too late. Failures and disruptions like this can happen at any time. One key feature for choosing a transport layer protocol is whether or not the application is requesting a [reliable transport](#term-reliable-transport) service that attempts to correct failures that occur. UDP provides fast but [unreliable transport](#term-unreliable-transport), while TCP features reliable transport.

///5.3.1. Unreliable Transport: UDP[¶]
--------------------------------------

The simpler approach to transport-layer service is to provide an unreliable transport, which is the purpose of the [User Datagram Protocol](#term-user-datagram-protocol) (UDP), defined in RFC 768. UDP provides a fast _best effort_ delivery service, which is a polite way to say that it will try but it makes no guarantees. UDP provides a minimal amount of end-to-end error checking to determine if the full payload has been received and the contents have not been corrupted during transmission.

0-15

16-31

`source port`

`destination port`

`length`

`checksum`

`payload`  
(application-layer data)

Table 5.2: Structure of a UDP segment

[Table 5.2](#tbl5-2) shows the structure of a UDP segment. The datagram contains a header with four fields: the port numbers of the source and destination processes, the length of the segment (including both the header and the payload), and a 16-bit checksum calculation. The following examples show the UDP segment for a DNS request from the previous chapter.

📜 Example 5.3.1

* * *

In this example, the client had opened ephemeral port 5000 (`0x1388`) and sent the request to the OpenDNS server, which was listening on port 53 (`0x0035`). The header requires eight bytes and the request payload was 29 bytes, so the total length of the UDP segment was 37 bytes; the response was 53 bytes, due to the longer response in the payload.

Header

`1388   0035   0025   f693`

`source port = 5000 (0x1388)   destination port = 53 (0x0035)   length = 37 (0x0025)   checksum`

Payload

`1234 0100 0001 0000 0000 0000   0765 7861 6d70 6d64 0363 6f6d   0000 0100 01`

DNS request for `example.com`

Header

`0035   1388   0035   af04`

`source port = 53 (0x0035)   destination port = 5000 (0x1388)   length = 53 (0x0035)   checksum`

Payload

`1234 8180 0001 0001 0000 0000   0765 7861 6d70 6d64 0363 6f6d   0000 0100 01c0 0c00 0100 0100   00e9 4900 045d b8d8 22`

DNS response for `example.com`

The `checksum` value is the result of repeated one’s complement addition of 16-bit values in the UDP segment, as shown in [Code Listing 5.1](#cl5-1). In UDP, the checksum is evaluated over the payload, parts of the IP header (which we are ignoring here, as we have not examined IP yet), and a UDP [pseudo-header](#term-pseudo-header). The UDP pseudo-header contains the `source port`, `destination port`, and the `length` fields of the regular UDP header. The `checksum` field is replaced with a 16-bit value containing information about the protocol, which is defined in RFC 762; for UDP, this 16-bit value is 0x0011. Regarding the payload, if there are an odd number of bytes (as shown in the previous messages), that last byte is concatenated with zeroes to create a 16-bit value.

```cpp
/* Code Listing 5.1:
   Calculate a 16-bit checksum for use in UDP
 */

/* Calculates a 16-bit checksum */
uint16_t
cksum (uint16_t *bytes, size_t length)
{
  uint32_t sum = 0;
  /* Loop through, adding 16 bits at a time */
  for (size_t i = 0; i < length / 2; i++)
    {
      sum += bytes[i];
      /* If there is a leading 1, combine the two halves */
      if (sum & 0x80000000)
        sum = (sum & 0xffff) + (sum >> 16);
    }
  /* If there are an odd number of bytes, pad the last with leading
     zeroes */
  if ( (length % 2) == 1)
    sum += ((uint8_t *)bytes)[length - 1];

  /* Finalize the result by combining the two halves and flipping
     the bits */
  sum = (uint16_t) sum + (uint16_t) (sum >> 16);
  return ~sum;
}
```

[Table 5.3](#tbl5-3) illustrates the mechanics of how one’s complement addition works by adding the values `0x7d09` and `0xb5fc.` The result of this calculation would be `0x13305` based on binary arithmetic that does not require a fixed size. However, UDP checksums are fixed at a length of 16 bits. Whenever the addition would produce a carry out of 1 (requiring a 17th bit), that bit is folded around to the least-significant bit location. Consequently, the leading bit of `0x13305` would be removed (creating `0x3305`) and added to the least-significant bit, yielding the result `0x3306`.

`7d09`

 

`0 1 1 1`

`1 1 0 1`

`0 0 0 0`

`1 0 0 1`

`b5fc`

 

`1 0 1 1`

`0 1 0 1`

`1 1 1 1`

`1 1 0 0`

`13305`

`1`

`0 0 1 1`

`0 0 1 1`

`0 0 0 0`

`0 1 0 1`

`3306`

 

`0 0 1 1`

`0 0 1 1`

`0 0 0 0`

`0 1 1 0`

UDP checksum for two 16-bit values

The UDP checksum provides a minimal form of error-checking for end-to-end communication. As we will note later in this chapter, both IP and Ethernet also provide error detection. As such, if the segment is sent with these network and link layer protocols, UDP is performing a redundant service. Furthermore, as UDP will make no attempt to correct the errors it detects, the UDP checksum may seem to be pointless redundancy. This criticism is fair, although UDP can also be used with other network or link-layer protocols that do not perform error correction. One disadvantage of a layered architecture, such as the Internet protocol stack, is that the individual layers must be designed without underlying assumptions of the other layers.

At first glance, UDP’s unreliable transport service may appear to be a poor choice, and for certain applications it is. But UDP provides service that is good enough for other applications, while avoiding the overhead penalties associated with a reliable transport. For instance, UDP is commonly used in streaming multimedia applications that only require _most_ of the data to be successfully transmitted; the application can compensate for lost data by providing slightly worse quality, such as pixelated images or temporarily pausing the media until more data can be received and buffered. The application can detect the lost or corrupted data by checking the return value from `recvfrom()`, sending new requests only as needed. UDP is also commonly used in application-layer services that users do not typically interact with directly, such as DNS or DHCP.

///5.3.2. Reliable Transport: TCP[¶]
------------------------------------

Although UDP provides a lightweight, fast transport service, the unreliability is simply not appropriate for some applications. Consider how frustrating it would be to visit a web page that was missing a portion of text; if the missing data occurred in the middle of HTML formatting or link tags, then the appearance would be wrong or links to other web pages would be broken. As such, many applications require the reliable transport service provided by the [Transmission Control Protocol](#term-transmission-control-protocol) (TCP), which is defined primarily in RFC 793. TCP is a [connection-oriented](#term-connection-oriented-protocol) protocol, indicating that the hosts maintain some form of state between messages. That is, the hosts create a connection to establish a [session](#term-session) that is likely to contain multiple messages sent back and forth between the hosts; during this session, hosts may resend messages that are lost or corrupted, while also taking steps to avoid overwhelming each other with too much data at any time. The full operation of TCP is rather complex, but we will restrict our focus to the key concepts of _reliable data delivery_ and [flow control](#term-flow-control).

As with UDP, TCP segments include the 16-bit `source` and `destination port` numbers to designate the processes at either end of the connection, as shown in [Table 5.4](#tbl5-4). Similarly, the TCP header contains a `checksum` that is used to detect errors that may have occurred in transmission; in contrast to UDP, the TCP pseudo-header is based on information from the IP header. The `urgent data pointer` is not used in modern practice, and the `optional fields` are beyond the scope of this book. The other fields are explained below to illustrate the functioning of TCP reliable transport and flow control.

0-7

8-15

16-23

24-31

`source port`

`destination port`

`sequence number (SEQ)`

`acknowledgement number (ACK)`

`flags`

`receive window`

`checksum`

`urgent data ptr`

`optional fields`

`payload`  
(application-layer data)

Table 5.4: Structure of a TCP segment

In TCP segments, the [sequence number](#term-sequence-number) (`SEQ`) is an identifier associated with the current segment. Each host randomly chooses an initial value for the sequence number. Each time a host sends a segment, it increments its internal counter for the sequence number by the size of the payload. For example, consider a client application that uses `SEQ=25` for a segment containing the five bytes `"Hello"`. If the client’s next segment is to sends the seven bytes `"Goodbye"`, it would use `SEQ=30`. As this segment contains seven bytes, the client’s next segment would use `SEQ=37`. In short, the sequence number denotes the order and the size of the application-layer payload data.

![A TCP data exchange of four messages](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.5.2.png)

Figure 5.3.2: A TCP data exchange of four messages

At the other end, the [acknowledgement number](#term-acknowledgement-number) (`ACK`) allows a receiving host to inform the sender that the segment was received. In the previous scenario, when the server receives the client’s `"Hello"` segment (five bytes sent with `SEQ=25`), the server’s next segment would contain the `ACK=30` (`SEQ + 5`). [Figure 5.3.2](#internettcp) shows the sample exchange that we have been describing. The client’s first segment used `SEQ=25`; the client sent the `ACK=42`, indicating that was the value it expected for the server’s next segment. (This `ACK` is based on segments shown before this sequence.) The server then responds with a segment that does, in fact, use `SEQ=42`. Since this segment (`"Hello back"`) contains 10 bytes, the client indicates it received the segment by using `ACK=52` in its next segment.

During the session, the OS on each host maintains a buffer for storing data until the application reads it from the socket. As this buffer is finite in size, the hosts need to cooperate to prevent a buffer overflow. The [receive window](#term-receive-window) achieves this by declaring the maximum number of bytes that the sender is capable of receiving in the next segment. To observe how this value is used, consider the following analogous line of code when working with strings:

strncpy (buffer, input, sizeof (buffer));

The third parameter indicates the maximum number of bytes that will be copied into the `buffer`. For instance, if the `buffer` has space for 20 bytes, but the `input` is a string that is 50 bytes in length, the third parameter prevents the additional 30 bytes from being written beyond the end of the `buffer`. The receive window serves the same purpose within the context of TCP segments. If the receiver’s next segment exceeds this size, then that host would need to break up its response and send it across multiple segments. This cooperation is the TCP [flow control](#term-flow-control) service, as each host takes steps to avoid sending too much data at a time.

There is subtle point regarding flow control that is easily misunderstood. Consider the sequence of segments in [Figure 5.3.2](#internettcp). In that scenario, the server sent two segments to the client: one containing the string `"Hello back"` and one containing `"Farewell"`. One possible explanation is that the application issued two system calls to write to the socket. Flow control provides another explanation. Although this string in particular is unlikely, the server application may have written the string `"Hello backFarewell"` to the socket. The client’s first segment (sequence number 25) may have included a receive window indicating it only had space for 10 bytes, so the server’s TCP implementation split the segment. The key point is that TCP itself provides flow, and the application is typically not made aware it is happening. The sender splits the segment without informing the application, and the receiving host’s TCP implementation concatenates all segments in its internal buffers as needed.

📜 Example 5.3.2

* * *

The example shown here illustrates the full TCP header and payload for an HTTP `GET` request sent to `example.com`. The destination port number is 80, which is the well-known port number for HTTP, whereas the source port is an ephemeral port number, generated by the client’s OS. The receive window indicates that the client is requesting a maximum of 4096 bytes in response. The `flags` field will be explained in the next discussion on the TCP handshake, and the `urgent data pointer` field is not used. The `payload` here contains the HTTP application-layer request.

Header

`1388   0050   0000 0017   0000 002a   5010   1000   cf33   0000`

`source port = 5000 (0x1388)   destination port = 80 (0x0050)   sequence number = 23 (0x17)   acknowledgement number = 42 (0x2a)   flags   receive window = 4096 (0x1000)   checksum   urgent data ptr`

Payload

`4745 5420 2f20 4854 5450 2f31   2e31 0d0a 486f 7374 3a20 6578   616d 706c 652e 636f 6d0d 0a43   6f6e 6e65 6374 696f 6e3a 2063   6c6f 7365 0d0a 0d0a`

`GET / HTTP/1   .1\r\nHost: ex   ample.com\r\nC   onnection: c   lose\r\n\r\n`

///5.3.3. TCP Handshake and Connections[¶]
------------------------------------------

Unlike the connectionless UDP that allows applications to send and receive data at any time, TCP requires that the two hosts must first establish a connection to begin a communication session. That is, prior to exchanging application-layer data, the hosts must send and receive some initial segments to ensure both hosts know to expect the data exchange. At each end, this procedure involves allocating internal buffers and variables, such as those needed for flow control. Just as importantly, the hosts must also execute the [TCP handshake](#term-tcp-handshake), a lightweight initial protocol that allows the hosts to declare their initial sequence numbers, receive windows, and other values.

The TCP handshake is implemented by exchanging empty segments with particular bits set in the TCP header. Specifically, recall from [Table 5.4](#tbl5-4) that the TCP header contains a 16-bit `flags` field. [Table 5.5](#tbl5-5) shows the internal structure of this field for a normal TCP segment. The `data offset` field denotes the length of the TCP header in terms of 32-bit words. The standard header, which contains no optional fields, has a length of five words; that constitutes the minimum value of this field. If optional fields are used, this value can increase to 15, which declares that TCP headers can be no longer than 60 bytes. The remainder of this section describes the use of the `ACK`, `SYN`, and `FIN` bits.

Index

0-3

4-9

10-15

Meaning

data  
offset

unused

`U   R   G`

`A   C   K`

`P   S   H`

`R   S   T`

`S   Y   N`

`F   I   N`

Value

`0`

`1`

`0`

`1`

`0`

`0`

`0`

`0`

`0`

`0`

`0`

`0`

`0`

`0`

`0`

`0`

Hex

`5`

`0`

`0`

`0`

Table 5.5: Structure of the 16-bit TCP flags field

![The TCP handshake](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.5.3.png)

Figure 5.3.4: The TCP handshake

The TCP handshake is initiated by a client application that calls the `connect()` socket function described in the previous chapter. The steps of the TCP handshake are shown in [Figure 5.3.4](#internethandshake) and consists of three segments, commonly referred to as `"SYN"`, `"SYN-ACK"`, and `"ACK"` because of the bits they set. Note that this figure uses `"ACKnum"` to refer to the 32-bit acknowledgement number and `"ACK"` to denote the bit in the 16-bit flags field. A client initiates a TCP connection by sending a SYN segment (`"synchronize"`) that contains its randomly chosen `SEQ` value, setting `SYN=1` and putting `0x5002` into the 16-bit flags field. The server acknowledges receiving the synchronization request (setting `SYN=1` and `ACK=1` to get `0x5012`) and indicates its own initial sequence number. The client then responds again with the `ACK=1` (`0x5010`). Note that all of these segments use an empty payload, but the sequence numbers are incremented as if they contained a single byte.

After completing the TCP handshake, the client and server share a logical connection. Both hosts know each other’s sequence numbers and initial receive window sizes. The OS on both hosts has established internal buffers, variables, and other data structures as needed. This internal state is maintained until the two parties close the connection. To close the connection, one host sends a segment with an empty payload and the _finish_ bit `FIN=1`; the other host responds with an `ACK=1` segment. These two segments are then repeated, but with the other party sending the `FIN=1` segment. As such, either side can sever the connection at any time.

🔍 Note

* * *

A common misunderstanding is that the `ACK` bit is used only in the TCP handshake or the `FIN-ACK` closing exchange. That is not correct. The `ACK` bit is set any time that the acknowledgement number is significant. That is, any time that the sender intends for the receiver to interpret this number as an acknowledgement of a previous message, the sender sets the `ACK` bit.

Recall the distinction between HTTP/1.0 and HTTP/1.1 discussed in the previous chapter. HTTP applications use TCP for their reliable transport layer. As such, any HTTP data exchange (such as accessing a web page, as well as retrieving the needed images and script files, require an initial TCP handshake to establish the connection. In HTTP/1.0, every object retrieved requires its own TCP connection session; even if all of the objects are stored on the same server, the seven segments (three for the handshake and four to close the connection) all must be performed, and both hosts’ OS must repeated set up and destroy internal buffers and data structures. With HTTP/1.1, the TCP handshake is only done once per server. The connection is maintained until the client application sends a segment containing the HTTP header `"Connection: close"`. The server and client then exchange segments to close the connection.

📜 Example 5.3.3

* * *

This example illustrates the flow of a TCP handshake to set up an HTTP request. As in [Example 5.3.2](#transtcpexample), the source and destination port numbers consist of the well-know port 80 and an ephemeral port. When the client (typically a web browser) establishes the connection, it starts by picking a random sequence number (4973 in this case) and using 0 as the acknowledgement number. The client sends an empty request (i.e., there is no payload and the message is just the TCP header) to the server as a `SYN` request.

The server responds with a `SYN-ACK` that sets both of these bits in the `flags` field. (Observe that the port numbers reverse in this middle message to indicate the direction switched to be “server to client.”) As with the `SYN` request, the server selects a random initial sequence number (627). The acknowledgement number here is the client’s sequence number incremented by 1 (4973 + 1). Finally, the client completes the handshake with an `ACK` message back to the server. This ACK message uses the incremented sequence number (4974) and the incremented acknowledgement of the server’s sequence number (628). At this point, the connection is established and both hosts have established the sequence and acknowledgement numbers for future messages.

`SYN` request (client to server)

`1388   0050   0000 136d   0000 0000   5002   1000   2e67   0000`

`source port = 5000 (0x1388)   destination port = 80 (0x0050)   sequence number = 4973 (0x136d)   acknowledgement number = 0   flags = SYN   receive window = 4096 (0x1000)   checksum   urgent data ptr   `

`SYN-ACK` response (server to client)

`0050   1388   0000 0273   0000 136e   5012   1000   2be3   0000`

`source port = 80 (0x0050)   destination port = 5000 (0x1388)   sequence number = 627 (0x273)   acknowledgement number = 4973 (0x136d)   flags = SYN and ACK   receive window = 4096 (0x1000)   checksum   urgent data ptr`

`ACK` response (client to server)

`1388   0050   0000 136e   0000 0274   5010   1000   2bf6   0000`

`source port = 5000 (0x1388)   destination port = 80 (0x0050)   sequence number = 4974 (0x136d)   acknowledgement number = 628 (0x274)   flags = ACK   receive window = 4096 (0x1000)   checksum   urgent data ptr`

///5.3.4. TCP Timeout and Packet Loss[¶]
----------------------------------------

The combination of the sequence number (`SEQ`) acknowledgement number (`ACK`) and the `checksum` forms a rudimentary error detection scheme to support reliable transport. If a host receives a segment with unexpected values in any of these fields, it can determine that _something_ has gone wrong in the data exchange. While the receiver may not be able to determine the exact cause of the problem, the incorrect information can provide some guidance:

> *   **Incorrect ACK**: The sender may be indicating that the previous segment was corrupted or not received. For example, assume a client sends `"Hello"` with `SEQ=10`; if the response contains `ACK=10`, the `"Hello"` segment has not been acknowledged and should be re-sent.
> *   **Incorrect SEQ**: A previous segment from the server may be delayed or lost, and the server is not aware of this fact. As an example, assume a client is expecting `SEQ=42` based on a previous segment from the server. If the next segment it receives has `SEQ=52`, then there is some data the client has not received. The missing data could be 10 one-byte segments or a single 10-byte segment; the client cannot know which is true and does not actually need to. Again, the client could re-send its last segment that contained `ACK=42`, effectively refusing to acknowledge the newer data until the lost data is recovered.
> *   **Incorrect checksum**: Some part of the TCP segment or the payload has been corrupted or removed in some way. As with the previous two cases, re-sending the last segment based on acknowledged data serves as a request for the server to repeat its own segment.

![The client resends the message if the acknowledgement is lost](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.5.4.png)

Figure 5.3.7: The client resends the message if the acknowledgement is lost

[Figure 5.3.7](#internetresend) illustrates the notion of [packet loss](#term-packet-loss), another common scenario involving TCP reliability. In this case, the server received and acknowledge the client’s segment, but the client did not receive the server’s response. As a result, the client re-sent the segment as a second attempt. From an omniscient perspective of all network traffic, this segment seems unnecessary: We can observe that the server did, in fact, receive the segment the first time. However, such a perspective is impossible; hosts can only observe the segments they send and receive. From the client’s perspective, it is possible that the first segment—not the reply—was lost.

This simple scenario illustrates a critical design question for reliable transport: how long must the client want before declaring a packet lost? Hosts need to wait long enough for packets to traverse the network physically but waiting too long for a lost packet delays the recovery process. To complicate matters further, network conditions can change, so determining the optimal amount of time to wait is a moving target.

The original TCP specification in RFC 793 proposed measuring the [round-trip time](#term-round-trip-time) (RTT), which is the amount of time that elapses between sending a segment and receiving the response. As the host sends and receives additional segments, it computes a [smoothed round-trip time](#term-smoothed-round-trip-time) (SRTT) as a rolling average of these delays.Based on an update in RFC 6298, in modern systems, the SRTT is initialized to the first RTT measurement. Once a new measurement (denoted as $R’$) is made, the SRTT is updated according to the following formula, with $\alpha$ typically set to 1/8.

$\large SRTT := (1 - \alpha) SRTT + \alpha R'$

📜 Example 5.3.4

* * *

To illustrate this calculation, assume that the first measurement is $R = 4ms$. This value becomes the initial SRTT. The table below shows the changes that would occur if the next three segments arrive with updated $R’$ values of 10 ms, 20 ms, and 2 ms, in that order.Note that all time units get rounded based on the granularity of the clock, which is assumed to be 1 ms in this case.

Old SRTT

$R'$

Updated SRTT

4

10

(7/8) * 4 + (1/8) * 10 = 4.75 _[rounded to 5]_

5

20

(7/8) * 5 + (1/8) * 20 = 6.875 _[rounded to 7]_

7

2

(7/8) * 7 + (1/8) * 2 = 6.375 _[rounded to 6]_

Table 5.6: SRTT calculations for a sequence of three received segments

The SRTT serves as an estimate for predicting the next RTT. The coefficients $\alpha$ and $(1 - \alpha)$ act as a relative weighting factor that influences how much the new observed value changes the estimate. Increasing the value of $\alpha$ places more weight on the most recent measurement, thus decreasing then influence of historical values. On the other hand, decreasing $\alpha$ has the opposite effect, relying more on the new RTT and less on the historical values.

The new SRTT value is used to update the retransmission timeout (RTO), which is the amount of time that the host will wait before declaring a lost packet. While RFC 793 proposed a formula for calculating RTO based just on SRTT, RFC 6298 updated this calculation. One problem with the original formulation is that it does not consider the impact of variance in RTT. For example, a system that repeatedly experiences an RTT of 20 ms every time should not be treated the same as one that alternates between 1 ms and 39 ms, despite both systems having the same average RTT. To compensate for this difference, RFC 6298 defines another rolling average, the RTT variation (RTTVAR), using $\beta = 1/4$:

$\large RTTVAR := (1 - \beta) RTTVAR + \beta | SRTT - R' |$

To make sense of this factor, consider the role of the absolute value $| SRTT – R’ |$. This value denotes the difference between the predicted RTT (SRTT) and the RTT that was actually observed ($R’$). If the prediction was perfect every time, then this absolute value would be 0. Then, each new segment would shrink the RTTVAR to become 3/4 of its previous value. However, if there is great variance from segment to segment, then this absolute value will be positive, potentially increasing RTTVAR. As before, $\beta$ acts as a weighting factor to determine how much influence the new variation should have compared with the historical values.

📜 Example 5.3.5

* * *

When calculating the new RTTVAR, the SRTT value used should be the old SRTT, not the updated value from [Example 5.3.4](#rttexample). The initial RTTVAR is set to $R/2$ when there are no previous measurements to use. Since the example above used $R = 4$ as the initially observed RTT, RTTVAR is initialized to 2. The table below illustrates the calculations for RTTVAR for the sequence of observations $R’$ from before, using the standard value of $\beta = 1/4$.

SRTT

$R'$

Old RTTVAR

Updated RTTVAR

4

10

2

(3/4) * 2 + (1/4) * | 4 – 10 | = 3

5

20

3

(3/4) * 3 + (1/4) * | 5 – 20 | = 6

7

2

6

(3/4) * 6 + (1/4) * | 7 – 2 | = 5.75 _[rounded to 6]_

Table 5.7: SRTT calculations for a sequence of three received segments

Based on these pieces, we can now define the RTO calculation. This formula uses a constant value $K = 4$, while G denotes the clock’s minimum granularity. That is, if the clock can only measure time to the accuracy of ms, it would not make sense to adjust the RTO by an unmeasurable fraction of a ms. The new RTO is calculated by adding the SRTT and the maximum of $G$ and $K * RTTVAR$.

$\large RTO := SRTT + max (G, K * RTTVAR)$

The RTO will always be the rolling average of the RTT plus a small amount of extra time for leeway. As the variance of RTT increases, the quantity added to RTT would increase. However, if the system experiences perfectly consistent RTT values, the $max(G, K * RTTVAR)$ would eventually shrink to the minimum clock granularity, providing the bare minimum of flexibility in deviation from the SRTT.

📜 Example 5.3.6

* * *

The table below shows the RTO calculations, assuming all values are in terms of ms and the clock granularity has a minimum value of 1 ms. To summarize the results in this table, the first measurement of $R = 4$ created the initial SRTT of 4 and RTTVAR of 2. By combining these values in the RTO calculation, the system would wait for up to 12 ms before declaring a packet loss. The next message arrived with an observed RTT of 10 ms, so this segment arrived before the timeout clock expires. This segment increased the RTO value to 17 ms, but the next segment missed the timeout by arriving with an RTT of 20 ms. After the 17 ms had elapsed, the host would have considered this a packet loss and re-sent the previous segment. After the retransmission, the observed RTT dropped to 2; this drop increased the variation RTTVAR to 6, which has the effect of raising the RTO to 31.

SRTT

RTTVAR

Updated RTO

4

2

4 + max(1, 4 * 2) = 4 + 8 = 12 _[initial value]_

5

3

5 + max(1, 4 * 3) = 5 + 12 = 17

7

6

7 + max(1, 4 * 6) = 7 + 24 = 31

6

6

6 + max(1, 4 * 6) = 6 + 24 = 30

Table 5.8: Combining the SRTT and RTTVAR values for the new RTO

It may seem odd to increase the RTO when the observed RTT actually dropped, but the drop creates a higher level of variance. Consequently, TCP interprets these measurements as an indication that the network is behaving unpredictably and grants the system more time before giving up on an expected segment. That is, since the expected and actual RTT values differ significantly, TCP has less confidence that replies will be received within a short amount of time. If the variance later decreases, TCP would shrink the RTO value, under the premise that the predictable behavior is likely to continue.

🔍 Note

* * *

Our description of TCP here has focused on the basic principle of reliable data transport, specifically focusing on the role of the TCP handshake and detecting packet loss. TCP is a significantly more complex protocol than we have presented, as it uses additional techniques to further increase the reliability of communication over the Internet. For instance, rather than acknowledging every message, TCP hosts can use _cumulative acknowledgement_, in which multiple messages can be acknowledged all at once. In addition, TCP _congestion control_ allows a host to detect delays in the network, throttling their sending rate to reduce traffic and prevent future delays. For a more complete discussion of these topics, we refer interested readers to textbooks that focus exclusively on the topic of networking, such as _Computer Networking: A Top-Down Approach (7th Edition)_ [[Kurose2016]](Bibliography.html#kurose2016) by Kurose and Ross.

In summary, TCP provides two key transport services that UDP does not: reliable data transport and flow control. These services are implemented by combining multiple pieces of information in the TCP segment header with observations of the network behavior. Hosts implement TCP reliability by tracking the sequence and acknowledgement numbers of each message, along with maintaining a rolling average of the expected wait times for segments. If a segment is not acknowledged within an expected time frame, hosts can implicitly request re-transmission by re-sending previous messages. Throughout this process, TCP hosts use the receive window as a means of flow control; by informing each other of the current capacity of their internal data buffers, the hosts are taking proactive steps to support reliability by preventing buffer overflows at the other end. These features make TCP the preferred choice for applications that depend on the successful transmission of all data, rather than compensating for lost data with reduced quality of service.

🔍 Note

* * *

Unlike the application layer, there is no universal, straightforward mechanism to access the transport, network, or link layer protocol headers. Accessing these layers requires the use of a raw socket, created by passing `SOCK_RAW` as the type parameter when calling `socket()`. The difficulty here is that many systems require root privileges to create raw sockets. Consequently, normal user-mode programs cannot easily gain access to these headers.

From a learning standpoint, there is also little value to demonstrating code for accessing these headers. The technique would be the same as we used in the section describing DNS. Specifically, the primary difference between that example and one that illustrates what UDP adds would involve increasing the size of the message buffer to contain the eight bytes of the UDP header at the beginning.

Readers interested in the intricacies of these lower layers should consider using a packet analyzer, such as `tcpdump` (`http://www.tcpdump.org/`) or Wireshark (`https://www.wireshark.org/`). These programs are freely available as open-source software. In addition, the `tcpdump` site provides a reusable library, `libpcap`, for building additional tools. Readers should exercise caution, however, as using these tools—particularly in a way that capture’s other people’s data—may violate the terms of use for access to the local network or may even be illegal based on one’s locality.



//5.4. Network Security Fundamentals[¶]
=======================================

The previous section suggested that the length and checksum calculations in UDP and TCP headers provide rudimentary error checking. This statement should not be read to mean that either UDP or TCP provide secure data exchange in any meaningful sense. While many people talk about _security_ in a general sense, within the domain of computer systems, security refers primarily to the ability to provide three services: [confidentiality](#term-confidentiality), [integrity](#term-integrity), and [availability](#term-availability).

Confidentiality is the service that a message can be read only by an entity that is authorized to do so. Consider the case where a person named Alice has a secret to share with Bob. Alice intends that Bob is the only other person authorized to learn this information. Confidentiality is the guarantee that this goal is achieved. Confidentiality would be broken if a third person, Eve [[1]](#f33), could learn the secret in some way.

Integrity is the service that the communication is protected against unauthorized modifications. Returning to Alice and Bob, consider a message from Alice that authorizes a bank transfer of \$1,000 to Bob’s bank account. Integrity ensures that the bank receives the correct message. Integrity would be violated if Mallory could corrupt the message to reduce the transfer down to \$1; Bob would not get the full expected transfer and would be rather upset. Mallory could also violate integrity by increasing the amount to \$100,000 in an attempt to remove all of the money from Alice’s account; Mallory may be motivated purely by the goal of harming Alice rather than intending to help Bob. Or Mallory may try to profit off the attack by changing the account number to redirect the transfer.

Availability requires that authorized users are not prevented from performing authorized actions, whether that action involves reading or modifying data. In the first case, Mallory may break the guarantee of availability by preventing Bob from receiving the secret message that Alice sent. In the second case, Mallory might launch a [denial-of-service](#term-denial-of-service) attack against the bank to prevent it from receiving Alice’s message authorizing the transfer.

Neither TCP nor UDP provide any of these three guarantees. The contents of the application-layer payload are not hidden or scrambled in any way, so there is no guarantee of confidentiality. The checksum calculation can be trivially manipulated to break integrity without detection: change the order of the bytes in the segment. Checksums are essentially just addition, and it is a fundamental tenet of basic arithmetic that 3 + 5 and 5 + 3 produce the same result. Although TCP aims to provide reliable data transport, its services do not meet the standard required for availability. TCP’s reliability consists of trying again when an expected segment does not arrive; this second (and third…) attempt can easily fail again in a successful denial-of-service. Consequently, neither TCP nor UDP guarantee any of these security services.

[Cryptography](#term-cryptography) is a specialized field that uses a variety of techniques—typically advanced mathematical concepts—to create secure communication channels. This section provides a very brief overview of two cryptographic techniques that are relevant to the current discussion. [Encryption](#term-encryption) involves encoding a message (the [plaintext](#term-plaintext)) into random-looking binary string (the [ciphertext](#term-ciphertext)) in a way that allows the process to be reversed. Encryption, then, provides confidentiality by hiding the secret message in an unreadable way. [Cryptographic hash functions](#term-cryptographic-hash-function), on the other hand, encode the message into a fixed-length ciphertext that does not allow the plaintext to be recovered. Cryptographic hash functions provide integrity, as they are designed to prevent adversaries from finding a corrupted plaintext that produces the same ciphertext.

Neither of these techniques guarantees availability, which is extremely difficult to provide. To illustrate an availability attack, recall the three steps of the TCP handshake. In a [SYN flood](#term-syn-flood), an adversary repeatedly sends SYN packets to initiate the handshake and open a connection; as each of these arrive at the server, the server allocates internal data structures to handle the request. However, the adversary never completes the handshake for any of these requests or closes the connection. Consequently, the server eventually runs out of memory or other resources, preventing legitimate users from connecting. Detecting these types of attacks is not straightforward, making availability a challenge. To make matters worse, availability is often at odds with confidentiality and integrity. Providing confidentiality or integrity can make availability more difficult, and vice versa. As our goal is to focus on the basic principles of security, we limit our discussion to the basics of encryption and cryptographic hash functions; interested readers should explore the recommended readings for more information.

///5.4.1. Symmetric Key Encryption[¶]
-------------------------------------

The process of encryption involves combining a plaintext message with a [key](#term-cryptographic-key) to create a ciphertext that looks like an unreadable series of random bits. In a perfect encryption system [[2]](#f34), when Alice sends a ciphertext to Bob, Eve learns nothing about the plaintext. However, Bob will be able to decrypt the message back into its plaintext form with the corresponding key.

Encryption and decryption are often written in a formal notation, using $m$ and $c$ to refer to the plaintext message and ciphertext, along with $e_K$ and $d_K$ to denote encryption and decryption functions using a particular key. This notation leads to the following generic equations:

$\large c = e_K(m) \hspace{1.5in} m = d_K(c)$

There are two distinct styles of encryption that are in common use. In [symmetric key](#term-symmetric-key-cryptography) encryption, the same key is required for both encryption and decryption. That is, Alice and Bob must have an agreement before exchanging the ciphertext of what the key is. In contrast, [public key](#term-public-key-cryptography) encryption uses two distinct keys for encryption and decryption. In this [key pair](#term-key-pair), one (public) key can be made publicly available while the other (private) must remain secret. For instance, Alice can use Bob’s public key to encrypt a message, allowing only Bob to decrypt it using his private key. On the other hand, Alice can encrypt a message using her private key, allowing Bob or anyone else to decrypt it using Alice’s public key. This latter approach is the basic idea behind [cryptographic signatures](#term-cryptographic-signature), as it provides a way to confirm that a message truly came from Alice; if the message can be decrypted with Alice’s public key, it must have been encrypted with her private key (that only she knows).

The most commonly used symmetric key cipher in modern practice is the [Advanced Encryption Standard](#term-advanced-encryption-standard) (AES). [National Institute of Standards and Technologies](#term-national-institute-of-standards-and-technologies) (NIST)—a part of the U.S. federal government that defines cryptographic and other technical standards—selected AES as a replacement for older symmetric key ciphers when those were found to have weaknesses. AES uses a technique called the _Rijndael block cipher_, breaking the plaintext into a sequence of 128-bit blocks that are each encrypted. AES has three variations based on the length of the key (128, 192, or 256 bits long).

![The iterative structure of AES](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.5.5.png)

Figure 5.4.1: The iterative structure of AES

Rather than a single procedure, AES encryption is an iterative process that uses several keys—one per round of the iteration. [Figure 5.4.1](NetSec.html#netsecaes) illustrates the overall structure of AES. The original 128-, 192-, or 256-bit key is first used as the input to a `KeyExpansion` algorithm, generating a [key schedule](#term-key-schedule) of multiple round keys. Then, for each 128-bit block of the plaintext, AES arranges the bits into a 4x4 matrix of bytes, which represents the state of the block. Within each round, the round key is combined (`AddRoundKey`) with the _state_ by applying the bitwise XOR operation. Next, the `SubBytes` step replaces the contents by looking each byte up in a substitution box (S-box); for instance, any byte that contains the value 13 might be replaced with the value 275. The `ShiftRows` step rotates each row by a different number of places to the left (e.g., row 0 stays intact, row 1 bytes shift one place to the left, and so on). The `MixColumns` applies a linear transformation that mixes the placement of bits within a single column. To decrypt an AES ciphertext, the same key schedule is used, but in reverse. The steps of each round are also reversed.

The strength of symmetric key algorithms like AES derives from the strong probabilistic guarantees associated created by non-linear operations. For example, consider the XOR operation. If the output of a one-bit XOR operation is 1, there are two equally likely possibilities: either the plaintext bit is 0 and the key is 1, or the plaintext is 1 and the key is 0. This approach is the basis of a [one-time pad](#term-one-time-pad), in which all of the bits of a plaintext are XORed with a random key that is the same length as the input. Under many ideal assumptions, one-time pads achieve [information theoretic security](#term-information-theoretic-security), which means that an adversary trying to break the code can only guess. Every bit of the key is equally likely to be a 1 or a 0. Regardless of how long an adversary’s program runs, the probability of each bit never changes, and the program cannot make progress toward breaking the code. In essence, one-time pads are perfectly secure in theory; in practice, they are not, because the plaintext messages cannot achieve the level of randomness required. For example, assume that Eve knows Alice sent either “Hello” or “Goodbye” with a one-time pad; Eve does not need to know the key, because she can determine the message by examining just the length of the ciphertext. Hence, practical symmetric key approaches like AES combine XOR with other operations that further obscure the message.

The operations used in AES are very fast, measured on the scale of CPU cycles. The full encryption process for a single block requires 10, 12, or 14 rounds (based on the key size). Consequently, AES can encrypt a significant amount of data very efficiently. The only problem is how should Alice and Bob determine what key to use, particularly if they have never spoken before? For instance, when you use a web browser to visit your school’s or bank’s website for the first time, the browser and server need to determine a key to use. As a complicating factor, depending on more advanced characteristics of the algorithm, it may be necessary to ensure the key has not been used before.

📦 C OpenSSL library functions – `<openssl/evp.h>`

* * *

This is not a standard library, so you may need to use the -I and -L compiler flags.

`EVP_CIPHER_CTX *EVP_CIPHER_CTX_new (void);`

Creates a new context for encryption or decryption.

`void EVP_CIPHER_CTX_free (EVP_CIPHER_CTX *ctx);`

Frees up resources allocated to a context.

`int EVP_EncryptInit (EVP_CIPHER_CTX *ctx, const EVP_CIPHER *cipher, const unsigned char *key, const unsigned char *iv);`

Initializes the context to use a cipher given a key and initialization vector.

`int EVP_EncryptUpdate (EVP_CIPHER_CTX *ctx, unsigned char *out, int *outl, const unsigned char *in, int inl);`

Updates the encryption computation for the plaintext chunk in, writing the results to out.

`int EVP_EncryptFinal (EVP_CIPHER_CTX *ctx, unsigned char *out, int *outl);`

Writes the final ciphertext into the buffer out.

[Code Listing 5.2](#cl5-2) shows the basic structure of initializing an encryption context using the OpenSSL `crypto` library. [[3]](#f35) All symmetric key ciphers require specifying a _cipher mode_. In this example, we are using cipher block chaining (CBC), a very widely used mode.


```cpp
/* Code Listing 5.2:
   Create an encryption context for AES-256 in CBC mode
 */

/* Create REALLY BAD 256-bit key and initialization vectors. For 
   AES-256, key must be 256 bits (32 bytes) and initialization
   vector is 128 bits (16 bytes). */
unsigned char *key = (unsigned char *) "aaaabbbbccccddddeeeeffffgggghhhh";
unsigned char *iv = (unsigned char *) "iiiijjjjkkkkllll";

/* Set up the context and initialize it */
EVP_CIPHER_CTX *ctx = EVP_CIPHER_CTX_new();
EVP_EncryptInit (ctx, EVP_aes_256_cbc (), key, iv);
```

[Code Listing 5.3](#cl5-3) illustrates how to encrypt a plaintext message using the initialized context. `EVP_EncryptUpdate()` can be called repeatedly as needed for long messages. For instance, if the plaintext consisted of a large file stored on disk, the file could be read into memory in small chunks at a time; each chunk would then be passed to `EVP_EncryptUpdate()` accordingly. [Code Listing 5.3](#cl5-3) only requires a single call on line 12, as the `plaintext` is short. Line 18 writes the final result into the allocated buffer `ciphertext` buffer. Note that this data is not in a printable format, so an appropriate encoding (such as Base64) will need to be used for many applications.


```cpp
/* Code Listing 5.3:
   Encrypting a string using the AES context from Code Listing 5.1
 */

char *plaintext = "Hello world";
int plain_length = 12;
unsigned char ciphertext[128];
memset (ciphertext, 0, sizeof (ciphertext));
int length = 0, cipher_length = 0;

/* Update the encryption context based on the plaintext */
EVP_EncryptUpdate (ctx, ciphertext, &length, (unsigned char *)plaintext,
                   plain_length);
cipher_length = length;

/* Finalize the context, which may require appending some data to
   the end of the ciphertext buffer */
EVP_EncryptFinal (ctx, ciphertext + length, &length);
cipher_length += length;
EVP_CIPHER_CTX_free (ctx);

printf ("Plaintext is:\n");
BIO_dump_fp (stdout, (const char *)plaintext, plain_length);
printf ("Ciphertext is:\n");
BIO_dump_fp (stdout, (const char *)ciphertext, cipher_length);
```

Assuming OpenSSL is installed in the directories shown, we can combine [Code Listings 5.2](#cl5-2) and [5.3](#cl5-3) into a single `encrypt.c` file, then can compile and run this program on macOS as follows (Linux should not need the `-L` and `-I` flags, as OpenSSL is typically installed in the standard locations checked by the compiler). Lines 23 and 25 use the OpenSSL `BIO_dump_fp()` function to write the `plaintext` and `ciphertext` buffers out to `STDOUT` in a format similar to `hexdump`.

```sh
$ gcc -o encrypt encrypt.c -L/usr/local/opt/openssl@1.1/lib \
> -I/usr/local/opt/openssl@1.1/include -lcrypto
$ ./encrypt
Plaintext is:
0000 - 48 65 6c 6c 6f 20 77 6f-72 6c 64 00               Hello world.
Ciphertext is:
0000 - a8 46 db 10 77 55 c8 91-18 19 22 b4 73 1a d2 fc   .F..wU....".s...
```

Decrypting messages uses the same structure as [Code Listings 5.2](#cl5-2) and [5.3](#cl5-3). The only difference is the function names change accordingly, such as `EVP_DecryptUpdate()`. The order of the arguments in each of these functions is identical to the encryption versions.

🐞🐛🐌 Bug Warning

* * *

[Code Listings 5.2](#cl5-2) and [5.3](#cl5-3) omitted error checking on the calls to the OpenSSL library functions for simplicity. In practice, every OpenSSL function’s return type should be checked to ensure that the encryption or decryption is working successfully. Failure to do so could cause a segmentation fault and crash the program.

///5.4.2. Public Key Encryption[¶]
----------------------------------

Public key encryption provides a solution to the problem of [key exchange](#term-key-exchange-protocol). With public key encryption, one of the hosts can generate a fresh (unused) AES key, then encrypt the key with the other host’s public key. That is, Alice might generate a [session key](#term-session-key) $K$ to use for AES, then send Bob the message $c = e_P (K)$, using Bob’s public key $P$. Bob can then use his secret key $S$:

$\large d_S(c) = d_S(e_P(K)) = K$

Unlike symmetric key, public key encryption is not derived from operations like XOR that achieve information theoretic security. Instead, public key systems strive for [semantic security](#term-semantic-security) guarantees that reduce the adversary to having a _negligible_ probability of successfully guessing the key or plaintext. The notion of negligible probability means that the encryption procedure leaks a small amount of information (i.e., it provides hints to the adversary), but this leakage is so miniscule that a successful guess in a reasonable amount of time is absurdly unlikely. Given enough time (think in terms of billions of years), a successful attack is possible, but the adversary would be long dead before that point. Semantic security is created by a number of mathematical problems that are known to be difficult to solve. Some of these schemes involve advanced number theory concepts, such as elliptic curves, prime number factorization, and discrete logarithms.

To illustrate the basic idea, consider the premise of the [RSA](#term-rsa-cryptosystem) (Rivest-Shamir-Adleman) algorithm. To prepare for RSA encryption, the application chooses two large prime numbers $p$ and $q$; $p$ and $q$ remain secret, while their product $n = pq$ is made public. The public and private keys, denoted $e$ and $d$ in this scenario, are chosen to have a particular property. For any number $m$ (the plaintext message):

$\large (m^e)^d \equiv (m^d)^e \equiv m \mbox{ mod } n$

That is, when e and d are used as two consecutive exponents and the result is calculated modulo the (public) $n$ value, the original value $m$ is returned. In effect, $e$ and $d$ cancel each other out in modular exponentiation. Note that both $n$ and $e$ can be made public; an adversary with no knowledge of $d$ is cannot efficiently compute $m$ after observing the ciphertext $c = m^e \mod n$. Additionally, observing the ciphertext $c’ = m^d \mod n$ does not immediately reveal the value of $d$, even if both $m$ and $n$ are known.

📜 Example 5.4.1

* * *

As a simple example of the basic functioning of RSA, consider $n = 55$ ($p = 5, q = 11$). Based on these parameters, we can let $e = 3$ and $d = 7$ (the rationale for this choice is beyond the current scope of this text). If we choose our _message_ $m = 6$, then we can compute $m^e = 63 = 216 ≡ 51 \mod 55$. Then, $51^{7} = 897,410,677,851$, which is equivalent to $6 \mod 55$. Hence, the values $e = 3$ and $d = 7$ are inverses for modular exponentiation, given $n = 55$. We could arbitrarily consider one of these values as our private key and one to release as a public key, although their size is absurdly small. (This example could be impelemtned with 8-bit unsigned integers, whereas values in real implementations of RSA would be on the scale of 4096-bit unsigned integers.)

For completeness, **this particular form of RSA should never be used in practice**. Just as AES required more operations than just XOR, making RSA practical requires more work than just modular exponentiation to achieve semantic security. Our goal here is to provide an illustrative example of the key idea of public key cryptography: Posting one of the keys publicly allows anyone to encrypt a message that can only be read by someone with the corresponding private key. In contrast to symmetric key, public key encryption is extremely slow, and it is not practical for encrypting large amounts of data; however, public key encryption can protect small pieces of data, such as 128-bit session keys, to allow hosts to set up a secure communication channel.

///5.4.3. Cryptographic Hash Functions[¶]
-----------------------------------------

Both symmetric key and public key encryption focus on providing confidentiality guarantees for security. Once a message has been encrypted into ciphertext, it can only be read by someone who has the corresponding decryption key. To provide integrity, though, we need a different technique. Cryptographic hash functions serve this purpose by taking an arbitrary-length input message and running it through a compression algorithm to produce a fixed-length result.

In essence, a cryptographic hash function is a one-way function to map any input to an unpredictable bit string. Readers with a background in data structures may be familiar with the concept of hash functions as used in hash tables; for instance, a simple hash function would simply apply a modulus operation to an input value. Cryptographic hash functions are very different. Cryptographic hash functions are designed to be intentionally slow, as the goal is to avoid the discovery of [collisions](#term-collision), two inputs that share the same hash. The [avalanche effect](#term-avalanche-effect) contributes to this goal, as flipping a single bit in the input produces a very different final result.

![One round of SHA-2 compression. Image source: Wikipedia (recreated)](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.5.6.png)

Figure 5.4.5: One round of SHA-2 compression. Image source: Wikipedia (recreated)

[Figure 5.4.5](NetSec.html#netsecsha) illustrates the compression technique used in the SHA-2, one currently recommended family of cryptographic hash functions. The message to be hashed is broken up into a series of chunks denoted as $W_t$. that are combined with a series of constants $K_t$. The running hash value—which might be 224, 256, 384, or 512 bits in length—is broken into eight smaller blocks of equal length, labeled as `A-H` at the top. Some blocks (`A-C` and `E-G`) are shifted to different locations (`B-D` and `F-H`) in the output. The output `A` and `E` blocks are the result of combining the eight input blocks through various operations, including XOR, bit shifting, logical operators, and one’s complement addition. The result is then fed back in to the compression algorithm; this process is repeated either 64 or 80 times, depending on the SHA-2 variant used.

Note that the compression described above only works on a single block (e.g., 256 bits) of data. The full SHA-2 procedure breaks the original, arbitrary-length message into a sequence of these blocks. At each step, the output from compressing the current block is combined with the next input block, creating a chained structure known as a [Merkle-Damgård construction](#term-merkle-damgard-construction). The Merkle-Damgård construction is susceptible to an attack that involves extending the hash calculation with more data. That is, once the output of the cryptographic hash is computed, the adversary can continue to append more data until a collision is found.

![MAC-then-Encrypt structure used in TLS. Image source: Wikipedia (recreated)](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.5.7.png)

Figure 5.4.6: MAC-then-Encrypt structure used in TLS. Image source: Wikipedia (recreated)

At the time of this writing, SHA-2 is still recommended for use (unlike SHA-0 and SHA-1), as there is no known practical attack. However, given the susceptibility of the Merkle-Damgård construction to this type of attack, NIST has also released [SHA-3](#term-sha-3). SHA-3 replaces the Merkle-Damgård construction with a different [sponge construction](#term-sponge-construction). NIST recommends SHA-3 as an available alternative—not a replacement—for SHA-2, and both remain in use.

On their own, cryptographic hash functions provide a basic mechanism for integrity. If a message $m$ is known and someone claims that the hash value $H(m)$ is the correct hash value, the truth of this statement can be easily confirmed. However, that construction alone does not allow anyone to claim to be the author if $m$. Instead, the cryptographic hash function must combine $m$ with a key. [Figure 5.4.6](NetSec.html#netsecmte) shows one technique for doing this, known as [MAC-then-Encrypt](#term-mac-then-encrypt) (MtE). The first step is to use a [keyed cryptographic hash function](#term-keyed-cryptographic-hash-function) that combines the plaintext with a secret key to compute a [message authentication code](#term-message-authentication-code) (MAC). The MAC is then appended to the plaintext, and the resulting message is encrypted. MtE provides guarantees of both confidentiality and integrity, but it also provides a claim of [authenticity](#term-authenticity) about authorship—only someone with the key could generate the MAC and ciphertext.

📦 C OpenSSL library functions – `<openssl/sha.h>`

* * *

This is not a standard library, so you may need to use the -I and -L compiler flags.

`int SHA256_Init (SHA256_CTX *c);`

Initializes a new context for SHA-256 cryptographic hash calculations.

`int SHA256_Update (SHA256_CTX *c, const void *data, size_t len);`

Updates the internal context for a block of data being hashed.

`int SHA256_Final (unsigned char *md, SHA256_CTX *c);`

Finalized the hash calculation into a message digest md.

`unsigned char *SHA256 (const unsigned char *d, size_t n, unsigned char *md);`

Performs all steps to hash the data in one function call. The last parameter points to a buffer used for temporary storage.

[Code Listing 5.4](#cl5-4) demonstrates how to use the OpenSSL functions for SHA-256. In this scenario, we are assuming that the data to be hashed has been broken into several fixed-size chunks. (If the data resides in a single block of memory, the `SHA256()` function performs the initialization, calculation, and finalization of the hash value in a single call.) The context value keeps track of the internal state of the hash over repeated calls to `SHA256_Update()`.

```cpp
/* Code Listing 5.4:
   Compute a single cryptographic hash over a sequence of fixed-size blocks
 */

SHA256_CTX context;
SHA256_Init (&context);
/* Assume we have several blocks that are all the same size;
   update the hash value accordingly */
for (size_t i = 0; i < number_of_blocks; i++)
  SHA256_Update (&context, &blocks[i], blocksize);
/* Write the hash into a byte array */
uint8_t hash[SHA256_DIGEST_LENGTH];
SHA256_Final (hash, &context);
```

///5.4.4. Transport-Layer Security (TLS)[¶]
-------------------------------------------

As we have noted previously, neither TCP nor UDP provide any security guarantees. This service is provided, instead, by [Transport-Layer Security](#term-transport-layer-security) (TLS). When an application requires secure communication (denoted by attaching an “s” to a protocol name to get HTTPS, SFTP, or IMAPS), TLS is the source of that security. TLS is the successor of the secure sockets layer (SSL); due to security vulnerabilities, SSL has been prohibited by NIST since 2015 and has been removed from common applications such as web browsers. The specification for TLS 1.2 is defined in RFC 5246, while TLS 1.3 is defined in RFC 8446.

TLS works collaboratively with TCP to create secure end-to-end communication at the application layer. Consider the security needs of such a communication channel. The payload of the TCP segment needs both confidentiality and integrity, whereas the header cannot be kept confidential. To be precise, if the TCP header is encrypted, the receiving OS could not determine the port number and, as a result, the intended recipient process. Consequently, the TCP headers must remain unencrypted, but they still need to be protected from tampering.

![The TCP and TLS handshakes](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.5.8.png)

Figure 5.4.8: The TCP and TLS handshakes

[Figure 5.4.8](NetSec.html#netsectls) shows the relationship of TCP and TLS. For example, consider a web browser navigating to `https://www.example.com` instead of the standard `http://www.example.com`. Recall that one difference between these two is that the HTTP request would be delivered to port 80 at the server, whereas HTTPS would go to port 443. In both cases, the client initiates a TCP handshake to establish the connection. After the ACK message, with standard HTTP, the client would be expected to send an unencrypted HTTP request. With HTTPS, however, the client’s next message must be a `ClientHello` that initiates a [TLS handshake](#term-tls-handshake). The purpose of the TLS handshake is for the two hosts to exchange information to establish a symmetric key for encrypting messages to each other. The server replies with a `ServerHello` message, containing the server’s `Certificate`, `ServerKeyExchange`, and `CertiticateRequest`. The client responds with similar data until both hosts send a `Finished` message to indicate the TLS session has begun.

The Certificate portion of the TLS handshake messages consists of an X.509 certificate, a data structure that contains that host’s public key. The certificate, especially on the server side, is typically cryptographically signed by a certificate authority (CA). That is, a CA is a company or governmental organization that has used a keyed cryptographic hash function to sign [[4]](#f36) the digital equivalent of the message “This CA confirms Company X has public key 12345.” The public key in the Certificate is then used to trade the ServerKeyExchange and ClientKeyExchange messages. These messages encode information for a key exchange protocol that allows the two parties to agree on a session key. At the end of the TLS handshake, both hosts have agreed on the session key, and they use it to encrypt the payloads of the application-layer messages.

To perform the key exchange, TLS supports three approaches: a pre-shared key (PSK), Diffie-Hellman exchange (DHE), or a combination of the two (PSK with DHE). In PSK, the two hosts have been configured to use the same key. This approach is used for organizations that are under common system administration, but it does not work for two hosts that haveno prior contact. Hosts with no prior contact use the DHE approach. Without delving into the details, consider the case where the two hosts determine a common message. The two hosts could come to an agreement on the key by encrypting the same message twice:

$\large K = e_A(e_B(m)) = e_B(e_A(m))$

That is, assume Alice has a private key $A$ and Bob’s private key is $B$. Because the order of exponents (if this used RSA) does not matter, if Alice encrypts $m$ and Bob encrypts the result, that value would be the same as if Bob encrypted $m$ first and Alice encrypted the result. Since no one else knows the private keys, Eve cannot compute K even if she knows $m$, $e_A(m)$, and $e_B(m)$.

To protect the TCP headers, TLS computes a MAC of the original TCP header and attaches the MAC as an optional TCP field; the MAC can be computed using a number of hash functions, though SHA-2 is perhaps the most common. Attaching the MAC provides an integrity check for both hosts to know that no third party has tampered with the segment by replacing the payload contents. TLS 1.2 protected the application-layer payload using SHA-2 and AES in the MtE technique shown in [Figure 5.4.8](NetSec.html#netsectls). Because MtE is not secure against attacks that reuse the ciphertext (e.g., including an old payload), TLS 1.3 has removed support for MtE and replaced it with [authenticated encryption with attached data](#term-authenticated-encryption-with-attached-data) (AEAD), a technique that does the MAC and encryption in parallel in a way that avoids the ciphertext reuse threat.

In summary, TLS provides security guarantees that TCP and UDP cannot. By encrypting the application-layer payload, its contents are hidden from view while the segment traverses through the Internet. Doing so allows applications to send and receive sensitive data, such as passwords or credit card numbers; this information is only observed by the processes at the two end hosts. TLS also provides assurance that the data has not been tampered with or corrupted without detection. Ultimately, the application-layer process has no awareness that these additional calculations are being done, as TLS—like TCP—operates within the OS, returning the results to the applications after they have already been decrypted and their integrity confirmed.

///5.4.5. TLS in Practice: HTTPS[¶]
-----------------------------------

In the previous chapter, we used the `netcat` utility to communicate with an HTTP server. Recall that `netcat` takes two command-line arguments: a hostname and a port number. While standard HTTP servers listen for incoming requests on port 80, HTTPS uses port 443. Given that HTTPS communicates using the same protocol as HTTP—the only difference is that HTTPS runs on top of TLS—it is tempting to use the same approach as before, using the different port number:

$ netcat www.example.com 443
Warning: Inverse name lookup failed for \`93.184.216.34'
www.example.com [93.184.216.34] 443 (https) open
GET / HTTP/1.0
read(net): Connection reset by peer

What is not immediately obvious in print form is that the last line of this output is produced by netcat immediately after the `GET` line is entered. That is, the protocol is aborted immediately by the server and the connection is shut down. The reason for this connection loss is that port 443 is expecting a TLS handshake before any HTTP messages can be processed. Since `netcat` sends all messages in an unencrypted format, it cannot be used for TLS sessions. Instead, we can use the `openssl` utility (`https://www.openssl.org`) for this purpose. [[5]](#f37) The `openssl` equivalent of the netcat command from above is as follows:

```sh
$ openssl s_client -crlf -connect www.example.com:443
[...almost 100 lines of immediate output...]
```

The first parameter indicates the openssl command to execute, as the same utility can be used to run a TLS server, generate cryptographic keys, or many other functions. The `-crlf` option tells `openssl` to use both `'\r\n'` when the user hits the enter key, and the `-connect` option is used to specific the `host:port` combination. Once the `openssl` command is entered, the utility establishes a TCP connection followed by the TLS handshake. This procedure creates almost 100 lines of output, including specific information about the server’s certificate and how much data was exchanged during the handshake. The last part of this output contains information such as the following lines:

    SSL-Session:
        Protocol  : TLSv1.2
        Cipher    : ECDHE-RSA-AES128-GCM-SHA256

These lines indicate that the client and server have agreed to use TLS version 1.2. The key exchange protocol is the elliptic-curve version of Diffie-Hellman, using RSA for signatures. The session key will encrypt the application-layer payload using a variant of 128-bit AES in Galois/Counter Mode (GCM). Any cryptographic hashes will be computed using SHA-256, a 256-bit version of SHA-2. Eventually, the handshake messages end and the remainder of the session proceeds as follows:

    [...TLS handshake messages before this point...]
    GET / HTTP/1.1
    Host: www.example.com
    Connection: close

    HTTP/1.1 200 OK
    [...remainder of HTTP response and HTML code...]
    closed

As before, we can enter type the HTTP messages in plaintext. In this case, we are using HTTP/1.1, which would maintain a persistent connection. When using HTTP/1.1, the last request should include the header line `Connection: close`, which closes the TCP connection after processing the request. The HTML ends with the message `closed`, which indicates that the connection has been successfully terminated.

The `openssl` utility encrypts and decrypts all messages without any additional work done by the user. This behavior highlights a key strength of layered architectures: Application-layer protocols should not have to change to adjust for transport-layer variations. Consequently, applications can be written as before. After using `connect()` to create the TCP connection, the application can invoke function calls using the OpenSSL library to perform the handshake then proceed as normal. To be clear, this procedure is not trivial, as the developer must configure a number of options, load valid certificates, and so on. However, once this configuration is done, all messages sent to and received from the server will be encrypted.

[[1]](#id1)

The literature of cryptography commonly uses particular names to denote certain abilities. While Alice and Bob are generic benevolent characters, the name Eve (“eavesdropper”) denotes someone attempting to learn a secret through passive observation. Similarly, Mallory (“malicious”) implies an adversary who strives to cause harm, whether through action or inaction.

[[2]](#id2)

In practice, no encryption system is perfect. Some have subtle flaws in their algorithms that create statistical patterns that can reveal hints. Others have implementation errors that leak information by taking longer or producing more heat depending on whether a particular bit of the key is a 1 or a 0. Worse still, users of the system might make errors, such as improperly re-using a key or plaintext message. During World War II, German operators re-used certain plaintext phrases, letting Polish and British cryptographers at Bletchley Park to break messages encrypted with Enigma on a daily basis.

[[3]](#id3)

OpenSSL is available at `https://openssl.org` and can generally be installed through OS package managers. Documentation and code samples are available through the Wiki at `https://wiki.openssl.org/`.

[[4]](#id10)

It may seem circular to use a public key to sign a certificate declaring what someone else’s public key is. That is, how do we know the signature on the certificate is valid? Modern web browser and similar applications come pre-installed with the public keys for several pre-defined trusted CAs that can be used to confirm the signatures of certificates that company or organization has signed.

[[5]](#id11)

Recall that SSL was the predecessor of TLS. OpenSSL was created as an open-source SSL library; although SSL has been renamed to TLS, the library retains the OpenSSL name given that this name is widely recognized.



//5.5. Internet Layer: IP[¶]
============================

Transport-layer protocols, such as TCP and UDP, focus on the logical end-to-end communication between processes. Consequently, these protocols still avoid the fact that network sockets are exchanging data between different hosts. Instead, the transport layer still behaves very much like local forms of IPC. Internet layer protocols begin to remove this layer of abstraction. That is, the Internet layer extends the process-to-process structure of the transport layer with host-to-host communication.

The Internet layer consists of two interacting components. The [data plane](#term-data-plane) provides the overall structure of the network, assigning addresses to hosts. The [Internet Protocol](#term-internet-protocol) (IP) is the primary protocol for the data plane; IP identifies the logical location of a host within the Internet, both locally and across network boundaries. The [control plane](#term-control-plane) protocols define the routing paths that messages take through the network from a high level. These protocols include [Open Shortest Path First](#term-open-shortest-path-first) (OSPF), [Routing Information Protocol](#term-routing-information-protocol) (RIP), and [Border Gateway Protocol](#term-border-gateway-protocol) (BGP). We will provide a brief overview of the key features of the control plane protocols, but we will not be discussing the details of their operation.

///5.5.1. IP Addresses and Subnets[¶]
-------------------------------------

In the previous chapter, we introduced the notion of an IP address as an identifier that can be used to locate a host within the network. IP version 4 (IPv4), defined in RFC 791, specifies addresses as 32-bit values. For readability purposes, IPv4 addresses are typically written in dotted-decimal form; for example, the address `0x01020304` would be more commonly written as 1.2.3.4. The 32-bit length creates a serious problem for IPv4, as this size means there are only 4,294,967,296 possible IPv4 addresses. While the exact number of devices connected to the Internet is impossible to determine, all estimates are significantly higher than this limit of 4 billion; a more accurate estimate would be on the order of 50 billion.

As the number of connected devices grew, one technique to compensate for the limits of IPv4 was to re-use addresses by dividing the Internet into multiple [subnets](#term-subnet), particularly private subnets. Private subnets are defined by a range of IP addresses that cannot be reached from hosts outside of that subnet. Subnets can be denoted using the [Classless Inter-Domain Routing](#term-classless-inter-domain-routing) (CIDR) notation, in which a dotted-decimal value is appended with /n, where n is the length of the bit prefix for that subnet. For instance, 1.2.3.0/24 would denote the range of addresses from 1.2.3.0 up to 1.2.3.255, because the first 24 bits must match the [subnet mask](#term-subnet-mask), which is the bitmask (255.255.255.0 in this case) that can be applied to any address in the subnet.

Readers who have set up a home wireless router may be familiar with the 192.168.0.0/16 subnet. This CIDR notation means the all devices that connect to the wireless router are assigned IP addresses that range from 192.168.0.0 to 192.168.255.255; the first 16 bits are designated to be identical for all devices. The IPv4 specification declares that 192.168.0.0/16 is a range of addresses that can only be used for a private subnet; similarly, 10.0.0.0/8 and 172.16.0.0/12 are also reserved. Choosing between the three ranges depends on the number of devices that will be connected. The 192.168.0.0/16 subnet allows 16 bits to vary, so the subnet can host $2^{16}$ (65,536) possible devices; this size is adequate for a home router. However, large organizations with thousands of workers may require more than that. Using 172.16.0.0/12 allows for up to $2^{20}$ (1,048,576) devices because 24 bits can vary; 10.0.0.0/8 supports up to $2^{24}$ (16,777,216) devices.

It may seem odd to allow the same IP address to be reused repeatedly. Specifically, if two devices share the same IP address, it may seem that there is no way to tell them apart. [Figure 5.5.1](#netprivate) illustrates how this situation can be resolved. In this figure, Alice and Bob both have home networks connected to the same ISP. In both of their homes, Alice and Bob have wireless routers that assign addresses to devices that either connect wirelessly (such as a laptop) or via a cable (such as a desktop). Note that both networks have a device with the IP address 192.168.1.2. These IP addresses were assigned by the router using DHCP when the device joined the network. As these subnet ranges are private, Alice’s devices and Bob’s devices have no knowledge of each other. Alice’s phone (192.168.1.5) cannot communicate directly with Bob’s laptop at address 192.168.1.7. To Alice’s devices, Bob’s devices simply do not exist.

![Two private subnets with common IPv4 internal addresses](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.5.9.png)

Figure 5.5.1: Two private subnets with common IPv4 internal addresses

When any device needs to access a resource outside the subnet (such as the Internet in general), the router acts as a [gateway](#term-gateway-router), using the IP address assigned by their ISP rather than the private address. The router contains a [network address translation](#term-network-address-translation) (NAT) service that performs [IP masquerading](#term-ip-masquerading). In this approach, the router has two IP addresses: 192.168.1.0 is used for communication with other devices in Alice’s home, while 75.3.28.14 is the IP address (assigned by the ISP) the router uses with the outside world. If any of Alice’s devices try to contact a website or to communicate with a P2P client on Bob’s network, Alice’s packets appear to be coming from 75.3.28.14—the IP external IP address of Alice’s router—rather than a 192.168.0.0/16 address. When data is sent back to Alice’s device, her router consults the NAT data structures to forward the packet to the correct device.

NAT and subnets provide one mechanism to deal with the limited number of possible IPv4 addresses, but it only provides a temporary solution. As a more long-term approach, IETF created IPv6, specified in RFC 2460, as the successor to IPv4. IPv6 increased the length of addresses from 32 bits to 128 bits. This increased the range of possible addresses from just over 4 billion to over $3 * 10^{38}$, an address space size that will not be exhausted in the foreseeable future. Given that private subnets can provide an element of security by keeping certain devices hidden from the greater Internet, IPv6 continues to support this feature, referred to as [unique local addresses](#term-unique-local-address) (ULAs). The CIDR address fd00::/8 defines the range of ULAs.

Note, though, that this CIDR address does not allow local networks to host up to $2^{120}$ devices. Instead, addresses in this range must adhere to the structure shown in [Table 5.9](#tbl5-9). The first eight bits denote the value `0xfd`, which constitutes the `Prefix` `1111110` and the `L` bit `1`, thereby designating a ULA. (Setting `L` to `0` is reserved for future use.) The `Global ID` is a 40-bit pseudo-random assigned value that is intended to identify private networks uniquely. Within that private network, the next 16 bits can be used to denote separate private subnets. This structure then leaves 64 bits for individual devices, allowing up to $2^{64}$ ($1.8 * 10^{19}$) devices.

7 bits

1 bit

40 bits

16 bits

64 bits

`Prefix`

`L`

`Global ID`

`Subnet ID`

`Interface ID`

Table 5.9: Structure of a ULA in IPv6

///5.5.2. IP Packet Formats[¶]
------------------------------

As with TCP and UDP, IP attaches a header to a transport-layer segment (the payload) for delivery. [Table 5.10](#tbl5-10) shows the structure of an IPv4 packet. The source and destination address fields contain the 32-bit IPv4 address.

0-3

4-7

8-11

12-15

16-19

20-23

24-27

28-31

`version`

`length`

`type of service`

`total length`

`identification`

`flags`

`fragment offset`

`TTL`

`protocol`

`header checksum`

`source address`

`destination address`

`options`

`payload   (transport-layer segment)`

Table 5.10: Structure of an IPv4 packet

The IPv4 header contains several fields that are not currently used in practice or to support advanced features that go beyond the scope of this text. However, there are some fields that are important to highlight. The 4-bit version field distinguishes between IPv4 and IPv6. The `protocol` field specifies which transport-layer protocol (TCP or UDP) will be used to interpret the segment header. As with TCP and UDP, IPv4 performs a checksum calculation to determine that the header has not been corrupted. Note, however, that IP does not check that the payload arrives intact. Recall that the terms [packet](#term-packet) and [datagram](#term-datagram) are often treated synonymously; this lack of integrity is related to that practice. Specifically, the term datagram generically refers to any network data that is considered unreliable. Since IP provides no integrity check for the payload, it is providing unreliable service and an IP packet satisfies the implication of the term datagram.

The `TTL` (time-to-live) field designates the maximum number of times that a packet can be forwarded. Each time an IP packet is delivered to a router, either within a local network or within the Internet as a whole, the `TTL` value is decremented. Once the counter reaches 0, the packet will be destroyed by the router that currently holds it. **This is one source of packet loss as experienced by TCP**. The `TTL` mechanism is vital for the overall success of the Internet, as it prevents old packets from traversing the Internet forever, indefinitely consuming resources.

📜 Example 5.5.1

* * *

To illustrate the structure of IPv4 address, recall the TCP message from [Example 5.3.2](#transtcpexample). This message contained an HTTP GET request to a web server. The following example shows an IPv4 header structure that could be used with this particular message.

Header

`4500   0060   0000 0000   08   06   6862   867e 8ddd   5dd8 d822`

`IPv4, length = 20 bytes (5 words)   total length = 96 bytes   ID, flags, offset (not used here)   TTL   protocol (TCP)   checksum   source address 134.126.141.221   destination address 93.184.216.34`

Payload

See [Example 5.3.2](#transtcpexample)

The `length` field (5) indicates the size of the IPv4 header in four-byte words (20 bytes total); this field indicates whether or not optional headers are in use. The `total length` field indicates the full size of the packet (including the IPv4 header, the TCP header, and the HTTP `GET` request) is 96 bytes in size. The `TTL` indicates that the packet can be routed an additional 8 hops. The `protocol` field (6) tells the receiving host how to interpret the payload; in this case, the payload contains TCP data. Lastly, the source and destination addresses are shown in both hex format on the left and their dotted-decimal translations.

For completeness, [Table 5.11](#tbl5-11) shows the structure of an IPv6 packet. As expected, the source and destination address fields have been increased from 32 to 128 bits. The `TTL` field has been renamed as the `hop limit`, but the functionality of the field is the same. IPv4 contained a field specifying the `total length` of the entire packet, whereas IPv6 specifies only the `payload length`. This change arises because IPv6 headers are identical in length, containing no optional fields. The other fields are used for features not discussed here.

0-3

4-7

8-11

12-15

16-19

20-23

24-27

28-31

`version`

`traffic class`

`flow label`

`payload length`

`next header`

`hop limit`

`source address (128 bits)`

`destination address (128 bits)`

`payload   (transport-layer segment)`

Table 5.11: Structure of an IPv6 packet

///5.5.3. Network Routing Protocols[¶]
--------------------------------------

While there are many routing protocols in contemporary use, three that are widespread are OSPF, RIP, and BGP. These protocols illustrate two ways to distinguish routing protocols. One way to distinguish them is based on the scope in which they operate. Recall that the Internet is not a single network; rather, it is a network of interconnected networks. Each of these networks can be considered an [autonomous system](#term-autonomous-system) (AS). An AS is a network of hosts (or subnetworks) that are centrally controlled by a single entity. For instance, a business, university, or home ISP may operate and maintain an independent AS. OSPF and RIP perform intra-AS routing, indicating that these protocols determine how packets are forwarded from one router to the next only within the context of that AS. For instance, if you are connected to a university network and try to access the home page of the university’s computer science department, your packets will only be forwarded within the AS via either OSPF or RIP; your packets will not be delivered to the other side of the world before coming back. On the other hand, BGP provides inter-AS routing; this technique is what allows you to access servers on a different AS, owned and operated by a different organization.

BGP, defined in RFC 4271, is a router-to-router protocol that uses TCP, with servers listening on port 179. Specifically, BGP is executed on [gateway routers](#term-gateway-router) that serve as the entry and exits to an AS. Using BGP, the routers exchange information regarding particular attributes of that AS. One attribute, the `AS-PATH`, defines the sequence of ASs that would be traversed to a target network. For instance, an `AS-PATH` to a news website might go from your university to your university’s ISP to the news organization. The `NEXT-HOP` attribute serves as an identifier for the `AS-PATH`. When a gateway router receives a packet from its network, it uses these attributes to determine the shortest total `AS-PATH` length to the destination.

![A network can be modeled as a weighted graph](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.5.10.png)

Figure 5.5.3: A network can be modeled as a weighted graph

The second key difference, as illustrated by OSPF and RIP, centers on how the shortest path is determined. OSPF, defined in RFCs 2328 and 5340, implements [Dijkstra’s algorithm](#term-dijkstra-s-algorithm) for finding the shortest path between nodes in a graph. This algorithm is a form of [link-state routing](#term-link-state-routing), in which all of the routers within the AS maintain information about the network as a whole. To illustrate OSPF, consider [Figure 5.5.3](#netdijkstra), which models a simple network as a weighted, undirected graph. In this scenario, router A is trying to route a packet to router B. The weights on the edges indicate a “cost” of using that network link, such as a time delay. The shortest path, denoted with the arrows, has a total cost of 7 ms. Note that the edge marked with an X would involve fewer hops between routers (only two intervening routers instead of three), but the total cost would be 11 ms, making that path more expensive. In OSPF, routers broadcast messages to ensure all routers have the same view of the network.

RIP, on the other hand, does not use Dijkstra’s algorithm. Instead, RIP employs [dynamic programming](#term-dynamic-programming) to update each router’s local view of the network as things change, as described in RFC 2453. For instance, consider the router in the middle of [Figure 5.5.3](#netdijkstra). This router has a link to B that currently costs 10 ms. Consider the effect of that link cost dropping to 1 ms. When this router becomes aware of the change, it updates its own internal information about the minimal cost of the path to B. However, the router also forwards this updated information to its neighbor A. A now discovers that it has a new shortest path to B: the cost of this path is 3 ms, rather than the 7 ms for the path shown in [Figure 5.5.3](#netdijkstra). A would then inform its neighbors, but neither of A’s other neighbors benefit from this improved link. Consequently, the traversal of the information stops. This approach is known as [distance vector routing](#term-distance-vector-routing).

When the distance vector routing terminates, the costs of the paths determined are identical to those established by OSPF. To be clear, the actual paths returned by the two protocols may be different, but the minimal cost will be identical. Using either OSPF or RIP, the total cost of the path from A to B will be the same.

This raises the question of why one approach would be preferred over the other. The two protocols have different features that offer advantages over each other. RIP imposes lower overhead on the network, as messages are exchanged point-to-point, rather than being broadcast. RIP also requires less space overhead, as the routers only need to maintain information about the cost of the path to each node, rather than a universal view of the entire network. However, RIP imposes a maximum hop limit, thus limiting the size of the network. OSPF also provides features related to multicasting and security.

In summary, the Internet layer extends the process-to-process communication of the transport layer with host-to-host communication. IPv4 and IPv6 specify the identifiers (IP addresses) that are used to locate a host within the logical structure of the network. IP addresses thus define the data plane of the Internet layer. The control plane consists of multiple routing protocols that determine the path between hosts, as defined by the links between routers. The RIP and OSPF routing protocols are used for intra-AS routing, determining the minimal cost path between hosts in a network that is under centralized control. BGP provides inter-AS routing, thus providing a mechanism for data to be routed between ASs that are independently owned and operated.



//5.6. Link Layer[¶]
====================

The Internet layer determines the logical path that packets will traverse through a local network and the Internet as a whole. The link layer defines the protocols that control how the bits are transmitted across an underlying physical technology. For example, the network shown previously in [Figure 5.5.3](#netdijkstra) places an emphasis on [routing](#term-routing) a packet through a series of routers in an AS. These routers may be part of distinct networks that use different underlying technologies. For instance, each router may be operated and administered by separate departments that are part of the same organization; some links may consist of wireless connections, while other logical links are created by a chain of [switches](#term-switch)—router-like devices that are connected by cables. The link layer, then, focuses on the task of [forwarding](#term-forwarding) packets across point-to-point connections between routers and end-point devices.

The distinction of routing and forwarding, like the distinction between routers and switches, is subtle and can be misunderstood. In essence, the classification of routing/routers is used to describe the communication between heterogeneous networks that may rely on different types of communication technologies. One network might employ a [packet switching](#term-packet-switching) technology (e.g., Ethernet) that uses a structured message format that allows any device to send and receive data at any time. A router might connect that network to one that uses [circuit switching](#term-circuit-switching) (e.g., FDDI or token ring), in which two devices communicate directly over a dedicated channel; other devices may be connected to the network, but they have to wait until it is their turn to control the transmission channel. In contrast, the classification of forwarding/switches refers to communication within a homogeneous network with a single underlying technology. A switch does not receive a message from one technology (Ethernet) and forward it using another (FDDI). Switches only serve as the links between hosts in a single network.

The switching that occurs at the link layer creates another possible source of packet loss that TCP’s reliability is intended to address. When a packet arrives, there is a [processing delay](#term-processing-delay) associated with the work to compute checksums, determine the higher-level protocol, and so on. [Queueing delays](#term-queueing-delay) occur while the packet is waiting to be processed or transmitted. [Transmission delays](#term-transmission-delay) are imposed by the work to encode the data into light signals or radio waves. Since the light signals and radio waves must travel across physical space, the packet also experiences [propagation delays](#term-propagation-delay). These delays occur at every switching link in the network, accumulating to create increasingly greater round-trip times. Consequently, link-layer protocols strive to balance correctness (ensuring limited re-transmissions) with efficient processing in order to avoid causing packet losses.

///5.6.1. LAN Packet Transmission: Ethernet[¶]
----------------------------------------------

Given the ubiquity of the technology, many readers probably associate the term [Ethernet](#term-ethernet) with the cable. In actuality, Ethernet is a collection of standards defined and maintained by the [IEEE 802.3](#term-ieee-802-3) working group. That is, Ethernet is not defined as a stand-alone protocol defined in an RFC; Ethernet involves several protocols that are co-designed with the physical cable technology that they use. These physical technologies can range from _twisted-pair_ copper wires to _fiber-optic_ wires made of glass or plastic.

Regardless of the type of the physical medium used, all Ethernet frames maintain the same basic structure, shown in [Table 5.12](#tbl5-12). Note the use of the term _octet_ rather than _byte_. Although modern systems have generally settled on the use of the term byte to denote eight bits, this connotation was not always true; some technologies used byte to refer to a basic addressable unit of memory, which was not necessarily eight bits in size. An octet, however, must be exactly eight bits. [[1]](#f38)

8 octets

6 octets

6 octets

2 octets

varies

4 octets

`Preamble`

`Destination`

`Source`

`Type`

`Payload`

`CRC`

Table 5.12: Structure of an Ethernet frame

The `preamble` of an Ethernet frame consists of seven octets of `10101010` followed by a single octet of `10101011`. The purpose of the `preamble` is to declare to a device intends to send a frame and to synchronize the other devices to listen as receivers. The destination and source addresses are 48-bit (6-octet) [media access control (MAC) addresses](#term-media-access-control-address). [[2]](#f39) Unlike IP addresses, MAC addresses are persistently associated with a hardware device and do not provide any implication of the device’s logical location in the network. MAC addresses are determined by the device manufacturer and are stored in either firmware or hard-wired storage. The type field of the Ethernet frame determines which Ethernet protocol standard is being used. The payload contains the Internet-layer data (e.g., an IP packet); the maximum size varies based on the version of Ethernet, but most have a [maximum transmit unit](#term-maximum-transmit-unit) (MTU) size of approximately 1500 octets. Finally, the frame ends with the `field checksum` (`FCS`), which is a 32-bit [cyclic redundancy check](#term-cyclic-redundancy-check) (CRC) calculation that provides a more robust error detection mechanism than checksums. As one example of the difference, CRC values can detect when the order of the octets has been changed, while checksums cannot.

The MTU size implies that a lot of network traffic requires multiple frames. Consider an HTTP request to load a GIF containing an Internet meme showing a short video of cats (people on the Internet love cat videos!). Image files tend to be multiple MB in size. If a single video is 3 MB in size, that image alone would require 2098 blocks of 1500 bytes. However, each frame must also have the TCP/UDP and IP headers attached, so some of the 1500 bytes is already accounted for. Using the bare minimum of 20 bytes for TCP and 40 for IP, the image would now require 2185 frames. This fragmentation exacerbates the reliability service of TCP, as all of these frames must be successfully transmitted (repeatedly) before the RTT timeout occurs. If any frame fails to arrive on time, the TCP client (i.e., the web browser) declares the entire image lost and provides the user with a (generally unhelpful) error message that the connection timed out. Hence the reason that OSPF, RIP, and BGP prioritize finding the shortest, most efficient path possible.

📜 Example 5.6.1

* * *

To illustrate the structure of an Ethernet frame, the following header extends the IPv4 datagram from [Example 5.5.1](#netipexample) (which extends the TCP segment from [Example 5.3.2](#transtcpexample)).

Preamble

Destination

Source

Type

Payload

FCS

`aaaaaaaaaaaaaaab`

`f0def12cc22b`

`f45c89bd332d`

`0800`

`...`

`64713722`

The destination field is the MAC address `f0-de-f1-2c-c2-2b`, and the source field is the address `4f-5c-89-bd-33-2d`. These identifiers are persistently associated with the networking hardware components. The type 0800 indicates that this frame is using Ethernet II, the most common style of Ethernet framing. Finally, the FCS is the 32-bit CRC calculation over the entire frame.

The figure below illustrates the complete structure of the Ethernet frame by combining this example with [Example 5.3.2](#transtcpexample) and [NetIPExample]. The frame begins with the Ethernet header. The Ethernet payload combines the IPv4 header, TCP header, and HTTP header. (As a `GET` request, the HTTP message body is empty and only the header is sent.) At the same time, the IPv4 payload consists of the TCP and HTTP headers, whereas the HTTP header is the payload of the TCP segment.

![Anatomy of a complete Ethernet frame with IPv4, TCP, and HTTP data](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.5.EX.png)

Anatomy of a complete Ethernet frame with IPv4, TCP, and HTTP data

///5.6.2. LAN Packet Transmission: ARP[¶]
-----------------------------------------

![Devices connected to the same Ethernet segment](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.5.11.png)

Figure 5.6.3: Devices connected to the same Ethernet segment

The previous discussion of Ethernet introduced a new form of addressing to locate hosts within a network. [Figure 5.6.3](LinkLayer.html#linkarp) shows a simple Ethernet segment with two end devices and a router; each of these three hosts has both a MAC address and an IP address. MAC addresses do not have any logical relationship to the network topology itself, while IP addresses are logical identifiers that are not tied to the hardware. As such, routers need some way to translate an IP address into a MAC address. Without such a mapping, routers would not be able to encapsulate the IP packet in an Ethernet frame for the intended host device.

The [Address Resolution Protocol](#term-address-resolution-protocol) (ARP) is a simple protocol for establishing this mapping, as defined in RFC 826. Assume that the two end host devices in [Figure 5.6.3](LinkLayer.html#linkarp) need to communicate, with the 192.168.1.2 host sending data to 192.168.1.3. The sender broadcasts an Ethernet frame containing an ARP query to the reserved MAC address `ff-ff-ff-ff-ff-ff`. All nodes receive the query, but only the intended recipient, 192.168.1.3, replies. At that point, the 192.168.1.2 host stores this mapping in a local cache for a period of time. After doing this, 192.168.1.2 can use the appropriate destination MAC address to transmit the IP packets as needed.

🔍 Note

* * *

ARP is an insecure protocol that assumes all connected devices behave correctly. In an [ARP cache poisoning](#term-arp-cache-poisoning) attack, an adversary that has access to a network can respond to ARP queries with its own MAC address. The protocol defines no authentication mechanism to confirm that the response is correct. This weakness is often acceptable if networks are secured so that only authorized devices can be used. However, in public settings, such as a free café Wi-Fi network, the assumption of trust can break down, allowing devices to intercept messages intended for others.

///5.6.3. What Lies Beneath: Carrier Signals[¶]
-----------------------------------------------

To summarize the Internet model up to this point, the application layer uses transport-layer protocols to create a process-to-process logical communication channel. The transport layer encapsulates this information in a host-to-host link using Internet-layer routing between potentially heterogeneous networks. The link layer then provides the mechanism for point-to-point data transmission in a homogeneous network using the same underlying physical technology. These layers of abstraction leave one question remaining: How do the bits actually get transmitted from one device to another?

[Figure 5.6.5](LinkLayer.html#linkcarrier) illustrates the basic principles involved in the physical data transmission. Fundamentally, all of the physical networking technologies are transmitting either light or radio signals, both of which can be modeled as an oscillating waveform. The default signal with no encoded information is called a [carrier signal](#term-carrier-signal). This signal can be _modulated_ to encode information by manipulating one of three characteristics of waves: frequency, amplitude, or phase. The frequency refers to the number of oscillations in a given time, as illustrated by how many times the wave oscillates between a maximum and minimum value. The amplitude denotes the height of the wave. The phase refers to the timing of when the wave begins and ends, illustrated by the alignment of the maximum and minimum values.

![Three techniques for modulating a carrier signal to encode bits](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.5.12.png)

Figure 5.6.5: Three techniques for modulating a carrier signal to encode bits

In [phase shift keying](#term-phase-shift-keying) (PSK), the carrier wave operates at a fixed frequency, but its phase is manipulated by changing the sine and cosine of the inputs. The precise calculations depend on the particular scheme being used, but these techniques generally all map the measurement values to points on the complex number plane. In binary PSK (BPSK), there are two possible points to indicate the values 0 or 1. Other schemes use more points to map multiple bits. For instance, each measurement in quadrature PSK (QPSK) maps to one of four points to encode two bits; 8-PSK uses eight points to encode three bits.

In [frequency modulation](#term-frequency-modulation) (FM), the frequency is changed to be either faster or slower than the carrier wave. If this technique were used on sound waves that were in a range audible to humans, FM would correspond to making the pitch higher or lower than the default range. [Amplitude modulation](#term-amplitude-modulation) (AM) keeps the frequency the same as the carrier wave but increases or decreases the magnitude of the difference between the maximum and minimum values. In the audible range, this would correspond to making the sound louder or softer.

FM and AM have long been used for analog signal transmission. Readers may associate these terms with radio stations, and for good reason: Radio stations with AM channels use amplitude modulation to encode sound in the range of 540 kHz to 1600 kHz (kHz = 1,000 cycles per second). FM radio stations use frequency modulation to encode sound between 88 MHz and 108 MHz (MHz = 1,000,000 cycles per second). So, a radio station that advertises itself as FM 101.1 is sending a stream of bits by changing the frequency to be slightly above and slightly below 101.1 MHz. Note, though, that FM and AM are not restricted to analog radio signals. All three techniques are used to modulate digital signals, as well.

Consequently, when we say that Ethernet is sending the preamble of a frame by changing the transmitted bit from 1 to 0, it means that the network device is using one of these techniques to manipulate the signal it is transmitting. The received performs the corresponding de-modulation to restore the carrier wave and records the transmitted bit. By coordinating this signal transmission, the link layer can transmit a packet from one network device to another. These links can then be chained together to establish network routing, leading to higher levels of communication protocols.

[[1]](#id1)

The bit ordering in Ethernet can be confusing to interpret. Ethernet writes the most significant _octet_ first, but the least significant _bit_ within the octet is written first. For example, the hexadecimal value `0xa7` would be written as `1110 0101` rather than `1010 0111`. We ignore this detail in [Example 5.6.1](LinkLayer.html#linkethernet) for simplicity.

[[2]](#id2)

There is no relationship between a MAC address as described here and the cryptographic notion of MAC—message authentication code.



//5.7. Wireless Connectivity: Wi-Fi, Bluetooth, and Zigbee[¶]
=============================================================

We close our survey of network technologies with a brief overview of three forms of wireless connectivity that facilitate three very different capabilities. [Wi-Fi](#term-wi-fi) is the common name for the set of wireless technologies defined by the [IEEE 802.11](#term-ieee-802-11) working groups and supported by the [Wi-Fi Alliance](#term-wi-fi-alliance). _Bluetooth_ is a set of standards defined by the [Bluetooth SIG](#term-bluetooth-special-interest-group). [Zigbee](#term-zigbee) is a suite of protocols maintained by the [Zigbee Alliance](#term-zigbee-alliance), intended to create short-range networks using radio communication defined by the [IEEE 802.15.4](#term-ieee-802-15-4) working group.

///5.7.1. Wireless Protocol Stacks and Uses[¶]
----------------------------------------------

Wi-Fi is designed to act as a substitute for the wired connections that make up the traditional link and physical layers. That is, Wi-Fi assumes that the goal is to use a wireless connection to facilitate Internet access for a mobile device. As such, devices like laptops and tablets use the 802.11 technology standards to communicate with an [access point](#term-access-point) that is connected to an ISP. Readers may be familiar with the different standards, such as 802.11a, 802.11g, and 802.11n. These standards offer different levels of performance by specifying different features of the radio that is used. For instance, 802.11n wireless routers can have more than one antenna to support [multiple input and multiple output](#term-multiple-input-and-multiple-output) (MIMO) access; with MIMO, the router can be sending and receiving communications with several devices all at one, increasing the efficiency of the overall system. Early standards did not assume MIMO-compatible radios, so the maximum speed and throughput were significantly lower.

In contrast, Bluetooth was not designed to connect a device to the Internet. Instead, Bluetooth devices focus on short-range [mobile ad hoc networks (MANETs)](#term-mobile-ad-hoc-network). In a MANET, unknown devices establish connections with efficient, minimal overhead and configuration; the goal is to allow a new device to join the network rapidly, exchange some data, then disconnect. In the simplest case, Bluetooth speakers or headphones may be used to play the sound that would otherwise be produced by a computer’s speakers; similarly, a Bluetooth keyboard or mouse reduce the number of cables that must physically connect a computer and its peripheral devices. Bluetooth devices are generally assumed to have low-power requirements, so the range of the connection can be as short as 1 to 10 meters.

![The Bluetooth protocol stack does not assume Internet access](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.5.13.png)

Figure 5.7.1: The Bluetooth protocol stack does not assume Internet access

Since Bluetooth is not intended to provide Internet access, its protocol stack is very different than the 5-layer model we have examined previously. [Figure 5.7.1](Wireless.html#wirelessbluetooth) shows the main structure of the Bluetooth protocol stack. At the lowest layer, the baseband and the link manager/link controller (LM/LC) layers provide the basic point-to-point modulation of the radio signal. The logical link control and adaptation protocol (L2CAP) acts in a manner similar to the traditional link layer by multiplexing access to the radio for higher-layer protocols. Given Bluetooth’s usage model is for point-to-point communication, its protocol stack does not offer an equivalent of the Internet layer.

Bluetooth defines a number of protocols that are analogous to a transport layer. The RFCOMM protocol provides a generic serial port interface, similar to a TCP stream socket. Multichannel adaptation protocol (MCAP) defines a common transport layer for medical devices. Lastly, the generic access profile (GAP) creates an extendible interface that can be used for future applications that are not defined in the specification.

[Bluetooth profiles](#term-bluetooth-profile) serve as the equivalent of the application layer. The hands-free protocol (HFP) defines the interface for connecting a cell-phone to a wireless peripheral, such as a headset or an automobile. HFP and object exchange (OBEX)—a generic profile for exchanging binary data—run on top of the RFCOMM protocol. The health device protocol (HDP) provides a common interface for healthcare devices, such as thermometers or heart meters, running on the MCAP protocol. Some profiles, such as human interface device (HID) and audio/video distribution transport protocol (AVDTP), run directly on top of L2CAP to minimize their latency to support real-time human interactions. HID is used for wireless devices such as a mouse and keyboard, whereas AVDTP is used for devices like headphones. More recently, the advanced audio distribution profile (A2DP) has been added on top of AVDTP to provide improved service for audio-visual devices.

The Bluetooth SIG was established with a core set of applications in mind. The designers also had a stated design goal to use existing radios, rather than designing new technology from scratch. These design goals allowed the Bluetooth SIG to get devices on the market for consumer use very rapidly. By working within the constraints of the top and bottom layers (the applications and the existing radios), the designers were able to construct a working protocol stack in a short period of time.

The Zigbee wireless standard lies somewhere in between the goals of Wi-Fi and Bluetooth. Like Bluetooth, Zigbee is used for MANETs, but on a larger and more complex scale. To be specific, Bluetooth is generally based on a [star topology](#term-star-topology), with multiple peripherals (keyboard, mouse, speakers) connecting to a central device (laptop); no one expects their keyboard and mouse to be exchanging data packets. In contrast, Zigbee provides support for [wireless mesh networks](#term-wireless-mesh-network), in which devices provide support for routing packets if two hosts are too far apart to communicate directly. Note that the Internet backbone can be described as a wired mesh network, allowing multiple routing paths; Zigbee takes this idea and replaces the wired connections with wireless ones. Zigbee uses this structure to support applications like low-powered sensor networks. Zigbee is one of the protocols used to support the [Internet of Things](#term-internet-of-things) (IoT). IoT is the general concept of connecting non-traditional computing devices, such as home appliances, thermostats, or entertainment devices, to each other and to the Internet as a whole.

![The Zigbee protocol stack supports custom short-range networks](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.5.14.png)

Figure 5.7.2: The Zigbee protocol stack supports custom short-range networks

[Figure 5.7.2](Wireless.html#wirelesszigbee) shows the structure of the Zigbee protocol stack. The lowest layers, MAC and PHY, are defined by the 802.15.4 working group, independent of Zigbee itself. These layers define a generic point-to-point communication that can be used with low-power radios. Zigbee refers specifically to the layers built on top of this this foundation. The NWK layer coordinates the mesh routing that builds the network. The APS and Zigbee Device Object (ZDO) Management provide common services that applications can use, such as cryptographic key management and information about devices on the network. Applications can access ZDOs through an interface at the application layer, or they can request services through application end points.

These three protocol stacks serve illustrate how wireless connectivity can be used to provide very different services. Wi-Fi is designed to extend the traditional Internet model to mobile devices, replacing the last link of the network with a wireless access point. Bluetooth defines a local point-to-point mechanism that allows computer peripherals to communicate without a cable physically connecting the devices. Zigbee creates a wireless mesh network that can be used to route data, such as sensor readings, between low-powered devices. Zigbee devices can also have additional components added that allow them to link to an Internet access point, connecting the sensor network and the Internet. These separate infrastructures provide support for application designers to build services that can communicate without physical connections.



//5.8. Extended Example: DNS Client[¶]
======================================

This Extended Example is a minimal client for performing a DNS query for IPv4 addresses. Given a domain name (such as `example.com`), the client sets up the DNS question in `setup_dns_request()` (lines 200 – 234). The `packed` attribute of the `dns_record_a_t` (lines 31 – 39) ensures that the compiler does not add any unnecessary padding that would make the question improperly structured. The `build_domain_qname()` function (lines 95 – 134) replaces the dots in the domain name with an integer to denote the length of the next field. Lines 65 – 82 perform the actual query, sending the request to OpenDNS using a UDP socket. The client is only designed to support DNS `A` type records for IPv4. I.e., this client does not support `CNAME`, `NS`, or `MX` records. The `print_dns_response()` function (lines 153 – 198) will stop if any other record types are encountered.

```cpp
#include <arpa/inet.h>
#include <assert.h>
#include <inttypes.h>
#include <netdb.h>
#include <netinet/in.h>
#include <stdint.h>
#include <stdio.h>
#include <stdint.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

/* Structure of the bytes for a DNS header */
typedef struct {
  uint16_t xid;
  uint16_t flags;
  uint16_t qdcount;
  uint16_t ancount;
  uint16_t nscount;
  uint16_t arcount;
} dns_header_t;

/* Structure of the bytes for a DNS question */
typedef struct
{
  char *name;
  uint16_t dnstype;
  uint16_t dnsclass;
} dns_question_t;

/* Structure of the bytes for an IPv4 answer */
typedef struct {
  uint16_t compression;
  uint16_t type;
  uint16_t class;
  uint32_t ttl;
  uint16_t length;
  struct in_addr addr;
} __attribute__((packed)) dns_record_a_t;

char * build_domain_qname (char *);
void print_byte_block (uint8_t *, size_t);
void print_dns_response (uint8_t *);
uint8_t * setup_dns_request (char *, size_t *);

int
main (int argc, char *argv[])
{
  if (argc != 2)
    {
      fprintf (stderr, "ERROR: Must pass a domain name\n");
      return 1;
    }

  char *hostname = argv[1];
  
  /* Set up the packet and get the length */
  size_t packetlen = 0;
  uint8_t *packet = setup_dns_request (hostname, &packetlen);

  /* Print the raw bytes formatted as 0000 0000 0000 ... */
  printf ("Lookup %s\n", hostname);
  print_byte_block (packet, packetlen);

  /* Send the packet to OpenDNS. Create an IPv4 UDP socket to
     208.67.222.222 (0xd043dede), the IP address for OpenDNS.
     DNS servers listen on port 53. */
  int socketfd = socket (AF_INET, SOCK_DGRAM, 0);
  struct sockaddr_in address;
  address.sin_family = AF_INET;
  address.sin_addr.s_addr = htonl (0xd043dede);
  address.sin_port = htons (53);

  /* Send the request and get the response */
  sendto (socketfd, packet, packetlen, 0, (struct sockaddr *)&address,
          (socklen_t)sizeof (address));

  socklen_t length = 0;
  uint8_t response[512];
  memset (&response, 0, 512);
  ssize_t bytes = recvfrom (socketfd, response, 512, 0,
                            (struct sockaddr *)&address, &length);

  /* Print the raw bytes formatted as 0000 0000 0000 ... */
  printf ("Received %zd bytes from %s:\n", bytes,
          inet_ntoa (address.sin_addr));
  print_byte_block (response, bytes);

  /* Parse the DNS response into a struct and print the result */
  print_dns_response (response);

  return 0;
}

char *
build_domain_qname (char *hostname)
{
  assert (hostname != NULL);

  char *name = calloc (strlen (hostname) + 2, sizeof (uint8_t));

  /* Leave the first byte blank for the first field length */
  memcpy (name + 1, hostname, strlen (hostname));

  /* Example:
     +---+---+---+---+---+---+---+---+---+---+---+
     | a | b | c | . | d | e | . | c | o | m | \0|
     +---+---+---+---+---+---+---+---+---+---+---+

     becomes:
     +---+---+---+---+---+---+---+---+---+---+---+---+
     | 3 | a | b | c | 2 | d | e | 3 | c | o | m | 0 |
     +---+---+---+---+---+---+---+---+---+---+---+---+
   */

  uint8_t count = 0;
  uint8_t *prev = (uint8_t *)name;
  for (int i = 0; i < strlen (hostname); i++)
    {
      /* Look for the next ., then copy the length back to the
         location of the previous . */
      if (hostname[i] == '.')
        {
          *prev = count;
          prev = (uint8_t *)name + i + 1;
          count = 0;
       }
     else
       count++;
    }
  *prev = count;

  return name;
}

void
print_byte_block (uint8_t *bytes, size_t length)
{
  printf ("  ");
  for (int i = 0; i < length; i++)
    {
      printf ("%02x", bytes[i]);
      if (i == length - 1)
        printf ("\n");
      else if ((i + 1) % 16 == 0)
        printf ("\n  ");
      else if ((i % 2) != 0)
        printf (" ");
    }
  printf ("\n");
}

void
print_dns_response (uint8_t *response)
{
  /* First, check the header for an error response code */
  dns_header_t *response_header = (dns_header_t *)response;
  if ((ntohs (response_header->flags) & 0xf) != 0)
    {
      fprintf (stderr, "Failed to get response\n");
      return;
    }

  /* Reconstruct the question */
  uint8_t *start_of_question = response + sizeof (dns_header_t);
  dns_question_t *questions
    = calloc (sizeof (dns_question_t), response_header->ancount);
  for (int i = 0; i < ntohs (response_header->ancount); i++)
    {
      questions[i].name = (char *)start_of_question;
      uint8_t total = 0;
      uint8_t *field_length = (uint8_t *)questions[i].name;
      while (*field_length != 0)
        {
          total += *field_length + 1;
          *field_length = '.';
          field_length = (uint8_t *)questions[i].name + total;
        }
      questions[i].name++;
      /* Skip null byte, qtype, and qclass */
      start_of_question = field_length + 5;
    }

  /* The records start right after the question section. For each record,
     confirm that it is an A record (only type supported). If any are not
     an A-type, then return. */
  dns_record_a_t *records = (dns_record_a_t *)start_of_question;
  for (int i = 0; i < ntohs (response_header->ancount); i++)
    {
      printf ("Record for %s\n", questions[i].name);
      printf ("  TYPE: %" PRId16 "\n", ntohs (records[i].type));
      printf ("  CLASS: %" PRId16 "\n", ntohs (records[i].class));
      printf ("  TTL: %" PRIx32 "\n", ntohl (records[i].ttl));
      printf ("  IPv4: %08" PRIx32 "\n",
              ntohl ((uint32_t)records[i].addr.s_addr));
      printf ("  IPv4: %s\n", inet_ntoa (records[i].addr));
    }
}

uint8_t *
setup_dns_request (char *hostname, size_t *packetlen)
{
  /* Set up the DNS header */
  dns_header_t header;
  memset (&header, 0, sizeof (dns_header_t));
  header.xid= htons (0x1234);    /* Randomly chosen ID */
  header.flags = htons (0x0100); /* Q=0, RD=1 */
  header.qdcount = htons (1);    /* Sending 1 question */

  /* Set up the DNS question */
  dns_question_t question;
  question.dnstype = htons (1);  /* QTYPE 1=A */
  question.dnsclass = htons (1); /* QCLASS 1=IN */
  question.name = build_domain_qname (hostname);

  /* Copy all fields into a single, concatenated packet */
  *packetlen = sizeof (header) + strlen (hostname) + 2
               + sizeof (question.dnstype) + sizeof (question.dnsclass);
  uint8_t *packet = calloc (*packetlen, sizeof (uint8_t));
  uint8_t *p = (uint8_t *)packet;

  /* Copy the header first */
  memcpy (p, &header, sizeof (header));
  p += sizeof (header);

  /* Copy the question name, QTYPE, and QCLASS fields */
  memcpy (p, question.name, strlen (hostname) + 2);
  p += strlen (hostname) + 2;
  memcpy (p, &question.dnstype, sizeof (question.dnstype));
  p += sizeof (question.dnstype);
  memcpy (p, &question.dnsclass, sizeof (question.dnsclass));

  return packet;
}
```


/Chapter 6   Concurrency with Multithreading[¶]
===============================================

//6.1. Concurrency with Multithreading[¶]
=========================================

![Timeline of major CSF topics with Multicore and Threads highlighted](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Timeline.6.png)

> “The good news about computers is that they do what you tell them to do. The bad news is that they do what you tell them to do.”
> 
> Ted Nelson

Plus ça change, plus c’est la même chose. Or put another way, everything old is new again. IBM OS/360 introduced threads in 1967. The 1990s brought renewed interest in threads, with POSIX.1c-1995 standardizing their interface. One key factor behind this interest was the development of Linux as an open-source platform for cluster systems. Another factor was the impact of the “power wall” on Moore’s law and integrated circuit design. As a result of this resurgence, threads are now unavoidable as a core abstraction in modern computing.

🚀 Chapter Objectives

* * *

In this chapter, we will address the following instructional objectives:

*   We will compare and contrast the concepts of threads and processes, describing the advantages and limitations of each.
*   We will examine code using the POSIX thread library, a library that is widely available on a variety of platforms.
*   We will explore race conditions as a new source of bugs that arise in multithreaded software.
*   We will consider the benefits and drawbacks of using implicit threading libraries and built-in language support.

Throughout the first part of this book, we referred to processes whenever we were discussing a unit of execution. Specifically, in [Processes and Multiprogramming]()2_2_, we described multiprogramming as the technique of dividing time on the CPU among multiple processes. After one process ran for a certain amount of time, the kernel would change the virtual address space to switch the execution context to a different process.

While process context switching provides a robust mechanism for multiprogramming, modern systems require a more efficient approach. Specifically, many programmers write applications or systems programs that achieve concurrency by switching between multiple [threads of execution](#term-thread) (or, more simply, [threads](#term-thread)) within a single process. This technique is known as [multithreading](#term-multithreading).

There are many different languages and software libraries available for writing multithreaded code. Some of these options are unique to a single operating system or programming language, while others are more broadly supported. In this and later chapters, we will primarily focus on using the POSIX library (called [pthreads](#term-pthreads)), a C library available on all major platforms, though we will also explore some of the benefits of alternative approaches.



//6.2. Processes vs. Threads[¶]
===============================

Recall from [Processes and OS Basics](ProcessesOverview.html) that a [process](#term-process) is fundamentally defined by having a unique virtual memory space. The process contains a code segment that consists of the executable machine-language instructions, while the statically allocated global variables are stored in the data segment. The heap and stack segments contain the dynamically allocated data and local variables for this execution of the program.

Every process begins by executing the instructions contained in `main()`. The `%pc` register defines the flow of the code by pointing to the next instruction to be executed. The call instruction can modify the control flow by jumping to other functions. When this happens, the return address (and other related information) is placed on the stack to maintain the program’s logical flow. This single, logical sequence of executing instructions within a process is known as a _thread of execution_, which we typically just call a [thread](#term-thread).

///6.2.1. Multithreading[¶]
---------------------------

[Multithreaded](#term-multithreading) processes have multiple threads that perform tasks concurrently. Just like the thread that runs the code in `main()`, additional threads each use a function as an entry point. To maintain the logical flow of these additional threads, each thread is assigned a separate stack. However, all of the other segments of memory, including the code, global data, heap, and kernel, are shared.

Another way to consider the relationship between threads and processes is to separate the system functions of [scheduling](#term-scheduling) and _resource ownership_. Switching from one thread to another would change the system from working toward one computational goal to working toward another. This means that the purpose of switching between threads is to create a schedule that controls when each thread is running on the CPU.

On the other hand, processes act as containers for resource ownership. As the process runs, it may request access to files stored on the disk, open network connections, or request the creation of a new window on the desktop. All of these resources are allocated to the process, not individual threads.


```cpp
/* Code Listing 6.1:
   Structure of a simple multithreaded program framework
 */

#include <stdio.h>
/* Include thread libraries */

int number = 3;

void *
thread_A (void* args)
{
  int x = 5;
  printf ("A: %d\n", x + number);
  /* Exit thread */
}

void *
thread_B (void* args)
{
  int y = 2;
  printf ("B: %d\n", y + number);
  /* Exit thread */
}

int
main (int argc, char** argv)
{
  /* Create thread A */
  /* Create thread B */
  /* Wait for threads to finish */
  return 0;
}
```

[Code Listing 6.1](#cl6-1) shows the structure of a simple program that creates two threads that run concurrently. For now, comments are used to indicate where the thread management code would be written. Thread A declares a local variable `x` and prints out the message A: 8, which is the sum of the value of `x` (5) and the global variable `number` (3). Thread B declares `y` and prints out B: 5, for similar reasons. One observation about this sample is that the code for the threads looks like normal functions in a typical program. The difference only becomes apparent when we run the program.

When the program is run, the scheduling that controls when the threads run is [nondeterministic](#term-nondeterminism) from the programmer’s perspective. By looking at the code, we can conclude that line 13 runs before line 14, just like line 21 runs before line 22. However, we cannot predict the ordering between line 13 and line 21. This choice is made at run-time and is controlled by the kernel. When looking at source code for multithreaded programs, we assume the lines of code are [interleaved](#term-interleaved), meaning that the kernel can switch back and forth whenever it chooses to do so.

📜 Example 6.2.1

* * *

[Table 6.1](#tbl6-1) shows three of the possible interleavings of [Code Listing 6.1](#cl6-1). In each of these cases, the indentation is used to indicate which thread performs an action (thread A is not indented and thread B is). In interleaving 1, thread A runs to completion before thread B starts. In interleaving 2, thread B sets the value of y before thread A prints its message. **Note that this timing is not observed without a debugger, since setting y produces no visible result.** Finally, in interleaving 3, thread B’s print statement occurs before thread A’s.

Interleaving 1

Interleaving 2

Interleaving 3

`x = 5   print "A:8"    y = 2    print "B:7"`

`x = 5    y = 2   print "A:8"    print "B:7"`

 `y = 2   x = 5    print "B:7"   print "A:8"`

Table 6.1: Three possible interleavings of Code Listing 6.1

More interleavings are possible than those shown in [Table 6.1](#tbl6-1). The only requirement is that the order of instructions **within a single thread** must match the order specified in the code. The selection of which thread has an instruction run is seemingly randomized.

///6.2.2. Advantages and Disadvantages of Threads[¶]
----------------------------------------------------

Using multiple threads offers a number of advantages over creating an application from multiple processes. First, using multiple threads helps programmers build modularity into their code more effectively. Complex software, especially the types of applications built with object-oriented programming, typically involves modular components that interact with each other. Oftentimes, the program can run more efficiently if these components can run concurrently without performing context switches. Threads provide a foundation for this programming goal.

One common example of using threads for different purposes is an interactive graphical user interface. Application programmers build in keyboard or mouse listeners that are responsible for detecting and responding to key presses, mouse clicks, and other such events. These types of event listeners are simply concurrent threads within the process. By implementing the listener behavior in a separate thread, the programmer can simplify the structure of the program.

Threads also require significantly less overhead for switching and for communicating. A process context switch requires changing the system’s current view of virtual memory, which is a time-consuming operation. Switching from one thread to another is a lot faster. For two processes to exchange data, they have to initiate [interprocess communication (IPC)](#term-interprocess-communication), which requires asking the kernel for help. IPC generally involves performing at least one context switch. Since all threads in a process share the heap, data, and code, there is no need to get the kernel involved. The threads can communicate directly by reading and writing global variables.

At the same time, the lack of isolation between threads in a single process can also lead to some disadvantages. If one thread crashes due to a segmentation fault or other error, all other threads and the entire process are killed. This situation can lead to data and system corruption if the other threads were in the middle of some important task.

As an example, assume that the application is a server program with one thread responsible for logging all requests and a separate thread for handling the requests. If the request handler thread crashes before the logging thread has a chance to write the request to its log file, there would be no record of the request. The system administrators left to determine what went wrong would have no information about the request, so they may waste a lot of time validating other requests that were all good.

The lack of isolation between threads also creates a new type of programming bugs called [race conditions](#term-race-condition). In a race condition, the behavior of the program depends on the timing of the thread scheduling. One example of a race condition is when two threads both try to change the value of a global variable. The final value of the global variable would depend on which thread set the value last.

///6.2.3. Thread Models[¶]
--------------------------

Throughout this chapter, we will be focusing on the POSIX thread model, which is independent of language and OS (though mostly associated with UNIX-like OS). The POSIX specification defines a thread as a _single flow of control within a process._ Based on this definition, all threads within a process exist in a single address space; as such, all threads in a process can access anything stored in memory if the address can be determined. On the other hand, each thread has a unique copy of certain pieces of data that are required for execution; these unique copies include the `errno` variable (used for detecting errors that occur in C standard library functions), general purpose registers, and a stack pointer. [Table 6.2](#tbl6-2) summarizes what the POSIX specificat declares is unique per thread and what is accessible to all threads.

Unique per thread

Accessible by all threads in process

*   Thread ID
*   Scheduling priority and policies
*   `errno` value
*   Floating point environment
*   Thread-specific key/value bindings
*   Resources required for control flow

*   Static variables
*   Storage obtained by `malloc()`
*   Directly addressable storage
*   Automatic variables

Table 6.2: Data that is unique to or shared between POSIX threads

In typical implementations, scheduling of POSIX threads is typically performed by the kernel. This approach allows the kernel to make scheduling decisions consistently across processes, while also making context switches between threads efficient (as the virtual memory image does not change). The POSIX model is not the only way to structure and organize threads. There are other models that have been used for multithreading in a variety of systems.

Early versions of Java used the [Green threads](#term-green-threads) model (named for the Green Team inside the Sun Microsystems that implemented the library). In this model, the Java virtual machine (JVM), which ran as a single process, is responsible for scheduling between threads in an application. The kernel underlying the JVM had no awareness of the presence of multiple threads. An advantage of this approach is that switching between threads is efficient, as there are no system calls that require the intervention of the kernel. However, all threads had to subdivide the process’s given CPU time; as the number of threads increased, performance would dramatically decrease, as each thread would get less and less CPU time.

Solaris and other forms of System V UNIX adopted a model known as [light-weight processes (LWPs)](#term-light-weight-process). In this model, multiple LWPs within a single process execute on top of a single kernel thread. Like the Green threads model, scheduling between LWPs is handled by a user-mode thread library rather than the kernel. However, unlike Green threads, the LWP model allows a single process to run on top of multiple kernel threads. This approach creates a many-to-many relationship between user- and kernel-mode threads. Consequently, processes with several LWPs could distribute their execution time across multiple kernel threads for more CPU time.

Windows and the .NET framework use a [fiber](#term-fiber) model, which is similar to the POSIX model. Like a POSIX thread, fibers share a single address space and each fiber is mapped to a single kernel thread. The primary difference is that fibers within a single process use cooperative multitasking. This approach offers a significant advantage for ensuring safe access to resources shared between the threads, as the fibers have greater control over when switches occur.

[Figure 6.2.2](ProcVThreads.html#thrmodel) summarizes how the four threading models map user threads to kernel threads. In all cases but Green threads, we assume that the process consists of three kernel threads that can be scheduled independently. The POSIX threads and fibers form a 1:1 model from user threads to kernel threads, with the primary difference being the intervention of the fiber scheduler. The Green threads model forms an N:1 relationship, as the kernel is not aware of the multiple threads running on top of the JVM. The LWP model is described as M:N because there is no direct link between the number of user and kernel threads. For each of the N kernel threads, there is a virtual CPU, and the LWP library dynamically maps the M user threads to these virtual CPUs according to its own scheduling policies.

![Mapping models for POSIX, Green threads, fibers, and lightweight processes](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.6.1.png)

Figure 6.2.2: Mapping models for POSIX, Green threads, fibers, and lightweight processes



//6.3. Race Conditions and Critical Sections[¶]
===============================================

[Multithreading](#term-multithreading) shares some of the basic principles as processes and multiprogramming. Specifically, threads are used to achieve [concurrent execution](#term-concurrency) of software. [[1]](#f40) Furthermore, most modern systems use kernels that are thread-aware. That is, thread scheduling decisions are made by the kernel itself, just as the choice of processes during context switches are. Consequently, thread execution is nondeterministic, leading to unpredictable timing. As threads in a process share the same virtual memory space, this nondeterministic timing can lead to several new types of bugs.

///6.3.1. Race Conditions[¶]
----------------------------

A [race condition](#term-race-condition) is the general term for a bug that arises from the nondeterministic timing of execution. If the threads are scheduled in one particular order, everything may be fine; however, if there is a slight delay or change in the order of scheduling, the process may encounter an error and crash. Race conditions can be extremely difficult to debug simply because the bug itself depends on the timing of nondeterministic events. It is quite common that the bug cannot be recreated by testers, particularly if the problematic timing results from a rare circumstance.

The Therac-25 radiation therapy machine is a classic and well-cited example of a race condition with deadly effects. The software for this machine included a one-byte counter. If the machine’s operator entered terminal input to the machine at the exact moment that the counter overflowed (quite common for a counter with only 256 possible values), a critical safety lock failed. This flaw made it possible for patients to receive approximately 100 times the intended radiation dose, which directly caused the deaths of three patients. [[2]](#f41)

Race conditions with threads can arise from a variety of causes, including (but not limited to):

> *   Two threads try to modify the same global variable at the same time. (See the discussion of “Critical Sections” below.)
> *   Data exists when a thread is created, but becomes invalid when the thread tries to access it later. For instance, a thread may be passed a pointer to a dynamically allocated data structure; before the thread gets a chance to run and retrieve the data, another thread calls `free()` to deallocate the memory on the heap.
> *   A thread returns a pointer to internal variables that were declared within the thread’s scope (e.g., a local variable on the thread’s stack). Once the thread exits, its stack (and all such variables) are destroyed.
> *   Multiple threads call the same non-[thread-safe](#term-thread-safe) function at the same time. (See below.)

///6.3.2. Critical Sections[¶]
------------------------------

A [critical section](#term-critical-section) is a sequence of instructions that must be executed [atomically](#term-atomic). That is, a critical section contains multiple instructions that create race conditions if they are [interleaved](#term-interleaved) with other threads. Every access to a global variable in a multithreaded program creates a critical section. For instance, consider the single line of C code `globalvar++;`, which simply adds 1 to a global variable. In x86 assembly language, this single line of code turns into three instructions:

```as
movq  _globalvar(%rip), %rsi    # copy from memory into %rsi register
addq  $1, %rsi                  # increment the value in the register
movq  %rsi, _globalvar(%rip)    # store the result back into memory
```

To understand how this creates a race condition, assume that the variable initially stores the value of 5 and there are two threads (A and B) running. If both threads get to this section of code at the same time, a race condition arises. Specifically, consider the effects of this order of _possible_ timing:

> *   Thread A executes the first line, copying the current value of `globalvar` (5) into the `%rsi` register.
> *   The kernel triggers a thread switch from A to B. When this occurs, the register values get copied into A’s stack.
> *   Thread B executes all three lines of code. The first line loads the current value (5) into %rsi, adds 1 to it, and stores the result (6) back into `globalvar`.
> *   Thread A resumes some time later, restoring the original value (5) from its stack back into the `%rsi` register. A then adds 1 and also stores the result (6) back into `globalvar`.
> *   When another thread tries to get the value of `globalvar` later, its value would be 6 even though it was (technically) incremented twice!

The key aspect of this example is the timing of the thread switch as the second step in this sequence of events. If thread A had been able to execute all three instructions without interruption (which is also possible!), the final value would be correct. The most frustrating aspect of this situation is that re-running the program is unlikely to experience that thread switch in exactly the same instant. As such, re-running the program may repeatedly yield the correct value of 7 over and over again, giving the programmer the false impression that this line of code is behaving correctly. [Synchronization](#term-synchronization) of critical sections is a large and complex topic that will be addressed in its own chapter.

///6.3.3. Thread Safety and Reentrancy[¶]
-----------------------------------------

A function is considered to be [thread-safe](#term-thread-safe) if it can be called concurrently by multiple threads and produce correct results. For instance, a [mutex](#term-mutex) is a mechanism that can force a sequence of instructions to execute in a mutually exclusive (i.e., atomic) manner; in the example above, using a mutex with the `globalvar` increment would make the bad interleaving impossible. A function is considered to be thread-safe if it can be called by multiple threads concurrently without introducing any race conditions.

[Reentrancy](#term-reentrant) is a closely related concept to thread safety, and the terms are sometimes (incorrectly) used interchangeably. The confusion arises because reentrant functions tend to be thread safe, and vice versa. However, a reentrant function is one that can be interrupted during execution and called again safely before the first call is resumed.

An example of a function that is neither thread-safe nor reentrant is the string tokenizer `strtok()`, which takes a string and breaks it into one substring at a time based on a specified separator. The problem with `strtok()` is that it uses an internal static pointer to keep track of where it needs to continue. That is, when `strtok()` returns one a token, the internal pointer is pointing to the next token within that particular string. If `strtok()` is interrupted and called by another thread, then the internal pointer will be switched to this second thread’s string. When the first thread resumes, it will then attempt to tokenize the second thread’s string rather than its own.

📦 C library functions – `<string.h>`

* * *

`char *strtok (char *restrict s, const char *restrict sep);`

Splits a string at instances of the substring `sep`; passing `NULL` as `s` continues with previous string.

`char *strtok_r(char *str, const char *sep, char **lasts);`

Reentrant string tokenizer that uses `lasts` to point to the continuation location.

The main requirements for reentrancy are that a function cannot hold any global or static non-constant data, and it cannot call any other non-reentrant function. It is still possible for a function to be reentrant but not thread safe. For instance, repeatedly using the value of a global variable is acceptable with reentrancy; but doing so is not thread safe, as another thread may change the value in between uses. On the other hand, a non-reentrant function can be made thread-safe by using a mutex to protect any critical sections.

Ultimately, when writing multithreaded code, programmers must take great care to ensure that the functions their threads use are appropriate. Using functions that are not thread-safe is an easy way to make debugging extraordinarily difficult.

[[1]](#id1)

Early thread implementations were handled solely within the process itself, with the thread library enforcing the thread scheduling. This style had several disadvantages. First, single- and multi-threaded processes were given the same quantum by the scheduler. If a process had 16 threads, each thread would be given only 1/16-th of the amount of CPU time as a single-threaded process. In fact, the situation was worse than this, as the thread library had to inject timed signals to force the switch from one thread to another, yielding significant performance overhead. Second, scheduling decisions had to be made twice and the code was redundant. In essence, the thread library was reproducing kernel code within the user-mode process itself. Consequently, it was possible for the library and the kernel to enforce different scheduling policies, leading to poor scheduling decisions. Finally, kernel thread-awareness is important to gain performance improvements with [multiprocessing](#term-multiprocessing) hardware (including [multicore](#term-multicore)). Once the kernel is aware that a particular process consists of several threads, the kernel can schedule multiple threads at the same time for the threads to benefit from shared caches. If the kernel is not aware of the threads, it will miss these opportunities for potentially considerable speedup.

[[2]](#id2)

The Therac-25 story is complicated and there are many lessons that should be studied by anyone involved in designing and creating software. Another problem with the system is that the software would frequently issue warnings based on potential problems. However, these warnings did not shut the system down and had no obvious effects or meaning to the operators. Consequently, operators learned to ignore all warnings. This behavior is a natural response, as the machine would have been entirely unusable if the operators had to stop and investigate every warning. In other words, software designers should be careful when handling failures and exceptional cases. Blaming users for problems that arise is irresponsible and can make bad situations significantly worse.



//6.4. POSIX Thread Library[¶]
==============================

In this section, we’ll return to the code from [Code Listing 6.1](ProcVThreads.html#cl6-1) and replace the comments with functions from the [POSIX thread library (pthreads)](#term-posix-thread-library). Pthread implementations are available for a wide variety of platforms, including Linux, macOS, and Windows. After considering the basic techniques to get threads started, we’ll look at how to pass arguments to and return data from the thread.

///6.4.1. Creating and Joining Threads[¶]
-----------------------------------------

Three functions define the core functionality for creating and managing threads. The `pthread_create()` function will create and start a new thread inside a process. The `start_routine` parameter specifies the name of the function to use as the thread’s entry point, just as `main()` serves as the main thread’s entry point. The pthread_exit() is used to exit the current thread and optionally return a value. Finally, the `pthread_join()` function is the thread equivalent of the `wait()` function for processes. That is, calling `pthread_join()` on a child thread will cause the current (parent) thread to wait until the child finishes and calls `pthread_exit()`.

📦 C library functions – `<pthread.h>`

* * *

`int pthread_create (pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine)(void*), void *arg);`

Create a new thread starting with at the `start_routine` function.

`void pthread_exit (void *value_ptr);`

Exit from the current thread.

`int pthread_join (pthread_t thread, void **value_ptr);`

Join a specified thread.

The arg argument to the `pthread_create()` function gets passed verbatim as the argument to the `start_routine()` function. That is, when the thread is created, it begins executing as if the programmer had called `start_routine (arg);` in their code. The only thing that is different is that the function will begin executing in a different thread.

The prototype for the `pthread_create()` function can be rather difficult to read if you are not completely comfortable with [function pointers](#term-function-pointer). Specifically, this prototype declares that the third parameter must be a pointer to a function that adheres to the following type declaration:

    void *start_routine (void *args);

This prototype requirement simplifies the interface specification for thread creation. The trade-off is that passing parameters becomes more complicated. The following program illustrates how to create a child thread and wait for it to finish.

```cpp
/* Code Listing 6.2:
   A POSIX thread version of "Hello world"
 */

#include <stdio.h>
#include <pthread.h>
#include <assert.h>

/* Function that will run in the child thread */
void *
start_thread (void *args)
{
  printf ("Hello from child\n");
  pthread_exit (NULL);
}

int
main (int argc, char **argv)
{
  pthread_t child_thread;

  /* Create a child thread running start_thread() */
  assert (pthread_create (&child_thread, NULL, start_thread, NULL) 
          == 0);

  /* Wait for the child to finish, then exit */
  pthread_join (child_thread, NULL);
  pthread_exit (NULL);
}
```

[Code Listing 6.2](#cl6-2) illustrates a subtle point that is not immediately obvious to novices: **The \`\`main()\`\` function defines a thread.** Specifically, every program begins with a single thread of execution with `main()` as its entry point. Once the pthread library creates an additional thread, it is important that `main()` should be treated as a thread. In the simplest terms, this perspective implies that `main()` needs to end with `pthread_exit()` instead of the standard return statement. Using `pthread_exit()` ensures that all threads are managed and run correctly.

The main thread uses `pthread_join()` function to wait for the thread running `start_thread()` to call `pthread_exit()`. The standard thread terminology uses the term [joining](#term-fork-join-pattern) to mean waiting on a thread to complete. When a thread exits, any resources associated with it (such as its run-time stack) remain allocated until the thread is joined. As such, the `pthread_join()` and `pthread_exit()` combination can be used to return a value from the exiting thread to its parent. In this example, nothing is returned, so both of these functions take NULL as an argument.

Creating threads with `pthread_create()` happens [asynchronously](#term-asynchronous). That is, `pthread_create()` requests the allocation of resources for a new thread and returns 0 if the request is successful. **The new thread may or may not begin running by the time \`\`pthread_create()\`\` returns.** In fact, the new thread may have already run to completion by the time `pthread_create()` returns! The timing for running both the new and existing threads are determined by the system (the pthread library and the kernel). From the programmer’s perspective, this choice is nondeterministic.

🐞🐛🐌 Bug Warning

* * *

There are a couple of common mistakes with pthread function parameters. With `pthread_create()`, the first parameter must point to a `pthread_t` instance. It is common to (incorrectly) declare the variable as a `pthread_t*`. Instead, the variable should be a `pthread_t` and the address of it should be passed as shown above. Similarly, the first parameter for `pthread_join()` must be a `pthread_t` instance, not a pointer. Next, the third parameter to `pthread_create()` must be the name of the function and must not include (). The parentheses would indicate that the function should be called (before creating the thread) and the function’s return value specifies the address to start at in the new thread. The following code sample illustrates these bugs.


```cpp
/* WRONG! thread should not be a pointer in this example */
pthread_t *thread;
/* WRONG! thread is invalid pointer and start should not have () */
pthread_create (thread, NULL, start (), NULL);
/* WRONG! pthread_join() doesn't take pointer as first parameter */
pthread_join (thread, NULL); 
```

///6.4.2. Attached and Detached Threads[¶]
------------------------------------------

In some scenarios, a thread will be created as [detached](#term-detached-thread) instead of joinable. When a detached thread exits, its resources are immediately reclaimed for more efficient reuse. However, once a thread is detached, it can never be joined. This means that no other thread can wait on it to finish. According to the pthread specification, all threads are supposed to be created as joinable by default. The `pthread_attr_setdetachstate()` function can be used to change the `attr` argument to `PTHREAD_CREATE_DETACHED` to override the default.

```cpp
pthread_attr_t attr;

/* Get the default set of attributes */
pthread_attr_init(&attr);

/* Mark the thread as detached */
pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
```

Alternatively, the `pthread_detach()` function can be used to detach a running thread instance. A thread can detach itself by using its own `pthread_t` identifier as an argument to `pthread_detach()`.

pthread_detach (pthread_self ());

📦 C library functions – `<pthread.h>`

* * *

`int pthread_detach (pthread_t thread);`

Detaches the specified thread.

`pthread_t pthread_self (void);`

Returns the current thread’s pthread_t identifier.



//6.5. Thread Arguments and Return Values[¶]
============================================

The `pthread_create()` imposes a strict format on the prototype of the function that will run in the new thread. It must take a single `void*` parameter and return a single `void*` value. The last parameter of `pthread_create()` is passed as the argument to the function, whereas the return value is passed using `pthread_exit()` and `pthread_join()`. This section looks at the details of these mechanisms and their implications.

///6.5.1. Passing a Single Argument to Threads[¶]
-------------------------------------------------

Passing a single argument to a thread seems straightforward, but is easy to do incorrectly. As a simple example to illustrate the danger, [Code Listing 6.5](#cl6-5) is designed to run in a separate thread:


```cpp
/* Code Listing 6.5:
   A thread that will print a single integer value
 */

void *
child_thread (void *args)
{
  /* POTENTIALLY DANGEROUS TIMING */
  int *argptr = (int *) args;
  int arg = *argptr;

  /* Print the local copy of the argument */
  printf ("Argument is %d\n", arg);
  pthread_exit (NULL);
}
```

The danger of this code can be illustrated with the loop in [Code Listing 6.6](#cl6-6). The intent is to pass the value 1 to the first thread, 2 to the second, and so on. However, it is critical to note that **there is only a single copy of the** `i` **variable**. That is, this code passes the address of the single variable to all 10 threads; the code almost certainly does not pass the intended values.

```cpp
/* Code Listing 6.6:
   Passing a pointer to a variable that repeatedly changes is a common error with threads
 */

/* BAD CODE - DON'T DO THIS */
/* What value is actually passed to the thread? */
for (int i = 1; i <= 10; i++)
  assert (pthread_create (&child[i], NULL, child_thread, &i) == 0);
```

The key problem is that thread creation and execution is [asynchronous](#term-asynchronous). That means that it is impossible to predict when each of the new threads start running. One possible timing is that all 10 threads are created first, leading to `i` storing the value 11. At that point, each of the threads dereference their respective `argptr` variable and all get the same value of 11.

One common solution to this problem is to cast numeric values as pointers, as shown in [Code Listing 6.7](#cl6-7). That is, the int `i` variable gets cast as a `(void*)` argument in the call to `pthread_create()`. Then, the `void*` argument to `child_thread()` casts the argument back to a `int` instance.

```cpp
/* Code Listing 6.7:
   Each thread should be given a separate value, rather than a shared address
 */

/* FIXED VERSION */
/* ints are passed by value, so a COPY gets passed to each call */
for (int i = 1; i <= 10; i++)
  assert (pthread_create (&child[i], NULL, child_thread, (void *)i) == 0);
```

What makes this code work is the fact that scalar variables (e.g., `int` variables) are passed using call-by-value semantics. When this code prepares for the `pthread_create()` call, a separate copy of the current value of the `i` variable is placed into a register or onto the stack. [Code Listing 6.8](#cl6-8) shows the corrected version of [Code Listing 6.5](#cl6-5). The `child_thread()` function then gets this copy, regardless of any changes to the original `i` variable. When the child thread then casts its `args` parameter to a local `arg_value`, it is working with the correct value that was passed.

```cpp
/* Code Listing 6.8:
   A safer version of Code Listing 6.5
 */

/* Convention: It is common to name a void* parameter with a name
   that begins with _, then cast it to a local variable that has
   the same (or nearly the same) name without the _. So _args will
   become args. Recall that _ has no special meaning and is treated
   like a normal alphabetical character. */

void *
child_thread (void *_args)
{
  /* Safe whenever size of int <= size of pointer (which is
     usually true) */
  int arg = (int) _args;

  /* Print the local copy of the argument */
  printf ("Argument is %ld\n", arg);
  pthread_exit (NULL);
}
```

🐞🐛🐌 Bug Warning

* * *

Casting integral values to pointers and back again is a common practice for passing parameters to pthreads. However, while it is generally safe in practice, it is potentially a bug on some platforms. Specifically, this technique relies on the fact that pointers are at least as large as standard integer types. That is, `int` variables are typically (but not required to be) 32 bits in size. Modern CPU architectures tend to use 32- or 64-bit addresses. As such, casting a 32-bit `int` up to a `void*` then back to a 32-bit `int` is safe.

On the other hand, assume the argument was declared as a `long` variable instance. If the code is running on a 32-bit architecture (which is not uncommon for virtualized systems) but the `long` type is 64 bits in size, then half of the argument is lost by down-casting to the pointer for the call to `pthread_create()`!

///6.5.2. Passing Multiple Arguments to Threads[¶]
--------------------------------------------------

When passing multiple arguments to a child thread, the standard approach is to group the arguments within a `struct` declaration, as shown in [Code Listing 6.9](#cl6-9). The address of the `struct` instance gets passed as the `arg` to `pthread_create()`. The new thread’s entry point receives a `void*` parameter that can then be cast into the `struct` type.

```cpp
/* Code Listing 6.9:
   Passing multiple arguments to a thread requires grouping them into a struct
 */

/* Assume we have:
     struct thread_args {
       int first;
       const char *second;
     };
 */

struct thread_args *args = malloc (sizeof (struct thread_args));
args->first = 5;
args->second = "Hello";

/* Note that the data structure resides on the heap */
assert (pthread_create (&child, NULL, hello_thread, args) == 0);
```

[Code Listing 6.10](#cl6-10) shows the new thread receiving the pointer to the `struct` and freeing the allocated memory when it is finished with the data.

```cpp
/* Code Listing 6.10:
   The child thread receives multiple values through the passed struct
 */

/* Using the convention of casting _args to args */
void *
hello_thread (void *_args)
{
  /* Cast args into a meaningful pointer type that we can use */
  struct thread_args *args = (struct thread_args *) _args;
  printf ("First: %d; Second: '%s'\n", args->first, args->second);

  /* Do not forget to free the struct used for arguments */
  free (args);
  pthread_exit (NULL);
}
```

🐞🐛🐌 Bug Warning

* * *

A common mistake with passing arguments in this manner is to declare the `struct` instance as a local variable instead of using dynamic allocation. The problem, again, is the asynchronous nature of `pthread_create()`. Consider this sample code:

```cpp
/* Create a local instance on the current thread's stack */
struct thread_args args;
args.first = 5;
args.second = "Hello";

/* Pass a reference to the local instance */
assert (pthread_create (&child, NULL, hello_thread, &args) == 0);

/* Parent thread exits, but the child may not have run yet */
pthread_exit (NULL);

/* Future references to args are invalid! */
```

If the child thread runs immediately before `pthread_create()` returns, then everything would be fine. However, there is no guarantee that this happens. Instead, it is just as likely that `pthread_create()` returns and the parent thread exits. Once that happens, all data on the parent thread’s stack (including the `struct thread_args` instance) become invalid. The child thread now has a dangling pointer to potentially corrupted data. This is another example of a race condition that can happen with threads.

///6.5.3. Returning Values from Threads[¶]
------------------------------------------

There are three common ways to get return values back from a thread. All three use techniques that are similar to those used for passing arguments. [Code Listing 6.11](#cl6-11) shows one simple technique, which is to augment the `struct` declaration to include space for any return values.

```cpp
/* Code Listing 6.11:
   Allocating space for a return value as part of the struct passed
 */

/* Approach 1: Include space for return values in the struct */

/* Thread argument struct declaration */
struct numbers {
  int a;
  int b;
  int sum;
};

void *
sum_thread (void *_args)
{
  /* Cast the arguments to the usable struct type */
  struct numbers *args = (struct numbers *) _args;

  /* Place the result into the struct itself (on the heap) */
  args->sum = args->a + args->b;
  pthread_exit (NULL);
}
```

The child thread receives a pointer to the `struct` instance, using the input parameters as needed. In this case, the values of `a` and `b` are added, and the resulting sum is copied back into the `struct`. As shown in [Code Listing 6.12](#cl6-12), the main thread uses `pthread_join()` to wait until the child thread exits. Once the child finishes, the main thread can retrieve all three values (`a`, `b`, and `sum`) from the `struct` itself.

```cpp
/* Code Listing 6.12:
   The main thread can retrieve the return value from the struct after joining the child thread
 */

/* Allocate and pass a heap instance of the struct type */
struct numbers *args = calloc (sizeof (struct numbers), 1);
args->a = 5;
args->b = 8;

assert (pthread_create (&child, NULL, sum_thread, args) == 0);

/* Wait for the thread to finish */
pthread_join (child, NULL);
/* The struct is still on the heap, so the result is accessible */
printf ("%d + %d = %d\n", args->a, args->b, args->sum);

/* Clean up the struct instance */
free (args);
args = NULL;
```

There are three key observations about this approach:

> *   The main and the child threads have access to both the input and the output. This fact means that the main thread has information about how this particular child thread was invoked. If the main thread is keeping track of many threads, this additional information may be helpful.
> *   Responsibility for memory management resides in one location: the main thread. If responsibility is split between the programmer maintaining the main thread and the programmer maintaining the child thread, there is the possibility for miscommunication leading to memory leaks (or worse, premature de-allocation).
> *   The major disadvantage of this approach is that the input parameters may be kept on the heap for much longer than needed, particularly if the child thread runs for a significant amount of time.

[Code Listing 6.13](#cl6-13) shows an alternative approach for simple scalar return types, which is to reuse the trick of casting to and from the `void*` type. When a thread calls `pthread_exit()`, it can specify a pointer to return as an argument.

```cpp
/* Code Listing 6.13:
   A second technique to return a value is to pass it as the thread’s exit code
 */

/* Approach 2: Scalar return types as void* with pthread_exit() */

/* Thread argument struct contains only input parameters */
struct numbers {
  int a;
  int b;
};

void *
sum_thread (void *_args)
{
  /* Cast the argument to the usable struct */
  struct numbers *args = (struct numbers *) _args;

  /* Pass the result back by casting it to the void* */
  pthread_exit ((void *) (args->a + args->b));
}
```

[Code Listing 6.14](#cl6-14) shows how the main thread calls `pthread_join()` to retrieve the pointer. Unless the thread has been detached (or it was created with the `PTHREAD_CREATE_DETACHED` attribute), the pointer returned with `pthread_exit()` will remain associated with the thread until it is joined.

```cpp
/* Code Listing 6.14:
   Retrieving a thread’s exit code when it is joined
 */

/* Allocate the struct like before and pass it to the thread */
struct numbers *args = calloc (sizeof (struct numbers), 1);
args->a = 5;
args->b = 8;

assert (pthread_create (&child, NULL, sum_thread, args) == 0);

/* Wait for thread to finish and retrieve the void* into sum */
void *sum = NULL;
pthread_join (child, &sum);
printf ("Sum: %d\n", (int) sum);

free (args);
args == NULL;
```

[Code Listing 6.15](#cl6-15) shows a third approach to returning values from the thread. In this style, the child thread allocates a separate `struct` dynamically to hold the return values. This technique allows a thread to return multiple values rather than a single scalar. For instance, consider the following `calculator` thread. It receives two `int` values as input and returns the results of five simple arithmetic operations.

```cpp
/* Code Listing 6.15:
   The child can dynamically allocate space for the return values
 */

/* Approach 3: Allocate separate struct for return values */

/* struct for passing arguments to the child thread */
struct args {
  int a;
  int b;
};

/* struct for returning results from the child thread */
struct results {
  int sum;
  int difference;
  int product;
  int quotient;
  int modulus;
};

void *
calculator (void *_args)
{
  /* Cast the args to the usable struct type */
  struct args *args = (struct args *) _args;

  /* Allocate heap space for this thread's results */
  struct results *results = calloc (sizeof (struct results), 1);
  results->sum        = args->a + args->b;
  results->difference = args->a - args->b;
  results->product    = args->a * args->b;
  results->quotient   = args->a / args->b;
  results->modulus    = args->a % args->b;
  /* De-allocate the input instance and return the pointer to
     results on heap */
  free (args);
  pthread_exit (results);
}
```

It is critical to note that the struct instance here must be allocated dynamically. Once the thread calls `pthread_exit()`, everything on its stack becomes invalid. A thread should never pass a pointer to a local variable with `pthread_exit()`.

Retrieving the returned data can be accomplished with `pthread_join()`. In the following example, the main thread creates five separate instances of the `calculator` thread. Each of these child threads gets a pointer to a unique `struct args` instance with the corresponding parameters. Each child then allocates its own `struct results` instance on the heap. This allows the data to persist after the thread has finished. In [Code Listing 6.14](#cl6-14), the main thread gets each thread’s pointer one at a time, with a separate call to `pthread_join()`. Since the child thread has already finished at this point, the main thread must bear the responsibility for calling `free()` to de-allocate the `struct` results instance.

```cpp
/* Code Listing 6.16:
   The main thread passes arguments to the child threads and frees the results
 */

/* Create 5 threads, each calling calculator() */
pthread_t child[5];

/* Allocate arguments and create the threads */
struct args *args[5] = { NULL, NULL, NULL, NULL, NULL };
for (int i = 0; i < 5; i++)
  {
    /* args[i] is a pointer to the arguments for thread i */
    args[i] = calloc (sizeof (struct args), 1);

    /* thread 0 calls calculator(1,1)
       thread 1 calls calculator(2,4)
       thread 2 calls calculator(3,9)
       and so on... */
    args[i]->a = i + 1;
    args[i]->b = (i + 1) * (i + 1);
    assert (pthread_create (&child[i], NULL, calculator, args[i]) 
            == 0);
  }

/* Allocate an array of pointers to result structs */
struct results *results[5];
for (int i = 0; i < 5; i++)
  {
    /* Passing results[i] by reference creates (void **) */
    pthread_join (child[i], (void **)&results[i]);

    /* Print each of the results and free the struct */
    printf ("Calculator (%d, %2d) ==> ", i+1, (i+1) * (i+1));
    printf ("+:%3d;   ", results[i]->sum);
    printf ("-:%3d;   ", results[i]->difference);
    printf ("*:%3d;   ", results[i]->product);
    printf ("/:%3d; ", results[i]->quotient); 
    printf ("%%:%3d\n", results[i]->modulus); 
    free (results[i]); 
}
```

🐞🐛🐌 Bug Warning

* * *

All of the functions for creating threads, passing arguments, and getting return values involve a lot of pointers. Furthermore, the pointers are dereferenced and manipulated asynchronously because of the nature of multithreading. It is vital to remember the types and lifetimes of each pointer and the corresponding data structure.

> *   The first parameter for `pthread_create()` is a `pthread_t*`. The argument should typically be an existing `pthread_t` passed by reference with the `&` operator.
> *   The final parameter to `pthread_create()` must either be a scalar (cast as a pointer) or a pointer to data that persists until the child thread runs. That is, the target of the pointer must not be modified by the main thread until the child thread has been joined (to guarantee the child has run).
> *   The parameter to `pthread_exit()` must be a scalar value (cast as a pointer) or a pointer to non-stack data. The data must be guaranteed to be valid even after the thread has been completely destroyed.
> *   The final parameter to `pthread_join()` must be a pointer that is passed by reference. That is, `pthread_join()` will change this pointer to point to the returned data structure.



//6.6. Implicit Threading and Language-based Threads[¶]
=======================================================

The POSIX thread library is a powerful and robust mechanism for concurrent systems programming. However, the library places a significant burden on the programmer to ensure that the implementation avoids race conditions and other bugs. In [Synchronization Problems](SynchProblemsOverview.html), we will examine common patterns that emerge in these types of programs and how to avoid subtle errors.

Since threads were first introduced, language designers have explored a number of techniques that reduce the complexity and responsibility of managing threads. This section will examine three different approaches for making multithreading easier. The first approach is a general style called [implicit threading](#term-implicit-threading) which aims to hide the management of threads as much as possible. The second approach is to treat threads as objects in languages like Java and Python. The third (and most modern) approach is to design the language around the concept of concurrency as a fundamental feature.

///6.6.1. Implicit Threading with OpenMP[¶]
-------------------------------------------

Implicit threading is the use of libraries or other language support to hide the management of threads. In the context of C, the most common implicit threading library is [OpenMP](#term-openmp). OpenMP uses the `#pragma` compiler directive to detect and insert additional library code at compile time. As an example, consider the prime number calculator from the Extended Examples. [Code Listing 6.17](#cl6-17) shows the OpenMP equivalent.

```cpp
/* Code Listing 6.17:
   OpenMP can make some multithreading trivial to implement
 */

#include <stdio.h>
#include <stdbool.h>
#include <omp.h>

int
main (int argc, char *argv[])
{
  /* Set up the overall algorithm parameters */
  unsigned long end = 100000000L;
  unsigned long iter = 0;
  unsigned long count = 0;

  /* OpenMP parallel for-loop with reduction on count. Each thread
     will have its own count, but they'll be combined when all
     threads are done. */
  #pragma omp parallel for default(shared) private(iter) \
          reduction(+:count)
  for (unsigned long alue = 2; value < end; value++)
    {
      bool is_prime = true;
      for (iter = 2; iter * iter <= value && is_prime; iter++)
        if (value % iter == 0)
          is_prime = false;
      if (is_prime)
        count++;
    }

  printf ("Total number of primes less than %ld: %ld\n", end, 
          count);
  return 0;
}
```

With implicit threading, the focus of the programmer is on writing the algorithm rather than the multithreading. The OpenMP library itself takes care of managing the threads. Specifically, the `#pragma` line indicates that OpenMP (`omp`) should parallelize a for-loop (`parallel for`) with some constraints on the variables. The OpenMP implementation on that system will then inject code to perform the thread creation and join.

OpenMP in C is built on top of the pthread library. As such, any code that can be written using OpenMP can be converted into a more verbose pthread equivalent. However, the disadvantage of OpenMP is that it only works for certain types of tasks. There are many types of programs, such as the keyboard listener example, that can be implemented in pthreads but not OpenMP.

///6.6.2. Threads as Objects[¶]
-------------------------------

In other languages, traditional object-oriented languages provide explicit multithreading support with threads as objects. In these types of languages, classes are written to either extend a thread class or implement a corresponding interface. This style resembles the pthread approach, as the code is written with explicit thread management. However, the encapsulation of data within the classes and additional synchronization features simplify the task.

### 6.6.2.1. Java Threads[¶]

Java provides both a `Thread` class and a `Runnable` interface that can be used, as shown in [Code Listing 6.18](#cl6-18). Both require implementing a public void `run()` method that defines the entry point of the thread. Once an instance of the object is allocated, the thread can be started by invoking the `start()` method on it. As with pthreads, starting the thread is asynchronous, so the timing of the execution is nondeterministic.

```java
/* Code Listing 6.18:
   Java offers both a class and an interface to support threads as objects
 */

class ThreadExtender extends Thread {
  public void run() {
    /* Implement code here */
  }
}

class RunImplement implements Runnable {
  public void run() {
    /* Implement code here */
  }
}

public static void main(String args[]) {
  /* Instantiate the ThreadExtender and start it */
  ThreadExtender te = new ThreadExtender();
  te.start();

  /* Instantiate the Runnable version and start it */
  RunImplement ri = new RunImplement();
  Thread t = new Thread(ri);
  t.start();

  /* Join the threads */
  te.join();
  j.join();
}
```

### 6.6.2.2. Python Threads[¶]

[Code Listing 6.19](#cl6-19) demonstrates two mechanisms for multithreading in Python. One approach is similar to the pthread style, where a function name is passed to a library method `thread.start_new_thread()`. This approach is very limited and lacks the ability to join or terminate the thread after it starts. A more flexible technique is to use the threading module to define a class that extends threading. Similar to the Java approach, the class must have a `run()` method that provides the thread’s entry point. Once an object is instantiated from this class, it can be explicitly started and joined later.

```py
# Code Listing 6.19:
# Python offers a pthread-like interface and an object-oriented module for threading */

#!/usr/bin/python

import thread
import threading
import time

# Low-level procedural approach with the thread module
def proc_thread (name):
    # thread code here
    print "Ran thread " + name

# Use thread module to create and start the thread
try:
    thread.start_new_thread(proc_thread, ("First Thread", ))
except:
    print "Failed to start thread"

# OOP approach using the threading module
class ObjThread (threading.Thread):
    def __init__(self, name):
        threading.Thread.__init__(self)
        self.name = name
    def run(self):
        # Thread entry point here
        print "Running object thread " + self.name

# Create an instance of the object to start and join
threadObj = ObjThread("Second Thread")
threadObj.start()
threadObj.join()

# Delay to make sure both run
time.sleep(2)
```

///6.6.3. Concurrency as Language Design[¶]
-------------------------------------------

Languages such as C, Java, and Python were all designed before [multicore](#term-multicore) architectures rose to prominence in the early 2000s. As such, multithreading support in these languages was added as a supplement to the language, rather than a core feature. These languages were originally designed for a uniprocessing procedural or object-oriented paradigm. As a result, the memory models that underlie these languages are not adequate to prevent race conditions. The multithreading libraries had to provide additional features that allowed programmers to synchronize access to shared data. Or, put another way, programmers were forced to do extra work to make their programs work correctly.

Newer programming languages have avoided this problem by building assumptions of concurrent execution directly into the language design itself. For instance, Go combines a trivial implicit threading technique (goroutines) with channels, a well-defined form of message-passing communication. Rust adopts an explicit threading approach similar to pthreads. However, Rust has very strong memory protections that require no additional work by the programmer.

### 6.6.3.1. Goroutines[¶]

The Go language includes a trivial mechanism for implicit threading: place the keyword go before a function call. In [Code Listing 6.20](#cl6-20), the line go `keyboard_listener(messages)` launches a new thread that will execute the keyboard listener function. The new thread is passed a connection to a message-passing _channel_. Then, the main thread calls `success := <-messages`, which performs a blocking read on the channel. Once the user has entered the correct guess of 7, the keyboard listener thread writes to the channel, allowing the main thread to progress.

Channels and [goroutines](#term-goroutine) are core parts of the Go language, which was designed under the assumption that most programs would be multithreaded. This design choice streamlines the development model, allowing the language itself to bear the responsibility for managing the threads and scheduling.

```cpp
/* Code Listing 6.20:
   Go provides built-in concurrency techniques with goroutines and channels for communication
*/

package main

import (
	"bufio"
	"fmt"
	"os"
	"strconv"
	"strings"
)

/* Main program entry point */
func main() {
	/* Create a channel for communication */
	messages := make(chan string)

	fmt.Print("Guess a number between 1 and 10: ")

	/* Start keyboard listener as a goroutine with the channel */
	go keyboard_listener(messages)

	/* Wait until there is data in the channel */
	success := <-messages
	if success == "true" {
		fmt.Println("You must have guess 7.")
	}
}

/* Define the keyboard listener thread with channel back to main */
func keyboard_listener(messages chan string) {
	stdin := bufio.NewReader(os.Stdin)

	/* Loop forever, reading keyboard input */
	for {
		text, _ := stdin.ReadString('\n')

		/* Try to convert the input text to an int 7 */
		value, err :=
			strconv.ParseInt(strings.Trim(text, "\n"), 10, 32)
		if err == nil {
			if value == 7 {
				/* Success. Send a message back through
				   the channel and exit */
				messages <- "true"
				return
			}
		}
		fmt.Print("Wrong. Try again. ")
	}
}
```

### 6.6.3.2. Rust Concurrency[¶]

Rust is another language that has been created in recent years, with concurrency as a central design feature. [Code Listing 6.21](#cl6-21) illustrates the use of `thread::spawn()` to create a new thread, which can later be joined by invoking `join()` on it. The argument to `thread::spawn()` beginning at the `||` is known as a closure, which can be thought of as an anonymous function. That is, the child thread here will print the value of `x`.

```rust
/* Code Listing 6.21:
   Rust threads can be created with anonymous functions known as closure
 */

use std::thread;

fn main() {
    /* Initialize a mutable variable x to 10 */
    let mut x = 10;

    /* Spawn a new thread */
    let child_thread = thread::spawn(move || { 
        /* Make the thread sleep for one second, then print x */
        x -= 1;
        println!("x = {}", x)
    });

    /* Change x in the main thread and print it */
    x += 1;
    println!("x = {}", x);

    /* Join the thread and print x again */
    child_thread.join();
}
```

However, there is a subtle point in this code that is central to Rust’s design. Within the new thread (executing the code in the closure), the `x` variable is distinct from the `x` in other parts of this code. Rust enforces a very strict memory model (known as _ownership_) which prevents multiple threads from accessing the same memory. In this example, the move keyword indicates that the spawned thread will receive a separate copy of `x` for its own use. Regardless of the scheduling of the two threads, the main and child threads cannot interfere with each other’s modifications of `x`, because they are distinct copies. It is impossible for the two threads to share access to the same memory.

In this small example, the issue of ownership may not seem to be a big deal. However, if you learn more about Rust and concurrency, you’ll quickly realize that it is. Ownership makes Rust very unique and makes it a very powerful language for concurrent programming. The crux is that ownership completely eliminates several types of race conditions, since it is impossible for multiple threads to share the same memory location. Furthermore, it achieves this memory safety **without imposing any run-time performance penalty**. Ownership constraints are checked and enforced at compile-time. This combination of memory safety and efficient performance gives Rust a significant advantage over other languages in regard to multithreading.



//6.7. Extended Example: Keyboard Input Listener[¶]
===================================================

This example illustrates how threads can be created to perform a particular task. This style of multithreading forms the basis of programs that have graphical user interfaces. Specifically, the example shown here acts as a custom keyboard event listener; the child thread continually reads from the keyboard until the user enters exactly a 7. Similarly, programs with mouse event handlers use threads specifically for that purpose.

```cpp
#include <stdio.h>
#include <stdbool.h>
#include <stdlib.h>
#include <string.h>
#include <pthread.h>
#include <assert.h>

#define MAX_LENGTH 40

void flush_buffer (char *buffer);

void *
keyboard_listener (void *args)
{
  char buffer[MAX_LENGTH + 1];
  memset (buffer, 0, MAX_LENGTH + 1);

  /* Read a line of input from STDIN */
  while (fgets (buffer, MAX_LENGTH, stdin) != NULL)
    {
      /* Try to convert input to integer 7. All other values
         are wrong. */
      long guess = strtol (buffer, NULL, 10);
      if (guess != 7)
        {
          printf ("Wrong. Try again. ");
          /* Clear the buffer for another try. */
          flush_buffer (buffer);
          memset (buffer, 0, MAX_LENGTH);
        }
      else
        {
          /* Successfully read a 7 from input; exit with true */
          printf ("Congratulations!\n");
          pthread_exit ((void*)true);
        }
    }

  /* In case input fails for some reason, exit thread with false */
  pthread_exit ((void*)false);
}

int
main (int argc, char **argv)
{
  pthread_t guess;
  printf ("Guess a number between 1 and 10: ");

  /* Create a thread that reads until a 7 is input and exits. */
  assert (pthread_create (&guess, NULL, keyboard_listener, NULL)
          == 0);

  /* Wait until the thread exits */
  void *result;
  pthread_join (guess, &result);

  if (result)
    printf ("You must have guessed 7.\n");

  return 0;
}

/* Helper function for reading input. If the input line was too
   long (so no '\n' was read), clear out the rest of the input to 
   try again. */
void
flush_buffer (char *buffer)
{
  int chr;
  
  /* Check for '\n' */
  char *newline = strchr (buffer, '\n');

  /* If none found, retrieve and discard rest of the characters */
  if (newline == NULL)
    while (((chr = getchar()) != '\n') && !feof (stdin) &&
           !ferror (stdin))
      ;
}
```



//6.8. Extended Example: Concurrent Prime Number Search[¶]
==========================================================

This second example shows a different style of multithreaded programming. Instead of assigning a unique task to a particular thread, this example creates multiple threads that all execute the same code, but on different input parameters. For systems with [multiprocessing](#term-multiprocessing) capabilities, this style of programming forms the foundation of parallel execution. Since each thread’s calculations are independent of all the others’ calculations, they can run in parallel and compute the same answers.

```cpp
#include <stdio.h>
#include <stdbool.h>
#include <pthread.h>
#include <stdlib.h>
#include <assert.h>

/* Specify the number of threads to run in concurrently */
#define NUM_THREADS 4

/* Each thread gets a start and end number and returns the number
   Of primes in that range */
struct range {
  unsigned long start;
  unsigned long end;
  unsigned long count;
};

/* Thread function for counting primes */
void *
prime_check (void *_args)
{
  /* Cast the args to a usable struct type */
  struct range *args = (struct range*) _args;
  unsigned long iter = 2;
  unsigned long value;

  /* Skip over any numbers < 2, which is the smallest prime */
  if (args->start < 2) args->start = 2;

  /* Loop from this thread's start to this thread's end */
  args->count = 0;
  for (value = args->start; value <= args->end; value++)
    {
      /* Trivial and intentionally slow algorithm:
         Start with iter = 2; see if iter divides the number evenly.
         If it does, it's not prime.
         Stop when iter exceeds the square root of value */
      bool is_prime = true;
      for (iter = 2; iter * iter <= value && is_prime; iter++)
        if (value % iter == 0) is_prime = false;

      if (is_prime) args->count++;
    }

  /* All values in the range have been counted, so exit */
  pthread_exit (NULL);
}

int
main (int argc, char **argv)
{
  pthread_t threads[NUM_THREADS];
  struct range *args[NUM_THREADS];
  int thread;

  /* Count the number of primes smaller than this value: */
  unsigned long number_count = 100000000L;

  /* Specify start and end values, then split based on number of
     threads */
  unsigned long start = 0;
  unsigned long end = start + number_count;
  unsigned long number_per_thread = number_count / NUM_THREADS;

  /* Simplification: make range divide evenly among the threads */
  assert (number_count % NUM_THREADS == 0);

  /* Assign a start/end value for each thread, then create it */
  for (thread = 0; thread < NUM_THREADS; thread++)
    {
      args[thread] = calloc (sizeof (struct range), 1);
      args[thread]->start = start + (thread * number_per_thread);
      args[thread]->end =
        args[thread]->start + number_per_thread - 1;
      assert (pthread_create (&threads[thread], NULL, prime_check,
                              args[thread]) == 0);
    }

  /* All threads are running. Join all to collect the results. */
  unsigned long total_number = 0;
  for (thread = 0; thread < NUM_THREADS; thread++)
    {
      pthread_join (threads[thread], NULL);
      printf ("From %ld to %ld: %ld\n", args[thread]->start,
              args[thread]->end, args[thread]->count);
      total_number += args[thread]->count;
      free (args[thread]);
    }

  /* Display the total number of primes in the specified range. */
  printf ("===============================================\n");
  printf ("Total number of primes less than %ld: %ld\n", end, 
          total_number);

  return 0;
}
```

The time command-line utility can be used to measure the amount of time it takes to run a program. However, time gives you multiple pieces of information to interpret. The _real_ time is how much total time elapsed on the system clock from the start to finish. The _user_ time is how much total CPU time was spent on all of the processors combined. For instance, if a program runs in perfect parallelism on 2 cores for 15 minutes, the real time would be 15 minutes, but the user time would be 30 minutes.

The following output shows the results for running the prime number program above twice. The first set of results (approximately 7.5 minutes for both real and user time) used only a single thread. The second set used 4 threads (best choice for the quad-core system used).

```sh
$ time ./prime
From 2 to 99999999: 5761455
===============================================
Total number of primes less than 100000000: 5761455

real	7m39.361s
user	7m38.650s
sys	0m0.325s

[... recompile to use 4 threads ...]

$ time ./prime 
From 2 to 24999999: 1565927
From 25000000 to 49999999: 1435207
From 50000000 to 74999999: 1393170
From 75000000 to 99999999: 1367151
===============================================
Total number of primes less than 100000000: 5761455

real	3m24.030s
user	9m49.405s
sys	0m0.438s
```

First, note that the parallel version took more total CPU time (almost 10 minutes); this kind of overhead is typical, as there are overhead performance penalties for using multiple threads. Second, the real time was only about 1/3 of the user time, not 1/4. In practice, no system ever achieves perfect parallel execution. Chapter 9 will explain the limits of parallelism in more detail.


/Chapter 7   Synchronization Primitives[¶]
==========================================

//7.1. Synchronization Primitives[¶]
====================================

![Timeline of major CSF topics with Multicore and Threads highlighted](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Timeline.7.png)

> “It’s not an idea until you write it down.”
> 
> Ivan Sutherland

The resurgence of interest in multithreading in the late 1990s brought synchronization with it as a core topic. When multiple threads share an address space, they must ensure not to conflict in their memory accesses. Edsger Dijkstra’s THE OS introduced semaphores as a software construct in 1968, while the POSIX.1c-1995 standard defined an interface for them. In addition, higher-level primitives have made synchronization more manageable as a key infrastructure component for concurrent software.

🚀 Chapter Objectives

* * *

In this chapter, we will address the following instructional objectives:

*   We will justify the need for synchronization primitives as solutions to race conditions and timing constraints.
*   We will compare and contrast primitive synchronization mechanisms and what problems they solve.
*   We will examine code using the POSIX thread library’s synchronization primitive implementations.
*   We will identify the conditions that lead to deadlock, a difficult race condition that can arise in synchronized software.

Concurrent programs introduce a new class of software bugs called [race conditions](#term-race-condition). Race conditions arise whenever the _timing_ of multiple [threads of execution](#term-thread) determines the outcome. The problem is that the scheduling of threads is out of the control of the programmer who wrote the code and the user running the program.

The scheduling is determined by the operating system kernel, a special program that manages the system’s resources. In modern general-purpose computer systems, there may be hundreds or thousands of threads that must take turns getting access to a small number of CPU processing cores. The kernel makes the decision of which thread to run based on what else the machine is doing, how long each thread has already run, and many other factors. The programmer cannot predict or control any of these factors; in fact, the exact combination of factors is likely to be unique every time the program runs.

In many cases, the [nondeterministic](#term-nondeterminism) nature of thread scheduling is not a problem. If one thread is responsible for factoring 5,182,397,724,980 into a product of primes while another is computing the billionth digit of Pi, it probably doesn’t matter which calculation finishes first. However, if these two results are going to be combined in some way, the program must guarantee that both of them have completed before attempting this third step. That is, these steps must be [synchronized](#term-synchronization).

There are several forms of synchronization that various programs may require. Some common examples include the following:

> *   Multiple threads may try to modify a shared data structure, but each write must be performed one at a time to ensure the final result is correct.
> *   The system may need to place a limit on the number of simultaneous accesses to a shared resource to avoid delays. For instance, a web server may limit the number of incoming network requests to make sure it does not consume too much memory.
> *   Particular events may need to be performed in a specific order, such as when the output produced by one thread is required as input to another thread.
> *   The system may need to make sure that a minimum number of calculations have been completed before proceeding to take some sort of action. For example, some programs run multiple different algorithms that should produce the same result; the system may require at least three algorithms agree before having the confidence that the result is correct.

The [synchronization primitives](#term-synchronization-primitive) described in this chapter can achieve all of these goals. Synchronization primitives are variable types that have one critical aspect: operations that manipulate the primitives are guaranteed to be [atomic](#term-atomic). This feature contrasts with standard variables that lack this guarantee. For instance, consider the simple line of C code `x++` that increments an `int` variable. This line requires three separate instructions to load the variable into a register, increment the register, then store the result back into memory. In between these instructions, the kernel might interrupt the execution and switch to another thread. In contrast, a synchronization primitive would use special-purpose hardware techniques to guarantee that this kind of multi-step operation happens in a single step that cannot be interrupted by the kernel.

Careless misuse of synchronization primitives can cause a variety of problems. For one thing, some synchronization primitives impose a performance penalty by turning off all parallel execution in a system. This shutdown requires CPU cores temporarily save copies of their data and stop running; they may also need to perform pending writes back to multiple levels of cache. Even worse, synchronization primitives can lead to [deadlock](#term-deadlock), a condition in which two (or more) threads are simultaneously waiting on each other. Finally, some synchronization algorithms contain subtle flaws that can be easily overlooked.

The goal for this chapter is to examine the common synchronization primitives that are widely supported, particular in the POSIX thread library. The focus here is on the basic intent of each primitive and some principles for avoiding deadlock and significant performance penalties. In [Synchronization Problems](SynchProblemsOverview.html), we will explore how to combine primitives to solve more complex problems.



//7.2. Critical Sections and Peterson’s Solution[¶]
===================================================

[Critical sections](#term-critical-section) are sequences of instructions that cannot be interleaved among multiple threads. A simple example of a critical section arises when two threads share a global variable `globalvar` and both try to change its value with `globalvar++`. Recall from the [previous chapter](RaceConditions.html) that this line of code compiles into three assembly-language instructions to load, modify, and store the result.

```as
movq  _globalvar(%rip), %rsi    # copy from memory into %rsi register
addq  $1, %rsi                  # increment the value in the register
movq  %rsi, _globalvar(%rip)    # store the result back into memory
```

When multiple threads execute this code, a race condition arises since the lines can be interleaved. For instance, an interrupt during the second instruction’s execution could trigger a switch from one thread to another. [Code Listing 7.1](#cl7-1) shows the interleaved sequence of instructions (distinct indentation denotes the two threads) that would result from this unfortunate timing of the interrupt.

```as
# Code Listing 7.1:
# A possible sequence of assembly language instructions with a race condition

# Thread A copies globalvar (5) into %rsi, adds 1 to it
movq  _globalvar(%rip), %rsi
addq  $1, %rsi

# Interrupt: Kernel switches to start running thread B

    # Thread B begins same code, also loads value of 5 into %rsi
    movq  _globalvar(%rip), %rsi
    addq  $1, %rsi
    movq  %rsi, _globalvar(%rip)
    # Thread B has copied 6 back into globalvar in memory

# Kernel later switches back to thread A

# Thread A also copies 6 back into globalvar
movq  %rsi, _globalvar(%rip)
```

The result of this interleaving, assuming `globalvar` was originally 5, is that the threads (together) performed `globalvar++` twice, but the value only gets changed to 6 instead of the correct value of 7. The problem is that an interrupt occurred before the first thread was able to update `globalvar`, then the kernel switched to a different thread that read the old value. If this switch had not occurred until later, then the first thread would have updated `globalvar` with the value of 6, which is what the second thread would observe instead of the outdated 5.

It is important to note that the problem isn’t the interrupt, per se. If the interrupt occurs and the kernel switches to _any other thread_, or if the second thread were executing a different portion of code that had nothing to do with `globalvar`, the correct result would be produced. The error arises because of the combination of the interrupt/switch with the fact that both threads are accessing the same variable.

///7.2.1. Peterson’s Solution[¶]
--------------------------------

One approach to solving the problem of critical sections is to employ Peterson’s solution, an algorithmic approach that uses shared memory to declare intentions. [Code Listing 7.2](#cl7-2) shows the algorithm, which uses a `bool` array (`flag`) to signify a thread’s intent to enter and an `int` variable (`turn`) to indicate which thread is allowed to enter. [[1]](#f42)

```cpp
/* Code Listing 7.2:
   Peterson's solution to solving the critical section problem
 */

flag[self] = true;
turn = other;

/* busy wait until it is safe to enter */
while (flag[other] == true && turn == other) ;

/* critical section */
flag[self] = false;
```

The `turn` variable plays a critical role in ensuring the correct functioning of the solution. Specifically, this variable makes threads adopt a _polite_ approach to requesting access: when a thread wants to enter the critical section, it must declare that the other thread gets to go first. Since this type of variable manipulation consists of a single write to memory, it can be considered atomic. Consequently, this update does not create a race condition.

[Figure 7.2.1](CritSect.html#petersonstate) illustrates all possible states and transitions for Peterson’s solution. In this model, there are three basic types of events for each thread: `WAIT`, `ENTER`, and `EXIT`. The six states at the top of the model show the transitions when there is no contention. For example, from the left non-critical state (neither are waiting, `turn` = 1), thread 0 could indicate it attempts to enter the critical section with the `WAIT` event and immediately enters with the `ENTER` event. The `EXIT` event restores the system to the non-critical state.

![State model of Peterson's solution](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.7.1.png)

Figure 7.2.1: State model of Peterson’s solution

The four states at the bottom illustrate how the algorithm handles contention. If neither thread has entered the critical section (states `Waiting 0` and `Waiting 1`), then the turn variable determines which thread enters. That is, the condition of the while loop can only resolve to true for one of the threads, blocking the other from entering. On the other hand, one thread may already be in the critical section (states `Critical 0` and `Critical 1`) when the other attempts to enter. In that case, the new thread is forced to wait by the turn variable. Note that Peterson’s algorithm can be extended to more than two threads, as well.

📜 Example 7.2.1

* * *

The state model in [Figure 7.2.1](CritSect.html#petersonstate) can be difficult to trace. Although it would be impractical to provide a full description of all possible sequences, we can describe a basic trace that traverses through seven of the states. As before, we use indentation to denote the separate threads, with thread `T0` unindented and `T1` indented. Comments interspersed indicate the state and changes to relevant variables.

```cpp
/* Initial state: Non-critical with turn=1 */
flag[0] = true;
turn = 1; /* Change to Waiting 0, turn=1 */
    flag[1] = true;
    turn = 0; /* Change to Both waiting, turn=0 */

/* Both are waiting, so turn is the tie-breaker */
while (flag[1] && turn == 1) ;     /* FALSE, turn=0 */
    while (flag[0] && turn == 0) ; /* TRUE, T1 blocked */

/* Enter Critical 0, Waiting 1
   Thread 0 does critical work here */
flag[0] = false; /* Enter Waiting 1, turn=0 */

    while (flag[0] && turn == 0) ; /* FALSE, flag[0] now false */
    /* Enter Critical 1
       Thread 1 does critical work here */
    flag[1] = false; /* Enter Non-critical with turn=0 */
```

In this trace, both threads begin in non-critical code with neither attempting to enter a critical section until `T0` executes line 2. `T1` then declares its intention to enter on line 4. Lines 3 and 5 are the key events to control the ordering; whichever thread sets turn last loses the rase. Since `T1` changes turn after `T0` does, `T0` will be the thread to enter the critical section. To confirm this fact, check the `while`-loop conditions on lines 8 and 9. On these lines, `flag[0]` and `flag[1]` are both true. The condition then rests exclusively on turn. Later, after `T0` leaves the critical section (line 13), `T1`’s while-loop condition changes because `flag[0]` is now false; this change allows `T1` to break out of the loop and continue into the critical section.

///7.2.2. Synchronization Properties[¶]
---------------------------------------

Peterson’s solution provides three desirable properties for solutions to synchronization problems:

> *   [Safety](#term-safety) – There is no way for two threads to be in the critical section concurrently, so they cannot interfere with each other. This property is also called [mutual exclusion](#term-mutual-exclusion).
> *   [Liveness](#term-liveness) – If no thread is in the critical section and at least one tries to enter, then one of the threads will be able to enter. This property is also called [progress](#term-progress).
> *   [Fairness](#term-fairness) – Assuming neither thread can remain in the critical section indefinitely, if a thread attempts to enter the critical section, it will eventually do so. This property is also called [bounded waiting](#term-bounded-waiting).

Examining [Figure 7.2.1](CritSect.html#petersonstate) provides informal assurance that these properties hold. Safety is achieved, because no state allows both threads to be in the critical section at any time. Liveness is supported by the fact that all of the waiting states have a transition defined for an `ENTER` event; liveness could only be broken if the system could get stuck in a waiting state. Fairness is somewhat more difficult to observe, but arises from the `Crit M`, `Wait N` states in which one thread is in the critical section and the other is trying to enter. All possible sequences of events from these states must pass through a state in which the waiting thread gets to enter. Thus, no thread that tries to enter the critical section can be infinitely blocked from doing so.

The combination of all three of these properties is very difficult to achieve for synchronization problems. Generally speaking, safety is a requirement and no approach would be considered a solution without guaranteeing mutually exclusive access. Liveness is also a standard requirement; approaches that make it possible to enter a [deadlock](#term-deadlock) state that blocks all threads from entering the critical section are also not considered viable solutions.

In contrast, fairness is quite often difficult to achieve in practice and may even be unnecessary. For instance, consider a system that picks randomly between threads that are waiting to enter. In addition, assume that each thread will immediately try to enter the critical section after leaving it. It is theoretically possible that one thread might be extremely unlucky and never gets picked. If the choice truly is randomly distributed, this outcome would be as likely as flipping a coin and getting the same result every time. It is theoretically possible, but unlikely to happen in practice.

This scenario highlights a key misunderstanding of the liveness property. If there are no threads in the critical section and one tries to enter, there is no guarantee that thread will actually be the one allowed in. Rather, without the fairness property, it may be possible for another thread to come along and enter without waiting. To illustrate this point, consider the approach shown in [Code Listing 7.3](#cl7-3). This code meets the safety and liveness properties by permanently blocking out one of the threads.

```cpp
/* Code Listing 7.3:
   An approach that guarantees safety and liveness but not fairness 
 */

if (thread == 0)
  while (1) ; /* thread 0 waits infinitely */

/* enter critical section */
```

///7.2.3. Peterson’s Solution and Modern Hardware[¶]
----------------------------------------------------

The elegance of Peterson’s solution makes it an interesting approach that provides these three key properties. However, this approach is generally not used in modern systems. Peterson’s solution rests on the assumption that the instructions are executed in a particular order and memory accesses can be achieved atomically. Both of these assumptions can fail with modern hardware. Due to complexities in the design of pipelined CPUs, the instructions may be executed in a different order. Additionally, if the threads are running on different cores that do not guarantee immediate cache coherency, the threads may be using different memory values.

Although there are hardware mechanisms available to overcome these issues, Peterson’s solution also suffers from its lack of abstraction. From the systems programmer’s perspective, it is more desirable to use abstractions such as _locking_ and _unlocking_ a critical section rather than setting memory bits. As such, the remainder of this chapter will focus on higher-level abstractions and techniques that facilitate more robust and adaptable solutions, called synchronization primitives, to these types of problems. In general, these primitives fail to guarantee fairness and thus cannot provide the same theoretical properties of Peterson’s solution. However, their higher level of abstraction provides greater clarity in their practical use.

[[1]](#id1)

The standard explanation of Peterson’s solution uses 0 and 1 to refer to the two threads, respectively. Here, we adopt a higher level of abstraction, using `self` and `other` to refer to the two. For thread 0, `self` would be 0 and `other` would be 1; the reverse is true for thread 1.



//7.3. Locks[¶]
===============

One of the most fundamental synchronization primitives is to use a [lock](#term-lock) to eliminate the race conditions in critical sections. A lock is also called a _mutex_ or _mutex lock_ because it **provides mutually exclusive access to a critical section**. That is, once a thread acquires a lock and enters a critical section, no other thread can also enter the critical section until the first thread completes its work. Locks have the following key features:

> *   **Mutual exclusion**: A lock is initially considered to be unlocked. Only one thread can _acquire_ the lock at a time. Once a lock has been acquired by one thread, no other lock can acquire it until it has been _released_.
> *   **Non-preemption**: If a thread acquires a lock, no other thread can take it away. The thread that has acquired the lock must voluntarily release it.
> *   **Atomic acquire and release operations**: The implementation of these operations guarantee their executions to be free of race conditions. If two threads both try to acquire a lock at the same time, either one succeeds and one fails, or both fail (if the lock was previously acquired). Similarly, when a thread releases a lock that it has acquired, this operation cannot fail due to interrupts or other exceptional control flow.
> *   **Blocking acquire operations**: If a thread tries to acquire a lock that is currently being held by another thread, the current request to acquire the lock blocks and is added to a request queue for the lock. When a thread that holds a lock with pending requests releases it, exactly one of the waiting threads will become unblocked and acquire it.

In the case of the threads trying to increment `globalvar`, the solution is to acquire the lock prior to the increment and release the lock afterwards:

```cpp
/* acquire lock */
globalvar++;
/* release lock */
```

If an interrupt and thread switch occur after the first thread has acquired the lock, then the second thread would become blocked. The first thread would be allowed to increment `globalvar` atomically, storing back the new value of 6 to the variable in memory. Then, the first thread would release the lock, allowing the second thread to enter the critical section and increment `globalvar` again, producing the correct final result of 7.

///7.3.1. POSIX Mutexes[¶]
--------------------------

In the POSIX interface, three functions provide the basic functionality for locks, which POSIX simply calls [mutexes](#term-mutex). POSIX provides functions for initializing, destroying, locking, and unlocking mutexes. POSIX also provides `pthread_mutex_trylock()`, which avoids blocking if the lock has already been acquired. The caller can check the return value if the mutex was successfully acquired; the function returns 0 if successful, or a non-zero error value otherwise. This function is useful if the system has several equivalent copies of a resource that are interchangeable. If the code tries to acquire the lock for one copy and fails, it can move on to another without blocking.

📦 C library functions – `<pthread.h>`

* * *

`int pthread_mutex_init (pthread_mutex_t *mutex, const pthread_mutexattr_t *attr);`

Initialize a mutex based on initial attributes.

`int pthread_mutex_destroy (pthread_mutex_t *mutex);`

Destroy an allocated mutex.

`int pthread_mutex_lock (pthread_mutex_t *mutex);`

Acquire the specified mutex lock.

`int pthread_mutex_trylock (pthread_mutex_t *mutex);`

Try to acquire the mutex lock; return a non-zero value if unsuccessful.

`int pthread_mutex_unlock (pthread_mutex_t *mutex);`

Release the specified mutex lock.

Returning to the example of modifying globalvar described above, [Code Listing 7.4](#cl7-4) shows how to use the mutex within a thread.

```cpp
/* Code Listing 7.4:
   Locking a global variable with a mutex
 */

/* Initialize the global variable to 5 */
int globalvar = 5;

/* Each increment thread gets a pointer to the mutex */
void *
increment (void *args)
{
  pthread_mutex_t *mutex = (pthread_mutex_t *) args;
  
  /* Lock for the critical section, then release the mutex */
  pthread_mutex_lock (mutex);
  globalvar++;
  pthread_mutex_unlock (mutex);

  pthread_exit (NULL);
}
```

The thread receives a pointer to the mutex as its argument. The mutex is locked before entering the critical section `globalvar++`, then unlocked immediately after. Given this code, regardless of how many threads are created and running, `globalvar` will be correctly incremented each time. [Code Listing 7.5](#cl7-5) shows how to create the mutex and pass it to the threads. In this simple scenario, we only create two threads.

```cpp
/* Code Listing 7.5:
   Passing a mutex to multiple threads
 */

pthread_t threads[2];
pthread_mutex_t mutex;

/* Initialize the mutex, which will be passed to the threads */
pthread_mutex_init (&mutex, NULL);

/* Create the child threads, passing a pointer to the mutex */
assert (pthread_create (&threads[0], NULL, increment, &mutex)
        == 0);
assert (pthread_create (&threads[1], NULL, increment, &mutex)
        == 0);

/* Join the threads and confirm the result */
pthread_join (threads[0], NULL);
pthread_join (threads[1], NULL);

/* globalvar was initialized to 5 and incremented twice */
assert (globalvar == 7);
```

The behavior of these operations is undefined in the POSIX specification under certain unusual circumstances. These situations are typically errors in the code, but there is not enough information to determine precisely what happened. As such, POSIX leaves the following behaviors undefined as a default:

> *   Attempting to lock a mutex that the thread has already acquired; this is also called _relocking._
> *   Attempting to unlock a mutex owned by another thread; this violates the non-preemption characteristic of locks.
> *   Attempting to lock or unlock an uninitialized mutex; this is simply a programming error.

///7.3.2. Spinlocks[¶]
----------------------

Traditional locks also suffer from one very subtle (but occasionally significant) performance penalty: context switches. The penalty arises from the fact that operations to acquire the lock can block the current thread if the lock is not available. At this point, the kernel must select another thread to run (takes time), save the current thread’s registers (takes time), possibly flush one or more levels of the cache (takes time), possibly switch virtual memory spaces (takes time), and so on.

As a rough average estimate, context switches on modern systems take around 2000 ns. In many cases, those time penalties are expected and unavoidable. But consider a case where thread A is in possession of a lock and is almost ready to release it. If thread B is running and attempts to acquire the lock, it would have to wait at least 4000 ns for the kernel to switch from thread B to A and then back from thread A to B. However, what if B’s execution could be delayed for 10 ns, during which time A releases the lock? That is, by waiting 10 ns, B could avoid almost 4000 ns of unnecessary context. If this happens often enough, the system could experience a significant speedup.

In traditional uniprocessing systems, this argument is nonsensical. If one thread is running, then it is impossible for another thread to be releasing the lock at the same time. The reason is that uniprocessing systems could only execute one instruction (i.e., one thread) at a time. However, in [multiprocessing](#term-multiprocessing) systems, including [multicore](#term-multicore), there is true parallelism happening. That is, it is possible for threads A and B to be running—and working with the lock—at the exact same moment in time.

To exploit this advantage in parallel execution, POSIX (like other modern thread libraries) include a newer variation on locks called [spinlocks](#term-spinlock). A spinlock behaves similarly to a traditional lock with one exception: Instead of blocking if the lock is owned by another thread, the spinlock stays in a loop that keeps trying. If the lock becomes immediately available, the thread can then continue without the penalty of a context switch. The POSIX interface for spinlocks is virtually identical to the one for mutexes. The difference is primarily in the internal implementation.

📦 C library functions – `<pthread.h>`

* * *

`int pthread_spin_init (pthread_spinlock_t *lock, int pshared);`

Initialize a spinlock.

`int pthread_spin_destroy (pthread_spinlock_t *lock);`

Destroy an allocated spinlock.

`int pthread_spin_lock (pthread_spinlock_t *lock);`

Acquire the specified spinlock.

`int pthread_spin_trylock (pthread_spinlock_t *lock);`

Try to acquire the spinlock; return a non-zero value if unsuccessful.

`int pthread_spin_unlock (pthread_spinlock_t *lock);`

Release the specified spinlock.

Given that spinlocks can benefit from avoiding unnecessary context switches, it may seem that they are inherently superior to mutexes. However, this is not the case, and mutexes often perform better. Note that spinlocks only benefit when waiting takes less time than executing a context switch. In the example above, we assumed that the thread only needed to wait 10 ns for the lock to become available.

However, if the lock’s owner is performing a complex and long-running operation, the thread using the spinlock will consume the CPU time for the rest of its [quantum](#term-quantum), which is typically on the order of 100 ms (i.e., 100,000,000 ns). That is, if the lock does not become immediately available, the thread will **waste millions** of clock cycles in an attempt to **save thousands**. Clearly, this would be a horrible trade-off!

The choice of spinlocks vs. traditional mutexes ultimately comes down to the typical duration of the critical sections. If all of the critical sections are short in duration, then spinlocks are likely to provide superior performance, as there is an increased likelihood that the lock’s holder will be releasing it soon. If long critical sections are common, then mutexes are preferable to avoid the wasted CPU time that comes from spinlocks’ busy waiting.

///7.3.3. Defining Critical Sections[¶]
---------------------------------------

Selecting the appropriate size for a critical section is very complicated. Consider the two loops shown in [Code Listing 7.6](#cl7-6).

```cpp
/* Code Listing 7.6:
   Two possible critical sections
 */

/* Acquire/release the lock 1,000,000 times inside the loop */
for (i = 0; i < 1000000; i++)
  {
    pthread_mutex_lock (&mutex);
    globalvar++;
    pthread_mutex_unlock (&mutex);
  }

/* Acquire/release the lock only once outside the loop */
pthread_mutex_lock (&mutex);
for (i = 0; i < 1000000; i++)
  globalvar++;
pthread_mutex_unlock (&mutex);
```

If no other thread tries to access `globalvar` while this code is running, the end result of each loop is identical except for one critical factor: speed. Both of the loops would guarantee that `globalvar` gets incremented 1,000,000 times. The first version of the loop accomplishes this feat by acquiring and releasing the lock 1,000,000 times, whereas the second only acquires and releases the lock once. The first version would suffer from a tremendous overhead penalty. (A simple timing experiment showed the first loop consistently took 10 times as much time as the second.)

Despite the performance penalty of the first loop structure, there are times when it is necessary. Specifically, consider what is accomplished with the two loops. The first structure executes 1,000,000 tiny critical sections that each increment the global variable by 1. The second executes one gigantic critical section that increments the global variable by 1,000,000. While the second approach is much faster _for this thread_, it could lead to performance penalties for other threads. If you make the critical section too large, then you are increasing the amount of time that other threads have to wait.

Ultimately, there is no universal answer or approach to which style should be used, because the choice depends on the needs of the system being built. The best (and perhaps not useful) advice is to make the critical section as large as it needs to be, but no larger.



//7.4. Semaphores[¶]
====================

[Semaphores](#term-semaphore) are a flexible synchronization primitive that can be used for many purposes. They can be used as a form of message-passing IPC to allow processes to synchronize access to shared memory or memory-mapped files. But in contrast to other forms of IPC, semaphores can be used for thread synchronization, as well.

As described previously, a semaphore is a non-negative integer with atomic operations for incrementing and decrementing its value. The POSIX function for decrementing the semaphore is `sem_wait()`, while the `sem_post()` function increments the value. If the semaphore’s value is 0 prior to decrementing, then the `sem_wait()` operation will block the current thread. If a thread calls `sem_post()` and there is at least one thread waiting on the semaphore, then one of the threads becomes unblocked. The choice of which thread gets unblocked is implementation dependent.

📦 C library functions – `<semaphore.h>`

* * *

`int sem_wait (sem_t *sem);`

Decrement the semaphore’s value; block if the value is currently 0.

`int sem_post (sem_t *sem);`

Increment the semaphore’s value; resume another process if the value is 0.

In this section, we’ll describe three basic techniques of using semaphores: signaling, mutual exclusion, and multiplexing. In essence, all of these techniques look identical, but with one exception: the initial value. For signaling, the semaphore is initialized to 0; for mutual exclusion, the initial value is 1; for multiplexing, the initial value is a positive number greater than 1. To summarize, the general practice is that **the initial value of the semaphore is the desired number of initial allowed concurrent accesses**.

///7.4.1. Semaphores as Signaling[¶]
------------------------------------

Semaphores can be used to send general signals to other threads that some application-specific event has occurred. Note that this use of semaphores is different from the pre-defined [signals](#term-signal) such as `SIGKILL`. Events with semaphores have no pre-defined meaning and are simply indications that _something_ has occurred.

To illustrate signaling, consider the two threads shown in [Code Listing 7.7](#cl7-7). One thread (`keyboard_listener()`) reads a line of input from `STDIN` into a shared buffer. Once the input has been received, the thread posts to the semaphore to alert the second thread that the event (receiving input) has occurred. The second thread (`keyboard_echo()`) starts by waiting on the semaphore before trying to read the input from the buffer. The semaphore ensures that the echo thread cannot try to read from the buffer until the listener has finished writing to the buffer.

```cpp
/* Code Listing 7.7:
   Two threads cooperating to echo input text
 */

/* Struct instance will contain pointers to semaphore and buffer */
struct args {
  sem_t *semaphore;
  char *buffer;
};

#define MAX_LENGTH 40

/* Listener thread gets input, then ups the semaphore */
void *
keyboard_listener (void * _args)
{
  /* Cast the args to a usable struct type */
  struct args *args = (struct args *) _args;
  printf ("Enter your name here: ");
  assert (fgets (args->buffer, MAX_LENGTH, stdin) != NULL);

  /* After receiving input, up the semaphore and exit */
  sem_post (args->semaphore);
  pthread_exit (NULL);
}

void *
keyboard_echo (void * _args)
{
  /* Cast the args to a usable struct type */
  struct args *args = (struct args *) _args;

  /* Wait on the signal from the semaphore */
  sem_wait (args->semaphore);

  /* Trim off at the newline character */
  char *newline = strchr (args->buffer, '\n');
  if (newline != NULL) *newline = '\0';

  /* Echo back the name and exit */
  printf ("Hello, %s\n", args->buffer);
  pthread_exit (NULL);
}
```

The key with signaling is that the semaphore must be created and initialized with an initial value of 0 as shown in [Code Listing 7.8](#cl7-8). By using this initial value, the semaphore eliminates the issue of nondeterministic thread scheduling. Specifically, if the `keyboard_echo()` thread runs first, it must wait until the `keyboard_listener()` reads the input and ups the semaphore. However, if `keyboard_listener()` runs first and ups the semaphore before `keyboard_echo()` gets scheduled, then the semaphore will have a value of 1 and the thread will not be blocked.

```cpp
/* Code Listing 7.8:
   Initializing a semaphore and passing it to threads
 */

/* Allocate the buffer and open the named semaphore */
char buffer[MAX_LENGTH+1];
memset (buffer, 0, sizeof (buffer));

/* For signaling, initialize the semaphore to 0 */
sem_t *sem = sem_open ("/OpenCSF_Sema", O_CREAT | O_EXCL,
                       S_IRUSR | S_IWUSR, 0);
assert (sem != SEM_FAILED);

/* Set up struct instance with both pointers; pass it to threads */
struct args args = { sem, buffer };
assert (pthread_create (&threads[0], NULL, keyboard_listener,
                        &args) == 0);
assert (pthread_create (&threads[1], NULL, keyboard_echo, &args)
        == 0);

/* Wait for both threads to finish, then unlink the semaphore */
pthread_join (threads[0], NULL);
pthread_join (threads[1], NULL);
sem_unlink ("/OpenCSF_Sema");
```

🐞🐛🐌 Bug Warning

* * *

In this basic signaling pattern, it is possible to have the two threads repeatedly post to or wait on a single semaphore. This approach is a common technique for handling repeated events. However, it is fraught with peril. In this example, there is only one buffer. So if the `keyboard_listener()` repeatedly reads input (posting each time), the buffer would be repeatedly overwritten, and there is no guarantee that the `keyboard_echo()` thread would read any of the input.

Another complication arises if there are multiple threads repeatedly waiting on the same semaphore. In the default implementation of POSIX semaphores, the order in which threads are unblocked with `sem_post()` is unspecified. As such, it is possible that one thread might be repeatedly unblocked while the other waiting thread is never unblocked.

///7.4.2. Mutual Exclusion with Semaphores[¶]
---------------------------------------------

Semaphores can also be used to ensure mutually exclusive access to a shared variable or resource. Consider the two threads, shown in [Code Listing 7.9](#cl7-9), that atomically increment or decrement an int counter variable. Each change to the shared variable is preceded by a call to `sem_wait()` and followed by a call to `sem_post()`. This wrapping is analogous to the mutual exclusion style used with locks.

```cpp
/* Code Listing 7.9:
   Two threads that synchronize modifications to a shared variable
 */

/* Struct containing the semaphore and integer */
struct args {
  sem_t *semaphore;
  int value;
};

/* Adder thread that repeatedly adds 10 */
void *
add (void * _args)
{
  /* Cast the args to a usable struct type */
  struct args *args = (struct args *) _args;
  int i;

  /* Atomically add 10 to value 100000 times */
  for (i = 0; i < 100000; i++)
    {
      sem_wait (args->semaphore);
      args->value += 10;
      sem_post (args->semaphore);
    }
  pthread_exit (NULL);
}

/* Subtracter thread that repeatedly subtracts 10 */
void *
subtract (void * _args)
{
  /* Cast the args to a usable struct type */
  struct args *args = (struct args *) _args;
  int i;

  /* Atomically subtract 10 from value 100000 times */
  for (i = 0; i < 100000; i++)
    {
      sem_wait (args->semaphore);
      args->value -= 10;
      sem_post (args->semaphore);
    }
  pthread_exit (NULL);
}
```

As shown in [Code Listing 7.10](#cl7-10), the difference between signaling and mutual exclusion is the initial value of the semaphore. For mutual exclusion, the semaphore is initialized to 1.

```cpp
/* Code Listing 7.10:
   Initializing a semaphore for mutual exclusion instead of signaling
 */

/* For mutual exclusion, initialize the semaphore to 1 */
sem_t *sem = sem_open ("/OpenCSF_Sema", O_CREAT | O_EXCL,
                       S_IRUSR | S_IWUSR, 1);
assert (sem != SEM_FAILED);

/* Set up a struct instance with semaphore and initial value 0 */
struct args args = { sem, 0 };
assert (pthread_create (&threads[0], NULL, add, &args) == 0);
assert (pthread_create (&threads[1], NULL, subtract, &args) == 0);
```

As illustrated in [Code Listing 7.9](#cl7-9), semaphores can be used identically to mutex locks as shown previously. In fact, [Code Listing 7.11](#cl7-11) shows that we can use semaphores as a foundation to create locks. Acquiring the lock involves waiting on the semaphore and setting oneself as the lock owner. Releasing the lock clears the ownership field and posts to the semaphore. If multiple threads try to acquire the lock, they all end up trying to down the semaphore, and only one is successful. Once that thread releases the lock, the semaphore is upped and a single thread is unblocked; that thread then sets itself as the owner.

```cpp
/* Code Listing 7.11:
   Creating locks from semaphores
 */

/* We can create a lock with a semaphore and owner field */
typedef struct lock {
  sem_t *semaphore;
  pthread_t owner;
} lock_t;

/* To acquire the lock, wait on semaphore and set self as owner */
int
mutex_lock (lock_t *lock)
{
  int retvalue = sem_wait (lock->semaphore);
  lock->owner = pthread_self ();
  return retvalue;
}

/* To release, clear the owner field and post to the semaphore */
int
mutex_unlock (lock_t *lock)
{
  /* Only the owner can release the lock */
  if (lock->owner != pthread_self ())
    return -1;
  lock->owner = 0;
  return sem_post (lock->semaphore);
}
```

The key difference between locks and semaphores is that a semaphore must be explicitly initialized to the integer value 1, whereas the initialization for a lock is opaque. Or, put a different way, **mutex locks are preferred because they adhere to the principles of encapsulation and information hiding**. The notion of _acquiring a lock_ is more concrete than _waiting on a semaphore,_ and is a more appropriate abstraction for mutual exclusion.

///7.4.3. Multiplexing with Semaphores[¶]
-----------------------------------------

Both locks and semaphores can be used for mutual exclusion, though locks are typically the preferred abstraction. However, the mutual exclusion style of semaphores can be extended to allow for multiple but limited shared accesses. This concept is known as [multiplexing](#term-multiplexing-semaphore). That is, while a lock can only be used to grant access to a single thread at a time, a semaphore can be set to allow 5, 10, or any other number of concurrent accesses. Once this limited number has been reached, any additional threads would become blocked, just as they would if trying to acquire a locked mutex.

As a real-world example of multiplexing, consider a popular restaurant or club. Building safety codes specify that there is a maximum occupancy, say 100 people. As patrons start arriving when the place opens, they are allowed in one at a time. However, once the 100th person has been allowed in, the management must make sure that no one else enters (or risk legal trouble and fines). At that point, a line forms out front. Once a single person leaves, the next patron is allowed in. In this case, **the employee enforcing this restriction at the entrance is acting like a multiplexing semaphore with an initial value of 100**.

Within the context of computing, multiplexing has many uses. One example is to limit the number of concurrent accesses to a pool of resources where more than one is required for each operation. For example, consider a networking hub that has 10 incoming ports and 10 outgoing ports that can be locked independently. Consider the scenario where 9 threads have already locked both an incoming and an outgoing port. Two new threads arrive; one of them acquires an incoming port and the second acquires an outgoing port. At this point, no ports of either kind are available. If these last two threads then try to acquire the missing port, they cannot. As a result, there are two ports (one incoming and one outgoing) that are both locked but not in use. This waste of resources could be prevented with the structure in [Code Listing 7.12](#cl7-12).

```cpp
/* Code Listing 7.12:
   Using a semaphore to multiplex access to a thread pool
 */

/* Acquire access to the pool of resources */
sem_wait (pool_semaphore);

/* Try to acquire incoming port. If taken, move on to the next. */
for (i = 0; i < 10; i++)
  if (pthread_mutex_trylock (incoming_mutex[i]))
    {
      in = i;
      break;
    }

/* Work with incoming port, even if no outgoing port is needed */

/* Once an outgoing port is needed, acquire it in the same way. */
for (i = 0; i < 10; i++)
  if (pthread_mutex_trylock (outgoing_mutex[i]))
    {
      out = i;
      break;
    }

/* To finish, release both locks and up the semaphore */
pthread_mutex_unlock (incoming_mutex[in]);
pthread_mutex_unlock (outgoing_mutex[out]);
sem_post (pool_semaphore);
```

Assuming the `pool_semaphore` was initialized to 10, every thread granted access to the pool of resources (ports in this example) will have access to one of each type. Moreover, this structure makes it possible for the incoming and outgoing ports to be acquired in any order; this flexibility is commonly associated with the problem of [deadlock](#term-deadlock), but multiplexing in this manner avoids that issue.



//7.5. Barriers[¶]
==================

Semaphores provide a general mechanism that allows one thread to indicate that a particular event has happened. One type of event that arises quite commonly in concurrent applications is the need to make sure all threads reach some common point before any of them continues on. That is, multiple threads may perform some initial computation then pause; at that point some manager thread checks these initial results and allows the threads to continue once their results have been approved.

As an example, consider the case of an autonomous car or flying drone. These types of systems require accurate measurements of real-world phenomena like speed, position, and acceleration. However, these measurements involve floating-point calculations that may encounter rounding errors. To improve the accuracy of the vehicle’s autonomous controls, multiple threads may be simultaneously and independently performing these calculations. The manager control thread will occasionally require these threads to reach a common point in time where each thread reports on its best guess of the measurements. The manager may then correct errors in some threads, then allow them to continue.

Achieving this kind of synchronization with semaphores is complex and error-prone. One approach would be to use two semaphores: the manager thread would repeatedly wait on one of them as the threads were finishing their calculations; the manager would then signal on a different semaphore that each thread was waiting on. However, if all threads agree but one crashes, then the entire system would fail; the manager thread would continue to wait on that last thread to finish (which will never happen).

As an alternative, [synchronization barriers](#term-barrier) allow programs to specify a _minimum_ number of threads that must reach a common point before any can continue. For instance, if the autonomous vehicle manager thread receives reports from 7 out of 10 threads about the current position, then the system may be designed to consider this measurement _good enough_ and let the threads continue rather than requiring the last three threads to finish.

The base functionality of barriers is specified with thread functions. The barrier is initialized with `pthread_barrier_init()`, with the last parameter indicating the _minimum_ number of threads that must reach the barrier before any can be allowed to continue. Threads use the `pthread_barrier_wait()` function to indicate that they have reached the common point. If the minimum number of threads have not yet reached the barrier, then the current thread will become blocked. Once the minimum number has been reached, all threads waiting at the barrier become unblocked; any future thread that calls `pthread_barrier_wait()` will pass right through the barrier without any delay. Finally, `pthread_barrier_destroy()` is used to clean up any resources associated with the barrier.

📦 C library functions – `<pthread.h>`

* * *

`int pthread_barrier_init (pthread_barrier_t *barrier, const pthread_barrierattr_t *attr, unsigned count);`

Initialize a synchronization barrier with the specified attributes.

`int pthread_barrier_destroy (pthread_barrier_t *barrier);`

Delete a synchronization barrier.

`int pthread_barrier_wait (pthread_barrier_t *barrier);`

Make a thread wait until enough have reached the barrier.

///7.5.1. Concurrent Calculations with Barriers[¶]
--------------------------------------------------

To show how barriers work, consider using two threads to determine which function grows faster: the Fibonnacci sequence or the exponential function. That is, is the 100th Fibonacci number greater than $c^{100}$ for some constant $c$? [[1]](#f43) It can be shown that each Fibonacci number is slightly less than double the previous value, so the answer would be no for any constant integer greater than 1. But what if the constant is 1.6? [[2]](#f44) Then the answer is not obvious, but it is easy enough to calculate.

[Code Listing 7.13](#cl7-13) shows how barriers can address this problem. The `fibonacci()` and `exponential()` threads take a struct that contains a shared `pthread_barrier_t`, a digit that specifies how far into the sequence and what exponent to calculate, and a base value that specifies the base of the exponent. That is, if digit is specified as 30 and base is 1.6, `fibonacci()` will calculate the 30th Fibonacci number, while `exponent()` will calculate (approximately) $1.6^{30}$.

```cpp
/* Code Listing 7.13:
   Using a barrier to pause multiple threads
 */

/* Use a common struct with the barrier and parameters */
struct args {
  pthread_barrier_t *barrier;
  unsigned int digit;
  double base;
  unsigned long fib;
  unsigned long exp;
};

/* Iterative version of the Fibonacci calculation */
void *
fibonacci (void *_args)
{
  struct args *args = (struct args *) _args;
  unsigned long previous = 1;
  unsigned long current = 1;
  unsigned int i;

  for (i = 2; i < args->digit; i++)
    {
      current += previous;
      previous = current - previous;
    }
  /* current is the i-th Fibonacci number */
  args->fib = current;

  /* Wait at the barrier and compare results */
  pthread_barrier_wait (args->barrier);
  if (args->fib > args->exp)
    printf ("Fibonacci wins: %lu > %lu\n", args->fib, args->exp);

  pthread_exit (NULL);
}

/* Calculate base^digit using repeated multiplication */
void *
exponential (void *_args)
{
  struct args *args = (struct args *) _args;
  double result = 1;
  unsigned int i;

  /* WARNING: Don't actually do this in real code that needs
     Accurate calculations. Repeated floating point arithmetic
     inherently has rounding errors that will compound. */

  /* Calculate base^digit; cast to unsigned long for comparison */
  for (i = 1; i < args->digit; i++)
    result *= args->base;
  args->exp = (unsigned long) result;

  /* Wait on threads to reach the barrier and check the result */
  pthread_barrier_wait (args->barrier);
  if (args->exp > args->fib)
    printf ("Exponential wins: %lu > %lu\n", args->exp, args->fib);

  pthread_exit (NULL);
}
```

🐞🐛🐌 Bug Warning

* * *

As indicated in the code, you should not try to calculate the exponential value as shown in this code. For one thing, this code is (intentionally) inefficient and could be easily optimized. More importantly, though, the result is guaranteed to be inaccurate. Floating-point arithmetic is susceptible to rounding errors, particularly with repeated calculations and large values. Consequently, this example should primarily be considered for its use of barriers rather than its accuracy regarding the comparison of Fibonacci and exponential growth.

The key lines to note regarding these threads are the calls to `pthread_barrier_wait()` and the references to the fields `fib` and `exp`, which store the results of each calculation. Specifically, note that there is no race condition regarding these fields because the threads only _read_ the values after the barrier. That is, nothing changes after the barrier, so there is no concern that the values checked are inaccurate. This guarantee arises from the fact that both threads must reach the barrier (i.e., the `pthread_barrier_wait()` call) before either can proceed.

One question that cannot be answered with this code is which thread runs faster. With barriers, all we can say for sure is that both threads reach the barrier before either proceeds. We have no information about which one arrived first. From the perspective of the barrier, such a question is irrelevant. The only thing that matters is that both finish before they each check the results.

[Code Listing 7.14](#cl7-14) shows the initialization required. The main thread initializes the barrier so that 2 threads are required, then creates the threads. Notice that both threads receive pointers to the same `struct` instance, so they share the same `barrier`, `fib`, and `exp` fields. The main thread joins both of the helper threads, but the join must occur after the threads have called `pthread_exit()`. Since both threads call `pthread_barrier_wait()` before that, the main thread is guaranteed to clean up the barrier only _after_ it is no longer needed.

```cpp
/* Code Listing 7.14:
   Initializing the barrier and data structures for Code Listing 7.13 
 */

/* Initialize the barrier so that both threads must wait */
pthread_barrier_t barrier;
pthread_barrier_init (&barrier, NULL, 2);

/* Set up parameters to get the 30th Fibonacci and 1.6^30 */
struct args args;
args.barrier = &barrier;
args.digit = 30;
args.base = 1.6;
args.fib = 0;
args.exp = 0;

/* Create and join the threads, then destroy the barrier */
assert (pthread_create (&threads[0], NULL, fibonacci, &args) == 0);
assert (pthread_create (&threads[1], NULL, exponential, &args) == 0);

pthread_join (threads[0], NULL);
pthread_join (threads[1], NULL);

pthread_barrier_destroy (&barrier);
```

[[1]](#id2)

To be clear, the `exponential()` function is truly the exponential $c^n$ rather than the polynomial $n^c$. In both `exponential()` and `fibonacci(),` we are using $n = 100$.

[[2]](#id3)

The selection of 1.6 as the value was not an arbitrary choice. There is a closed-form solution known as Binet’s formula, and calculations based on this formula can be done to show that each Fibonacci number is approximately 1.6 times the value of the previous number.



//7.6. Condition Variables[¶]
=============================

One of the primary uses of semaphores is to perform application-specific signaling. One thread waits on a semaphore until another thread indicates that some important event has occurred. While semaphores are flexible, they have a number of short-comings for many programs.

> *   Semaphore operations do not adhere to strong principles of encapsulation and abstraction. That is, the practice of incrementing and decrementing an integer value does not have an obvious mapping to synchronization problems. For instance, this contrasts with the more intuitive _lock_ and _unlock_ operations on mutex locks.
> *   There are several different implementations of semaphores that vary in the features that they provide. Furthermore, different systems provide varying levels of support and compliance in their semaphore implementations.
> *   In most implementations, semaphores can only send a signal to one other thread (or process) at a time. Semaphores provide no mechanism for broadcasting messages to multiple other threads.
> *   When receiving a signal, threads have to perform an extra step to secure mutually exclusive access to shared data. The delay in the timing between the signal and acquiring the mutex can introduce race conditions.

[Condition variables](#term-condition-variable) overcome many of these short-comings of semaphores. Similar to the POSIX semaphore interface, condition variables provide _wait_ and _signal_ functions. These provide a more natural mapping to the problems of synchronization, as one or more threads are waiting on a signal from another thread that a condition has occurred.

📦 C library functions – `<pthread.h>`

* * *

`int pthread_cond_init (pthread_cond_t *cond, const pthread_condattr_t *attr);`

Initialize a condition variable.

`int pthread_cond_wait (pthread_cond_t *cond, pthread_mutex_t *mutex);`

Release a mutex, wait on a condition, then re-acquire the mutex.

`int pthread_cond_signal (pthread_cond_t *cond);`

Send a signal to a waiting thread.

`int pthread_cond_broadcast (pthread_cond_t *cond);`

Send a signal to all waiting threads.

`int pthread_cond_destroy (pthread_cond_t *cond);`

Delete a condition variable and clean up its associated resources.

///7.6.1. Condition Variables vs. Semaphores[¶]
-----------------------------------------------

Condition variables and semaphores appear very similar, as they both provide a mechanism that allow threads to signal that a custom event has occurred. But the differences between condition variables and semaphore signaling go beyond just a shift in terminology.

> *   The `pthread_cond_wait()` function performs multiple functions. It first releases the mutex and blocks until the corresponding signal is received; it then re-acquires the mutex that had been locked. Specifically, both of these actions are considered to occur atomically; unless an error occurs, the thread is guaranteed to have acquired the mutex by the time the function returns.
> *   Condition variables support broadcasting. The `pthread_cond_broadcast()` function will notify all threads that are waiting on the condition. Moreover, each thread will resume one at a time with the mutex acquired. With the additional mutual exclusion guarantees, condition variables can be combined in a thread-safe manner with other pieces of data to make the condition more meaningful.
> *   Condition variables are a standard part of the POSIX thread library, and they are more widely supported. For instance, some systems include the unnamed POSIX semaphore interface, but the implementation is empty, as named semaphores are preferred. There is no similar distinction in condition variables, and there is wider support for them.

///7.6.2. How to Use a Condition Variable[¶]
--------------------------------------------

There are several conventional practices for condition variables that may not be immediately obvious.

> *   A thread must acquire the mutex before calling `pthread_cond_wait()`, which will release the mutex. Calling `pthread_cond_wait()` without having locked the mutex leads to undefined behavior.
> *   Calls to `pthread_cond_wait()` should be made inside a while loop. The POSIX specification allows threads to wake up even if the signal was not sent (called a _spurious wake up_). By checking the return value of `pthread_cond_wait()`, the thread can determine if the wake up was spurious and go back to waiting if necessary.
> *   Just calling `pthread_cond_signal()` or `pthread_cond_broadcast()` is insufficient to wake up waiting threads, as the threads are locked by the mutex rather than the condition variable. The functions must be followed by a call to `pthread_mutex_unlock()`, which will allow `pthread_cond_wait()` to acquire the mutex and return.

///7.6.3. A Condition Variable Example[¶]
-----------------------------------------

The following sample program uses one thread read lines of keyboard input from `STDIN`, then passing on the information to two other threads. If the input is a string, the second thread gets the length of the string and adds it to a counter. If the input can be converted to an long using `strtol()`, then the integer value is added to the counter by the third thread.

The threads all rely on the following shared `struct`. The `input_cond` condition variable is used to indicate that a line of input has been received. The `input_processed_cond` variable is used to indicate that the two helper threads have processed the input and the keyboard listener can get more input. The other fields are used to pass information between the threads.

```cpp
#define MAX_LENGTH 100

/* Common struct with pointers to all variables needed */
struct args {
  int counter;
  pthread_cond_t *input_cond;
  pthread_cond_t *input_processed_cond;
  pthread_mutex_t *mutex;
  char *buffer;
  long current_value;
  bool shutdown;
  bool input_is_string;
};
```

The keyboard listener starts by acquiring the mutex, guaranteeing that this thread has mutually exclusive access to the shared data. After reading a line of input with `fgets()`, this thread tries to convert the input to an long with `strtol()`. If so, it sets the current_value field to this integer value. If not, the string is checked against the string `"shutdown"`, which is used to make the program stop. In all three cases, `pthread_cond_broadcast()` signals to the other threads that input has been received. The listener then waits on the `input_processed_cond` condition, which indicates that the input has been processed completely by another thread. Finally, if the shutdown message was received, this thread sets a boolean value that the others detect during another broadcast. [Code Listing 7.15](#cl7-15) shows the keyboard listener thread.

```cpp
/* Code Listing 7.15:
   A thread that uses condition variable broadcasting to share data
 */

/* Keyboard listener thread; gets input and signals to helper 
   threads */
void *
keyboard_listener (void *_args)
{
  struct args *args = (struct args *) _args;

  /* With condvars, always acquire the lock before waiting */
  pthread_mutex_lock (args->mutex);
  while (1)
    {
      /* Get input from the user */
      printf ("Enter a string/number (or shutdown to quit): ");
      memset (args->buffer, 0, MAX_LENGTH + 1);
      if (fgets (args->buffer, MAX_LENGTH, stdin) == NULL)
        break;

      /* Check for a numeric value or "shutdown" */
      long guess = strtol (args->buffer, NULL, 10);
      if (guess != 0)
        {
          args->current_value = guess;
          args->input_is_string = false;
        }
      else if (strncmp (args->buffer, "shutdown", 8) != 0)
        args->input_is_string = true;
      else
        break;

      /* Send signal to all waiting threads */
      pthread_cond_broadcast (args->input_cond);

      /* Wait for confirmation the input is processed */
      pthread_cond_wait (args->input_processed_cond, args->mutex);
      flush_buffer (args->buffer);
    }

  /* Send the shutdown input condition */
  args->shutdown = true;
  pthread_cond_broadcast (args->input_cond);
  pthread_mutex_unlock (args->mutex);

  pthread_exit (NULL);
}
```

[Code Listing 7.16](#cl7-16) shows the additional threads that share this data. The `count_chars()` and `add_number()` threads behave in approximately the same manner. They both start by acquiring the mutex, then waiting on the `input_cond`. In both cases, the call to `pthread_cond_wait()` releases the lock at this point. Once the signal has been received and the mutex is re-acquired, the threads check if they need to shutdown. If not, they each check if the input was a string. Only the `count_chars()` thread processes string input, whereas only the `add_number()` thread processes numeric input. If the thread is not supposed to process input, it uses continue to go back to the beginning of the loop and wait on the condition again. If the thread did process its appropriate input, it signals on the `input_processed_cond` variable, which allows `keyboard_listener()` to move on to reading the next line of input.

```cpp
/* Code Listing 7.16:
   Threads that process data as it is received via a condition variable
 */

/* Thread for counting characters in a string */
void *
count_chars (void *_args)
{
  struct args *args = (struct args *) _args; 
  /* With condvars, always acquire the lock before waiting */
  pthread_mutex_lock (args->mutex);
  while (1)
    {
      /* Wait on the input condition */
      pthread_cond_wait (args->input_cond, args->mutex);
      /* Signal received, mutex has been re-acquired */

      /* If the input was shutdown signal, quit */
      if (args->shutdown) break;

      /* If the input was a number, ignore this signal */
      if (!args->input_is_string) continue;

      /* Input was string not shutdown; get length without \n */
      char *newline = strchr (args->buffer, '\n');
      if (newline != NULL) *newline = '\0';
      args->counter += strlen (args->buffer);

      /* Restore the newline for buffer flushing */
      if (newline != NULL) *newline = '\n';

      /* Signal back to keyboard listener that input processing 
         is done */
      pthread_cond_signal (args->input_processed_cond); 
      /* Do NOT unlock mutex; pthread_cond_wait() will do that */
    }

  /* Shutting down. Send acknowledgement signal back */
  pthread_cond_signal (args->input_processed_cond);
  pthread_mutex_unlock (args->mutex);
  pthread_exit (NULL);
}

void *
add_number (void *_args)
{
  struct args *args = (struct args *) _args; 
  /* With condvars, always acquire the lock before waiting */
  pthread_mutex_lock (args->mutex);
  while (1)
    {
      /* Wait on the input condition */
      pthread_cond_wait (args->input_cond, args->mutex);
      /* Signal received, mutex has been re-acquired */

      /* If the input was shutdown signal, quit */
      if (args->shutdown) break;

      /* If the input was a string, ignore this signal */
      if (args->input_is_string) continue;

      /* Add current value to the counter and send signal back */
      args->counter += args->current_value;
      pthread_cond_signal (args->input_processed_cond); 
      /* Do NOT unlock mutex; pthread_cond_wait() will do that */
    }

  /* Shutting down. Send acknowledgement signal back */
  pthread_cond_signal (args->input_processed_cond);
  pthread_mutex_unlock (args->mutex);
  pthread_exit (NULL);
}
```

🐞🐛🐌 Bug Warning

* * *

For completeness, both the `count_chars()` and `add_number()` threads should check the return value of `pthread_cond_wait()` to determine if the signal was spurious. However, this code omits this check for brevity and algorithmic clarity.

Finally, [Code Listing 7.17](#cl7-17) illustrates how to initialize the condition variables, create the threads, and clean up all of the resources for the condition variables and the mutex.

```cpp
/* Code Listing 7.17:
   Initializing a condition variable and other data structures
 */

pthread_t threads[3];
pthread_cond_t input_cond;
pthread_cond_t input_processed_cond;
pthread_mutex_t mutex;
char buffer[MAX_LENGTH + 1];

/* Initialize the mutex and the condition variables */
pthread_mutex_init (&mutex, NULL);
pthread_cond_init (&input_cond, NULL);
pthread_cond_init (&input_processed_cond, NULL);

/* Set up the args for all threads */
struct args args;
args.counter = 0;
args.input_cond = &input_cond;
args.input_processed_cond = &input_processed_cond;
args.mutex = &mutex;
args.buffer = buffer;
args.current_value = 0;
args.shutdown = false;
args.input_is_string = false;

/* Create and join the threads */
assert (pthread_create (&threads[0], NULL, count_chars, &args) == 0);
assert (pthread_create (&threads[1], NULL, add_number, &args) == 0);
assert (pthread_create (&threads[2], NULL, keyboard_listener, &args) == 0);
pthread_join (threads[0], NULL);
pthread_join (threads[1], NULL);
pthread_join (threads[2], NULL);

/* Print out total, destroy the mutex and condition variables */
printf ("Total: %d\n", args.counter);
pthread_mutex_destroy (&mutex);
pthread_cond_destroy (&input_cond);
pthread_cond_destroy (&input_processed_cond);
```

///7.6.4. Monitors and Synchronized Methods[¶]
----------------------------------------------

![Architecture of a monitor with synchronized data access](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.7.2.png)

Figure 7.6.3: Architecture of a monitor with synchronized data access

The preceding code is an example of an object-oriented construct known as a monitor. [Figure 7.6.3](Condvars.html#synchmonitor) illustrates the general architecture of a [monitor](#term-monitor). Specifically, a monitor is a class or data structure that combines condition variables, mutexes, and other application-specific data. All of the internal data is considered private to the monitor, and other pieces of code can only interact with the monitor by invoking a method on the object.

The key characteristic of a monitor is that all methods are mutually exclusive in execution. That is, as with the thread functions above, methods in a monitor begin by locking the monitor’s mutex. Doing so guarantees that only one thread is _in the monitor_ at any given moment. The method ends by releasing the mutex, allowing other threads to enter the monitor.

Within the monitor, there are associated condition variables to synchronize access between the monitor and the general execution environment. While the methods are executing, they can use these condition variables to detect key events or to check for safe conditions. If the condition variable check (i.e., `pthread_cond_wait()`) fails, then the thread must release the mutex and exit the monitor. The thread then waits in the condition variable’s associated waiting queue until it can resume execution.

Readers familiar with the Java synchronized keyword have been essentially using monitors by a different name. That is, the primary function of the synchronized keyword is to have injected code that acquires a hidden mutex at the beginning of the method execution, then releases it when returning.



//7.7. Deadlock[¶]
==================

The [synchronization primitives](#term-synchronization-primitive) described in this chapter provide flexible mechanisms to solve many problems related to the timing of concurrent threads. These mechanisms can enforce mutually exclusive access to [critical sections](#term-critical-section), they can send signals of custom events, they can require a minimum number of threads reach a common point, and so on. However, there are common pitfalls and errors that can arise from their use. One problem (described in the section on locks) is making the critical section the wrong size, introducing slow performance. But that is a minor problem in comparison to [deadlock](#term-deadlock).

Deadlock is the **permanent and unresolvable blocking** of two or more threads that results from each waiting on the other. It is important to note that deadlock is different from [starvation](#term-starvation), where a thread may have to wait for a long time. For instance, consider a thread that repeatedly tries to lock a mutex; when it fails, it goes to sleep for a while and tries again. Now, assume this thread is very unlucky and it fails every time. This thread is _not_ experiencing deadlock; it is experiencing starvation. The difference is that, with starvation, it is _possible_ that the thread might get lucky in the future. With deadlock, there is no possibility.

[Code Listing 7.18](#cl7-18) provides a simple illustration of how deadlock can happen. There are two threads, running the `first()` and `second()` functions. The code for `first()` tries to acquire semaphore `sem_a` before `sem_b`, while `second()` acquires them in the opposite order.

```cpp
/* Code Listing 7.18:
   Code that is likely to lead to deadlock
 */

/* struct contains shared semaphores */
struct args {
  sem_t sem_a;
  sem_t sem_b;
};

void *
first (void * _args)
{
  struct args *args = (struct args *) _args;
  /* Acquire "sem_a" before "sem_b" */
  sem_wait (&args->sem_a);
  sem_wait (&args->sem_b);
  /* other code here */
}

void *
second (void * _args)
{
  struct args *args = (struct args *) _args;
  /* Acquire "sem_b" before "sem_a" */
  sem_wait (&args->sem_b);
  sem_wait (&args->sem_a);
  /* other code here */
}
```

The problem arises if there is a thread switch after one of the threads successfully acquires the first semaphore it needs. That is, assume that `first()` runs and acquires `sem_a`, then a thread switch occurs. At that point, `second()` starts running and acquires `sem_b`. But `second()` gets blocked when it tries to acquire `sem_a`, which is held by `first()`. At that point, the system switches back to `first()`, which gets blocked trying to acquire `sem_b`. At this point, both threads are waiting on each other permanently.

It is important to emphasize that code like that shown in [Code Listing 7.18](#cl7-18) does not guarantee that deadlock occurs. This fact becomes clear when you consider [Figure 7.7.1](Deadlock.html#deadlockstate), which shows the state model for this code. Once `first()` acquires `sem_a`, the emergence of deadlock depends on `second()` waiting on `sem_b` before `first()` does. If the order is different, the system could return back to the state in which neither semaphore is acquired.

![State model for Code Listing 7.18](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.7.3.png)

Figure 7.7.1: State model for Code Listing 7.18

To make the description a little more concrete, imagine that you and a friend are sitting at a table to eat. There is one pair of chopsticks available; in order to eat, you need both chopsticks. When the food arrives, you both forget your manners and try to grab the chopsticks first. You manage to grab one while your friend got the other. You’re both so hungry that you refuse to let go of the chopstick you have. But neither of you can eat, because you only have one chopstick. And since neither of you will give up the one you have, you’re stuck.

///7.7.1. Necessary Conditions[¶]
---------------------------------

So what exactly causes deadlock? First, it’s important to note that deadlock is a [race condition](#term-race-condition). You might write a program that runs millions of times over the span of years, only avoiding deadlock through lucky timing. As the code base gets larger and the system gets more complex, the possibility of deadlock arising gets even worse. For instance, you might have multiple threads that (without your awareness) use synchronization primitives through layers of encapsulated code. At some point, the timing of these primitives may line up precisely to cause deadlock.

There are four necessary conditions that can lead to deadlock. The first three conditions are generally unavoidable system characteristics. Deadlock could be made impossible by eliminating a single one of these characteristics.

> *   **Mutual exclusion**: Once a resource has been acquired up to its allowable capacity, no other thread is granted access.
> *   **No preemption**: Once a thread has acquired a resource, the resource cannot be forcibly taken away. For instance, only the owner of a mutex can unlock it.
> *   **Hold and wait**: It is possible that a thread can acquire one resource and retain ownership of that resource while waiting on another.

In modern, general-purpose computing, these three characteristics are _de facto_ requirements. Violating any of these characteristics would make concurrent systems development significantly more difficult. For instance, by throwing out mutual exclusion, `pthread_mutex_lock()` would not block a thread if the mutex has already been acquired. By eliminating the _no preemption_ characteristic, a thread would repeatedly need to check if it still holds the mutex or if the semaphore’s value is still acceptable.

Eliminating _hold and wait_ would lead to devastating and undesirable performance. For instance, consider a database with millions of records, each of which can be locked independently. Once a transaction is ready to commit, the thread may acquire a single manager lock for the entire database. The reason for this split design is that manipulating the local record may require several operations and take a while, but they are independent and can happen in parallel. The final commit must happen one at a time, but it is very quick. Eliminating _hold and wait_ would create the worst possible combination. Since both locks would have to be acquired and released at the same time, it would be impossible for the records to be manipulated in parallel. At the same time, the manager lock would have to be held for far too long while a single record is manipulated.

Ultimately, the three characteristics above are unavoidable and undesirable in modern systems. But it’s important to note that these three characteristics by themselves are not sufficient for deadlock. Deadlock is a race condition that arises by the fourth necessary condition.

> *   **Circular wait**: One thread needs a resource held by another, while this second thread needs a different resource held by the first.

Circular wait can be extended for more than two threads intuitively. Thread A needs something from thread B, which needs something from thread C, which is waiting on thread A. This chain of dependencies can be made arbitrarily long. Counterintuitively, circular wait can also arise even if there are multiple copies of a resource. This is a complex situation that arises in very advanced types of systems, though, and we refer interested readers to textbooks on OS design for more information.

🐞🐛🐌 Bug Warning

* * *

It is a common mistake to assume that deadlock only arises with certain types of synchronization primitives, such as mutex locks. This confusion arises because the term _mutual exclusion_ is often used to mean that only one thread has access to a critical section. The term is used more broadly here, allowing for the possibility that there may be multiple concurrent accesses; however, those accesses are blocking out some others. Ultimately, deadlock can arise with the use of locks, semaphores, barriers, or condition variables.

///7.7.2. Livelock and False Solutions[¶]
-----------------------------------------

[Livelock](#term-livelock) is a race condition that is similar to deadlock, though there is a slight difference. The key distinction with deadlock and livelock is whether or not the thread is changing state and executing anything. With deadlock, a thread is blocked and not executing any code; with livelock, the thread repeatedly changes state between blocked and unblocked, but accomplishes nothing.

As a simple illustration of livelock, consider [Code Listing 7.19](#cl7-19), which attempts to fix the deadlock in the example above. In this code, after successfully acquiring the first semaphore, the threads use `sem_try_wait()` on the second. If the wait fails (because the semaphore is locked by the other thread), the thread releases the semaphore it has first acquired and starts over.

```cpp
/* Code Listing 7.19:
   Livelock occurs when the system is not deadlocked but no work is done
 */

void *
first (void * _args)
{
  struct args *args = (struct args *) _args;
  /* Acquire "sem_a" before trying to acquire "sem_b" */
  while (1)
    {
      sem_wait (&args->sem_a);
      /* Now try for the sem_b, breaking out if successful */
      if (sem_try_wait (&args->sem_b))
        break;
      /* Failed to acquire sem_b, so start over */
      sem_signal (&args->sem_a);
    }
  /* other code here */
}

void *
second (void * _args)
{
  struct args *args = (struct args *) _args;
  /* Acquire "sem_b" before trying to acquire "sem_a" */
  while (1)
    {
      sem_wait (&args->sem_b);
      /* Now try for the sem_a, breaking out if successful */
      if (sem_try_wait (&args->sem_a))
        break;
      /* Failed to acquire sem_a, so start over */
      sem_signal (&args->sem_b);
    }
  /* other code here */
}
```

This code makes deadlock impossible by voluntarily breaking the characteristic of hold and wait. Note that it works in this scenario because no work is done after the first semaphore is acquired. This approach is not generalizable, though, because most systems will perform work in between the two semaphore acquisitions; in many instances, that work performed cannot be undone.

While the code avoids deadlock, it still allows for the possibility of livelock. That is, it is possible that both threads _repeatedly_ are successful when acquiring the first semaphore, leading to both failing at the second. Then both threads release the semaphore they hold and start over again. If this keeps happening, the threads are experiencing livelock. They are not in deadlock because there is the _possibility_ that a good timing can lead to recovery. However, unlucky timing is causing the threads to repeatedly block each other, preventing the system from making progress.

In practice, this code sample is unlikely to experience livelock for a long time. Specifically, it is very improbable that a thread switch occurs at the exact same moment repeatedly. But this code structure is a very simple and special case. As such, this structure is not considered a reliable solution to avoiding deadlock.

///7.7.3. Avoiding Deadlock[¶]
------------------------------

As three of the deadlock conditions (mutual exclusion, no preemption, hold and wait) are standard features in modern systems, the typical solution to the avoiding deadlock in software[[1]](#f45) is to avoid circular waiting. There are a variety of strategies that can be employed to avoid circular waiting that can be applied, depending on the needs of the system being built.

> *   Impose an ordering on resources. If one thread acquires `sem_a` prior to acquiring `sem_b`, require other threads to follow this order.
> *   Use timed or non-blocking variants that can provide immediate notification of failure. If a function like `pthread_mutex_trylock()` or `pthread_cond_timedwait()` returns an error, release other held resources and try again later.
> *   Limit the number of potential thread accesses. For instance, consider a scenario where there are five resource instances, and each thread needs two of them. Using a semaphore initialized to four would guarantee at least one thread will always have access to both the instances it needs; this strategy allows that thread to finish its work, eventually releasing both resource instances for the other threads to use.
> *   Employ higher-level synchronization primitives and strategies. In the next chapter, we describe some common well-known solutions that are known to be deadlock-free.

[[1]](#id3)

Advanced systems software, such as OS kernels, use a variety of techniques for solving the deadlock problem. These include applying the Banker’s algorithm for deadlock avoidance or executing deadlock detection algorithms that can alert a system administrator. Each approach has its benefits and drawbacks, and there is no universally accepted solution.



//7.8. Extended Example: Event Log File[¶]
==========================================

Many concurrent systems use a common log file to store records of key events. For instance, journaling file systems use a log to keep track of changes that can be validated in case of a system crash. This Extended Example uses multiple threads that create random fake events that then get written to the common log file. Access to the log file is synchronized to ensure that the writes do not interfere with each other.

```cpp
#include <assert.h>
#include <fcntl.h>
#include <pthread.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/stat.h>
#include <time.h>
#include <unistd.h>

#define SHALENGTH 64
#define LOGFILE "var.log"

/* Change these to increase the number of threads and events */
#define NUM_THREADS 5
#define NUM_ITERATIONS 10

void append_log_message (FILE *, size_t, char *);
char * compute_log_hash (FILE *, size_t);
void create_initial_log (void);
char * generate_random_log (int);
bool verify_log (FILE *);

/* Thread parameter struct */
struct log_params {
  int tid;                /* thread ID */
  pthread_mutex_t *mutex; /* shared mutex */
  size_t *log_index;      /* shared log index */
  size_t iterations;      /* number of events */
};

/* Event-generating thread. Each copy of this thread will
   sleep for a random amount of time, then wake up to generate
   an event. When that happens, the thread acquires the lock
   for the log file, verifies that the log is still valid,
   appends a new log message, and closes the file. After a
   certain number of iterations has occurred, the thread exits */
void *
event_thread (void * _params)
{
  struct log_params *params = (struct log_params *) _params;
  unsigned seed = (unsigned) params->tid;
  size_t i;
  for (i = 0; i < params->iterations; i++)
    {
      usleep ((rand_r (&seed) % 500000) + 1);
      pthread_mutex_lock (params->mutex);

      /* Here begins the critical section */
      FILE *fp = fopen (LOGFILE, "a+");
      if (fp == NULL)
        {
          pthread_mutex_unlock (params->mutex);
          pthread_exit (NULL);
        }

      if (! verify_log (fp))
        {
          perror ("Failed to validate log file");
          fclose (fp);
          pthread_mutex_unlock (params->mutex);
          pthread_exit (NULL);
        }

      /* All previous log messages have hashed correctly. Append
         a new message, which includes an updated hash value. */
      char *log_message = generate_random_log (params->tid);
      append_log_message (fp, *params->log_index, log_message);
      free (log_message);

      /* Increment shared log index number and close the file */
      (*params->log_index)++;
      fclose (fp);

      /* End of critical section */
      pthread_mutex_unlock (params->mutex);
    }
  pthread_exit (NULL);
}

/* Main entry point */
int
main (void)
{
  /* Create a bogus initial log to start with */
  create_initial_log ();
  size_t log_index = 1;

  pthread_mutex_t mutex;
  pthread_mutex_init (&mutex, NULL);

  /* Create the thread parameters and the threads */
  pthread_t threads[NUM_THREADS];
  struct log_params params[NUM_THREADS];
  size_t i;
  for (i = 0; i < NUM_THREADS; i++)
    {
      params[i].tid = i;
      params[i].mutex = &mutex;
      params[i].log_index = &log_index;
      params[i].iterations = NUM_ITERATIONS;
      pthread_create (&threads[i], NULL, event_thread, &params[i]);
    }

  /* Join the threads then destroy the mutex and exit */
  for (i = 0; i < NUM_THREADS; i++)
    pthread_join (threads[i], NULL);

  pthread_mutex_destroy (&mutex);
  pthread_exit (NULL);
}

/* Log file verification routine */
bool
verify_log (FILE *fp)
{
  /* Compute the hash value from all lines except the last bytes
     that contain the final hash */
  char *computed = compute_log_hash (fp, SHALENGTH + 2);

  /* Now read the final hash and confirm that it matches */
  char expected[SHALENGTH + 3];
  memset (expected, 0, sizeof (expected));
  fread (expected, sizeof (char), SHALENGTH + 2, fp);

  if (strncmp (computed, expected, SHALENGTH))
    return false;

  free (computed);
  return true;
}

/* Append a message to the log file */
void
append_log_message (FILE *fp, size_t index, char *message)
{
  /* Add the message at the end of the file and flush the data
     to ensure it gets written to disk */
  fseek (fp, 0, SEEK_END);
  fprintf (fp, "%zd: %s\n", index, message);
  fflush (fp);

  /* Re-compute the hash with the new message and append it to
     the end of the file */
  char *computed = compute_log_hash (fp, 0);
  fseek (fp, 0, SEEK_END);
  fprintf (fp, "%s\n\n", computed);
  free (computed);
}

/* For a given file, compute the SHA-256 hash sum of the contents.
   The trailing parameter indicates the number of bytes at the
   end of the file to omit from the computation. Returns a
   dynamically allocated string that must be freed. */
char *
compute_log_hash (FILE *fp, size_t trailing)
{
  /* Allocate pipes for parent-to-child communication and create
     a child process */
  int p2c_pipe[2], c2p_pipe[2];
  pipe (p2c_pipe);
  pipe (c2p_pipe);
  pid_t child_pid = fork ();
  assert (child_pid >= 0);

  /* Child process will read data from the pipe, redirecting this
     data as stdin to the shasum program. The stdout will be
     redirected through the pipe back to the parent. */
  if (child_pid == 0)
    {
      close (p2c_pipe[1]);
      close (c2p_pipe[0]);
      dup2 (p2c_pipe[0], STDIN_FILENO);
      dup2 (c2p_pipe[1], STDOUT_FILENO);
      execlp ("shasum", "shasum", "-a", "256", "-", NULL);
    }

  /* Parent resumes from here by closing the unused pipe ends */
  close (p2c_pipe[0]);
  close (c2p_pipe[1]);

  /* Determine the size of the file without the last two lines
     (containing a shasum and a blank line) */
  struct stat file_info;
  assert (stat (LOGFILE, &file_info) >= 0);
  size_t remaining = file_info.st_size - trailing;
  fseek (fp, 0, SEEK_SET);

  /* Instead of reading the entire file contents (which could be
     large), read a chunk at a time and write the contents into
     the pipe for the child to access */
  size_t size = 1024;
  char buffer[size + 1];
  while (remaining > size)
    {
      memset (buffer, 0, sizeof (buffer));
      fread (buffer, sizeof (char), size, fp);
      write (p2c_pipe[1], buffer, size);
      remaining -= size;
    }

  /* If there is any left over, read those bytes and send them to
     the child */
  if (remaining > 0)
    {
      memset (buffer, 0, sizeof (buffer));
      fread (buffer, sizeof (char), remaining, fp);
      write (p2c_pipe[1], buffer, remaining);
    }

  /* All data have been written, so close the pipe so the child
     can finalize its calculation */
  close (p2c_pipe[1]);

  /* Read the computed shasum back and compare it with the last
     shasum in the file */
  char shabuf[SHALENGTH + 1];
  memset (shabuf, 0, sizeof (shabuf));
  read (c2p_pipe[0], shabuf, SHALENGTH);
  close (c2p_pipe[0]);

  return strdup (shabuf);
}

/* asctime_r returns 24 characters plus \n and \0 */
#define ASCTIME_BUFSIZE 26

char *
generate_random_log (int tid)
{
  /* Determine the current local time and convert it to an
     ASCII-printable format */
  char timebuf[ASCTIME_BUFSIZE];
  memset (timebuf, 0, sizeof (timebuf));
  struct tm localbuf;
  memset (&localbuf, 0, sizeof (localbuf));

  time_t current = time (NULL);
  struct tm *local = localtime_r (&current, &localbuf);
  char *timemsg = asctime_r (local, timebuf);

  char *message = calloc (100, sizeof (char));
  assert (message != NULL);

  /* Use the ASCII-formatted time to create a log message like:
     [Wed Jun 26 10:32:15 1996] Event from thread 5
   */
  message[0] = '[';
  strncpy (&message[1], timemsg, ASCTIME_BUFSIZE - 2);
  strncat (message, "] Event from thread ", 100 - ASCTIME_BUFSIZE);
  snprintf (&message[ASCTIME_BUFSIZE + 19], 100 - strlen (message), "%d", tid);

  /* Return a heap-allocated copy of the string */
  return strdup (message);
}

/* Create bogus initial commit message to have a starting point */
void
create_initial_log (void)
{
  unlink (LOGFILE);
  struct stat file_info;
  stat (LOGFILE, &file_info);

  int file = open (LOGFILE, O_RDWR | O_CREAT);
  assert (file >= 0);

  fchmod (file, 0644);
  write (file, "0: [Tue Jun 25 22:09:41 2019] Initial commit\n", 45);
  write (file, "6231aaee038b4ab9b3fd50e5bf604f26"
               "97af98b5c126f4f86f5aaca809439c65\n\n", SHALENGTH + 2);
  close (file);
}
```


/Chapter 8   Synchronization Patterns and Problems[¶]
=====================================================

//8.1. Synchronization Patterns and Problems[¶]
===============================================

![Timeline of major CSF topics with Multicore and Threads highlighted](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Timeline.8.png)

> “If you’re not failing 90% of the time, then you’re probably not working on sufficiently challenging problems.”
> 
> Alan Kay

From the earliest work on synchronization, it became clear that there were common software patterns that could solve key timing issues and race conditions that frequently occurred. Edsger Dijkstra characterized one such pattern as the Dining Philosophers problem in 1965. Tony Hoare—while working on his Communicating Sequential Processes (CSP) language to support formal analysis of concurrency—revisited this problem in 1985 and defined the standard version of it. Solutions for these types of synchronization problems are powerful and reusable techniques for creating robust and safe implementations of concurrent software.

🚀 Chapter Objectives

* * *

In this chapter, we will address the following instructional objectives:

*   We will illustrate several design patterns that can be used for simple synchronization tasks.
*   We will examine classical synchronization problems that have been studied extensively, with well-known solutions.
*   We will highlight classical synchronization problems that demonstrate the limitations of basic synchronization primitives.
*   We will consider additional synchronization problems with more complex requirements that can be mapped to simpler design patterns.

[Race condition](#term-race-condition) bugs can be extremely difficult to diagnose and fix. Observing these bugs depends on the programmer’s ability to reconstruct the timing that makes the bug manifest. This task becomes even more difficult when the bug is caused by the interaction of multiple system components that may or may not be under that programmer’s control. Fortunately, many common tasks in concurrent software are simple in nature and can be achieved by applying basic synchronization design patterns with semaphores, locks, or other [synchronization primitives](#term-synchronization-primitive). Other tasks are variations on well-known [synchronization problems](#term-synchronization-problem). These problems tend to have an underlying structure that has a known solution that provides safe and efficient execution.

This chapter will focus on known solutions to common tasks that arise in concurrent software. We will describe the scenarios in which these design patterns and solutions apply and provide informal explanations of why they work. For the more complex problems, we will consider how the requirements can be decomposed into pieces that map to the more basic design patterns.



//8.2. Basic Synchronization Design Patterns[¶]
===============================================

[Locks](#term-lock) are very simple synchronization primitives, as they have only one intended purpose: ensuring mutually exclusive access to a critical section. Other primitives, such as [semaphores](#term-semaphore), can also provide mutual exclusion. However, semaphores are flexible and can be used for a variety of synchronization goals. This section describes four common techniques.

///8.2.1. Signaling[¶]
----------------------

The simplest synchronization design pattern uses semaphores for [signaling](#term-signaling-synchronization). Signaling arises when one thread needs to wait until some particular event has occurred. This timing is accomplished by waiting on shared semaphore that is incremented immediately after the event.

> *   Initialize the semaphore to 0.
> *   One thread calls `sem_wait()` to block until some critical event has occurred.
> *   A second thread detects that the event has occurred, then calls `sem_post()` to unblock the waiting thread.

The key observation with signaling is that the scheduling of the threads does not affect the correctness of the results. That is, there are only two possible scenarios to consider. In one scenario, the thread that calls `sem_wait()` runs first. Since the semaphore is initialized to 0, the thread must block until the other thread runs and calls `sem_post()`. Alternatively, the second thread runs first and calls `sem_post()`, incrementing the semaphore’s value to 1. Then, when the other thread calls `sem_wait()`, it can proceed without blocking because the event has already occurred.

[Code Listing 8.1](#cl8-1) uses a separate thread to perform some sort of initialization work. For instance, this initialization might involve reading in a large amount of data from configuration files, allocating request queues, overwriting the default signal handlers, or other such tasks. This initialization may be done concurrently with other work that the main thread is trying to accomplish. But at a certain point, the main thread needs to pause until it can be guaranteed that all of the initialization is done. The semaphore guarantees the timing of this pause.

```cpp
/* Code Listing 8.1:
   The general structure of a program that uses signaling after initialization
 */

void *
initialize (void *args)
{
  /* Cast the arguments into a useful struct type */
  sem_t *semaphore = (sem_t *) args;
  /* Perform some time-consuming initialization here */
  /* Alert calculate thread that initialization is complete */
  sem_post (semaphore);
  /* Perform some clean-up or do other work before exiting */
  pthread_exit (NULL);
}

int
main (int argc, char **argv)
{
  /* Declarations omitted for brevity */
  /* For signaling, initialize the semaphore to 0 */
  sem_t *init_sem = sem_open ("/OpenCSF_Sema", O_CREAT | O_EXCL, S_IRUSR | S_IWUSR, 0);
  assert (init_sem != SEM_FAILED);
  /* Create initialization thread */
  assert (pthread_create (&init, NULL, initialize, init_sem) == 0);
  /* Perform other work while initialization occurs */
  /* Pause until initialization is complete */
  sem_wait (init_sem);
  sem_unlink ("/OpenCSF_Sema");
  /* Do other work that depended on complete initialization */
  return EXIT_SUCCESS;
}
```

The use of the semaphore here may seem unnecessary, and that would be a fair objection to this particular scenario. That is, the same timing could be achieved by having the main thread call `pthread_join()` when it needs to wait for the initialization thread to complete. However, using the semaphore allows the initialization thread to signal that the critical work has been done _before_ the thread finishes. This early notification may be beneficial if the thread needs to perform additional work, such as freeing up allocated memory or cleaning up files. Furthermore, there may be other threads that are also waiting on the initialization; since they are not the thread’s parent, they cannot join it and must be signaled using a mechanism such as a semaphore.

///8.2.2. Turnstiles[¶]
-----------------------

A [turnstile](#term-turnstile) is a variant on signaling that can cause a chain-reaction that unblocks several threads one at a time. The key structure of a turnstile is to follow `sem_wait()` immediately with a call to `sem_post()`. As with signaling, the semaphore must be initialized to 0, which ensures that every thread executing the turnstile gets blocked. That is, the turnstile acts like a locked door and the threads form a queue waiting to get in.

Once it becomes acceptable for the threads to enter, one thread makes a single call to `sem_post()`. This call unblocks exactly one thread that is waiting at the turnstile; that thread returns from the `sem_wait()` that had blocked it and immediately calls `sem_post()`, unblocking the next thread. This pattern then continues, with each thread unblocking the next thread in line, one at a time. [Code Listing 8.2](#cl8-2) illustrates this pattern, with the `user()` waiting at the turnstile.

```cpp
/* Code Listing 8.2:
   Turnstiles create a sequence of unblocking all other threads
 */

void *
user (void *args)
{
  sem_t *semaphore = (sem_t *) args;

  /* Turnstile causes a chain reaction of unblocking */
  sem_wait (semaphore);
  sem_post (semaphore);
  /* Do other work and exit the thread */
}

int
main (int argc, char **argv)
{
  /* Declarations and earlier work */
  /* Create helper threads */
  for (i = 0; i < NUM_THREADS; i++)
    assert (pthread_create (&thread[i], NULL, user, sem) == 0);
  /* Unblock the first thread, which unblocks the second, etc. */
  sem_post (sem);
  /* Continue with other work */
}
```

Turnstiles allow POSIX semaphores to behave similar to the broadcast feature of [condition variables](#term-condition-variable) or using the System V `semop()` function to increase a semaphore by more than 1. There is a key difference that distinguishes the intended use of turnstiles, though. Turnstiles cause the unblocking to propagate to all threads that are waiting at the turnstile, but also those that have not arrived yet. Broadcasting only notifies threads that are already waiting, and `semop()` unblocks a maximum of the number of threads specified by the `sem_op` argument. In other words, turnstiles permanently unblock all current and future threads based on a key event.

///8.2.3. Rendezvous[¶]
-----------------------

A [rendezvous](#term-rendezvous) is a mutual signaling between two threads. The goal of a rendezvous is to ensure that two threads meet at a pre-defined common point. To create a rendezvous, two semaphores are initialized to 0. The two threads then call `sem_post()` on one semaphore just prior to calling `sem_wait()` on the other. At run-time, one thread will arrive at the rendezvous first and get blocked by the call to `sem_wait()`. Then, when the other thread arrives, it unblocks the first by calling `sem_post()`; however, this thread is not blocked by the call to `sem_wait()`, as the semaphore’s value has already been incremented to 1.

To illustrate the use of a rendezvous, assume that a program wants to detect corrupted files on a web server. To do this, the program uses one thread to retrieve the file from the web site; a second thread reads the same file from a location that is known to be secure and correct. The two threads need to complete reading the two copies before comparing their contents (a mismatch would indicate corruption). [Code Listing 8.3](#cl8-3) shows the structure for these two threads.

```cpp
/* Code Listing 8.3:
   Separate threads can be used to download a file twice to detect corruption
 */

void *
download (void *_args)
{
  /* Cast the arguments into a useful struct type */
  struct args *args = (struct args *) _args;
  /* Download the file from the potentially corrupted server */
  /* Rendezvous; must wait until secure loading is done */
  sem_post (args->download_sem);
  sem_wait (args->secure_sem);
  /* Additional work or thread cleanup here */
}

void *
secure_load (void *_args)
{
  struct args *args = (struct args *) _args;
  /* Securely load the file from a correct location */
  /* Rendezvous; must wait until downloading is done */
  sem_post (args->secure_sem);
  sem_wait (args->download_sem);
  /* Additional work or thread cleanup here */
}
```

///8.2.4. Multiplexing[¶]
-------------------------

In many cases, mutual exclusion is too strong of a system requirement, but it is reasonable to place a limit on the number of concurrent accesses. For example, network servers (e.g., web or e-mail servers) allow hundreds or thousands of concurrent network connections to serve content to remote users. Allowing an unlimited number of connections would exhaust the system resources, such as consuming all of the system’s available memory or trying to read too many files on a single hard drive.

The solution in this case is [multiplexing](#term-multiplexing-semaphore). Multiplexing allows multiple concurrent accesses up to a given limit; additional requests beyond that limit are blocked until more resources become available. The key defining feature of multiplexing is to initialize the semaphore’s value to a positive integer greater than 1. This initial value is the _maximum_ number of concurrent accesses allowed. Once the thread semaphore has been decremented to 0, the limit has been reached and future accesses are blocked. Mutual exclusion is a special case of multiplexing, where the initial value is 1.

[Code Listing 8.4](#cl8-4) illustrates the code framework for a server that launches a new thread each time a request has been received. Whenever a request is received, the main thread first decrements the semaphore to determine if the limit has been reached. If the limit has not been reached, a new thread handles the request. However, if the maximum number of threads has already been reached, the main thread gets blocked. Eventually, one of the threads serving content finishes. Just before exiting, that thread increments the semaphore, which unblocks the main thread; at that point, the main thread creates a new thread to handle the pending request it had received.

```cpp
/* Code Listing 8.4:
   Semaphore multiplexing can place a limit on the number of concurrent threads
 */

void *
serve_content (void *_args)
{
  /* Cast the args struct to get the semaphore and arguments */
  struct args *args = (struct args *) _args;
  /* ... Serve content based on the request ... */
  /* Thread is exiting; post to allow more threads in */
  sem_post (args->semaphore);
  pthread_exit (NULL);
}

int
main (int argc, char **argv)
{
  /* Initialization and other work */
  while (1)
    {
      /* ... Wait for incoming request ... */

      /* Request received; can a new thread be created? */
      sem_wait (create_thread_sem);
      /* Thread limit is not reached, so create a new thread */
      args = malloc (sizeof (struct args));
      args->semaphore = create_thread_sem;
      /* Additional args initialization here */
      /* Create the thread; up the semaphore if creation fails */
      if (pthread_create (&child_thread, NULL, serve_content, args) != 0)
        sem_post (create_thread_sem);
    }
}
```

///8.2.5. Lightswitches[¶]
--------------------------

When providing concurrent access to a resource, it is sometimes necessary to distinguish between the type of threads that are allowed access. For instance, consider a web-based application that allows multiple people to edit a document collaboratively. Each user is assigned a separate thread for modifying the contents of the document. In addition, a separate thread is responsible for storing a backup copy automatically. Other threads might be running to check the spelling or grammar. When there are different types of threads, it may be necessary for one type of thread to lock out other types while still allowing concurrent access for the same type.

The [lightswitch](#term-lightswitch) pattern makes it possible to enforce this type of constraint. In addition, the lightswitch allows the first thread of a particular type to perform some initialization as needed. The name derives from the idea of a group of people entering a room; the first person to enter turns on the lights by flipping the lightswitch and the last person to leave turns the lights off. [Code Listing 8.5](#cl8-5) shows one variant on a lightswitch. Note that the `if` statement in both cases can be extended to perform additional initialization and clean-up work if needed.

```cpp
/* Code Listing 8.5:
   Functions for creating an asymmetric lightswitch
 */

void
enter (pthread_mutex_t *lock, sem_t *can_enter, int counter)
{
  pthread_mutex_lock (lock);
  /* First thread waits to be able to enter */
  if (++counter == 1)
    sem_wait (can_enter);
  pthread_mutex_unlock (lock);
}

void
leave (pthread_mutex_t *lock, sem_t *can_enter, int counter)
{
  pthread_mutex_lock (lock);
  /* Last thread waits to be able to enter */
  if (--counter == 0)
    sem_post (can_enter);
  pthread_mutex_unlock (unlock);
}
```

The implementation in [Code Listing 8.5](#cl8-5) works for an _asymmetric_ approach in which one type of thread allows concurrent access while another does not. The `enter()` and `leave()` functions would be used by the thread type that allows concurrency. The thread type that requires mutual exclusion would work directly with the `can_enter` semaphore without using the lock. That is, the lightswitch could be used as shown in [Code Listing 8.6](#cl8-6).

```cpp
/* Code Listing 8.6:
   Using a lightswitch in asymmetric concurrency
 */

void *
concurrent_thread (void * _args)
{
  /* Cast the args struct to get the data fields */
  struct args *args = (struct args *) _args;
  enter (args->lock, args->can_enter, args->counter);
  /* Critical section for concurrent access */
  leave (args->lock, args->can_enter, args->counter);
  /* Do other work and exit thread */
}

void *
mutex_thread thread (void * _args)
{
  /* Cast the args struct to get the data fields */
  struct args *args = (struct args *) _args;
  sem_wait (args->can_enter);
  /* Critical section for concurrent access */
  sem_post (args->can_enter);
  /* Do other work and exit thread */
}
```

🐞🐛🐌 Bug Warning

* * *

The lightswitch in [Code Listing 8.6](#cl8-6) is not safe if all types of threads allow for concurrent access to their respective critical sections. Consider the case where a thread of type A has previously entered. After that point, a thread of type B tries to enter, but is blocked by the semaphore within the `if` statement. Note, though, that this thread still retains the lock as it gets blocked. When any type A thread tries to leave, they will get blocked trying to acquire the lock. Consequently, none of the type A threads can post to the semaphore according to the `leave()` function. The system would then enter a state of [deadlock](#term-deadlock), as the type A threads are waiting on the type B thread that has the lock while the type B thread is waiting on a type A thread to post to the semaphore.

If the concurrency is not asymmetric—that is, multiple types of threads allow concurrent access—then the `if` statement would need to be modified. Using the structure in [Code Listing 8.5](#cl8-5), the `enter()` function could be modified to release the lock prior to calling `sem_wait(can_enter)`, then re-acquiring it as shown in [Code Listing 8.7](#cl8-7). Note that the `leave()` function remains unchanged, as `sem_post(can_enter)` would never block.

```cpp
/* Code Listing 8.7:
   A lightswitch for concurrency of all thread types
 */

void
enter (pthread_mutex_t *lock, sem_t *can_enter, int counter)
{
  pthread_mutex_lock (lock);
  /* First thread waits to be able to enter */ 
  if (++counter == 1)
    {
      pthread_mutex_unlock (lock);
      sem_wait (can_enter);
    }
  else
    pthread_mutex_unlock (lock);
}
```

As an alternative, the lightswitch could be modified to use condition variables as shown in [Code Listing 8.8](#cl8-8). In this case, a bool variable is added to indicate whether or not a new type of thread can enter its respective critical section. When the first thread of a new type enters the critical section, it would set `can_enter` to false, blocking out other threads that would try to enter until the last leaving thread sets `can_enter` back to true.

```cpp
/* Code Listing 8.8:
   Adapting the lightswitch for condition variables
 */

void
enter (pthread_mutex_t *lock, pthread_cond_t *leave,
       bool can_enter, int counter)
{
  pthread_mutex_lock (lock);
  /* First thread waits to be able to enter */ 
  if (++counter == 1)
    {
      while (! can_enter)
        pthread_cond_wait (leave, lock);
      can_enter = false;
    }
  pthread_mutex_unlock (lock);
}

void
leave (pthread_mutex_t *lock, pthread_cond_t *leave,
       bool can_enter, int counter)
{
  pthread_mutex_lock (lock);
  /* Last thread waits to be able to enter */
  if (--counter == 0)
    {
      can_enter = true;
      pthread_cond_signal (leave);
    }
  pthread_mutex_unlock (unlock);
}
```

Given that the lightswitch pattern is more complex than the other patterns, it is a good design choice to encapsulate the variables into a single `struct` that can then be passed around. Any thread needing access to the same lightswitch could be passed a pointer to the `struct` instance in the thread arguments as shown in [Code Listing 8.9](#cl8-9). Note that this assumes the interface for `enter()` and `leave()` have been adapted to receive the struct parameter instead of the individual fields.

```cpp
/* Code Listing 8.9:
   A modular approach to lightswitches
 */

typedef struct lightswitch {
  pthread_mutex_t *lock;
  sem_t *can_enter;
  int counter;
} ls_t;

void *
lightswitch_user (void * _args)
{
  /* Cast the args struct to get the data fields */
  ls_t *lightswitch = (struct args *) _args;
  enter (lightswitch);
  /* Critical section */
  leave (lightswitch);
  /* Do other work and exit thread */
}
```



//8.3. Producer-Consumer Problem[¶]
===================================

One of the most common task structures in concurrent systems is illustrated by the [producer-consumer problem](#term-producer-consumer-problem). In this problem, threads or processes are divided into two relative types: a producer thread is responsible for performing an initial task that ends with creating some result and a consumer thread that takes that initial result for some later task. Between the threads, there is a shared array or queue that stores the results being passed. One key feature of this problem is that the consumer removes the data from the queue and _consumes_ it by using it in some later purpose. There is no way for the consumer thread or threads to repeatedly access data in the queue.

As an example, consider a researcher who discovers a previously unknown play and they believe it may have been written by a famous author. As part of their work, this researcher wants to know if the newly discovered text uses common words with the same frequency that author used in other works. This work could be done with a pipe-and-filter application that reads in the content of the texts, builds a search index of all words based on their frequency, performs some computation that compares the search indexes of all of the works. One thread is assigned the tasks of reading in the contents and _producing_ the list of words, placing a word at a time in a queue. A second thread _consumes_ the words by removing them from the queue and adding them to the data used to build the search index. This thread then becomes a producer because it provides the index to yet another thread that will be performing the index comparisons.

///8.3.1. Producer-Consumer with Unbounded Queue[¶]
---------------------------------------------------

As a first variant on this problem, consider two threads that share an unbounded queue. This approach can be implemented using a linked list approach for the queue. Specifically, we can assume that there is a `queue_t` structure that contains pointers to the front and back of the queue, where the nodes in the queue are of type `queue_node_t`. The nodes all contain pointers to some sort of `data_t` field. [Code Listing 8.10](#cl8-10) shows the framework for enqueueing and dequeueing data.

```cpp
/* Code Listing 8.10:
   Enqueue and dequeue operations for a linked list implementation of a queue
 */

void
enqueue_unsafe (queue_t *queue, data_t *data)
{
  /* Create a new node and make it the new back of the queue */
  queue->back->next = calloc (1, sizeof (queue_node_t));
  assert (queue->back->next != null);
  queue->back = queue->back->next;
  queue->back->data = data;
}

data_t *
dequeue_unsafe (queue_t *queue)
{
  /* If back = front, then the queue is empty */
  if (queue->back == queue->front)
    return NULL;
  data_t * data = queue->front->data;
  queue_node_t * next = queue->front->next;
  free (queue->front);
  queue->front = next;
  return data;
}
```

This implementation, which would be acceptable for a single-threaded application, has a race condition when used in a concurrent setting. Specifically, if one thread begins to `enqueue()` some data, another thread that tries to `dequeue()` the data at the same time may get a `NULL` pointer because the first thread has not yet reached the line the advances the back pointer.

The solution here would be to refactor these functions to become a monitor as shown in [Code Listing 8.11](#cl8-11). Each function would be passed a reference to a shared mutex that would be locked on entry and released just before the function returns. This solution eliminates the race condition regarding the timing of access to the queue’s `back` field. Additionally, this solution is generalizable regardless of the number of producers and consumers. If there are multiple producers trying to `enqueue()` data, the mutex ensures that they will not try to manipulate the queue at that same time. For brevity, [Code Listing 8.11](#cl8-11) calls the functions from [Code Listing 8.10](#cl8-10).

```cpp
/* Code Listing 8.11:
   A synchronized version of linked list enqueueing and dequeueing
 */

void
enqueue (queue_t *queue, data_t *data, pthread_mutex_t *lock)
{
  pthread_mutex_lock (lock);
  enqueue_unsafe (queue, data);
  pthread_mutex_unlock (lock);
}

data_t *
dequeue (queue_t *queue, pthread_mutex_t *lock)
{
  pthread_mutex_lock (lock);
  data_t * data = dequeue_unsafe (queue); 
  pthread_mutex_unlock (lock);
  return data;
}
```

///8.3.2. Single Producer-Single Consumer Solution Using a Bounded Queue[¶]
---------------------------------------------------------------------------

In many cases, the unbounded queue of the previous section is neither feasible nor desirable. For instance, this approach makes it very easy to launch a [denial-of-service](#term-denial-of-service) attack against the application, particularly if this code is used in a server. [Code Listing 8.12](#cl8-12) shows a single line of code that quickly exhausts the program’s dynamic memory resources, leading to a system crash.

```cpp
/* Code Listing 8.12:
   A trivial denial-of-service attack against the code in 8.11
 */

while (1)
  enqueue (queue, data, lock);
```

Given this weakness, systems software typically imposes constraints on the number of items in the queue. One approach would be to use semaphores with the linked list queue above. Another approach is to use a finite circular array as the basis. For this approach, we will still assume the use of a `queue_t` data type to represent the queue, however the internal implementation uses an array instead of linked nodes. [Figure 8.3.1](ProdCons.html#circarray) illustrates the structure of the queue.

![A circular queue using an array](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.8.1.png)

Figure 8.3.1: A circular queue using an array

[Code Listing 8.13](#cl8-13) shows the unsafe version of the `enqueue()` and `dequeue()` operations for an array implementation of a queue. Within the `queue_t` data type, contents is an array of pointers to the enqueued data, while `front` and `back` are indexes into this array. When either of the indexes are incremented, the operations must recalculate the value modulo the array size to ensure that the values stay within the bounds of the array. This approach creates a circular structure, where the spaces in the array can be reused. Once the back index reaches the end of the array, it would return to the first position and continue from there.

```cpp
/* Code Listing 8.13:
   Enqueue and dequeue operations for an array queue implementation
 */

void
enqueue_unsafe (queue_t *queue, data_t *data)
{
  /* Store the data in the array and advance the index */
  queue->contents[queue->back++] = data;
  queue->back %= queue->capacity;
}

data_t *
dequeue_unsafe (queue_t *queue)
{
  data_t * data = queue->contents[queue->front++];
  queue->front %= queue->capacity;
  return data;
}
```

The implementation in [Code Listing 8.13](#cl8-13) has a fundamental flaw in both operations, as neither enforces the limited capacity. This design choice was intentional, as this code was meant to encapsulate the enqueueing and dequeueing behavior. That is, we can now reason through the synchronization issues for the producer-consumer problem without regard for the specific queue implementation.

To build the rationale for the solution to the producer-consumer problem, consider [Code Listing 8.14](#cl8-14). This implementation attempts to keep track of the number of items (`queue->size`) in the queue, comparing it with values that indicate whether the queue is full or empty.

```cpp
/* Code Listing 8.14:
   An unsuccessful attempt at solving producer-consumer
 */

void
enqueue_failure (queue_t *queue, data_t *data)
{
  if (queue->counter == queue->capacity) return;
  enqueue_unsafe (queue, data);
  queue->counter++;
}

data_t *
dequeue_failure (queue_t * queue)
{
  if (queue->counter == 0)
    return NULL;
  data_t * data = dequeue_unsafe (queue); 
  queue->counter--;
  return data;
}
```

The attempted solution in [Code Listing 8.14](#cl8-14) fails because it has race conditions on the queue’s `counter` variable. If the producer is attempting to enqueue an item at the same moment the consumer is dequeueing one, the outcome depends on whether the producer’s check for available space happens before or after the consumer decrements the `counter`. A similar race condition arises if the queue is empty and both functions are called. This race condition could be fixed by wrapping the accesses to the `counter` with a mutex.

[Code Listing 8.14](#cl8-14) has another flaw from the producer’s perspective: **there is no indication that there was a failure to enqueue the item**. Addressing this failure would require changing the function’s interface to return a status to indicate success or failure. This approach, however, imposes an undesirable burden on the user. That is, the programmer who is building the producer and consumer threads or processes must build in a fail-safe mechanism to respond accordingly if the `enqueue()` or `dequeue()` operations fail.

A better, more user-friendly solution to the producer-consumer problem is to use semaphores. Note that semaphores incorporate the key functionality that we need: **atomic incrementing and decrementing of a counter variable**. The previous approach was trying to re-invent this already-solved problem. [Code Listing 8.15](#cl8-15) shows the framework for solving the producer-consumer problem for a single producer and single consumer.

```cpp
/* Code Listing 8.15:
   Solution for a single producer and single consumer
 */

void
enqueue (queue_t *queue, data_t *data, sem_t *space, sem_t *items)
{
  sem_wait (space);
  enqueue_unsafe (queue, data);
  sem_post (items);
}

data_t *
dequeue (queue_t * queue, sem_t *space, sem_t *items)
{
  sem_wait (items);
  data_t * data = dequeue_unsafe (queue); 
  sem_post (space);
  return data;
}
```

The structure of this approach is to use signaling with two semaphores. The key insight here is that there are actually _two_ bounds that need enforced: a maximum and a minimum. If the queue is full, then the producer needs to wait until there is a space available. That is, the producer must call `sem_wait(space)` and wait if necessary. The `space` semaphore is initialized to the capacity of the queue, so it would only be 0 if that many items are already in the queue. Once the consumer has removed an item from the queue, it performs the counterpart call `sem_post(space)` to alert the producer that a space is available. In short, the `space` semaphore enforces the maximum capacity of the queue.

At the same time, the consumer must not attempt to remove an item from the queue if none have been enqueued. Depending on the internal state of the queue, this attempt to dequeue an item might return old data that has previously be removed or an unexpected `NULL` pointer. The `items` semaphore, which is initialized to 0 and represents the number of enqueued items, enforces this constraint. If the queue is empty, this semaphore would have an internal value of 0, causing the consumer to block when it calls `sem_wait(items)`. Once the producer puts an item into the queue, it would perform the corresponding `sem_post(items)` that would increment the semaphore and unblock the consumer.

///8.3.3. Multiple Producers Solution Using a Bounded Queue[¶]
--------------------------------------------------------------

The solution in the previous section works successfully if there is only a single producer. However, if there are multiple producers, the previous solution will not work and should not be used. The problem is the line highlighted in [Code Listing 8.16](#cl8-16). Recall that post-increments in C are not atomic operations. That is, the internal execution at the machine language level involves a three-step process: load the value into a register, increment the value in the register, and store the result back into memory. If two threads try to perform this increment at the same time, the increments could interfere with each other.

```cpp
/* Code Listing 8.16:
   Non-atomic increments are always race conditions
 */

queue->contents[queue->back++] = data;
```

The solution in this case would be to add a lock as shown in [Code Listing 8.17](#cl8-17). Acquiring and releasing the lock as shown here minimizes the size of the critical section. That is, the operations on the semaphores do not require protection. Including the calls to `sem_wait()` and `sem_post()` within the critical section for the lock would unnecessarily prevent the other producers from manipulating the semaphores, partially eliminating the benefit of using a semaphore.

```cpp
/* Code Listing 8.17:
   Solution for a single producer and single consumer
 */

void
enqueue (queue_t *queue, data_t *data, sem_t *space, sem_t *items,
         pthread_mutex_t *lock)
{
  sem_wait (space);
  pthread_mutex_lock (lock);
  enqueue_unsafe (queue, data);
  pthread_mutex_unlock (lock);
  sem_post (items);
}
```

Note that this approach does not require any modification to the `dequeue()` operation used by the consumer. Since there is only a single consumer, there is no race condition on incrementing the `queue->front` variable. This solution could be extended to multiple consumers by introducing a second lock for consumers used in `dequeue()` as shown in [Code Listing 8.18](#cl8-18). It is important, in this case, to use separate locks rather than simply reusing the same lock for both producers and consumers. When both the `space` and `items` semaphores have positive values, then the front and back of the queue are guaranteed to be in different places. That is, there are multiple items in the queue, so the producer and consumer within the critical sections would not be interfering with each other. Consequently, there is no reason that the producer needs to lock out the consumer, or vice versa.

```cpp
/* Code Listing 8.18:
   Solution for a multiple producers and consumers
 */

void
enqueue (queue_t *queue, data_t *data, sem_t *space, sem_t *items,
         pthread_mutex_t *producer_lock)
{
  sem_wait (space);
  pthread_mutex_lock (producer_lock);
  enqueue_unsafe (queue, data);
  pthread_mutex_unlock (producer_lock);
  sem_post (items);
}

data_t *
dequeue (queue_t * queue, sem_t *space, sem_t *items,
         pthread_mutex_t *consumer_lock)
{
  sem_wait (items);
  pthread_mutex_lock (consumer_lock);
  data_t * data = dequeue_unsafe (queue); 
  pthread_mutex_unlock (consumer_lock);
  sem_post (space);
  return data;
}
```



//8.4. Readers-Writers Problem[¶]
=================================

The [readers-writers problem](#term-readers-writers-problem) illustrates a second common pattern in concurrent software. In this problem, multiple readers are sharing concurrent access to a resource. Unlike the consumers in the producer-consumer problem, the readers _do not change_ the shared resource in any way; the reader retrieves a copy of the data, but the original shared copy remains intact. In addition to the readers, one or more writers are responsible for modifying the data.

To illustrate the readers-writers problem, consider a web-based e-commerce application. This application is built on a multithreaded server that assigns requests to distinct threads. Some threads are queries that are requesting information about the company’s available products, their prices, and so on. As these requests do not modify the inventory or create purchase orders, these threads are acting as readers. On the other hand, some threads are processing requests to insert a new order; other threads are initiated from the delivery team to update the inventory and to indicate that an order has been shipped. These threads are writers.

///8.4.1. A Solution Using Lightswitches[¶]
-------------------------------------------

One common solution for the readers-writers problem aligns with the asymmetric lightswitch described previously. As the readers allow concurrent access, the reader thread would use the `enter()` and `leave()` routines to access the shared resource. The writers, however, require mutual exclusion, both with other writers and with all of the readers. Consequently, the writers do not need to employ the lightswitch, simply accessing the semaphore directly. [Code Listing 8.19](#cl8-19) shows the framework for this solution.

```cpp
/* Code Listing 8.19:
   Asymmetric lightswitch solution to readers-writers
 */

void *
reader (void * _args)
{
  /* Cast the args struct as a lightswitch */
  ls_t *lightswitch = (ls_t  *) _args;
  enter (lightswitch);
  /* Read the shared data */
  leave (lightswitch);
  /* Do other work and exit thread */
}

void *
writer (void * _args)
{
  /* Cast the args struct as a lightswitch */
  ls_t *lightswitch = (ls_t  *) _args;
  sem_wait (lightswitch->semaphore);
  /* Write to the shared data structure */
  sem_post (lightswitch->semaphore);
  /* Do other work and exit thread */
}
```

///8.4.2. Fairness to Writers[¶]
--------------------------------

![An unfair timing for the writer](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.8.2.png)

Figure 8.4.1: An unfair timing for the writer

The solution to the readers-writers problem as shown in [Code Listing 8.19](#cl8-19) has an important flaw to highlight. This approach fails to achieve [fairness](#term-fairness), particularly in relation to the writers. Specifically, consider the timing of events shown in [Figure 8.4.1](ReadWrite.html#unfair). In this scenario, Reader A arrives first and decrements the semaphore. When the writer arrives and tries to do the same, it gets blocked. The writer must then wait until _all readers_ have left. Once Reader B arrives, the two readers take turns leaving and re-entering. Since at least one reader is always in the system, the writer is blocked indefinitely, a situation known as [starvation](#term-starvation).

The writer’s starvation in this scenario can be fixed by placing a turnstile before the readers can enter as shown in [Code Listing 8.20](#cl8-20). As long as there is no writer attempting to enter the critical section, the readers can each pass through the turnstile and invoke `enter()` on the lightswitch. However, once a writer arrives, it will call `sem_wait()` on the turnstile semaphore, blocking new readers from passing through the turnstile. The writer will then call `sem_wait()` on the lightswitch semaphore and block. When the last of the readers that are already in the critical section call `leave()`, that thread will `sem_post()` to the lightswitch semaphore, allowing the writer to enter. The writer can then `sem_post()` to the turnstile, allowing readers to pass through again. The first reader through calls `enter()` on the lightswitch and gets blocked until the writer leaves.

```cpp
/* Code Listing 8.20:
   A starvation-free readers-writers solution
 */

struct args {
  ls_t *lightswitch;
  sem_t *turnstile;
};

void *
reader (void * _args)
{
  /* Cast the args as struct with lightswitch and turnstile */
  struct args *args = (struct args *) _args;
  sem_wait (args->turnstile);
  sem_post (args->turnstile);
  enter (args->lightswitch);
  /* Read the shared data */
  leave (args->lightswitch);
  /* Do other work and exit thread */
}

void *
writer (void * _args)
{
  /* Cast the args as struct with lightswitch and turnstile */
  struct args *args = (struct args *) _args;
  sem_wait (args->turnstile);
  sem_wait (lightswitch->semaphore);
  sem_post (args->turnstile);
  /* Write to the shared data structure */
  sem_post (lightswitch->semaphore);
  /* Do other work and exit thread */
}
```

Depending on the particular context, it may be acceptable or desirable to use the solution that potentially allows writer starvation. For instance, if read accesses to the critical section are extremely fast and concurrent reads are rare, starvation would not occur; the turnstile would then impose unnecessary system calls and potential context switches that may accumulate. However, this cost may be acceptable to mitigate potential delays to the writers.

///8.4.3. Search-Insert-Delete Problem[¶]
-----------------------------------------

The [search-insert-delete problem](#term-search-insert-delete-problem) is a variant on the readers-writers problem. In this variant, multiple threads can search through a data structure concurrently; the searchers are essentially identical to the readers from before. However, the writers are broken into two distinct types of threads: inserters are adding new data while deleters are removing elements. As with the original writers, deletions must be mutually exclusive with all other accesses to the shared data structure. However, insertions have a more relaxed requirement: they must be mutually exclusive with themselves and with deletions, but concurrent searches are still allowed. That is, the inserters and searchers cannot lock each other out, but only one inserter is allowed access at a time; deleters, on the other hand, require mutual exclusion.

This problem can be solved with slight modifications of the original solution for the readers-writers. The searcher thread is identical to the reader, as shown in [Code Listing 8.21](#cl8-21).

```cpp
/* Code Listing 8.21:
   Searchers are identical to readers
 */

void * 
searcher (void * _args)
{
  /* Cast the args struct as a lightswitch */
  ls_t *search_switch = (struct args *) _args;
  enter (search_switch);
  /* critical section */
  leave (search_switch);
  /* Do other work and exit thread */
}
```

Inserters are also very similar to readers. The main difference is that inserters also require mutual exclusion among themselves, requiring a lock as shown in [Code Listing 8.22](#cl8-22).

```cpp
/* Code Listing 8.22:
   Inserters also require a lock for mutual exclusion to the data structure
 */

struct ins_args {
  ls_t *insert_switch;
  pthread_mutex_t *insert_lock;
};

void *
inserter (void * _args)
{
  /* Cast the args struct as a lightswitch */
  struct ins_args *args = (struct ins_args *) _args;
  enter (args->insert_switch);
  pthread_mutex_lock (args->insert_lock);
  /* critical section */
  pthread_mutex_lock (args->insert_lock);
  leave (args->insert_switch);
  /* Do other work and exit thread */
}
```

To complete the solution, the deleter threads behave identically to the writers. The only difference is that the deleter must rely on both lightswitches for the other two types of threads as shown in [Code Listing 8.23](#cl8-23). If the deleter successfully passes the semaphore for the `search_switch`, then there are no searchers in the critical section and new searchers will be blocked by the lightswitch. Once the last inserter leaves the critical section, the deleter would be able to pass the `insert_switch` semaphore and enter the critical section. Additional inserters would also be locked out at this point. Once the deleter exits the critical section, inserters and searchers would be allowed back in.

```sh
/* Code Listing 8.23:
   Deleters must work with lightswitches for both searchers and inserters
 */

void *
deleter ()
{
  sem_wait (search_switch->semaphore);
  sem_wait (insert_switch->semaphore);
  /* critical section */
  sem_post (insert_switch->semaphore);
  sem_post (search_switch->semaphore);
}
```

This solution, which is a variant on the original readers-writers solution, has the same weakness previously discussed. The deleter threads can potentially face starvation, as long as there is at least one searcher or inserter in their critical sections. Interestingly, though, the searcher threads can also face starvation. Specifically, consider the case where all the searchers leave the critical section and a deleter has arrived. As long as there are inserters in the critical section, the deleter cannot enter. However, the deleter has already successfully locked out future searchers. Consequently, the future searchers would be effectively locked out by the inserters. Adapting the turnstile approach used for readers-writers would successfully prevent starvation in this case, too.



//8.5. Dining Philosophers Problem and Deadlock[¶]
==================================================

The previous chapter introduced the concept of [deadlock](#term-deadlock). Deadlock is the permanent blocking of two or more threads based on four necessary conditions. The first three are general properties of synchronization primitives that are typically unavoidable. The last is a system state that arises through a sequence of events.

> *   [Mutual exclusion](#term-mutual-exclusion): Once a resource has been acquired up to its allowable capacity, no other thread is granted access.
> *   [No preemption](#term-no-preemption): Once a thread has acquired a resource, the resource cannot be forcibly taken away. For instance, only the owner of a mutex can unlock it.
> *   [Hold and wait](#term-hold-and-wait): It is possible that a thread can acquire one resource and retain ownership of that resource while waiting on another.
> *   [Circular wait](#term-circular-wait): One thread needs a resource held by another, while this second thread needs a different resource held by the first.

![Providing the same number of plates and forks creates a problem for a set of dining philosophers](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.8.3.png)

Figure 8.5.1: Providing the same number of plates and forks creates a problem for a set of dining philosophers

The [dining philosophers problem](#term-dining-philosophers-problem) is a metaphor that illustrates the problem of deadlock. The scenario consists of a group of philosophers sharing a meal at a round table. As philosophers, they like to take some time to think; but they are at a meal, so they also need to eat. As illustrated in [Figure 8.](.html#diningphil), there is a large serving dish in the middle of the table. Every philosopher has a plate and two serving forks, one to their right and one to their left. When a philosopher decides that they are hungry enough, they stop thinking and grab the forks to serve themselves from the serving dish in the middle.

The problem arises when all of the philosophers decide to eat at the same time. Consider the case where all of the philosophers independently decide that they will try to grab the fork to their left first. When this happens, assuming all of the places at the table are occupied, then all of the forks have been taken. That is, each of the five forks shown are to the left of exactly one of the five philosophers. At this point, every philosopher has exactly one fork, but there are none available for anyone to get their second fork. Unless one of the philosophers decides to give up on eating and put a fork down, all of the philosophers will starve.

[Code Listing 8.24](#cl8-24) illustrates how this scenario translates into code with semaphores. Multiple _philosopher_ threads share an array of N _fork_ semaphores (numbered 0 through N-1) and each thread tries to acquire two of them. Thread 0 waits on semaphores 0 and 1, thread 1 waits on semaphores 1 and 2, and so on, until thread N-1 waits on semaphores N-1 and 0 (since the modulus operator is applied).

```cpp
/* Code Listing 8.24:
   The dining philosophers problem in code with semaphores
 */

void *
philosopher (void * _args)
{
  /* Cast the args as struct with self identifier, semaphores */
  struct args *args = (struct args *) _args;
  int self = args->self;       /* unique thread identifier */
  int next = (self + 1) % SIZE;
  sem_wait (args->sems[self]); /* pick up left fork */
  sem_wait (args->sems[next]); /* pick up right fork */
  /* Critical section (eating) */
  sem_post (args->sems[next]); /* put down right fork */
  sem_post (args->sems[self]); /* put down left fork */
  /* Do other work and exit thread */
}
```

Assuming they are initialized to 1 and the standard `sem_wait()` is used, semaphores exhibit the three required system features of deadlock. They enforce mutual exclusion because the second thread to attempt to down the semaphore will become blocked since the internal value would be negative. Since the threads can acquire one and get blocked trying to acquire the second semaphore, they satisfy the hold-and-wait criterion. And since no thread can break another thread’s claim to a semaphore, the no preemption criterion is met. The fourth criterion for deadlock, circular wait, arises from the sequence in which the threads wait on the respective semaphores. Every philosopher is waiting on the fork to their right, which has been grabbed by someone else as their left fork.

📜 Example 8.5.1

* * *

[Table 8.1](#tbl8-1) illustrates how the circular wait arises. Every thread successfully waits on one semaphore and gets blocked by the second.

Thread 0

Thread 1

Thread 2

Thread 3

Thread 4

sem_wait(0);  
  SUCCESS  
sem_wait(1);  
  BLOCKED

sem_wait(1);  
  SUCCESS  
sem_wait(2);  
  BLOCKED

sem_wait(2);  
  SUCCESS  
sem_wait(3);  
  BLOCKED

sem_wait(3);  
  SUCCESS  
sem_wait(4);  
  BLOCKED

sem_wait(4);  
  SUCCESS  
sem_wait(0);  
  BLOCKED

Table 8.1: If all threads get through the first semaphore, no one gets by the second

It is important to note that this problem applies to other synchronization primitives, not just semaphores. That is, locks and condition variables also meet the system requirements for deadlock. To illustrate how this problem could arise in practice with locks, consider the software that links incoming and outgoing connections in a network switch. In this scenario, assume that each thread responsible for forwarding data packets acquires locks on dedicated physical ports. As such, the software maintains an array of locks `nic_locks`. There is no assumption that the ports need to be adjacent; instead, the switch code just grabs any two that are available. [Code Listing 8.25](#cl8-25) shows a simplistic approach that fits the dining philosophers structure. The first loop iterates through the locks until one is assigned to be this thread’s incoming port; the second loop acquires the outgoing port. To make matters worse, these two loops may be in different portions of the code base, so someone reviewing the code **may not think they are actually connected in this way**!

```cpp
/* Code Listing 8.25:
   It is easy to fall into the dining philosophers structure without realizing it
 */

while (true)
  {
    in++;
    in %= NUMBER_OF_PORTS;
    if (!pthread_mutex_trylock (nic_locks[in]))
      break;
  }
  /* successfully locked network card in for incoming data */

while (true)
  {
    out++;
    out %= NUMBER_OF_PORTS;
    if (!pthread_mutex_trylock (nic_locks[out]))
      break;
  }
  /* successfully locked network card out for outgoing data */
```

///8.5.1. Solution of Limiting Accesses[¶]
------------------------------------------

One approach to solving the dining philosophers problem is to employ a multiplexing semaphore to limit the number of concurrent accesses. To return to the original metaphor, this solution would require that one of the seats at the table must always remain unoccupied. Assuming all of the philosophers try to grab their left fork first, the fork to the left of the empty seat would not be claimed. Consequently, the philosopher to the left of that seat could grab the fork as their second, as it is the fork to their left. After this philosopher eats, they can put both forks down, making their left fork available as the right fork for the next philosopher.

[Code Listing 8.26](#cl8-26) shows how to incorporate this approach into the structure of [Code Listing 8.24](#cl8-24). A single additional semaphore (`can_sit`) is created and initialized to N-1 for N semaphores. This semaphore prevents all N semaphores from being decremented by the first call to `sem_wait()`. As such, there must be at least one semaphore that can be decremented by the second call, guaranteeing one thread enters the critical section. Once that thread leaves, it increments its _fork_ semaphores and the new semaphore, allowing a new thread to enter.

```cpp
/* Code Listing 8.26:
   Solving dining philosophers with a multiplexing semaphore
 */

void *
philosopher (void * _args)
{
  /* Cast the args as struct with self identifier, semaphores */
  struct args *args = (struct args *) _args;
  int self = args->self;       /* unique thread identifier */
  int next = (self + 1) % SIZE;
  sem_wait (args->can_sit);    /* multiplexing semaphore */
  sem_wait (args->sems[self]); /* pick up left fork */
  sem_wait (args->sems[next]); /* pick up right fork */
  sem_post (args->can_sit);    /* multiplexing semaphore */
  /* Critical section (eating) */
  sem_post (args->sems[next]); /* put down right fork */
  sem_post (args->sems[self]); /* put down left fork */
  /* Do other work and exit thread */
}
```

📜 Example 8.5.2

* * *

At first glance, it may appear that placing the call to `sem_post()` before the critical section could still allow the same problem as before. Specifically, this structure allows N calls to `sem_wait()`, just as the original version did. However, the order of the outcomes is different, as highlighted in [Table 8.2](#tbl8-2). If thread 1 was initially blocked by the multiplexing semaphore, thread 0 is able to call `sem_wait(1)` successfully first. This order of events breaks the circular wait.

Thread 0

Thread 1

Thread 2

Thread 3

Thread 4

sem_wait(0);  
  SUCCESS  
sem_wait(1);  
  **_SUCCESS_**

sem_wait(1);  
  **_BLOCKED_**

sem_wait(2);  
  SUCCESS  
sem_wait(3);  
  BLOCKED

sem_wait(3);  
  SUCCESS  
sem_wait(4);  
  BLOCKED

sem_wait(4);  
  SUCCESS  
sem_wait(3);  
  BLOCKED

Table 8.2: The multiplexing semaphore changes where threads get blocked

///8.5.2. Solution by Breaking Hold-and-wait[¶]
-----------------------------------------------

There are times where the previous approach would not be optimal, particularly if there is a large gap between the two calls to `sem_wait()` in the initial approach. For instance, if we consider the scenario described in [Code Listing 8.25](#cl8-25), the threads might retrieve a large amount of data from their incoming port before locking an outgoing port. The multiplexing approach would reduce the number of threads that can perform this initial work until at least one gets past the second semaphore. Depending on the needs of the specific application, this delay may be undesirable.

[Code Listing 8.27](#cl8-27) outlines a different approach focused on breaking the hold-and-wait criterion. Rather than using `sem_wait()` on the second semaphore, `sem_try_wait()` provides a mechanism to detect the failure without blocking. If the semaphore is successfully decremented, the thread continues as normal. However, if the decrement would cause the thread to block, it posts to the first semaphore and starts over from scratch. In the terms of the dining philosopher scenario, if someone fails to grab their right fork they would put their left fork back down and try again. In the meantime, the philosopher to their left could grab the fork before it is picked back up.

```cpp
/* Code Listing 8.27:
   Solving dining philosophers with a multiplexing semaphore
 */

while (! success)
  {
    sem_wait (args->sems[self]);     /* pick up left fork */
    /* perform some initial work */
    if (sem_try_wait (args->sems[next]) != 0)
      {
        /* undo current progress if needed and possible */
        sem_post (args->sems[self]); /* drop left fork */
      }
    else
      success = true;
  }
```

This approach depends on the work that is done between waiting on the two semaphores. If the initial work cannot be undone, it is not clear what should be done if the `sem_try_wait()` fails. One possibility would be simply to discard the partial results, which may be acceptable in some cases. As an example of where this is true, consider a streaming media player. Partial results can happen when some but not all of the data packets have arrived; the result may be that the player switches to a low-resolution form (creating pixelated images) or switches to audio only.

On the other hand, consider a financial database where the initial work is to withdraw money from one account. After waiting on the second semaphore (if successful), the money would be deposited in a second account. However, if the `sem_try_wait()` fails and the withdraw cannot be undone, the money would be lost. This is clearly not an acceptable result. As such, this approach should be used only in cases where it is clear that the initial work can be undone or discarded safely.

///8.5.3. Solution by Imposing Order[¶]
---------------------------------------

A third possibility for solving the dining philosophers problem is to impose a linear ordering on the semaphores. This order could be imposed by requiring `i < j` anytime `sems[i]` is accessed before `sems[j]`. As before, thread 0 would wait on semaphores 0 and 1 (in that order), thread 1 would wait on semaphores 1 and 2, and so on. However, the last thread would have a different ordering. If there are N semaphores (numbered 0 through N-1), the last thread would have to wait on semaphore 0 before semaphore N-1 to adhere to the linear ordering. [Code Listing 8.28](#cl8-28) shows how this order can be imposed by adding a single `if` statement.

```cpp
/* Code Listing 8.28:
   Enforcing a linear ordering by requiring self < next
 */

void *
philosopher (void * _args)
{
  /* Cast the args as struct with self identifier, semaphores */
  struct args *args = (struct args *) _args;
  int self = args->self;       /* unique thread identifier */
  int next = (self + 1) % SIZE;
  if (self > next) swap (&next, &self); /* enforce order */
    
  sem_wait (args->sems[self]); /* pick up left fork */
  sem_wait (args->sems[next]); /* pick up right fork */
  /* Critical section (eating) */
  sem_post (args->sems[next]); /* put down right fork */
  sem_post (args->sems[self]); /* put down left fork */
  /* Do other work and exit thread */
}
```

📜 Example 8.5.3

* * *

To visualize how this change affects the outcomes to prevent deadlock, consider the highlights in [Table 8.3](#tbl8-3). Since thread 4 must adhere to the linear order, it must try to wait on semaphore 0 before it can wait on semaphore 4. Assuming thread 0 arrived earlier and decremented semaphore 0 successfully as shown, thread 4 becomes blocked from the start. Consequently, thread 3 is successful in decrementing semaphore 4. The linear ordering prevents the circular wait that would cause deadlock.

Thread 0

Thread 1

Thread 2

Thread 3

Thread 4

sem_wait(0);  
  SUCCESS  
sem_wait(1);  
  BLOCKED

sem_wait(1);  
  SUCCESS  
sem_wait(2);  
  BLOCKED

sem_wait(2);  
  SUCCESS  
sem_wait(3);  
  BLOCKED

sem_wait(3);  
  SUCCESS  
sem_wait(4);  
  **_SUCCESS_**

sem_wait(0);  
  **_BLOCKED_**  
sem_wait(4);

Table 8.3: Thread 4 gets blocked by semaphore 0, allowing thread 3 to proceed



//8.6. Cigarette Smokers Problem and the Limits of Semaphores and Locks[¶]
==========================================================================

One key feature of the dining philosopher problem is that all of the resources can be treated interchangeably. One person’s left fork can serve as someone else’s right fork, and vice versa. In the array of semaphores, the threads could theoretically wait on the semaphores in a random order and still achieve the circular wait if circumstances align properly. Consequently, it is tempting to think that circular wait cannot happen if different types of resources are used. For instance, is it possible to have deadlock if the resources consist of a single semaphore, a single lock, and a single condition variable?

The answer to this question is yes; using different types of synchronization primitives that satisfy the three system properties (mutual exclusion, hold and wait, no preemption) does not provide safety against deadlock. The [cigarette smokers problem](#term-cigarette-smokers-problem) [[1]](#f46) helps to illustrate this point, but it actually proves a stronger point as well. While the dining philosophers problem could be solved by using an additional semaphore, the cigarette smokers problem highlights a scenario that is provably impossible to solve with semaphores alone.

The scenario for the cigarette smokers problem consists of four threads: three smokers and one agent. In order to smoke, the smoker needs to acquire three items: tobacco, paper, and a match. Once the smoker has all three, they combine the paper and tobacco to roll a cigarette and use the match to light it. Each of the three smokers has an infinite supply of exactly one item and needs the other two. [Code Listing 8.29](#cl8-29) shows a sample outline for the smoker threads. The smoker shown there is assumed to have an infinite supply of tobacco but needs the match and paper. The smoker sends a signal to request more paper and matches when they are finished smoking. The other two threads are similar, but one would wait on semaphores for a match and tobacco and the third would wait on tobacco and paper.

```cpp
/* Code Listing 8.29:
   Sample structure for a smoker thread
 */

void *
smoker_with_tobacco (void)
{
  while (true)
    {
      sem_wait (match_sem); /* grab match from table */
      sem_wait (paper_sem); /* grab paper from table */
      /* roll cigarette and smoke */
      sem_post (more_needed); /* signal to agent */
    }
}
```

The fourth thread is an agent that creates two of the three items at a time, placing the items on the table. Once the items are taken away, the agent waits on a signal to produce more. [Code Listing 8.30](#cl8-30) shows an outline of an agent thread. The agent picks randomly between three cases, producing some combination of two out of the three items. The agent places the items on the table, then waits for a request for more items.

```cpp
/* Code Listing 8.30:
   Sample structure for the agent thread
 */

void *
agent (void)
{
  while (true)
    {
      int number = rand() % 3;
      switch (number)
        {
           case 0: sem_post (match_sem); /* match and paper */
                   sem_post (paper_sem);
                   break;
           case 1: sem_post (match_sem); /* match and tobacco */
                   sem_post (tobacco_sem);
                   break;
           case 2: sem_post (paper_sem); /* tobacco and paper */
                   sem_post (tobacco_sem);
                   break;
        }
      sem_wait (more_needed); /* wait for request for more */
    }
}
```

To understand the implications of the cigarette smokers problem, it is important to emphasize that [Code Listings 8.29](#cl8-29) and [8.30](#cl8-30) are just sample implementations. The `smoker_with_tobacco()` could be changed to wait on the `paper_sem` before the `match_sem`; the order does not matter. Similarly, the `agent()` could be written so that case 0 would post to `paper_sem` before `match_sem`. The result does not rely on any particular ordering of these lines of code.

In addition to the description above, there is one constraint placed on the agent thread. Once it is written, it cannot change. The problem is a metaphor for working with an OS. The agent is the OS, providing a service of some sort; it is not aware of the design of applications that will be running on the system. This restriction, then, makes sense. You would not expect your OS to behave differently whenever you install or update the applications running on it.

📜 Example 8.6.1

* * *

Based on this restriction of the agent thread, the cigarette smokers problem states that it is _impossible_ to create the smoker threads that would avoid deadlock. That is, regardless of how you order the calls to `sem_wait()` in the smoker threads, the possibility of deadlock cannot be avoided. For instance, consider the outcomes in [Table 8.4](#tbl8-4) if the agent produces a match and tobacco. If these items are grabbed by two threads, no smoker can progress.

Smoker with Tobacco

Smoker with Paper

Smoker with Match

sem_wait(match_sem);  
  SUCCESS  
sem_wait(paper_sem);  
  BLOCKED

sem_wait(tobacco_sem);  
  SUCCESS  
sem_wait(match_sem);  
  BLOCKED

sem_wait(paper_sem);  
  BLOCKED  
sem_wait(tobacco_sem);  
  BLOCKED

Table 8.4: Possible scenario when agent places match and tobacco on the table

It is tempting to claim that there is a simple fix: rewrite the `smoker_with_tobacco()` so that it must wait on the `paper_sem` before the `match_sem`. Switching this order would require both `smoker_with_tobacco()` and `smoker_with_match()` to wait on `paper_sem` first. Since the agent had only incremented the `tobacco_sem` and `match_sem`, the `smoker_with_paper()` could proceed without being blocked. While that would be the case in this situation, consider the next round where the agent places a different combination on the table. If that combination included paper and tobacco, the `smoker_with_tobacco()` could, once again, push the system into deadlock as in [Table 8.5](#tbl8-5).

Smoker with Tobacco

Smoker with Paper

Smoker with Match

sem_wait(paper_sem);  
  SUCCESS  
sem_wait(match_sem);  
  BLOCKED

sem_wait(tobacco_sem);  
  SUCCESS  
sem_wait(match_sem);  
  BLOCKED

sem_wait(paper_sem);  
  BLOCKED  
sem_wait(tobacco_sem);  
  BLOCKED

Table 8.5: Imposing a linear ordering does not fix the deadlock

///8.6.1. Implications of the Cigarette Smokers Problem[¶]
----------------------------------------------------------

The key claim of the cigarette smokers problem is that this scenario has no solution for traditional semaphores, as they existed at the time. When this problem was initially proposed, semaphores only provided operations for incrementing or decrementing their internal value by one. The problem proves that, if we are limited to those operations only, there are situations in which avoiding deadlock is provably impossible. Regardless of how the agent and the smoker threads are constructed, once the agent’s structure is fixed, any construction of the smokers will create a possible deadlock situation.

As a result, the cigarette smokers problem proves that some form of introspection is needed for synchronization primitives. This introspection is provided by POSIX operations such as `sem_try_wait()` or `pthread_mutex_trylock()`. These operations provide information about whether or not the synchronization attempt will succeed. [Code Listing 8.31](#cl8-31) illustrates how these forms could be used in a successful solution to the cigarette smokers problem.

```cpp
/* Code Listing 8.31:
   Sample structure for a smoker thread
 */

void *
smoker_with_tobacco (void)
{
  while (true)
    {
      sem_wait (match_sem); /* grab match from table */
      if (sem_try_wait (paper_sem) == 0) /* grab paper */
        {
          /* roll cigarette and smoke */
          sem_post (more_needed); /* signal to agent */
        }
      else sem_post (match_sem); /* drop the match */
    }
}
```

///8.6.2. On Cigarette Smokers and Dining Philosophers[¶]
---------------------------------------------------------

The last solution proposed to the dining philosophers problem worked by imposing a linear ordering on the semaphores. Examining the order of calls to `sem_wait()` in [Table 8.5](#tbl8-5), we can observe that this approach does not always succeed in avoiding deadlock. Specifically, [Table 8.5](#tbl8-5) is built on a solution that imposes an order of `paper_sem` before `tobacco_sem` and `tobacco_sem` before `match_sem`. By transitivity, this requires the `paper_sem` before `match_sem`, as implemented in the `smoker_with_tobacco()`.

It is fair to ask, then, why the linear ordering works for dining philosophers but fails for cigarette smokers. The key difference has to do with the sum of the semaphores’ internal values. In the dining philosophers problem, all semaphores are initialized to one; consequently, if there are N semaphores, their sum is also N. In contrast, the cigarette smokers problem initializes all semaphores to zero. The agent then posts to two out of the three semaphores. The semaphores are then both decremented and the cycle repeats. As a result, the sum of the semaphores never reaches three.

Based on these comparisons, we could generalize the cigarette smokers problem to more than three threads. In this generalized form, there would be N smokers and the agent would place only N-1 items on the table. If every thread requires two resources (decrementing two semaphores, acquiring two locks, etc.), then a linear ordering will not prevent deadlock. The total number of available resources must be at least the total number of possible requests that can be made. If there are N threads that can all issue concurrent requests, there must be N instances available for the linear ordering to prevent deadlock.

[[1]](#id1)

The name of this problem illustrates how cultural norms can change over time. The cigarette smokers problem was first described in a paper in 1971, when smoking was considered much more socially acceptable than now and rolling one’s own cigarettes was a commonly known practice.



//8.7. Extended Example: Parallel Modular Exponentiation[¶]
===========================================================

Modular exponentiation is a mathematical calculation that is used in a variety of applications, including public key cryptography. This operation simply consists of performing an integer exponentiation and applying a modulus. For instance, 23 mod 7 = 8 mod 7 ≡ 1 mod 7. This parallel form makes use of the fact that multiplication is associative; if we want to compute (23 * 54 ) mod 7, we can calculate 23 mod 7 and 54 mod 7 in parallel, then multiply their results together. This form uses an intentionally slow implementation of modular exponentiation to illustrate the performance improvement from parallelism.

```cpp
#include <assert.h>
#include <inttypes.h>
#include <pthread.h>
#include <semaphore.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdint.h>
#include <stdlib.h>
#include <string.h>

uint64_t mod_power (uint64_t, uint64_t);
void * mod_power_consumer (void *);
uint64_t * randomize_powers (size_t, unsigned, double, size_t);

/* Can use this to change the queue size */
#define QUEUE_SIZE 10

/* Synchronization primitives and global result */
pthread_mutex_t mutex;
#define SEM_AVAILABLE "/OpenCSF_Available"
#define SEM_READY "/OpenCSF_Ready"
sem_t *space_available;
sem_t *item_ready;
uint64_t result = 1;

/* Each entry is for a single base^power computation */
struct queue_entry {
  uint64_t base;
  uint64_t power;
};

/* Variables that define the queue structure */
struct queue_entry queue[QUEUE_SIZE];
size_t next_out = 0;
size_t next_in = 0;

/* Set of 100 primes and the modulus to use for calculations */
uint64_t primes[] = {
  2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53,
  59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113,
  127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181,
  191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251,
  257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317,
  331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397,
  401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463,
  467, 479, 487, 491, 499, 503, 509, 521, 523, 541
};
size_t primes_length = 100;
uint64_t modulus = 524287;

int
main (void)
{
  /* Change these numbers as desired. This set will create 4
     threads to multiple 50% of the primes, each to a power of
     at least 10,000. */
  size_t number_of_threads = 4;
  unsigned seed = 25;
  double threshold = 0.5;
  size_t minimum = 10000000;
  
  /* Generate an array of random powers to use */
  uint64_t *powers =
    randomize_powers (primes_length, seed, threshold, minimum);

  /* Initialize synchronization primitives as needed */
  pthread_mutex_init (&mutex, NULL);
  sem_unlink (SEM_AVAILABLE); // Delete an old semaphore instance
  sem_unlink (SEM_READY);     // Delete an old semaphore instance
  space_available =
    sem_open (SEM_AVAILABLE, O_CREAT | O_EXCL, S_IRUSR | S_IWUSR, QUEUE_SIZE);
  item_ready = sem_open (SEM_READY, O_CREAT | O_EXCL, S_IRUSR | S_IWUSR, 0);

  /* Create the pool of worker threads; each will  */
  pthread_t threads[number_of_threads];
  memset (threads, 0, sizeof (threads));

  size_t i;
  for (i = 0; i < number_of_threads; i++)
    pthread_create (&threads[i], NULL, mod_power_consumer, NULL);

  /* Producer side of the producer/consumer. Select a pair
     of prime/power and add the entry to the queue. */
  for (i = 0; i < primes_length; i++)
    {
      struct queue_entry entry;
      entry.base = primes[i];
      entry.power = powers[i];

      /* Wait until space is ready */
      sem_wait (space_available);
      queue[next_in] = entry;
      next_in = (next_in + 1) % QUEUE_SIZE;

      /* Signal that an item was added */
      sem_post (item_ready);
    }

  /* No more powers needed from the array */
  free (powers);
  powers = NULL;

  /* Now start filling the queue with -1 values to terminate
     the helper threads */
  for (i = 0; i < number_of_threads; i++)
    {
      struct queue_entry blank;
      blank.base = -1;
      blank.power = -1;
      
      /* Wait for space */
      sem_wait (space_available);
      queue[next_in] = blank;
      next_in = (next_in + 1) % QUEUE_SIZE;

      /* Signal that an item was added */
      sem_post (item_ready);
    }

  /* Join all of the threads when finished */
  for (i = 0; i < number_of_threads; i++)
    pthread_join (threads[i], NULL);

  /* Clean up all synchronization primitives */
  pthread_mutex_destroy (&mutex);
  sem_close (item_ready);
  sem_close (space_available);
  sem_unlink (SEM_AVAILABLE);
  sem_unlink (SEM_READY);

  printf ("Product is %" PRIu64 "\n", result);

  return EXIT_SUCCESS;
}

/* Consumer thread. Each thread runs until it receives a
   power of -1 (indicating finished). After a base/power
   pair is pulled from the queue, the thread will compute
   base^power mod modulus, then multiple that by the global
   result product. */
void *
mod_power_consumer (void * _args)
{
  struct queue_entry entry;
  while (true)
    {
      /* Consumer side waits until an item is ready */
      sem_wait (item_ready);

      /* Multiple consumers, so lock the queue */
      pthread_mutex_lock (&mutex);
      entry = queue[next_out];
      next_out += 1;
      next_out %= QUEUE_SIZE;
      pthread_mutex_unlock (&mutex);

      /* Let producer no there's at least one space */
      sem_post (space_available);

      /* Check for termination signal */
      if (entry.power == -1) break;

      /* Computer base^power, then multiple result with
         the global product */
      uint64_t modp = mod_power (entry.base, entry.power);
      pthread_mutex_lock (&mutex);
      if (entry.power > 1)
        printf ("%" PRIu64 " ^ %" PRIu64 " mod %" PRIu64 " = %" PRIu64 "\n",
                entry.base, entry.power, modulus, modp);
      result *= modp;
      result %= modulus;
      pthread_mutex_unlock (&mutex);
    }

  pthread_exit (NULL);
}

/* Intentionally slow mod power routine. Computes
   base^power mod modulus by multiplying a running
   product repeatedly by the base. Slow enough to
   see a speedup from parallel executions. */
uint64_t
mod_power (uint64_t base, uint64_t power)
{
  uint64_t index = 0;
  uint64_t product = 1;

  while (index < power)
    {
      product *= base;
      product %= modulus;
      index++;
    }
  return product;
}

/* Create an array of random powers for the primes.
   The threshold parameter determines (approximately)
   what percentage of these powers are non-zero. */
uint64_t *
randomize_powers (size_t length, unsigned seed,
                  double threshold, size_t minimum)
{
  size_t i;
  uint64_t *pows = calloc (length, sizeof (uint64_t));
  srand (seed);
  size_t randoms = length * threshold;

  for (i = 0; i < randoms; i++)
    {
      size_t index = rand () % length;
      pows[index] = (rand () % 10) * minimum;
      pows[index] += rand () % minimum;
    }
  return pows;
}
```


/Chapter 9   Parallel and Distributed Systems[¶]
================================================

//9.1. Parallel and Distributed Systems[¶]
==========================================

![Timeline of major CSF topics with Parallelism and Distributed Systems highlighted](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Timeline.9.png)

> “A distributed system is one in which the failure of a computer you didn’t even know existed can render your own computer unusable.”
> 
> Leslie Lamport

As with multithreading, the general concepts of distributed computing are decades old. The Internet, for instance, can be characterized as a distributed computing system. Leslie Lamport introduced distributed timestamps in 1978. However, the past two decades have brought these techniques to new heights with massively parallel supercomputers (such as Cray Titan or IBM BlueGene) and grid computing (Berkeley Open Infrastructure for Network Computing (BOINC), Folding@home, Great Internet Mersenne Prime Search (GIMPS)). Furthermore, the domains of parallel and distributed computing remain key areas of computer science research. This chapter provides an introduction to key results and concepts in these areas, providing a foundation for interested readers to pursue more advanced study.

🚀 Chapter Objectives

* * *

In this chapter, we will address the following instructional objectives:

*   We will distinguish the notions of concurrency and parallelism and examine the relationship of both techniques to the hardware capabilities.
*   We will explore parallel design patterns that can be applied toward the construction of algorithms, program implementations, or program execution.
*   We will examine the theoretical and practical limits to parallelism.
*   We will consider three foundational concepts—timing of events, object location, and consensus—of distributed systems that have been very influential in this field.

The principles of concurrency and synchronization can apply to a wide variety of systems. In particular, throughout most of the history of computing, most systems implemented multiprogramming in the software (the operating system) to improve utilization on a processor that was capability of one task at a time. In recent years, there has been an increasing focus on and adoption of hardware capable of parallel execution units. The ability to exploit parallelism on modern hardware makes it possible to achieve significant gains in the speed of computing.

The benefits of parallelism, however, are not automatic and unlimited. Creating software that can take full advantage of hardware support for parallelism requires careful design and implementation. Some algorithms simply cannot be executed in parallel; for algorithms that can be parallelized, the code must be properly structured to account for the nondeterministic nature of the timing that can arise. Furthermore, even if the algorithm can be parallelized, there are theoretical and practical limits to how much the speed can be improved. In this chapter, we will explore these issues, strategies, and limits of parallelism.

In addition to parallelism on a single computer, we can link a larger collection of [distributed systems](#term-distributed-system) of independent computers. The goal of distributed systems is to link many computers together, executing coordinated software to accomplish a single, larger goal. Distributed computing involves confronting the challenge of unexpected and intermittent failures of nodes in the system. As an introduction to this area of computing, we will explore how computer scientists have addressed three well-known challenges in distributed computing: communicating the timing of events, achieving an agreement — known as [consensus](#term-consensus-protocol) — on the state of the system, and storing data so that it can still be accessed when nodes fail. Readers who find these topics of interest should explore the recommended readings at the end of the chapter.



//9.2. Parallelism vs. Concurrency[¶]
=====================================

As a starting point, it is important to emphasize that the terms [concurrency](#term-concurrency) and [parallelism](#term-parallelism) are often used as synonyms, but there is a distinction. Both terms generally refer to the execution of multiple tasks within the same time frame. However, concurrency does not necessarily mean that the tasks are simultaneously running at any given moment. Instead, concurrency can be achieved on a single-core processor through the use of [multiprogramming](#term-multiprogramming). In multiprogramming, the OS rapidly switches back and forth between multiple programs that have been loaded into memory. We can say that blocks of the two programs’ instructions are [interleaved](#term-interleaved), meaning that the processor alternates which program is running. Because this happens so quickly, the concurrency provided by multiprogramming creates the illusion of parallel execution. The user believes the two programs are running at the same time, but they actually are not. Within a single program, concurrency is focused on the logical structure of the tasks involved. For example, a program with a graphical user interface employs concurrency by creating separate threads for handling keyboard input, auto-saving backup copies of modified files, responding to mouse clicks or screen touches, and so on.

In contrast, parallelism means that the multiple tasks are simultaneously executing. The goal in parallelism is focused more on improving the [throughput](#term-throughput) (the amount of work done in a given amount of time) and [latency](#term-latency) (the time until completion of a task) of the system. In essence, parallelism is focused on trying to do more work faster. Parallel execution implies that there is concurrency, but not the other way around. As a starting point for parallel programming, we often talk about identifying opportunities for concurrency, then apply techniques to parallelize the concurrent tasks.

///9.2.1. Multiprocessing Systems[¶]
------------------------------------

![A typical cache and core arrangement for a dual-core system](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.9.1.png)

Figure 9.2.1: A typical cache and core arrangement for a dual-core system

Parallelism requires hardware that is capable of [multiprocessing](#term-multiprocessing), which is the ability to execute multiple processes simultaneously. In the case of a [multicore](#term-multicore) system (which would include a typical modern laptop computer), the CPU has multiple distinct physical _processing cores_ that are all capable of executing instructions. That is, each core acts as a miniature version of a CPU, with its own instruction control unit, ALU, and cache memory. In a multicore system, the OS can arrange to execute the two programs simultaneously on separate cores. [Figure 9.2.1](#9_2_dualcore) shows the logical structure of a typical dual-core system, with three levels of caching. In this scenario, the L2 and L3 cache levels are _unified_, storing both data and instructions; the L1 caches are _banked_, with one cache per core devoted to instructions and another devoted to data.

Another form is [symmetric multiprocessing (SMP)](#term-symmetric-multiprocessing), which is the class of systems that contain multiple CPUs interconnected within a single machine. As all CPUs in an SMP system share the same memory, the hardware design tends to be complex, making the system expensive to build and maintain. Cluster systems, on the other hand, use multiple machines closely connected on a network. [Cluster](#term-cluster-system) systems tend to use standard hardware, making it possible to drastically increase the number of processing units relative to a similarly priced SMP system. The tradeoff is that communication between nodes in a cluster is slower than SMP, because the communication takes place over a network rather than through shared memory.

The power of multiprocessing is not restricted to kernel designers. Rather, many types of systems and application software can be built to leverage these platforms for complex and efficient software. While concurrent program can be challenging, there are many common parallel design patterns that provide effective strategies for exploiting parallelism.



//9.3. Parallel Design Patterns[¶]
==================================

There are multiple levels of parallel design patterns that can be applied to a program. At the highest level, _algorithmic strategy patterns_ are strategies for decomposing a problem in its most abstract form. Next, _implementation strategy patterns_ are practical techniques for implementing parallel execution in the source code. At the lowest level, _parallel execution patterns_ dictate how the software is run on specific parallel hardware architectures.

///9.3.1. Algorithmic Strategy Patterns[¶]
------------------------------------------

The first step in designing parallel processing software is to identify opportunities for concurrency within your program. The two fundamental approaches for parallel algorithms are identifying possibilities for [task parallelism](#term-task-parallelism) and [data parallelism](#term-data-parallelism). Task parallelism refers to decomposing the problem into multiple sub-tasks, all of which can be separated and run in parallel. Data parallelism, on the other hand, refers to performing the same operation on several different pieces of data concurrently. Task parallelism is sometimes referred to as _functional decomposition_, whereas data parallel ism is also known as _domain decomposition_.

A common example of task parallelism is input event handling: One task is responsible for detecting and processing keyboard presses, while another task is responsible for handling mouse clicks. [Code Listing 9.1](#cl9-1) illustrates an easy opportunity for data parallelism. Since each array element is modified independently of the rest of the array, it is possible to set every array element’s value at the same time. The previous examples are instances of [embarrassingly parallel problems](#term-embarrassingly-parallel) [[1]](#f47), which require little or no effort to parallelize, and they can easily be classified as task or data parallelism.

```cpp
/* Code Listing 9.1:
   An embarrassingly parallel loop, as each array element is initialized independently
   of all other elements.
 */

for (i = 0; i < 1000000000; i++)
  array[i] = i * i;
```

There are other ways to classify algorithms that exploit parallelism. One common approach is a [recursive splitting](#term-recursive-splitting) or [divide-and-conquer](#term-divide-and-conquer-algorithm) strategy. In a divide-and-conquer strategy, a complex task is broken down into concurrent sub-tasks. One example of this strategy is the quicksort algorithm: The array is first partitioned into two sub-arrays based on a pivot value; the sub-arrays can then be sorted recursively in parallel.

_Merge sort_ is a common example of an algorithm that is both embarrassingly parallel and a divide-and-conquer approach. Consider the basic outline of the algorithm, as shown in [Code Listing 9.2](#cl9-2). Merge sort begins by recursively splitting an array into two halves [[2]](#f48). The left and right halves are sorted independently, then their resulting sorted versions are merged. This algorithm is considered embarrassingly parallel, because sorting the left and right halves in parallel are naturally independent tasks. It is also considered divide-and-conquer because it takes the larger problem and breaks it down into smaller tasks that can be parallelized. Not all divide-and-conquer algorithms are embarrassingly parallel, and vice versa; however, there is a significant amount of overlap between these classifications.

```cpp
/* Code Listing 9.2:
   Merge sort is an embarrassingly parallel problem, given its natural
   divide-and-conquer structure
 */

void
mergesort (int * values, int start, int end)
{
  if (start >= end)
    return;
  int mid = (start + end) / 2;
  mergesort (values, start, mid);   /* sort the left half */
  mergesort (values, mid + 1, end); /* sort the right half */
  merge (values, start, end);
}
```

Another common strategy is [pipelining](#term-pipelining). In pipelining, a complex task is broken down into a sequence of independent sub-tasks, typically referred to as stages. There are multiple real-world scenarios that help to demonstrate the key ideas that underlie pipelining. On example is to think about doing laundry. Once a load of clothes has finished washing, you can put them into the dryer. At the same time, you start another load of clothes in the washing machine. Another example is to consider the line at a cafeteria-style restaurant where you make multiple selections. The line is often structured so that you select (in order) a salad, an entrée, side dishes, a dessert, and a drink. At any time, there can be customers in every stage of the line; it is not necessary to wait for each customer to pass through all parts of the line before making your first selection.

The canonical example of pipelining is the five-stage RISC processor architecture. Executing a single instruction involves passing through the fetch (IF), decode (ID), execute (EX), memory access (MEM), and write-back (WB) stages. The stages are designed so that five instructions can be executing simultaneously in a staggered pattern. From the software perspective, command-line programs are commonly linked together to run in parallel as a pipeline. Consider the following example:

$ cat data.csv | cut -d’,’ -f1,2,3 | uniq

In this scenario, a comma-separated value (CSV) file is read and printed to `STDOUT` by the `cat` program. As this is happening, the `cut` program parses these lines of data from its `STDIN` and prints the first three fields to its `STDOUT`. The `uniq` program eliminates any duplicate lines. These three processes can be run in parallel to a certain extent. The call to `cut` can begin processing some of the data before `cat` has managed to read the entire file. Similarly, `uniq` can start to eliminate lines from the first part of the file while the other two processes are still working. The key to making this successful is that `cut` and `uniq` continue to run as long as they are still receiving data from `STDIN`.

///9.3.2. Implementation Strategy Patterns[¶]
---------------------------------------------

Once you have identified the overall algorithmic strategy for parallel execution, the next step is to identify techniques for implementing the algorithm in software. There are several well-established approaches for doing so. One of the most common is the [fork/join pattern](#term-fork-join-pattern), illustrated in [Figure 9.3.1](ParallelDesign.html#forkjoin). In this pattern, the program begins as a single main thread. Once a parallel task is encountered, additional threads are created and executed in parallel. All threads must complete and be destroyed before the main thread can continue the next portion of the code. This pattern is very common with data parallelism, as well as with [loop parallelism](#term-loop-parallelism), where the code contains loops that are computationally expensive but independent. [Code Listing 9.1](#cl9-1) was an example of loop parallelism.

![Illustration of sequential tasks (above) and the corresponding fork/join parallel implementation (below). Image source: Wikipedia (recreated)](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.9.2.png)

Figure 9.3.1: Illustration of sequential tasks (above) and the corresponding fork/join parallel implementation (below). Image source: Wikipedia (recreated)

Implementing the fork/join pattern in practice can be straightforward, particularly in the cases of embarrassingly parallel problems. The fork stage consists of setting up the arguments that each thread should receive. [Code Listing 9.3](#cl9-3), for example, shows how to break the loop from [Code Listing 9.1](#cl9-1) into 10 threads that each process 1/10th of the array calculations (encapsulated in a `multiply()` function). The join stage would combine their results after calling `pthread_join()`.

```cpp
/* Code Listing 9.3:
   The fork stage of a fork/join pattern
 */

for (i = 0; i < 10; i++) /* Assume we are creating 10 threads */
  {
    args[i].array = array;
    args[i].start = i * 100000000;
    assert (pthread_create (&threads[i], NULL,
                            multiply, &args[i]) != 0);
  }
```

The fork/join pattern is so common, particularly for loop parallelism, that many libraries provide simple mechanisms to automate the thread management for it when programming. [Code Listing 9.4](#cl9-4) shows the OpenMP version of parallelizing [Code Listing 9.1](#cl9-1). When the compiler encounters this `pragma`, it will inject code that handles the thread creation and cleanup with no additional work by the programmer. This pragma makes implementing the fork/join pattern trivial in this and many cases.

```cpp
/* Code Listing 9.4:
   OpenMP works very well with embarrassingly parallel fork/join patterns
 */

#pragma omp parallel for
for (i = 0; i < 1000000000; i++)
  array[i] = i * i;
```

![Map/reduce shares a common structure as fork/join](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.9.3.png)

Figure 9.3.2: Map/reduce shares a common structure as fork/join

[Map/reduce](#term-map-reduce-pattern), shown in [Figure 9.3.2](ParallelDesign.html#mapreduce), is another strategy that is closely related to fork/join. As in fork/join, a collection of input data is processed in parallel by multiple threads. The results are then merged and collected as the threads are joined until a single answer is reached. Although they are structurally identical, the type of work being done reflects a somewhat different philosophy. The idea behind the map stage is based on a technique in _functional programming_ languages. A single function is mapped to all inputs to yield new results; these functions are self-contained and free of side effects, such as producing output. Several map stages can be chained together to compose larger functions. The map and reduce stages are also more independent than standard fork/join; mapping can be done without a reduction, and vice versa. Map/reduce is a popular feature in cluster systems, such as the Apache Hadoop system.

An implementation strategy common with task parallelism is the [manager/worker](#term-manager-worker) pattern. In this scenario, independent tasks are distributed among several worker threads that communicate only with the single manager thread. The manager can monitor the distribution of the tasks to balance the workload evenly. To return to an earlier example, input event handlers are often implemented using the manager/worker pattern. In this case, the key press handler and the mouse event handler would each be implemented in separate workers; when either event occurs, the worker would inform the manager thread that would then process the data within the context of the program.

[Code Listing 9.5](#cl9-5) shows one way to structure a worker thread. The thread arguments contain a pointer to a piece of data to process (`args->data`) along with a lock (`args->lock`) and a condition variable (`args->data_received`). When the manager thread has a task to assign this particular thread, it would update the data pointer and send a signal with the condition variable. If each worker thread has its own data pointer, the manager can send data to a specific worker thread. On the other hand, if the data pointer is shared, this structure could still work; the main difference is that the thread would need to make a local copy of the data just before releasing the lock on line 17. Since condition variables also support broadcasting, the manager can send data to all of the worker threads with a single message. In addition, the thread arguments contain a boolean value `args->running`. The manager thread can stop all of the workers by setting this value to false and broadcasting on the condition variable (setting all `args->data` values to anything other than `NULL`).

```cpp
/* Code Listing 9.5:
   A simple worker with condition variables
 */

void *
worker (void * _args)
{
  struct args *args = (struct args *) _args;
  while (true)
    {
      /* Wait for the next available data */
      pthread_mutex_lock (args->lock);
      while (args->data == NULL)
        pthread_cond_wait (args->data_received, args->lock);
      if (! args->running)
        break;
      pthread_mutex_unlock (args->lock);
      /* Do something with the data here */
    }
  pthread_mutex_unlock (args->lock);
  pthread_exit (NULL);
}
```

///9.3.3. Parallel Execution Patterns[¶]
----------------------------------------

![A thread pool retrieves tasks from the associated queue and returns completed results](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.9.4.png)

Figure 9.3.3: A thread pool retrieves tasks from the associated queue and returns completed results

Once the implementation strategy has been established, the software designer needs to make decisions about how the parallel software will run given the underlying hardware support available. One technique is to create a [thread pool](#term-thread-pool) with an associated [task queue](#term-task-queue). A thread pool is a fixed number of threads that are available for the program to use. As parallel tasks arrive, they are placed into a task queue. If a thread in the pool is available, it will remove a task from the queue and execute it. [Figure 9.3.3](ParallelDesign.html#threadpool) shows the logical structure of this approach.

At first glance, thread pools may look identical to the manager/worker implementation pattern described above. The difference is that the manager/worker pattern describes _what_ is to be done, whereas the thread pool structure describes _how_ it will be done. The master/worker pattern is in contrast to the fork/join pattern. Master/worker employs task parallelism, with different workers may be performing different tasks; fork/join employs data parallelism, with identical threads performing the same task on different data. A thread pool can be used for both approaches.

The main idea of a thread pool is to create all of the threads needed once at the beginning of the program, rather than when needed. [Code Listing 9.3](#cl9-3), for instance, did not use a thread pool to parallelize the loop. This could be contrasted with [Code Listing 9.6](#cl9-6), which assumes the presence of a thread pool. In this approach, the for-loop employs the producer-consumer `enqueue()` operation from Chapter 8 to place the starting values into the shared queue. The `space` and `items` semaphores help to ensure that the queue is modified safely. After enqueueing all of the data, the main thread waits at a barrier until the pool threads have reached the end of their calculations. The barrier prevents the main thread from moving past the fork/join structure until all of the pool threads have reached the same point.

```cpp
/* Code Listing 9.6:
   Using a thread pool to parallelize Code Listing 9.1
 */

/* Initialize barrier for this thread + 10 from the thread pool */
pthread_barrier_init (&barrier, NULL, 11);

/* Use the producer-consumer enqueue from Code Listing 8.15 */
for (i = 0; i < 10; i++)
  enqueue (queue, i * 100000000, space, items);

pthread_barrier_wait (barrier);
```

[Code Listing 9.7](#cl9-7) shows the structure of a thread in the thread pool. (For simplicity and focus on the thread pool, we are assuming all semaphores, the queue, the lock, and the barrier are globally accessible.) This thread starts by retrieving a piece of data from the shared queue, using the producer-consumer `dequeue()` operation. Note that, since there are multiple threads in the pool, this thread needs the `dequeue()` operation that employs a lock. This version synchronizes the pool threads’ access to the variables that maintain the queue structure. After retrieving the starting value (which was passed through the queue as a pointer), the thread performs the desired work and waits at the barrier (indicating completion to the main thread).

```cpp
/* Code Listing 9.7:
   The threads in the pool for parallelizing Code Listing 9.1
 */

void *
pool_thread (struct args *args)
{
  /* ... Declarations and other work omitted for brevity ... */
  /* Use the dequeue from 8.18, given multiple consumers */
  int starting = (int)dequeue (queue, space, items, lock);
  for (i = starting; i < starting + 100000000; i++)
    array[i] = i * i;
  pthread_barrier_wait (barrier);
  /* ... Additional work may happen later ... */
}
```

One way to characterize the difference between the thread pool approach in [Code Listing 9.6](#cl9-6) with the basic fork/join style of [Code Listing 9.3](#cl9-3) is to distinguish them as either a _pull_ or a _push_ model. Thread pools are typically implemented using a pull model, where the pool threads manage themselves, retrieving data from the queue at their availability and discretion. In contrast, the original approach used a push model, with the main thread controlling which thread accomplished which task.

The thread pool approach has several advantages. First, it minimizes the cost of creating new threads, as they are only created once when the program begins. Many naive multithreaded implementations fail to realize the benefits of parallelism because of the overhead cost of creating and managing the threads. Second, thread pools make the resource consumption more predictable. A simultaneous request to create a large number of threads could cause a spike in memory consumption that could cause a variety of problems in the system. Third, as just noted previously, thread pools allow the threads to be more self-managing based on local performance characteristics. If one thread is running on a core that is overloaded with other work, that thread can naturally commit less work to this task.

The self-management of thread pools can also be a disadvantage, as well. If there is no coordination between the threads and they are running on different processor cores, the cache performance may suffer, as data may need to be shuffled around frequently. Similarly, if there is no logging of which core takes which task or data set, responding to hardware failures might be difficult and data could be lost. Finally, the shared queue must be managed, which could induce performance delays in the synchronization that could otherwise be avoided.

![The four paradigms of Flynn's taxonomy, showing how single/multiple instructions and single/multiple data are linked to processing units. Image source: Wikipedia (recreated)](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.9.5.png)

Figure 9.3.4: The four paradigms of Flynn’s taxonomy, showing how single/multiple instructions and single/multiple data are linked to processing units. Image source: Wikipedia (recreated)

Another factor that influences the execution of parallel systems is the capabilities of the hardware. [Flynn’s taxonomy](#term-flynn-s-taxonomy) describes four paradigms of parallel hardware architectures in terms of the decompositions they support. [Figure 9.3.4](ParallelDesign.html#flynn) illustrates the logical organization of each of the four paradigms. In each structure, a processing unit (“PU”) is provided with one or more parallel instructions (SI for “single instruction” and MI for “multiple instruction”) and performs the desired computation on a single input (SD) or multiple pieces of data (MD).

Traditional uniprocessing software adheres to the [SISD](#term-sisd) model, as a single processor executes a single instruction on a single piece of data at a time; as such, SISD does not support parallel execution but is included in the taxonomy for completeness. Of the parallel models, [SIMD](#term-simd) is perhaps the most intuitive and the one with which most readers would be familiar. Modern CPUs provide SIMD support through various extensions to the ISA, such as the streaming SIMD extensions (SSE) or advanced vector extensions (AVX) for Intel processors, as well as the Neon or Helium instruction sets for ARM processors. _Graphics processing units (GPUs)_ provide native support for SIMD operations, such as manipulating the rows of pixels in an image in parallel. Given this native support, GPUs are also widely used for applications that perform independent calculations in parallel. For instance, many scientific or security applications involve brute-force searches of a large set of data; GPU SIMD instructions can facilitate parallelizing the computations needed for these applications.

The [MISD](#term-misd) model is often confusing when first encountered, and many people do not immediately perceive its value as parallelism. One common use of MISD would be to provide fault tolerance in programs that require precision. The multiple instructions are all executed in parallel on the same input data; the results of the computations can then be evaluated to confirm that no errors occurred. Systolic array architectures, which are specialized systems for parallelizing adanced mathematical operations, can also be classified as MISD. For instance, specialized designs can optimize the parallel calculation of the multiplication and addition operations found in matrix multiplication. MISD hardware implementations are not common and are typically only found in custom hardware designs.

[MIMD](#term-mimd) architectures allow the processors to work independently, performing different instructions on different pieces or sets of data at the same time. SPMD (single program, multiple data) is a common subset of MIMD in distributed computing; in SPMD, a single program with multiple instructions can be deployed independently to run in parallel. The key distinction between SPMD and SIMD is that SIMD instructions are synchronized. In a SIMD system, all processors are executing the same instruction at the same time according to the same clock cycle; SPMD architectures provide more autonomy, with each processor executing instructions independently of the rest of the system.

Large-scale parallel architectures, particularly MIMD, typically rely on a memory architecture that complicates their software development. In traditional SISD computing models (e.g., personal computers and laptops), all memory accesses are essentially equal; accessing a global variable near address 0x0804a000 takes the same amount of time as accessing a stack variable near 0xbfff8000. In large-scale MIMD systems, such as those used for high-performance computing, that claim is not necessarily true. These large-scale systems use [non-uniform memory access (NUMA)](#term-non-uniform-memory-access) designs. In NUMA, the memory hardware is distributed throughout the system. Some portions of memory are physically closer to a processing unit than others; consequently, accessing these closer portions of memory is faster than others.

[[1]](#id1)

While “embarrassingly parallel” is the dominant term for these types of problems, some researchers in the field dislike this term, as it has a negative connotation and can be interpreted as suggesting these problems are undesirable. Instead, they tend to use “naturally parallel” to suggest that parallelism naturally aligns with the problem.

[[2]](#id2)

We are showing the most trivial form of merge sort here for illustration. In practice, merge sort would never be implemented this way, as the overhead of the recursive function calls becomes a significant burden. Instead, practical implementations include optimizations that switch to a more efficient iterative solution once the size of the recurrence becomes small.



//9.4. Limits of Parallelism and Scaling[¶]
===========================================

While hardware support is required to achieve parallel computation, it is not sufficient on its own. Many problems or algorithms simply do not support parallel computation. For example, consider merge sort as shown in [Code Listing 9.2](#cl9-2). Although it is true that the left and right halves of the array could be sorted in parallel with two threads, the `merge()` routine cannot be parallelized; a single thread must traverse through both the left and right halves to assemble the results. Consequently, there are limits to how much merge sort can be improved with parallel execution.

///9.4.1. Amdahl’s Law and Strong Scaling[¶]
--------------------------------------------

[Amdahl’s law](#term-amdahl-s-law) provides a way to quantify the theoretical maximum [speedup in latency](#term-speedup-in-latency) (also called the [speedup factor](#term-speedup-factor) or just [speedup](#term-speedup-factor)) that can occur with parallel execution. Specifically, Amdahl’s law describes the ratio of the original execution time with the improved execution time, assuming perfect parallelism and no overhead penalty. That is, Amdahl’s law provides a theoretical limit to how much faster a program can run if it is parallelized. If $p$ denotes the percent of a program that can be executed in parallel and $N$ denotes the number of parallel execution units, Amdahl’s law states that the theoretical maximum speedup would be:

$\large S = \displaystyle\frac{1}{(1 - p) + \frac{p}{N}}$

This formula can be naturally derived by taking the ratio of the original execution time and the improved execution time for the parallelized version. If we use $T_{orig}$ to denote the original execution time, then $(1 – p) T_{orig}$ would signify the portion of the execution time that must run sequentially. Assuming perfect parallelism as Amdahl’s law does, the remainder of the time would be divided across the $N$ processors. This gives us the derivation of Amdahl’s law:

$\large S = \displaystyle\frac{T_{orig}}{T_{parallel}} = \frac{T_{orig}}{(1 - p)T_{orig} + \frac{p}{N}T_{orig}} = \frac{T_{orig}}{((1 - p) + \frac{p}{N}) T_{orig}}$

By cancelling out the $T_{orig}$ values from the numerator and denominator, we are left with the formulation of Amdahl’s above. A variant of Amdahl’s law uses $f$ to denote the portion that must be run sequentially; that is, $f = 1 – p$. This leads to another derivation of Amdahl’s law. The result will be identical as the original formulation, but the calculation might be easier.

$\large S = \displaystyle\frac{1}{(1 - p) + \frac{p}{N}} = \frac{N}{N(1 - p) + p} = \frac{N}{Nf + 1 - f} = \frac{N}{1 + (N - 1)f}$

📜 Example 9.4.1

* * *

As an example, consider a program that runs in 20 ms. The way the program is written, 20% of it must be run sequentially; the remaining 80% will be run in parallel on a quad-core. Per Amdahl’s law, the maximum theoretical speedup of this program would be:

$\large S = \displaystyle\frac{1}{0.2 + \frac{0.8}{4}} = \frac{1}{0.2 + 0.2} = 2.5$

Using the alternative derivation, we would still get the same result:

$\large S = \displaystyle\frac{4}{1 + (4 - 1)(0.2)} = \frac{4}{1 + 0.6} = \frac{4}{1.6} = 2.5$

In this case, we could also determine that the parallelized version would spend 4 ms in the sequential portion of the program. The remaining portion (16 ms in the original) would be divided across the 4 cores, so the parallel portion would take 4 ms. Consequently, parallelizing the program would improve the run time from 20 ms to 8 ms, which is a speedup factor of 2.5. The advantage of Amdahl’s law, however, is that **we did not need to know the original run-time**. As long as we know what portion can be parallelized and how many processing units we have, we can determine the speedup factor. Amdahl’s law also emphasizes a key point about parallelism: **Improving the percent of a program that can be parallelized has more impact than increasing the amount of parallelism**.

📜 Example 9.4.2

* * *

To illustrate this point, let us consider two variants on the previous scenario. In one variant, the program has been restructured so that 90% can be parallelized rather than 80%. This now leads to a speedup factor of:

$\large S = \displaystyle\frac{1}{0.1 + \frac{0.9}{4}} = \frac{1}{0.1 + 0.225} \approx 3.08$

In the second variant, we can still only parallelize 80% of the program, but we have increased the number of cores from four to six. This variant produces a speedup of:

$\large S = \displaystyle\frac{1}{0.2 + \frac{0.8}{6}} = \frac{1}{0.2 + 0.133} = 3.00$

In other words, increasing the percent of parallelized code by 12.5% had a bigger improvement than increasing the number of cores by 50%.

As the number of processing units continues to increase, the precise calculation of Amdahl’s law becomes less important. Specifically, we can determine a faster approximation of the speedup limit by considering the impact of an arbitrarily large number of processing units; that is, we can derive a simplified estimate by calculating the limit of $S$ as $N$ goes to infinity:

$\large \displaystyle\lim_{N \to \infty} \frac{1}{(1 - p) + \frac{p}{N}} = \frac{1}{(1 - p) + 0} = \frac{1}{1 - p} = \frac{1}{f}$

Using this simplified estimate, we can determine that the upper bound on the speedup for the program in [Example 9.4.2](Scaling.html#amdahl) would be 5 (i.e., 1 / 0.2).

///9.4.2. Gustafson’s Law and Weak Scaling[¶]
---------------------------------------------

Although Amdahl’s law provides an initial estimate to quantify the speedup from parallel execution, it is important to note that it rests on unrealistic assumptions. Amdahl’s law assumes that the problem solved by the program exhibits [strong scaling](#term-strong-scaling), meaning that the difficulty of the problem is unaffected by the number of processors involved. In perfectly strong scaling, there is no overhead penalty for creating more threads or using more processors. A program run on a system with 100 processors will run in 1/100th of the time than it would on a single-processor system. In contrast, a more realistic and common property is [weak scaling](#term-weak-scaling), which emphasizes accomplishing more work rather than running in less time. In weak scaling, the additional processors are used to tackle bigger and more complex problems, while holding the expected run time to be the same.

![Fence painting appears to show strong scaling initially, but only for a few painters](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.9.6.png)

Figure 9.4.3: Fence painting appears to show strong scaling initially, but only for a few painters

To illustrate the difference between strong and weak scaling, consider a painting business. [Figure 9.4.3](Scaling.html#gustafson) illustrates the scenario where the company has been hired to paint a fence that is 20 feet in length. This job initially seems to exhibit strong scaling. If one painter could finish painting the fence in one hour, then four painters could probably finish the job in 15 minutes. However, as more painters are added, the scaling becomes weak. If the company tries to send 20 painters for the same fence, they are unlikely to finish the job in only three minutes. Rather, the fence would become too crowded and painters would have to wait on each other. A better choice for the company would be to send the additional 16 painters to paint other fences. If they work in groups of four to paint multiple 20-foot fences, the company could paint five fences in a 15-minute time period. Alternatively, the company could choose to send just one painter per fence, completing 20 fences in a single hour. The fences are not necessarily painted any faster than before, but the company is getting more work accomplished in the same amount of time.

The usefulness of Amdahl’s law is limited by its reliance on strong scaling and unrealistic assumptions of parallel execution. Specifically, Amdahl’s law deliberately ignores any performance cost associated with creating and managing threads, as well as system-specific factors such as NUMA or processor workload. Amdahl’s law is also limited by its exclusive focus on parallelism; Amdahl’s law cannot be used to predict the impact of changing the layout of data within NUMA. [Gustafson’s law](#term-gustafson-s-law) provides an alternative formulation for speedup that addresses these limitations.

Similar to Amdahl’s law, Gustafson’s law uses $p$ to denote the percent of the work that can benefit from an improvement of some sort. Unlike Amdahl’s law, this improvement is not tied to parallelism solely; the improvement could result from an improvement in how the OS manages threads, moving data around within a NUMA architecture, increasing the parallelism, or any other such change. The amount of the improvement [[1]](#f49) is denoted as $s$. Gustafson’s law then states that the maximum theoretical speedup of deploying the improvement is:

$\large S = 1 - p + sp$

📜 Example 9.4.3

* * *

As an example, consider a program that can be partially improved with parallel execution. Let us assume that 20% of the program cannot be improved and some initial empirical results suggest that the parallel execution portion runs in 1/5th of the time that it takes sequentially (i.e., an improvement factor of 5). Note that this does not assume anything about how many processors are used, so it can be based on more realistic measurements by running some initial tests. In this case, the speedup would be:

$\large S = 0.2 + 5 * 0.8 = 0.2 + 4.0 = 4.2$

It is key to note that this speedup has a different meaning than the speedup described by Amdahl’s law. This speedup factor does not mean that the program runs 4.2 times as fast as the original, which is an assertion built on strong scaling. Instead, the proper interpretation of the Gustafson’s law notion of speedup is that this program can achieve 4.2 times as much work in the same amount of time, which is based on weak scaling. If the original program could process 10 MB of data in a minute, then the improved version could process 42 MB **in the same amount of time**. With Gustafson’s law, the emphasis is on the [throughput](#term-throughput) (amount of work done) rather than a faster time.

[[1]](#id1)

To be fair, Gustafson’s law can also be criticized for ignoring complicating factors such as synchronization or communication overhead. However, this objection is not as strong as it is for Amdahl’s, as the improvement factor $s$ can be based more empirically.



//9.5. Timing in Distributed Environments[¶]
============================================

As we have noted in this and previous chapters, concurrency and parallelism introduce an element of nondeterminism that can complicate the design, implementation, and execution of software. Once multiple threads or processes are running independently, the order in which key events are processed can become unpredictable. However, as individual computers possess a single, universal system clock that can be shared, it would be straightforward to use synchronization primitives to keep an orderly log of key events, as shown in [Code Listing 9.8](#cl9-8).

```cpp
/* Code Listing 9.8:
   Sample routine for atomic log file append 
 */

void
append_log (char *message)
{
  /* Uses a global variable for locking the log */
  pthread_mutex_lock (log_lock);
  struct timespec current_time;
  clock_gettime (CLOCK_MONOTONIC, &current_time);
  FILE *file = fopen (SYSTEM_LOG_FILE, "a");
  if (file != NULL)
    {
      fprintf (file, "%lld.%09ld %s\n",
               (long long) current_time.tv_sec,
               current_time.tv_nsec, message);
      fclose (file);
    }
  pthread_mutex_unlock (log_lock);
}
```

The challenge of logging events in distributed environments is much more difficult, as there is no universal clock that can be used. Each system has its own internal system clock, but these clocks may be misconfigured or naturally drift over time. This problem could potentially be addressed by synchronizing the system clocks to agree on the time. However, as the system becomes larger and the physical distance between nodes increases, delays in the network can become unpredictable and too difficult to counteract. The solution in that case would be to give up on trying to synchronize the clocks and focus on a coherent ordering of events instead.

///9.5.1. Clock Synchronization[¶]
----------------------------------

![Sequence of messages and events in NTP](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.9.7.png)

Figure 9.5.1: Sequence of messages and events in NTP

One approach to solving the timing problem in distributed environments is to synchronize the clocks on all nodes with a centralized server. [Figure 9.5.1](DistTiming.html#ntpsequence) shows the exchange of messages in the [Network Time Protocol (NTP)](#term-network-time-protocol), one solution built on this idea. In NTP, each node will occasionally poll the server, keeping track of the times when certain events occur. Specifically, the client records $T_1$ as the time that it sent the poll message. The server received the message at $T_2$ according to its internal clock, sending the response back at time $T_3$ (according to the server’s clock). The client then records $T_4$ as the time that it received the response from the server. Based on this information, the client would calculate two values:

$\large \mbox{offset} = \displaystyle\frac{(T_2 - T_1) + (T_3 - T_4)}{2}$  
  
$\large \mbox{delay} = (T_4 - T_1) - (T_3 - T_2)$

The delay is used to eliminate the time that it takes for the messages to travel through the network from consideration. The offset is used to detect the clock skew between the nodes. For instance, assume $T_1 = 20$, $T_2 = 30$, $T_3 = 32$, and $T_4 = 46$. Then the offset would be calculated as $(10 - 14)/2 = -2$, and the delay would be $26 - 2 = 24$. The message from client to server was measured as taking 10 time units, whereas the response took longer (14 time units). One possible cause is that the client’s clock is ahead of the server’s clock. If the client’s clock was adjusted back so that $T_1 = 18$ and $T_4 = 44$, then the offset would be 0, as the two messages would be measured as taking 12 time units in each direction. The client passes the offset and delay values, along with previous measurements, through a suite of protocols to filter and select the most accurate values. The client then combines these _survivor_ offsets to adjust the system clock time and frequency. The protocol can then be repeated until the offset is sufficiently small. [[1]](#f50)

///9.5.2. Logical Clocks and Lamport Timestamps[¶]
--------------------------------------------------

As systems grow larger and nodes increase in physical distance, trying to synchronize the internal clocks of nodes in the system becomes unmanageable. The number of messages needed for protocols like NTP impose a significant burden on the network that would interfere with other work. An alternative approach is to establish a reasonable understanding of the sequence of events using [logical clocks](#term-logical-clock). A logical clock uses messages, not real time measurements, to track the relative, logical ordering of events. These messages are [asynchronous](#term-asynchronous), in the sense that there are no timing guarantees; network and other processing delays introduce random timing differences between when a message is sent and received.

[Lamport timestamps](#term-lamport-timestamp) are a simple approach to logical clocks. Each process (a task running on a separate node) maintains an internal counter of events that it experiences. These events could be a local action or computational result that is relevant to the system as a whole. Whenever a local event occurs, or a process sends or receives a message, this internal counter is incremented. When processes send messages, they append their counter. Given their distributed nature, processes can only observe local events and messages they send or receive; they cannot be aware of messages received by other processes or events that occur elsewhere in the system. [Code Listing 9.9](#cl9-9) shows the basic algorithm for receiving a message with a Lamport timestamp. The message’s timestamp is compared with the local process’s counter, and the local timestamp is updated to be the greater of the two. The timestamp is then incremented, keeping track of the event of receiving the message.

```cpp
/* Code Listing 9.9:
   Reading a message from a socket with a Lamport timestamp attached
 */

ssize_t bytes = read (socketfd, &message, sizeof (message));
/* Update local timestamp if message has greater value */
if (message.timestamp > local_timestamp)
  local_timestamp = message.timestamp;
/* Increment local timestamp */
local_timestamp++;
```

![Events and messages using logical clocks](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.9.8.png)

Figure 9.5.2: Events and messages using logical clocks

As an example, consider a cloud computing system that provides Internet-accessible data storage service. [Figure 9.5.2](DistTiming.html#logicalclocks) shows a sample sequence of events that may occur; the numbers in brackets denote the Lamport timestamps. Process A provides an interface for users and receives a request for the files “foo” and “zoo”; this request constitutes a local event for A, which has the timestamp 1. A then sends a message to process B (timestamp 2) and another to process C (timestamp 3); receiving these messages occur at times 3 and 4, respectively. Note that process B has no knowledge of the message sent to C, so B’s local timestamp only reflects its knowledge of the message it received.

After receiving their messages from process A, processes B and C concurrently load the two files requested. C sends the contents of “zoo” to B (timestamp 6) and B receives this message. Note that B’s timestamp jumps from 4 to 7 when this happens, reflecting the timestamp it observes in the message from C. B then merges the files (timestamp 8) and sends the result to A.

With logical clocks, it is critical to note that an earlier timestamp does not necessarily guarantee that an event occurred before one with a later timestamp. Consider process B’s perspective in [Figure 9.5.2](DistTiming.html#logicalclocks). B observes the local logical ordering of events at timestamps 3, 4, and 7. However, B cannot know if “foo” was loaded before or after “zoo.” In fact, B cannot even know that the message it received from A was sent before the message from A to C. B can only observe the logical ordering of events within its own time line. If B later receives a message from A with a timestamp greater than 11, it knows that message comes after the message when it sent foo+zoo to A. However, if B’s next interaction with C is receiving a message with timestamp 11, B cannot determine precisely when this message was sent; it’s possible that C experienced several events before B merged the files at timestamp 8.

In summary, logical clocks provide a mechanism for processes to exchange information about the relative ordering of events based on their perspectives. If processes A and B exchange messages back and forth, they can determine the relative ordering of these events. Once a third process C starts exchanging messages with A, process A will be able to determine the relative ordering of these messages, but neither B nor C would have the full view of the exchanges.

///9.5.3. Vector Clocks[¶]
--------------------------

[Figure 9.5.2](DistTiming.html#logicalclocks) illustrates one of the shortcomings of logical clocks with Lamport timestamps. Note that process A’s timeline jumps from timestamp 3 to the message it receives from B with timestamp 9. Given only this information, process A could not distinguish this sequence of events from a different timeline in which B experienced only local events. That is, the timestamp that process A observes in the message from B does not even provide assurance that B received the original message from A, nor does it suggest that B has been communicating with C. Lamport timestamps alone do not provide sufficient information to distinguish these scenarios.

![The events of :num:`Figure #LogicalClocks` with vector clocks](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.9.9.png)

Figure 9.5.3: The events of [Figure 9.5.2](DistTiming.html#logicalclocks) with vector clocks

[Vector clocks](#term-vector-clock) extend the basic idea of logical clocks with more information to address this problem. Whereas logical clocks kept only a single counter to establish the relative ordering of events, vector clocks use one counter per process. When a process sends a message, it appends the vector of counters from its perspective, implicitly acknowledging its understanding of the other process’s timestamps. [Figure 9.5.3](DistTiming.html#vectorclocks) illustrates the same sequence of events from [Figure 9.5.2](DistTiming.html#logicalclocks), using vector clocks instead of Lamport timestamps. When process B receives the first message from A, B increments its own counter in the vector.

[Figure 9.5.3](DistTiming.html#vectorclocks) illustrates a key strength of vector clocks with the message from process C to B containing the contents of zoo. This message contains the vector [3,0,3], indicating that C believes process A’s timestamp is at least 3. But B had previously received the vector [2,0,0] from process A, so process B knows that A’s message to C was sent after A’s message to B. Note, though, that process B cannot be certain about the order of events that C experienced. That is, the vector [2,0,0] was A stating it believed C’s timestamp was 0. The best that process B can state is that the message it received from C had been sent after A sent the first message to B. For an asynchronous system, this claim is the best that can be made.

[[1]](#id1)

For more specifics on how the offset and delay are used, see the NTP homepage available at `https://www.eecis.udel.edu/~mills/ntp/html/warp.html`.



//9.6. Reliable Data Storage and Location[¶]
============================================

Distributed systems are commonly used to store collections of data. Consequently, a key issue in distributed systems is knowing where to find the data in question. As an example, consider a traditional static web page, such as `http://www.abc.com/index.html`. The structure of this URL indicates that there is a file named index.html located in the root directory of a web server. Since the URL is an HTTP request, this web server is a process listening on port 80 from a machine somewhere. The hostname `www.abc.com` would be translated into an IP address to determine the machine’s logical location within the Internet. When the HTTP request gets made, the client sends the request to a router and the request gets forwarded until it reaches this machine. In other words, completing this single HTTP request requires identifying the machine that stores the file, the logical location of the machine in the Internet, the physical location of the network card that connects the machine, and the specific process on that machine responsible for hosting that file. If any of those components fail, then the file cannot be served to the user’s browser.

This scenario illustrates a common goal in distributed systems: how to locate and retrieve data objects reliably, **even when components fail**. The routing protocols of the Internet are designed around the assumption of failure. If a wire gets cut or a machine crashes, the routing protocols attempt to find an alternative path by updating their routing data structures. This design principle of reliable service with unreliable components can be extended to the application layer of the network stack, as well.

One fundamental building block of reliably locating objects in a distributed system is [replication](#term-343). When an object is replicated, multiple servers store identical copies, so clients can retrieve the object from several locations. Replication avoids the problem of a _single point of failure_ — one node failure is not sufficient to eliminate all service. Additionally, replication provides a means of load balancing. Since the same object is accessible from multiple servers within the system, clients can request copies from any of them rather than overloading servers that host the most popular objects.

///9.6.1. Google File System[¶]
-------------------------------

As an example of how replication provides reliable service for data storage, consider the _Google File System (GFS)_. [Figure 9.6.1](DistDataStorage.html#google) shows the structure of the main components of GFS. In GFS, the assumption is that files are large (e.g., each file might be multiple terabytes in size), so they are broken up into _chunks_. The individual chunks with the file contents are stored on _chunkservers_. Each of these chunkservers contains its own traditional local file system; for example, assuming the machines are running Linux, then the chunks would be stored as local files within that chunkserver’s ext4 file system. In addition, a GFS master maintains a non-authoritative table that maps files to their locations.

![The structure of the Google File System (GFS)](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.9.10.png)

Figure 9.6.1: The structure of the Google File System (GFS)

For each given chunk, there is a designated _primary chunkserver_ along with multiple [replicas](#term-343). The primary and replicas all store identical copies of the chunks, but the primary is designated as having a _lease_ on the file and has exclusive access to modify it if needed. For instance, in [Figure 9.6.1](DistDataStorage.html#google), the first chunk of File 1 can be retrieved from either node `a3d2` (the left-most chunkserver) or node `c9c4` (the right-most). Ultimately, each chunkserver has full control over the chunks that it stores, and this information might differ from the records in the GFS master. To keep the GFS master’s records updated, the GFS master sends out periodic _HeartBeat_ messages to the chunkservers to determine their current status and provide additional instructions.

///9.6.2. Distributed Hash Tables[¶]
------------------------------------

GFS was designed to create a distributed file system for a single organization. Consequently, Google could build in some assumptions into the design about the locations of nodes. When the GFS master in [Figure 9.6.1](DistDataStorage.html#google) informed the client that node `a3d2` was the assigned primary for chunk 1 of file 1, the client knew which machine to contact, because the clients maintain information about the IP addresses of nodes in the system. In other distributed file systems—particularly those designed to be openly accessible—clients do not have this information.

![A Chord ring with up to 32 nodes. Black nodes are live (available); nodes with a red X are considered failed (absent or unavailable)](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.9.11.png)

Figure 9.6.2: A Chord ring with up to 32 nodes. Black nodes are live (available); nodes with a red X are considered failed (absent or unavailable)

Instead of relying on the clients to have information about node locations, systems can create a [distributed hash table (DHT)](#term-distributed-hash-table) that maps objects to machines that host them. [Chord](#term-chord) was an early [[1]](#f51) and influential DHT. In Chord, all nodes were assigned unique identifiers and arranged in a logical ring structure. [Figure 9.6.2](DistDataStorage.html#chord) shows the logical structure of a simplified Chord ring with support for 32 nodes. (Chord node identifiers are created by calculating the hash of the machine’s IP address; for a 256-bit hash value, the ring would have up to $2^{256}$ nodes.) Most of the nodes are _live_, meaning those machines are running and connected to the system; nodes marked with an X (e.g., 2, 8, and 14) are not connected.

[Figure 9.6.2](DistDataStorage.html#chord) also shows a key data structure to define the nature of the Chord ring: the _finger table_. Every node contains a finger table, which contains information about other Chord ring indexes. Each entry in the finger table is based on the current node’s identifier plus a power of 2. The finger table shown here would be the table for node 0. Node 0 would contain information about the indexes 1, 2, 4, 8, and 16 (i.e., $0+2^0$, $0+2^1$, $0+2^2$, $0+2^3$, and $0+2^4$). Similarly, node 1 would contain information about 2, 3, 5, 9, and 17 ($1+2^0$, $1+2^1$, $1+2^2$, $1+2^3$, and $1+2^4$). However, given that some nodes are missing (such as 2 and 8), Chord nodes keep track of the _successor_ of the target index, which is the closest value greater than or equal to the index. Since 1, 4, and 16 are present, these nodes are their own successors. Since 2 and 8 are missing, 3 is the successor of 2 and 9 is the successor of 8. Within the finger table, the Chord nodes keep track of information about that successor, including its IP address.

The logical ring structure of Chord makes it possible for items to be located very efficiently, even though nodes in the system have very little information about the structure. Assume that a user is running a client on node 6. This user tries to open the file `"chord/data/foo"`. As with the node identifiers, objects are mapped to keys using hashes, so the user would calculate the hash of the file name. If `"chord/data/foo"` hashes to the value 19, then the successor of 19 is deemed the location of that object.

![Routing a Chord lookup message for key 19 from node 6 to node 21](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.9.12.png)

Figure 9.6.3: Routing a Chord lookup message for key 19 from node 6 to node 21

[Figure 9.6.3](DistDataStorage.html#chordsucc) shows the routing of the request message through the Chord ring. Node 6 would have entries for the successors of 14 ($6+2^3$) and 22 ($6+2^4$). Since 22 is greater than the target key 19, node 16 would try to contact the successor of 14. Since node 14 is absent from the ring, node 6 would actually contact node 16. Note that node 6 determines this itself, because this information is stored in its own finger table. When node 16 receives the request for key 19, it would find the successor of 18, which is $16+2^1$. Since nodes 18, 19, and 20 are all missing, node 21 is the successor of 18, so this node is the location of the requested file.

The routing structure of Chord is deceptively powerful. Although it seems complicated, the structure is straightforward to implement in code. [Code Listing 9.10](#cl9-10) illustrates the algorithm for this lookup procedure. The logic of the algorithm is to start with the last entry of the finger table (assuming 64 entries because of 64-bit integer types; supporting a 256-bit identifier would require larger integer types) and work backwards to find the first entry that precedes the key.

```cpp
/* Code Listing 9.10:
   Algorithm for finding the next node in a Chord finger table
 */

#define NUM_ENTRIES 63 /* assume all keys are 64 bits */
uint64_t
lookup (uint64_t key)
{
  for (int index = NUM_ENTRIES; index > 0; index--)
    {
      if (node_id < finger_table[index])
        {
          if (key >= finger_table [index] || key < node_id)
            return finger_table [index];
        }
      else if (key >= finger_table [index] && key < node_id)
        return finger_table [index];
    }
  return node_id; /* default response is current node */
}
```

The logic in [Code Listing 9.10](#cl9-10) is complicated by the modular arithmetic imposed by the Chord ring structure. Essentially, the algorithm starts by looking at `node_id + 2`max and incrementally decreases the exponent. Each time, the algorithm checks that the key would occur _after_ the jumped node but before the current node. For instance, considering the lookup in [Figure 9.6.2](DistDataStorage.html#chord), the algorithm would look at $6+^2$4, to check if the key lies after 22 but before 6; if so, the algorithm would return 22 as the next node to jump to, otherwise it would look at $6+2^3$. If we consider a similar query, though, starting from node 18, the algorithm would first look at $18+2^4$, but this value must be mapped to the ring by applying mod 32. As such, the algorithm would need to check if the key comes after node 2 and before node 18. With modular arithmetic, determining whether a key value is _between_ two nodes on the ring can be tricky to get the logic correct.

The benefit of the Chord structure is that it places a very efficient upper-bound of Θ(log n) on the number of messages to find an object in the system. To put that into practical figures, given a system with 1,000,000 nodes storing data, any item could be found by forwarding the request at most 20 times. If the number of nodes doubles, the maximum number of request messages would only increase by one. Consequently, the network overhead with Chord is very light.

To improve the system performance further, Chord also employs replication through caching. Whenever a node is involved in looking up an object’s location, that node receives a copy of the object, as well. Returning to [Figure 9.6.2](DistDataStorage.html#chord), since node 16 was used as a step toward locating key 19 at node 21, node 16 also gets a copy. At that point, any request for the file `"chord/data/foo"` that goes through either node 6 or 16 will get a faster response. Since these nodes have a local replica of the file, there is no need to forward the request. Because of this caching technique and other forms of replication, Chord provides efficient times to find objects and high levels of availability (even when nodes fail).

[[1]](#id1)

During the 2001-2003 time frame, several DHTs were designed and proposed. Chord, along with Pastry, Tapestry, and CAN, was one of the four original systems created at this time. All of these systems contributed important ideas to this subfield of distributed systems.



//9.7. Consensus in Distributed Systems[¶]
==========================================

Distributed systems are often designed and implemented as [state machines](#term-state-machine). In the state machine approach, the system contains a number of variables that can be affected by the commands that processes perform. Returning to GFS as an example, which chunk servers store a particular file chunk would constitute a state variable; sending an extra copy to another chunk server for additional redundancy would constitute a command that affects such a variable. As the processes in a distributed system are running on different—possibly physically remote—machines, the commands are performed by sending messages (including all relevant data) across the network.

The notion of state machine replication can seem very theoretical and abstract, leading to confusion. To make it a bit more concrete, consider a process running on a traditional computing system. This process includes the lines shown in [Code Listing 9.11](#cl9-11). This code opens a file and reads the first part of the contents into a buffer, then compares the bytes read in with the string indicating the location of the Python interpreter.

```cpp
/* Code Listing 9.11:
   Sample code that checks if a provided file is a Python script
 */

FILE *file = fopen ("/usr/bin/smtpd.py", "r");
fread (file, buffer, sizeof (buffer));
/* Confirm the file is a Python script */
if (strncmp (buffer, "#!/usr/bin/python ", 18) != 0)
  printf ("ERROR: File is not a Python script\n");
```

This code implicitly relies on several pieces of information that could be considered state variables. To start, the call to `fopen()` needs information about the file system structure, eventually determining which exact block to read from which particular storage device. In the process, the system call required checking that the user running this program had appropriate access privileges to read from the file. More subtly, this code relies on the directory structure of the file system and the use of the `'/'` character to denote the directory hierarchy; in other words, there is an assumption that `"/usr/bin/smtpd.py"` is the name that uniquely identifies the file to be opened. The conditional check also relies on the assumption that the Python executable is present in a particular location. All of this information could be considered as part of the system state. The goal of [consensus](#term-consensus-protocol) in state machine [replication](#term-343) is to get all of the processes to agree on the values of these variables. For instance, one machine might have Python installed at `/usr/local/bin/python`, rather than the location shown above; this machine would be breaking the system’s consensus on where Python gets installed.

Consensus in a distributed system, then, is the notion that all of the nodes agree on a state variable’s value. To be precise, consensus is a property that may be achieved by protocols attempting to determine a variable’s value. Processes in the system exchange messages to _propose_ values until they _decide_ on a final value. A protocol reaches consensus by achieving three properties:

> *   **Termination**: Eventually, every correct (non-failing) process decides on a value.
> *   **Integrity**: If all correct processes propose the same value, then any correct process must decide on that value.
> *   **Agreement**: All correct processes decide on the same value.

As an example, consider the question of whether a file chunk storing the file `"foo"` is being stored on a particular chunk server. A consensus protocol would provide a definitive answer to all nodes in the file system, thus allowing every user to access the file from that chunk server. As another example, consider the clock synchronization approach using NTP. Consensus would occur if all nodes in the system execute the protocol until their internal clocks have been adjusted to be identical.

///9.7.1. Byzantine Generals Problem[¶]
---------------------------------------

The key challenge in distributed systems is the fact that processes fail. There may be intermittent delays or lost connections in the network, or software bugs may lead to crashes or other errors such as infinite loops. Even worse, some processes may be acting maliciously; someone may have joined their computer to the system for criminal or other adversarial purposes. The presence of failing processes is a defining characteristic of distributed systems, and it has implications for protocols that strive to achieve consensus.

![Byzantine generals communicating](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.9.13.png)

Figure 9.7.1: Byzantine generals communicating

The [Byzantine generals problem](#term-byzantine-generals-problem) presents an analogy to illustrate the severity of failing processes. In its simplest form, there are three generals that each lead a part of the army of the Byzantine empire; one general is a commander and the other two are lieutenants. The three generals have surrounded a city and they are deciding whether to attack or retreat. If only one or two decide to attack, the city’s defenses are strong enough that the attackers will definitely lose. As such, either all three need to decide to attack or all three retreat. [Figure 9.7.1](DistConsensus.html#byzantine) shows the communication for the generals. The commander sends its command to the two lieutenants, who exchange messages with each other to confirm that everyone received the same order.

If all three of the generals are loyal, then the messages X and Y would be the same. That is, if the commander decides to order an attack, the lieutenants confirm with each that the commander said to attack; similarly, if the order is to retreat, then all of the messages are the same. However, one of the generals might be treacherous and might be trying to make the Byzantine army fail. If the commander is treacherous, then message X might say to attack and Y might say to retreat; the two lieutenants then exchange conflicting messages. But if the commander is loyal and one of the lieutenants is the disloyal general, then the messages from the commander would match, but the lieutenants’ messages would conflict. From the perspective of a loyal lieutenant, both situations would appear identical.

///9.7.2. Limits on Consensus[¶]
--------------------------------

The Byzantine generals problem highlights a major hurdle to achieving consensus in distributed systems: misbehaving processes can prevent the correct functioning of other processes. Furthermore, because correct processes cannot have perfect information about the rest of the system, one or more processes may appear to different parts of the system as both correct and failing. To be precise, a [Byzantine failure](#term-byzantine-failure) (also called a [Byzantine fault](#term-byzantine-fault)) occurs when one process appears as a failure to some correct processes while also appearing correct to other processes; in a Byzantine failure, different processes have access to different, conflicting observations and cannot determine what information is reliable. As such, the system cannot determine how to respond to information from processes—which may be unreliable or faulty.

Work on the Byzantine generals problem has established a significant theoretical limitation on consensus in distributed systems. If messages cannot be authenticated and more than 1/3 of the processes fail, then it will be impossible for correct, non-faulty processes to achieve consensus with any protocol. To illustrate this insight, consider the scenarios described earlier based on [Figure 9.7.1](DistConsensus.html#byzantine). Assume that the left lieutenant is correct and it receives the message X=attack from the commander. In the confirmation stage of the protocol, the left receives the message that “C said Retreat” from the right lieutenant. The left lieutenant is a correct process, but it cannot terminate because it cannot decide which action is the correct choice. Later work showed that this insight can be generalized for any system with 1/3 of the processes failing, for instance if there were 4 treacherous lieutenants in an army with 10 generals.

If the system can guarantee that fewer than 1/3 of the processes fail, consensus is still possible in some scenarios. To illustrate this informally, assume that there is a third lieutenant in the scenario from [Figure 9.7.1](DistConsensus.html#byzantine), and at most one of the generals is treacherous. If the commander is loyal and orders an attack, then any loyal lieutenant will receive the order to attack along with at least one confirmation of that order; if one of the lieutenants declares “C said Retreat,” the other lieutenants can determine which general is the traitor by looking at the majority.

![Consensus is possible even if the failing process is the commander](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.9.14.png)

Figure 9.7.2: Consensus is possible even if the failing process is the commander

But consider the case where the commander is traitorous and the three lieutenants are loyal. Then two lieutenants receive the message to attack and one to retreat (or vice versa), leading to the exchange of messages as shown in [Figure 9.7.2](DistConsensus.html#byzfour). Since the lieutenants are all loyal, they report what they received as an order accurately. Lieutenants 1 and 2 both receive three messages: “Attack” (from C), “C said Attack” (from the other), and “C said Retreat” (from Lieutenant 3). Lieutenant 3 receives “Retreat” from C along with two copies of “C said Attack” from the other lieutenants. The lieutenants can again reach consensus by looking at which order was in the majority. Interestingly, note that lieutenants 1 and 2 cannot determine which process is failing (the messages would be the same if lieutenant 3 was the disloyal general) but they can still achieve consensus.

There is a nuance that needs to be added to this conclusion, though. The Byzantine generals problem, as described above, implicitly assumes the use of synchronous communication. That is, the commander issues the orders “Do X” and “Do Y” at some time, and the lieutenants exchange confirmation messages within a particular time frame after that. In an asynchronous environment, though, the impact of failures is much worse. Without time limits on responses, processes can respond to previous messages at any time in the future. As such, when a lieutenant receives the message “C said attack,” that lieutenant cannot determine if that message is in response to the current order or to one from years ago. When asynchronous communication is involved, it is impossible for any protocol to _guarantee_ that consensus can be reached if there is even a _single failing node_. In practice, there is a non-zero probability that the processes can come to an agreement, but once a single node fails, there is no guarantee that consensus is possible.

///9.7.3. Practical Byzantine Fault Tolerance[¶]
------------------------------------------------

Given that consensus is provably impossible in the presence of certain conditions, it is worth considering what level of reliable service can be accomplished by processes in a distributed system. That is, one could ask whether it is possible to gather enough reliable information to reach a probably correct answer. By applying the [practical Byzantine fault tolerance (PBFT)](#term-practical-byzantine-fault-tolerance) algorithm, it is possible to reliable and efficient state replication, so long as the system maintains the property that fewer than 1/3 of the processes fail. Note that the system does not strive for consensus—which would require all correct processes to reach the same decision—but for sufficient consistency among enough responses to provide usable service.

![Reliable service with PBFT](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.9.15.png)

Figure 9.7.3: Reliable service with PBFT

In PBFT, a client contacts a designated primary process responsible for the requested data. The _primary_ is assumed to be correct in the normal case, which is shown in [Figure 9.7.3](DistConsensus.html#byzpaxos). The primary will then forward the request to multiple replicas that should also have the requested data. Once the client receives a response from enough processes (including the primary and the replicas), the client considers the majority response to be correct.

[Figure 9.7.3](DistConsensus.html#byzpaxos) shows the stages of message exchanges in PBFT. When the primary receives a request, it enters a _pre-prepare_ phase, alerting the replicas of a pending request. During the _prepare_ phase, all non-faulty replicas multicast messages acknowledging the request to all other replicas and to the primary. Once the primary and replicas have received enough prepare messages, the system then enters the _commit_ phase, when the primary sends the actual request to the replicas. The client accepts the result after receiving the responses.

There are several key elements that allow PBFT to provide reliable service. First, all messages are cryptographically signed to prevent forgeries; a faulty or malicious service cannot inject a fake message that appears to come from a correct process. Second, the pre-prepare and prepare messages include sequence information that create a total ordering on requests. When a primary receives a request from a client, it will attempt to process that request before any that come later. Specifically, the primary will not send out a pre-prepare message for a later request until the current request has been processed. Third, messages include timestamps that place a limited duration on how long a request can be pending; if the replicas take too long to respond with prepare messages, the primary will abandon the request and no commit message will be sent.

Finally, the primary requires a threshold number of processes that must respond at each stage; assuming the system guarantees $f ≤ (n-1) / 3$ faulty replicas (i.e., the system meets the requirement that at most 1/3 of the processes are failing at any given moment), then f prepare messages are required to enter the commit phase, and the client will require $f+1$ responses with the same result. This requirement matches the assumptions of the system’s behavior. If 1/3 of the system processes are failing, then 2/3 are correct and they would all agree to the same value. Once the client and/or the primary receives more than 1/3 responses that match, _at least_ one of these responses came from a correct node and the response is probably reliable.

PBFT and similar approaches are regularly used in a variety of settings to create reliable services in unpredictable, asynchronous settings. For example, Byzantine fault tolerance algorithms are used in applications for Internet-based cloud storage providers, as well as aircraft systems that require real-time safety-critical behavior. PBFT cannot guarantee consensus, given the limitations that have been proven when Byzantine failures can occur. However, PBFT can provide a sufficiently reliable level of service for systems, so long as there are limits on the number of processes that can fail.

🔍 Note

* * *

PBFT is a fault tolerant extension to the [Paxos](#term-paxos) family of consensus protocols. The goal of the Paxos protocols is to replicate a server’s state machine across multiple nodes in a distributed system. That is, instead of having a single server (which is prone to failure), the work can be spread across many machines that can continue to function even if some nodes fail. As part of this replication, it is critical for the distributed nodes to agree on the values of particular variables, such as the server’s state (e.g., running, rebooting, shutting down). The Paxos family describes how to perform this work under a variety of assumptions. PBFT, which is also known as Byzantine Paxos, is one such protocol.

A full discussion of Paxos, which is a complicated set of protocols, is beyond the scope of this book. We focused on PBFT here as an instance of how protocols can provide some level of assurance despite the impossibility result of the Byzantine generals problem. As starting resources, interested readers should consider Lamport’s “Paxos Made Simple” paper, which provides a plain-text overview of the main algorithm, or Altınbüken’s “Paxos Made More Complex” website (`https://paxos.systems`), which describes multiple variants and provides sample Python code. Paxos is also a standard topic for distributed systems textbooks, some of which can be found in the reading list for this chapter.



//9.8. Extended Example: Blockchain Proof-of-Work[¶]
====================================================

A blockchain is a sequence of messages that create a verifiable ordering of events. Blockchains can be used in a variety of applications, such as cryptocurrency (Bitcoin) or other distributed database ledgers. One key idea behind a blockchain is that new entries are difficult to generate. A common technique is to require a proof-of-work calculation, such as finding a string that produces a cryptographic hash value with several leading 0s.

Consider the example output below. Each line contains a message index (0, 1, 2, …) followed by the previous line’s hash value (`0000...`, `f74d...`, and so on), a timestamp (omitted for brevity), then a log message (“Genesis Block”). The line then ends with an integer value (called a _nonce_), such as `0000`, `22d75`, and `6ea` as shown. The entire line ending at the nonce is then used as input to a cryptographic hash function. If the output of the hash does not begin with 0s, the nonce is increased and the line is re-hashed. For instance, the second line required 142,709 (`0x22d75`) attempts before the hash output began with 0s. Once this entry is found, it can be added to the blockchain.

```sh
$ ./blockchain
Here is the blockchain:
  ...0 00000000... [...] Genesis Block ...000000 f74d3a10...
  ...1 f74d3a10... [...] Data message 1 ...022d75 0000b38b...
  ...2 0000b38b... [...] Data message 2 ...0006ea 0000e2c4...
  ...3 0000e2c4... [...] Data message 3 ...00be80 00002c45...
  ...4 00002c45... [...] Data message 4 ...019e63 0000a65e...
  ...5 0000a65e... [...] Data message 5 ...0188a8 0000cb6f...
```

The second field in each line is the previous line’s hash output, which serves the purpose of declaring that the new line is the only legitimate successor of the previous line. That is, each line explicitly ties itself to the previous line by including the previous line’s hash in the input. Note that the first line, often called a _Genesis block_, is special, as it does not follow any entry. As such, a nonce of 0 is often used for this value and the hash value does not need to begin with 0s. In this extended example, we use SHA-1 as a cryptographic hash function simply to make the search more efficient. **In practice, SHA-1 is considered insecure and should not be use for applications like this**.

```cpp
#include <assert.h>
#include <inttypes.h>
#include <openssl/sha.h>
#include <pthread.h>
#include <stdbool.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

#define ASCTIME_LENGTH 24
#define BLOCKCHAIN_ZEROES 3

char * construct_block (uint64_t, char *, struct tm *,
                        char *, size_t, uint64_t, size_t *);
char * create_genesis_block (void);
char * hash_to_string (uint8_t *hash);
char * append_block (char *, char *, size_t);

int
main (void)
{
  size_t number_of_threads = 4;
  size_t number_of_blocks = 10;

  /* Generate the blockchain pointers and create genesis block */
  char *blockchain[number_of_blocks];
  memset (blockchain, 0, sizeof (blockchain));
  blockchain[0] = create_genesis_block ();

  /* Use a generic message to append each time */
  char *message = calloc (50, sizeof (char));
  size_t i;
  for (i = 1; i < number_of_blocks; i++)
    {
      strncpy (message, "Data message ", 50);
      snprintf (message + strlen (message), 50 - strlen (message), "%zd", i);
      blockchain[i] =
        append_block (message, blockchain[i-1], number_of_threads);
    }
  free (message);

  /* Print the results of the computations */
  printf ("Here is the blockchain:\n");
  for (i = 0; i < number_of_blocks; i++)
    {
      printf ("  %s\n", blockchain[i]);
      free (blockchain[i]);
    }

  return EXIT_SUCCESS;
}

/* Parameters for each copy of the mining thread */
struct mining_params {
  char *block;
  size_t blocklen;
  uint64_t nonce;
  size_t attempts;
  bool success;
};

/* Blockchain mining thread. Each instance of this thread starts
   with a block string to work with. The string contains a nonce
   that is iteratively incremented until the string hashes to the
   correct structure (leading zeroes) or the thread runs out of
   attempts. */
void *
mining_thread (void * _params)
{
  struct mining_params *params = (struct mining_params *) _params;
  char *block = params->block;
  char *nonce_loc = block + strlen (block) - 16; 
  size_t count = 0;
  uint8_t hash[SHA_DIGEST_LENGTH];

  while (count < params->attempts)
    {
      /* Update nonce in the string and re-calculate the hash */
      snprintf (nonce_loc, 17, "%016" PRIx64, params->nonce++);
      memset (hash, 0, SHA_DIGEST_LENGTH);
      unsigned char *md =
        SHA1 ((unsigned char *)block, (unsigned long) params->blocklen, hash);

      /* Check to see if the hash begins with the desired number of
         leading zeroes */
      bool zeroes = true;
      int i = 0;
      for (i = 0; i < BLOCKCHAIN_ZEROES; i++)
        if (md[i] != 0) zeroes = false;

      /* If the leading zeroes match, update the block and exit */
      if (zeroes)
        {
          char *hash_str = hash_to_string (md);
          block[strlen (block)] = ' ';
          strncat (block, hash_str, SHA_DIGEST_LENGTH * 2);
          free (hash_str);
          params->success = true;
          pthread_exit (NULL);
        }
      count++;
    }
  pthread_exit (NULL);
}

/* Create a block to append to the end of the chain. Can be
   parallelized for improved performance, but blockchain
   calculations are designed to be slow. */
char *
append_block (char *data, char *previous, size_t num_threads)
{
  uint64_t index = (uint64_t) strtoul ((char *)previous, NULL, 16);

  char *prev_hash = calloc (SHA_DIGEST_LENGTH * 2 + 1, sizeof (char));
  strncpy (prev_hash,
           (char *)&previous[strlen ((char *)previous) - SHA_DIGEST_LENGTH * 2],
           SHA_DIGEST_LENGTH * 2);

  /* Last block was validated, now construct a new block */
  time_t now = time (NULL);
  size_t blocklen = 0;
  char *block =
    construct_block (index + 1, prev_hash, localtime (&now),
                     data, strlen (data), 0, &blocklen);
  free (prev_hash);

  /* Repeatedly implement a fork/join pattern until the search is
     successful. Each time through the while-loop, create a fixed
     number of threads that will perform a maximum number of
     calculations with different nonces. For instance, using
     1,000,000 attempts per thread, the first thread will use nonces
     0 through 999,999, the second 1,000,000 through 1,999,999, and
     so forth. Once all threads have run, check to see if any were
     successful. If not, create a new set of threads that continue
     with the next group of possible nonces. */

  size_t attempts_per_thread = 1000000;
  uint64_t nonce = 0;
  bool success = false;

  while (! success)
    {
      struct mining_params params[num_threads];
      pthread_t thread[num_threads];
      int i;
      for (i = 0; i < num_threads; i++)
        {
          /* Each thread needs its own copy of the block, so that
             it can change the nonce portion of the string. Note
             that we cannot just use strdup() here, because that
             would not allocate the additional bytes needed to
             append the hash for the successul search. */
          params[i].block =
            calloc (blocklen + 2 + 2 * SHA_DIGEST_LENGTH, sizeof (char));
          strncpy (params[i].block, block, blocklen);
          params[i].blocklen = blocklen;
          params[i].nonce = nonce;
          params[i].attempts = attempts_per_thread;
          params[i].success = false;
          pthread_create (&thread[i], NULL, mining_thread, &params[i]);

          /* The starting nonce for each thread is repeatedly
             incremented to ensure no two threads search the same
             set of values */
          nonce += attempts_per_thread;
        }

      /* For this group of threads, check to see if any were
         successful. If so, replace the generated block string with
         that thread's copy. */
      for (i = 0; i < num_threads; i++)
        {
          pthread_join (thread[i], NULL);
          if (params[i].success && !success)
            {
              free (block);
              block = params[i].block;
              success = true;
            }
          else
            free (params[i].block);
        }
    }

  return block;
}

/* Create a genesis block to serve as the foundation */
char *
create_genesis_block (void)
{
  /* Start off with a bogus 0 hash */
  uint8_t hash[SHA_DIGEST_LENGTH];
  memset (hash, 0, SHA_DIGEST_LENGTH);
  char *hash_str = hash_to_string (hash);

  /* Create a genesis block with the current time */
  time_t now = time (NULL);
  size_t blocklen = 0;
  char *genesis =
    construct_block (0, (char*) hash_str, localtime (&now),
                     "Genesis Block", 13, 0, &blocklen);
  free (hash_str);

  /* Compute the hash of the genesis block and append it */
  unsigned char *md =
    SHA1 ((unsigned char *)genesis, (unsigned long) blocklen, hash);
  hash_str = hash_to_string (md);
  genesis[strlen ((char *)genesis)] = ' ';
  strncat ((char *)genesis, hash_str, SHA_DIGEST_LENGTH * 2);
  free (hash_str);

  return genesis;
}

/* Constrct a block of the following form:

     index.previous hash.[timestamp].data.nonce

   (Dots denote spaces for readability of the line.)
   Note that the dynamic allocation also includes space for this
   block's hash at the end, but those bytes are initialized to
   all 0. */
char *
construct_block (uint64_t index, char *previous,
                 struct tm *timestamp, char *data, size_t length,
                 uint64_t nonce, size_t *block_length)
{
  *block_length =
    16 + 16 + length + 4 +  // index, nonce, length, spaces
    SHA_DIGEST_LENGTH * 2 + // previous hash
    ASCTIME_LENGTH + 2;     // [ASCII timestamp]

  /* Allocate extra space for the block's hash, as well */
  char *block =
    calloc (*block_length + 2 + SHA_DIGEST_LENGTH * 2, sizeof (char));

  /* Both the index and the nonce are padded with leading 0s to be
     fixed width */
  snprintf (block, 17, "%016" PRIx64, index);
  block[16] = ' ';
  strncat (block, previous, SHA_DIGEST_LENGTH * 2);

  /* For the [timestamp] the asctime returns a trailing
     '\n' that must be overwritten with the ']' */
  block[SHA_DIGEST_LENGTH * 2 + 17] = ' ';
  block[SHA_DIGEST_LENGTH * 2 + 18] = '[';
  strncat (block, asctime (timestamp), *block_length - strlen (block));
  block[strlen (block) - 1] = ']';

  /* For readability of data, add a space before and after */
  block[strlen (block)] = ' ';
  strncat (block, data, length);
  block[*block_length - 17] = ' ';

  snprintf (block + *block_length - 16, 17, "%016" PRIx64, nonce);

  return block;
}

/* Convert a SHA hash to a readable hex string */
char *
hash_to_string (uint8_t *hash)
{
  size_t j;
  char *str = calloc (SHA_DIGEST_LENGTH * 2 + 1, sizeof (char));

  /* Each byte gets split into two hex digits */
  for (j = 0; j < SHA_DIGEST_LENGTH; j++)
    sprintf (&str[j*2], "%02x", hash[j]);
  return str;
}
```


/Appendix A[¶]
==============

//10.1. C Language Reintroduction[¶]
====================================

> “Program testing can be used to show the presence of bugs, but never to show their absence!”
> 
> Edsger Dijkstra

This book assumes some prior experience with the C programming language. However, the way that C is taught as a programming language and how it is used in the systems world are not always the same. As a simple example, consider the following trivial example of C code:

int maximum_size = 65536;

In many examples of systems code, this type declaration is too vague to be safe. Specifically, according to the C language specification, **there is no fixed size for the** `int` **type**. While an `int` can often be assumed (on modern laptops, desktops, or server machines) to be four bytes in size, it doesn’t have to be. On some embedded systems, for instance, an `int` is only two bytes in size. This declaration would then be a problem, because the value **65,536 cannot be represented as a two-byte signed integer** (which range from -32,768 to 32,767). Consequently, systems programmers typically use more explicit types that clearly indicate the exact size, such as `uint32_t` (unsigned 32-bit integer).

This Appendix is intended as a re-introduction to the C language as used by this text. It is not intended as a complete introduction for readers who have never used C before. For instance, there is no coverage here of standard control structures, such as loops or conditional statements. We also do not cover file-related functions (such as `fopen()`) that use pointers to FILE instances, as these functions are oriented toward ASCII-formatted text files; systems programming typically involves working with binary-formatted data instead. Instead, this Appendix focuses on intermediate or advanced features of C that are commonly used in systems programming. We start this Appendix with a discussion of how to consult documentation and debugging, as these are critical skills for systems programming.



//10.2. Documentation and Debugging[¶]
======================================

The Internet provides many ways to find documentation via web searches that lead to Stack Overflow. This approach can be helpful when the provided code is easily adaptable, but it can also be frustrating when the explanation is incomplete or incorrect. [[1]](#f51) In particular, solutions found in this way can often demonstrate what the correct approach is but not clearly identify the source of the error or misunderstanding. System manuals (referred as man pages to in the UNIX tradition) and command-line debuggers can become powerful tools when learned.

///10.2.1. Man Pages[¶]
-----------------------

Throughout this book, we have generally adhered to the POSIX.1-2017 specification for the C programming interface (also known officially as IEEE Std 1003.1, 2017 Edition and The Open Group Technical Standard Base Specifications, Issue 7). This specification is published freely online by The Open Group at:

> *   [https://publications.opengroup.org/standards/unix/c181](https://publications.opengroup.org/standards/unix/c181)
> *   [https://pubs.opengroup.org/onlinepubs/9699919799/](https://pubs.opengroup.org/onlinepubs/9699919799/)

Often, however, it is convenient or even necessary to access the system manual directly from the command line. For instance, recall that macOS is a UNIX OS based on BSD UNIX, while Linux is a UNIX-like OS that was developed independently. As such, there are slight differences between the C interfaces (particularly in relation to IPC) between the two. When these differences arise, it is necessary to consult the documentation that is specific to that particular OS. The `man` command-line utility provides that interface. Documentation for any C function or system call can (generally, with an exception described below) be found by typing `man` followed by the name of the function. To get started with `man`, you can read its own `man` page (use arrows to move up/down and press `'q'` to quit):

$ man man

One issue that arises with man pages is that there are naming conflicts between command-line utilities and C functions. These conflicts can be resolved by specifying the _section_ of the manual as an integer parameter before the function name. The most common sections of interest for systems programming are sections 1 (executable programs and command-line utilities), 2 (system calls provided by the kernel), and 3 (C standard library functions that are not system calls). As an example, compare the following two `man` page entries; the former brings up the page for the `bash mkdir` command, while the latter brings up the C function documentation):

```sh
$ man mkdir
$ man 2 mkdir
```

The header of the `man` page indicates a more precise naming convention to indicate the section under consideration. Using the examples above, the default behavior for man `mkdir` is to find `mkdir(1)`, the command-line utility, as opposed to `mkdir(2)`, the system call. Besides a header and footer that document the function’s section of the manual, the structure for `man` pages for C functions generally follows a specified format (may not contain all of these fields):

Field title

Purpose of the field

`NAME`

Quick description of the function or utility

`LIBRARY`

Which libraries must be linked to the compiled code (sometimes included as part of the `SYNOPSIS`)

`SYNOPSIS`

Required header `#include` statements and the function prototype

`DESCRIPTION`

A detailed description of what the function does, with key usage issues or considerations highlighted

`RETURN VALUES`

How to interpret possible values returned from the function

`ERRORS`

A list of constants that the function might assign to `errno` when an error occurs; these constants begin with `'E'` and are printed in brackets

`SEE ALSO`

Other functions that serve related purposes

`STANDARDS`

Which POSIX standard defines the function

`HISTORY`

When was the function introduced to UNIX

`BUGS`

Possible input sources that cause known bugs

Table A.1: Common fields of a `man` page

Beyond just providing information about what the function is or does, these sections provide hints for how to deal with errors. Specifically, when a problem arises with compilation or a run-time crash occurs, the following `man` page fields provide a quick solution:

> *   `SYNOPSIS` – Many functions rely on a particular `struct` declaration that is defined in a standard header file. This field enumerates all of the headers that are required to be set as `#include` statements to use the function. This field is also particularly helpful for making sure that arguments are being passed in the correct order.
> *   `LIBRARY` – Some functions require linking to additional libraries. For instance, the `pow()` function (used to calculate raising a base to some power) is in the C math library; some systems require explicitly linking executables with the `-lm` flag for `gcc`.
> *   `RETURN VALUES` – Some functions return a simple binary value to indicate success or failure, while others return a quantitative value (such as the number of bytes read). Treating return values incorrectly can lead to many bugs in systems code.
> *   `ERRORS` – Many functions use a generic return value to indicate an error has occurred. For instance, `read()` returns -1 to indicate that the requested operation failed; the global variable `errno` is set to explain why the failure occurred. In the case of `read(2)`, the possible errors include `EAGAIN` (file is marked for non-blocking I/O, but no data is ready to read), `EINTR` (the device was interrupted by a signal), `EINVAL` (the file descriptor was negative), or `EBADF` (the file descriptor is not open for reading). Comparing `errno` with these constants in the case of a failure to read can be an important clue in debugging.

One last note about `man` pages is that non-standard libraries typically provide an interface that can be consulted as above. The primary difference is that these libraries might be installed in places that the man command does not know to check. The `-M` flag can fix this problem. As an example, on macOS, the Homebrew [[2]](#f52) utility can be used to install OpenSSL. The default behavior of Homebrew is to install such libraries in `/usr/local/opt`, which is not searched by `man`. As such, the documentation for OpenSSL functions can be found by specifying the path as follows:

$ man -M /usr/local/opt/openssl/share/man EVP_EncryptInit

///10.2.2. Debugging with GDB[¶]
--------------------------------

One hurdle that all programmers must overcome is that errors and crashes indicate that there is a difference between what the code _does_ and what the code _should do_ (or rather, what the programmer _thinks_ the code should do). Debugging is the art of bridging this gap. When a program crashes or produces obviously incorrect output, the programmer must figure out where in the code this gap exists and what is causing it. The GNU debugger (GDB) is a powerful tool that can help to bridge this gap. Besides just stepping through code, GDB provides a number of built-in tools that can assist with debugging. More documentation on GDB can be found at:

> *   [https://www.gnu.org/software/gdb/documentation/](https://www.gnu.org/software/gdb/documentation/)

### 10.2.2.1. Breakpoints and Watchpoints[¶]

In the systems programming field, particularly with languages like C, the lack of exception handling makes debugging challenging. Often, the only indication that the error has occurred is the notoriously vague “Segmentation fault.” To get started with GDB, we will use it to examine [Code Listing A.1](#cla-1).

```cpp
/* Code Listing A.1:
   Code for tracing watchpoints and breakpoints in GDB 
 */

#include <stdio.h>

int
helper (int input)
{
  return input + 1;
}

int
main (int argc)
{
  int x = argc;
  int y = helper (x);
  x = 10;
  printf ("x = %d; y = %d\n", x, y);

  return 0;
}
```

To get started with GDB, we must compile our program with the `-g` flag for `gcc`, indicating that we want to include debugging symbols. Without debugging symbols, we cannot refer to variables or (in certain circumstances) functions by their names. In the example code above, the steps to compile it, run it (without GDB), and run it again (with GDB) are as follows (note that GDB requires passing the `--args` option if there are command-line arguments):

```sh
$ gcc -g -o watch watch.c
$ ./watch 5
x = 10; y = 3
$ gdb --args ./watch 5
GNU gdb (Ubuntu 8.1-0ubuntu3.2) 8.1.0.20180409-git
```

The following GDB session illustrates how to use _watchpoints_ and _breakpoints_. In GDB, we can set a watchpoint on a variable and be notified any time that variable’s value changes. (In fact, we can set watchpoints for any arbitrary memory location, but that is beyond our current scope.) We can set a breakpoint for a function name and be notified any time that function is called. In this first GDB session, we set a breakpoint for the `helper()` function and a watchpoint for the `x` variable in `main()`.

```sh
GNU gdb (Ubuntu 8.1-0ubuntu3.2) 8.1.0.20180409-git
[GDB license info omitted...]
Reading symbols from ./watch...done.
(gdb) start
Temporary breakpoint 1 at 0x661: file watch.c, line 12.
Starting program:  /home/csf/watch 

Temporary breakpoint 1, main (argc=2) at watch.c:12
13	  int x = 5;
(gdb) break helper
Breakpoint 2 at 0x555555554651: file watch.c, line 6.
(gdb) watch x
Hardware watchpoint 3: x
(gdb) watch input
No symbol "input" in current context.
(gdb)
```

For simplicity, we have omitted the several lines of introductory text that GDB shows regarding its license and documentation. The GDB command prompts are shown as bolded lines, where `(gdb)` indicates the prompt and our input is shown afterwards. On line 4 of this session, we use the `start` command to begin the program’s execution, which will pause when the `main()` function begins execution. When GDB executes lines of code as the result of a command, it displays the next line of code that will be executed in the future. So on line 9, GDB is indicating that it paused just before line 12 of [Code Listing A.1](#cla-1), which initializes the `x` variable.

Lines 10 and 12 of the GDB session set up the breakpoint and watchpoint with their respective commands, `break` and `watch`. Note that these commands require the target symbols to be visible by GDB when they are run. That means that the program must be compiled with debugging symbols included and the variable must be in scope. On line 14 of this GDB session, we cannot set a watchpoint for the `input` variable, which is only defined within the scope of the `helper()` function; GDB is currently executing inside the `main()` function scope.

The main commands to execute code in GDB are `step`, `next`, and `continue`. The `step` and `next` commands (not shown here) would simply execute the next line of code. In the session above, running either of these commands would execute the line 12 of [Code Listing A.1](#cla-1), which sets the variable `x` to 5. The difference between these two commands arises when the next line of code is a function call. The `step` command would _step into_ the called function’s body, whereas the `next` command treats the function call as an opaque box, executing the entire function as one step. Based on the previous session, we use the `continue` command, which allows the program to run until it is interrupted.

```sh
(gdb) continue
Continuing.

Hardware watchpoint 3: x

Old value = 0
New value = 2
main () at watch.c:13
14	  int y = helper (x);
(gdb) continue
Continuing.

Breakpoint 2, helper (input=2) at watch.c:6
7	  return input + 1;
(gdb) continue
Continuing.

Hardware watchpoint 3: x

Old value = 2
New value = 10
main () at watch.c:15
16	  printf ("x = %d; y = %d\n", x, y);
(gdb) print x
$1 = 10
(gdb)
```

In this session, we can observe the effects of the watchpoint and breakpoint that we set previously. When GDB encounters code that changes the value of a variable being observed with a watchpoint, it will pause the execution to indicate the old and new values (`x` has changed from 0 to 5), as well as the next line of code when execution resumes (line 13 to call the `helper()` function). By continuing a second time (line 10), this session encounters the breakpoint when the execution of `helper()` begins. Using breakpoints like this makes it easy to skip over large chunks of code and pausing right before the execution of a function that we wish to focus on. Line 15 of this session performs another continue, pausing on line 15 of the code; as `x`’s value changes from 5 to 10, the watchpoint is triggered again. Finally, note that we can also print in-scope variables at any point to observe their current value (line 24).

To illustrate the end of a GDB session, we execute the `continue` command two more times. The first continue causes an interrupt to arise when the `return` from `main()` occurs. Once this `return` happens, the variable `x` no longer exists, so the watchpoint can be deleted (lines 5 and 6). Note that this deletion implies GDB can distinguish between the variable being temporarily out of scope (as `x` is not in scope while `helper()` is executing) and being no longer needed. Lines 7 – 9 of this session can be ignored here, as it is simply GDB informing us (indirectly) that the source code containing the next line to execute cannot be found; this typically happens when GDB pauses during the executing of the C standard library, as these libraries are not compiled with debugging symbols included. The last continue (line 10) runs the program until it finishes, with line 12 informing us that the process has been terminated.

```sh
(gdb) continue
Continuing.
x = 10; y = 6

Watchpoint 3 deleted because the program has left the block in
which its expression is valid.
__libc_start_main (main=0x555555554659 <main>, argc=2, argv=0x7fffffffea18, init=<optimized out>, 
    fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffea08) at ../csu/libc-start.c:344
344	../csu/libc-start.c: No such file or directory.
(gdb) continue
Continuing.
[Inferior 1 (process 16005) exited normally]
(gdb)
```

### 10.2.2.2. Backtrace[¶]

One significant source of frustration for systems programming is the lack of information regarding segmentation faults. _Backtrace_ is an essential tool for debugging segmentation faults, such as the code shown in [Code Listing A.2](#cla-2).


```cpp
/* Code Listing A.2:
   Dereferencing a NULL pointer to trigger a segmentation fault
 */

void
segfault (int value)
{
  int *nullptr = (int *) value;
  *ptr = 5;
}

int
main (void)
{
  segfault ();
  return 0;
}
```

Compiling and running this program would produce the following unhelpful results: [[3]](#f53)

```sh
$ gcc -g -o segfault segfault.c
segfault.c: In function ‘segfault’:
segfault.c:4:18: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast]
   int *nullptr = (int *) value;
                  ^
$ ./segfault
Segmentation fault
```

Diagnosing and fixing segmentation faults can be very difficult. One reason is that the message above provides no information about what line of code was executing when the segmentation fault occurred. Many programmers, particularly novices, try to address this with “`printf()` debugging,” the ad hoc approach of adding `printf()` statements to the code. This approach is inefficient and ineffective for a number of reasons. First, it requires re-editing and re-compiling the program. Second, adding any code, including `printf()`, **changes the program**. In some cases, this seemingly innocuous change will cause the segmentation fault to disappear—but only if the `printf()` stays in the code; removing the `printf()` brings the segmentation fault back. Lastly, adding code increases the opportunity for bugs to cause misleading results. For instance, changing the `main()` of [Code Listing A.2](#cla-2) to that shown in [Code Listing A.3](#cla-3) will produce identical results; without a `'\n'`, the `STDOUT` buffer will not be flushed and the message in the `printf()` will never be displayed to the screen.

```cpp
/* Code Listing A.3:
   Printing without newlines may prevent the display of the message
 */

int
main (void)
{
  printf ("Hello from main()");
  segfault ();
  return 0;
}
```

GDB’s `backtrace` utility provides a more efficient and effective approach to debugging. Line 10 of this GDB session informs us that GDB encountered a segmentation fault (`SIGSEGV)` while executing line 5 of [Code Listing A.2](#cla-2). Line 11 also provides information about the function that was executing at the time (`segfault()`), and what its current input arguments were (`value=0`). That information might be enough if the function is only called once or if the relationship between the arguments and the segmentation fault is clear. However, it is often the case that more context is needed; in order to debug the segmentation fault, we need to know what specific call—including the arguments passed and where it was called from—produced these results. This information is provided by `backtrace`, which shows the sequence of function calls that led to the segmentation fault.

```sh
(gdb) start
Temporary breakpoint 1 at 0x61b: file segfault.c, line 11.
Starting program: /home/csf/segfault 

Temporary breakpoint 1, main () at segfault.c:11
11      segfault (0);
(gdb) continue
Continuing.

Program received signal SIGSEGV, Segmentation fault.
0x000055555555460e in segfault (value=0) at segfault.c:5
5      *nullptr = 5;
(gdb) backtrace
#0  0x000055555555460e in segfault (value=0) at segfault.c:5
#1  0x0000555555554625 in main () at segfault.c:11
(gdb)
```

Lines 14 and 15 show that the segmentation fault occurred while executing line 5 of [Code Listing A.2](#cla-2). This line exists within the `segfault()` function, which was specifically called with the argument `value=0`. The call to `segfault(0)` occurred on line 11 of the code, which exists as part of the call to `main()`. Unless the contents of the stack have become corrupted (which go beyond our discussion here), this history will show the full trace [[4]](#f54) back to the `main()` context. That information provides more context that the programmer can use to determine what might be the root cause of the segmentation fault.

### 10.2.2.3. Tracing multiple processes[¶]

Debugging segmentation faults—and other bugs—with multiple processes can be particularly challenging. For one thing, the asynchronous timing of the execution might intersperse output messages in odd ways. Even more frustrating, messages written to the `STDERR` stream are also lost unless the output streams are specifically linked. Consider the example shown in [Code Listing A.4](#cla-4), which uses a slight variant on the same `segfault()` function from [Code Listing A.2](#cla-2).

```cpp
/* Code Listing A.4:
   The child's "Segmentation fault" will not appear on screen
 */

#include <assert.h>
#include <stdio.h>
#include <sys/wait.h>
#include <unistd.h>

void
segfault (int *ptr)
{
  *ptr = 5;
}

int
main (void)
{
  int *nullptr = NULL;
  pid_t child = fork ();
  assert (child >= 0);

  if (child == 0)
    segfault (nullptr);
  else
    wait (NULL);

  printf ("Goodbye\n");
  return 0;
}
```

As with the previous cases, running this code should result in a segmentation fault. However, the results would actually look as follows:

```sh
$ gcc -g -o forkfault forkfault.c
$ ./forkfault
Goodbye
```

In this case, the parent process calls `fork()` to create a child process. Based on the nondeterministic timing of process scheduling, either the parent continues executing or the child runs first. This nondeterminism is not an issue here, though; if the parent runs, it calls `wait()` until the child runs to completion. Consequently, when the parent executes the `printf()` call on line 24, the child has been guaranteed to run to completion; that child process definitely terminated with a segmentation fault because of the code on line 13. The reason the child’s error message does not appear on the terminal is that the child process’s `STDERR` output stream is thrown away.

To debug these kinds of errors in multiple processes, GDB allows you to specify which process to follow when a `fork()` occurs. Specifically, the default behavior is that GDB will follow the parent; if you use next or continue to move past a `fork()`, GDB will show a message about a new process but continue executing the parent. To change this behavior, the set command can be used to change GDB’s `follow-fork-mode` setting as shown below. Based on this new setting, GDB will switch to the child process after the call to `fork()`; additional GDB commands will be sent to the child process rather than the parent.

```sh
(gdb) start
Temporary breakpoint 2 at 0x6f7: file forkfault.c, line 15.
Starting program:  /home/csf/forkfault 

Breakpoint 1, main () at forkfault.c:15
15      int *nullptr = NULL;
(gdb) set follow-fork-mode child
(gdb) continue
Continuing.
[New process 9666]

Thread 2.1 "forkfault" received signal SIGSEGV, Segmentation fault.
[Switching to process 9666]
0x0000555555554726 in segfault (ptr=0x0) at forkfault.c:9
9      *ptr = 5;
(gdb) backtrace
#0  0x0000555555554726 in segfault (ptr=0x0) at forkfault.c:9
#1  0x000055555555477e in main () at forkfault.c:20
(gdb) 
```

The full GDB manual provides much more information about debugging multiple processes. One particularly useful command is `info proc`, which can be used to examine information about the status of a process, including any memory map regions, memory usage, number of threads, and many others. Information on `info proc` can be found at:

> *   [https://sourceware.org/gdb/current/onlinedocs/gdb/Process-Information.html](https://sourceware.org/gdb/current/onlinedocs/gdb/Process-Information.html)

[[1]](#id1)

The use of web searches and sites like Stack Overflow also raise serious ethical concerns about attribution and plagiarism. Reusing others’ code, particularly without properly citing these sources or documenting that the use is permitted, can lead to legal ramifications in professional practice and to academic misconduct charges.

[[2]](#id2)

Homebrew can be installed from `https://brew.sh`. Once Homebrew is installed, the OpenSSL library can be installed by running `brew install openssl`.

[[3]](#id5)

First, note that the compiler tries to help us avoid the segmentation fault. Warnings are quite frequently an indication that the code is syntactically legal but semantically wrong. Technically, we’re allowed to do something, but it’s probably not going to work out correctly.

[[4]](#id9)

This type of trace might look familiar to readers with experience in other languages, such as Java or Python. Those languages include built-in exception-handling mechanisms, using the same mechanisms that GDB relies on, that provide this information for free.



//10.3. Basic Types and Pointers[¶]
===================================

C defines several basic integer types, as illustrated in [Table A.2](#tblA-2). For each of these types, there is both a signed (the default) and unsigned version (considered distinct types). [[1]](#f55) (Note that `short`, `long`, and `long long` can also be called `short int`, `long int`, and `long long int`, respectively.) For each type, there is a designated minimum range of values that the type must be capable of storing. The ranges are shown in the middle two columns. When printing a variable of the specified type, there are both `%d` (signed) and `%u` (unsigned) format specifiers with added `h` (half) or `l` (long) characters to vary the size. The char type can use %c to print the ASCII interpretation of the byte, whereas the `%hhd` and `%hhu` strings will produce the integer format.

    Type       Unsigned range (minimum)   Signed range (minimum)     Format strings
    char       [0, 255]                   [-127, 127]                `%c`, `%hhd`, `%hhu`
    short      [0, 65535]                 [-32767, 32767]            `%hd`, `%hu`
    int        [0, 65535]                 [-32767, 32767]            `%d`, `%u`
    long       [0, 4294967295]            [-2147483647, 2147483647]   `%ld`, `%lu`
    long long  [0, 18446744073709551615]  [-9223372036854775807, 9223372036854775807]  `%lld`, `%llu`

Table A.2: C primitive integer types

🔍 Note

* * *

The basic `char` type is built on the ASCII character standard. The original version of this standard uses seven bits (the eighth bit was used as a parity check to detect errors), allowing for the representation of up to 127 unique characters (not including the reserve null byte). This limited size is clearly not sufficient to support international communication, which requires thousands of unique characters. ASCII has been succeeded by other standards, with Unicode being the current international standard. Unicode supports 16-bit (UTF-16) and 32-bit (UTF-32) character representations, exceeding the range of the `char` type. C has since introduced the `wchar_t` (wide character) to support these larger character representations.

There are some important points to note about the ranges shown in [Table A.2](#tblA-2). First, the table does not contain a typo; the `short` and `int` types are required to cover the exact same ranges. These ranges can be represented in 16 bits for either type. However, in a typical modern system, a `short` is a 16-bit (2 byte) variable, whereas an `int` is 32-bit (4 bytes). To add to the confusion, the ranges required for long can be represented in 32 bits, but a `long` is typically 64-bit (8 bytes). And finally, the range for `long long` can be represented in 64 bits, which is the typical size (8 bytes) for this type. In other words, `short` and `int` variables are required to represent the same range of values, but they are typically different sizes; on the other hand, `long` and `long long` are required to represent different ranges of values, but they are typically the same size!

Another, subtle aspect of these ranges is that there seems to be a discrepancy in the number of signed and unsigned values. For instance, take the `char` type; the unsigned version can represent 256 different numbers (0 through 255), whereas the signed version can only represent 255 (-127 to 0 to 127). In all of these cases, the signed version must support one’s complement integer representation in contrast to the two’s complement arithmetic typical in modern architectures. As such, the value -128 (0x80) can typically be used as a `char` value, but this is technically outside the range for this type.

🔍 Note

* * *

Until the C99 standard was published, C had no built-in Boolean variable type to indicate true or false. Instead, C used an integer for this purpose, with the convention that 0 means _false_ and any non-zero value means _true_. This convention is still frequently manipulated, particularly as a short-hand for condition checks. For instance, strcmp() function, which is used to compare two strings, returns 0 if the strings are identical; if not, `strcmp()` returns either 1 or -1. Based on the convention that 0 means false, it is common to see code written as follows:

if (! strcmp (str1, str2))
  printf ("Strings are identical\n");

If the strings `str1` and `str2` are identical, `strcmp()` returns 0 (false). The logical negation operator (`!`) then flips this value; if the strings are identical, the condition in the if-statement is true. If the strings are not identical, `strcmp()` will return either 1 or -1, both of which evaluate to true; applying the `!` flips this value and the condition evaluates to false, skipping the body of the if-statement. Logically, the condition shown above is equivalent to testing `if (strcmp (str1, str2) == 0)`.

C99 introduced a new header file, `stdbool.h`. This header defines a `bool` type, as well as the constants `true` and `false`. This new type is generally preferred, as it makes for more readable code.

///10.3.1. C99 Fixed-width Types[¶]
-----------------------------------

In systems programming, this sort of inconsistent definition can lead to serious mistakes. If an embedded microcontroller (such as a tiny wireless sensor monitoring a car’s velocity) needs to send data to a centralized controller, it would be beneficial for both devices to agree on the size of various types of data. To fix this, systems code typically uses explicitly named types. These types, defined in `stdint.h`, are shown in [Table A.3](#tblA-3).

    Signed type  INT_n_MIN      INT_n_MAX             Unsigned type  UINT_n_MAX
    int8_t       -128           127                   uint8_t        255
    int16_t      -32768         32767                 uint16_t       65535
    int32_t      -INT32_MAX-1   2147483647            uint32_t       4294967295U
    int64_t      -INT64_MAX-1   9223372036854775807LL uint64_t       18446744073709551615ULL

Table A.3: C99 fixed-width integer types

The type name is constructed as `int`_n_`_t` or `uint`_n_`_t`, where the n is one of 8, 16, 32, or 64. For each size, there is a signed minimum (`INT`_n_`_MIN`), a signed maximum (`INT`_n_`_MAX`), and an unsigned maximum (`UINT`_n_`_MAX`). (The unsigned minimum is 0 for all sizes.) Note that `INT32_MIN` and `INT64_MIN` cannot be represented directly because of the way that C defines numeric constants. Instead, they have to be written in relation to the corresponding `INT`_n_`_MAX` values, but they are the values -2,147,483,648 and -9,223,372,036,854,775,808, respectively. Also observe that the large values need to be appended with U, LL, or ULL so that the C compiler will interpret the values correctly.

[Code Listing A.5](#cla-5) illustrates a few key points about using these standard and fixed types. Lines 6 – 10 declare a `char` variable and print it both in ASCII (using `%c`) and as an integer value (in decimal, octal, and hexadecimal format). This code would work identically even if the declaration of the achar variable was changed to `uint8_t`, the 8-bit unsigned integer type. Lines 13 – 16 demonstrate the standard printing for an `int` type.

```cpp
/* Code Listing A.5:
   Using and printing various integer types
 */

/* Use char, printing it in 4 formats */
char achar = 'A'; 
printf ("achar has value %c (character)\n", achar);
printf ("achar has value %hhd (dec)\n", achar);
printf ("achar has value %hho (oct)\n", achar);
printf ("achar has value %04hhx (hex)\n\n", achar);

/* Use basic int type, printing value in 3 formats */
int intval = 42;
printf ("intval has value %d (dec)\n", intval);
printf ("intval has value %o (oct)\n", intval);
printf ("intval has value %04x (hex)\n\n", intval);

/* Use 32-bit signed integer, printing value */
int32_t i32val = 42;
printf ("i32val has value %d (dec)\n", i32val);
printf ("i32val has value %" PRId32 " (dec)\n", i32val);
printf ("i32val has value %" PRIo32 " (oct)\n", i32val);
printf ("i32val has value %04" PRIx32 " (hex)\n\n", i32val);

/* Use 64-bit unsigned integer, printing value */
uint64_t ui64val = 42;
printf ("ui64val has value %d (dec)\n", ui64val);
printf ("ui64val has value %lld (dec)\n", ui64val);
printf ("ui64val has value %" "lld" " (dec)\n", ui64val);
printf ("ui64val has value %" PRId64 " (dec)\n", ui64val);
printf ("ui64val has value %" PRIo64 " (oct\)\n", ui64val);
printf ("ui64val has value %04" PRIx64 " (hex)\n\n", ui64val);
```

The remaining lines (19 – 31) demonstrate similar handling using the fixed-width integer types. These format strings initially look rather confusing, because they exploit a feature of string constants that is often overlooked (or not taught) to those new to C: consecutive string constants are concatenated automatically by the compiler. For example, the code `printf ("hello world");` is treated identically to `printf ("hello "  "world");` by the compiler. (The several white spaces between the strings are ignored and used here only for effect.)

The format specifiers for these integer types are defined in the `inttypes.h` header. Within this file, there is a preprocessor definition that will translate `PRId32` or `PRIx64` into the appropriate character strings (`"d"` and `"llx"` in this case) from [Table A.2](#tblA-2) for the target machine. In general, the format specifier has the format `PRI`_fn_, where _n_ is the size (8, 16, 32, or 64 bits) and _f_ is the desired integer format (`d` or `i` for signed decimal, `u` for unsigned decimal, `o` for octal, `x` or `X` for hexadecimal). Consequently, the C compiler would treat lines 29 and 30 in [Code Listing A.5](#cla-5) identically to line 28. The `PRId64` is translated by the processor into `"lld"`, so lines 29 and 30 literally become identical when the compiler begins to work with the code; the compiler then transparently concatenates the string constants to create the string shown in line 28. When compiling this code, observe that line 27 produces a warning, as the variable and the corresponding format specifier are specifying different sizes (64-bit for `uint64_t` and 32-bit for `%d`).

🐞🐛🐌 Bug Warning

* * *

The quotation marks around the C99 format strings is a common point of confusion that can lead to frustrating compiler errors. Specifically, note that there is a quote after the `%` character that begins the format specifier. This quote is necessary, as it terminates the string constant up to that point. Any additional characters that modify the format specifier (such as 0 to indicate leading zeros, a number to specify a fixed display width, or a `-` to indicate left-justification within that space) must appear before the quotation mark, as these are not part of the C99 format processing.

One common task with working with C types is using the `sizeof()` operator [[2]](#f56) to examine the number of bytes allocated for a variable or a type. In the case of a type, `sizeof()` indicates how many bytes _would_ be allocated for such a variable instance. [Code Listing A.6](#cla-6) demonstrates the use of `sizeof()`, based off of the variable declarations in [Code Listing A.5](#cla-5). As the value returned from `sizeof()` is considered to be of type `size_t`, the `%zd` format specifier is required when printing these values. On most systems, lines 7, 8, 12, and 13 will all produce the same size result (4), as `int` and `int32_t` are typically identical.

```cpp
/* Code Listing A.6:
   Using sizeof() for introspection of variable and type sizes
 */

/* Use sizeof to get the size of a variable name */
size_t size = sizeof (intval);
printf ("intval has size %zd\n", size);
printf ("i32val has size %zd\n", sizeof (i32val));
printf ("ui64val has size %zd\n\n", sizeof (ui64val));

/* Use sizeof to get the size of a variable type */
printf ("Primitive int type has size %zd\n", sizeof (int));
printf ("C99 int32_t type has size %zd\n", sizeof (int32_t));
printf ("C99 int64_t type has size %zd\n", sizeof (int64_t));
printf ("Primitive size_t type has size %zd\n", sizeof (size_t));
```

///10.3.2. Pointer Basics[¶]
----------------------------

As this Appendix is intended as a re-introduction to C, readers are generally assumed to have had some prior exposure to key features of the language, including pointers. However, few concepts in programming cause as many problems or as much frustration as pointers; as such, we will reiterate the basic concept here, particularly as it relates to the preceding discussion on `sizeof()`. Later in this Appendix, we will also introduce some intermediate or advanced techniques of working with pointers. To start, we emphasize the following key idea:

> A pointer is a variable that stores an address.

When a pointer variable is declared, its declaration generally indicates what type of data is stored at that address. This target type is generally included when discussing a pointer’s type, but it does not change the fact that the pointer stores an address. For instance, by declaring `int *x = NULL;`, we have declared the variable named `x` to be a _pointer to an_ `int`. That is, we might write that `x` is an `int*`. **It cannot be overstated that this statement is different from saying that** `x` **is an** `int`. Part of what makes this subtly complex is that addresses are, fundamentally, integer values that have a particular meaning. [Code Listing A.7](#cla-7) illustrates this confusion in a common manner.

```cpp

/* Code Listing A.7:
   A subtle segfault bug
 */

int ival = 10;
int *iptr = &ival; // iptr points to ival

*iptr = 0; // sets ival to 0
printf ("iptr = %p; *iptr = %d\n", iptr, *iptr);

iptr = 0; // makes iptr point to nowhere (i.e., NULL)
/* next line causes a segfault since iptr is now NULL */
printf ("iptr = %p; *iptr = %d\n", iptr, *iptr);
```

Lines 5 and 6 declare two variables, one (`ival`) has the type `int` and the other (`iptr`) has the type `int*`. The value of `ival` is initialized to 10, whereas the value of `iptr` is the address of `ival` (`&ival`). That is, `iptr` is initialized so that it points to `ival`. We might also say that `iptr` is an indirect reference to `ival`. Line 8 dereferences the pointer (i.e., follows the arrow to the data pointed to) and sets that portion of memory to 0; specifically, by dereferencing the pointer, line 8 is changing the value of `ival` from 10 to 0. As such, line 9 runs without error, printing the value of `iptr` (which is the address of `ival`) along with the new value of `ival` (because we are again dereferencing the pointer).

Line 11 is fundamentally different, however. Since line 11 is not dereferencing the pointer (note that there is no `*` on this line), it is changing the value of `iptr` itself. Recall from above that a pointer stores an address; **if we change a pointer’s value, we are changing the memory address that we are pointing to**. By setting `iptr` to 0, the pointer now points to address 0, which is nothingness. That is, `iptr` no longer points to `ival`; it is now a `NULL` pointer (`NULL` is literally the constant 0 in C!). As such, line 13 causes a segmentation fault, because it tries to deference a null pointer (`*iptr`).

A common misunderstanding about this example is that setting the pointer to `NULL` does not necessarily cause an error. [Code Listing A.8](#cla-8) extends [A.7](#cla-7), with the exception that a new line is added just before the final printf() statement. In [Code Listing A.7](#cla-7), the code set `iptr` to 0 and immediately called `printf()`; in [Code Listing A.8](#cla-8), line 7 occurs between these steps, setting `iptr` to point back to `ival`. In restoring `iptr`’s value to point to a valid location, [Code Listing A.8](#cla-8) removes the segmentation fault that occurred in [A.7](#cla-7). In short, setting a pointer’s value to `NULL` does not cause a segmentation fault; the fault only occurs once the pointer is dereferenced.

```cpp
/* Code Listing A.8:
   Restoring successful execution to Code Listing A.7
 */

iptr = 0; // makes iptr point to nowhere (i.e., NULL)

iptr = &ival; // makes iptr point to ival again

/* next line no longer segfaults */
printf ("iptr = %p; *iptr = %d\n", iptr, *iptr);
```

To return to the discussion of type sizes, a common misunderstanding arises when we combine the concepts of pointers and `sizeof()`. Recall two facts: `sizeof()` returns the number of bytes required for a variable of a particular type, and all pointers store addresses (and nothing else!). As such, calling sizeof() on a pointer will always return the same answer: the size of an address. **From the perspective of** `sizeof()`**, pointer declarations are all identical**. [Code Listing A.9](#cla-9) demonstrates this fact. The variables `ival` and `cval` have distinct sizes (typically, 4 bytes for `ival` as an `int` and 1 byte for `cval` as a `char`). In contrast, `iptr` and `cptr` have different types, but they are the same size (8 bytes, assuming this code runs on a 64-bit CPU architecture). As such, the assertions on lines 17 and 18 are both true.

```cpp
/* Code Listing A.9:
   Using sizeof() with pointers produces different results from basic types
 */

int ival = 10;
int *iptr = &ival;
printf ("Size of ival = %zd\n", sizeof (ival));
printf ("Size of iptr = %zd\n", sizeof (iptr));
printf ("Size of *iptr = %zd\n\n", sizeof (*iptr));

char cval = 'a';
char *cptr = &cval;
printf ("Size of cval = %zd\n", sizeof (cval));
printf ("Size of cptr = %zd\n", sizeof (cptr));
printf ("Size of *cptr = %zd\n", sizeof (*cptr));

assert (sizeof (int *) == sizeof (char *));
assert (sizeof (int) != sizeof (char));
assert (sizeof (*iptr) != sizeof (*cptr));
```

Lastly, observe that `sizeof()` can be called successfully on pointer dereferences, as shown in lines 9 and 15. In this case, the size returned is the number of bytes for the referenced type, not the pointer. Line 9 will print that the size is 4, as that is the typical size for an `int` (which is the type of `*iptr`, also known as `ival`). Line 15 will print that the size is 1, given that `*cptr` (also known as `cval`) is a `char`. Consequently, the assertion on line 19 holds, as `sizeof(*iptr)` gets the size of an `int`, whereas `sizeof(*cptr)` is the size of a `char`.

Given that a pointer is a variable that stores an address, an intuitive follow-up question is whether or not a pointer can store the address of a pointer. The answer is yes, and this is common in a variety of circumstances that we will examine later in this Appendix. [Code Listing A.10](#cla-10) illustrates an example of using pointers to pointers. Line 6 starts by declaring `iptr` (abbreviation of “integer pointer”) as an `int*` and making it point to `ival`. Line 7 takes this a step farther, making `pptr` (“pointer pointer”) be a pointer to a pointer to an `int` (`int**`), and initializing it to point to `iptr`; this initialization is correct, as `pptr` is pointing to an `int*`. Line 8 introduces the use of void with a pointer structure; specifically, the declaration indicates thenslate">vptr (“`void` pointer”) is a pointer to a pointer to a pointer to _something_, but the type of that something is unknown.

```cpp
/* Code Listing A.10:
   Multiple layers of indirection with pointers to pointers
 */

int ival = 10;
int *iptr = &ival;
int **pptr = &iptr;
void ***vptr = (void***) &pptr; // this is okay

/* Multiple layers of redirection to the same place */
printf ("%d = %d = %d\n", ival, *iptr, **pptr);
// NOT ALLOWED: printf ("%d\n", ***vptr);

/* The size of any pointer is always the same */
assert (sizeof (iptr) == sizeof (pptr));
assert (sizeof (iptr) == sizeof (vptr));

/* Just for the full, ridiculous effect */
assert (sizeof (char*) == sizeof (int******));
assert (sizeof (void*) == sizeof (double***));
```

[Figure 10.3.4](BasicTypes.html#cpointer) illustrates the relationship of the pointers as declared in lines 5 – 8. Line 11 prints the value 10 multiple times by dereferencing the pointers as needed. Specifically, line 11 starts by printing `ival` directly, and the value of `ival` is 10. Then, the value of `ival` is printed again by dereferencing `iptr`; since `iptr` is an `int*` and the value of `iptr` is `&ival`, dereferencing it once (as `*iptr`) ends up at the `int` variable `ival`. Similarly, the value of `ival` is printed one more time, as `pptr` is dereferenced twice (`**pptr`); the first dereference `*` follows the arrow from `pptr` to `iptr` and the second `*` follows the arrow from `iptr` to `ival`.

![The pointer structure of Code Listing A.10](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.A.1.png)

Figure 10.3.4: The pointer structure of Code Listing A.10

The comment on line 12 indicates that we cannot simply dereference `vptr` three times to get the same value. The problem is that dereferencing requires knowing the type that we are pointing to. The `void` pointer type—regardless of how many layers of indirection are present—serves the same purpose in C as the `Object` class in Java; it acts as a catch-all type for passing around pointers. We could successfully dereference `vptr` by first casting it (i.e., by using `***((int***)vptr)`).

Lines 14 – 20 highlight another key insight about pointers: **All pointers have the same size**. It is a very common misunderstanding to think that the type of the pointer (`int*` vs. `char*`) has some influence on the size of the pointer. This is not true. All pointers are allocated exactly the amount of space required to store an address on the target architecture. The pointer’s type is only important for dereferencing the pointer. Note that the assertion on line 20 also holds. A common misunderstanding is that `void*` either has no size or it points to nothing; neither statement is true (unless the pointer is specifically initialized to `NULL`, then the latter is true). The `void*` type means that we have a pointer variable and its size is the same as any other pointer, but we do not know the size of what it is pointing to. We can cast a `void*` to be any other type of pointer; doing so would indicate how to use the bytes being pointed to when dereferencing the pointer.

[[1]](#id1)

To be pedantic, `char` and `signed char` are distinct types in C. The `char` type is defined as the smallest addressable unit for containing a machine’s basic character set; as this basic character set is ASCII for modern systems, the smallest addressable unit is an 8-bit byte. The `signed char` is defined in terms of the range shown.

[[2]](#id5)

The `sizeof()` operator looks like a function and generally behaves like one, taking an input argument and return a value. This appearance is deceiving, though, as `sizeof()` does not execute according to the standard function calling semantics. Instead, `sizeof()` is an operator that is built into the language, like `+` to indicate addition.



//10.4. Arrays, Structs, Enums, and Type Definitions[¶]
=======================================================

As with other common typed languages (such as Java), C supports arrays of other types. When declaring an array, the compiler must be able to determine the exact length. One way to do this is to specify the length inside the brackets of the declaration (such as `int array[10];`). Another way is to provide an explicit initialization array. [Code Listing A.11](#cla-11) demonstrates this second technique, omitting the length from inside the brackets on line 6 (although it could be included here).

```cpp
/* Code Listing A.11:
   C array names are implicitly pointers to their first entry
 */

/* Instantiate an array and print its starting address */
uint32_t data[] = { 1, 2, 3, 4, 5 };
printf ("data     = %p\n", data);
printf ("&data    = %p\n", &data);
printf ("&data[0] = %p\n\n", &data[0]);
```

[Code Listing A.11](#cla-11) illustrates a key aspect of the relationship between arrays and pointers: **Array names are implicitly pointers to their first element**. To see this, consider lines 7 – 9; line 7 prints the _value_ of `data`, line 8 prints the address of data, and line 9 prints the _address_ of `data[0]`. When this program is run, these three lines produce the same value as the output. The convention in C is that any array variable is an alias for the starting address. All three of these notations can be used (and frequently are) depending on the context and style preferences of the programmer.

[Code Listing A.12](#cla-12) extends the previous example to explore the relationship between arrays and pointers further. Line 7 starts by declaring a pointer and initializing it to point to the array. On this line, we could also initialize `u32ptr = &data[0]`, but (due to convoluted type checking rules) it is a compiler warning to initialize `u32ptr = &data` (despite the fact that data and &data are the same). Line 8 sets the `walker` pointer in the same way.

```cpp
/* Code Listing A.12:
   C arrays and pointer arithmetic work identically
 */

/* Make two pointers to data, which also means they point to
   the first element in the array */
uint32_t *u32ptr = data; // &data[0] is also ok, &data is not
uint32_t *walker = data; // a second pointer to data

/* Loop through the elements, observing that array bracket
   notation and pointer arithmetic behave identically; after
   each time through the loop, advance the walker pointer */
for (size_t i = 0; i < 5; i++, walker++)
  {
    printf ("data[%zd]   address %p and value %08" PRIx32 "\n",
            i, &data[i], data[i]);
    printf ("u32ptr[%zd] address %p and value %08" PRIx32 "\n",
            i, &u32ptr[i], u32ptr[i]);
    printf ("u32ptr+%zd  address %p and value %08" PRIx32 "\n\n",
            i, u32ptr + i, *(u32ptr + i));
    printf ("walker    address %p and value %08" PRIx32 "\n\n",
            walker, *walker);
  }
```

Lines 13 – 23 in [Code Listing A.12](#cla-12) demonstrate the equivalency between array dereferencing and _pointer arithmetic_. That is, when C performs an operation like `u32ptr + 5`, it is not simply the value of `u32ptr` plus the number 5 in standard arithmetic; instead, `u32ptr + 5` requires taking the value of `u32ptr` (which is an address) and adding 5 times the size of what `u32ptr` points to. In this case, if u32ptr stores the address `0x7ffee0000720`, `u32ptr + 5` would add 20 to this value (since `u32ptr` is a pointer to 4-byte `uint32_t` values), yielding `0x7ffee0000734`. In general terms, for an arbitrary pointer `ptr`, `&ptr[n]` and `ptr+n` are identical for any integer `n`; `ptr[n]` and `*(ptr+n)` also yield the same value. (As the bracket notation tends to be more familiar, many C programmers use it whenever pointer arithmetic is needed.)

Lines 21 and 22 do not require adding any value to the `walker` pointer so that it accesses the correct location. Instead, walker is set up to _walk through_ the array, accessing one element at a time. Specifically, line 8 initializes `walker` to point to the first element, and the increment field of the `for`-loop causes walker to advance after each iteration (line 13). (Observe that it is possible to specify multiple increments in a `for`-loop, separated by a comma as shown with `i++`, `walker++`.) A subtle aspect of this increment is that it does not simply add 1 to the address `walker` is pointing to; rather, just like the additions on lines 18 and 20, `walker++` will increment the address by the size of a `uint32_t`. Consequently, this incrementing style is a common technique to use a pointer to traverse through the elements of an array.

Although pointer variables can be treated as arrays, the reverse is not true. That is, even though an array name is implicitly a pointer to the first element of the array, we cannot use pointer notation for a variable declared as an array. In [Code Listing A.11](#cla-11) and [A.12](#cla-12), trying to access `*data` or `*(data+i)` would produce a compiler error.

[Code Listing A.13](#cla-13) illustrates a slight variation on [Code Listing A.12](#cla-12). In this scenario, `data` is declared as an array of 32-bit values. Since the array consists of two of these values, the array occupies eight consecutive bytes of memory. Line 12 declares a pointer like before, but the pointer is a `uint8_t*`, so it points to 8-bit values. Lines 13 – 19 will traverse through all of the bytes of the `data` array, but accessing it a byte at a time instead of just examining the two 32-bit entries. One advantage of this approach is that using a `uint8_t*` pointer provides a mechanism to explore the endianness of multi-byte integers. In this example, `data[0]` stores the 32-bit value `0x01020304`, spread across four memory locations. Assuming this runs on a little-endian architecture (such as x86), 0x04 is stored at the first of these four byte locations; consequently, `u8ptr[0]` accesses `0x04`, `u8ptr[1]` access `0x03`, and so on. Once the loop would get to `u8ptr[4]`, the result would be 0x08, which is the first byte stored for `data[1]`.


```cpp
/* Code Listing A.13:
   Using a pointer to traverse an array of a different size
 */

/* Instantiate an array of two 32-bit integers */
uint32_t data[] = { 0x01020304, 0x05060708 };
printf ("data starts at %p\n", data);
printf ("data[0] address %p and value %08" PRIx32 "\n", &data[0], data[0]);
printf ("data[1] address %p and value %08" PRIx32 "\n\n", &data[1], data[1]);

/* Make a pointer to the first byte of the array */
uint8_t *u8ptr = (uint8_t *)data;
for (size_t i = 0; i < 8; i++)
  {
    printf ("u8ptr[%zd] address %p and value %02" PRIx8 "\n",
            i, &u8ptr[i], u8ptr[i]);
    printf ("u8ptr+%zd  address %p and value %02" PRIx8 "\n\n",
            i, u8ptr + i, *(u8ptr + i));
  }
```

🐞🐛🐌 Bug Warning

* * *

It is critical to understand that C does not explicitly store the length of an array anywhere, so there is no way to learn this information for a pointer to an arbitrary array. Consider the `data` and `ptr` variables as declared below. By examining this code, we can determine that the `data` array takes up 16 bytes and consists of four consecutive 32-bit values; i.e., this portion of source code makes it clear that the length of the array is four.

```cpp
uint32_t data[4]; // an array of four 4-byte ints
uint32_t *ptr = data;

printf ("sizeof(data) = %zd\n", sizeof (data)); // 16, NOT 4
printf ("sizeof(ptr) = %zd\n", sizeof (ptr)); // 8, NOT 4

printf ("number of elemenst = %zd\n", sizeof (data) / sizeof (uint32_t)); // SUCCESS!
```

A very common mistake is to try to use `sizeof()` as shown on lines 4 and 5. With one exception (shown on line 7), `sizeof()` **cannot be used to determine the length of an array**. Recall that `sizeof()` returns the number of bytes for a variable; it is not aware of the subdivision of those bytes into an array of consecutive elements. As such, the `sizeof(data)` on line 4 returns 16, which is the total number of bytes for the array. Similarly, the `sizeof(ptr)` on line 5 returns 8, which is the size of a pointer (i.e., an address) on a 64-bit CPU architecture. Neither of these return the number of elements in the array.

Line 7 is successful because we know the total amount of space for data (`sizeof(data)` = 16 bytes) and we know data is an array of `uint32_t` items (`sizeof(uint32_t)` = 4 bytes for each item). Having both of these pieces of information allows us to perform this calculation. We cannot perform this same calculation using `sizeof(ptr)`, however. That is, if we are given a pointer (such as `ptr`) and we know that it is pointing to an array of `uint32_t` items, we cannot determine the length of the array. The problem is that ptr technically does not point to an array; rather, it points only to the _first element_ of the array. There is no way to attach the additional information of the size of the array to the pointer.

This point becomes really important later, when we discuss the relationship of arrays and functions. Specifically, arrays cannot be directly passed as an argument to a function call. Rather, arrays are always passed as pointers. Because of this fact, the length of the array must be passed explicitly as a separate parameter. The simplest example of this is the parameter list of `main()`, which consists of an array (`argv`) and its array length (`argc`).


```cpp
int
main (int argc, char *argv[])
{
   ...
}

```
///10.4.1. Two-dimensional Arrays[¶]
------------------------------------

One side effect of C not storing array lengths is the complexity of working with multi-dimensional arrays. Consider [Code Listing A.14](#cla-14) as an example. Line 6 declares an array that contains two rows of three columns each. When declaring `data` on this line, the 3 must be specified within the brackets to indicate the number of columns per row. That is, this declaration could not be written as `data[][]`, even with the explicit initialization on the right side of the line.

```cpp
/* Code Listing A.14:
   Traversing through a two-dimensional array
 */

/* Instantiate a2-d array and print its starting address */
uint32_t data[][3] = { { 1, 2, 3 }, { 4, 5, 6 } };
printf ("data starts at %p\n\n", data);

/* Make u32ptr point to data, but observe that this causes a
   compiler warning; the warning can be fixed by casting data
   ((uint32_t *)data) or using one layer of brackets (data[0]) */
uint32_t *u32ptr = data;

/* Loop through the elements, observing that array bracket
   notation and pointer arithmetic behave identically */
for (size_t i = 0; i < 2; i++)
  for (size_t j = 0; j < 3; j++)
    {
      printf ("data[%zd][%zd] address %p, value %08" PRIx32 "\n",
              i, j, &data[i][j], data[i][j]);
      printf ("u32ptr[%zd]  address %p, value %08" PRIx32 "\n",
              i * 3 + j, &u32ptr[i * 3 + j], u32ptr[i * 3 + j]);
      printf ("u32ptr+%zd   address %p, value %08" PRIx32 "\n\n",
              i * 3 + j, u32ptr + (i*3+j), *(u32ptr +(i*3+j)));
    }
```

![Creating a virtual 2-d array with an array of pointers](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.A.2.png)

Figure 10.4.2: Creating a virtual 2-d array with an array of pointers

The rest of [Code Listing A.14](#cla-14) is modeled off of the structure of [Code Listing A.12](#cla-12). As in that example, there is a `u32ptr` that is set to point to the array (line 12). Based on the declaration, though, `u32ptr` is a pointer to a `uint32_t`. Because of this declaration, it can be used as a one-dimensional array, but not as a two dimensional array. That is not a problem in this case, because C stores arrays in _row-major order_; in this order, the first element of the second row is placed immediately after the last element of the first row. That is, `data[1][0]` (the same as `u32ptr[3]`) immediately follows `data[0][2]` (the same as `u32ptr[2]`). Thus, a one-dimensional array pointer like u32ptr can navigate the two-dimensional structure by calculating its index as `i * 3 + j` (row times columns/row, plus the column number of the current row). This requires, of course, knowledge of the number of columns per row. When two-dimensional arrays are passed as arguments to functions, this additional information must be passed explicitly as separate parameters.

[Code Listing A.15](#cla-15) demonstrates a common variation on two-dimensional arrays. In this case, data is not declared as a two-dimensional array; rather, it is one-dimensional array of two pointers. Its initialization in line 10 provides those two pointers: the addresses of the two arrays `row0` and `row1`. Unlike the declaration structure of [Code Listing A.14](#cla-14), this version does not guarantee that the elements can all be accessed in row-major order. The `row0` and `row1` arrays are not guaranteed to be in any particular order in memory; consequently, we cannot say that `row1[0]` immediately follows `row0[2]`. [Figure 10.4.2](Arrays.html#twodptr) illustrates the pointer structure of this declaration.

```cpp
/* Code Listing A.15:
   Two-dimensional arrays as arrays of pointers
 */

/* Instantiate the rows separate */
uint32_t row0[] = { 1, 2, 3 };
uint32_t row1[] = { 4, 5, 6 };

/* data is an array of two pointers, NOT a 2-d array */
uint32_t *data[] = { row0, row1 };

/* But we can still treat data as a 2-d array */
for (size_t i = 0; i < 2; i++)
  {
    printf ("data[%zd] = %p\n", i, data[i]);
    for (size_t j = 0; j < 3; j++)
      printf ("&data[%zd][%zd] = %d <%p>\n", i, j, data[i][j], &data[i][j]);
  }
```

![The argv array of command-line arguments is an array of pointers](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.A.3.png)

Figure 10.4.3: The `argv` array of command-line arguments is an array of pointers

Despite this more complex inner structure, lines 13 – 18 illustrate that the array can still be treated like a two-dimensional array for accessing elements. This code works as an artifact of an earlier point in this section: pointers can be dereferenced using bracket notation. In this case, data is declared as an array of `uint32_t*` elements, so `data[0]` is a pointer storing the address of `row0`. Accessing `data[0][1]`, then, is the same as accessing `row0[1]` because of the pointer dereferencing.

[Figure 10.4.3](Arrays.html#argvptrs) illustrates a familiar example for this same concept. Consider the command line to list files as `"ls -ltr"`. In this case, `argv[0]` is `"ls"` and `argv[1]` is `"-ltr"` internal in the process that runs this program. Since the parameter is declared `char *argv[]` (i.e., an array of pointers to `chars`), we can see that each element is a pointer to a string.

///10.4.2. Structs and Packing[¶]
---------------------------------

Elements in an array are, by definition, all of the same type. Given the declaration `int data[5]`, we know that the five elements are `int`s and these elements occupy a single block of consecutive bytes in memory. The total size of this block is exactly the number of elements (5) times the size of each element (an int is typically 4 bytes). To distinguish the elements within an array, each element has a unique index that can be used in the bracket notation. As described previously, bracket notation is semantically equivalent to calculating the offset to a memory location using pointer arithmetic. That is, `data[2]` is equivalent to `*(data + 2)`, and both notations refer to the calculation of a particular offset from the starting address of `data`.

Like arrays, structs are used to create a chunk of contiguous data in memory, but with two differences. First, the fields (rather than elements) of a `struct` are accessed with a name, rather than an index. Second, the fields in a `struct` do not have to adhere to the same type. [Code Listing A.16](#cla-16) illustrates both of these facts with a `struct` declaration for keeping track of time records. It is important to emphasize that lines 5 – 8 only define the structure of one of these structs (similar to defining a class in an object-oriented language like Java), rather than creating an instance of a `struct` in memory. In contrast, line 13 creates an instance as a local variable, with lines 14 and 15 initializing the fields of this struct. This declaration and initialization could be done on a single line, similar to initializing an array; line 13 could be extended to read `ts = { 100, 258.9275 }`, though this style requires knowing the specific order of elements in the `struct`.

```cpp
/* Code Listing A.16:
   A struct declaration to keep track of a time record
 */

struct timestamp {
  int date; // days since a given start data
  double time; // seconds since midnight of the current day
};

int
main (void)
{
  struct timestamp ts;
  ts.date = 100;
  ts.time = 258.9275;

  printf ("Time stamp: %d:%lf\n", ts.date, ts.time);

  return 0;
}

In the computer systems field, this view of structs as _objects without methods_ is not necessarily sufficient. In particular, when two different machines are exchanging data, the two systems need to agree on the layout and interpretation of the bytes without the `struct`. This agreement is not necessarily guaranteed, even if the same source code is used. Different compilers may arrange the fields in different orders, and the CPU may interpret multi-byte sequences differently due to endianness issues. [Code Listing A.17](#cla-17) demonstrates how to perform introspection into the layout of a `struct` in code.

```cpp
/* Code Listing A.17:
   Exploring the size and layout of a struct
 */

struct alternating {
  uint8_t a;
  uint32_t b;
  uint16_t c;
  uint8_t d;
};

struct alternating alt = { 1, 2, 3, 4 };

printf ("sizeof(alt) = %zd\n", sizeof (alt));
printf ("alt.a at %p, value %" PRId8 "\n", &alt.a, alt.a);
printf ("alt.b at %p, value %" PRId32 "\n", &alt.b, alt.b);
printf ("alt.c at %p, value %" PRId16 "\n", &altp">.c, alt.c); printf ("alt.d at %p, value %" PRId8 "\n", &alt.d, alt.d);

Line 14 prints the size of one `struct alternating` instance, `alt`. Reading lines 5 – 10 suggests that line 14 will indicate that the size of `alt` is 8 bytes (4 for `a`, 1 for `b`, 2 for `c`, and 1 for `d`). This intuition is wrong for typical compilers and modern hardware. Using both the `clang` and `gcc` compilers on an x86 architecture, the compiled code indicates that `alt` is 12 bytes in size. The specific layout of the memory for `alt` is shown in [Table A.4](#tblA-4). To be precise, the address of `alt` is the address of the byte on the left, containing the value `0x01`; the memory addresses increase from left to right in this table. As x86 is a little-endian architecture, the bytes of the multi-byte fields `b` and `c` are structured with the least-significant byte on the left (indicating a lower address offset within the `struct`).

`a`

`??`

`??`

`??`

`b`

`c`

`d`

`??`

`01`

`??`

`??`

`??`

`02`

`00`

`00`

`00`

`03`

`00`

`04`

`??`

Table A.4: The layout of an unpacked struct with padding bytes

The fields shown as `??` in [Table A.4](#tblA-4) indicate additional bytes of padding. The compiler injects this padding because of how memory is physically access by the CPU. The compiler can also (but does not in this case, re-order the fields if necessary. Generally speaking, bytes that are stored in the same _memory word_ can be accessed in a single CPU cycle. A memory word is a sequence of four consecutive bytes, but these bytes need to begin at a _word boundary_ (an address that is evenly divisible by four). If we assume that alt begins at address `0x7ffe000027c0` (which is a word boundary), the three bytes of padding just after the `a` ensure that `b` starts at another word boundary, `0x7ffe000027c4`. If b, instead, began immediately after `a` at address `0x7ffe000027c1`, then the contents of `b` would span two distinct memory words. As a result, accessing `b` would now require two CPU cycles instead of just one. This additional cycle might sound trivial (since there are billions of cycles per second), but the cumulative effect over all programs would be significant. The padding after the d rounds up the size of the `struct` to be an exact multiple of three words. This padding helps ensure other data on the stack around the `alt` variable also begin at word boundaries.

But what if the code needs to adhere to a specification that the `struct alternating` cannot have padding? That is, each `struct` instance needs to be exactly eight bytes, interpreted in the order defined by the `struct`. In this case, the `struct` needs to be declared as _packed_, meaning that there is no padding or re-ordering allowed. To declare the `struct` as packed, the only change is to modify the first line of [Code Listing A.17](#cla-17) with the packed attribute:

struct __attribute__((__packed__)) alternating {

🔍 Note

* * *

Computer systems frequently take advantage of very compact data representations, particular in the networking domain. Bit masks, for instance, use the individual bits within a byte to represent distinct pieces of information. Within the `struct` declaration, the individual field names are append with `:n` to indicate that the field occupies `n` bits. Packed structs also help with this data compression by ensuring that there is no padding added. Consider the following example:

```cpp
struct __attribute__((__packed__)) bits {
  unsigned type:2;   // can be 0, 1, 2, or 3
  unsigned urgent:1; // can be true (1) or false (0)
  unsigned length:7; // anywhere from 0 to 31
};

struct bits data;
memset (&data, 0, sizeof (data)); // ensure all bits init to 0
data.type = 3;
data.urgent = false;
data.length = 27;

printf ("Size of data is %zd\n", sizeof (data));

/* Cast the address of data to a uint16_t* and dereference it */
printf ("data as a hex int is %04" PRIx16 "\n", *((uint16_t*) &data));

The declaration of `struct bits` indicates that one instance requires 10 bits. Since we cannot actually allocate data at this bit level, this instance would be padded to make it exactly two bytes (16 bits) in size. Among those 16 bits, the C standard does not specify an exact order. The fields should only be accessed through their names, as shown in line 9 – 11. It is possible, however, to determine the layout on a particular system through trial and error. The `printf()` call on lines 15 and 16 reveal that one possible layout is as follows:

First byte

Second byte

_[unnamed and unused]_

`length`

`urg`

`type`

`0`

`0`

`0`

`0`

`0`

`0`

`0`

`0`

`1`

`1`

`0`

`1`

`1`

`0`

`1`

`1`

`0`

`0`

`d`

`b`

Based on this illustration, the seven bits of the `length` field spans the two bytes, whereas `urgent` and `type` both reside in the second byte. The bottom line shows the hexadecimal representation of four bits at a time. Printing the full value, as on lines 15 and 16, produces the output `0x00db`.

///10.4.3. Enums and Type Definitions[¶]
----------------------------------------

From the programmer’s perspective, C’s built-in types and `struct`s are low-level primitives that make it difficult to read and understand the code. For instance, in the case of `struct`s, the type is always the full name consisting of `struct` and the identifier following it. In the case of `alt` from [Code Listing A.17](#cla-17), its type is `struct alternating`; its type is not `struct`, nor is its type `alternating`. Copying the word `struct` around gets tedious and it detracts from the readability. As for the primitive types, an int indicates an integer value, but what if there are only certain integers that are valid? For instance, consider a variable that is used to keep track of the day of the week (Sunday through Saturday) as an integer (Sunday is 1, Saturday is 7). Referring to day 37 might lead to an unpredictable error.

To start with the latter problem, the solution is to define an enumerated type, or `enum`. An `enum` is a custom integer type that allows the programmer to use names instead of numeric constants. [Code Listing A.18](#cla-18) declares an `enum` for the days of the week as previously described. In the declaration, the values in the `enum` are automatically incremented. By setting `SUN` = 1 (instead of allowing the default starting value of 0), `MON` would be 2, `TUE` would be 3, and so on. The advantage of the `enum` is that we can use these meaningful names (`MON`, `TUE`, `WED`, …) instead of memorizing numeric values. We can also declare variables using the `enum` type as shown on line 6.

```cpp
/* Code Listing A.18:
   Declaring the days of the week as an enumerated type
 */

enum days { SUN = 1, MON, TUE, WED, THU, FRI, SAT };
enum days today = WED;
printf ("Tomorrow is %d\n", today + 1);

To be clear, `enum`s are a syntactical mechanism of convenience, not security. Internally, an `enum` is just an `int`, and C does not perform any bounds checking to ensure that the values of an `enum` variable (such as today) match the names or the range in the definition. Line 6 could initialize today to be 37, or line 7 could be changed to use `today - 452`; either would be allowed, as today is ultimately just an `int` variable.

The `enum` keyword has the same problem as the `struct` keyword, in that it must be included in the type name and passed around throughout the code. In [Code Listing A.18](#cla-18), the type of the today variable is `enum days`, not `enum` or `days`. To make the code more readable for both `enum`s and `struct`s, we can declare a new custom type name with the `typedef` keyword. [Code Listing A.19](#cla-19) uses `typedef` on the `enum` from [A.18](#cla-18) and the `struct` from [A.17](#cla-17).

```cpp
/* Code Listing A.19:
   Simplifying enum and struct type names with a typedef
 */

typedef enum days { SUN = 1, MON, TUE, WED, THU, FRI, SAT } day_t;
day_t today = WED;

typedef struct alternating {
  uint8_t a;
  uint32_t b;
  uint16_t c;
  uint8_t d;
} alt_t;

alt_t alt = { 1, 2, 3, 4 };

The general structure for declaring a new type is `typedef [existing type] [new type name];`. By convention, the new type name typically ends with `_t` to indicate that this is a type. Observe that there are many such type definitions in the C standard library. For instance, the `size_t` type is defined (indirectly, as there are several chained type definitions involved) in the `ctype.h` header as shown below. The advantage of using the type definition is that it provides additional semantics. A variable declared as a `size_t` is not just being used as an integer, but as the size of something.

typedef int32_t size_t;

One common problem with type definitions for `struct`s,\`in particular, is when there are circular dependencies. Consider [Code Listing A.20](#cla-20) that defines a `person_t` type and an `age_t` type. In this application, each person (`person_t`) has a unique name and age, but each `age_t` has and unique year and pointers to up to 5 people. In other words, the person_t definition needs to know about the `age_t` type, while the `age_t` definition needs to know about the `person_t` type. The problem is that the definition of `person_t` (lines 7 – 10) comes before the C compiler has learned about the `age_t` type (line 15); the C compiler cannot look ahead, so using age_t on line 9 would be a compiler error.

```cpp
/* Code Listing A.20:
   Defining two structs with a circular definition
 */

struct age; // dummy struct definition

typedef struct person {
  char *name;
  struct age *age;
} person_t;

typedef struct age {
  int year;
  person_t *people[5];
} age_t;

The solution is to use a dummy `struct` definition on line 5 that matches the name on line 12. (It is vital that the name of both `struct` types match.) By structuring the code this way, the compiler is able to correctly link the type of the age field within `person_t` as a pointer to an `age_t` instance later. Once this is done, the circular definition can be ignored, as shown in [Code Listing A.21](#cla-21). The `person_t` instance is able to set its age field to the address of an `age_t` instance without causing a compiler error or warning.

```cpp
/* Code Listing A.21:
   When using the structs, the circular definition is not visible
 */

person_t person;
age_t age;

person.name = "Gustavo";
person.age = &age;

age.year = 35;
age.people[0] = &person;



//10.5. Functions and Scope[¶]
==============================

As with every modern programming language, C uses functions to create modularity as a step toward robust software. Encapsulating portions of a program’s code this way allows the programmer to isolate the functions for the purposes of testing and debugging. One key aspect of writing functions is to get the _scope_ of variables correct. [Code Listing A.22](#cla-22) illustrates the three main scopes for variables in C programs: _global_, _local_, and _static_. The split of global and local is fairly straightforward: if the variable declaration occurs inside the body of a function (see line 13), it is local to that function and unavailable to others; if the declaration is outside any function definition (see line 7), the variable is global and accessible by all functions for reading or modification.

```cpp
/* Code Listing A.22:
   Declaring local and global functions in a helper function
 */

#include <stdio.h>

int global = 1;
static int global_static = 2;

void
helper (void)
{
  int local = 5;
  static int local_static = 10;

  printf ("global = %d\n", global++);
  printf ("global_static = %d\n", global_static++);
  printf ("local = %d\n", local++);
  printf ("local again = %d\n", local++);
  printf ("local_static = %d\n\n", local_static++);

    {
      static int hidden = 20;
      printf ("hidden = %d\n\n", hidden++);
    }
  /* hidden cannot be accessed here */
}

In addition to their differences in visibility, local and global variables differ in another key way: initialization. Global variables are always initialized, whether the code does so explicitly or not. Consider changing line 7 from [Code Listing A.22](#cla-22) as shown here:

int global;

The variable `global` is still declared as an `int` variable. It is also initialized to the value 0. One subtle difference is that the variable now exists in a different memory section than before. Recall that we can collectively refer to the _data segment_ as the portion of memory storing global variables. This segment is subdivided into smaller sections, including `.data` (initialized global variables), `.bss` (_block started by symbol_ for uninitialized global variables), and `.rodata` (initialized read-only data, such as string constants). In the original form of [Code Listing A.22](#cla-22), `global` was allocated space in `.data`, and the executable file would contain its initial value (1). In this modified version, `global` would be allocated to `.bss`. In the executable file, the `.bss` stores nothing, because it doesn’t need to; everything allocated to `.bss` has the same initial value of 0. As such, the executable only needs to contain information about what variables map to `.bss` to know the total size that needs to be allocated in memory when the process starts. All global variables are mapped into one of these three sections, ensuring that they all start with some initial value.

![Location of %rsp before and after allocating the stack frame for helper()](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.A.4.png)

Figure 10.5.1: Location of %rsp before and after allocating the stack frame for helper()

Local variables are different. **Local variables are never initialized unless done so explicitly**. Unlike global variables, in which there is a single instance that can be stored persistently in the executable file, local variables are only created at run-time when a function is called. This allocation happens in a single step: adjusting the stack pointer register (`%rsp` in x86). [Figure 10.5.1](Functions.html#rsplocation) illustrates the portion of the stack before `main()` calls `helper()` and after doing so.

All global variables have their initial values loaded automatically when the program is first loaded into memory. Local variables are automatically allocated space in the stack frame when the function defining their scope is called. However, initializing the variable requires extra instructions, and these are only performed if the program specifically requests it. That is, if you do not explicitly initialize your local variables (as line 13 of [Code Listing A.22](#cla-22) does), then the initial value of the local variable happens to be whatever was already in that memory location. Note that, in general, other functions are likely to have been called before the function you are currently writing; i.e., `main()` (or related start-up functions) likely called other functions before `helper()` gets called the first time. Consequently, the initial value of `local` would be whatever data was left over from those previous function calls.

🐞🐛🐌 Bug Warning

* * *

At the risk of overkill on this particular point, it is critical to develop the habit of always initializing local variables. When this is not done, the program can behave truly randomly. It may work fine nine times in a row before having a segmentation fault on the tenth run; when the program is then re-run to debug the segmentation fault, it goes back to working perfectly. It is incredibly common that such random behavior can be traced back to an uninitialized local variable.

This failure to initialize local variables often arises when working with non-primitive data types, such as arrays or structs. In this case, the simplest approach is to use `memset()`. The first parameter is a pointer to a buffer to initialize, the second parameter is what value to write into each byte and the third parameter is the length of the buffer.

int data[100];
memset (data, 0, sizeof (data)); // initialize all elements to 0

struct stat info;
memset (&info, 0, sizeof (info)); // initialize all fields to 0

📦 C library functions – `<ctype.h>`

* * *

`void * memset(void *b, int c, size_t len);`

Sets len consecutive bytes to the value c, starting at location b.

To return to the general discussion of scope, the third type of variable scope centers around the `static` keyword. Variables that are declared as `static` occupy a sort of middle ground between global and local. Static scope indicates that the variable is **lexically bound but persistent**. The phrase _lexically bound_ means that the variable name can only be referenced by the code block that contains the declaration; code blocks in C are defined by files, loop constructs (e.g., `for` or `while` loops) curly braces (`{ ... }`). In [Code Listing A.22](#cla-22), line 14 declares `local_static` so that it can only be accessed from within the helper() function; no other function can have direct access to this variable. Line 23 declares hidden so that it is only accessible within the block of code in lines 22 – 25; once that block is finished, as the comment on line 26 indicates, the hidden variable can no longer be accessed, even within the `helper()` function. The `global_static` variable, declared on line 8, is visible throughout all of the functions in this particular file. However, if this file is compiled and linked along with a different piece of C code, that other code would not be able to access `global_static`.

In addition to being lexically bound, static variables are persistent. Unlike normal local variables that are created and destroyed with every call of a function, there is only one copy of each static variable, and that copy persists for the duration of the process execution. In that way, static variables are akin to global variables. [Code Listing A.23](#cla-23) demonstrates this fact by calling `helper()` twice. Each time that `helper()` is called, the `local` variable is initialized to 5 and incremented twice. In contrast, `local_static` is initialized once to the value 10. With the first call of `helper()`, line 20 of [Code Listing A.22](#cla-22) prints this initial value and increments it to 11. This value is then used when `helper()` is called again. In other words, the initialization on line 14 of [Code Listing A.22](#cla-22) is very deceiving; it only executes once, rather than every time the function is called. Similarly, the static variable hidden is also initialized once to 20 and incremented during the first call to `helper()`; the second call to the function begins with the modified value 21.

```cpp
/* Code Listing A.23:
   Using a separate file to call the function in Code Listing A.22
 */

#include <stdio.h>

/* Function prototypes and extern variable declarations
   are normally declared in a separate .h header file */
void helper (void);
extern int global;

int
main (void)
{
  printf ("global is originally %d\n\n", global);

  printf ("First call to helper:\n");
  helper();
  printf ("Second call to helper:\n");
  helper();

  printf ("global ends up as %d\n", global);
  return 0;
}

Lines 9 and 10 of [Code Listing A.23](#cla-23) illustrate behavior that is normally specified in a header file. For instance, if [Code Listing A.22](#cla-22) was stored in a file called `"scope.c"`, these two lines would likely be in a file called `"scope.h"` that would look like [Code Listing A.24](#cla-24). The first line is a _function prototype_ for helper(), which serves the purpose of declaring the parameter types and return type for the function. The compiler uses this information to know that any calls to the function are correctly formatted. The `extern` keyword is used for a global variable to indicate that this variable is defined _somewhere_. When [Code Listing A.23](#cla-23) is compiled (before it is linked), the compiler only needs to know that `global` is an `int` variable to know that lines 15 and 22 are valid. The linking process will later tie these lines of code to the correct variable.

```cpp
/* Code Listing A.24:
   A header file for the public interface of Code Listing A.22
 */

#ifdef __csf_appendix_scope_h__
#define __csf_appendix_scope_h__

void helper (void);
extern int global;

#endif

🐞🐛🐌 Bug Warning

* * *

Global variables should never appear in a header file without the `extern` keyword. Without this keyword, the C compiler will think that the programmer’s intention is to create an instance of this global variable in the compiled object. As such, if the header file is included in multiple C source code files, the compiler will create such a global variable inside each of them. During the compilation stage, this is not a problem; the problem arises later during the linking stage. Specifically, C strictly indicates that there can be only one instance of a variable name in a given scope. (Note that the type doesn’t matter; `int x` and `char x` would not be allowed in the same scope.) When the linker would try to combine the compiled objects into a single executable, it would report an error due to multiple global variables with the same name.

///10.5.1. Function Parameters and Return Values[¶]
---------------------------------------------------

C functions can be defined to take any number of parameters and return a single value. This definition follows from the mathematical definition of a function. For example, consider the following mathematical function definition:

$f(x) = x^2$

The function f(x) takes one input parameter (x) and maps it to a single value by squaring it. C functions operate on the same principle, as shown in [Code Listing A.25](#cla-25). In this example, the `add()` function takes two input parameters, adds them together, and returns the result; that is, `add()` would map two input values to a particular output in the traditional mathematical sense. Observe that function parameters operate like local variables; the variables `x` and `y` can be accessed from within the function body and their values are not persistent from one call to the next.

```cpp
/* Code Listing A.25:
   A trivial C function to add two numbers
 */

int
add (int x, int y)
{
  return x + y;
}

This traditional notion of a function does not completely match with the common use of C functions as _subroutines_. The semantic difference is that a subroutine is simply a modular piece of code to encapsulate some behavior; subroutines are not bound by the idea of mapping inputs to a single return value. One common example of this is using a `void` return type, which indicates that there is no mapped result. In mathematical terms, it would be nonsensical to talk about a function f(x) that does not map any input value x to a specific output; such an f(x) would not be a function [[1]](#f57) in the conventional sense. In C subroutines, it happens all the time. Consider a function that takes two inputs and passes them to `printf()` along with a format string; this function would have no need for a return type.

When working with subroutines rather than mathematical functions, there are several types of behavior that C supports to create flexible programming styles. One such behavior is the ability to specify a variable number of parameters. Perhaps the most common example of this behavior is the `printf()` function. The first parameter is a string constant to represent how the output is to be formatted; the number of additional parameters depends on the number of format specifiers (such as `%d` or `%s`). The function prototype for `printf()` is written as follows:

int printf (const char *, ...);

The ellipsis (`...`) here is not this book’s notation to indicate “more stuff here.” Rather, the ellipsis is part of the C syntax to indicate that there are additional variables of unknown types. In the case of `printf()`, only the first argument’s type (`const char *`) is explicitly declared. [Code Listing A.26](#cla-26) demonstrates how to define a function with a variable-length parameter list. The first key feature is that the `stdarg.h` header file must be included (line 6). This header file defines a number of preprocessor macros—which look like functions—to process the arguments.

```cpp
/* Code Listing A.26:
   Defining a function with a variable-length parameter list
 */

#include <stdio.h>
#include <stdarg.h>

int
sum (size_t length, ...)
{
  /* Declare and initialize the variable argument list
     with the specified length */
  va_list args;
  va_start (args, length);

  /* Loop through each argument, adding it to the total */
  int total = 0;
  for (size_t i = 0; i < length; i++)
    {
      /* Get the next argument as an int */
      int arg = va_arg (args, int);
      total += arg;
    }
  va_end (args);
  return total;
}

In [Code Listing A.26](#cla-26), the sum() function takes a `size_t` variable to indicate the number of integer values to add together. To begin processing these inputs, lines 13 and 14 declare and instantiate a variable-length argument list (type `va_list`) of the specified length. From there, each argument can be accessed from the list exactly once, using the `va_arg()` macro (line 21). The second argument to this function indicates the type of the variable; in this case, each argument in the list will be converted to an `int`. [Code Listing A.27](#cla-27) demonstrates how to use the `sum()` function. In both calls, the first argument is required to indicate the number of arguments that should be added together; line 5 only passes the value 42, whereas line 7 will calculate the sum 2 + 4 + 6 + 8.

```cpp
/* Code Listing A.27:
   Calling a function with a variable-length parameter list
 */

int total = sum (1, 42);
printf ("Total is %d\n\n", total); // prints 42
total = sum (4, 2, 4, 6, 8);
printf ("Total is %d\n\n", total); // prints 20

🐞🐛🐌 Bug Warning

* * *

It is common to return pointers for a variety of reasons. For instance, a function might dynamically allocate some space (see the section on Pointers and Dynamic Allocation) to use as a buffer or make a copy of a string. However, it is critical that a **function should never return a pointer to a local variable**. This rule includes returning local copies of strings, as shown in the following example:

```cpp
char *
broken_function (void)
{
  char string[] = "Hello!"; // string exists in stack frame
  return string;            // stack frame becomes invalid
}

The problem with returning pointers to local variables is that the data associated with the variable exists in a stack frame that is linked to the function call; once the function returns, the stack frame is de-allocated, making any future access to this part of memory invalid. Unfortunately, such code occasionally works without crashing because the stack frame has not been overwritten yet; this makes debugging the code very difficult, because the crashes seem random.

///10.5.2. Call-by-Reference Parameters[¶]
------------------------------------------

The limitation of returning only a single value can be frustrating, as a complex subroutine may need to provide the caller with multiple pieces of data. One example of this need is a function that is supposed to return a pointer. A common convention for such functions is to return the `NULL` pointer if an error occurs in the function. However, this approach does not explain what the error was or how the caller should react; if the function took a file descriptor in as input, was the problem that the file was closed, the user running the program did not have access to the file, or the file had no contents? Clearly, it would be desirable to provide such information.

One approach to providing multiple returns is simply to cheat: use global variables. This is the approach that is commonly used for error indications as in the previous example. If the function returns `NULL` because of an error, it can also set the `errno` global variable to indicate the reason why the failure occurred. With few clearly defined exceptions (such as `errno`), **this approach is undesirable and generally unsafe**. Global variables require the programmer to be careful to avoid name collisions. Furthermore, the very nature of global variables allows any function to change them at any point (assuming the variable name is made visible with `extern`). This global accessibility is particularly fraught in concurrent systems, as multiple threads might simultaneously need to call the same function, potentially creating a race condition with interleaved modifications of the variable.

A better approach is to use _call-by-reference_ parameters. Consider performing traditional integer division as taught in the early grades of school. Calculating 17 ÷ 5 has two parts to the answer: a quotient of 3 and a remainder of 2 (since 5 * 3 + 2 = 17). [Code Listing A.28](#cla-28) demonstrates how to define a division function using a call-by-reference parameter.

```cpp
/* Code Listing A.28:
   Returning multiple parameters with call-by-reference
 */

int
divide (int dividend, int divisor, int *remainder)
{
  /* Set the remainder (dividend % divisor), then
     return the quotient (dividend / divisor) */
  assert (remainder != NULL);
  *remainder = dividend % divisor;
  return dividend / divisor;
}

![Stack frames for main() and divide()](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.A.5.png)

Figure 10.5.6: Stack frames for `main()` and `divide()`

In this function, the `remainder` parameter is a pointer to an `int`. The `assert()` call on line 10 is a safety check that will prevent anyone from passing a `NULL` pointer to this function. Within the `divide()` function, we will use this pointer to change the original `int`’s value. Line 11 performs this by dereferencing the pointer and storing the result of the modulus operation `dividend % divisor`. Once we have done this, line 12 returns the normal C integer division quotient (`dividend / divisor`). [Code Listing A.29](#cla-29) demonstrates how to call this function by passing the address of `remainder`. (Recall that pointers store addresses, so the address of a variable becomes a pointer.) [Figure 10.5.6](Functions.html#stackframes) illustrates the relationship between the stack frames for `main()` ([Code Listing A.29](#cla-29)) and the call to `divide()`. The remainder parameter for `divide()` contains a pointer back to the `rem` variable in `main()`’s stack frame.

```cpp
/* Code Listing A.29:
   Calling a function with a variable-length parameter list
 */

int rem = 0;

/* pass the address of rem for divide to set its value */
int quot = divide (17, 5, &rem);

printf ("17 ÷ 5 is %d R %d\n", quot, rem);

🐞🐛🐌 Bug Warning

* * *

A very common mistake among programmers who are new to C’s call-by-reference parameters is to try to match the variable declaration instead of using the address-of operator (`&`) as illustrated here:

int *remainder; // Don't EVER leave a pointer uninitialized!
int quotient = divide (17, 5 remainder);

This code would probably work successfully, but **it is wrong and very dangerous**. Yes, the types are correct. The `remainder` variable is an `int*`, which matches the type that the `divide()` function expects. The problem lies in the answer to this question: What is the initial value of `remainder`? In other words, what portion of memory is `remainder` _pointing to_?

In C, local variables are never initialized unless the programmer explicitly does so. The example code above declares a pointer, but does not initialize it. When this happens, the value of the variable (`remainder` in this case) is whatever random values happen to already be there on the stack. In other words, **this code tells the machine to initialize** `remainder` **so that it points to a random location**. In the context of a large, complex program, this code will produce a segmentation fault _if we are lucky_! The segmentation fault would be an indication that there is a problem. If we are unlucky, `remainder` (by random chance) points to a valid location; the problem is that the `divide()` function will overwrite the contents of that random location. Depending on that that memory location is supposed to be storing, this bug could cause a completely unrelated part of the program to crash seconds, minutes, or hours later, with no indication that this call to `divide()` is the cause.

The difference between this code and [Code Listing A.29](#cla-29) is that the address-of operator requires us to know what we are initializing the pointer to. That is, it requires us to answer “address of _what_?” By passing `&remainder` for an `int` variable, rather than declaring and passing an `int*`, we are providing the answer: the address of the local variable `remainder` that was just declared.

///10.5.3. Arrays as Parameters[¶]
----------------------------------

In the earlier discussion on arrays, we made the observation that array lengths must be passed as explicit parameters when an array is passed to another function. The reason for this is that arrays are always passed as call-by-reference parameters. [Code Listing A.30](#cla-30) provides an example of a function that takes an array as a parameter and modifies the values stored in the array. On line 6, the parameter list indicates that values is an array of int values. This declaration is not entirely true; values is, in fact, just a pointer to an `int`. Using the declaration `int values[]` instead of `int *values` serves the purpose of indicating how values will be used, but both declarations are acceptable.

```cpp
/* Code Listing A.30:
   All arrays are passed as call-by-reference pointers
 */

void
dub_all (size_t length, int values[])
{
  for (size_t i = 0; i < length; i++)
    values[i] *= 2;
}

int
main (void)
{
  int data[] = { 1, 2, 3 };
  dub_all (3, data);
  for (size_t i = 0; i < 3; i++)
    printf ("data[%zd] = %d\n", i, data[i]);

  int faker = 5;
  dub_all (1, &faker);
  printf ("faker is now %d\n", faker);

  return 0;
}

![Stack frames for main() and dub_all()](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.A.6.png)

Figure 10.5.8: Stack frames for `main()` and `dub_all()`

[Figure 10.5.8](Functions.html#stackframesdub) illustrates the relationship between `main()` and the first call to `dub_all()` (for simplicity, the variable `faker` is not shown). Although `data` is ostensibly passed to the `dub_all()` function, it is actually just the address of the first element that is passed. This structure is consistent with an observation that we made earlier: array names are simply aliases for the address of their first element.

To emphasize the point even further, observe that line 21 makes another call to `dub_all()`. In this call, we are passing the address of the local variable `faker` as an _array_ of size 1. Since array names and pointers can both be indexed using the array bracket notation, `dub_all()` can still successfully access the faker variable and change its value. From the perspective of `dub_all()`, there is no way to tell if the `values` parameter is pointing to a single `int` or something that was actually declared as an array. For this reason, the length of the array must also be passed as a parameter to `dub_all()` to control the number of iterations in that function’s `for`-loop on line 8.

[[1]](#id10)

Pedantically, such an f(x) would fit the definition of a mathematical function with an empty codomain. Such a function would not be conventional, however.



//10.6. Pointers and Dynamic Allocation[¶]
==========================================

The previous section explored the use of pointers for call-by-reference parameters, which is a very common and effective use of pointers. A second common use of pointers is for _dynamic memory allocation_, which is a necessary aspect of even moderately sized programs. Dynamic memory allocation is used for several key purposes:

> *   the exact size of a piece of data can only be determined at run-time based on features of the operating environment, including user input;
> *   the size of the data needs to change over time;
> *   the data manipulation involves combining or splitting the data, which is particularly common with string processing;
> *   the data is extremely large;
> *   the data needs persistence and scope that are not global, but are also not tied to the execution context of a single function.

The first three bullet points may seem intuitive, as they all involve the size of the data changing in one way or another. The fourth bullet point arises from the fact that all modern OS place a default maximum stack size for a thread, typically 8 MB. Declaring a large local variable, such as an array with thousands of elements, is likely to lead to a system crash when the stack runs out of space and overflows.

The final bullet point is common in many different types of languages, applications, or systems, though the terminology might be different. [Code Listing A.31](#cla-31) provides an example in the Java programming language using the Factory design pattern. In the Factory pattern, there is a designated class (the _factory_) that is responsible for creating new instances of some class (here, just generic Objects). The Factory class only has a single method, typically with a name that starts with “get” or “create.” Later, in another part of the code (such as `main()`), a single instance of the Factory is created; this instance is then used to create the Objects over and over as needed.

```cpp
/* Code Listing A.31:
   The Java Factory design pattern uses dynamic allocation for
   persistence and non-local scope
 */

public class Factory {
  public Object getObject () {
    return new Object();
  }
}

public class Demo {
  public static void main (String[] args) {
    Factory factory = new Factory();
    Object object = factory.getObject();
  }
}

Line 8 of the Factory class is an example of dynamic memory allocation for persistence and non-local scope. This line of code dynamically allocates a chunk of memory to store an Object. However, the Object is not tied to the scope of the `getObject()` method; it is intended to be used after `getObject()` has finished executing. In fact, we can generalize this example even further and suggest that **the vast majority of uses of the new keyword in Java are semantically equivalent to dynamic memory allocation in C with** `malloc()` **or** `calloc()`.

Despite the fact that dynamic memory allocation serves several different purposes in C, there are only two standard mechanisms for doing this: the `calloc()` and `malloc()` functions. Both functions take parameters to indicate how much space is needed, allocate that much space on the heap, and return a pointer to the beginning of the space. The primary difference is that `calloc()` guarantees these bytes are initialized to all zero; `malloc()` performs no initialization on the bytes, so there is the possibility that they hold random initial values. The difference in the calling parameters is superficial. For `malloc(),` the only parameter is the total number of bytes needed. For `calloc()`, the first parameter specifies how many consecutive elements there are (similar to specifying the length of an array) and the second parameter indicates how many bytes are needed for each element; these two values are internally multiplied to determine the total size, and `calloc()` ends up allocating exactly that much space (just like `malloc()`). Because `calloc()` guarantees the data is initialized to zero, it is typically considered safer and preferred for general use. On the other hand, `malloc()` is faster since it does not do any initialization; this function is acceptable (or even preferred) if you know that the data will be entirely initialized before it will be otherwise used.

📦 C library functions – `<stdlib.h>`

* * *

`void * calloc(size_t count, size_t size);`

Allocate count*size bytes of space on the heap and zero them out.

`void * malloc(size_t size);`

Allocate size bytes of space on the heap without initialization.

`void * realloc(void *ptr, size_t size);`

Resize a previously dynamically allocated memory space.

`void free(void *ptr);`

Mark the previously allocated space that ptr points to as available for future allocation.

Unlike Java, C does not have a _garbage collector_. When a piece of data is dynamically allocated with `malloc()` or `calloc(),` the programmer must determine when the data is no longer needed. Once you determine that the data can be safely deleted, you would need to call `free()` on the pointer to the space on the heap. This designates that space as available for future dynamic allocations.

[Code Listing A.32](#cla-32) demonstrates the C equivalent of the Factory pattern from [A.31](#cla-31). The `get_integer()` function dynamically allocates space on the heap and returns it. In C, the `void*` type acts comparably to the Object class in Java; any type of pointer can be implicitly cast into `void*`. Once the space is allocated on line 8, this space on the heap is not necessarily tied to the duration of any particular function. The integer variable name in `main()` is lexically bound to that function’s scope, but the heap space is not. Consequently, if `main()` returned without calling `free()` on integer (line 17), then this heap space would still be allocated but without a way to de-allocate it. That is, there is no other pointer reference to that space. This situation is known as a _memory leak_. With enough memory leaks over time, the heap would eventually run out of space that can be allocated; when that happens additional calls to `malloc()` and `calloc()` would return `NULL` to indicate the allocation failure.

```cpp
/* Code Listing A.32:
   A C version of the Factory pattern from Code Listing A.31
 */

void *
get_integer (void)
{
  return calloc (1, sizeof (int));
}

int
main (void)
{
  int *integer = (int *) get_integer ();
  *integer = 5;

  free (integer);
  integer = NULL;

  return 0;
}

🐞🐛🐌 Bug Warning

* * *

Using a pointer after calling `free()` on it (called _use-after-free_) leads to unpredictable and unsafe behavior. While there is the possibility that the data stored on the heap remains intact, there is no guarantee that this is the case. As such, every call to `free()` should be followed by a line that sets the pointer to `NULL`, as shown on line 18 above. Once that is done, if the pointer is used again, there will be a segmentation fault. In this case, the segmentation fault would be better than the alternative: accessing a portion of memory that might unpredictably change at any point in the future.

[Code Listing A.33](#cla-33) demonstrates an example of the first bullet-point use case described previously. In this case, the `build_array()` function is creating and initializing an array of `int` values. Since the length of the array is determined by the argument passed as `length`, this function uses dynamic allocation to create it as the correct size.

```cpp
/* Code Listing A.33:
   Creating a dynamic-sized array
 */

int *
build_array (size_t length)
{
  int *array = calloc (length, sizeof (int));

  for (size_t i; i < length; i++)
    array[i] = i * i;

  return array;
}

As the program continues to run after dynamically allocating space, it frequently occurs that the space allocated needs to change. Perhaps the space is acting as an array that holds the list of records that make up a student’s grade for a class. As the student completes more and more assignments, the array may become full and not have space for additional records. The `realloc()` function provides a simple mechanism for changing the size of the space. The first parameter to `realloc()` is a pointer to the space to be resized (this space must be on the heap), and the second parameter is the new total size that is desired. If there is sufficient unused memory immediately after the existing space, the record for the space in the heap is simply modified. If there isn’t sufficient space, `realloc()` will allocate a new space that is big enough for the new size and copy the original data into this new space; the old space is automatically cleaned up. In either, `realloc()` returns a pointer to the newly resized heap space.

[Code Listing A.34](#cla-34) uses the array from the function in [A.33](#cla-33), but later determines that more capacity is needed. Initially, after lines 5 and 6, the `array` consists of 40 bytes (10 elements that are 4-byte `int` values). Line 9 doubles the value of `len` from 10 to 20, then passes this new length to `realloc()`. Like `malloc()`, `realloc()` takes a single size parameter to indicate the new total number of bytes; since the array will now be 80 bytes (20 elements of 4-byte `int` values), we have to specifically multiply `len` by `sizeof(int)` to get the correct size request. Finally, line 10 uses a new variable name to capture the return value from `realloc()`. If, for some reason, `realloc()` failed to allocate the new space, it would return `NULL`. However, the original `array` would still be allocated and we might need that original pointer; storing the return value of `realloc()` directly into `array` would lose the reference to the original space on the heap, creating a memory leak. Once line 11 ensures that the return value was not `NULL`, line 12 updates array as needed.

```cpp
/* Code Listing A.34:
   Resizing the array built from A.33
 */

size_t len = 10;
int *array = build_array (len);

/* Double the capacity of the array */
len *= 2;
int *new_array = realloc (array, len * sizeof (int));
assert (new_array != NULL);
array = new_array;

🐞🐛🐌 Bug Warning

* * *

The `ptr` variable passed to `realloc()` must point to the heap; pointers to other memory segments will produce run-time exceptions. For instance, a common mistake arises when `ptr` is currently pointing to a string constant (e.g., `char *ptr = "hello world"`); `realloc()` cannot be used to resize this string.

Another common bug with `realloc()` arises when the old pointer is freed:

```cpp
int *newptr = realloc (oldptr, newsize);
free (oldptr);
*newptr = 5;

When `realloc()` runs, there are two possibilities: 1) the data does not move, so `oldptr` and `newptr` point to the same place, and 2) the data moves. In case 1, freeing `oldptr` means that `newptr` is also freed, making line 3 an example of a use-after-free bug. In the second case, `realloc()` transparently frees the old space for you; line 2 then becomes an example of a _double free_ in which the same heap space is freed twice, which triggers a run-time exception.

Lastly, note that `realloc()` behaves like `malloc()` when the resizing means increasing the size of the space. The additional bytes of memory allocated are not guaranteed to be initialized to any particular value.

Finally, as a general piece of guidance, you should only dynamically allocate data if you need to, according to the five scenarios identified above. One common mistake—not technically a bug, but unnecessarily complex code—follows from misapplication of the semantic equivalence of Java’s `new` keyword with `malloc()`. In Java, all objects (i.e., any variable that is not a primitive type) must be allocated with the new keyword. However, in C, any local variable will automatically be allocated space in the stack frame. If an instance of a `struct` does not need to persist beyond the scope of the current function, dynamically allocating space for it on the heap is a waste of both time and space.

[Code Listing A.35](#cla-35) compares these two styles of creating a local `struct` instance. In this case, the style of declaring the `jasmine` instance is preferred. When the function begins executing, the stack pointer is moved once to allocate all of the space needed for local variables; this takes the same amount of time regardless of how many variables there are. Consequently, the space for `jasmine` is automatically allocated by the function calling semantics itself. The allocation of `philippe,` on the other hand, requires significant extra work. Specifically, `malloc()` has to find available space, requiring modifications to the internal data structures that define the heap; then, then programmer must remember to `free()` the space later, causing more changes to the heap. Ultimately, this extra work is done for no reason; in fact, the changes to the heap might actually produce even worse performance later, because the `free()` might not return the heap to its original state (i.e., if there are multiple threads involved). It is true that C structs perform a similar function as Java objects; however, that does not automatically require the use of dynamic allocation.

```cpp
/* Code Listing A.35:
   Unnecessarily allocating a struct instance with malloc()
 */

struct user {
  char *name;
  int id;
};

int
main (void)
{
  struct user jasmine;
  jasmine.name = "Jasmine";
  jasmine.id = 50;

  struct user *philippe = malloc (sizeof (struct user));
  philippe->name = "Philippe";
  philippe->id = 75;
  free (philippe);

  return 0;
}



//10.7. Strings[¶]
==================

It is often the case that a seemingly simplistic idea or design can turn out to be surprisingly complicated. We saw one example of this previously in the discussion of pointers. The definition of equating a pointer with an address seems straightforward; the implications of their usage for dynamic memory allocation, call-by-reference parameters, variable sizes, and so forth quickly become challenging for the programmer. The same can be said for strings in C. As with pointers, we start with a basic definition:

> A string is an array of characters ending in the null byte.

To interpret the situation in a slightly different way, the C programming language does not actually have a string type in the intuitive sense that makes sense to humans. Instead, C just provides a thin veneer of interface for working with fixed-size arrays of `char` data. A string in the C sense consists of the array of `char`s that are (typically) observable to a human reader, with one additional `char` added to the end of the array. [Code Listing A.36](#cla-36) illustrates this fact by defining the string `"Hello"` in a very unusual manner: as an array of six `uint8_t` values. One key idea here is that everything in the machine is just a number. The meaning and interpretation of those bytes as the string `"Hello"` is created by the `%s` format specifier, which tells the display to present the ASCII interpretation of the bytes to the user instead of the numeric values.

```cpp
/* Code Listing A.36:
   Printing "Hello" and turning it into "Ha!"
 */

uint8_t string[] = { 72, 101, 108, 108, 111, 0 };
printf ("The string is '%s'\n", string);

string[1] = 'a';
string[2] = 0x21;
string[3] = (char) NULL;
printf ("The string is '%s'\n", string);

Since the string is an array, its individual elements can be accessed and modified; line 8 changes the `'e'` to `'a',` line 9 changes the first `'l'` to `'!',` and line 10 changes the second `'l'` to the null byte `'\0'` (literally the number 0). These changes cause line 11 to print the string as `"Ha!"` instead of the original `"Hello"`. These lines did not change the `'o'` byte stored as `string[4]`, nor did the original `'\0'` stored in `string[5]` change; both bytes are still there in memory as part of the original array. The only reason they do not get printed by line 11 is, again, because of the `%s` format specifier, which tells `printf()` to stop printing at the first null byte. [Table A.5](#tblA-5) illustrates the memory content of this array of chars from before and after the modifications, based on three different interpretations for formatting. Note that the ASCII interpretation `'\0'` is not displayed to the screen, but is shown here for completeness.

ASCII interpretation `%c` or `%s`

`H`

`e`

`l`

`l`

`o`

`\0`

Hexadecimal format `%x`

`48`

`65`

`6c`

`6c`

`6f`

`00`

Decimal format `%d`

`72`

`101`

`108`

`108`

`111`

`0`

ASCII interpretation `%c` or `%s`

`H`

`a`

`!`

`\0`

`o`

`\0`

Hexadecimal format `%x`

`48`

`61`

`21`

`00`

`6f`

`00`

Decimal format `%d`

`72`

`97`

`33`

`0`

`111`

`0`

Table A.5: Three interpretations of the bytes that make up the strings from Code Listing A.36

It is important to observe that there are two things missing from this representation. First, the quotation marks `"` used to begin and end the string appear only in the program’s source code. They are a construct of the C programming language (and other languages, as well), but they do not exist in the memory representation of the string. C needs the quotes to know where the string begins and ends in the source code. The machine does not; the string begins at the address of the first character and ends at the null byte.

Second, there is no explicit storage of the string length. This fact follows from the design structure that strings are null-terminated arrays. The designers of the language made the choice that the string length could always be dynamically determined by traversing through memory until the null byte was found. This design choice—using a single extra byte for a null terminator instead of four bytes to store an explicit length field—is a quintessential example of the space-time tradeoff that system designers face. By requiring extra time to search the string manually, the language could save three bytes of space per string; as programs could store and work with many, many strings, the cumulative space savings of three bytes per string could be potentially very large. At the time the language was designed, execution time was cheap but memory space was prohibitively expensive; thus, this design decision was a good tradeoff at the time, given the circumstances.

Given this understanding of strings as arrays of `char`s, we can now focus on issues related to using them in practice. Specifically, we can simply use the more conventional and readable notation of `"Hello"` rather than the (equivalent and perhaps more accurate) `{ 'H', 'e', 'l', 'l', 'o', '\0' }`, assuming that the reader has the correct mental model of the computer’s internal representation. The first important consideration to highlight at this point, then, is the question of where these six bytes are actually stored in memory.

[Code Listing A.37](#cla-37) illustrates this point by creating three different versions of the string `"Hello"`. The differences between lines 5, 6, and 7 are small but significant, which the other lines of the code reveal. Assuming this code is run inside the body of a function, all three variables (`array`, `pointer`, and `heap`) are local and associated with the idea of storage on the stack. Line 5, by declaring a local `array`, behaves in the intuitive manner in this regard; the `array` variable indicates an array of six bytes that are placed in the function’s stack frame. That is, line 5 operates in a similar manner to declaring an array of `int` values or any other such local array. Line 6, in contrast, places a pointer variable on the stack; the actual bytes of the string are placed into the program’s read-only data section (`.rodata`). Line 7 also places a pointer on the stack, but the `strdup()` function returns a pointer to a dynamically allocated copy of the string’s bytes on the heap. In short, these three lines illustrate how we can determine which memory segment (`stack,` `data,` or `heap)` will contain the bytes of the string.

```cpp
/* Code Listing A.37:
   Three different ways to create the string "Hello"
 */

char array[] = "Hello";
char *pointer = "Hello";
char *heap = strdup ("Hello");

/* None of these is the string length */
printf ("Sizes: %zd %zd %zd\n", sizeof (array),
        sizeof (pointer), sizeof (heap));

array[1] = 'a';
printf ("Array version: %s\n", array);
heap[1] = 'a';
printf ("Heap version: %s\n", heap);
pointer[1] = 'a'; // run-time exception

Line 6 illustrates a very common source of confusion for those new to the intricacies of C strings. Recall that the `sizeof()` operator returns the number of bytes required to store a particular variable. In the cases of the pointer and heap variables, `sizeof()` will always return the same answer regardless of the string: 8 (assuming this is a 64-bit architecture). Both of these variables are pointers, so sizeof() returns the size of an address; sizeof() never dereferences a pointer to determine the size (or length) of the object being pointed to. In the case of the `array` variable, `sizeof()` returns the total number of bytes allocated for the variable on the stack: 6. That is, calling `sizeof()` on the array version of declaration will include the null byte. Furthermore, assume that we had modified the `array` variable as in [Code Listing A.37](#cla-37), changing the string from `"Hello"` to `"Ha!"`; `sizeof()` would still return an answer of 6 (not 4), because that is how much storage space the compiler statically associated with the variable named `array`. In short, `sizeof()` **should never be used to determine the length of a string; it does not ever examine the actual contents**. Instead, if you need to compute the length of a string, you should always use `strlen()` or `strnlen()`.

🐞🐛🐌 Bug Warning

* * *

The use of pointers to declare strings leads to a number of subtle misunderstandings that end up as bugs in programs. One misunderstanding is that there is a difference between initializing a pointer to the empty string (`""`) as opposed to `NULL`. The empty string is a `char` array that consists of a single `char`: the null byte `'\0'`. As such, initializing a `char*` to the empty string makes the pointer point to a valid memory location (the address of the null byte). In contrast, setting the `char*` variable to `NULL` makes it point to nothing; dereferencing the pointer would produce a segmentation fault. This point of confusion leads to potential errors when the strings are used. Consider the following example:

```cpp
char *empty = "";
char *null = NULL;
printf ("Empty: %s; null: %s\n", empty, null);

Although there is no `*` on line 3, this code involves two pointer dereferences. That is, when `printf()` processes the `%s` format specifiers, it needs to get the contents of the string by dereferencing the empty and null pointers. When the empty string is processed, nothing interesting happens; it is a valid string, but it has no characters to print.

In contrast, when `printf()` encounters the null pointer, there is a problem; processing `%s` involves dereferencing the pointer (which is `NULL`), so this line would traditionally cause a segmentation fault. Newer implementations of the C library have modified `printf()` to detect and avoid such crashes by printing the string `(null)` when given a `NULL` pointer. This new version only makes this exception for `NULL` exactly. If the pointer is not NULL, but the value is not a valid address (e.g., try changing the code above to point to use `char *null = (char *)1;`), `printf()` will cause a segmentation fault.

📦 C library functions – `<string.h>`

* * *

`char * strdup(const char *s1);`

Dynamically allocate a copy on the heap of the string pointed to by s1.

`size_t strlen(const char *s);`

Compute the length of the string pointed to by s, measured in bytes (not including the null byte).

`size_t strnlen(const char *s, size_t maxlen);`

Compute the length like strlen(), but never scan more than maxlen bytes.

🔍 Note

* * *

All of the C string library functions have a version that starts with `str` and a version that starts with `strn`. The `strn` versions take an additional parameter (`n`) that specify a maximum number of bytes to operate on. The n parameter provides a safety termination of the operation in case the null byte that is supposed to terminate the string has been overwritten. For instance, `strlen()` would continue scanning the bytes following the intended string until a random null byte is encountered. Consequently, calling `strlen()` on such a string would turn a length that is (possibly significantly) larger than the actual length. If we started with the string `"hello"` and the null terminator was changed, we might end up with `strlen()` indicating that the string is 2500 bytes in length. This incorrect response might cause a crash or some other problem later, but the call to `strlen()` itself will not cause direct harm.

On the other hand, some functions are considered so dangerous that the `str` version should never be used. In fact, many projects scan for these functions and automatically reject code submissions that contain them. The most famous example of this is the `strcpy()` function that copies one string into a buffer that has already been allocated. If the buffer is not big enough, strcpy() will write beyond the end of it anyways, potentially corrupting other parts of memory after the buffer. For instance, if you allocate a buffer that can store only four bytes of data, using `strcpy()` to copy the string `"Hello world from your evil hacker friend!"` will write 41 bytes of data; the first four will go into the buffer, and the remaining 37 will clobber the contents of memory (i.e., other variables) after the end of the buffer. Over the past several decades, this one programming error has been one of the most common and persistent sources of security vulnerabilities.

///10.7.1. Investigating String Contents[¶]
-------------------------------------------

Given a pointer to a string, particular an input string, it is common to investigate the string’s contents for a variety of purposes. The C standard library provides several functions that can be used to examine a string. One of the most common is `strcmp()`, which takes the pointers to two strings, dereferences them, and compares their contents. The return value for `strcmp()` can be -1, 0, or 1, with 0 indicating the strings are identical. The -1 and 1 values are used to indicate the lexicographic [[1]](#f58) ordering (i.e., how they would appear in an alphabetized list) if there is a mismatch; `strcmp ("hello", "goodbye")` would return the positive value to indicate that the first argument should be ordered after the second. Switching the order of the arguments would flip the result to -1. Two additional common functions are `strchr()` and `strstr()`, which are used for searching within the contents of the string; `strchr()` looks for a specified character in the string (passed as an int rather than a `char`), while `strstr()` looks for a substring. If the character or substring is found, these functions return a pointer to the first location; otherwise, they return `NULL`.

📦 C library functions – `<string.h>`

* * *

`int strcmp(const char *s1, const char *s2);`

Compare two strings for the same content.

`char * strchr(const char *s, int c);`

Search for the first occurrence of a character c in a larger string s.

`char * strstr(const char *haystack, const char *needle);`

Search for one string (needle) as a substring of another (haystack).

[Code Listing A.38](#cla-38) demonstrates some common uses of these functions. Lines 5 and 6 specify two strings to work with. Line 9 then compares them using `strcmp()`, implicitly relying on a convention in C that 0 indicates _false_ and anything non-zero indicates _true_. Since these strings do not match, strcmp() would return 1 or -1 (1 in this particular case); C interprets this value as _true_, so the assertion is satisfied. (Note that it is a common practice to write `!strcmp(s1, s2)` to evaluate if the strings are identical. If they match, `strcmp()` returns 0 (false) and the logical negation (`!`) operator negates this value to true; if they do not match, the `!` would convert the 1 or -1 returned into false.)

```cpp
/* Code Listing A.38:
   Comparing strings and searching for substring/character occurrences
 */

char *longer = "breathe";
char *shorter = "eat";

/* Assertion holds because they are not the same */
assert (strcmp (longer, shorter));

char *substr = strstr (longer, shorter);
printf ("Substring starting at \"%s\" is %s\n", shorter, substr);

size_t count = 0;
char *walker = strchr (longer, 'e');
while (walker != NULL)
  {
    count++;
    walker = strchr (walker + 1, 'e');
  }

printf ("There are %zd occurrences of 'e' in %s\n", count, longer);

Line 11 checks if the string “eat” occurs anywhere as a substring in the longer string “breathe”. Since it does, `strstr()` would return the pointer of the first `'e'` in the string. Note that `strstr()` does not alter the original string in any way; it simply returns a pointer to the middle of the existing string. Because of this, line 12 will print the substr variable as the string `"eathe"`, as `printf()` processes `%s` by traversing through the characters until the null byte is encountered. That is, if a `strstr()` finds a substring anywhere, printing that substring will print the contents from the first occurrence of the substring all the way to the end of the original string.

Lines 14 – 20 use `strchr()` in a loop to count the number of occurrences of a particular character, `'e'` in this case. Line 15 initializes the `walker` variable to point to the first `'e'`, the third byte of the string. If line 15 had search for ‘q’ instead, `walker` would be initialized to NULL. Within each iteration of the `while`-loop, line 19 finds the next location of an `'e'` (if one exists). In this case, the call to `strchr()` indicates that it needs to start looking at `walker+1`, the first byte after an ‘e’ that has already been found. (Calling `strchr(walker, 'e');` on line 19 would create an infinite loop, since it would repeatedly find the same `'e'`!) Assuming the original string longer is null-terminated (as it is), the `while`-loop is guaranteed to terminate as written. The function `strchr()` will stop and return `NULL` once it encounters the null byte. Even if `walker` ends up pointing to the last character of the string (as it does in this case) `walker+1` can never accidentally skip over the null terminator, because `walker` is always set to point to an `'e'`.

🐞🐛🐌 Bug Warning

* * *

Many languages have built-in string types that allow easy comparison with the standard equality operator. Again, C is not one of those languages. The only safe way to check if two strings have the same contents is to use `strcmp()`. Using other comparisons, such as the `==` operator, can lead to erroneous results if not interpreted correctly. With primitives like `int` and `char`, this operator compares the values and returns true if the values match. The same is true of strings (and pointers in general), but this fact does not match our intuitions. Specifically, the value of a string (`char*`) variable or any other pointer is _the address being pointed to_. That is, the `==` operator checks if the pointers are pointing to the same location, not that the strings themselves match. The following example illustrates key features of this distinction:

```cpp
char *first = "hello";
char *second = "hello";
char third[] = "hello";
char fourth[] = "hello";

printf ("Comparing first and second:\n");
printf ("Same contents? %s\n", (! strcmp (first, second) ? "yes" : "no"));
printf ("Same string? %s\n\n", (first == second ? "yes" : "no"));

printf ("Comparing first and third:\n");
printf ("Same contents? %s\n", (! strcmp (first, third) ? "yes" : "no"));
printf ("Same string? %s\n\n", (first == third ? "yes" : "no"));

printf ("Comparing third and fourth:\n");
printf ("Same contents? %s\n", (! strcmp (third, fourth) ? "yes" : "no"));
printf ("Same string? %s\n", (third == fourth ? "yes" : "no"));

Lines 1 – 4 declare the string `"hello`” four times, twice with a `char*` and twice with a `char` array. These declarations influence the equality checks that follow. In all three cases, the `strcmp()` function will return 0 to indicate that they match; this should not be surprising since they are all initialized with the same string. With the equality check, it should not be surprising that the equality check on line 12 returns false. Recall that that `char*` initialization style puts the contents of the string in `.rodata`, whereas the char array style places the contents on the stack. In other words, second and third are pointing to different memory segments.

The equality checks on lines 8 and 16 are somewhat less predictable initially. Line 8 returns true, indicating that the `first` and `second` pointers are pointing to the same place, despite the fact that they are both initialized with what appears to be a separate copy of the string. In fact, the compiler determines that the strings are the same, which makes it redundant to store two copies in `.rodata`; by definition, the strings in `.rodata` cannot change, so one shared copy is sufficient. Line 16, on the other hand, returns false. The array declaration style must create two distinct instances, because each one can be modified independently of the other; it does not matter that the initial contents are the same. In fact, the compiler produces a warning on this line to indicate that such array comparisons always evaluate to false.

Another common task with strings is to determine if the characters fit into particular classes, such as numeric, alphanumeric, whitespace, printable, upper- or lower-case, etc. The functions defined in the `ctype.h` file provide these tests without requiring the programmer to recreate the pattern-matching required. [Code Listing A.39](#cla-39) illustrates how these class tests could be used to validate the strength [[2]](#f59) of a password. Line 9 performs a standard safety check. Functions that take a pointer as input—particular from user-supplied input—need to check explicitly for `NULL` arguments. Line 12 then throws out passwords that are shorter than 16 characters in length.

```cpp
/* Code Listing A.39:
   Using ctype.h tests to determine if a password uses multiple classes
 */

bool
is_strong (char *password)
{
  /* Safety check: Don't accept a NULL pointer */
  assert (password != NULL);

  /* Short passwords are bad */
  if (strlen (password) < 16)
    return false;

  char *walker = password;
  bool digit = false, lower = false, upper = false, punct = false;
  while (*walker != '\0')
    {
      digit |= isdigit (*walker);
      lower |= islower (*walker);
      upper |= isupper (*walker);
      punct |= ispunct (*walker++);
    }
  /* Return true only if all are true */
  return digit && lower && upper && punct;
}

Lines 15 – 23 perform the bulk of the checking. The four `bool` variables are all initialized to false, indicating that we have not yet encountered a digit (`'0'` – `'9'`), lower-case letter (`'a'` – `'z'`), upper-case letter (`'A'` – `'Z'`), or a punctuation mark (see `ispunct(3)` for the full list). The `walker` variable is set to traverse through each byte of the string until the null byte is encountered (observe that line 22 advances `walker` after all checks have been done for one character). Within the `while`-loop, each bool variable is bit-wise `OR`ed (`|`) with the result of applying the `isX` functions to the current character `*walker`. The first time that a character passes one of the tests (e.g., when `*walker` points to `'Z'` and `isupper(*walker)` is called), the corresponding bool variable will be set to 1 (true). From then on, that variable can never become false, because applying bit-wise OR of 1 with any value will always produce a non-zero result. Consequently, line 25 will return true the password contains at least one character from each of the four classes.

📦 C library functions – `<ctype.h>`

* * *

`int isalnum(int c);`

Determines if c is alphanumeric.

`int isalpha(int c);`

Determines if c is alphabetical letter.

`int isdigit(int c);`

Determines if c is a numeric digit.

`int isspace(int c);`

Determines if c is a whitespace character (including tab, newline, etc.).

`int islower(int c);`

Determines if c is lower-case alphabetical character.

`int isupper(int c);`

Determines if c is upper-case alphabetical character.

`int ispunct(int c);`

Determines if c is a punctuation mark.

///10.7.2. Common String Manipulations[¶]
-----------------------------------------

Most modern programming languages provide a simple mechanism for a very common task: merging strings. Some languages use the `+` operator, such as `string1 + string2` to concatenate the two strings; others use a `.` operator instead. Unfortunately, C is not such a language. There are various functions that can be used for this purpose, with `strncpy()` and `strncat()` being two of the first encountered. Both of these functions copy the contents of one string (`s2`, passed as the second argument) into a portion of memory identified by the first argument (`s1`). The key difference between the two is that `strncpy()` will copy the bytes starting at the exact location that s1 points to; `strncat()` appends the strings by copying the bytes starting at the first null byte at or after `s2`. In other words, `strncpy()` replaces the contents of the first string, whereas `strncat()` concatenates the two.

Unlike their unsafe cousins `strcpy()` and `strcat()` (which should **NEVER** be used), `strncpy()` and `strncat()` take a third argument that specifies a maximum number of bytes to copy. If the length of `s2` is less than `n`, then the function will stop before processing `n` bytes. The `memcpy()` function shown below behaves similarly to `strncpy()`, except that it ignores the null byte; that is, `memcpy()` is used to copy an arbitrary memory buffer from one location to another, regardless of whether that buffer contains a string. In that regard, `memcpy()` will always copy exactly n bytes, unless some unusual circumstance occurs (such as the `dst` and `src` buffers overlapping, which is undefined behavior in the C specification).

📦 C library functions – `<string.h>`

* * *

`char * strncpy(char *s1, const char *s2, size_t n);`

Copy string s2 into the buffer s1; stops after copying n bytes or at the first ‘0’.

`char * strncat(char *s1, const char *s2, size_t n);`

Appends string s2 after the string s1; stops after copying n bytes or at the first ‘0’.

`void * memcpy(void *dst, const void *src, size_t n);`

Copy n bytes of memory from src to dst; does not stop at ‘0’.

Although it is certainly fair to refer to `strcpy()` or `strcat()` as an unsafe version of `strncpy()` or `strncat()`, it would be a mistake to consider the latter two functions truly safe. One key aspect of this is whether or not these functions guarantee that the result is null terminated. [Code Listing A.40](#cla-40) demonstrate two examples of this problem. The `n` argument on line 10 ensures that only the `'h'` and `'e'` characters get copied into the buffer array. That is, the `n` argument for `strncpy()` places a maximum number of bytes copied, and the function does not guarantee that one of these is a null byte. Line 11, then, is likely to print additional characters after the string `"he"`, because there is no null byte in buffer. As such, the `%s` causes `printf()` to continue traversing through memory until a null byte is encountered.

```cpp
/* Code Listing A.40:
   strncpy() and strncat() do not agree on null-termination of strings
 */

char buffer[2];
strncpy (buffer, "hello", 2);
printf ("buffer: %s\n", buffer);

char trouble[10];
strncpy (trouble, "hello", 10);
strncat (trouble, " world", 5);
printf ("trouble: %s\n", trouble);

🐞🐛🐌 Bug Warning

* * *

The `strncpy()` and `strncat()` functions are a frequent source for error. As describe above, they differ on their interpretation of the `n` parameter and whether or not null-termination is guaranteed (yes for `strncat()`, no for `strncpy())`. Besides the confusion around these issues, they still leave plenty of room for errors on the part of the programmer. One common mistake is to switch the order of the first two arguments, mistaking the source and the destination of the copy operation. Another common mistake with these functions can be illustrated in the following line of code:

strncpy (destination, source, strlen (source));

This line of code, in essence, re-creates the functionality of the banned `strcpy()` function. When `strcpy()` runs, it will only stop when it encounters a null byte in source; in the process, it has copied `strlen(source)` bytes over to the destination. By making the n parameter be the same as the number of bytes in the string, this line of code is setting a redundant maximum length check (`strcpy()` would already stop after `strlen(source)` of `data`). **The third parameter must always be based on how much space is available in the destination, never the source**.

Another common mistake that occurs with these functions is due to confusion regarding the `sizeof()` operator as discussed previously. Consider the following example:

```cpp
char *buffer = calloc (100, sizeof (char));
strncpy (buffer, "This is a string", sizeof (buffer));

The first line of this example creates a dynamically allocated buffer of 100 bytes of space. Since it uses `calloc()`, all of the bytes are set to null bytes (0 = `'\0'`). On the second line, the source argument string is 16 characters in length; clearly the buffer has sufficient space for the entire string. The problem with this line of code is that only the bytes `"This is "` will be copied over, due to the use of `sizeof()`. As described previously, sizeof() can never check how much space a pointer is pointing to. Instead, `sizeof()` returns the amount of space required for the variable itself. Since `buffer` is a `char*` (i.e, it is a pointer), its size is the size of an address: 8 bytes. It does not matter that `buffer` is pointing to 100 bytes allocated on the heap. Based on the first line of code (with a hard-coded size of 100), the last argument to `strncpy()` would need to be the hard-coded value 99 (keeping the $100^{th}$ byte as 0 to guarantee a null-terminated string).

A third common mistake occurs when the programmer forgets about the implications of memory segment permissions. In the following example, `message` is declared as a `char*` that points to the hard-coded string `"Hello, "`, which resides in the read-only global data segment (`.rodata`). Line 2, then, is an attempt to write into read-only memory. The result would be a segmentation fault or an abort trap, depending on the architecture.

```cpp
char *message = "Hello, ";
strncat (message, username, 20);

While `strncpy()` and `strncat()` focus on building or merging strings, another common task is to split a string into smaller parts, a procedure known as _tokenizing_. C provides two functions, `strtok()` and `strtok_r()`, for this purpose. In both cases, when the function is first called, the `str` parameter points to the string to tokenize; on subsequent calls, `str` is set to `NULL` to indicate that the function is continuing to process the previous string. The sep parameter is a pointer to a string of separator characters; whenever one or more of these characters is encountered in a row, `strtok()` or `strtok_r()` would return a pointer to the token ending at that character. The difference between the two functions is that `strtok_r()` is [reentrant](#term-reentrant), while `strtok()` is not. (Reentrancy is discussed in Chapter 7.) In short, `strtok()` uses a static variable to keep track of where to continue within the string. This approach fails when there are multiple threads calling `strtok()` on distinct strings; the threads might accidentally receive each other’s tokens. If there are multiple threads in execution, the `strtok_r()` version is needed to avoid this dilemma; the third parameter, `lasts`, keeps track of the tokenization of the string, thus eliminating the race conditions that can occur with static variables.

📦 C library functions – `<string.h>`

* * *

`char * strtok(char *str, const char *sep);`

Split the string `str` at an occurrence of the separator `sep`.

`char * strtok_r(char *str, const char *sep, char **lasts);`

Thread-safe version of `strtok()`; sets lasts to the beginning of the next token.

As an example of tokenization, consider a comma-separated value (CSV) file, a common format for sharing collections of data. Each line in a CSV file consists of a number of data fields with a comma to separate them. As an example, consider a CSV file of holidays for the year 2020. One line of the file might look as follows:

Wed,Jan,01,2020,New Year's Day

Once the file contents have been read into memory, the lines might be tokenized to retrieve the individual fields of the line. [Code Listing A.41](#cla-41) splits this line one token at a time, storing the fields in the fields of a `struct` declared as the `holiday_t` type.

```cpp
/* Code Listing A.41:
   Using strtok_r() to split a CSV file line
 */

/* Assume line contains "Wed,Jan,01,2020,New Year's" */
holiday_t nyd;
char *save = NULL;
nyd.wkd = strtok_r (line, ",", &save); // set weekday to "Wed"
nyd.mon = strtok_r (NULL, ",", &save); // set month to "Jan"
nyd.day = strtok_r (NULL, ",", &save); // set day to "01"
nyd.yer = strtok_r (NULL, ",", &save); // set year to "2020"
nyd.nam = strtok_r (NULL, ",", &save); // set name "New Year's"

In each of the lines 8 – 12, the call-by-reference parameter `&save` changes the save pointer to keep track of the continuation point that immediately follows the separator instance. For instance, line 8 sets save to point to the `'J'` in `"Jan"` and returns a pointer to the string `"Wed"`. When the first parameter to `strtok_r()` is `NULL,` this continuation point determines where the function will look for the next delimiter. On line 9, then, `strtok_r()` starts looking at the ‘J’ and finds the comma just after `"Jan"`; `strtok()` then updates save to point to the first ‘0’ and returns the token `"Jan"`.

There are subtle aspects to the behavior of `strtok()` and `strtok_r()` that require consideration. First, these functions do not return copies of the tokens; they _modify the original string_ and return a pointer into it. Specifically, the first occurrence of any of the characters in the separator string `sep` is replaced with the null byte. [Table A.6](#tblA-6) illustrates two snapshots of the string pointed to by line in [Code Listing A.41](#cla-41), both the original version and after two calls to `strtok_r()`. After two calls, the first two commas in the line have been overwritten with a null byte. Because of this modification, the pointer that is returned is a complete string. The first call to `strtok_r()` returns a pointer to the `'W'` at the beginning of the line, but the token returned is the string `"Wed"`. The fact that the original string gets modified means that **string constants cannot be tokenized**. Since string constants are stored in `.rodata`, tokenizing it would require writing a null byte into read-only memory.

Original string contents:

`W`

`e`

`d`

`,`

`J`

`a`

`n`

`,`

`0`

`1`

`,`

`2`

`0`

`2`

`0`

`,`

`N`

`e`

`w`

 

`Y`

`e`

`a`

`r`

`'`

`s`

`\0`

After line 5 of [Code Listing 41](#cla-41):

`W`

`e`

`d`

`\0`

`J`

`a`

`n`

`\0`

`0`

`1`

`,`

`2`

`0`

`2`

`0`

`,`

`N`

`e`

`w`

 

`Y`

`e`

`a`

`r`

`'`

`s`

`\0`

Table A.6: The contents of the line variable before line 4 and after line 5 of [Code Listing A.41](#cla-41)

Second, because the pointers returned are to the original string, freeing or modifying the original data can corrupt the tokens. In the CSV example above, we assumed that the entire file contents were read into memory. What if this were not the case? Instead, the program reads a line of the file into memory at a time, repeatedly overwriting the buffer variable `line`. This approach would allow the first line to be tokenized successfully, and the fields of the `holiday_t` `struct` would be pointing to their tokens. But the next line of the file would get read into this exact same memory. As such the fields of the `holiday_t` would now be pointing to characters in the second line of data, not the first. On the other hand, perhaps the `line` variable points to a dynamically allocated buffer that is created anew for each line of the file. In this case, the `holiday_t` fields could be corrupted if `line` is freed; the fields would still be pointing to the heap where the contents of `line` were stored, but that part of the heap would now be invalid.

Third, `strtok()` and `strtok_r()` ignore repeated instances of separators. This behavior can be problematic for CSV files, as fields can be blank. For example, assume that the CSV file from above was modified to include a location field between the year and name of the holiday. If these fields were missing for the New Year’s Day holiday, that line (and another) of the file might look like:

Wed,Jan,01,2020,,New Year's Day
Fri,Feb,14,2020,Charlottesville,Valentine's Day

One call to `strtok_r()` would get the string `"2020"`. The next call would then get the string `"New Year's Day"`, rather than an empty string to indicate the missing location field. There are times when skipping over repeated separators is advantageous (consider skipping over repeated whitespace in a C source code file), but there are also times where it can lead to incorrect results. The `strtok()` and `strtok_r()` work well for the former cases, but other approaches are needed for the latter.

[Code Listing A.42](#cla-42) demonstrates two techniques for splitting a file’s contents based on lines. To start, assume that the file’s contents have been read into `file_contents` and (for simplicity) this buffer is null-terminated. Lines 5 – 17 store copies of the lines without using `strtok()` or `strtok_r()`. Instead, these lines use `start_of_line` to keep track of where a line begins (initially, the start of the file contents). Line 6 then uses `strchr()` to identify the end of the first line by looking for the `'\n'` character. Line 10 uses `strndup()` to make a dynamically allocated copy of the line. The semantics of `strndup()` are like strncat(); it will copy up to the specified number of bytes and it will add on a null terminator. Since `end_of_line – start_of_line` is the exactly the number of bytes in the line, line 10 makes a complete null-terminated copy and stores the address of this copy into an array. Line 12 then moves `start_of_line` just past the newline character so that it points to the beginning of the next line. Line 14, then, starts looking for the next `end_of_line` after that point. The additional copy on line 17 is necessary because the `while`-loop will terminate when there are no more newline characters; when this occurs, `start_of_line` is pointing to the last line (which has no `'\n'` after it). Line 17 can use the standard `strdup()` instead of `strndup()`, because the original `file_contents` are null terminated.

```cpp
/* Code Listing A.42:
   Tokenizing and storing file lines without and with strtok()
 */

char *start_of_line = file_contents;
char *end_of_line = strchr (start_of_line, '\n');
while (end_of_line != NULL)
  {
    line_copies[lineno++] =
      strndup (start_of_line, end_of_line - start_of_line);
    /* Next line starts after the '\n' */
    start_of_line = end_of_line + 1;
    /* Find the next end of line */
    end_of_line = strchr (start_of_line, '\n');
  }
/* Copy the last line */
line_copies[lineno] = strdup (start_of_line);

lineno = 0;
char *line = strtok (file_contents, "\n");
while (line != NULL)
  {
    /* Dynamically allocate a copy and store the pointer */
    all_lines[lineno++] = strdup (line);
    line = strtok (NULL, "\n");
  }

When line 19 begins processing, the original `file_contents` have not been altered in any way. The use of `strchr()` and `strndup()` in lines 6 – 17 do not write anything into this buffer. Consequently, we can begin to use `strtok()` and start over. Through each iteration of the `while`-loop in lines 21 – 26, the `line` variable points to the current (null-terminated) line. To keep copies of the lines, again, we use `strdup()`. In practice, it does not make sense to perform both of these loops, since they are keeping track of the same data. The purpose of combining them in [Code Listing A.42](#cla-42) is to show that they ultimately end up as two equivalent ways to accomplish the same goal; the only difference is that the `strtok()` approach modifies the original file_contents, whereas the `strchr()` approach does not. The `while`-loop structure in lines 21 – 26 is a common approach for using `strtok()`.

///10.7.3. Converting Between Strings and Integers[¶]
-----------------------------------------------------

One final common task in relation to strings involves converting numeric values back and forth between representations. When reading user input or data from a file, numeric text data (`"123"`) might need to be converted to one of C’s integer primitive representations (123) for easy manipulation or compact storage. On the other hand, integers often need to be converted to their string format to append to other text data (e.g., writing the HTTP header line `"Content-Length: 123\r\n"` when the length has been stored as a `size_t` variable). [Code Listing A.43](#cla-43) illustrates the difference in the internal representations of 123 (as a `uint8_t`) and `"123"` (as a string).

```cpp
/* Code Listing A.43:
   Printing the byte contents of an integer and string
 */

uint8_t integer = 123;
char string[] = "123";

uint8_t *walker = &integer;
for (size_t i = 0; i < sizeof (integer); i++)
  printf ("%02" PRIx8 "  ", *walker++);
printf ("\n");

walker = (uint8_t *) &string;
for (size_t i = 0; i < sizeof (string); i++)
  printf ("%02" PRIx8 "  ", *walker++);
printf ("\n");

The particular `for`-loops here might appear odd, but they are used to show that the two variables are being handled in the same way. Specifically, the loop on line 9 will only have one iteration, because integer is only one byte in size (as a `uint8_t`). At the same time, the loop on line 14 deliberately uses `sizeof()` on a string (instead of `strlen()`), which is a common source of bugs; however, this approach lets us examine all four bytes in the `char` array.

The first loop demonstrates that the internal representation of `integer` is the single byte `0x7b`. The second loop demonstrates that the representation of `string` is the four consecutive bytes `0x31`, `0x32`, `0x33`, and `0x00`. (Printing these four bytes were at once would produce the value `0x00333231` due to endianness.) The issue of conversion focuses on ways to translate automatically between these two byte representations, which do not appear to be similar.

📦 C library functions – `<stdlib.h>`

* * *

`long strtol(const char *str, char **endptr, int base);`

Translate the numeric string str into a long representation for the provided base.

The `strtol()` function handles the conversion from integer to string. [[3]](#f60) The `str` parameter points to a string containing the number, and the base indicates an arbitrary numeric base (10 for decimal, 16 for hexadecimal, or an arbitrary base such as 13 for base-13). The string can contain multiple numeric values separated by non-numbers (e.g., `"123 456 -42"` for the three values 123, 456, and -42). When `endptr` is non-`NULL` (i.e., it is a call-by-reference value), it will be set to point to the first character after the current number.

[Code Listing A.44](#cla-44) demonstrates multiple ways that `strtol()` can be used. Line 5 starts by creating a string `"123 -32 alpha"`. This can be broken down into the integer values 123 and -32, but the `"alpha"` cannot be interpreted as an integer. Lines 9, 14, and 20 use `strtol()` to parse this string into these numeric components. Line 9 uses the original `numbers` string as the first argument, whereas lines 14 and 20 use `end` for this parameter. Using `end` on the subsequent calls is necessary, because `strtol()` does not keep track of any prior progress; using `numbers` each time would repeatedly return the first value, 123.

```cpp
/* Code Listing A.44:
   Converting from string to integer representations
 */

char *numbers = "123 -32 alpha";
char *end = NULL;

/* strip off the 123 and make end point to " -32 alpha" */
long result = strtol (numbers, &end, 10);
assert (errno != EINVAL); // no match indicates success
printf ("Result = %ld; end = '%s'\n", result, end);

/* continue from " -32 alpha" to get the -32 */
result = strtol (end, &end, 10);
assert (errno != EINVAL); // no match indicates success
printf ("Result = %ld; end = '%s'\n", result, end);

/* continue from " alpha", which cannot be processed */
char *final = NULL;
result = strtol (end, &final, 10);
assert (errno == EINVAL); // match indicates strtol() failed
printf ("Result = %ld\n", result);
printf ("end = '%s'; final = '%s'\n", end, final);
assert (final == end);

/* use a bizarre base-11 format, ignoring endptr */
numbers = "60a1";
result = strtol (numbers, NULLclass="p">, 11); printf ("Result = %ld\n", result);

The `strtol()` function provides two ways to check for errors in the processing. The first (and most straightforward) way is to use the errno global variable. On a failure (such as `" alpha"`), `strtol()` sets errno to `EINVAL`, which is a positive constant (`errno` is set to 0 on success). The `assert()` calls on lines 10, 15, and 21 all pass, indicating the calls to `strtol()` on lines 9 and 14 succeed, while line 20 fails. The other mechanism is through the `endptr` parameter. Line 20 uses `end` as the input, pointing to the string `" alpha"`. After `strtol()` runs, final is also set to this location. If the endptr ends up at the beginning of the string (i.e., `final` after the call matches `end`, which hasn’t changed), then `strtol()` was unable to process any data successfully.

Lines 27 – 29 demonstrate other features of `strtol()`. First, the endptr parameter can be (and is often) ignored by passing `NULL`. Even with a `NULL` `endptr,` we could still check errno to determine if the conversion succeeded. Second, `strtol()` supports generally arbitrary base values (2 – 36 are allowed). Conventionally, C numeric constants use `0x` as a prefix to indicate hexadecimal format (e.g., `0x7ff`) and a leading 0 to indicate octal (e.g., `0644`); otherwise, the number is interpreted as decimal. Importantly, this means that C has no convention to declare binary constants. The `strtol()` function supports this by taking 2 as the base parameter.

Converting values in the opposite direction, from integers to strings, is mostly intuitive, because it is very similar to one of the first functions novices learn in C: `printf()`. The main difference is that the `snprintf()` function takes two parameters before the format string to indicate the destination and the maximum number of bytes. (The `sprintf()` function does not take a maximum number of bytes, which makes this function unsafe in the same ways as `strcpy()` or `strcat()`. As such, `sprintf()` should never be used.)

📦 C library functions – `<stdlib.h>`

* * *

`int snprintf(char *str, size_t size, const char *format, ...);`

Format a string in memory similar to printing to the screen.

[Code Listing A.45](#cla-45) highlights the similarities between `snprintf()` and the more familiar `printf()`. Both functions take a format string (`"%d"` or `"%d\n"`) to indicate how the number should be formatted, along with the number as an additional argument. The primary difference is that `snprintf()` also indicates a destination to write the formatted value into (the `buffer`). Once the value has been written into the buffer, it can be printed (if needed) using the `%s` format specifier.

```cpp
/* Code Listing A.45:
   Converting from integer to string is similar to printing to standard I/O
 */

int number = 42;
char buffer[3];

/* Print the number to the screen */
printf ("%d\n", number);

/* "Print" the number into the buffer */
snprintf (buffer, 3, "%d", number);

/* Print the string */
printf ("%s\n", buffer);

Recall from the discussion of `strncpy()` and `strncat()` that the two functions had different interpretations of the respective maximum size parameter, `n`. Specifically, `strncpy()` would copy a maximum of `n` bytes, potentially leaving the string un-terminated if those `n` bytes did not contain the null byte. In contrast, `strncat()` would write a maximum of `n+1` bytes, because it always appends the null byte. The `snprintf()` function adds a third interpretation: it will print up to `n-1` bytes and then append the null byte. Frustrating! [Code Listing A.46](#cla-46) summarizes this situation. Since both `strncat()` and `snprintf()` guarantee null termination, they both end up writing a null byte; however, `strncat()` appends this after the two bytes `'h'` and `'e'`, whereas `snprintf()` does so after only one byte `'4'`. Unlike the other two, `strncpy()` does not guarantee null termination.

```cpp
/* Code Listing A.46:
   Converting from integer to string is similar to printing to standard I/O
 */

strncat (buffer_1, "hello", 2); // copies 3 bytes 'h', 'e', '\0'
strncpy (buffer_2, "hello", 2); // copies 2 bytes 'h' and 'e'
snprintf (buffer_3, 2, "%d", 42); // copies 2 bytes '4' and '\0'

🐞🐛🐌 Bug Warning

* * *

The `snprintf()` function, once again, creates a very common vector for buffer overflow vulnerabilities. One of the challenges—and common mistakes—arises from the anticipation of what is a likely integer value as compared to what is a _possible_ one. The buffer from [Code Listing A.45](#cla-45) is not a safe size for the format specifier %d. As an int is typically four bytes, its string form can be as long as 12 characters in length (for example, including the negative sign and null byte for the `INT_MIN` constant `"-2147483647")`. As such, the buffer should generally be larger than required. One simple way to do this (and to ensure the bytes are all initialized to 0) is to use `calloc()` to allocate enough space. If needed, `realloc()` could then be used to shrink the buffer.

```cpp
char *buffer = calloc (12, sizeof (char));
snprintf (buffer, 12, "%d", 35);

/* Shrink it down to size, keeping an extra byte for '\0' */
buffer = realloc (buffer, strlen (buffer) + 1);

Since `snprintf()` takes a normal format string (which can contain a mix of string data and multiple format specifiers), it creates an easier mechanism to concatenate multiple values together into a single string. [Code Listing A.47](#cla-47) demonstrates a simple example of this practice to build the string `"5 + 10 = 15\n"` using `int` variables.

```cpp
/* Code Listing A.47:
   Using snprintf() to build a string that mixes integer and string data
 */

int x = 5, y = 10;
char *sum = calloc (100, sizeof (char));
snprintf (sum, 100, "%d + %d = %d\n", x, y, x + y);

Chapter 4 introduces the structure of HTTP headers. These headers consist of a series of lines, each ending in `"\r\n"`. As one example, consider the following header snippet:

HTTP/1.0 200 OK\r\n
Content-Length: 37\r\n
Connection: close\r\n
Content-Type: text/html\r\n
\r\n

Assuming some of these fields are stored in variables, this could be constructed with a single `snprintf()` call, as shown in [Code Listing A.48](#cla-48).

```cpp
/* Code Listing A.48:
   Building an HTTP response header with one snprintf()
 */

snprintf (header, MAX_HEADER_LENGTH,
          "HTTP/%d.%d %d %s\r\n" // version, code, status
          "Content-Length: %d\r\n" // length
          "Connection: close\r\n"
          "Content-Type: %s\r\n\r\n", // type
          vers_major, vers_minor, code, status, length, type);

[Code Listing A.48](#cla-48) relies on the fact that string constants are concatenated in C. As such, lines 6 – 9 all build a single format string. Displaying them as separate lines in the code makes the organization easier to understand from the programmer’s perspective. This string could also be built a line at a time with repeated calls to `strncat()`, but this version simplifies the processing as a single function call.

🐞🐛🐌 Bug Warning

* * *

Another common use of `snprintf()` is to inject formatted numbers into the middle of an existing string. For instance, consider an event logging mechanism that uses a common reporting form for events. The `snprintf()` function could be used to fill these in, but requires special care as shown in the example below.

```cpp
char record[] = "Month [   ] Day [  ] Year [    ]";
snprintf (record + 7, 4, "%s", month);  // write [mon]
snprintf (record + 17, 3, "%-2d", day); // write [da]
snprintf (record + 27, 5, "%4d", year); // write [year]

/* Restore the ] characters that snprintf() overwrote with the
   null byte */
record[10] = ']';
record[19] = ']';
record[31] = ']';

The problem is that `snprintf()` always null terminates what it writes. As such, if line 2 writes the month as `"Jan"`, the record variable would become the string `"Month [Jan"`. The rest of the string would still be there in memory, but `snprintf()` overwrite the first bracket with the null byte; printing the string at that point would stop there instead of showing the full record. Lines 8 – 10 fix this problem by restoring the brackets to their original locations, overwriting the null bytes that `snprintf()` had added.

Also note that the original record string created three spaces between the brackets for the month, two for the day, and four for the year. The `size` parameter for lines 2 – 4 added one to each of these values (four, three, and five, respectively), because `snprintf()` includes the null byte in this count. Thus, writing the string `"Jan"` into the month field requires writing four bytes, not three.

[[1]](#id3)

The use of _lexicographic_ instead of _alphabetic_ is common and intentional in computing, as the former is more general and works with non-alphabetical characters. For instance, it does not make sense to characterize the _alphabetical_ ordering of “15” as compared to “3” since neither contain letters in the alphabet. However, “15” comes before “3” in lexicographical ordering.

[[2]](#id4)

Determining if a password is strong is significantly more complicated than this function, and this should not be used for real security purposes. For instance, the password also needs to be compared with common dictionary words, previous passwords, easily guessed patterns, etc. This example just illustrates how these character class checks can be used as part of this procedure.

[[3]](#id7)

C also has an older function `atoi()` for this purpose, though this function is deprecated and should not be used in new code. The `strtol()` function adds explicit support for multiple bases, whereas atoi() handled this implicitly within the string; `strtol()` also returns a long rather than the int returned by `atoi()`, supporting larger values. Finally, and most importantly, `atoi()`’s error handling was weak, returning a 0 for bad input; as such, it was not possible to distinguish between `"0"` and truly bad input.



//10.8. Function Pointers[¶]
============================

As we have seen, pointers have many uses in the C language. They can be used to provide indirect references to primitive types, form the basis of dynamically sized arrays, create instances of structs on demand, manipulate string data, and so on. One additional use of pointers is to create a reference to a function. That is, a [function pointer](#term-function-pointer) is a variable that stores the address of a function. Readers who have previous experience with assembly language may recall that the name of a function is the same as a global _label_ for an instruction. In other words, the name of a function is an alias for the address of the first instruction in the function’s body. A function pointer, then, stores this address.

To get started with function pointers, [Code Listing A.49](#cla-49) defines two simple functions that we will use later. A key aspect of these functions is that they have the same _signature_. That is, the two functions take the same number of arguments, the types of the arguments are identical (the names of the parameters do not matter), and the return type is the same. In this example, both functions take two `int` arguments and return an `int`. Lines 6 and 7 are explicit _function prototypes_ that declare the functions’ interfaces. Function prototypes are only required when a function is used before it is defined. For instance, if line 12 were changed to make a call to `sub()`, the compiler would use the prototype on line 7 to confirm that the call is correctly formatted; calling `sub (1, 2)` would be acceptable, while calling `sub ("marine")` would not, due to the required parameter types. Function prototypes can be omitted (and usually are) if the function definition occurs before the first call to it.

```cpp
/* Code Listing A.49:
   Two simple arithmetic functions with the same function signature
 */

/* Function prototypes as they might appear in a header file */
int add (int, int);
int sub (int, int);

int
add (int addend_1, int addend_2)
{
  return addend_1 + addend_2;
}

int
sub (int minuend, int subtrahend)
{
  return minuend - subtrahend;
}

[Code Listing A.50](#cla-50) uses the function declarations from [A.49](#cla-49). Line 5 starts by declaring a function pointer variable called `fun`. The full explanation of function pointer declarations is provided below. For now, it is sufficient to note that `fun` is a pointer, so it can store an address. The names of the functions from [Code Listing A.49](#cla-49), `add` and `sub`, are simply readable aliases for the addresses of the functions’ code. In other words, we can think of `add` and `sub` as address constants; setting `fun = add` is similar to setting `fun = NULL` to create a null pointer. Lines 5 and 8, then assign the addresses of add and sub to the pointer `fun`. The effect of this assignment is to make `fun` an alias for the function at that particular point. Lines 6 and 9 demonstrate that using a function pointer to make a function call works exactly like a standard function call. That is, the call to `fun (5, 3)` is identical to calling `add (5, 3)` directly.

```cpp
/* Code Listing A.50:
   Declaring a function pointer and assigning its value
 */

int (*fun) (int, int) = add;
int sum = fun (5, 3);

fun = sub;
int diff = fun (5, 3);

printf ("Sum is %d, difference is %d\n", sum, diff);

Function pointer declarations follow the structure defined in [Figure 10.8.1](FunctionPointers.html#funptr). To understand the declaration, the components need to be read from the inside to the outside. That is, the middle portion, `(*fun),` is read first. Specifically, the `*` declares a pointer, just as any other pointer declaration. Wrapping the `*` and the variable name in parentheses indicates that it is a function pointer, in particular. Traversing outward, the rest of the line completes the function pointer’s type. The variable declaration starts with a single return type and ends with the list of parameter types in parentheses. In sum, a function pointer declaration looks exactly like a standard function prototype, with the exception of the `(*name)` structure where the prototype would just have the function name.

![Components of a function pointer declaration](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.A.7.png)

Figure 10.8.1: Components of a function pointer declaration

🐞🐛🐌 Bug Warning

* * *

When working with function pointers, it is critical to understand the placement and exact role of parentheses. The `(*name)` structure is required to declare a variable instance. Similarly, the parameter list must also be in parentheses, just like standard function prototypes and definitions. One common mistake is to use parentheses on the _right side_ of the assignment statement as shown below:

int (*ptr) (void) = trouble ();

With one exception, this line is incorrectly written. The declaration on the left indicates that there is a new function pointer variable, `ptr`. Any function that `ptr` can be set to must adhere to the interface that it accepts no arguments (specified by `(void)`) and returns an `int`. Given this declaration, the compiler will check to see if the value being assigned to `ptr` has the same type. That is, does the right side of the assignment evaluate to a pointer to a function, specifically one that takes no arguments and returns an `int`? Consider one possible declaration for the function called `trouble()`:

```cpp
int trouble (void); // trouble's function prototype

int 
trouble (void)
{
  /* Do something */
  return 5;
}

At first glance, everything looks okay. The prototype on line 1 and the definition of `trouble()` both indicate that the function takes no arguments and returns an `int`. However, **that is exactly why the assignment above is incorrect**. On the original line, we are not assigning the address of the function to `ptr`. Rather, the right hand of the assignment _calls_ `trouble()`, which runs and returns the `int` value 5. As such, the line above is equivalent to setting `ptr = 5`; this is assigning an `int` value to a pointer variable, not assigning an address!

The exception to this description is if `trouble()` was specifically a function that returned another function. That is, `trouble()` is a function that takes no arguments and returns a pointer to a function, such that the returned function takes no arguments and returns an `int`. The syntax for this version of `trouble()` is, frankly, atrocious and unreadable. We provide it here for the curious, though we do not recommend ever writing code like this. If it is necessary to return a function pointer, `typedef`s can make this code significantly more readable.

```cpp
/* The full prototype to be precise */
int (*trouble (void)) (void);

int
(*trouble (void)) (void)
{
  /* return a function pointer */
}
```

Perhaps the most confusing aspect of this declaration is which parameter list goes with which function. Counterintuitively, the inner portion defines the input parameters for `trouble()`. That is, the portion that reads (`*trouble (void)`) indicates that `trouble()` takes no parameters; the portions outside of these parentheses indicate the return type and parameter list for the function that `trouble()` returns.

///10.8.1. Passing Function Pointers as Arguments[¶]
----------------------------------------------------

Given the complexity of declaring and working with function pointers, it is natural to ask why they are beneficial or even necessary. The two most common uses are to pass a function as an argument to another function and to build a lookup table of functions. We will start with the first example. This application might seem familiar to readers who are familiar with functional programming or the concept of lambdas that has been added to languages like Java. [Code Listing A.51](#cla-51) provides an example of this kind of usage, using the (much maligned) Bubble Sort algorithm.

```cpp
/* Code Listing A.51:
   Bubble sort implementation that takes a custom comparison operation
 */

void
bubble_sort (void *array[], size_t length, int (*compare) (void *, void *))
{
  for (size_t i = 0; i < length - 1; i++)
    for (size_t j = 0; j < length - i - 1; j++)
      if (compare (array[j], array[j+1]) > 0)
        {
          void *temp = array[j];
          array[j] = array[j+1];
          array[j+1] = temp;
        }
}
```

The `bubble_sort()` function takes three parameters: an array of pointers, an array length, and a comparison function. As the `array` parameter is an array of `void*` elements, this implementation can sort any type of objects. That is, the array could contain pointers to strings, pointers to `FILE` objects (e.g., to sort based on file size), or pointers to any custom data types. The compare function pointer must point to a function that takes two `void*` inputs and returns an `int`. The type cannot indicate this, but any function pointer passed as the compare parameter needs to adhere to a convention based on `strcmp()`; return 1 if the first item is “greater” than the second, -1 if the second item is “greater,” and 0 if they match. [Code Listing A.52](#cla-52) provides an example of such a comparison that performs a case-insensitive string comparison (as opposed to the case-sensitive nature of `strcmp()`).

```cpp
/* Code Listing A.52:
   A case-insensitive string comparison
 */

int
strcmp_nocase (char *first, char *second)
{
  /* Assume non-empty strings for simplicity */
  char *first_walk = first;
  char *second_walk = second;

  /* Walk until one of the strings reaches a null byte */
  while (*first_walk != '\0' && *second_walk != '\0')
    {
      /* Make a lower-case copy of the characters and compare */
      int fc = tolower (*first_walk);
      int sc = tolower (*second_walk);
      if (fc > sc)
        return 1;
      if (fc < sc)
        return -1;

      /* Continue to the next character if the current matches */
      first_walk++;
      second_walk++;
    }

  if (*first_walk == *second_walk) // Strings are identical
    return 0;

  if (*first_walk == '\0') // First was shorter than second
    return -1;

  return 1; // First was longer than second
}
```

[Code Listing A.53](#cla-53) applies these two functions to demonstrate the power of function pointers for flexible code execution. Line 5 starts with an array of strings with a mixture of cases. Line 7 passes this array to `bubble_sort()` along with a pointer to the `strcmp_nocase()` function. To be precise, the type of `strcmp_nocase()` does not match the type required for the compare parameter to `bubble_sort()`; `strcmp_nocase()` takes two `char*` arguments, whereas the type declaration of compare indicates that the function needs to take two `void*` arguments. Hence, line 7 explicitly casts `strcmp_nocase()` as needed. Similar casting is needed at line 14 for `strcmp()`.

```cpp
/* Code Listing A.53:
   Passing two functions as parameters to the generic Bubble Sort
 */

char *strings[] = { "Hello", "goodbye", "Hi", "HIT", "ha" };

bubble_sort ((void **)strings, 5, (int (*)(void *, void *))strcmp_nocase);

printf ("SORTED (case-insensitive):");
for (size_t i = 0; i < 5; i++)
  printf (" %s", strings[i]);
printf ("\n");

bubble_sort ((void **)strings, 5, (int (*)(void *, void *))strcmp);

printf ("SORTED (case-sensitive):");
for (size_t i = 0; i < 5; i++)
  printf (" %s", strings[i]);
printf ("\n");

Although the preceding code works, the number of parentheses and `*` operators can make it hard to read. One common way to improve the readability is to use a typedef to simplify the type declarations and function calls. [Code Listing A.54](#cla-54) demonstrates this practice by defining a new type, `comp_t`, as a function pointer that takes two `void*` parameters and returns an `int`. Readers who are comfortable with `typedef` may fine the version on line 5 to be odd; normally, the new type name appears at the end of the line, just before the `;` character. With function pointers, however, the type name is placed where the variable name would go in a function pointer declaration. Despite this oddity, line 8 is now easier to read than in [Code Listing A.51](#cla-51); the compare parameter now matches the `comp_t` type, and the reader does not need to parse the complexities of function pointer declarations. [Code Listing A.55](#cla-55) shows how the typedef simplifies the calls to `bubble_sort()` with cleaner casting.

```cpp
/* Code Listing A.54:
   Simplifying the bubble_sort() interface with a typedef
 */

typedef int (*comp_t) (void *, void *);

void
bubble_sort (void *array[], size_t length, comp_t compare)
{
  /* ... omitting the rest ... */

```cpp
/* Code Listing A.55:
   Using the simplified bubble_sort() interface
 */

/* Replacement for A.53, line 7 */
bubble_sort ((void **)strings, 5, (comp_t)strcmp_nocase);

/* Replacement for A.53, line 14 */
bubble_sort ((void **)strings, 5, (comp_t)strcmp);
```

///10.8.2. Function Pointer Lookup Tables[¶]
--------------------------------------------

Another common use for function pointers is to create a lookup table of functions. That is, rather than building complex logic structures in code to determine which function needs to be called under certain conditions, this information is represented in a table format, such as a one- or two-dimensional array. To illustrate this concept, assume that [Code Listing A.49](#cla-49) has been extended to support the five basic arithmetic operations (addition, subtraction, multiplication, division, modulus). Using these functions, [Code Listing A.56](#cla-56) builds a single interface, `calculate()`, that can be used for any of them.

```cpp
/* Code Listing A.56:
   Using the simplified bubble_sort() interface
 */

typedef enum ops { ADD, SUB, MUL, DIV, MOD, NOP } op_t;
typedef int (*arith_t) (int, int);

int
calculate (op_t operation, int x, int y)
{
  static const arith_t ops[] = { add, sub, mul, div, mod };
  
  if (operation >= NOP || operation < 0)
    {
      errno = EINVAL;
      return 0;
    }

  return ops[operation] (x, y);
}
```

Lines 5 and 6 start by defining custom types for the valid operations (`op_t`) and the binary arithmetic funtions (`arith_t)`. Line 11 defines the lookup table; it is a simple array of the five function pointers. (Again, note that these entries are just the names of the functions and do not have parentheses.) This line reiterates the value of using a `typedef` for function pointers, as the syntax to declare an array of function pointers, along with the `static` and `const` keywords, is unnecessarily cumbersome. Defining the enum on line 5 and performing the check on line 13 ensures that the `operation` is one of the five supported. Line 19, then, actually performs the operation. By indexing into the `ops` array, `ops[operation]` evaluates to a pointer to the specified function. The `(x, y)` notation after that indicates that this function is to be executed with `x` and `y` as the arguments. This calculate() function takes no special steps based on which function is being called (although it would be appropriate to prevent the use of 0 for `y` with `div` and `mod`); rather, the logic of determining the function lies entirely in the lookup table structure. [Code Listing A.57](#cla-57) uses this simplified interface to perform each of the specified operations.

```cpp
/* Code Listing A.57:
   Using the single calculate() interface
 */

printf ("SUM: %d\n", calculate (ADD, 5, 3));
printf ("DIF: %d\n", calculate (SUB, 5, 3));
printf ("PRO: %d\n", calculate (MUL, 5, 3));
printf ("QUO: %d\n", calculate (DIV, 5, 3));
printf ("REM: %d\n", calculate (MOD, 5, 3));

🔍 Note

* * *

Based just on the toy example of five arithmetic operations, it is fair to object to the need for tables of function pointers. Clearly, [Code Listing A.57](#cla-57) could simply call `add (5, 3)` instead of calling `calculate (ADD, 5, 3)` to get the same result. From this example, the benefits of function pointers might not be evident. It is important to note, though, that function pointers are a difficult topic; this example was designed to be as simple as possible to explain the mechanics.

These lookup tables are widely used in a variety of fields, including compiler design. As the parser (one component of a compiler) reads the characters of a file a byte at a time, it needs to keep track of what has been read and what was just read. For instance, a C compiler would need to distinguish reading a space after `"int"` and reading a space after `"int_value"`, as the former is a language keyword and the latter could be a variable name. Trying to write standard logic code for this processing is extremely difficult due to the sheer number of possible branches. Instead, a two-dimensional lookup table (a very large one!) provides a more manageable approach. The details vary, but a simple way to envision it is that each row indicates a [state](#term-state) that represents what has been read (i.e., `"i"`, `"in"`, and `"int`” would be different states); the columns would then be used to identify a different function pointer for each possible input character.



//10.9. Files[¶]
================

The idea of organizing and storing data as a file is one of the oldest abstractions in computing, with references to files dating back to the earliest computers in the 1940s. From the perspective of C and the UNIX OS tradition, a file is just a sequence of bytes. Chapter 2 explores the implications of this definition in further detail, such as the fact that not all files exist on persistent storage devices and not all files have names. In this Appendix, for simplicity, we will adopt the conventional interpretation of a file stored on a device, such as a hard drive or a USB-attached device.

///10.9.1. File Permissions and Ownership[¶]
--------------------------------------------

When a file is stored, there is a certain amount of information—called [metadata](#term-metadata)—that is stored alongside the file’s contents. Metadata is stored in a data structure called an [inode](#term-inode), as discussed in Chapter 2. The inode contains a number of fields, one of which is the files permission mode. The permission mode indicates which actions (read, write, or execute) can be performed on a file by a particular user. These permission modes are commonly written as three octal values to specify permissions for the user (the owner of the file), the group (a pre-defined set of users), or others (everyone else). For each octal value, 4 (binary 100) indicates read permission, 2 (010) is write, and 1 (001) is execute. Setting one of these bits to 0 removes that permission.

📦 C library functions – `<sys/stat.h>`

* * *

`int chmod(const char *path, mode_t mode);`

Change the permissions associated with a file.

`int fchmod(int fildes, mode_t mode);`

Change the permissions associated with an open file given its file descriptor.

The `chmod()` and `fchmod()` functions, which can only be used by the owner of a particular file, provide an interface to change the permissions on a file. The first argument specifies which file, either by the standard file name (`path`) or using a file descriptor (`filedes`). The mode argument is the new permission bit mask to apply. Although the `mode_t` type is just an integer (e.g., 493), these numbers are written in their octal equivalent (`0755`). (The leading 0 is required to indicate octal format.) Among the three digits, the first applies to the user, the second to the group, and the third to others.

```cpp
/* Code Listing A.58:
   Changing two files' permissions within a program
 */

/* Everyone has read access, user also has write */
chmod ("data.txt", 0644);

/* User and group can execute the Python script, and the user can
   modify (write) it; others have no access */
chmod ("script.py", 0750);

/* Restrict access to a private directory */
chmod ("/home/csf/private_files", 0700);

[Code Listing A.58](#cla-58) changes the permissions on three files using the `chmod()` function. The bit mask on line 6 (`0644`) indicates that the user will have read and write permission (applying the bit-wise OR operator, `4 | 2 = 6`), while the group and other have only read permission. Line 10 gives the user all permissions (`4 | 2 | 1 = 7`, meaning read, write, and execute), whereas the group only has read and execute (`4 | 1 = 5`). Line 13 changes the permission on a _directory_—a special file that “contains” other files (explained below). Directories in the UNIX tradition are the equivalent of _folders_ in the Windows family of operating systems. The permissions here give the user full access to the directory, but the group and others are blocked out. These operations can also be performed on the command line as follows (note that the `chmod` utility is a C program that calls `chmod()` internally!). The `ls -ld` command displays the permissions at the first part of the file, in a format `rwxrwxrwx` for the three octal values (`-` indicates that permission is not set).

$ chmod 644 data.txt
$ chmod 750 script.py
$ chmod 700 /home/csf/private_files
$ ls -ld data.txt script.py /home/csf/private
-rw-r--r--  1 csf  staff  8528 Jul 31 23:20 data.txt
-rwxr-x---  1 csf  staff   440 Jul 31 23:20 script.py
drwx------ 11 csf  staff   352 Jul 31 23:20 /home/csf/private

🔍 Note

* * *

The permission bits—particularly read and execute—are not as intuitive as their names suggest. For instance, why is the read bit required to run the script.py Python script? As a piece of code, the goal is to run—i.e., execute—it. Similarly, what does it mean to “execute” a directory? The interpretation of these bits requires considering the type of file in a greater context.

For normal files (i.e., non-directories), the read bit indicates that a running process can open the file and copy its contents into memory. In the case of scripting languages like Python, it is important to understand that the process that is running is not executing the `script.py` code. It can’t, because to “execute” code means that the CPU is performing machine-language instructions; Python code is not machine language, because it is not compiled. Rather, the process is running the Python interpreter (typically stored in a location like `/usr/bin/python`). The read permission bit gives the Python interpreter access to read in `script.py` as text data. The interpreter then simulates the execution of the program. For performance reasons, the interpreter might compile parts of the script into machine language using _just-in-time compilation_, but that is not necessary.

So if Python only needs to read the file, what is the purpose of setting the execute bit on `script.py`? In truth—and somewhat ironically—the execute bit does not indicate that `script.py` can be executed. The execute bit on files declares that the file should be found as an executable within the `$PATH` environment variable—a list of locations to search for executables—that the shell (`bash`) uses. When you type a command, `bash` searches through the locations listed in the `$PATH` for the first match of that command name that has the execute bit set. To start the process, `bash` looks into the file contents to look at the first line of the program. In the case of scripts like `script.py`, the first line contains a _shebang_ operator that indicates the location of the interpreter that is needed:

#!/usr/bin/python

Consequently, executing a Python script requires the execute bit to add it to the `$PATH` that is searched, but requires the read bit for the program to actually run.

The execute bit on directories is also frequently misunderstood, primarily because the bit is misnamed for directories. A directory is not a piece of code that can be run (executed). Instead, reading a directory is defined as listing its contents, whereas the execute bit allows the user to _enter or pass through_ the directory. For instance, consider the following file name (storing a cryptographic key that can be used to log into a remote email service), based on the permissions above:

/home/csf/private/keys/email.id_rsa

[Code Listing A.58](#cla-58) set `0700` permissions for the `/home/csf/private`. The read bit allows the user to list the file contents (using the `ls` command, for instance). However, accessing those files or the subdirectories requires the execute bit. Since the group and others have no permissions, no one else can get access to the `keys` subdirectory, because doing so requires passing through `private`. Since the read and execute actions are distinct, the bits can be set independently. Setting a directory to `0444` would allow everyone to see the names of files in the directory, but no one could access any of them or access any subdirectories. On the other hand, setting the directory to `0111` would give everyone access to the files and subdirectories…if they already knew the name; these permissions would not allow anyone to list the directory contents to see the names of the files.

For completeness, writing to a directory means adding or removing files. When you save a new file into a directory, you are writing to the directory by adding an entry. When you delete a file, you are also writing by removing the entry. It is important to note that these operations are changes to the directory, not the file itself. To be precise, this means that **you do not have to own a file in order to delete it**. When you are deleting a file, you are not writing to it; you are writing to the directory. As such, if you have data that is important and should never be deleted, you should not place it in a directory that others can write to.

The `access()` function provides an interface to check a file’s permissions before trying to access it. As with the `chmod()` function, the first parameter specifies the name of the file being checked. The `mode` parameter is not the same as the `mode_t` type used above, because `access()` is only checking the permissions for the current user. Furthermore, `access()` can check more than just the standard read, write, and execute bits, so this mode does not follow the same octal structure.

📦 C library functions – `<unistd.h>`

* * *

`int access(const char *path, int mode);`

Check for permission to access the specified file.

[Code Listing A.59](#cla-59) uses `access()` to determine if a file is readable. Intuitively, the `R_OK`, `W_OK`, and `X_OK` arguments (which can be successfully combined with bit-wise OR) check for read, write, and execute permissions. The `F_OK` argument could be used to check just if the file exists, regardless of the other access permissions. Different systems also support additional values that could allow the user to check if a file can be deleted, if the user can change its permissions, and so forth; these options are system-specific, though, and are not widely used.

>  1
>  2
>  3
>  4
>  5
>  6
>  7
>  8
>  9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 
> /* Code Listing A.59:
>    Checking for read permission to access a file
>  */
> 
> /* Check for permission */
> if (access ("data.txt", R_OK) != 0)
>   {
>     /* Access failed, so check global errno to determine why.
>        Requires including <errno.h> */
>     if (errno == EACCES)
>       printf ("Access denied\n");
>     else
>       printf ("Access failed for other error\n");
>   }
> else
>   printf ("Access granted\n");

The `unistd.h` header file provides additional functions that are relevant to the discussion of access permissions. The `chown()` and `fchown()` functions provide interfaces to change which user or group is considered the file’s owner. These functions are generally restricted in certain ways; clearly, it would not be advisable to allow a random user to take over another’s files by changing their ownership. Some systems allow users to run these functions only on files that they own, whereas others restrict access to these functions to system adminstrators.

📦 C library functions – `<unistd.h>`

* * *

`int chown(const char *path, uid_t owner, gid_t group);`

Change the ownership of a file path.

`int fchown(int fildes, uid_t owner, gid_t group);`

Change the ownership of a file specified by a file descriptor.

The `getuid()`, `geteuid()`, `getgid()`, and `getegid()` functions are not about files, per se, but they are relevant to the current discussion. These functions get information about the current process that is executing. Whenever you run a program, you create a process; the process inherits a specified user ID and group ID that control the process’s access. Consequently, when the previous examples referred to checking the “user’s” access to a file, this check is based on the user ID associated with the process executing this code.

📦 C library functions – `<unistd.h>`

* * *

`uid_t getuid(void);`

Gets the real user ID of the calling process.

`uid_t geteuid(void);`

Gets the effective user ID of the calling process.

`gid_t getgid(void);`

Gets the real group ID of the calling process.

`gid_t getegid(void);`

Gets the effective group ID of the calling process.

The functions below make a distinction between the _real user_ and the _effective user_. (All of the points here also apply to the real group and effective group; we only mention the user for brevity.) The real user is the ID of the account that initiated the creation of the process, whereas the effective user is the one attached to the process as it runs; these IDs are typically the same, but they do not have to be. Functions that check permissions, such as `access()`, generally use the effective user ID, though there are exceptions where the real user ID can also influence access decisions.

As an example of the distinction between the real and effective user, consider an executable file with the `SETUID` bit set (see Chapter 2); this bit sets the process’s effective user ID to be the user ID of the executable file’s owner, rather than the user who ran the command. One common use of this is to have the `SETUID` bit set on a file owned by the administrative (`root`) account; the process that runs will then have full access to the system, as it is running as root rather than a restricted user account. This technique is how a login program can run to check a user’s password against the full password list, but the user does not have direct access to the password file.

Another example of the real vs. user distinction can be manually created with the su command. Contrary to a common belief, `su` does not necessarily mean “super user” (a common term for `root`); rather, su means _substitute user_. Running a command with `su` will change the effective user ID of the process, even if the `SETUID` bit is not set. For example, the following command line would run the `ls` command based on the `csf` username [[1]](#f61), rather than the default user typing the command. Consequently, the permission checks (to determine if the process is allowed to list the current directory’s file contents) are based on the `csf` username, not the actual user.

$ su csf ls -ltr

///10.9.2. Persistent Storage[¶]
--------------------------------

Chapter 2 covers most of the details about working with files, such as reading from and writing to them. That section, however, uses a broader definition of files than this part of the Appendix. Here, we are focusing just on files in the traditional, common sense; i.e., those that are persistently stored on a device such as a hard drive. The storage techniques used for these files raise particular issues that are beneficial to understand for systems programming.

The first issue, which we have already discussed, is that of metadata. When a file is stored, the metadata is stored in an inode structure. It is important to realize two facts: 1) the inode—not the contents—actually defines the file, and 2) the inode is stored separately from the contents. In other words, the inode—which is stored on the device—contains information to identify other blocks of data on the device that store the file’s contents. When a file is modified, the new contents may be written to the existing blocks, but maybe not. The new contents may be written to different blocks, and the inode is simply changed to point to these new blocks; the old content blocks are still present on the device, the inode just doesn’t point to them anymore. Similarly, when a file is deleted, the content blocks are not necessarily destroyed; just the inode is deleted (in fact, that’s not even entirely true, as we will describe in relation to directories).

When a file is in use (such as when you are editing a source code file in a text editor), all of the file contents exist in memory. Generally speaking, you are not directly interfacing with the file stored on disk; compared to accessing a copy in memory, accessing the stored copy would be horrendously slow. As you modify the file, the version in memory and the original stored copy become different. The unistd.h header file defines three key functions for fixing this. The `ftruncate()` and `trunctate()` functions are used to resize [[2]](#f62) the file’s in-memory representation; for instance, if you delete a large chunk of your program’s code, the text editor may run `truncate()` to reduce the memory allocated for it.

These changes are not automatically propagated to the inode stored on the device. This update typically (the details depend on the OS and the file system in use) only occurs when the file is closed or when the process calls `fsync()`. The `fsync()` function—which is run when you save a file—writes the contents to the storage device and updates the stored inode accordingly.

📦 C library functions – `<unistd.h>`

* * *

`int fsync(int fildes);`

Synchronize the in-memory file contents with persistent storage.

`int ftruncate(int fildes, off_t length);`

Truncate or extend a file size to the specified length.

`int truncate(const char *path, off_t length);`

Truncate or extend a file size to the specified length.

///10.9.3. Directories and Links[¶]
-----------------------------------

It is likely that the reader is familiar with the term _directory_ as it is used in relation to files. When working at the systems level, it helps to have a more in-depth understanding than the basic idea of directories (or folders) as places where files are stored. Specifically, it is important to note that **files are not actually stored in directories**. As noted above, a file is defined by an inode, as persistent data structure that contains the file’s metadata and pointers to the file’s contents. The location of the inode on the device is determined by the OS and the file system. The _inode number_ is a persistent location identifier that tells the OS where to find the inode on the device.

From this perspective of inodes, one may observe that we have not mentioned the concept of file names. The reason for this is that **file names do not, in fact, identify files**. When we are working at the level of inodes, files do not have names. Names are introduced by the concept of directories. A directory is simply a special type of file that contains a mapping between a string (name) and the corresponding inode. [Figure 10.9.7](Files.html#filesysblocks) illustrates this concept for two directories and two regular files.

![Sample blocks containing inodes and a directory's contents](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/_images/CSF-Images.A.8.png)

Figure 10.9.7: Sample blocks containing inodes and a directory’s contents

To start, assume that we are examining the directory `/home/csf`. Since directories are files (just a special type of file), this directory has an inode and an inode number. In [Figure 10.9.7](Files.html#filesysblocks), the inode number for `/home/csf` is 4526, which identifies the block [[3]](#f63) of that number. That inode indicates that the file is a directory with permissions set to `0755`, etc. The `Content` field indicates the block number containing the directory’s contents. By examining the contents in block 51003 (the block number and the inode number are the same), we can see that this directory contains three files: `data.txt`, `script.py`, and `private.` The directory indicates that the string `"data.txt"` (or the full path `/home/csf/data.txt`) refers to inode 4873. This inode indicates that the file is a regular file and its contents begin at block 52002. (For historical reasons, blocks are 512 bytes in size; if the file exceeds 512 bytes, it will require multiple content blocks.) Similarly, blocks 5922 and 202 contain the inodes for `script.py` and `private`, respectively; the inode for the latter indicates that `private` is a directory.

Readers who have become comfortable with working on the command line are likely to have heard references to “being in” or “changing to” a directory. These phrases refer to examining or changing the current process’s _current working directory_, the default location that the process will look for files. Note that, when you are working on the command line, you are always running a process: the shell (typically `bash`). The `cd` command tells the `bash` process to change its current working directory. When you run a separate program by typing a command, bash creates a new process, and that new process inherits bash’s current working directory; this new process could change its current working directory with the `chdir()` and `fchdir()` functions.

📦 C library functions – `<unistd.h>`

* * *

`int chdir(const char *path);`

Change the current working directory based on the provided `path`.

`int fchdir(int fd);`

Change to the directory given as the open file descriptor `fd`.

At this point, we have established that directories do not contain files or inodes; they just map a string to an inode number. Similarly, the `chdir()` function can change which directory the process will look in by default. Based on this understanding, we can now distinguish between _hard links_ and _symbolic links_. A hard link is a directory reference to an inode. In [Figure 10.9.7](Files.html#filesysblocks), block 51003 contains an entry for `data.txt` that links to inode 4873. Within inode 4873, there is a field that counts the number of such links; so 4873’s link count would be at least one. This figure did not show the contents of the `/home/csf/private` directory, but assume that it contains an entry for a file named copy.txt that also maps to inode 4873; i.e., there is a `/home/csf/private/copy.txt` file, and it points to the same inode as `/home/csf/data.txt`. In other words, the full path names `/home/csf/data.txt` and `/home/csf/private/copy.txt` are both hard links to the same inode and the inode’s reference count must be at least two. The effect of this hard link is that these two files are the same; changing the contents of one will change the other. Such hard links are created with the `link()` function.

📦 C library functions – `<unistd.h>`

* * *

`int link(const char *path1, const char *path2);`

Make `path2` a hard link with the same attributes as `path1`.

`int symlink(const char *path1, const char *path2);`

Make `path2` a symbolic link to the file identified by `path1`.

`int unlink(const char *path);`

Remove a link to a file specified by the given `path`.

Unlike changes to a file, the `unlink()` function only removes a single such link by deleting the entry from the relevant directory. That is, calling `unlink()` on `/home/csf/private/copy.txt` would remove the entry for `copy.txt` from the `/home/csf/private` directory. The file contents could still be accessed using the `/home/csf/data.txt` path name. As long as there is at least one reference to the file contents, the file cannot actually be “deleted” in the casual sense. In fact, even when the reference count for an inode drops to 0, the contents are not necessarily destroyed. Instead, the blocks containing the inode and the file contents are marked as available for future use. Neither is truly destroyed until they are overwritten when they are assigned to a different file in the future.

In contrast to hard links, a symbolic link (also known as a _shortcut_ in the Windows family of OS) is a distinct file with its own inode. The inode’s type field would indicate that it is a symbolic link (or symlink for short) rather than a regular file. The inode’s Content would point to a content block just like a regular file; however, the contents would be the path name that the symbolic link points to. To illustrate the difference, consider the following command lines, which start with an original C source file called `hello.c`:

$ ln hello.c link.c
$ ln -s hello.c symlink.c
$ ls -li hello.c link.c symlink.c 
12909398185 -rw-r--r--  2 csf  staff  74 Feb 23 17:09 hello.c
12909398185 -rw-r--r--  2 csf  staff  74 Feb 23 17:09 link.c
12909398210 lrwxr-xr-x  1 csf  staff   7 Feb 23 17:10 symlink.c -> hello.c

The first `ln` command uses link() to make `link.c` a hard link to the contents of `hello.c`. The `-s` flag on the second `ln` command calls `symlink()` instead, making `symlink.c` a symbolic link to `hello.c`; that is, `symlink.c` is a distinct special file whose contents are the literal string `"hello.c"` to indicate the target of the link. (Note that the symbolic link does not actually check for—or have any knowledge of—the existence of `hello.c` itself.) The `ls -li` command shows metadata about the three files. The first field on each line is the respective inode number (produced by the `-i` flag). Since `hello.c` and `link.c` are hard links to the same contents, their inode is the same. On the other hand, `symlink.c` is a separate file with a distinct inode number.

One of the key differences between hard links and symbolic links can be observed through deletion. Hard links are bound to the actual file contents, whereas symbolic links are bound to the name. If the target of a symbolic link (`hello.c` in this case) is deleted (the `rm` command calls `unlink()` on the `hello.c` file name) and recreated with the same name, accessing the symbolic link will reflect the new contents; accessing the other hard link will get the old contents. The original contents will only be deleted when all hard links are removed.

$ rm hello.c
$ cat link.c
[original contents here...omitted for brevity]
$ cat symlink.c
cat: symlink.c: No such file or directory
$ echo "goodbye" > hello.c
$ cat link.c
[original contents here...omitted for brevity]
$ cat symlink.c
goodbye

///10.9.4. Advisory Locks[¶]
----------------------------

When concurrent software is working with files, there is the danger that multiple processes or threads might try to access or modify an existing file at the same time. This situation would allow one process or thread to change the file’s contents in ways that can cause errors in the other process or thread. Different OS and file systems provide a variety of mechanisms to prevent this from happening; as these approaches can vary between systems, it can be difficult to rely on them for cross-platform software. One mechanism that generally has cross-platform support is the use of _advisory locks_, which provide a mechanism for cooperating processes to try to avoid this situation. In essence, an advisory lock allows a process to check if another process has already locked the file; if so, the new process can wait until the first process is finished. As the name implies, though, these are advisory in nature; the new process can still proceed regardless of the first process’s claim.

📦 C library functions – `<sys/file.h>`

* * *

`int flock(int fd, int operation);`

Apply or remove an advisory lock on an open file.

The `flock()` function provides one mechanism to create an advisory lock on a file. Unlike some of the previous functions we have described, `flock()` only works with file descriptors (see Chapter 2), not path names. That is, the process must have access to the file and must have already successfully opened it. [Code Listing A.60](#cla-60) demonstrates the use of flock(). (Note that this code relies on `fork()` and `open()`, which are explained in Chapter 2. Advisory locks are not a critical topic for the main chapter and are provided here for reference after that chapter has been completed.)

>  1
>  2
>  3
>  4
>  5
>  6
>  7
>  8
>  9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 
> /* Code Listing A.60:
>    Setting an advisory lock on a file using the flock() interface */ /* Create a child process and race to get the file */ pid_t child = fork (); assert (child >= 0); /* Both parent and child will open the file */ int fd = open ("movies.csv", O_RDWR); assert (fd > 0); /* Only one will succeed in locking it */ if (flock (fd, LOCK_EX | LOCK_NB) == 0) printf ("Successfully locked file\n"); else printf ("Failed to lock\n"); close (fd); /* Add a slight pause to observe the locking */ sleep (2);

The `LOCK_EX | LOCK_NB` argument specifies that this process is requesting exclusive (`LOCK_EX`) access to the file (only one access at a time) and `flock()` should run in non-blocking mode (`LOCK_NB`). The default behavior of `flock()` is to cause current process to wait (block) if another process already has exclusive access. In non-blocking mode, `flock()` returns a non-zero value to indicate that the current process failed to acquire the lock. The process can then proceed to other work that may be necessary and try to acquire the lock at a later time. The call to `sleep()` on line 21 is included in this example to create a slight pause in both processes. That is, without this `sleep()`, it is possible that one process would run, acquire the lock, and exit (releasing the lock) before the other process had a chance to try for access. Including line 21 makes such timing unlikely, increasing the likelihood that the reader can observe the effects of the failed lock when running this code.

In the preceding discussion, we mentioned that advisory locks allow _cooperating_ processes to coordinate their work. To be precise, two processes are cooperating in this sense only if they originate from the same original process. That is, the process specifically uses `fork()` to create at least one child process. Running the same program multiple times does not create cooperating processes; these processes would not recognize each other’s claims to locking the file.

🐞🐛🐌 Bug Warning

* * *

Although it is generally cross-platform, `flock()` does not successfully create advisory locks on macOS. For code that truly needs to be cross-platform, `fcntl()` is the required interface for advisory locks. The primary disadvantage of `fcntl()` is that it is an older, generic interface for many different operations on files. Advisory locks are just one of the possible operations.

📦 C library functions – `<fcntl.h>`

* * *

`int fcntl(int fildes, int cmd, ...);`

Perform an operation on a file descriptor.

[Code Listing A.61](#cla-61) demonstrates the `fcntl()` interface for creating an advisory lock. On lines 19 and 26, the `F_SETLK` argument tells `fcntl()` that the process is trying to set or release a lock; the `struct` reference passed as the third argument distinguishes between these two actions. Specifically, setting the `struct`’s `l_type` field to `F_WRLCK` indicates the process is requesting a lock for writing to the file; locking the file for reading would require also setting the `F_RDLCK` bit in this field. Setting the `l_type` field to `F_UNLCK` and calling `fcntl()` again releases the lock on the file.

>  1
>  2
>  3
>  4
>  5
>  6
>  7
>  8
>  9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> 25
> 26
> 27
> 28
> 29
> 30
> 31
> 
> /* Code Listing A.61:
>    Setting an advisory lock on a file using the fcntl() interface
>  */
> 
> /* Create a child process then race to get the file */
> pid_t child = fork ();
> assert (child >= 0);
> 
> /* Both parent and child will open the file */
> int fd = open ("movies.csv", O_RDWR);
> assert (fd > 0);
> 
> /* Initialize the lock for writing */
> struct flock lock;
> memset (&lock, 0, sizeof(lock));
> lock.l_type = F_WRLCK;
> 
> /* Only one will succeed in locking it */
> if (fcntl (fd, F_SETLK, &lock) == 0)
>   {
>     printf ("Successfully locked file\n");
>     /* Add a slight pause to observe the locking */
>     sleep (2);
>     /* Now release the lock */
>     lock.l_type = F_UNLCK;
>     fcntl (fd, F_SETLK, &lock);
>   }
> else
>   printf ("Failed to lock\n");
> 
> close (fd);

[[1]](#id3)

In the UNIX tradition, _username_ refers to the human-readable identifier that user can remember and use easily. Internally, all usernames are mapped to a user ID, which is just an integer value.

[[2]](#id4)

These functions are poorly named. The standard definition of the word “truncate” means to reduce the size of something. However, the C `truncate()` function can also be used to increase the file’s size.

[[3]](#id5)

Typical storage devices organize the device into what is essentially a gigantic array of fixed-size blocks. While the details are more complicated than this, it is sufficient for our purposes to think of accessing a block just as we would index an array.


/Appendix B[¶]
==============

//11.1. Glossary[¶]
===================

A2DP

Abbreviation for [Advanced Audio Distribution Profile](#term-advanced-audio-distribution-profile).

access point

A router, typically Wi-Fi, that connects a host device to the Internet.

acknowledgement number

An integer used in TCP to acknowledge the receipt of a message; the acknowledgement number is the previous message’s sequence number plus the number of bytes in the message.

acquire

The act of using a lock’s interface to gain mutually exclusive access to a critical section.

Address Resolution Protocol

Abbreviated as [ARP](#term-arp). A link-layer protocol that translates IP addresses into MAC addresses within a local network.

Advanced Audio Distribution Profile

Abbreviated as A2DP. A [Bluetooth profile](#term-bluetooth-profile) used to improve performance of AVDTP.

Advanced Encryption Standard

Abbreviated as AES. The symmetric key encryption standard published by NIST for use in contemporary computing systems; based on the Rijndael block cipher.

AEAD

Abbreviation for [Authenticated Encryption with Attached Data](#term-authenticated-encryption-with-attached-data).

AES

Abbreviation for [Advanced Encryption Standard](#term-advanced-encryption-standard).

agreement (consensus protocol)

The characteristic that all correct nodes participating in a consensus protocol eventually reach the same answer to a question.

Amdahl’s law

The mathematical formulation that there is a theoretical maximum speedup that can be achieved through parallelizing parts of a calculation.

amplitude modulation

Manipulating the amplitude of a carrier signal to encode binary data.

application layer

The top layer of the Internet model, often providing services to end users.

architectural style

An abstract description that instance of system architectures follow.

ARP

Abbreviation for [Address Resolution Protocol](#term-address-resolution-protocol).

ARP cache poisoning

A link-layer protocol attack that allows an attacker to change which machine on the local network correspond to an IP address, thus making it possible for the attacker to redirect and intercept network traffic.

AS

Abbreviation for [autonomous system](#term-autonomous-system).

asynchronous

A style of communication in which the timing of sending and receiving are not necessarily linked, allowing senders to move on to other tasks during transmission. Also used to describe a form of execution in which concurrent events occur without coordination.

atomic

The abstraction that a step or sequence of steps are performed as a single unit without interruption or interference by other entities.

atomic transfer

Messages sent into an [IPC](#term-ipc) channel are entered as a single, contiguous unit.

Audio/Video Distribution Transport Protocol

Abbreviated as AVDTP. A [Bluetooth profile](#term-bluetooth-profile) designed to provide low latency for the wireless connection of audio/video devices, such as headphones.

Authenticated Encryption with Attached Data

Abbreviated as AEAD. A digital signature feature added in [TLS](#term-tls) 1.3 to perform the message authentication code (MAC) and encryption calculations in parallel to avoid a particular ciphertext reuse attack.

authenticity

The security property that a message’s origin can be determined.

authoritative name server

A DNS server maintained by an organization to provide the definitive mapping of IP addresses within the organization’s network.

autonomous system

Abbreviated as AS. A network of hosts or subnetworks controlled by a single entity.

availability

One of the three primary security properties (along with confidentiality and integrity); the ability to provide service to legitimate users.

avalanche effect

A feature of cryptographic hash functions; changing a small number of bits in the input leads to significantly different outputs.

AVDTP

Abbreviation for [Audio/Video Distribution Transport Protocol](#term-audio-video-distribution-transport-protocol).

backbone

The collection of routers that form the core of the Internet.

banked cache

A cache design in which some portion is designated for storing instructions while another portion is designated for data; L1 cache in modern multicore processors is typically banked.

barrier

A [synchronization primitive](#term-synchronization-primitive) that requires a minimum number of threads to reach a common point before any are allowed to progress.

baseband

The lowest layer of a wireless protocol stack, providing the basic functionality of encoding binary data on radio waves.

batch

The act of grouping together a set of jobs to submit at one time, leading to their sequential execution.

behavioral model

Another name for [dynamic model](#term-dynamic-model).

best effort service

The protocol feature that the system will attempt to provide a transmission service but cannot guarantee that it will be successful; often used to describe the unreliable transport service of UDP.

BGP

Abbreviation for [Border Gateway Protocol](#term-border-gateway-protocol).

blocked state

A process state in which the process is waiting on a particular event to occur.

blocking I/O

A form of input/output operation in which the process is temporarily paused (blocked) until the operation is performed.

Bluetooth profile

A generic Bluetooth application-layer protocol that device vendors can use in their products to ensure compatibility with other devices.

Bluetooth SIG

Abbreviation for [Bluetooth Special Interest Group](#term-bluetooth-special-interest-group).

Bluetooth Special Interest Group

Abbreviated as Bluetooth SIG. The industry standards group that defines the specifications for the Bluetooth short-range wireless technology.

boot loader

A piece of software that is responsible for locating and initiating the execution of the kernel.

boot sequence

The procedures that a computer executes to begin execution when power is first turned on.

Border Gateway Protocol

Abbreviated as BGP. A network-layer control plane protocol that supports inter-AS routing at the gateways between networks.

bounded waiting

A synchronization property that guarantees any entity waiting on access to a shared resource will eventually do so; also known as fairness.

broadcast

The ability to send a message to all nodes in a network.

buffer cache

A kernel storage location for data recently retrieved from disk storage.

buffer overflow

A software error in which more data is read into a buffer than the space allocated would allow; this error is a common cause of exploitable software vulnerabilities.

Byzantine failure

The situation in a distributed system where a correct node may simultaneously appear correct to other correct nodes while also appearing faulty to other correct nodes.

Byzantine fault

Another name for [Byzantine failure](#term-byzantine-failure).

Byzantine Generals Problem

A classical distributed systems problem that proves that it is impossible to guarantee that consensus can be achieved if more than 1/3 of the nodes are faulty.

CA

Abbreviation for [certificate authority](#term-certificate-authority).

canonical name

A DNS record that maps an alias to its definitive domain name.

carrier signal

A light or radio signal that is transmitted at a fixed amplitude and frequency.

CDN

Abbreviation for [content delivery network](#term-content-delivery-network).

centralized P2P index

A centralized database in a P2P network that maintains a full mapping of which nodes store which objects; used, for example, in the Napster file sharing system.

certificate authority

Abbreviated as CA. An organization responsible for validating information stored in a cryptographic certificate.

Chord

An early and influential [structured P2P network](#term-structured-p2p-network) in which nodes are arranged in a circle.

churn

The network property, typically a concern in P2P systems, that nodes join and leave the network frequently; can lead to poor performance or lost data.

CIDR

Abbreviation for [Classless Inter-Domain Routing](#term-classless-inter-domain-routing).

Cigarette Smokers Problem

A classical synchronization problem that demonstrates a scenario that cannot be solved using only a semaphore’s increment and decrement operations; all possible solutions have deadlock as a possible outcome.

ciphertext

The output of an encryption procedure that scrambles data in a secure way so that the original message cannot be determined.

circuit switching

A network technology in which hosts take turns transmitting information into a dedicated network; contrasted with packet switching.

circular wait

The situation in which multiple threads or processes each have control of one or more resources while waiting to gain access to the resources held by the other(s); one of the four requirements (along with hold and wait, mutual exclusion, and no preemption) for deadlock.

Classless Inter-Domain Routing

Abbreviated as CIDR. An IPv4 notation for subnetworks by referring to a group of addresses; the address ends in a series of zeroes and /n, where n indicates the number of trailing bits that can be either a 1 or 0 in any of the addresses.

client/server architecture

A system architecture in which multiple, independent clients request service from a single, centralized server.

cluster system

A type of multiprocessing system that is created by linking multiple, typically low-cost, independent computers on a closed, high-speed network; each node has independent processors and memory systems, and they communicate via message passing on the network.

collision

The property that a hash function produces the same result for two (or more) different inputs; cryptographic hash functions are designed to make finding collisions difficult.

complexity

A system characteristic that is created by entities providing feedback into the system that lead to unpredictable effects.

concurrency

The ability for multiple entities to make progress toward a goal within a single period of time; creates the appearance of parallel execution (which may be real or illusory).

condition variable

A [synchronization primitive](#term-synchronization-primitive) that can be used to wait until another thread indicates that a particular event has occurred or a desired condition has been achieved.

confidentiality

One of the three primary security properties (along with availability and integrity); the property that unauthorized reading of data is not permitted.

congestion

The state that occurs when there is too much traffic on a network, often leading to delays and dropped packets.

connection-oriented protocol

A protocol, such as [TCP](#term-tcp), that relies on both endpoints maintaining state information about a virtual connection between the hosts.

connectionless protocol

A protocol, such as [UDP](#term-udp), in which each message is sent as a stand-alone entity with no logical or persistent state.

consensus protocol

A distributed system protocol that aims for all correct nodes to agree to the same value of a state variable.

content delivery network

Abbreviated as CDN. An Internet service that stores copies of popular data in several locations distributed throughout the world, providing faster access to local copies.

content provider

An organization that provides a service on the Internet.

context switch

The change from one process to another, defined by replacing the virtual memory image that is accessible by the CPU.

control plane

The portion of the network layer that focuses on exchanging information between routers to determine paths between and through networks.

control program

One of the primary roles of the kernel, which is to handle errors and access violations in a safe manner.

cookie

A small file used in HTTP to maintain a persistent state or login information.

cooperative multitasking

An early form of multiprogramming in which a process maintained control of the CPU until it voluntarily relinquished control.

CPL

Abbreviation for [current privilege level](#term-current-privilege-level).

CRC

Abbreviation for [cyclic redundancy check](#term-cyclic-redundancy-check).

critical section

A sequence of instructions that must be executed as an atomic unit to avoid timing-related errors.

cryptographic hash function

A one-way mathematical function that converts any sequence of bytes into a fixed-size numeric value; designed to prevent anyone from discovering the original data or to find other data that produces the same result.

cryptographic key

A numeric value that an encryption or decryption routine combines with a piece of data to scramble or recover the message.

cryptographic signature

A cryptographic technique that allows an entity, potentially untrusted, to claim verifiable authorship of a message.

cryptography

The study of mathematical techniques that can be used to add security features to information.

current privilege level

Abbreviated as CPL. A CPU internal configuration parameter that controls what software instructions are allowed to be executed.

cyclic redundancy check

Abbreviated as CRC. An error-detection technique commonly used in network protocols.

data parallelism

A parallel programming strategy in which the same operation is applied to multiple pieces of data.

data plane

The portion of the network layer that focuses on assigning addresses to hosts.

datagram

A structured network communication message that does not assume the presence of reliable data transfer.

deadlock

The permanent blocking of multiple processes that are simultaneously waiting on each other.

demultiplexing

The transport-layer service of directing incoming data to the intended process.

denial-of-service

A type of attack on the availability of a service.

detached thread

An independent thread of execution that can no longer be joined by the parent thread that created it.

DHCP

Abbreviation for [Dynamic Host Configuration Protocol](#term-dynamic-host-configuration-protocol).

DHE

Abbreviation for [Diffie-Hellman exchange](#term-diffie-hellman-exchange).

DHT

Abbreviation for [distributed hash table](#term-distributed-hash-table).

Diffie-Hellman exchange

Abbreviated as DHE. A cryptographic protocol that allows two entities to securely establish a private common session key with no prior shared information.

Dijkstra’s algorithm

A graph theory algorithm for determining the least-cost path from one node to another; requires information about the cost between all nodes in the graph.

Dining Philosophers Problem

A classical synchronization problem that provides a metaphor for how deadlock can occur.

distance vector routing

A recursive protocol strategy in which local information is used to dynamically compute the route from one node to another in a graph.

distributed hash table

Abbreviated as DHT. A data structure that is widely used in [structured P2P network](#term-structured-p2p-network) systems to provide efficient searching for and retrieval of objects.

distributed P2P index

A database strategy used in [structured P2P network](#term-structured-p2p-network) systems, in which the information about object locations is distributed among all the nodes in the network.

distributed system

A system that is comprised of multiple, independent computer systems, often located across physically remote distances.

divide-and-conquer algorithm

An algorithmic strategy in which a collection of data is repeatedly broken into smaller collections until the problem becomes trivial to solve.

DNS

Abbreviation for [Domain Name System](#term-domain-name-system).

Domain Name System

Abbreviated as DNS. A distributed Internet database that maps human-readable domain names to IP addresses.

dual booting

Configuring a system with multiple OS images so that the OS to be used is selected during the boot sequence.

Dynamic Host Configuration Protocol

Abbreviated as DHCP. A protocol that allows a host to be configured dynamically for a local network when it first joins.

dynamic IP address

An IP address that is assigned to a host when it joins a network.

dynamic model

A system model that emphasizes changes to the system over time.

dynamic programming

A style of programming in which a larger problem is solved by applying local information to a smaller solved problem.

effect

An observable result of a transition from one system state to another.

embarrassingly parallel

The observation that a problem has an obvious parallel solution.

emergent property

Unanticipated system properties that result from the normal functioning of a system, in contrast to properties that are intended by design.

encryption

A cryptographic technique in which a message (plaintext) combined with a key to produce a scrambled message (ciphertext) to hide the original information; designed to be reversible by a corresponding decryption routine that converts the ciphertext into the plaintext.

end-to-end communication

The networking abstraction that two processes are communicating directly to each other as if they exist on the same host.

environment variable

A configuration parameter that can be passed from a parent process to a child process without changing the program interface.

ephemeral port

A randomly assigned port number in the upper range of possible values.

Ethernet

A family of link- and physical-layer protocols that provide wired connectivity.

event

A meaningful change in the state of a system or an entity such as a process.

event-driven architecture

A system architecture that emphasizes detecting and responding to pre-defined events.

exception

An internal event that disrupts the normal flow of a process, caused by the execution of a CPU instruction.

fairness

A synchronization property that guarantees any entity waiting on access to a shared resource will eventually do so; also known as bounded waiting.

fiber

A one-to-one threading model used by Windows and the .NET framework; unlike POSIX, related fibers use cooperative multiprogramming instead of preemptive.

fiber-optic wire

A type of wire made from glass that allows the controlled transmission and modulation of light signals.

FIFO

A first-in, first-out message-passing [IPC](#term-ipc) in which bytes are sent and retrieved as unstructured streams. Also known as a [named pipe](#term-named-pipe).

file descriptor

An integer used by the kernel to identify a file, such as an [IPC](#term-ipc) channel.

firmware

A reconfigurable hardware component that can persistently store small programs.

flow control

The TCP service that the transmission rate of data is reduced based on the capacity of the other host.

Flynn’s taxonomy

A classification of hardware architectures based on their ability to support multiple instructions and/or multiple pieces of data in parallel.

fork/join pattern

A parallel programming implementation pattern in which a single thread creates a collection of helper threads for parallel computation, then all threads are joined before the main thread continues processing.

formal model

A description of a system using a mathematical specification language.

forwarding

Propagating a network packet from one device to another without modification.

fragmentation

The network- and link-layer service in which a large packet is broken down into smaller packets based on the requirements of lower network layers.

frame

A structured link-layer network message.

frequency modulation

Manipulating the frequency of a carrier signal to encode binary data.

function pointer

A C programming language technique in which a pointer variable is used to store the address of a function rather than another variable or data structure.

GAP

Abbreviation for [Generic Access Profile](#term-generic-access-profile).

gateway router

A router that connects one autonomous system to another.

Generic Access Profile

Abbreviated as GAP. A default [Bluetooth profile](#term-bluetooth-profile) that can be used for applications that do not have pre-defined characteristics.

GFS

Abbreviation for [Google File System](#term-google-file-system).

Go channel

A Go programming language technique for message passing communication between threads or processes.

Google File System

Abbreviated as GFS. A distributed file system that strives to provide high-speed and high-availability access to large quantities of data.

Goroutine

A Go programming language thread.

Green threads

An early threading model used in Java; a single kernel thread was shared by all threads in the application.

Gustafson’s law

The mathematical formulation that there is a maximum speedup factor that can be achieved by any general performance improvement to a system.

Hands-Free Protocol

Abbreviated as HFP. A [Bluetooth profile](#term-bluetooth-profile) used to connect a cell phone to a headset or automobile.

HDP

Abbreviation for [Health Device Protocol](#term-health-device-protocol).

header

A structured collection of bytes attached to the beginning of a network message with metadata used by network protocols.

Health Device Protocol

Abbreviated as HDP. A [Bluetooth profile](#term-bluetooth-profile) used for medical devices; operates as an application layer that uses MCAP for a common transport layer.

HFP

Abbreviation for [Hands-Free Protocol](#term-hands-free-protocol).

HID

Abbreviation for [Human Interface Device](#term-human-interface-device).

hold and wait

A system property that allows a thread or process to hold mutually exclusive access to one resource while waiting on another; one of the four requirements (along with circular wait, mutual exclusion, and no preemption) for deadlock.

hop

A logical distance metric that indicates a single link in a network.

host

A computer or similar device that forms an end-point of a network.

Human Interface Device

Abbreviated as HID. A [Bluetooth profile](#term-bluetooth-profile) used for standard computer peripherals, such as a wireless mouse or keyboard.

ICANN

Abbreviation for [Internet Corporation for Assigned Names and Numbers](#term-internet-corporation-for-assigned-names-and-numbers).

IEEE 802.11

The working group that defines the specification for Wi-Fi; also used to refer to the collection of Wi-Fi standards themselves.

IEEE 802.15.4

The working group that defines the specification for low-power, short-range wireless devices, including sensor networks and the Zigbee protocol stack; also used to refer to the collection of standards themselves.

IEEE 802.3

The working group that defines the specification for Ethernet; also used to refer to the collection of Ethernet standards themselves.

IETF

Abbreviation for [Internet Engineering Task Force](#term-internet-engineering-task-force).

implicit threading

A programming language feature in which the execution environment, rather than the programmer, takes the responsibility for creating and managing threads.

information theoretic security

The cryptographic property that an adversary cannot determine the plaintext message for a ciphertext, even with infinite time and resources; not achievable (or needed) in practice; contrasted with semantic security.

inode

An on-disk data structure that identifies the location of data and contains the file metadata, such as permissions.

integrity

One of the three primary security properties (along with availability and confidentiality); the property that unauthorized modifications are not permitted.

integrity (consensus protocol)

The characteristic that, if all correct nodes propose the correct value in a consensus protocol, then any correct node will propose that value.

interface abstraction

The characteristic that describes how much detail of a system is revealed by how it is used.

interleaved

A semantic model for concurrent execution in which instructions from multiple entities are executed one at a time rather than parallel, but the system switches back and forth between the entities in a nondeterministic fashion.

Internet

An interconnected, world-wide network of multiple computer networks that are each independently owned and operated.

Internet Corporation for Assigned Names and Numbers

Abbreviated as ICANN. An international non-profit organization responsible for controlling the root level of DNS.

Internet Engineering Task Force

Abbreviated as IETF. The part of ISOC that is responsible for defining standards and protocols used to create the Internet.

Internet layer

The middle layer of the Internet protocol stack, defining logical locations within the Internet and routing paths between hosts.

Internet model

A layered architecture model to describe the role of major protocols used in the Internet.

Internet of Things

Abbreviated as IoT. The system created by connecting non-computational physical devices, such as household appliances, to the Internet.

Internet Protocol

Abbreviated as IP. A network-layer protocol that assigns a logical location (address) to a host on the Internet.

Internet Society

Abbreviated as ISOC. An international non-profit organization responsible for the development and leadership of the Internet.

interprocess communication

Abbreviated as IPC. A collection of techniques that allow processes to exchange information with the help of the kernel.

interrupt

An external event that disrupts the normal flow of a process, caused by a hardware component indicating that service is needed.

IoT

Abbreviation for [Internet of Things](#term-internet-of-things).

IP

Abbreviation for [Internet Protocol](#term-internet-protocol).

IP masquerading

Technique used by NAT in which a router replaces a host’s IP address in a packet with the router’s; allows several devices on a private network to share a single Internet-facing IP address.

IPC

Abbreviation for [interprocess communication](#term-interprocess-communication).

ISOC

Abbreviation for [Internet Society](#term-internet-society).

Java Runnable interface

A Java interface that classes can implement for multithreading; requires the class to implement the run() method that serves as the entry point for a thread.

Java Thread class

A Java class that classes can extend for multithreading; requires the class to implement the run() method that serves as the entry point for a thread.

job

A term used to describe a single execution of a computer program from start to finish.

join

The act of one thread waiting on one of its child threads to complete.

kernel

The most privileged software component in a computer system that controls access to shared resources and provides initial responses to system events.

kernel mode

The CPU configuration in which all instructions can be executed, resulting from setting the CPL to the highest privilege level.

key exchange protocol

A cryptographic protocol designed to allow two entities to securely agree to a shared session key, even if the entities have no prior knowledge of each other.

key pair

A single combination of a public key and its corresponding private key.

key schedule

A sequence of secret cryptographic keys derived from a single key; used in encryption algorithms (such as AES) that iteratively apply the same manipulations, but with a different key each time.

keyed cryptographic hash function

A cryptographic technique in which a message is combined with a key prior to performing a hash calculation.

L2CAP

Abbreviation for [Logical Link Control and Adaptation Protocol](#term-logical-link-control-and-adaptation-protocol).

Lamport timestamp

A technique for creating a logical clock in a distributed system.

LAN

Abbreviation for [local area network](#term-local-area-network).

latency

The time that elapses from the beginning of a task until its completion.

layered architecture

A system architecture in which entities can only interact with those directly above and below in a defined hierarchy.

level of abstraction

The characteristic that describes how much detail is included in a model.

light-weight process

Abbreviated as LWP. A many-to-many threading model used in Solaris; a run-time threading library dynamically mapped running process-level threads to a set of virtual processors that corresponded to kernel threads.

lightswitch

A synchronization programming pattern for locks and semaphores in which the first thread to enter a critical section performs a specific initial action and the last to leave performs a specific closing action.

link layer

The second-lowest layer of the Internet protocol stack, creating the logical point-to-point links between devices connected within a network.

Link Manager/Link Controller

Abbreviated as LM/LC. The second lowest layer in the Bluetooth wireless protocol stack, responsible for managing and controlling the baseband.

link-state routing

A protocol strategy in which all routers initially exchange information about the network structure prior to determining optimal routing paths.

livelock

A situation in which multiple processes are simultaneously preventing each other from progressing, but the processes are repeatedly changing state in an unproductive fashion.

liveness

A synchronization property that guarantees some entity will eventually be able to continue execution toward a goal; also known as progress.

LM/LC

Abbreviation for [Link Manager/Link Controller](#term-link-manager-link-controller).

local area network

Abbreviated as LAN. A network of computer systems operated by a single independent organization.

local P2P index

A P2P location-mapping strategy in which each node maintains information only about objects that it stores or has recently accessed; used in open, unstructured networks such as Bittorrent.

lock

A [synchronization primitive](#term-synchronization-primitive) that provides mechanism to gain mutually exclusive access to a resource.

logical clock

A distributed system technique for ordering events that is not dependent on real-time measurements.

Logical Link Control and Adaptation Protocol

Abbreviated as L2CAP. The middle Bluetooth layer that provides multiplexing service for application profiles to access the LM/LC layer; provides services comparable to the link layer in the 5-layer Internet model.

loop parallelism

A parallel programming strategy in which multiple iterations of a loop can be executed in parallel and will achieve the same result as if executed sequentially.

loopback address

The IP address that refers to the current host.

LWP

Abbreviation for [light-weight process](#term-light-weight-process).

MAC

Abbreviation for [message authentication code](#term-message-authentication-code). Also used as an abbreviation for [media access control address](#term-media-access-control-address).

MAC address

Abbreviation for [media access control address](#term-media-access-control-address).

MAC-then-Encrypt

Abbreviated as MtE. A cryptographic technique that can be used to establish a message’s authenticity by encrypting the output of a message’s cryptographic hash; used in TLS 1.2 but replaced in TLS 1.3, due to vulnerability to a ciphertext reuse attack.

manager/worker

A task parallelism implementation strategy in which one thread is designated as a manager that distributes tasks to other worker threads as needed.

MANET

Abbreviation for [mobile ad hoc network](#term-mobile-ad-hoc-network).

map/reduce pattern

A parallel programming implementation pattern in which a problem is broken down into small, independent problems that are distributed to nodes; the results of the parallel computations are then merged as needed.

maximum transmit unit

Abbreviated as MTU. The largest size of transmission message that a physical device will allow.

MCAP

Abbreviation for [Multichannel Adaptation Protocol](#term-multichannel-adaptation-protocol).

media access control address

Abbreviated as MAC address. A link-layer address used to identify the unique hosts connected within a local network segment.

memory-mapped file

A range of memory addresses that are linked to a file, possibly stored on disk.

Merkle-Damgård construction

A method for constructing cryptographic hash functions; used in the SHA-2 family.

message authentication code

Abbreviated as MAC. The result of applying a keyed cryptographic hash function to a message.

message passing

One of two standard [IPC](#term-ipc) models; there is low overhead to set up a communication channel, but each data exchange requires a system call and imposes a performance cost.

message queue

A form of message-passing [IPC](#term-ipc) in which structured messages are delivered as a single unit.

metadata

Information about a file or message that is not considered part of the file or message itself; can include information such as size, permissions, user identities, access times, or other related data.

MIMD

The multiple instruction, multiple data classification of Flynn’s taxonomy.

MIMO

Abbreviation for [Multiple input and multiple output](#term-multiple-input-and-multiple-output).

MISD

The multiple instruction, single data classification of Flynn’s taxonomy.

mobile ad hoc network

Abbreviated as MANET. A network of mobile computing devices that is designed to be short-lived and to require minimal configuration.

mode switch

Changing the CPL between kernel and user mode.

model

A simplified representation of a system.

monitor

An early piece of software responsible for setting up and executing jobs; the monitor is an early stage of the evolution of the kernel.

monitor (synchronization)

A software construct in which shared data is encapsulated into a structure and access to the data is only permitted through an interface of functions (or methods) that use a lock to ensure mutually exclusive access to the entire structure.

MtE

Abbreviation for [MAC-then-Encrypt](#term-mac-then-encrypt).

MTU

Abbreviation for [maximum transmit unit](#term-maximum-transmit-unit).

Multichannel Adaptation Protocol

Abbreviated as MCAP. A common Bluetooth transport layer used for medical devices that use the [HDP](#term-hdp) profile.

multicore

A type of multiprocessing hardware in which a processor contains more than one unit that can execute instructions independently and in parallel.

multiple input and multiple output

Abbreviated as MIMO. Wi-Fi technology that uses multiple antennas to support the simultaneous transmission and receipt of wireless information.

multiplexing (networking)

The transport-layer service that allows multiple processes to share the ability to send data to the network.

multiplexing (semaphore)

The practice of using a semaphore to provide concurrent access to a resource, but with a maximum number of allowed instances.

multiprocessing

The ability for a computer system to execute instructions in parallel.

multiprogramming

A kernel execution strategy in which multiple processes are loaded into memory, with the processes taking turns executing instructions on the CPU for a short time.

multitasking

A synonym for multiprogramming.

multithreading

The software ability to create multiple concurrent threads of execution.

mutex

A synonym for lock; a POSIX thread [synchronization primitive](#term-synchronization-primitive) that can be used to provide mutually exclusive access to a critical section.

mutual exclusion

At most one entity (such as a thread) has access to a resource or critical section at a given time; also known as safety.

mutual exclusion (deadlock condition)

The system property that a resource can be held by at most one thread or process at a time; one of the four requirements (along with circular wait, hold and wait, and no preemption) for deadlock.

name server

A DNS server that translates a human-readable domain name into an IP address.

named pipe

A POSIX pipe that is identified using a pipe, in contrast to anonymous pipes created with the pipe() system call.

named semaphore

A POSIX [semaphore](#term-semaphore) that is identified using a file name, in contrast to unnamed semaphores created using the sem_init() system call.

NAT

Abbreviation for [network address translation](#term-network-address-translation).

National Institute of Standards and Technologies

Abbreviated as NIST. Part of the U.S. government that is responsible for defining standards for cryptography and other technologies.

netlink socket

A Linux [socket](#term-socket) that allows application processes to communicate directly with a kernel service.

network address translation

Abbreviated as NAT. Local network technique that extends the number of devices that can use IPv4 by sharing a common Internet-facing IP address among several hosts on a private network.

network core

The collection of routers and switches that create the internal connections within a network or the Internet.

network edge

The collection of hosts at the outer-most end-points of a network, including laptops and other user-focused devices.

Network Time Protocol

Abbreviated as NTP. A distributed system protocol that aims to configure the system clocks of nodes to a common real time value.

NIST

Abbreviation for [National Institute of Standards and Technologies](#term-national-institute-of-standards-and-technologies).

no preemption

The system property that no thread may disrupt another thread’s claim to a resource, such as a lock or semaphore; one of the four requirements (along with circular wait, hold and wait, and mutual exclusion) for deadlock.

non-uniform memory access

Abbreviated as NUMA. The characteristic of memory systems in which accessing different ranges of addresses can require different amounts of time.

nondeterminism

An unpredictable ordering of events.

NTP

Abbreviation for [Network Time Protocol](#term-network-time-protocol).

NUMA

Abbreviation for [non-uniform memory access](#term-non-uniform-memory-access).

OBEX

Abbreviation for [Object Exchange](#term-object-exchange).

Object Exchange

Abbreviated as OBEX. A [Bluetooth profile](#term-bluetooth-profile) for exchanging files and objects between connected devices.

octet

A sequence of eight bits; normally a synonym for byte, although a byte does not require exactly eight bits.

one-time pad

A cryptographic technique in which every bit of plaintext is combined with a randomly chosen 1 or 0; achieves information-theoretic security, but is not practical to use.

Open Shortest Path First

Abbreviated as OSPF. A network-layer control plane protocol that uses link-state routing based on Dijkstra’s algorithm to determine routing paths within an autonomous system.

OpenMP

A multiprocessing library for C, C++, and Fortran that provides implicit threading.

operating system

Abbreviated as OS. The collection of system software that provides a platform for running applications; the kernel is the software component that provides the foundation of the OS structure.

OS

Abbreviation for [operating system](#term-operating-system).

OSPF

Abbreviation for [Open Shortest Path First](#term-open-shortest-path-first).

overlay network

A logical network, such as a P2P system, that consists of nodes that are distributed throughout an underlying network, such as the Internet.

P2P architecture

Abbreviation for [peer-to-peer architecture](#term-peer-to-peer-architecture).

packet

A structured network message, especially one at the network-layer of the Internet.

packet loss

The failure of a packet to be delivered to its intended host.

packet sniffer

A program that receives a copy of network packets intended for other processes.

packet switching

A network technology in which hosts transmit small, structured packets into a network immediately rather than waiting for dedicated control of the network; contrasted with circuit switching.

parallelism

The simultaneous execution of instructions or calculations.

Paxos

A family of consensus protocols for distributed systems.

payload

The portion of a network message that contains the data used at the next layer higher.

PBFT

Abbreviation for [Practical Byzantine Fault Tolerance](#term-practical-byzantine-fault-tolerance).

peer-to-peer architecture

Abbreviated as P2P architecture. A variant of a client/server architecture in which each entity acts as both a client and a server as needed.

peering agreement

Business and financial arrangements between Tier 1 ISPs to facilitate the exchange of Internet data between networks.

persistent connection

In HTTP, a logical connection between multiple requests and responses that allow the hosts to maintain a single TCP session.

Peterson’s solution

An algorithmic solution to the critical section problem; impractical in modern systems due to the design of processor architectures and compilers.

phase shift keying

Manipulating the phase of a carrier signal to encode binary data.

physical layer

The lowest layer of the Internet model, encoding bits for transmission across a physical medium.

physical memory

A hardware component that can be used to store information; RAM is one form of physical memory.

PID

Abbreviation for [process identifier](#term-process-identifier).

pipe

A first-in, first-out message-passing [IPC](#term-ipc) in which bytes are sent and retrieved as unstructured streams.

pipe-and-filter architecture

A system architecture defined by a unidirectional flow of information through a sequence of entities that can modify and process the information.

pipelining

A parallel programming algorithm design strategy in which a procedure is broken down into multiple sequential steps that allow for parallel execution.

plaintext

The meaningful data provided as input to an encryption routine.

port

A network end-point.

port number

A 16-bit unsigned integer used to identify a process on a host.

Portable Operating System Interface

Abbreviated as POSIX. A cross-platform specification supported by UNIX operating systems and those considered UNIX-like, such as Linux. The X in the name originally denoted that the interface was “based on UNIX.”

POSIX

Abbreviation for [Portable Operating System Interface](#term-portable-operating-system-interface).

POSIX thread library

Abbreviated as pthreads. A C programming language library that provides cross-platform support for multithreading.

Practical Byzantine Fault Tolerance

Abbreviated as PBFT. A distributed replication service that provides reliable service to stored data, provided no more than 1/3 of nodes fail.

pragmatics

One of three aspects of semiotics that focuses on the relationship between symbols and the entity interpreting the symbol.

pre-shared key

Abbreviated as PSK. A key exchange technique used in TLS; both entities are configured with a key that can be used to securely encrypt and transmit a session key.

preemptive multitasking

A multiprogramming strategy in which processes are granted a time-limited access to the CPU and interrupted when that time limit expires.

privileged instruction

Hardware instruction that can affect the full system rather than just the current process; privileged instructions cannot be executed in user mode.

process

A logical instance of a program during its execution; processes are distinguished by having a unique and isolated view of the virtual memory image that can be accessed.

process identifier

Abbreviated as PID. A number used by the kernel to identify a process.

processing delay

The time required for a switch or router to read network headers and determine how to process a packet.

Producer-Consumer Problem

A classical synchronization problem that provides well-known proven solutions for controlling access to a shared queue.

progress

A synchronization property that guarantees some entity will eventually be able to continue execution toward a goal; also known as liveness.

propagation delay

The time required for bits to traverse the physical medium between devices in a network.

protocol

A precise specification of the structure and intended meaning of communication messages.

pseudo-header

A sequence of bytes used in TCP and UDP to compute a checksum.

PSK

Abbreviation for [pre-shared key](#term-pre-shared-key).

pthreads

Abbreviation for [POSIX thread library](#term-posix-thread-library).

public key cryptography

A form of encryption that uses separate keys (one public, one private) for encryption and decryption.

pull model

A thread pool execution strategy in which threads access a common pool and select a task.

push model

A thread pool execution strategy in which one thread is responsible for assigning tasks from the common pool.

quantum

The maximum amount of continuous CPU time granted to a process in multiprogramming.

query flooding

A search strategy used in [unstructured P2P network](#term-unstructured-p2p-network) systems in which requests are forwarded from one node to its neighbors until the desired object’s location is found.

queueing delay

The time that a network packet must wait in a queue prior to transmission.

race condition

A situation where the result of a computation depends on the timing of events during a particular instance of execution; an error that results from nondeterministic system design or programming.

raw socket

A socket for writing data directly to the link layer without processing by the transport or network layers; used in protocols, such as ICMP, that can be used to monitor the behavior and performance of the network itself.

reactive

A system characteristic that indicates the primary purpose is to respond to events rather than to cause them to occur.

Readers-Writers Problem

A classical synchronization problem that highlights the difficulty of providing fair access to a critical section between two types of threads that perform asymmetric actions.

receive window

A field in the TCP header that is used to indicate a maximum amount of data that should be sent in response to the current segment; used to create the flow control service.

recursive splitting

Another name for [divide-and-conquer algorithm](#term-divide-and-conquer-algorithm).

redundancy

The practice of using multiple instances to serve as back-up copies if needed.

reentrant

A software property that the execution of a function can be safely interrupted and restarted by a separate thread without causing an error in the original execution.

release

The act of using a lock’s interface to relinquish mutually exclusive access to a critical section.

reliable transport

A communication protocol service, provided by TCP, in which lost packets will be retransmitted in an effort to overcome intermittent network failures.

rendezvous

A synchronization programming pattern for semaphores in which two threads must reach a common point before either can progress; can be generalized as a barrier.

replication

Storing the same data on multiple nodes in a network, allowing each to serve as backups in case of intermittent failures or attacks.

replication

A distributed system service in which multiple copies of data are stored to increase the availability of the resource.

Request for Comment

Abbreviated as RFC. The standard format for documents that define the protocols used in the Internet.

request-response protocol

A simple protocol in which one entity issues a request and the exchange is completed with the response from the other entity.

resident monitor

An early form of the kernel, in which the software for handling common events was loaded as part of the software job.

resource manager

One of the primary roles of the kernel, which is to provide shared access to system resources such as hardware peripherals.

retransmission timeout

Abbreviated as RTO. The amount of time a host will wait until declaring a packet to be lost in TCP.

RFC

Abbreviation for [Request for Comment](#term-request-for-comment).

RFCOMM

A [Bluetooth profile](#term-bluetooth-profile) that can be used as a generic serial port.

ring

A term used in the x86 family of processors as a synonym for the CPL; ring 0 is known as kernel mode and ring 3 is user mode.

RIP

Abbreviation for [Routing Information Protocol](#term-routing-information-protocol).

root name server

One of several servers that form the top-most layer of the DNS hierarchy; root name servers are primarily responsible for identifying the IP addresses of top-level domains.

round-trip time

Abbreviated as RTT. The amount of time between the sending of a TCP segment and when the corresponding acknowledgement is received.

router

A device that acts as a connection between two, possibly heterogeneous, networks.

routing

Identifying the path between nodes in a network or between networks.

Routing Information Protocol

Abbreviated as RIP. A network-layer control plane protocol that uses dynamic programming to perform distance vector routing within an autonomous system.

RSA cryptosystem

A public-key encryption technology designed by Ron Rivest, Adi Shamir, and Len Adleman based on modular exponentiation.

RTO

Abbreviation for [retransmission timeout](#term-retransmission-timeout).

RTT

Abbreviation for [round-trip time](#term-round-trip-time).

Rust closure

A Rust programming language construct that acts like an anonymous function.

safety

At most one entity (such as a thread) has access to a resource or critical section at a given time; also known as mutual exclusion.

scale

A system characteristic that refers to how the system behaves as the number of entities increases significantly.

scarcity of resources

A system characteristic that describes how resource limitations can pose constraints on the system design.

scheduling

The multiprogramming kernel responsibility to select which process to execute in user mode whenever an interrupt or exception occurs.

Search-Insert-Delete Problem

A variant of the readers-writers problem that illustrates how synchronization design techniques can be applied to common data structures.

security vs. usability

A common design tradeoff that arises from the fact that making a system more secure can make it more difficult to use.

segment

A structured transport-layer network message.

semantic security

The cryptographic property that an adversary has only a negligible probability to determine the plaintext message for a ciphertext, given limited time and resources; contrasted with information theoretic security.

semantics

One of three aspects of semiotics that focuses on the intended meaning of symbols.

semaphore

An integer with atomic operations for incrementing and decrementing the value; if the result of decrementing the value is negative, the current process becomes blocked until another process increments the value.

semiotics

The study of the use and interpretation of symbols.

sequence model

A type of UML diagram that can be used to illustrate the order of messages exchanged between entities and the corresponding responses.

sequence number

An integer used in TCP to identify the order of a segment; a particular segment’s sequence number is the previous segment’s sequence number plus the size of the previous segment’s payload.

session

A finite sequence of messages between entities until one or both parties determine that the exchange is complete.

session key

A randomly generated symmetric key used to encrypt and decrypt messages in a single session; should never be reused in future sessions to prevent eavesdropping attacks.

SHA-2

A family of cryptographic hash functions published by NIST to replace the insecure SHA-1 family; based on the Merkle-Damsgård construction technique.

SHA-3

A family of cryptographic hash functions published by NIST as an alternative—not a replacement—to the SHA-2 family; based on the sponge construction technique.

shared memory

One of two standard [IPC](#term-ipc) models; there is a significant performance cost to set up the shared region, but all subsequent data exchanges are immediate and do not require system calls.

signal

A pre-defined event that can be sent to another process to disrupt its normal execution.

signaling (synchronization)

A synchronization programming pattern for semaphores in which one thread can alert another that an event has occurred; often replaced with condition variables in modern usage.

SIMD

The single instruction, multiple data classification of Flynn’s taxonomy.

SISD

The single instruction, single data classification of Flynn’s taxonomy.

smoothed round-trip time

Abbreviated as SRTT. A rolling average of the RTT calculations; helps to prevent overreactions to outlier measurements by considering the history of previous results.

SMP

Abbreviation for [symmetric multiprocessing](#term-symmetric-multiprocessing).

socket

A form of message-passing [IPC](#term-ipc) that is primarily used for network communication.

space/time tradeoff

A common compromise to use more space for faster execution or slower execution for less memory usage.

speedup factor

A numerical factor that quantifies how much faster one implementation is than another.

speedup in latency

An improvement in the amount of time required for a computation by executing some portion of it in parallel.

spinlock

A [synchronization primitive](#term-synchronization-primitive) that provides the same functionality of a lock; uses busy-waiting instead of blocking when the spinlock has previously been acquired by another thread.

sponge construction

A method for constructing cryptographic hash functions; used in the SHA-3 family.

SRTT

Abbreviation for [smoothed round-trip time](#term-smoothed-round-trip-time).

star topology

A network topology in which a single node forms the center that other devices connect to individually.

starvation

The situation where a non-deadlocked thread is persistently prevented from accessing a critical section due to unfortunate timing or priority levels.

state

A significant and meaningful configuration of a system.

state machine

A system that executes by changing from one state to another.

state model

A type of UML diagram that can be used to illustrate how an entity’s state changes in response to events.

state space explosion

The phenomenon that increasing the number of states in a system leads to exponential growth in the number of possible transitions, making it infeasible to build a comprehensible model.

stateless protocol

A protocol in which none of the communicating parties keep state information between messages.

static IP address

A persistently configured IP address that is not dependent on the network configuration.

static model

A system model that focuses on the persistent features of a system that do not change over time.

strong scaling

A characteristic of a problem in which the difficulty of the problem does not increase with the number of entities operating in parallel.

structural model

Another name for [static model](#term-static-model).

structured P2P network

A style of P2P network in which nodes are arranged in a logical structure, such as a circle, to support predictable routing and increased availability of objects through replication.

subdomain

A domain name—ending in the organization’s authoritative domain name—that is typically used to add logical structure to the organization’s servers.

subnet

A smaller network that is part of a larger network, identified by a common range of network addresses.

subnet mask

A bitmask that can be applied to any address in a subnet to determine a common routing prefix.

suspended state

A process state in which the process has been indefinitely paused by the kernel, system administrator, or the user executing the process.

switch

A device that provides a link-layer connection between two homogeneous networks.

symmetric key cryptography

A form of encryption that uses a single key for both encryption and decryption.

symmetric multiprocessing

Abbreviated as SMP. A type of multiprocessing system that consists of multiple processors with a shared memory resource in a single computer.

SYN flood

A denial-of-service attack in which one or more hosts repeatedly initiate a TCP handshake with a server with SYN packets to cause the server to exhaust its memory resources, preventing legitimate requests from establishing connections.

synchronization

The act of controlling the timing of concurrent threads or processes.

synchronization primitive

A programming language construct that provides atomic operations for basic synchronization techniques.

synchronization problem

An easily understood scenario that illustrates common challenges that arise in concurrent systems.

synchronous

A style of communication in which the timing of sending and receiving are closely linked, forcing senders to wait until a message has been received.

syntax

One of three aspects of semiotics that focuses on the rules that define how symbols must be linked to create valid messages.

system

An integrated collection of entities and their interactions.

system architecture

A static model of a system that illustrates how entities can interact with each other.

system call

A request from a process for the kernel to perform an action.

System V

The specification that defines the requirements for an operating system to be considered UNIX.

systems theory

The rigorous study of systems and their properties.

task parallelism

A parallel programming strategy in which different threads are assigned different tasks to perform.

task queue

A data structure that stores the tasks to be performed by a thread pool.

TCP

Abbreviation for [Transmission Control Protocol](#term-transmission-control-protocol).

TCP handshake

A three-step protocol to establish a connection with a TCP server; the client initiates the procedure with a SYN packet, then the server responds with a SYN-ACK packet, and the client completes the handshake with an ACK packet.

termination (consensus protocol)

The characteristic that all correct nodes will eventually determine a value to propose.

Therac-25

A radiation therapy machine that is a commonly cited example of a race condition that can lead to catastrophic and fatal results.

thread

A coherent and independent execution sequence of software instructions.

thread pool

A parallel programming execution strategy in which a collection of threads are created at the start of the process and persist until completion; threads will perform parallel computations when required and may sit idle at other times.

thread-safe

A software property that a function uses appropriate synchronization techniques to prevent race conditions, allowing multiple threads to call the function concurrently in a safe manner.

throughput

The amount of work that can be accomplished in a given amount of time.

Tier 1 Internet service provider

One of several organizations that provide mutually beneficial Internet service by routing data cooperatively between each other’s clients.

TLD

Abbreviation for [top-level domain](#term-top-level-domain).

TLS

Abbreviation for [Transport-Layer Security](#term-transport-layer-security).

TLS handshake

A multistep procedure in which hosts exchange security configuration parameters to establish a common set of cryptographic protocols and keys for secure communication.

top-level domain

Abbreviated as TLD. One of several standard domain name extensions, such as “.com” or “.co.uk”, that can indicate the type of service or origin of an organization.

transition

A change from one system state to another.

Transmission Control Protocol

Abbreviated as TCP. A transport-layer protocol that provides reliable data transfer, flow control, and congestion control.

transmission delay

The time required to encode binary data into modulated radio or light signals within a communication medium.

transport layer

The second-highest layer of the Internet model, providing logical process-to-process (also called end-to-end) communication across a network.

Transport-Layer Security

Abbreviated as TLS. A suite of cryptographic protocols that facilitate secure communication of transport-layer segments; the successor of the deprecated Secure Sockets Layer (SSL).

trap instruction

A hardware instruction executed during a system call that results in a mode switch and jump to a portion of the kernel code.

turnstile

A synchronization programming pattern for semaphores in which several threads wait for an event, then each thread unblocks exactly one more after the first is unblocked.

twisted-pair wire

A type of cable wiring, typically made of copper, used in Ethernet networks.

UDP

Abbreviation for [User Datagram Protocol](#term-user-datagram-protocol).

ULA

Abbreviation for [unique local address](#term-unique-local-address).

UML

Abbreviation for [Unified Modeling Language](#term-unified-modeling-language).

unified cache

A cache design in which all portions can store both instructions and data.

Unified Modeling Language

Abbreviated as UML. A formal language that is widely used to define and model several aspects of computer systems and software design.

uniform resource identifier

Abbreviated as URI. A standard format for designating the location of an object on the Internet.

uniprogramming

A kernel execution strategy in which programs are executed one at a time in a sequential order.

unique local address

Abbreviated as ULA. An IPv6 address that can be used to create a private network without public registration.

UNIX domain socket

A [socket](#term-socket) that provides a form of local [IPC](#term-ipc).

UNIX file abstraction

A uniform interface model in which all devices are treated as a file, which simply refers to an abstract sequence of bytes without assumed structure.

unnamed semaphore

A POSIX [semaphore](#term-semaphore) that is created as an in-memory kernel object, in contrast to named semaphores created as file system objects.

unreliable transport

The transport-layer service that provides no guarantees that a message will be successfully delivered and no additional attempts will be made if the first fails.

unstructured P2P network

A type of P2P network in which nodes and object locations do not have a pre-defined logical structure, thus requiring expensive dynamic searches for objects.

URI

Abbreviation for [uniform resource identifier](#term-uniform-resource-identifier).

User Datagram Protocol

Abbreviated as UDP. A transport-layer protocol that provides only unreliable transport; used in multimedia applications and network-centric protocols such as DNS and DHCP.

user mode

The CPU configuration used by normal software applications, in which privileged instructions cannot be executed.

utilization

The ratio that describes how much a resource was actually used compared to the potential amount that it could have been used.

vector clock

A technique for creating a logical clock in a distributed system.

virtual memory

The logical, linear memory address space visible to a process.

visual model

A pictorial representation of a system intended to convey information in an intuitive manner that can be easily understood by people.

weak scaling

A characteristic of a problem in which increasing the number of parallel entities increases the complexity of the problem, but parallelism improves the amount of work that can be done.

well-known port

One of several designated port numbers that are used for common Internet services.

Wi-Fi

The common name for wireless technologies based on the IEEE 802.11 standards that allow portable computing devices, such as laptops and cell phones, to access the Internet through a physical close access point.

Wi-Fi Alliance

A non-profit organization responsible for defining specifications and certifying compatibility of Wi-Fi devices.

wireless mesh network

A wireless network topology in which router nodes can forward data through multiple possible paths.

X.509 certificate

A standard, verifiable data structure for exchanging public key and identity information.

Zigbee

A wireless protocol stack that can be used to build a wireless mesh network; commonly used in low-power sensor networks and the Internet of Things.

Zigbee Alliance

The industry standards group that defines the specifications for the Zigbee short-range wireless technology.





//11.2. Bibliography[¶]
=======================

1. [Bauman2019]

    A. Baumann, J. Appavoo, O. Krieger, and T. Roscoe. 2019. A `fork()` in the road. Workshop on Hot Topics in Operating Systems (HotOS ‘19).

2. [BenAri2006]

    M. Ben-Ari, _Principles of Concurrent and Distributed Programming_ Second Edition, Addison-Wesley, 2006.

3. [Bovet2005]

    D.P. Bovet and M. Cesati, _Understanding the Linux Kernel_, Third Edition, O’Reilly Media, 2005.

4. [Breshears2009]

    C. Breshears, _The Art of Concurrency: A Thread Monkey’s Guide to Writing Parallel Applications_ O’Reilly Media, 2009.

5. [Bryant2015]

    R.E. Bryant and D.R. O’Hallaron, _Computer Systems: A Programmer’s Perspective_, Third Edition, Pearson, 2015.

6. [Butenhof1997]

    D.R. Butenhof, _Programming with POSIX Threads_ Addison-Wesley Professional, 1997.

7. [Castro2002]

    M. Castro and B. Liskov. 2002. Practical Byzantine Fault Tolerance and Proactive Recovery. ACM Transactions on Computer Systems 20, 4 (November 2002), 398-461.

8. [Chandra2001]

    R. Chandra et al., _Parallel Programming in OpenMP_ CA: Morgan Kaufmann, 2001.

9. [Comer2015]

    D. Comer, _Operating System Design: The Xinu Approach_, Second Edition, Chapman and Hall/CRC, 2015.

10. [Copeland2010]

    J. Copeland, T. Flowers, et al., _Colossus: The Secrets of Bletchley Park’s Codebreaking Computers_, Oxford University Press, 2010.

11. [Coulouris2012]

    G. Coulouris et al., _Distributed Systems: Concepts and Design_ Fifth Edition, Addison-Wesley, 2012.

12. [Downey2008]

    A.B. Downey, _The Little Book of Semaphores_ Second Edition, Green Tea Press, 2008. Available at [http://greenteapress.com/semaphores](http://greenteapress.com/semaphores).

13. [Fall2012]

    K.R. Fall and W.R. Stevens, _TCP/IP Illustrated, Volume 1: The Protocols_ Second Edition, Addison-Wesley Professional, 2012.

14. [Gove2011]

    D. Gove, _Multicore Application Programming_ Addison-Wesley, 2011.

15. [Hicks2018]

    M. Hicks, _Programmed Inequality: How Britain Discarded Women Technologists and Lost Its Edge in Computing_, MIT Press, 2018.

16. [Kerrisk2010]

    M. Kerrisk, _The Linux Programming Interface_, No Starch Press, 2010.

17. [Kirk2013]

    D.B. Kirk and W.W. Hwu, _Programming Massively Parallel Processors: A Hands-on Approach_ Second Edition, Morgan Kaufmann, 2013.

18. [Kocher2018]

    P. Kocher, J. Horn, A. Fogh, D. Genkin, D. Gruss, W. Haas, M. Hamburg, M. Lipp, S. Mangard, T. Prescher, M. Schwarz, and Y. Yarom. 2019. Spectre attacks: Exploiting speculative execution. 40th IEEE Symposium on Security and Privacy (S&P ‘19).

19. [Kurose2016]

    J. Kurose and K. Ross, _Computer Networking: A Top-Down Approach_ Seventh Edition, Pearson, 2016.

20. [Lamport1978]

    L. Lamport. 1978. Time, clocks, and the ordering of events in a distributed system. Communications of the ACM 21, 7 (July 1978), 558-565.

21. [Lamport1982]

    L. Lamport, R. Shostak, and M. Pease. 1982. The Byzantine generals problem. ACM Transactions on Programming Languages and Systems 4, 3 (July 1982), 382-401.

22. [Lamport2001]

    L. Lamport. 2001. Paxos made simple. ACM SIGACT News (Distributed Computing Column) 32, 4 (December 2001), 51-58.

23. [Lin2009]

    C. Lin and L. Snyder, _Principles of Parallel Programming_ Addison-Wesley, 2009.

24. [Lipp2018]

    M. Lipp, M. Schwarz, D. Gruss, T. Prescher, W. Haas, A. Fogh, J. Horn, S. Mangard, P. Kocher, D. Genkin, Y. Yarom, and M. Hamburg. 2018. Meltdown: Reading kernel memory from user space. 27th USENIX Security Symposium (USENIX Security ‘18).

25. [Love2010]

    R. Love, _Linux Kernel Development_, Third Edition, Addison-Wesley Professional, 2010.

26. [McCool2012]

    M. McCool, A.D. Robison, and J. Reinders, _Structured Parallel Programming: Patterns for Efficient Computation_ Morgan Kaufmann, 2012.

27. [Nakamoto2008]

    S. Nakamoto. 2008. Bitcoin: A peer-to-peer electronic cash system. Available at [https://bitcoin.org/bitcoin.pdf](https://bitcoin.org/bitcoin.pdf).

28. [Pacheco2011]

    P. Pacheco, _An Introduction to Parallel Programming_ Morgan Kaufmann, 2011.

29. [Ratnasamy2001]

    S. Ratnasamy, P. Francis, M. Handley, R. Karp, and S. Shenker. 2001. A scalable content-addressable network. SIGCOMM Comput. Commun. Rev. 31, 4 (October 2001), 161-172.

30. [Renesse2015]

    R. V. Renesse and D. Altınbüken. 2015. Paxos made moderately complex. ACM Computer Surveys, Vol. 47, No. 3 (February 2015).

31. [Rowstron2001]

    A. Rowstron and P. Druschel. 2001. Pastry: Scalable, decentralized object location and routing for large-scale peer-to-peer systems. IFIP/ACM International Conference on Distributed Systems Platforms (Middleware) (November 2001), 329-350.

32. [Silberschatz2012]

    A. Silberschatz, P.B. Galvin, and G. Gagne, _Operating System Concepts_, Ninth Edition, Wiley, 2012.

33. [Singh2006]

    A. Singh, _Mac OS X Internals: A Systems Approach_, Addison-Wesley Professional, 2006.

34. [Stevens2013]

    W.R. Stevens and S.A. Rago, _Advanced Programming in the UNIX Environment_, Pearson, 2013.

35. [Stoica2001]

    I. Stoica, R. Morris, D. Karger, M. F. Kaashoek, and H. Balakrishnan. 2001. Chord: A scalable peer-to-peer lookup service for Internet applications. SIGCOMM Comput. Commun. Rev. 31, 4 (October 2001), 149-160.

36. [Zhao2004]

    B. Y. Zhao, L. Huang, J. Stribling, S. C. Rhea, A. D. Joseph, and J. D. Kubiatowicz. 2004. Tapestry: A resilient global-scale overlay for service deployment. IEEE Journal on Selected Areas in Communications 22, 1 (January 2004), 41-53.


/Index
======================


[**A**](#A) | [**B**](#B) | [**C**](#C) | [**D**](#D) | [**E**](#E) | [**F**](#F) | 
[**G**](#G) | [**H**](#H) | [**I**](#I) | [**J**](#J) | [**K**](#K) | [**L**](#L) | 
[**M**](#M) | [**N**](#N) | [**O**](#O) | [**P**](#P) | [**Q**](#Q) | [**R**](#R) | 
[**S**](#S) | [**T**](#T) | [**U**](#U) | [**V**](#V) | [**W**](#W) | [**X**](#X) | 
[**Z**](#Z)

//A
---

*   [**A2DP**](#term-a2dp)
*   [**access point**](#term-access-point)
*   [**acknowledgement number**](#term-acknowledgement-number)
*   [**acquire**](#term-acquire)
*   [**Address Resolution Protocol**](#term-address-resolution-protocol)
*   [**Advanced Audio Distribution Profile**](#term-advanced-audio-distribution-profile)
*   [**Advanced Encryption Standard**](#term-advanced-encryption-standard)
*   [**AEAD**](#term-aead)
*   [**AES**](#term-aes)
*   [**agreement (consensus protocol)**](#term-agreement-consensus-protocol)
*   [**Amdahl's law**](#term-amdahl-s-law)
*   [**amplitude modulation**](#term-amplitude-modulation)
*   [**application layer**](#term-application-layer)
*   [**architectural style**](#term-architectural-style)

*   [**ARP**](#term-arp)
*   [**ARP cache poisoning**](#term-arp-cache-poisoning)
*   [**AS**](#term-as)
*   [**asynchronous**](#term-asynchronous)
*   [**atomic**](#term-atomic)
*   [**atomic transfer**](#term-atomic-transfer)
*   [**Audio/Video Distribution Transport Protocol**](#term-audio-video-distribution-transport-protocol)
*   [**Authenticated Encryption with Attached Data**](#term-authenticated-encryption-with-attached-data)
*   [**authenticity**](#term-authenticity)
*   [**authoritative name server**](#term-authoritative-name-server)
*   [**autonomous system**](#term-autonomous-system)
*   [**availability**](#term-availability)
*   [**avalanche effect**](#term-avalanche-effect)
*   [**AVDTP**](#term-avdtp)

//B
---

*   [**backbone**](#term-backbone)
*   [**banked cache**](#term-banked-cache)
*   [**barrier**](#term-barrier)
*   [**baseband**](#term-baseband)
*   [**batch**](#term-batch)
*   [**behavioral model**](#term-behavioral-model)
*   [**best effort service**](#term-best-effort-service)
*   [**BGP**](#term-bgp)
*   [**blocked state**](#term-blocked-state)
*   [**blocking I/O**](#term-blocking-i-o)
*   [**Bluetooth profile**](#term-bluetooth-profile)

*   [**Bluetooth SIG**](#term-bluetooth-sig)
*   [**Bluetooth Special Interest Group**](#term-bluetooth-special-interest-group)
*   [**boot loader**](#term-boot-loader)
*   [**boot sequence**](#term-boot-sequence)
*   [**Border Gateway Protocol**](#term-border-gateway-protocol)
*   [**bounded waiting**](#term-bounded-waiting)
*   [**broadcast**](#term-broadcast)
*   [**buffer cache**](#term-buffer-cache)
*   [**buffer overflow**](#term-buffer-overflow)
*   [**Byzantine failure**](#term-byzantine-failure)
*   [**Byzantine fault**](#term-byzantine-fault)
*   [**Byzantine Generals Problem**](#term-byzantine-generals-problem)

//C
---

*   [**CA**](#term-ca)
*   [**canonical name**](#term-canonical-name)
*   [**carrier signal**](#term-carrier-signal)
*   [**CDN**](#term-cdn)
*   [**centralized P2P index**](#term-centralized-p2p-index)
*   [**certificate authority**](#term-certificate-authority)
*   [**Chord**](#term-chord)
*   [**churn**](#term-churn)
*   [**CIDR**](#term-cidr)
*   [**Cigarette Smokers Problem**](#term-cigarette-smokers-problem)
*   [**ciphertext**](#term-ciphertext)
*   [**circuit switching**](#term-circuit-switching)
*   [**circular wait**](#term-circular-wait)
*   [**Classless Inter-Domain Routing**](#term-classless-inter-domain-routing)
*   [**client/server architecture**](#term-client-server-architecture)
*   [**cluster system**](#term-cluster-system)
*   [**collision**](#term-collision)
*   [**complexity**](#term-complexity)
*   [**concurrency**](#term-concurrency)
*   [**condition variable**](#term-condition-variable)

*   [**confidentiality**](#term-confidentiality)
*   [**congestion**](#term-congestion)
*   [**connection-oriented protocol**](#term-connection-oriented-protocol)
*   [**connectionless protocol**](#term-connectionless-protocol)
*   [**consensus protocol**](#term-consensus-protocol)
*   [**content delivery network**](#term-content-delivery-network)
*   [**content provider**](#term-content-provider)
*   [**context switch**](#term-context-switch)
*   [**control plane**](#term-control-plane)
*   [**control program**](#term-control-program)
*   [**cookie**](#term-cookie)
*   [**cooperative multitasking**](#term-cooperative-multitasking)
*   [**CPL**](#term-cpl)
*   [**CRC**](#term-crc)
*   [**critical section**](#term-critical-section)
*   [**cryptographic hash function**](#term-cryptographic-hash-function)
*   [**cryptographic key**](#term-cryptographic-key)
*   [**cryptographic signature**](#term-cryptographic-signature)
*   [**cryptography**](#term-cryptography)
*   [**current privilege level**](#term-current-privilege-level)
*   [**cyclic redundancy check**](#term-cyclic-redundancy-check)

//D
---

*   [**data parallelism**](#term-data-parallelism)
*   [**data plane**](#term-data-plane)
*   [**datagram**](#term-datagram)
*   [**deadlock**](#term-deadlock)
*   [**demultiplexing**](#term-demultiplexing)
*   [**denial-of-service**](#term-denial-of-service)
*   [**detached thread**](#term-detached-thread)
*   [**DHCP**](#term-dhcp)
*   [**DHE**](#term-dhe)
*   [**DHT**](#term-dht)
*   [**Diffie-Hellman exchange**](#term-diffie-hellman-exchange)
*   [**Dijkstra's algorithm**](#term-dijkstra-s-algorithm)

*   [**Dining Philosophers Problem**](#term-dining-philosophers-problem)
*   [**distance vector routing**](#term-distance-vector-routing)
*   [**distributed hash table**](#term-distributed-hash-table)
*   [**distributed P2P index**](#term-distributed-p2p-index)
*   [**distributed system**](#term-distributed-system)
*   [**divide-and-conquer algorithm**](#term-divide-and-conquer-algorithm)
*   [**DNS**](#term-dns)
*   [**Domain Name System**](#term-domain-name-system)
*   [**dual booting**](#term-dual-booting)
*   [**Dynamic Host Configuration Protocol**](#term-dynamic-host-configuration-protocol)
*   [**dynamic IP address**](#term-dynamic-ip-address)
*   [**dynamic model**](#term-dynamic-model)
*   [**dynamic programming**](#term-dynamic-programming)

//E
---

*   [**effect**](#term-effect)
*   [**embarrassingly parallel**](#term-embarrassingly-parallel)
*   [**emergent property**](#term-emergent-property)
*   [**encryption**](#term-encryption)
*   [**end-to-end communication**](#term-end-to-end-communication)

*   [**environment variable**](#term-environment-variable)
*   [**ephemeral port**](#term-ephemeral-port)
*   [**Ethernet**](#term-ethernet)
*   [**event**](#term-event)
*   [**event-driven architecture**](#term-event-driven-architecture)
*   [**exception**](#term-exception)

//F
---

*   [**fairness**](#term-fairness)
*   [**fiber**](#term-fiber)
*   [**fiber-optic wire**](#term-fiber-optic-wire)
*   [**FIFO**](#term-fifo)
*   [**file descriptor**](#term-file-descriptor)
*   [**firmware**](#term-firmware)
*   [**flow control**](#term-flow-control)

*   [**Flynn's taxonomy**](#term-flynn-s-taxonomy)
*   [**fork/join pattern**](#term-fork-join-pattern)
*   [**formal model**](#term-formal-model)
*   [**forwarding**](#term-forwarding)
*   [**fragmentation**](#term-fragmentation)
*   [**frame**](#term-frame)
*   [**frequency modulation**](#term-frequency-modulation)
*   [**function pointer**](#term-function-pointer)

//G
---

*   [**GAP**](#term-gap)
*   [**gateway router**](#term-gateway-router)
*   [**Generic Access Profile**](#term-generic-access-profile)
*   [**GFS**](#term-gfs)

*   [**Go channel**](#term-go-channel)
*   [**Google File System**](#term-google-file-system)
*   [**Goroutine**](#term-goroutine)
*   [**Green threads**](#term-green-threads)
*   [**Gustafson's law**](#term-gustafson-s-law)

//H
---

*   [**Hands-Free Protocol**](#term-hands-free-protocol)
*   [**HDP**](#term-hdp)
*   [**header**](#term-header)
*   [**Health Device Protocol**](#term-health-device-protocol)
*   [**HFP**](#term-hfp)

*   [**HID**](#term-hid)
*   [**hold and wait**](#term-hold-and-wait)
*   [**hop**](#term-hop)
*   [**host**](#term-host)
*   [**Human Interface Device**](#term-human-interface-device)

//I
---

*   [**ICANN**](#term-icann)
*   [**IEEE 802.11**](#term-ieee-802-11)
*   [**IEEE 802.15.4**](#term-ieee-802-15-4)
*   [**IEEE 802.3**](#term-ieee-802-3)
*   [**IETF**](#term-ietf)
*   [**implicit threading**](#term-implicit-threading)
*   [**information theoretic security**](#term-information-theoretic-security)
*   [**inode**](#term-inode)
*   [**integrity**](#term-integrity)
    *   [**(consensus protocol)**](#term-integrity-consensus-protocol)
*   [**interface abstraction**](#term-interface-abstraction)
*   [**interleaved**](#term-interleaved)
*   [**Internet**](#term-internet)

*   [**Internet Corporation for Assigned Names and Numbers**](#term-internet-corporation-for-assigned-names-and-numbers)
*   [**Internet Engineering Task Force**](#term-internet-engineering-task-force)
*   [**Internet layer**](#term-internet-layer)
*   [**Internet model**](#term-internet-model)
*   [**Internet of Things**](#term-internet-of-things)
*   [**Internet Protocol**](#term-internet-protocol)
*   [**Internet Society**](#term-internet-society)
*   [**interprocess communication**](#term-interprocess-communication)
*   [**interrupt**](#term-interrupt)
*   [**IoT**](#term-iot)
*   [**IP**](#term-ip)
*   [**IP masquerading**](#term-ip-masquerading)
*   [**IPC**](#term-ipc)
*   [**ISOC**](#term-isoc)

//J
---

*   [**Java Runnable interface**](#term-java-runnable-interface)
*   [**Java Thread class**](#term-java-thread-class)

*   [**job**](#term-job)
*   [**join**](#term-join)

//K
---

*   [**kernel**](#term-kernel)
*   [**kernel mode**](#term-kernel-mode)
*   [**key exchange protocol**](#term-key-exchange-protocol)

*   [**key pair**](#term-key-pair)
*   [**key schedule**](#term-key-schedule)
*   [**keyed cryptographic hash function**](#term-keyed-cryptographic-hash-function)

//L
---

*   [**L2CAP**](#term-l2cap)
*   [**Lamport timestamp**](#term-lamport-timestamp)
*   [**LAN**](#term-lan)
*   [**latency**](#term-latency)
*   [**layered architecture**](#term-layered-architecture)
*   [**level of abstraction**](#term-level-of-abstraction)
*   [**light-weight process**](#term-light-weight-process)
*   [**lightswitch**](#term-lightswitch)
*   [**link layer**](#term-link-layer)
*   [**Link Manager/Link Controller**](#term-link-manager-link-controller)
*   [**link-state routing**](#term-link-state-routing)

*   [**livelock**](#term-livelock)
*   [**liveness**](#term-liveness)
*   [**LM/LC**](#term-lm-lc)
*   [**local area network**](#term-local-area-network)
*   [**local P2P index**](#term-local-p2p-index)
*   [**lock**](#term-lock)
*   [**logical clock**](#term-logical-clock)
*   [**Logical Link Control and Adaptation Protocol**](#term-logical-link-control-and-adaptation-protocol)
*   [**loop parallelism**](#term-loop-parallelism)
*   [**loopback address**](#term-loopback-address)
*   [**LWP**](#term-lwp)

//M
---

*   [**MAC**](#term-mac)
*   [**MAC address**](#term-mac-address)
*   [**MAC-then-Encrypt**](#term-mac-then-encrypt)
*   [**manager/worker**](#term-manager-worker)
*   [**MANET**](#term-manet)
*   [**map/reduce pattern**](#term-map-reduce-pattern)
*   [**maximum transmit unit**](#term-maximum-transmit-unit)
*   [**MCAP**](#term-mcap)
*   [**media access control address**](#term-media-access-control-address)
*   [**memory-mapped file**](#term-memory-mapped-file)
*   [**Merkle-Damgård construction**](#term-merkle-damgard-construction)
*   [**message authentication code**](#term-message-authentication-code)
*   [**message passing**](#term-message-passing)
*   [**message queue**](#term-message-queue)
*   [**metadata**](#term-metadata)
*   [**MIMD**](#term-mimd)
*   [**MIMO**](#term-mimo)
*   [**MISD**](#term-misd)

*   [**mobile ad hoc network**](#term-mobile-ad-hoc-network)
*   [**mode switch**](#term-mode-switch)
*   [**model**](#term-model)
*   [**monitor**](#term-monitor)
    *   [**(synchronization)**](#term-monitor-synchronization)
*   [**MtE**](#term-mte)
*   [**MTU**](#term-mtu)
*   [**Multichannel Adaptation Protocol**](#term-multichannel-adaptation-protocol)
*   [**multicore**](#term-multicore)
*   [**multiple input and multiple output**](#term-multiple-input-and-multiple-output)
*   [**multiplexing (networking)**](#term-multiplexing-networking)
    *   [**(semaphore)**](#term-multiplexing-semaphore)
*   [**multiprocessing**](#term-multiprocessing)
*   [**multiprogramming**](#term-multiprogramming)
*   [**multitasking**](#term-multitasking)
*   [**multithreading**](#term-multithreading)
*   [**mutex**](#term-mutex)
*   [**mutual exclusion**](#term-mutual-exclusion)
    *   [**(deadlock condition)**](#term-mutual-exclusion-deadlock-condition)

//N
---

*   [**name server**](#term-name-server)
*   [**named pipe**](#term-named-pipe)
*   [**named semaphore**](#term-named-semaphore)
*   [**NAT**](#term-nat)
*   [**National Institute of Standards and Technologies**](#term-national-institute-of-standards-and-technologies)
*   [**netlink socket**](#term-netlink-socket)
*   [**network address translation**](#term-network-address-translation)
*   [**network core**](#term-network-core)

*   [**network edge**](#term-network-edge)
*   [**Network Time Protocol**](#term-network-time-protocol)
*   [**NIST**](#term-nist)
*   [**no preemption**](#term-no-preemption)
*   [**non-uniform memory access**](#term-non-uniform-memory-access)
*   [**nondeterminism**](#term-nondeterminism)
*   [**NTP**](#term-ntp)
*   [**NUMA**](#term-numa)

//O
---

*   [**OBEX**](#term-obex)
*   [**Object Exchange**](#term-object-exchange)
*   [**octet**](#term-octet)
*   [**one-time pad**](#term-one-time-pad)
*   [**Open Shortest Path First**](#term-open-shortest-path-first)

*   [**OpenMP**](#term-openmp)
*   [**operating system**](#term-operating-system)
*   [**OS**](#term-os)
*   [**OSPF**](#term-ospf)
*   [**overlay network**](#term-overlay-network)

//P
---

*   [**P2P architecture**](#term-p2p-architecture)
*   [**packet**](#term-packet)
*   [**packet loss**](#term-packet-loss)
*   [**packet sniffer**](#term-packet-sniffer)
*   [**packet switching**](#term-packet-switching)
*   [**parallelism**](#term-parallelism)
*   [**Paxos**](#term-paxos)
*   [**payload**](#term-payload)
*   [**PBFT**](#term-pbft)
*   [**peer-to-peer architecture**](#term-peer-to-peer-architecture)
*   [**peering agreement**](#term-peering-agreement)
*   [**persistent connection**](#term-persistent-connection)
*   [**Peterson's solution**](#term-peterson-s-solution)
*   [**phase shift keying**](#term-phase-shift-keying)
*   [**physical layer**](#term-physical-layer)
*   [**physical memory**](#term-physical-memory)
*   [**PID**](#term-pid)
*   [**pipe**](#term-pipe)
*   [**pipe-and-filter architecture**](#term-pipe-and-filter-architecture)
*   [**pipelining**](#term-pipelining)
*   [**plaintext**](#term-plaintext)
*   [**port**](#term-port)

*   [**port number**](#term-port-number)
*   [**Portable Operating System Interface**](#term-portable-operating-system-interface)
*   [**POSIX**](#term-posix)
*   [**POSIX thread library**](#term-posix-thread-library)
*   [**Practical Byzantine Fault Tolerance**](#term-practical-byzantine-fault-tolerance)
*   [**pragmatics**](#term-pragmatics)
*   [**pre-shared key**](#term-pre-shared-key)
*   [**preemptive multitasking**](#term-preemptive-multitasking)
*   [**privileged instruction**](#term-privileged-instruction)
*   [**process**](#term-process)
*   [**process identifier**](#term-process-identifier)
*   [**processing delay**](#term-processing-delay)
*   [**Producer-Consumer Problem**](#term-producer-consumer-problem)
*   [**progress**](#term-progress)
*   [**propagation delay**](#term-propagation-delay)
*   [**protocol**](#term-protocol)
*   [**pseudo-header**](#term-pseudo-header)
*   [**PSK**](#term-psk)
*   [**pthreads**](#term-pthreads)
*   [**public key cryptography**](#term-public-key-cryptography)
*   [**pull model**](#term-pull-model)
*   [**push model**](#term-push-model)

//Q
---

*   [**quantum**](#term-quantum)

*   [**query flooding**](#term-query-flooding)
*   [**queueing delay**](#term-queueing-delay)

//R
---

*   [**race condition**](#term-race-condition)
*   [**raw socket**](#term-raw-socket)
*   [**reactive**](#term-reactive)
*   [**Readers-Writers Problem**](#term-readers-writers-problem)
*   [**receive window**](#term-receive-window)
*   [**recursive splitting**](#term-recursive-splitting)
*   [**redundancy**](#term-redundancy)
*   [**reentrant**](#term-reentrant)
*   [**release**](#term-release)
*   [**reliable transport**](#term-reliable-transport)
*   [**rendezvous**](#term-rendezvous)
*   [**replication**](#term-343), [**[1]**](#term-replication)
*   [**Request for Comment**](#term-request-for-comment)
*   [**request-response protocol**](#term-request-response-protocol)
*   [**resident monitor**](#term-resident-monitor)

*   [**resource manager**](#term-resource-manager)
*   [**retransmission timeout**](#term-retransmission-timeout)
*   [**RFC**](#term-rfc)
*   [**RFCOMM**](#term-rfcomm)
*   [**ring**](#term-ring)
*   [**RIP**](#term-rip)
*   [**root name server**](#term-root-name-server)
*   [**round-trip time**](#term-round-trip-time)
*   [**router**](#term-router)
*   [**routing**](#term-routing)
*   [**Routing Information Protocol**](#term-routing-information-protocol)
*   [**RSA cryptosystem**](#term-rsa-cryptosystem)
*   [**RTO**](#term-rto)
*   [**RTT**](#term-rtt)
*   [**Rust closure**](#term-rust-closure)

//S
---

*   [**safety**](#term-safety)
*   [**scale**](#term-scale)
*   [**scarcity of resources**](#term-scarcity-of-resources)
*   [**scheduling**](#term-scheduling)
*   [**Search-Insert-Delete Problem**](#term-search-insert-delete-problem)
*   [**security vs. usability**](#term-security-vs-usability)
*   [**segment**](#term-segment)
*   [**semantic security**](#term-semantic-security)
*   [**semantics**](#term-semantics)
*   [**semaphore**](#term-semaphore)
*   [**semiotics**](#term-semiotics)
*   [**sequence model**](#term-sequence-model)
*   [**sequence number**](#term-sequence-number)
*   [**session**](#term-session)
*   [**session key**](#term-session-key)
*   [**SHA-2**](#term-sha-2)
*   [**SHA-3**](#term-sha-3)
*   [**shared memory**](#term-shared-memory)
*   [**signal**](#term-signal)
*   [**signaling (synchronization)**](#term-signaling-synchronization)
*   [**SIMD**](#term-simd)
*   [**SISD**](#term-sisd)
*   [**smoothed round-trip time**](#term-smoothed-round-trip-time)
*   [**SMP**](#term-smp)
*   [**socket**](#term-socket)
*   [**space/time tradeoff**](#term-space-time-tradeoff)
*   [**speedup factor**](#term-speedup-factor)
*   [**speedup in latency**](#term-speedup-in-latency)
*   [**spinlock**](#term-spinlock)
*   [**sponge construction**](#term-sponge-construction)

*   [**SRTT**](#term-srtt)
*   [**star topology**](#term-star-topology)
*   [**starvation**](#term-starvation)
*   [**state**](#term-state)
*   [**state machine**](#term-state-machine)
*   [**state model**](#term-state-model)
*   [**state space explosion**](#term-state-space-explosion)
*   [**stateless protocol**](#term-stateless-protocol)
*   [**static IP address**](#term-static-ip-address)
*   [**static model**](#term-static-model)
*   [**strong scaling**](#term-strong-scaling)
*   [**structural model**](#term-structural-model)
*   [**structured P2P network**](#term-structured-p2p-network)
*   [**subdomain**](#term-subdomain)
*   [**subnet**](#term-subnet)
*   [**subnet mask**](#term-subnet-mask)
*   [**suspended state**](#term-suspended-state)
*   [**switch**](#term-switch)
*   [**symmetric key cryptography**](#term-symmetric-key-cryptography)
*   [**symmetric multiprocessing**](#term-symmetric-multiprocessing)
*   [**SYN flood**](#term-syn-flood)
*   [**synchronization**](#term-synchronization)
*   [**synchronization primitive**](#term-synchronization-primitive)
*   [**synchronization problem**](#term-synchronization-problem)
*   [**synchronous**](#term-synchronous)
*   [**syntax**](#term-syntax)
*   [**system**](#term-system)
*   [**system architecture**](#term-system-architecture)
*   [**system call**](#term-system-call)
*   [**System V**](#term-system-v)
*   [**systems theory**](#term-systems-theory)

//T
---

*   [**task parallelism**](#term-task-parallelism)
*   [**task queue**](#term-task-queue)
*   [**TCP**](#term-tcp)
*   [**TCP handshake**](#term-tcp-handshake)
*   [**termination (consensus protocol)**](#term-termination-consensus-protocol)
*   [**Therac-25**](#term-therac-25)
*   [**thread**](#term-thread)
*   [**thread pool**](#term-thread-pool)
*   [**thread-safe**](#term-thread-safe)
*   [**throughput**](#term-throughput)
*   [**Tier 1 Internet service provider**](#term-tier-1-internet-service-provider)

*   [**TLD**](#term-tld)
*   [**TLS**](#term-tls)
*   [**TLS handshake**](#term-tls-handshake)
*   [**top-level domain**](#term-top-level-domain)
*   [**transition**](#term-transition)
*   [**Transmission Control Protocol**](#term-transmission-control-protocol)
*   [**transmission delay**](#term-transmission-delay)
*   [**transport layer**](#term-transport-layer)
*   [**Transport-Layer Security**](#term-transport-layer-security)
*   [**trap instruction**](#term-trap-instruction)
*   [**turnstile**](#term-turnstile)
*   [**twisted-pair wire**](#term-twisted-pair-wire)

//U
---

*   [**UDP**](#term-udp)
*   [**ULA**](#term-ula)
*   [**UML**](#term-uml)
*   [**unified cache**](#term-unified-cache)
*   [**Unified Modeling Language**](#term-unified-modeling-language)
*   [**uniform resource identifier**](#term-uniform-resource-identifier)
*   [**uniprogramming**](#term-uniprogramming)
*   [**unique local address**](#term-unique-local-address)

*   [**UNIX domain socket**](#term-unix-domain-socket)
*   [**UNIX file abstraction**](#term-unix-file-abstraction)
*   [**unnamed semaphore**](#term-unnamed-semaphore)
*   [**unreliable transport**](#term-unreliable-transport)
*   [**unstructured P2P network**](#term-unstructured-p2p-network)
*   [**URI**](#term-uri)
*   [**User Datagram Protocol**](#term-user-datagram-protocol)
*   [**user mode**](#term-user-mode)
*   [**utilization**](#term-utilization)

//V
---

*   [**vector clock**](#term-vector-clock)a>

*   [**virtual memory**](#term-virtual-memory)
*   [**visual model**](#term-visual-model)

//W
---

*   [**weak scaling**](#term-weak-scaling)
*   [**well-known port**](#term-well-known-port)

*   [**Wi-Fi**](#term-wi-fi)
*   [**Wi-Fi Alliance**](#term-wi-fi-alliance)
*   [**wireless mesh network**](#term-wireless-mesh-network)

//X
---

*   [**X.509 certificate**](#term-x-509-certificate)

//Z
---

*   [**Zigbee**](#term-zigbee)

*   [**Zigbee Alliance**](#term-zigbee-alliance)


/OpenCSF and OpenDSA Licensing
==============================

This textbook consists of two independendent projects, distributed under two separate licenses. The OpenCSF project consists of the following components:

*   All `.html` files within the `OpenCSF/Books/csf/html` directory
*   All `.rst` files within the `OpenCSF/Books/csf/source` directory
*   All `.png` image files within the `OpenCSF/Books/csf/source/Images` directory
*   The `OpenCSF/Books/csf/html/_static/css/jmu.css` cascading style sheet

The OpenCSF project is licensed under a [Creative Commons Attribution-NonCommercial 4.0 International License](http://creativecommons.org/licenses/by-nc/4.0/).

[![Creative Commons License](https://i.creativecommons.org/l/by-nc/4.0/88x31.png)](http://creativecommons.org/licenses/by-nc/4.0/)

All other files consist of software from the [OpenDSA project](https://opendsa-server.cs.vt.edu/). OpenDSA is licensed under an MIT License.


OpenCSF License
===============

OpenCSF Project  
Distributed under the Creative Commons Attribution-NonCommercial 4.0 International Public License (CC-BY-NC)

Copyright (c) 2019-var d = new Date(); document.write(d.getFullYear()) - Michael S. Kirkpatrick

By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.

<a name="s1"></a>
**Section 1 - Definitions.**

1.  **Adapted Material** means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.
2.  **Adapter's License** means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.
3.  **Copyright and Similar Rights** means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section [2(b)(1)-(2)](#s2b) are not Copyright and Similar Rights.
4.  **Effective Technological Measures** means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.
5.  **Exceptions and Limitations** means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.
6.  **Licensed Material** means the artistic or literary work, database, or other material to which the Licensor applied this Public License.
7.  **Licensed Rights** means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.
8.  **Licensor** means the individual(s) or entity(ies) granting rights under this Public License.
9.  **NonCommercial** means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.
10.  **Share** means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.
11.  **Sui Generis Database Rights** means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.
12.  **You** means the individual or entity exercising the Licensed Rights under this Public License. **Your** has a corresponding meaning.

<a name="s2"></a>
**Section 2 - Scope.**

1.  **License grant**.
    1.  Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:
        1.  reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and
        2.  produce, reproduce, and Share Adapted Material for NonCommercial purposes only.
    2.  Exceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.
    3.  Term. The term of this Public License is specified in Section [6(a)](#s6a).
    4.  Media and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section [2(a)(4)](#s2a4) never produces Adapted Material.
    5.  Downstream recipients.
        
        1.  Offer from the Licensor - Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.
        2.  No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.
        
    6.  No endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section [3(a)(1)(A)(i)](#s3a1Ai).
2.  **Other rights**.
    
    1.  Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.
    2.  Patent and trademark rights are not licensed under this Public License.
    3.  To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.

<a name="s3"></a>
**Section 3 - License Conditions.**

Your exercise of the Licensed Rights is expressly made subject to the following conditions.

1.  **Attribution**.
    
    1.  If You Share the Licensed Material (including in modified form), You must:
        1.  retain the following if it is supplied by the Licensor with the Licensed Material:
            1.  identification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);
            2.  a copyright notice;
            3.  a notice that refers to this Public License;
            4.  a notice that refers to the disclaimer of warranties;
            5.  a URI or hyperlink to the Licensed Material to the extent reasonably practicable;
        2.  indicate if You modified the Licensed Material and retain an indication of any previous modifications; and
        3.  indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.
    2.  You may satisfy the conditions in Section [3(a)(1)](#s3a1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.
    3.  If requested by the Licensor, You must remove any of the information required by Section [3(a)(1)(A)](#s3a1A) to the extent reasonably practicable.
    4.  If You Share Adapted Material You produce, the Adapter's License You apply must not prevent recipients of the Adapted Material from complying with this Public License.

<a name="s4"></a>
**Section 4 - Sui Generis Database Rights.**

Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:

1.  for the avoidance of doubt, Section [2(a)(1)](#s2a1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only;
2.  if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and
3.  You must comply with the conditions in Section [3(a)](#s3a) if You Share all or a substantial portion of the contents of the database.

For the avoidance of doubt, this Section [4](#s4) supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.

<a name="s5"></a>
**Section 5 - Disclaimer of Warranties and Limitation of Liability.**

1.  **Unless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.**
2.  **To the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.**

3.  The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.

<a name="s6"></a>
**Section 6 - Term and Termination.**

<a name="s6a"></a>
1.  This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.

<a name="s6b"></a>
2.  Where Your right to use the Licensed Material has terminated under Section [6(a)](#s6a), it reinstates:
    
    1.  automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or
    2.  upon express reinstatement by the Licensor.
    
    For the avoidance of doubt, this Section [6(b)](#s6b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.
3.  For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.
4.  Sections [1](#s1), [5](#s5), [6](#s6), [7](#s7), and [8](#s8) survive termination of this Public License.

<a name="s7"></a>
**Section 7 - Other Terms and Conditions.**

1.  The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.
2.  Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.

<a name="s8"></a>
**Section 8 - Interpretation.**

1.  For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.
2.  To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.
3.  No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.
4.  Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.

OpenDSA License
===============

OpenDSA Project  
Distributed under the MIT License

Copyright (c) 2011-var d = new Date(); document.write(d.getFullYear()) - Ville Karavirta and Cliff Shaffer

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.