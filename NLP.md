# NLP
- [NLP - Natural Language Processing](https://algorithmia.com/blog/introduction-natural-language-processing-nlp)
- [Natural Language Toolkit(NLTK)](http://www.nltk.org/)
- [Apache OpenNLP](https://opennlp.apache.org/)
- [NLP with Python --- Analyzing Text with NLTK](http://www.nltk.org/book_1ed/)
- [自然语言处理是如何工作的？一步步教你构建 NLP 流水线](https://zhuanlan.zhihu.com/p/41850756)
- [夕小瑶的卖萌屋/ NLP](https://www.zhihu.com/people/tsxiyao/posts)
- [万字长文概述NLP中的深度学习技术](https://zhuanlan.zhihu.com/p/57979184)
- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [卷积神经网络 convolutional neural network，CNN](https://mp.weixin.qq.com/s/vcw_b2jBMiPQLW5gbjyVlg)
- []()

自然语言处理 NLP - Natural Language Processing 就是用计算机来处理、理解以及运用人类语言(如中文、英文等)，它属于人工智能的一个分支，是计算机科学与语言学的交叉学科，又常被称为计算语言学。由于自然语言是人类区别于其他动物的根本标志。没有语言，人类的思维也就无从谈起，所以自然语言处理体现了人工智能的最高任务与境界，也就是说，只有当计算机具备了处理自然语言的能力时，机器才算实现了真正的智能。

从研究内容来看，自然语言处理包括语法分析、语义分析、篇章理解等。从应用角度来看，自然语言处理具有广泛的应用前景。特别是在信息时代，自然语言处理的应用包罗万象，例如：机器翻译、手写体和印刷体字符识别、语音识别及文语转换、信息检索、信息抽取与过滤、文本分类与聚类、舆情分析和观点挖掘等，它涉及与语言处理相关的数据挖掘、机器学习、知识获取、知识工程、人工智能研究和与语言计算相关的语言学研究等。

NLP 作为一种人工智能方法，能够处理机器和人类自然语言之间的交互，即 NLP 帮助计算机机器以各种形式使用自然人类语言进行交流，包括进行分析、理解、改变或生成自然语言。主要涉及的范畴如下（维基百科）：

- 中文自动分词
- 词性标注
- 句法分析
- 文本分类
- 信息抽取
- 知识图谱
- 问答系统和自动聊天机器人
- 机器翻译
- 自动摘要

以下都是自然语言处理（NLP）的一些成功应用：

- 搜索引擎，比如谷歌，雅虎等等。谷歌等搜索引擎会通过 NLP 了解到你是一个科技发烧友，所以它会返回科技相关的结果。
- 社交网站信息流，比如 Facebook 的信息流。新闻馈送算法通过自然语言处理了解到你的兴趣，并向你展示相关的广告以及消息，而不是一些无关的信息。
- 语音助手，诸如苹果 Siri。
- 垃圾邮件程序，比如 Google 的垃圾邮件过滤程序，这不仅仅是通常会用到的普通的垃圾邮件过滤，现在，垃圾邮件过滤器会对电子邮件的内容进行分析，看看该邮件是否是垃圾邮件。

事实上，“人工智能”被作为一个研究问题正式提出来的时候，创始人把计算机国际象棋和机器翻译作为两个标志性的任务，认为只要国际象棋系统能够打败人类世界冠军，机器翻译系统达到人类翻译水平，就可以宣告人工智能的胜利。四十年后的1997年，IBM公司的深蓝超级计算机已经能够打败国际象棋世界冠军卡斯帕罗夫。而机器翻译到现在仍无法与人类翻译水平相比，从此可以看出自然语言处理有多么困难！

理解人类语言远比破译密码要复杂得多，1966 年的一份研究报告总结发现，经过十年之久的研究，结果远远未能达到预期，因此支持资金急剧下降，使自然语言处理（特别是机器翻译）的研究陷入长达二十年的低潮。直到二十世纪八十年代，随着电子计算机的计算能力的飞速提高和制造成本的大幅下降，研究者又开始重新关注自然语言处理这个极富挑战的研究领域。三十年沧海桑田，此时研究者已经认识到简单的语言规则的堆砌无法实现对人类语言的真正理解。研究发现，通过对大量的文本数据的自动学习和统计，能够更好地解决自然语言处理问题，如语言的自动翻译。这一思想被称为自然语言处理的**统计学习模型**，至今方兴未艾。

基于机器学习框架的自然语言处理主要困难或挑战很多，不过关键在于**消除歧义**，如词法分析、句法分析、语义分析等过程中存在的歧义问题，简称为消歧。而正确的消歧需要大量的知识，包括语言学知识（如词法、句法、语义、上下文等）和世界知识（与语言无关）。这带来自然语言处理的两个主要困难。

- 场景的困难：语言的多样性、多变性、歧义性
- 学习的困难：艰难的数学模型（hmm, crf, EM, 深度学习等）
- 语料的困难：什么的语料？语料的作用？如何获取语料？

首先，语言中充满了大量的歧义，这主要体现在词法、句法及语义三个层次上。歧义的产生是由于自然语言所描述的对象――人类活动非常复杂，而语言的词汇和句法规则又是有限的，这就造成同一种语言形式可能具有多种含义。

正确的单词切分取决于对文本语义的正确理解，而单词切分又是理解语言的最初的一道工序。这样的一个“鸡生蛋、蛋生鸡”的问题自然成了（中文）自然语言处理的第一条拦路虎。

其他级别的语言单位也存在着各种歧义问题。例如在短语级别上，“进口彩电”可以理解为动宾关系（从国外进口了一批彩电），也可以理解为偏正关系（从国外进口的彩电）。又如在句子级别上，“做手术的是她的父亲”可以理解为她父亲生病了需要做手术，也可以理解为她父亲是医生，帮别人做手术。总之，同样一个单词、短语或者句子有多种可能的理解，表示多种可能的语义。如果不能解决好各级语言单位的歧义问题，我们就无法正确理解语言要表达的意思。

另外一个方面，消除歧义所需要的知识在获取、表达以及运用上存在困难。由于语言处理的复杂性，合适的语言处理方法和模型难以设计。

在试图理解一句话的时候，即使不存在歧义问题，也往往需要考虑上下文的影响。以“小明欺负小亮，因此我批评了他”为例。在其中的第二句话中的“他”是指代“小明”还是“小亮”呢？要正确理解这句话，我们就要理解上句话“小明欺负小亮”意味着“小明”做得不对，因此第二句中的“他”应当指代的是“小明”。由于上下文对于当前句子的暗示形式是多种多样的，因此如何考虑上下文影响问题是自然语言处理中的主要困难之一。 

正确理解人类语言还要有足够的背景知识。举一个简单的例子，在机器翻译研究的初期，人们经常举一个例子来说明机器翻译任务的艰巨性。在英语中“The spirit is willing but the flesh is weak.”，意思是“心有余而力不足”。但是当时的某个机器翻译系统将这句英文翻译到俄语，然后再翻译回英语的时候，却变成了“The Voltka is strong but the meat is rotten.”，意思是“伏特加酒是浓的，但肉却腐烂了”。从字面意义上看，“spirit”（烈性酒）与“Voltka”（伏特加）对译似无问题，而“flesh”和“meat”也都有肉的意思。那么这两句话在意义上为什么会南辕北辙呢？关键的问题就在于在翻译的过程中，机器翻译系统对于英语成语并无了解，仅仅是从字面上进行翻译，结果自然失之毫厘，差之千里。 

从上面的两个方面的主要困难，我们看到自然语言处理这个难题的根源就是人类语言的复杂性和语言描述的外部世界的复杂性。人类语言承担着人类表达情感、交流思想、传播知识等重要功能，因此需要具备强大的灵活性和表达能力，而理解语言所需要的知识又是无止境的。

目前，人们主要通过两种思路来进行自然语言处理：

- 一种是基于规则的理性主义；
- 另外一种是基于统计的经验主义；

理性主义方法认为，人类语言主要是由语言规则来产生和描述的，因此只要能够用适当的形式将人类语言规则表示出来，就能够理解人类语言，并实现语言之间的翻译等各种自然语言处理任务。而经验主义方法则认为，从语言数据中获取语言统计知识，有效建立语言的统计模型。因此只要能够有足够多的用于统计的语言数据，就能够理解人类语言。然而，当面对现实世界充满模糊与不确定性时，这两种方法都面临着各自无法解决的问题。例如，人类语言虽然有一定的规则，但是在真实使用中往往伴随大量的噪音和不规范性。理性主义方法的一大弱点就是鲁棒性差，只要与规则稍有偏离便无法处理。而对于经验主义方法而言，又不能无限地获取语言数据进行统计学习，因此也不能够完美地理解人类语言。二十世纪八十年代以来的趋势就是，基于语言规则的理性主义方法不断受到质疑，大规模语言数据处理成为目前和未来一段时期内自然语言处理的主要研究目标。统计学习方法越来越受到重视，自然语言处理中越来越多地使用机器自动学习的方法来获取语言知识。


迈进二十一世纪，我们已经进入了以互联网为主要标志的海量信息时代，这些海量信息大部分是以自然语言表示的。一方面，海量信息也为计算机学习人类语言提供了更多的“素材”，另一方面，这也为自然语言处理提供了更加宽广的应用舞台。例如，作为自然语言处理的重要应用，搜索引擎逐渐成为人们获取信息的重要工具，涌现出以百度、谷歌等为代表的搜索引擎巨头；机器翻译也从实验室走入寻常百姓家，谷歌、百度等公司都提供了基于海量网络数据的机器翻译和辅助翻译工具；基于自然语言处理的中文（输入法如搜狗、微软、谷歌等输入法）成为计算机用户的必备工具；带有语音识别的计算机和手机也正大行其道，协助用户更有效地工作学习。总之，随着互联网的普及和海量信息的涌现，自然语言处理正在人们的日常生活中扮演着越来越重要的作用。

然而，我们同时面临着一个严峻事实，那就是如何有效利用海量信息已成为制约信息技术发展的一个全局性瓶颈问题。自然语言处理无可避免地成为信息科学技术中长期发展的一个新的战略制高点。同时，人们逐渐意识到，单纯依靠统计方法已经无法快速有效地从海量数据中学习语言知识，只有同时充分发挥基于规则的理性主义方法和基于统计的经验主义方法的各自优势，两者互相补充，才能够更好、更快地进行自然语言处理。

基本概念：

- 文本集合称为语料库 Corpus 当有几个这样的文本集合的时候，称之为语料库集合 Corpora。
- LSTM 长短期记忆网络 Long Short-Term Memory Networks
- GRU 门控循环单元 
- ResNet 和残差网络 
- RNN 循环神经网络 Recurrent Neural Network

    深度学习有一个 Recursive Neural Network 也称为 RNN，注意区别。

    人类的思考具有持续性，当你读这篇文章的时候，你会根据你对前面单词的理解来理解每个单词，你不会把所有的东西都扔掉然后从头开始思考。 

    传统的神经网络无法做到这一点，假设您想对电影中每一点发生的事件进行分类，目前还不清楚传统的神经网络如何利用它对电影中先前事件的推理来告知后来的事件。 

    递归神经网络解决了这个问题，它们是带有循环的网络，允许信息持续存在。

- CNN 卷积神经网络 convolutional neural network

    是一种专门用来处理网格结构数据（例如图像数据）的前馈神经网络，是由生物学家Hubel和Wiesel在早期关于猫脑视觉皮层的研究发展而来。

    卷积 Convolution 是分析数学中一种重要的运算，有着非常广泛的运用，在图像处理中，常用的是二维卷积。以单通道的灰度图像为例，对图像进行卷积操作，就是使用一个卷积核（也称滤波器，在本书中统一称为卷积核）分别与图像的同大小区域进行点乘，卷积核依次从左往右从上往下滑过该图像的所有区域，点乘后得到的矩阵其各个位置上的值累加，作为卷积后图像上的像素值。这种将图像和卷积核进行按位点乘后求和的操作，就是卷积神经网络中的卷积操作。

    卷积神经网络主要有以下三大特性：

    1. 局部连接

    前馈神经网络相邻的两层之间，前一层的每一个神经元与后一层的每一个神经元都有连接，这种情况称为全连接。全连接网络的一个缺点就是参数太多，假设我们输入到神经网络中的是一张三通道的彩色图像，图像像素量 128 * 128 和输入层单元个数一致。使用全连接网络的话，输入层到第一层隐藏层的每一个神经元都有49150个连接，随着网络层数的增加和每一层中神经元数量的增加，网络中的参数也会急剧增加。大量的参数不仅会拉低神经网络训练的效率，也很容易导致过拟合。

    在卷积神经网络中，层与层之间不再是全连接，而是局部连接，具体的实现方法就是我们在4.2节中会介绍的卷积操作。

    2. 权值共享

    在卷积神经网络中，每一层卷积层中都会有一个或者多个卷积核（也称为滤波器），这些卷积核可以识别图像中某些特定的特征，每个卷积核会去滑动卷积上一层的特征图，在卷积的过程中卷积核的参数是不变且共享的。这样在训练过程中，与之前的全连接神经网络训练大尺度输入样本时需要大量参数相比，卷积神经网络只需要相对少很多的参数就可以完成训练。

    3. 子采样

    子采样层（subsamplinglayer）也称作池化层（pooling layer）, 其作用是对上一卷积层进行聚合，使得上一层卷积层的输入特征图尺寸在经过该子采样层的聚合（也就是我们说的池化）后减小，从而降低特征数量，减少参数的数量。子采样层所做的事其实就是对上一层卷积层进行扫描，每次扫描特定区域，然后计算该区域特征的最大值（最大池化（maximum pooling））或者平均值（平均池化（mean pooling）），作为该区域特征的表示。

- DCNN 动态卷积神经网络 
- TDNN 时延神经网络 
- BP 反向传播算法 Backpropagation Algorithm，简称 BP算法，是深度学习的重要思想基础
- BERT - Bidirectional Encoder Representations from Transformers，是预训练语言表示的方法，可以在大型文本语料库（如维基百科）上训练通用的“语言理解”模型，然后将该模型用于下游NLP任务，比如机器翻译、问答。
- SRN 简单循环网络 Simple recurrent networks 又称为 Elman network，是 1990 年由 Jeff Elman 提出来的。



# NLP 库

现在有许多开源的自然语言处理库：

- Natural language toolkit (NLTK)
- Apache OpenNLP
- Stanford NLP suite
- Gate NLP library

自然语言工具包（NLTK）是最受欢迎的自然语言处理（NLP）库。它是用 Python 语言编写的，背后有强大的社区支持。

NLTK 也很容易入门，实际上，它将是你用到的最简单的自然语言处理（NLP）库。


NLP 库的核心功能比较：

|     名称     | SparkNLP | NLTK | SpaCy | CoreNLP |
|--------------|----------|------|-------|---------|
| 句子检测     | ✅        | ✅    | ✅     | ✅       |
| 细粒度单位化 | ✅        | ✅    | ✅     | ✅       |
| 词干提取     | ✅        | ✅    | ✅     | ✅       |
| 语法         | ✅        | ✅    | ✅     | ✅       |
| 磁性标注     | ✅        | ✅    | ✅     | ✅       |
| 命名实体识别 | ✅        | ✅    | ✅     | ✅       |
| 依赖分析     | ✅        | ✅    | ✅     | ✅       |
| 文本匹配     | ✅        | ❌    | ❌     | ✅       |
| 日期匹配     | ✅        | ❌    | ❌     | ✅       |
| 段落分解     | ✅        | ✅    | ✅     | ✅       |
| 拼写检查     | ✅        | ❌    | ❌     | ❌       |
| 情绪检测     | ✅        | ❌    | ❌     | ✅       |
| 预训练模型   | ✅        | ✅    | ✅     | ✅       |
| 训练模型     | ✅        | ✅    | ✅     | ✅       |

现代计算平台和流行编程语言的支持：

|            特性            | SparkNLP | NLTK | spaCy | CoreNLP | OpenNLP |
|----------------------------|-----------|------|-------|---------|---------|
| 完整支持JavaAPI            | ✅        | ❌   | ❌     | ✅     | ✅    |
| 完整支持ScalaAPI           | ✅        | ❌   | ❌     | ❌     | ❌    |
| 完整支持PythonAPI          | ✅        | ✅   | ✅     | ❌     | ❌    |
| 支持GPU训练                | ✅        | ❌   | ✅     | ❌     | ❌    |
| 支持用户定义的深度神经网络   | ✅        | ❌   | ❌     | ❌     | ❌    |
| 原生支持 Spark             | ✅        | ❌   | ❌     | ❌     | ❌    |
| 支持 Hadoop, YARN, HDFS    | ✅        | ❌   | ❌     | ❌    | ❌     |

许可支持的对比：

|   名字   |        语言       |   许可证   | 商业用途 | 商业支持 |
|----------|-------------------|------------|----------|----------|
| SparkNLP | Python Java Scala | Apache 2.0 | ✅       | ✅       |
| spaCy    | Python            | MIT        | ✅       | ✅       |
| NLTK     | Python            | Apache 2.0 | ✅       | ❌       |
| CoreNLP  | Java              | GNU GPL    | 付费许可 | ❌       |
| OpenNLP  | Java              | Apache 2.0 | ✅       | ❌       |

斯坦福大学出售 CoreNLP 的商业许可证，这是商业用途所必需的。为 spaCy 提供商业许可证和支持的 explosion.ai 同样也为快速标注迭代工具 prodigy、机器学习库 thinc 提供许可证。John Snow Labs 提供企业级 SparkNLP 服务，包括基本版，24×7 级别的支持，以及诸如命名实体解析，断言状态检测、ID 脱敏等高级功能。它还为提供医疗领域专用的 SparkNLP，其中包括一套针对生物医学 NLP 的最先进的模型和数据集。

大多数 NLP 库支持用户训练新模型，但 NLP 库具有现有的预训练的高质量模型这一点非常重要。

不过，大多数 NLP 库仅支持通用的预训练模型（POS，NER等）。由于其许可证的要求，某些库根据模型授权状态，不允许将预训练模型作商业用途。

|   名称   | 通用预训练模型 | 领域特定预训练模型 | 许可证是否允许商用 |
|----------|----------------|--------------------|--------------------|
| SparkNLP | ✅             | ✅（医疗领域）     | ✅（通用）         |
| spaCy    | ✅             | ❌                 | ✅（某些GPL许可）  |
| NLTK     | ✅             | ❌                 | ✅                 |
| CoreNLP  | ✅             | ❌                 | ❌                 |
| OpenNLP  | ✅             | ❌                 | ✅                 |

每个库打包的通用预训练模型：

|   名称   | 语法化 | 词性标注 | 命名实体识别 | 依赖关系解析 | 拼写检查 | 情感分析 |
|----------|--------|---------|------------|-------------|---------|----------|
| SparkNLP | ✅    | ✅      | ✅        | ✅         | ✅      | ✅       |
| spaCy    | ✅    | ✅      | ✅        | ✅         | ❌      | ❌       |
| NLTK     | ✅    | ✅      | ✅        | ✅         | ❌      | ❌       |
| CoreNLP  | ✅    | ✅      | ✅        | ✅         | ❌      | ✅       |
| OpenNLP  | ✅    | ✅      | ✅        | ✅         | ❌      | ❌       |



# NLTK
- [结巴 - 中文分词](https://github.com/fxsjy/jieba)

安装：

    pip install nltk

下载相应的语料库和训练模型，以 Brown Corpus 为例：

    >>> import nltk
    >>> nltk.download('browm')

不给 download 函数传入参数时会打开 NLTK Download 图形界面，参考其源代码 downloader.py 中的 DownloaderGUI 类实现。

使用语料库时，NLTK 会在以下目录搜索已经安装的语料库：

    - 'C:\\Users\\OCEAN/nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
    - 'C:\\Anaconda3\\nltk_data'
    - 'C:\\Anaconda3\\share\\nltk_data'
    - 'C:\\Anaconda3\\lib\\nltk_data'
    - 'C:\\Users\\OCEAN\\AppData\\Roaming\\nltk_data'

打印语料库的单词：

    >>> from nltk.corpus import brown
    >>> brown.words()
    ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]

以下程序示范：


示例程序要点：

- urllib.request.urlopen 获取 HTML 页面内容；
- bs4.BeautifulSoup 对 HTML 进行标签过滤得到文本；
- tokens = [t for t in text.split()] 对文本进行分割；
- nltk.FreqDist(tokens) 函数实现词频统计；
- 

有一些词，如"the," "of," "a," "an," 等等。这些词是停止词。一般来说，停止词语应该被删除，以防止它们影响我们的结果。

NLTK 具有大多数语言的停止词表。要获得英文停止词，你可以使用以下代码：

    from nltk.corpus import stopwords
    stopwords.words('english')

这样得到的结果更加清晰，因为没有了停止词的干扰。

使用 nltk 处理中文资料，怎么样分词是个大问题。NLTK 工具目前只能比较好的处理英文和其他的一些拉丁语系，这些语种单词之间有个空格隔开，分词较容易实现！中文汉字一个挨一个的，nltk 在分词这一关就过不去了，分词没法分，剩下的就都做不了。唯一能做的，就是对网上现有的中文语料进行处理，这些语料都分好了词，可以使用 nltk 进行类似与英文的处理。

NLTK 有一个 Sinica中央研究院提供的繁体中文语料库，安装了这个语料库 sinica_treebank。

    import nltk
    from nltk.corpus import sinica_treebank
     
    print(sinica_treebank.words())
    ['一', '友情', '嘉珍', '和', '我', '住在', '同一條', '巷子', '我們', ...]

来看一下 NLTK 中文语法树：

    >>>sinica_treebank.parsed_sents()[33].draw()

搜索中文：

    import nltk
    from nltk.corpus import sinica_treebank
     
    sinica_text=nltk.Text(sinica_treebank.words())
    print(sinica_text.concordance('我'))